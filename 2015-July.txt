From jsorkin at grecc.umaryland.edu  Wed Jul  1 04:22:10 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 30 Jun 2015 22:22:10 -0400
Subject: [R] mixtools? Fitting two-normal distributions to data where one of
 the two normal distributions (the one corresponding to lower values of x)
 is a left-truncated normal distribution.
Message-ID: <55931692020000CB0013180C@smtp.medicine.umaryland.edu>

I am trying to model the mixture of two normal distributions, where x values are in the range of zero to some positive value. I know about mixtools and would use it save for the fact that the the y values from the normal distribution corresponding to the lower values of x (i.e. from zero to x/n) are from what appears to be a left-truncated normal distribution (i.e. the y values are all from the upper half of a normal distribution). The y values from higher values of x (i.e. from x/n to x) all appear to come from a normal distribution. Can someone suggest how to fit two normal distributions where one of the two distributions is left-truncated? Can this be done using mixtools?
Thank you,
John 

John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 


Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From richard.asturia at gmail.com  Wed Jul  1 04:25:01 2015
From: richard.asturia at gmail.com (Richard Asturia)
Date: Tue, 30 Jun 2015 23:25:01 -0300
Subject: [R] Is there a package for Granger causality in panel models?
Message-ID: <CA+xNL7qYJqXput4uaNW=iAdMsbi=dTxAQZMnfuNdC5=32qELDw@mail.gmail.com>

Dear list, I have been searching without success for a package or function
to test for Granger causality-like in panel models. Has something like that
been already implemented?

Thanks,

Richard Asturia

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Wed Jul  1 05:55:45 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 30 Jun 2015 20:55:45 -0700
Subject: [R] Is there a package for Granger causality in panel models?
In-Reply-To: <CA+xNL7qYJqXput4uaNW=iAdMsbi=dTxAQZMnfuNdC5=32qELDw@mail.gmail.com>
References: <CA+xNL7qYJqXput4uaNW=iAdMsbi=dTxAQZMnfuNdC5=32qELDw@mail.gmail.com>
Message-ID: <14C9062E-1815-4EC9-95F4-4126CE7CD846@dcn.davis.CA.us>

I don't know, but I highly recommend the "sos" package so you can answer this for yourself.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On June 30, 2015 7:25:01 PM PDT, Richard Asturia <richard.asturia at gmail.com> wrote:
>Dear list, I have been searching without success for a package or
>function
>to test for Granger causality-like in panel models. Has something like
>that
>been already implemented?
>
>Thanks,
>
>Richard Asturia
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From tcmuigai at gmail.com  Wed Jul  1 06:22:46 2015
From: tcmuigai at gmail.com (Charles Thuo)
Date: Wed, 1 Jul 2015 07:22:46 +0300
Subject: [R] How to run a glm using one factor in a dichotomous categorical
	variable
Message-ID: <CAAJc=rMa9oj9cFa+EoOfAZeOULzwDXe273qJQ65mBqaDjCreNw@mail.gmail.com>

 am running a glm to model the number of accidents that result from diesel
or  petrol cars.


 How can  i run a glm that  on only one factor i.e. diesel only.

require(CASdatasets)
data(freMTPL2freq)
CONTRACTS<-
data.frame(freMTPL2freq$IDpol,freMTPL2freq$ClaimNb,freMTPL2freq$Exposure,freMTPL2freq$VehPower,freMTPL2freq$VehAge,freMTPL2freq$DrivAge,
freMTPL2freq$VehBrand,freMTPL2freq$VehGas,freMTPL2freq$Region,freMTPL2freq$Density)
colnames(CONTRACTS)<-
c("IDpol","ClaimNb","Exposure","VehPower","VehAge","DrivAge","VehBrand","VehGas","Region","Density")
CONTRACTS.f<-CONTRACTS

vY<- CONTRACTS.f$ClaimNb
head(vY)
vE<- CONTRACTS.f$Exposure
head(vE)

X1<-CONTRACTS.f$VehGas;head(X1)
tapply(vY,X1,sum)
tapply(vY,X1,sum)/tapply(vE,X1,sum)
df<- data.frame(vY,vE,X1);head(df);tail(df)
regpoislog<- glm(vY~0+X1+offset(log(vE)),data=df,family=poisson(link="log"))

coef(regpoislog)
 X1Diesel X1Regular
-2.327388 -2.267530

Kindly assist

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Wed Jul  1 08:50:42 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 1 Jul 2015 08:50:42 +0200
Subject: [R] ggmap warning
In-Reply-To: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
References: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
Message-ID: <CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>

Your data contains 4945 rows with missing or infinite values. These cannot
be handled by stat_density2d and are dropped for that reason.

Best regards,

Thierry
 Op 1 jul. 2015 08:43 schreef "Chichi Shu" <chichi.shu at hotmail.com>:

> Dear Listers
>
>
>
> I?ve been using ggmap package to produce crime Heat map. But I?ve noticed
> the following warning message when executing my code:
>
>
>
> In loop_apply(n, do.ply) :
>
>   Removed 4945 rows containing non-finite values (stat_density2d).
>
>
>
> I?ve googled this message but I couldn?t find any good answers.
>
>
>
> Is this related to ggmap package or one of its depending package?
>
>
>
> What does it mean?
>
>
>
> Thanks!
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chabot.denis at gmail.com  Wed Jul  1 12:03:52 2015
From: chabot.denis at gmail.com (Denis Chabot)
Date: Wed, 1 Jul 2015 06:03:52 -0400
Subject: [R] mixtools? Fitting two-normal distributions to data where
	one of the two normal distributions (the one corresponding to
	lower values of x) is a left-truncated normal distribution.
In-Reply-To: <55931692020000CB0013180C@smtp.medicine.umaryland.edu>
References: <55931692020000CB0013180C@smtp.medicine.umaryland.edu>
Message-ID: <8DC37473-B40B-4630-B03F-C47201D17C1B@gmail.com>

Hi John,

I don't know how well it will handle your truncated left distribution, but I use the function Mclust from package mclust to fit a mixture of normal distribution and it works very well. 

Denis
> Le 2015-06-30 ? 22:22, John Sorkin <jsorkin at grecc.umaryland.edu> a ?crit :
> 
> I am trying to model the mixture of two normal distributions, where x values are in the range of zero to some positive value. I know about mixtools and would use it save for the fact that the the y values from the normal distribution corresponding to the lower values of x (i.e. from zero to x/n) are from what appears to be a left-truncated normal distribution (i.e. the y values are all from the upper half of a normal distribution). The y values from higher values of x (i.e. from x/n to x) all appear to come from a normal distribution. Can someone suggest how to fit two normal distributions where one of the two distributions is left-truncated? Can this be done using mixtools?
> Thank you,
> John 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> 
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:12}}


From magdajoanasilva at gmail.com  Wed Jul  1 14:51:55 2015
From: magdajoanasilva at gmail.com (Magda Joana Silva)
Date: Wed, 1 Jul 2015 14:51:55 +0200
Subject: [R] package forecast is not working properly
Message-ID: <CAPDX_YsTPNeCa=49OXTs78ByGvnHjQvy2rCEonPtOQ3XAbrktQ@mail.gmail.com>

Hello all,

I just installed "forecast" package but I have the following error when I
try to load the library:


> library("forecast")
Loading required package: zoo

Attaching package: ?zoo?

The following objects are masked from ?package:base?:

    as.Date, as.Date.numeric

Loading required package: timeDate
Error : .onAttach failed in attachNamespace() for 'forecast', details:
  call: fun(libname, pkgname)
  error: 4 arguments passed to .Internal(nchar) which requires 3
In addition: Warning messages:
1: package ?forecast? was built under R version 3.2.1
2: package ?zoo? was built under R version 3.2.1
Error: package or namespace load failed for ?forecast?

I am running Windows 7 and the package is in my local library (I don't have
admin rights).

I would be very appreciated if someone could help me


mjs

	[[alternative HTML version deleted]]


From ragland.debra at yahoo.com  Wed Jul  1 14:50:21 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Wed, 1 Jul 2015 12:50:21 +0000 (UTC)
Subject: [R] Help with order.max in ar.yw.default
Message-ID: <966147229.327319.1435755021276.JavaMail.yahoo@mail.yahoo.com>

Hello,?

I am trying to fit my data to the default autoregressive models in R. I'm trying to apply the effectiveSize function from the coda package to a list of data frames using;?

sapply(splits, function(x) coda::effectiveSize(x["V5"]))?

However when I do, I get the error;?
Error in ar.yw.default(x, aic = aic, order.max = order.max, na.action = na.action, ?:?
? 'order.max' must be >= 1?

For some reason the command above works fine if my list is smaller (otherwise, not properly split). And it works perfectly fine on one of my data sets. If I apply it to anything else (data of a similar set up) I get the error. I've tried looking for solutions on the internet but so far I've come up with nothing.?

Has this happened to anyone before? Can I bypass this order. max call??

	[[alternative HTML version deleted]]


From ragland.debra at yahoo.com  Wed Jul  1 15:23:03 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Wed, 1 Jul 2015 13:23:03 +0000 (UTC)
Subject: [R] applying coda package effectiveSize function to a list
Message-ID: <1825185376.307349.1435756983448.JavaMail.yahoo@mail.yahoo.com>

Please help,I am spinning my wheels behind what should be a pretty simple solution. I found a solution by asking another question on here but it seems to not be effective on all my files of similar make up to the test case. I am simply trying to read data into R, group/order it by chain and atom number (V2 and V3 in the test data) and apply the effectiveSize function from the coda package to the grouped data. Previously I have tried splitting the data, and then applying the function over the new list, which, as I previously stated, only works in the test case.sapply(lst, function(x) coda::effectiveSize(x["variable"])) see comments?Apply a function from a specific R package to all files in folderI have tried aggregate, ddply, and a host of other commands/packages to no avail.My data files are very large so I am hoping that someone willing to help can retrieve a file from hereI cannot interpret the error I that I am getting, that seems to pop up no matter what I do.Error in ar.yw.default(x, aic = aic, order.max = order.max, na.action = na.action,  : 'order.max' must be >= 1One dataset is here?https://drive.google.com/open?id=0B0I4hNiJD0ErZWR5UDZjd1BiNm
	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Jul  1 16:40:09 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 1 Jul 2015 14:40:09 +0000
Subject: [R] applying coda package effectiveSize function to a list
In-Reply-To: <1825185376.307349.1435756983448.JavaMail.yahoo@mail.yahoo.com>
References: <1825185376.307349.1435756983448.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C331CB@SRVEXCHMBX.precheza.cz>

Hi

Please no HTML posting. It is almost unreadable.

Do not post twice the same question, it is unnecessary.

I do not know coda package but without further info you probably do not get sensible answer.

Is your splits data  mcmc or mcmc.list object?

How does objects for which the function works differ from your other tested objects?

Just a wild guess - some part of an object for which effectivSize throws error has dimension 1 or 0.

Cheers
Petr





> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of debra
> ragland via R-help
> Sent: Wednesday, July 01, 2015 3:23 PM
> To: r-help at r-project.org
> Subject: [R] applying coda package effectiveSize function to a list
>
> Please help,I am spinning my wheels behind what should be a pretty
> simple solution. I found a solution by asking another question on here
> but it seems to not be effective on all my files of similar make up to
> the test case. I am simply trying to read data into R, group/order it
> by chain and atom number (V2 and V3 in the test data) and apply the
> effectiveSize function from the coda package to the grouped data.
> Previously I have tried splitting the data, and then applying the
> function over the new list, which, as I previously stated, only works
> in the test case.sapply(lst, function(x)
> coda::effectiveSize(x["variable"])) see comments Apply a function from
> a specific R package to all files in folderI have tried aggregate,
> ddply, and a host of other commands/packages to no avail.My data files
> are very large so I am hoping that someone willing to help can retrieve
> a file from hereI cannot interpret the error I that I am getting, that
> seems to pop up no matter what I do.Error in ar.yw.default(x, aic =
> aic, order.max = order.max, na.action = na.action,  : 'order.max' must
> be >= 1One dataset is
> here https://drive.google.com/open?id=0B0I4hNiJD0ErZWR5UDZjd1BiNm
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Wed Jul  1 17:06:44 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 1 Jul 2015 15:06:44 +0000
Subject: [R] applying coda package effectiveSize function to a list
In-Reply-To: <A041FAF0-B7E4-491F-8AB6-87DC844C5BF6@yahoo.com>
References: <1825185376.307349.1435756983448.JavaMail.yahoo@mail.yahoo.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C331CB@SRVEXCHMBX.precheza.cz>
	<A041FAF0-B7E4-491F-8AB6-87DC844C5BF6@yahoo.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C331E9@SRVEXCHMBX.precheza.cz>

Hi

> -----Original Message-----
> From: Debra Ragland [mailto:ragland.debra at yahoo.com]
> Sent: Wednesday, July 01, 2015 4:53 PM
> To: PIKAL Petr
> Subject: Re: [R] applying coda package effectiveSize function to a list
>
> Hi Petr,
>
> How do I remove a post?

AFAIK, no way. Why?

Instead of removing speculating about removing your post you shall add some additional info so as knowledgable guys can find out an answer.

Petr

>
> Sent from my iPhone
>
> > On Jul 1, 2015, at 10:40 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> >
> > Hi
> >
> > Please no HTML posting. It is almost unreadable.
> >
> > Do not post twice the same question, it is unnecessary.
> >
> > I do not know coda package but without further info you probably do
> not get sensible answer.
> >
> > Is your splits data  mcmc or mcmc.list object?
> >
> > How does objects for which the function works differ from your other
> tested objects?
> >
> > Just a wild guess - some part of an object for which effectivSize
> throws error has dimension 1 or 0.
> >
> > Cheers
> > Petr
> >
> >
> >
> >
> >
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> debra
> >> ragland via R-help
> >> Sent: Wednesday, July 01, 2015 3:23 PM
> >> To: r-help at r-project.org
> >> Subject: [R] applying coda package effectiveSize function to a list
> >>
> >> Please help,I am spinning my wheels behind what should be a pretty
> >> simple solution. I found a solution by asking another question on
> >> here but it seems to not be effective on all my files of similar
> make
> >> up to the test case. I am simply trying to read data into R,
> >> group/order it by chain and atom number (V2 and V3 in the test data)
> >> and apply the effectiveSize function from the coda package to the
> grouped data.
> >> Previously I have tried splitting the data, and then applying the
> >> function over the new list, which, as I previously stated, only
> works
> >> in the test case.sapply(lst, function(x)
> >> coda::effectiveSize(x["variable"])) see comments Apply a function
> >> from a specific R package to all files in folderI have tried
> >> aggregate, ddply, and a host of other commands/packages to no
> >> avail.My data files are very large so I am hoping that someone
> >> willing to help can retrieve a file from hereI cannot interpret the
> >> error I that I am getting, that seems to pop up no matter what I
> >> do.Error in ar.yw.default(x, aic = aic, order.max = order.max,
> >> na.action = na.action,  : 'order.max' must be >= 1One dataset is
> here
> >> https://drive.google.com/open?id=0B0I4hNiJD0ErZWR5UDZjd1BiNm
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-
> >> guide.html and provide commented, minimal, self-contained,
> >> reproducible code.
> >
> > ________________________________
> > Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
> jsou ur?eny pouze jeho adres?t?m.
> > Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
> kopie vyma?te ze sv?ho syst?mu.
> > Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> > Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >
> > V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> > - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> > - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky
> ze strany p??jemce s dodatkem ?i odchylkou.
> > - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> > - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
> zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
> adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
> p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
> zn?m?.
> >
> > This e-mail and any documents attached to it may be confidential and
> are intended only for its intended recipients.
> > If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> > If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any
> manner.
> > The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
> >
> > In case that this e-mail forms part of business dealings:
> > - the sender reserves the right to end negotiations about entering
> into a contract in any time, for any reason, and without stating any
> reasoning.
> > - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer)
> excludes any acceptance of the offer on the part of the recipient
> containing any amendment or variation.
> > - the sender insists on that the respective contract is concluded
> only upon an express mutual agreement on all its aspects.
> > - the sender of this e-mail informs that he/she is not authorized to
> enter into any contracts on behalf of the company except for cases in
> which he/she is expressly authorized to do so in writing, and such
> authorization or power of attorney is submitted to the recipient or the
> person represented by the recipient, or the existence of such
> authorization is known to the recipient of the person represented by
> the recipient.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From boris.steipe at utoronto.ca  Wed Jul  1 17:11:13 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Wed, 1 Jul 2015 11:11:13 -0400
Subject: [R] Academic studies over R-Help
In-Reply-To: <422d6126a5a2f05c2babeba62c5aeed7.squirrel@wm3.uvic.ca>
References: <422d6126a5a2f05c2babeba62c5aeed7.squirrel@wm3.uvic.ca>
Message-ID: <E4D9EE45-9651-46C4-BC00-F9979E86C1F7@utoronto.ca>

Indeed, this is OT.  :-)

The way you have phrased your question, you are asking us for help to design your study. Certainly very OT.

Once your study is designed, and it is of clear, potential benefit to the R community, posting a concise _announcement_ here would not offend me.

Hope this helps.
B. 

On Jun 29, 2015, at 8:02 PM, Carlos Gomez <cagomezt at uvic.ca> wrote:

> Dear r-help mailing list,
> 
> Some colleagues and I are working on a series of research studies related
> to mailing list, and Stack Overflow. While I do understand that this email
> would be technically off topic - it's about a study that could involve
> R-Help users - not about coding in R. I was wondering:
> 
> - to what extent would you be interested in participate in academic studies?
> 
> - under what circumstances would you be willing to participate in
> interviews, user surveys, lab experiments or any other activity related
> to academic research?
> 
> - Which communication channels do you think are the appropriate to send
> these kind of announcements? (if any)
> 
> So, before upsetting the entire community (if this email is not doing that
> already), I will prefer to ask about it.
> 
> Thank you for your consideration!
> 
> Best regards,
> Carlos Gomez
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ss1272 at york.ac.uk  Wed Jul  1 16:40:48 2015
From: ss1272 at york.ac.uk (Saudi Sadiq)
Date: Wed, 1 Jul 2015 15:40:48 +0100
Subject: [R] checkConv and as.data.frame.default problems in R
Message-ID: <CAGpVz+jYLGCJT5woKjsWHjNp=4-JuQTSRSh3x64dm5vtr-O=fg@mail.gmail.com>

Hi All,

I have two datasets, vowels and qaaf, and both have 8 columns clarified as
follows:

1.  convergence: DV (whether participants succeeded to use CA (Cairene
Arabic) or fail to do so; hence, they use MA (Minia Arabic)

2.  speaker: 62 participants

3.  lexical.item: as pronounced

4.  style: careful and casual

5.  gender: males and females

6.  age: continues variable

7.  residence: urbanite, migrant to town or villager

8.  education: secondary or below, university or postgraduate

The only difference between the two datasets is the number of items. With
the vowels dataset, there are 1339 items; in the qaaf dataset there are
4064 items.

The aim of the test done was to know which independent variable is more
responsible for using CA forms. I used the lme4 package, function glmer.

I ran the model:

1.  modelvowels <- glmer(convergence ~ gender + age + residence +
education +  style+ (1|lexical.item) + (1|speaker), data=vowels,
family='binomial')

The message came on the screen:

2.  Warning message:

In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :

  Model failed to converge with max|grad| = 0.00210845 (tol = 0.001,
component 1)

Then I ran the model after removing STYLE as follows:

3.  modelvowels <- glmer(convergence ~ gender + age + residence +
education + (1|lexical.item) + (1|speaker), data=vowels,
family='binomial')

This produced a result. Then, I ran

4.  plot(allEffects(modelvowels))



and this gave four charts (for the four independent variables: gender, age,
residence and education).

Then, I moved to the qaaf dataset (4064 items) and ran the same model

5.  modelqaaf <- glmer(convergence ~ gender + age + residence +
education + (1|lexical.item) + (1|speaker), data=qaaf,
family='binomial')

which gave results with the vowels dataset but there was a warning message
this time

6.  Warning message:

In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :

  Model failed to converge with max|grad| = 0.429623 (tol = 0.001, component 8)

So, I removed one independent variable (residence) and ran this model again:

7.  modelqaaf <- glmer(convergence ~ gender + age + education +
(1|lexical.item) + (1|speaker), data=qaaf, family='binomial')

This gave a result. I removed another independent variable (gender) after
returning (residence) and ran the model:

8.  modelqaaf1 <- glmer(convergence ~ residence + age + education +
(1|lexical.item) + (1|speaker), data=q, family='binomial')

This gave a result as well. Then, I tried to create some graphs using

9.  plot(allEffects(modelqaaf)) and

10. plot(allEffects(modelqaaf1))



but there was the same error for both

11. Error in as.data.frame.default(data, optional = TRUE) :

  cannot coerce class ""function"" to a data.frame

 Now, my questions:

a.  why 1 did not work, why 3 worked, why 5 did not work though it has
the same four IVs of 3, why 7 and 8 worked with only three IVs, and
why 9 and 10 did not work though they are like 4 which worked well.



b.  What are the packages that must be installed with, before or after
the lme4 package?



 Best

-- 
Saudi Sadiq,
Assistant Lecturer, English Department,
Faculty of Al-Alsun,Minia University,
Minia City, Egypt &
PhD Student, Language and Linguistic Science Department,
University of York, York, North Yorkshire, UK,
YO10 5DD
http://york.academia.edu/SaudiSadiq
https://www.researchgate.net/profile/Saudi_Sadiq
Certified Interpreter by Pearl Linguistics

Forum for Arabic Linguistics conference ???? ???????
28-30th July 2015 - call for papers now open
https://sites.google.com/a/york.ac.uk/fal2015/

	[[alternative HTML version deleted]]


From chichi.shu at hotmail.com  Wed Jul  1 16:57:51 2015
From: chichi.shu at hotmail.com (Chichi Shu)
Date: Wed, 1 Jul 2015 10:57:51 -0400
Subject: [R] ggmap warning
In-Reply-To: <CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>
References: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
	<CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>
Message-ID: <BLU179-DS191DE4DB1D5238CE163F828FA80@phx.gbl>

Thanks Thierry. Is it the longitude and latitude that need to have finite values? 


From: Thierry Onkelinx 
Sent: Wednesday, July 01, 2015 2:50 AM
To: Chichi Shu 
Cc: r-help at r-project.org 
Subject: Re: [R] ggmap warning

Your data contains 4945 rows with missing or infinite values. These cannot be handled by stat_density2d and are dropped for that reason.

Best regards,

Thierry


Op 1 jul. 2015 08:43 schreef "Chichi Shu" <chichi.shu at hotmail.com>:

  Dear Listers



  I?ve been using ggmap package to produce crime Heat map. But I?ve noticed the following warning message when executing my code:



  In loop_apply(n, do.ply) :

    Removed 4945 rows containing non-finite values (stat_density2d).



  I?ve googled this message but I couldn?t find any good answers.



  Is this related to ggmap package or one of its depending package?



  What does it mean?



  Thanks!

          [[alternative HTML version deleted]]


  ______________________________________________
  R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
  https://stat.ethz.ch/mailman/listinfo/r-help
  PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
  and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From kirchman at udel.edu  Wed Jul  1 17:23:52 2015
From: kirchman at udel.edu (kirchman)
Date: Wed, 1 Jul 2015 08:23:52 -0700 (PDT)
Subject: [R] Extracting arrows from CCA plots
In-Reply-To: <1435611867609-4709190.post@n4.nabble.com>
References: <1435611867609-4709190.post@n4.nabble.com>
Message-ID: <1435764232683-4709273.post@n4.nabble.com>

Here's a partial answer to my own question. While I still don't know how to
get the arrow information out of a CCA plot, it is possible to reproduce the
arrows by first extracting the biplot data with scores (results.cca,
display="bp") and then using the ordiArrowMul function in vegan to rescale
the data as desired for the plot. 



--
View this message in context: http://r.789695.n4.nabble.com/Extracting-arrows-from-CCA-plots-tp4709190p4709273.html
Sent from the R help mailing list archive at Nabble.com.


From thierry.onkelinx at inbo.be  Wed Jul  1 21:59:42 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 1 Jul 2015 21:59:42 +0200
Subject: [R] ggmap warning
In-Reply-To: <BLU179-DS191DE4DB1D5238CE163F828FA80@phx.gbl>
References: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
	<CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>
	<BLU179-DS191DE4DB1D5238CE163F828FA80@phx.gbl>
Message-ID: <CAJuCY5yQ1x1V6TBXrQ7uiH5dFqwndMJ72nX5F0PKNtFvzimrZA@mail.gmail.com>

All variables used in the plot must contain finite values. Without your
code we can only speculate about your code.

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-01 16:57 GMT+02:00 Chichi Shu <chichi.shu at hotmail.com>:

>   Thanks Thierry. Is it the longitude and latitude that need to have
> finite values?
>
>
>  *From:* Thierry Onkelinx <thierry.onkelinx at inbo.be>
> *Sent:* Wednesday, July 01, 2015 2:50 AM
> *To:* Chichi Shu <chichi.shu at hotmail.com>
> *Cc:* r-help at r-project.org
> *Subject:* Re: [R] ggmap warning
>
>
> Your data contains 4945 rows with missing or infinite values. These cannot
> be handled by stat_density2d and are dropped for that reason.
>
> Best regards,
>
> Thierry
> Op 1 jul. 2015 08:43 schreef "Chichi Shu" <chichi.shu at hotmail.com>:
>
>> Dear Listers
>>
>>
>>
>> I?ve been using ggmap package to produce crime Heat map. But I?ve noticed
>> the following warning message when executing my code:
>>
>>
>>
>> In loop_apply(n, do.ply) :
>>
>>   Removed 4945 rows containing non-finite values (stat_density2d).
>>
>>
>>
>> I?ve googled this message but I couldn?t find any good answers.
>>
>>
>>
>> Is this related to ggmap package or one of its depending package?
>>
>>
>>
>> What does it mean?
>>
>>
>>
>> Thanks!
>>
>>         [[alternative HTML version deleted]]
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Wed Jul  1 22:01:31 2015
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 01 Jul 2015 16:01:31 -0400
Subject: [R] checkConv and as.data.frame.default problems in R
In-Reply-To: <CAGpVz+jYLGCJT5woKjsWHjNp=4-JuQTSRSh3x64dm5vtr-O=fg@mail.gmail.com>
References: <CAGpVz+jYLGCJT5woKjsWHjNp=4-JuQTSRSh3x64dm5vtr-O=fg@mail.gmail.com>
Message-ID: <web-564539582@cgpsrv2.cis.mcmaster.ca>

Dear Saudi Sadiq,

If you want answers to most of your questions, you'll likely have to provide your data so that people can reproduce the errors. I can, however, answer part of your last question without the data:

plot(allEffects(modelqaaf1)) fails because the data set is named q, and there is a standard R function named q(). Obviously there is a scoping issue that confuses allEffects() for a reason I don't yet understand.

Here's a simple example that reproduces this error:

---------- snip ------------

> library(lme4)
Loading required package: Matrix
Loading required package: Rcpp
> library(effects)
> fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), cake)
> allEffects(fm1)
 model: angle ~ recipe * temperature

 recipe*temperature effect
      temperature
recipe      175      185      195      205      215      225
     A 29.13333 31.53333 30.80000 33.53333 38.66667 35.06667
     B 26.86667 29.40000 31.73333 32.13333 34.46667 35.26667
     C 27.93333 28.93333 31.73333 30.86667 34.40000 35.73333
> q <- cake
> fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), data=q)
> allEffects(fm1)

 Error in as.data.frame.default(data, optional = TRUE) : 
  cannot coerce class ""function"" to a data.frame 

------------- snip -------------

Why, as you claim, you get the same error with plot(allEffects(modelqaaf)), where the data set is qaaf rather than q, is hard to understand, and perhaps you're mistaken.

I hope this helps,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	
	
On Wed, 1 Jul 2015 15:40:48 +0100
 Saudi Sadiq <ss1272 at york.ac.uk> wrote:
> Hi All,
> 
> I have two datasets, vowels and qaaf, and both have 8 columns clarified as
> follows:
> 
> 1.  convergence: DV (whether participants succeeded to use CA (Cairene
> Arabic) or fail to do so; hence, they use MA (Minia Arabic)
> 
> 2.  speaker: 62 participants
> 
> 3.  lexical.item: as pronounced
> 
> 4.  style: careful and casual
> 
> 5.  gender: males and females
> 
> 6.  age: continues variable
> 
> 7.  residence: urbanite, migrant to town or villager
> 
> 8.  education: secondary or below, university or postgraduate
> 
> The only difference between the two datasets is the number of items. With
> the vowels dataset, there are 1339 items; in the qaaf dataset there are
> 4064 items.
> 
> The aim of the test done was to know which independent variable is more
> responsible for using CA forms. I used the lme4 package, function glmer.
> 
> I ran the model:
> 
> 1.  modelvowels <- glmer(convergence ~ gender + age + residence +
> education +  style+ (1|lexical.item) + (1|speaker), data=vowels,
> family='binomial')
> 
> The message came on the screen:
> 
> 2.  Warning message:
> 
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> 
>   Model failed to converge with max|grad| = 0.00210845 (tol = 0.001,
> component 1)
> 
> Then I ran the model after removing STYLE as follows:
> 
> 3.  modelvowels <- glmer(convergence ~ gender + age + residence +
> education + (1|lexical.item) + (1|speaker), data=vowels,
> family='binomial')
> 
> This produced a result. Then, I ran
> 
> 4.  plot(allEffects(modelvowels))
> 
> 
> 
> and this gave four charts (for the four independent variables: gender, age,
> residence and education).
> 
> Then, I moved to the qaaf dataset (4064 items) and ran the same model
> 
> 5.  modelqaaf <- glmer(convergence ~ gender + age + residence +
> education + (1|lexical.item) + (1|speaker), data=qaaf,
> family='binomial')
> 
> which gave results with the vowels dataset but there was a warning message
> this time
> 
> 6.  Warning message:
> 
> In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> 
>   Model failed to converge with max|grad| = 0.429623 (tol = 0.001, component 8)
> 
> So, I removed one independent variable (residence) and ran this model again:
> 
> 7.  modelqaaf <- glmer(convergence ~ gender + age + education +
> (1|lexical.item) + (1|speaker), data=qaaf, family='binomial')
> 
> This gave a result. I removed another independent variable (gender) after
> returning (residence) and ran the model:
> 
> 8.  modelqaaf1 <- glmer(convergence ~ residence + age + education +
> (1|lexical.item) + (1|speaker), data=q, family='binomial')
> 
> This gave a result as well. Then, I tried to create some graphs using
> 
> 9.  plot(allEffects(modelqaaf)) and
> 
> 10. plot(allEffects(modelqaaf1))
> 
> 
> 
> but there was the same error for both
> 
> 11. Error in as.data.frame.default(data, optional = TRUE) :
> 
>   cannot coerce class ""function"" to a data.frame
> 
>  Now, my questions:
> 
> a.  why 1 did not work, why 3 worked, why 5 did not work though it has
> the same four IVs of 3, why 7 and 8 worked with only three IVs, and
> why 9 and 10 did not work though they are like 4 which worked well.
> 
> 
> 
> b.  What are the packages that must be installed with, before or after
> the lme4 package?
> 
> 
> 
>  Best
> 
> -- 
> Saudi Sadiq,
> Assistant Lecturer, English Department,
> Faculty of Al-Alsun,Minia University,
> Minia City, Egypt &
> PhD Student, Language and Linguistic Science Department,
> University of York, York, North Yorkshire, UK,
> YO10 5DD
> http://york.academia.edu/SaudiSadiq
> https://www.researchgate.net/profile/Saudi_Sadiq
> Certified Interpreter by Pearl Linguistics
> 
> Forum for Arabic Linguistics conference ???? ???????
> 28-30th July 2015 - call for papers now open
> https://sites.google.com/a/york.ac.uk/fal2015/
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From davide.risso at berkeley.edu  Wed Jul  1 21:30:55 2015
From: davide.risso at berkeley.edu (Davide Risso)
Date: Wed, 1 Jul 2015 12:30:55 -0700
Subject: [R] How to specified contrasts in anova (lm)
Message-ID: <CADg=1r3gxBWreUSn4Yxi7Ud4xUh-DApiP7+=23EwZ7ac5Qw0vQ@mail.gmail.com>

Dear list,

I have the following anova that I want to fit in R:

y_{ijk} = \mu + \alpha_i + \beta_{j(i)} + \epsilon_{ijk}

This is an application in biology, in which we are measuring a certain
(continuous) characteristic of a group of cells.
Hence y_{ijk} corresponds to the measurement on cell k of type i in batch j.

Batch is nested in type, meaning that we have multiple batches for each
cell type and each batch contains only cells of a given type. To complicate
things, the design is unbalanced.

I can "manually" fit the model, with the following constraints:
\sum_{i=1}^m \alpha_i = 0
and
\sum_{j=1}^{n_i} \beta_{j(i)} = 0.

This gives me m  + 1 constraints, where m is the number of types.

A minimal (toy) example in R is:

a <- as.factor(c(rep(1, 4), rep(2, 6)))
b <- as.factor(rep(1:5, each=2))
y <- rnorm(10)

fit <- lm(y ~ a + b)

This call to lm will fit the wrong model, using two constraints (with the
"contr.sum" specification), \sum_{i=1}^m \alpha_i = 0 and \sum_{j=1}^{n}
\beta_{j} = 0, and resulting in a singular design matrix.
Is there a way to specify the right constraints in lm?

Thanks in advance for any help,
davide

-- 
Davide Risso, PhD
Post Doctoral Scholar
Division of Biostatistics
School of Public Health
University of California, Berkeley
344 Li Ka Shing Center, #3370
Berkeley, CA 94720-3370
E-mail: davide.risso at berkeley.edu

	[[alternative HTML version deleted]]


From lid.zigh at gmail.com  Wed Jul  1 20:07:55 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Wed, 1 Jul 2015 13:07:55 -0500
Subject: [R] question
Message-ID: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>

I have 682 variables in a data frame , and a function that  I should feed
682 variables in this function one by one and each time save the file as a
special name!
for emaple:
my data frame file includes 682 names :
1  aaa
2  bbb
3  dfdsfg
4 fghh
.

682 fgfhg
and a function like prep(Z, aaa, .....) and each time I should change the
variable name in this function and read the variable from the data frame
and each time I should save the file as a special name such as:

prep1<- prep(z, aaa,...)
prep2<- prep(z, bbb,...)
prep3<- prep(z, dfdsfg,..)
Prep4<- prep(z, fghh,...)

How can I use loop function in R to that?

Thanks

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Jul  1 22:58:02 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 1 Jul 2015 13:58:02 -0700
Subject: [R] How to specified contrasts in anova (lm)
In-Reply-To: <CADg=1r3gxBWreUSn4Yxi7Ud4xUh-DApiP7+=23EwZ7ac5Qw0vQ@mail.gmail.com>
References: <CADg=1r3gxBWreUSn4Yxi7Ud4xUh-DApiP7+=23EwZ7ac5Qw0vQ@mail.gmail.com>
Message-ID: <CAGxFJbS8+RLYKPYchFSabBB0uy4wLUiK=wJC+hRQebk8c1tGNg@mail.gmail.com>

Note that the toy example you provided is garbage. It has nothing to
do with contrasts (i.e. constraints) -- the DESIGN is singular: The
(2) - (1) difference for factor A  is exactly the same as the (3,4,5)
average - (1,2)average for B. I suspect you need to use a mixed
effects model treating batch as random, not fixed, but that is just a
guess as I do not know in detail what you have done.

So unless you have misstated or I have misunderstood, this indicates
that your statistical understanding is wanting and you should consult
a local statistical expert (of which there must be many) instead of
proceeding on your own. This is **NOT** about R and is off topic here.

Also, if you do wish to post here on R questions in future, please
follow the posting guide and post in plain text, not HTML.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Jul 1, 2015 at 12:30 PM, Davide Risso <davide.risso at berkeley.edu> wrote:
> Dear list,
>
> I have the following anova that I want to fit in R:
>
> y_{ijk} = \mu + \alpha_i + \beta_{j(i)} + \epsilon_{ijk}
>
> This is an application in biology, in which we are measuring a certain
> (continuous) characteristic of a group of cells.
> Hence y_{ijk} corresponds to the measurement on cell k of type i in batch j.
>
> Batch is nested in type, meaning that we have multiple batches for each
> cell type and each batch contains only cells of a given type. To complicate
> things, the design is unbalanced.
>
> I can "manually" fit the model, with the following constraints:
> \sum_{i=1}^m \alpha_i = 0
> and
> \sum_{j=1}^{n_i} \beta_{j(i)} = 0.
>
> This gives me m  + 1 constraints, where m is the number of types.
>
> A minimal (toy) example in R is:
>
> a <- as.factor(c(rep(1, 4), rep(2, 6)))
> b <- as.factor(rep(1:5, each=2))
> y <- rnorm(10)
>
> fit <- lm(y ~ a + b)
>
> This call to lm will fit the wrong model, using two constraints (with the
> "contr.sum" specification), \sum_{i=1}^m \alpha_i = 0 and \sum_{j=1}^{n}
> \beta_{j} = 0, and resulting in a singular design matrix.
> Is there a way to specify the right constraints in lm?
>
> Thanks in advance for any help,
> davide
>
> --
> Davide Risso, PhD
> Post Doctoral Scholar
> Division of Biostatistics
> School of Public Health
> University of California, Berkeley
> 344 Li Ka Shing Center, #3370
> Berkeley, CA 94720-3370
> E-mail: davide.risso at berkeley.edu
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From davide.risso at berkeley.edu  Wed Jul  1 23:14:53 2015
From: davide.risso at berkeley.edu (Davide Risso)
Date: Wed, 1 Jul 2015 14:14:53 -0700
Subject: [R] How to specified contrasts in anova (lm)
In-Reply-To: <CAGxFJbS8+RLYKPYchFSabBB0uy4wLUiK=wJC+hRQebk8c1tGNg@mail.gmail.com>
References: <CADg=1r3gxBWreUSn4Yxi7Ud4xUh-DApiP7+=23EwZ7ac5Qw0vQ@mail.gmail.com>
	<CAGxFJbS8+RLYKPYchFSabBB0uy4wLUiK=wJC+hRQebk8c1tGNg@mail.gmail.com>
Message-ID: <CADg=1r1jLALxWYyxqgDqb8PTvxqG7WArSbKi_huNAGpkFyOS=Q@mail.gmail.com>

Hi Bert,

I apologize for the HTML. I will pay more attention in the future.

I know that the lm() command of my toy example fits the wrong model
with a singular design, maybe I shouldn't have included it.

I believe that the model with the additional constraint(s) is not
garbage. But I don't know how to specify the additional constraints in
lm() (hence, I believe it's an R question).

I also know that I can fit a mixed effect model, but this was not the
point of my previous email.

Best,
davide


From bgunter.4567 at gmail.com  Wed Jul  1 23:43:04 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 1 Jul 2015 14:43:04 -0700
Subject: [R] How to specified contrasts in anova (lm)
In-Reply-To: <CADg=1r1jLALxWYyxqgDqb8PTvxqG7WArSbKi_huNAGpkFyOS=Q@mail.gmail.com>
References: <CADg=1r3gxBWreUSn4Yxi7Ud4xUh-DApiP7+=23EwZ7ac5Qw0vQ@mail.gmail.com>
	<CAGxFJbS8+RLYKPYchFSabBB0uy4wLUiK=wJC+hRQebk8c1tGNg@mail.gmail.com>
	<CADg=1r1jLALxWYyxqgDqb8PTvxqG7WArSbKi_huNAGpkFyOS=Q@mail.gmail.com>
Message-ID: <CAGxFJbQ4SFTKXBGU1odMLeFt6QhMoxjqzFGxhbtHZ7gSs6Hn_A@mail.gmail.com>

You do not set constraints in R you set contrasts. See the relevant
sections of e.g. Venables and Ripley's MASS book.

?contrasts  ## what else!
?C

for details.

Beyond this, I cannot help.

-- Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Jul 1, 2015 at 2:14 PM, Davide Risso <davide.risso at berkeley.edu> wrote:
> Hi Bert,
>
> I apologize for the HTML. I will pay more attention in the future.
>
> I know that the lm() command of my toy example fits the wrong model
> with a singular design, maybe I shouldn't have included it.
>
> I believe that the model with the additional constraint(s) is not
> garbage. But I don't know how to specify the additional constraints in
> lm() (hence, I believe it's an R question).
>
> I also know that I can fit a mixed effect model, but this was not the
> point of my previous email.
>
> Best,
> davide


From jholtman at gmail.com  Thu Jul  2 02:49:20 2015
From: jholtman at gmail.com (jim holtman)
Date: Wed, 1 Jul 2015 20:49:20 -0400
Subject: [R] Extracting data from a file containing data
In-Reply-To: <1521656228.235174.1435310859067.JavaMail.yahoo@mail.yahoo.com>
References: <CAAxdm-5N1Ebjy-jysXWPjDuTarCnc2BTEN218zm5Nyd7on1tvA@mail.gmail.com>
	<1521656228.235174.1435310859067.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAAxdm-7r5aJ-_DnuHYpaaB_33auTmkb-f3uAMYJ3kFSAmHi4Kw@mail.gmail.com>

Here is a way to do case I.  It uses the 'tidyr' package and produces
results like:

> case1[[1]]
     YR JF-R_NINO1.2 MAM-R_NINO1.2 JJA-R_NINO1.2 OND-R_NINO1.2
1  1982           ML            ML            ME            SE
2  1983           SE            SE            SE            ME
3  1984           ML            ML            ML            ML
4  1985           SL            SL            SL            ML
5  1986           ME            ML            ML            ME
6  1987           ME            SE            SE            SE
7  1988           ML            ML            SL            SL
8  1989           ML            ML            ML            ML
9  1990           ML            ML            ML            ML
10 1991           ML            ML            ME            ME
11 1992           ME            SE            ME            ML
12 1993           ME            ME            ME            ME
13 1994           ML            SL            ML            ME
14 1995           ME            ML            ML            ML
15 1996           ML            SL            SL            SL
16 1997           ML            SE            SE            SE
17 1998           SE            SE            SE            ML
18 1999           ML            ML            SL            ML
19 2000           ML            ML            ML            ML
20 2001           ML            ME            ML            SL
21 2002           ML            ME            ML            ME
22 2003           ML            SL            ML            ME
23 2004           ML            ML            SL            ME
24 2005           ML            ML            ML            ML
25 2006           ME            ML            ME            SE
26 2007           ME            SL            SL            SL
27 2008           ML            ME            ME            ML
28 2009           ML            ME            ME            ME
29 2010           ME            ME            SL            SL
30 2011           ML            ME            ME            ML
31 2012           ML            ME            ME            ML




Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Fri, Jun 26, 2015 at 5:27 AM, Peter Tuju <peterenos at ymail.com> wrote:

> Dear Jim Holtman,
>
> Thank you very much for your help.
>
> The problem I'm trying to solve is ?To determine weather the evolution of
> ENSO can influence rainfall over Tanzania?. In this study I have two types
> of data, ie Rainfall data (for 23 stations) and Nino indices data, both
> spanning a period of 31 years (1982-2012).
>
>  *CASE I:*
> 1. In ?*Nino.indices.txt*? data for all columns of the nino regions (both
> for anomalies and SST), to calculate the Season means "January & February
> (JF)", ?March, April and may (MAM)", "June, July & August (JJA)" and
> "October, November and December (OND" for each year. and have the output in
> table form as;
>
>  *Nino indices Mean*
>       Years
>  JF
> SST Mean
> NINO1+2
>  JF
> ANOM Mean
> NINO1+2
>  MAM
> SST Mean
> NINO3
>  MAM
> ANOM Mean
> NINO3
>  JJA
> SST Mean
> NINO4
>  JJA
> ANOM Mean
> NINO4
>  OND
> SST Mean
> NINO3.4
>  OND
> SST Mean
> NINO3.4
>   1982
>
>
>
>
>
>
>
>
>    1983
>
>
>
>
>
>
>
>
>    - - - -
>
>
>
>
>
>
>
>
>    - - - -
>
>
>
>
>
>
>
>
>    2012
>
>
>
>
>
>
>
>
>
>  2. To use the Yearly anomalies for each column in nino regions to
> classify the events as;
> (i). If ANOM Mean> 1, then I assign it to ?SE? (Being as Strong El-nino)
> (ii). If 0<ANOM Mean<=1 , then I assign it to ?ME? (Being as Moderate
> El-nino)
> (iii). If ANOM== 0, then I assign it to ?NT? (Being as Neutral Condition)
> (iv). If ANOM Mean< (-1), then I assign it to ?SL? (Being as Strong
> La-nina)
> (v). If -1<=ANOM Mean< 0 , then I assign it to ?ML? (Being as Moderate
> La-nina)
> The output have to be in table form as;
>
>  *FOR NINO1+2*
>     Years
>  JF
> ANOM Mean
> NINO1+2
>  MAM
> ANOM Mean
> NINO1+2
>  JJA
> ANOM Mean
> NINO1+2
>  OND
> SST Mean
> NINO1+2
>   1982
>  SE
>
>
>
>    1983
>
>
>   SL
>
>    - - - -
>
>
>
>   ML
>   - - - -
>
>   ME
>
>
>    2012
>
>
>
>   *SL*
>
>  *FOR NINO3*
>     Years
>  JF
> ANOM Mean
> NINO3
>  MAM
> ANOM Mean
> NINO3
>  JJA
> ANOM Mean
> NINO3
>  OND
> SST Mean
> NINO3
>   1982
>  *SE *
>
>
>
>    1983
>
>
>
>
>    - - - -
>
>
>
>
>    - - - -
>
>   ME
>
>
>    2012
>
>
>
>   *SL*
>
>  *FOR NINO4*
>     Years
>  JF
> ANOM Mean
> NINO4
>  MAM
> ANOM Mean
> NINO4
>  JJA
> ANOM Mean
> NINO4
>  OND
> SST Mean
> NINO4
>   1982
>  *SE *
>
>
>
>    1983
>
>
>
>
>    - - - -
>  ML
>
>
>   SL
>   - - - -
>
>   ME
>
>
>    2012
>
>
>
>   *SL*
>
>
>  *FOR NINO3.4*
>     Years
>  JF
> ANOM Mean
> NINO3.4
>  MAM
> ANOM Mean
> NINO3.4
>  JJA
> ANOM Mean
> NINO3.4
>  OND
> SST Mean
> NINO3.4
>   1982
>  *SE *
>  SL
>
>
>    1983
>
>
>
>
>    - - - -
>
>
>   ML
>
>    - - - -
>
>   ME
>
>
>    2012
>
>
>
>   *SL*
>
>
>  3. To plot the time series graph for each nino regions using the Yearly
> Anomalies.
>
>
>  *CASE II:*
>  Consider the Rainfall station data;
>  1. In some files containing the data there are missing data labeled by
> variable ?m?. I want to substitute these missing data with long term mean.
>  2. Find the rowSum and anomalies of each file containing the data.
>  3. To find the cumsum of the rowSum of each file containing the data.
>  4. Plot the single mass curves ie. Plot(Year, cumsum) for each file and
> name its title as the name of the corresponding file name.
>  5. Plot the time series graphs for seasons JF, MAM, JJA and OND for each
> file and name give its name as ?Time series graph for ?name of the file??
>  6. To find the seasonal correlations for JF, MAM, JJA and OND using the
> anomalies of the rainfall station data and that of each nino region
> indices, and have the results in table form as;
>
>  *CORRELATIONS OF RAINFALL AND NINO1+2 ANOMALIES*
>     *Years*
>  *JF*
>  *MAM*
>  *JJA*
>  *OND*
>   *1982 *
>
>
>
>
>    *1983*
>
>
>
>
>    *- - - -*
>
>
>
>
>    *- - - -*
>
>
>
>
>    *2012*
>
>
>
>
>
>  *CORRELATIONS OF RAINFALL AND NINO3 ANOMALIES*
>     *Years*
>  *JF*
>  *MAM*
>  *JJA*
>  *OND*
>   *1982 *
>
>
>
>
>    *1983*
>
>
>
>
>    *- - - -*
>
>
>
>
>    *- - - -*
>
>
>
>
>    *2012*
>
>
>
>
>
>  *CORRELATIONS OF RAINFALL AND NINO4 ANOMALIES*
>     *Years*
>  *JF*
>  *MAM*
>  *JJA*
>  *OND*
>   *1982 *
>
>  ...
>
> [Message clipped]
-------------- next part --------------
input <- read.table("C:\\Users\\jh52822\\Downloads\\Nino_indices.txt"   
                    , header = TRUE
                    , as.is = TRUE
                    )
# create factors
input$season <- factor(c(rep("JF", 2), rep("MAM", 3), rep("JJA", 3)
        , NA, rep("OND", 3)
        )[input$MON], levels = c("JF", "MAM", "JJA", "OND"))
        
# leave off the MON (-2) column from the data            
res <- aggregate(. ~ season + YR, data = input[, -2], FUN = 'mean')
head(res,10)

# this function determines the labels to apply
f_labels <- 
function(x)
{
    result <- as.character(cut(x  # convert to character since it is a factor
        , breaks = c(-Inf, -1, 0, 1, Inf)
        , labels = c("SL", "ML", "ME", "SE")
        ))
    # check for zero
    result[x == 0] <- "NT"
    result
}

# get the "ANOM" columns for processing since these are the ones that
# we want to test for values.
anom <- which(grepl("^ANOM", names(res)))

# for each ANOM column iterate and compute the labels based on values
for (i in anom){
    # use the variable in previous column for the name
    res[[paste0("R_", names(res[i - 1L]))]] <- f_labels(res[[i]])
}

# the tidyr package helps to format the results
require(tidyr)
# columns to use as summary -- added above
sum_cols <- paste0("R_", names(res[anom - 1L]))

case1 <- lapply(sum_cols, function(.col){
    # need to restrict what data we want
    x <- spread_(res[, c("YR", "season", .col)], "season", .col)
    # append the name of the data to the season
    names(x)[-1] <- paste(names(x[-1]), .col, sep = '-')
    x  # return value
})

    



From jholtman at gmail.com  Thu Jul  2 03:00:35 2015
From: jholtman at gmail.com (jim holtman)
Date: Wed, 1 Jul 2015 21:00:35 -0400
Subject: [R] question
In-Reply-To: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
Message-ID: <CAAxdm-5yQ=Chicf8q66J4xuVko53qwdL8XoXvf05w=tJgu_0NQ@mail.gmail.com>

You never said how you wanted to save the data, so I will choose to use
'saveRDS' which should handle most anything.

for (i in name_file$names){
    saveRDS(prep(z, i),  file = paste0(i, '.RDS'))
}



Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Wed, Jul 1, 2015 at 2:07 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:

> I have 682 variables in a data frame , and a function that  I should feed
> 682 variables in this function one by one and each time save the file as a
> special name!
> for emaple:
> my data frame file includes 682 names :
> 1  aaa
> 2  bbb
> 3  dfdsfg
> 4 fghh
> .
>
> 682 fgfhg
> and a function like prep(Z, aaa, .....) and each time I should change the
> variable name in this function and read the variable from the data frame
> and each time I should save the file as a special name such as:
>
> prep1<- prep(z, aaa,...)
> prep2<- prep(z, bbb,...)
> prep3<- prep(z, dfdsfg,..)
> Prep4<- prep(z, fghh,...)
>
> How can I use loop function in R to that?
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Jul  2 03:03:55 2015
From: jholtman at gmail.com (jim holtman)
Date: Wed, 1 Jul 2015 21:03:55 -0400
Subject: [R] question
In-Reply-To: <CAAxdm-5yQ=Chicf8q66J4xuVko53qwdL8XoXvf05w=tJgu_0NQ@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
	<CAAxdm-5yQ=Chicf8q66J4xuVko53qwdL8XoXvf05w=tJgu_0NQ@mail.gmail.com>
Message-ID: <CAAxdm-6mr1H-x-K==pp2w+yJ-S6S7RkXaP2ph+Rt80R48=s_Tg@mail.gmail.com>

I forgot that you also wanted to change the variable name; I would suggest
that you don't use individual objects, but instead use a 'list'.  Here is
how it would change

result <- lapply(name_file$names, function(.file){
    result <- prep(z, .file)
    saveRDS(result, file = paste0(.file, '.RDS'))
    result  # return value that goes in the list
})



Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Wed, Jul 1, 2015 at 9:00 PM, jim holtman <jholtman at gmail.com> wrote:

> You never said how you wanted to save the data, so I will choose to use
> 'saveRDS' which should handle most anything.
>
> for (i in name_file$names){
>     saveRDS(prep(z, i),  file = paste0(i, '.RDS'))
> }
>
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Wed, Jul 1, 2015 at 2:07 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:
>
>> I have 682 variables in a data frame , and a function that  I should feed
>> 682 variables in this function one by one and each time save the file as a
>> special name!
>> for emaple:
>> my data frame file includes 682 names :
>> 1  aaa
>> 2  bbb
>> 3  dfdsfg
>> 4 fghh
>> .
>>
>> 682 fgfhg
>> and a function like prep(Z, aaa, .....) and each time I should change the
>> variable name in this function and read the variable from the data frame
>> and each time I should save the file as a special name such as:
>>
>> prep1<- prep(z, aaa,...)
>> prep2<- prep(z, bbb,...)
>> prep3<- prep(z, dfdsfg,..)
>> Prep4<- prep(z, fghh,...)
>>
>> How can I use loop function in R to that?
>>
>> Thanks
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From davide.risso at berkeley.edu  Thu Jul  2 05:05:33 2015
From: davide.risso at berkeley.edu (Davide Risso)
Date: Wed, 1 Jul 2015 20:05:33 -0700
Subject: [R] How to specified contrasts in anova (lm)
In-Reply-To: <CAGxFJbQ4SFTKXBGU1odMLeFt6QhMoxjqzFGxhbtHZ7gSs6Hn_A@mail.gmail.com>
References: <CADg=1r3gxBWreUSn4Yxi7Ud4xUh-DApiP7+=23EwZ7ac5Qw0vQ@mail.gmail.com>
	<CAGxFJbS8+RLYKPYchFSabBB0uy4wLUiK=wJC+hRQebk8c1tGNg@mail.gmail.com>
	<CADg=1r1jLALxWYyxqgDqb8PTvxqG7WArSbKi_huNAGpkFyOS=Q@mail.gmail.com>
	<CAGxFJbQ4SFTKXBGU1odMLeFt6QhMoxjqzFGxhbtHZ7gSs6Hn_A@mail.gmail.com>
Message-ID: <CADg=1r0iF6O3T2iyt2HySbncVNT7E4PrF654SSki80_m-3tX2w@mail.gmail.com>

Update:

I was able to fit the desired model by manually specifying the proper
contrast for the second factor, i.e. (going on with my example).

mat <- matrix(c(1, -1, 0, 0, 0, 0, 0, 1, 0, -1, 0, 0, 0, 1, -1), ncol=3)
fit <- lm(y~a+b, contrasts = list(a=contr.sum, b=mat))

I guess that the proper way would be to write a function analogous to
contr.sum() to specify the contrast matrix.

Best,
davide

On Wed, Jul 1, 2015 at 2:43 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> You do not set constraints in R you set contrasts. See the relevant
> sections of e.g. Venables and Ripley's MASS book.
>
> ?contrasts  ## what else!
> ?C
>
> for details.
>
> Beyond this, I cannot help.
>
> -- Bert
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Wed, Jul 1, 2015 at 2:14 PM, Davide Risso <davide.risso at berkeley.edu> wrote:
>> Hi Bert,
>>
>> I apologize for the HTML. I will pay more attention in the future.
>>
>> I know that the lm() command of my toy example fits the wrong model
>> with a singular design, maybe I shouldn't have included it.
>>
>> I believe that the model with the additional constraint(s) is not
>> garbage. But I don't know how to specify the additional constraints in
>> lm() (hence, I believe it's an R question).
>>
>> I also know that I can fit a mixed effect model, but this was not the
>> point of my previous email.
>>
>> Best,
>> davide



-- 
Davide Risso, PhD
Post Doctoral Scholar
Division of Biostatistics
School of Public Health
University of California, Berkeley
344 Li Ka Shing Center, #3370
Berkeley, CA 94720-3370
E-mail: davide.risso at berkeley.edu


From prophp at gmail.com  Wed Jul  1 22:23:31 2015
From: prophp at gmail.com (Calin Uioreanu)
Date: Wed, 01 Jul 2015 20:23:31 +0000
Subject: [R] question
In-Reply-To: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
Message-ID: <CAOXqi8Sbu9+d4THa-q2DdvFg0xq1RG0aG7wGQbm17T6SUTYUwQ@mail.gmail.com>

for (var_name in names(z)) {
 # assuming the prep function writes the content of z$var_name to the file
var_name.csv
 prep(z, var_name)
}

On Wed, Jul 1, 2015 at 10:18 PM Lida Zeighami <lid.zigh at gmail.com> wrote:

> I have 682 variables in a data frame , and a function that  I should feed
> 682 variables in this function one by one and each time save the file as a
> special name!
> for emaple:
> my data frame file includes 682 names :
> 1  aaa
> 2  bbb
> 3  dfdsfg
> 4 fghh
> .
>
> 682 fgfhg
> and a function like prep(Z, aaa, .....) and each time I should change the
> variable name in this function and read the variable from the data frame
> and each time I should save the file as a special name such as:
>
> prep1<- prep(z, aaa,...)
> prep2<- prep(z, bbb,...)
> prep3<- prep(z, dfdsfg,..)
> Prep4<- prep(z, fghh,...)
>
> How can I use loop function in R to that?
>
> Thanks
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul  1 19:45:21 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 1 Jul 2015 10:45:21 -0700
Subject: [R] package forecast is not working properly
In-Reply-To: <CAPDX_YsTPNeCa=49OXTs78ByGvnHjQvy2rCEonPtOQ3XAbrktQ@mail.gmail.com>
References: <CAPDX_YsTPNeCa=49OXTs78ByGvnHjQvy2rCEonPtOQ3XAbrktQ@mail.gmail.com>
Message-ID: <9C0187AD-E7B4-4755-82FD-DC3D9B3B5D9F@comcast.net>



This has been reported once on rhelp and once on StackOverflow and both times the advice to update to R 3.2.1 was effective in removing the difficulty. (You did get a warning.)

R 3.2.1 changed the number of arguments to nchar and this was announced in the NEWS.

-- 
David.
On Jul 1, 2015, at 5:51 AM, Magda Joana Silva wrote:

> Hello all,
> 
> I just installed "forecast" package but I have the following error when I
> try to load the library:
> 
> 
>> library("forecast")
> Loading required package: zoo
> 
> Attaching package: ?zoo?
> 
> The following objects are masked from ?package:base?:
> 
>    as.Date, as.Date.numeric
> 
> Loading required package: timeDate
> Error : .onAttach failed in attachNamespace() for 'forecast', details:
>  call: fun(libname, pkgname)
>  error: 4 arguments passed to .Internal(nchar) which requires 3
> In addition: Warning messages:
> 1: package ?forecast? was built under R version 3.2.1
> 2: package ?zoo? was built under R version 3.2.1
> Error: package or namespace load failed for ?forecast?
> 
> I am running Windows 7 and the package is in my local library (I don't have
> admin rights).
> 
> I would be very appreciated if someone could help me
> 
> 
> mjs
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ligges at statistik.tu-dortmund.de  Wed Jul  1 23:35:20 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 01 Jul 2015 23:35:20 +0200
Subject: [R] package forecast is not working properly
In-Reply-To: <CAPDX_YsTPNeCa=49OXTs78ByGvnHjQvy2rCEonPtOQ3XAbrktQ@mail.gmail.com>
References: <CAPDX_YsTPNeCa=49OXTs78ByGvnHjQvy2rCEonPtOQ3XAbrktQ@mail.gmail.com>
Message-ID: <55945D18.1020908@statistik.tu-dortmund.de>



On 01.07.2015 14:51, Magda Joana Silva wrote:
> Hello all,
>
> I just installed "forecast" package but I have the following error when I
> try to load the library:
>
>
>> library("forecast")
> Loading required package: zoo
>
> Attaching package: ?zoo?
>
> The following objects are masked from ?package:base?:
>
>      as.Date, as.Date.numeric
>
> Loading required package: timeDate
> Error : .onAttach failed in attachNamespace() for 'forecast', details:
>    call: fun(libname, pkgname)
>    error: 4 arguments passed to .Internal(nchar) which requires 3
> In addition: Warning messages:
> 1: package ?forecast? was built under R version 3.2.1

Apparently you are not using R-3.2.1? Either install forecast so that it 
fits to your R installation or upgrade R.

Best,
Uwe Ligges


> 2: package ?zoo? was built under R version 3.2.1
> Error: package or namespace load failed for ?forecast?
>
> I am running Windows 7 and the package is in my local library (I don't have
> admin rights).
>
> I would be very appreciated if someone could help me
>
>
> mjs
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From jfox at mcmaster.ca  Thu Jul  2 15:03:22 2015
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 02 Jul 2015 09:03:22 -0400
Subject: [R] checkConv and as.data.frame.default problems in R
In-Reply-To: <CAGpVz+jqwxgY+NOZ6rFa2wc+7XCiay6Nd3d-gC6BFTMazs1=PA@mail.gmail.com>
References: <CAGpVz+jYLGCJT5woKjsWHjNp=4-JuQTSRSh3x64dm5vtr-O=fg@mail.gmail.com>
	<web-564539582@cgpsrv2.cis.mcmaster.ca>
	<CAGpVz+jqwxgY+NOZ6rFa2wc+7XCiay6Nd3d-gC6BFTMazs1=PA@mail.gmail.com>
Message-ID: <web-564572467@cgpsrv2.cis.mcmaster.ca>

Dear Saudi Sadiq,

On Thu, 2 Jul 2015 11:58:38 +0100
 Saudi Sadiq <ss1272 at york.ac.uk> wrote:
> Hi Jon,
> Thanks for your reply. I will try again, after correcting mistakes. Then,
> if the model does not converge, I will come back to you with datasets
> attached.

To be clear, you might have more luck getting an answer if you make your data available to the r-help list, where you originally posted your question, rather than to me personally, since some other list member may have more insight into your problem. I responded to the part of your original message that pertained to the effects package, of which I'm a coauthor.

John

> Best
> 
> On 1 July 2015 at 21:01, John Fox <jfox at mcmaster.ca> wrote:
> 
> > Dear Saudi Sadiq,
> >
> > If you want answers to most of your questions, you'll likely have to
> > provide your data so that people can reproduce the errors. I can, however,
> > answer part of your last question without the data:
> >
> > plot(allEffects(modelqaaf1)) fails because the data set is named q, and
> > there is a standard R function named q(). Obviously there is a scoping
> > issue that confuses allEffects() for a reason I don't yet understand.
> >
> > Here's a simple example that reproduces this error:
> >
> > ---------- snip ------------
> >
> > > library(lme4)
> > Loading required package: Matrix
> > Loading required package: Rcpp
> > > library(effects)
> > > fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), cake)
> > > allEffects(fm1)
> >  model: angle ~ recipe * temperature
> >
> >  recipe*temperature effect
> >       temperature
> > recipe      175      185      195      205      215      225
> >      A 29.13333 31.53333 30.80000 33.53333 38.66667 35.06667
> >      B 26.86667 29.40000 31.73333 32.13333 34.46667 35.26667
> >      C 27.93333 28.93333 31.73333 30.86667 34.40000 35.73333
> > > q <- cake
> > > fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), data=q)
> > > allEffects(fm1)
> >
> >  Error in as.data.frame.default(data, optional = TRUE) :
> >   cannot coerce class ""function"" to a data.frame
> >
> > ------------- snip -------------
> >
> > Why, as you claim, you get the same error with
> > plot(allEffects(modelqaaf)), where the data set is qaaf rather than q, is
> > hard to understand, and perhaps you're mistaken.
> >
> > I hope this helps,
> >  John
> >
> > ------------------------------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario, Canada
> > http://socserv.mcmaster.ca/jfox/
> >
> >
> > On Wed, 1 Jul 2015 15:40:48 +0100
> >  Saudi Sadiq <ss1272 at york.ac.uk> wrote:
> > > Hi All,
> > >
> > > I have two datasets, vowels and qaaf, and both have 8 columns clarified
> > as
> > > follows:
> > >
> > > 1.  convergence: DV (whether participants succeeded to use CA (Cairene
> > > Arabic) or fail to do so; hence, they use MA (Minia Arabic)
> > >
> > > 2.  speaker: 62 participants
> > >
> > > 3.  lexical.item: as pronounced
> > >
> > > 4.  style: careful and casual
> > >
> > > 5.  gender: males and females
> > >
> > > 6.  age: continues variable
> > >
> > > 7.  residence: urbanite, migrant to town or villager
> > >
> > > 8.  education: secondary or below, university or postgraduate
> > >
> > > The only difference between the two datasets is the number of items. With
> > > the vowels dataset, there are 1339 items; in the qaaf dataset there are
> > > 4064 items.
> > >
> > > The aim of the test done was to know which independent variable is more
> > > responsible for using CA forms. I used the lme4 package, function glmer.
> > >
> > > I ran the model:
> > >
> > > 1.  modelvowels <- glmer(convergence ~ gender + age + residence +
> > > education +  style+ (1|lexical.item) + (1|speaker), data=vowels,
> > > family='binomial')
> > >
> > > The message came on the screen:
> > >
> > > 2.  Warning message:
> > >
> > > In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> > >
> > >   Model failed to converge with max|grad| = 0.00210845 (tol = 0.001,
> > > component 1)
> > >
> > > Then I ran the model after removing STYLE as follows:
> > >
> > > 3.  modelvowels <- glmer(convergence ~ gender + age + residence +
> > > education + (1|lexical.item) + (1|speaker), data=vowels,
> > > family='binomial')
> > >
> > > This produced a result. Then, I ran
> > >
> > > 4.  plot(allEffects(modelvowels))
> > >
> > >
> > >
> > > and this gave four charts (for the four independent variables: gender,
> > age,
> > > residence and education).
> > >
> > > Then, I moved to the qaaf dataset (4064 items) and ran the same model
> > >
> > > 5.  modelqaaf <- glmer(convergence ~ gender + age + residence +
> > > education + (1|lexical.item) + (1|speaker), data=qaaf,
> > > family='binomial')
> > >
> > > which gave results with the vowels dataset but there was a warning
> > message
> > > this time
> > >
> > > 6.  Warning message:
> > >
> > > In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> > >
> > >   Model failed to converge with max|grad| = 0.429623 (tol = 0.001,
> > component 8)
> > >
> > > So, I removed one independent variable (residence) and ran this model
> > again:
> > >
> > > 7.  modelqaaf <- glmer(convergence ~ gender + age + education +
> > > (1|lexical.item) + (1|speaker), data=qaaf, family='binomial')
> > >
> > > This gave a result. I removed another independent variable (gender) after
> > > returning (residence) and ran the model:
> > >
> > > 8.  modelqaaf1 <- glmer(convergence ~ residence + age + education +
> > > (1|lexical.item) + (1|speaker), data=q, family='binomial')
> > >
> > > This gave a result as well. Then, I tried to create some graphs using
> > >
> > > 9.  plot(allEffects(modelqaaf)) and
> > >
> > > 10. plot(allEffects(modelqaaf1))
> > >
> > >
> > >
> > > but there was the same error for both
> > >
> > > 11. Error in as.data.frame.default(data, optional = TRUE) :
> > >
> > >   cannot coerce class ""function"" to a data.frame
> > >
> > >  Now, my questions:
> > >
> > > a.  why 1 did not work, why 3 worked, why 5 did not work though it has
> > > the same four IVs of 3, why 7 and 8 worked with only three IVs, and
> > > why 9 and 10 did not work though they are like 4 which worked well.
> > >
> > >
> > >
> > > b.  What are the packages that must be installed with, before or after
> > > the lme4 package?
> > >
> > >
> > >
> > >  Best
> > >
> > > --
> > > Saudi Sadiq,
> > > Assistant Lecturer, English Department,
> > > Faculty of Al-Alsun,Minia University,
> > > Minia City, Egypt &
> > > PhD Student, Language and Linguistic Science Department,
> > > University of York, York, North Yorkshire, UK,
> > > YO10 5DD
> > > http://york.academia.edu/SaudiSadiq
> > > https://www.researchgate.net/profile/Saudi_Sadiq
> > > Certified Interpreter by Pearl Linguistics
> > >
> > > Forum for Arabic Linguistics conference ???? ???????
> > > 28-30th July 2015 - call for papers now open
> > > https://sites.google.com/a/york.ac.uk/fal2015/
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> 
> 
> -- 
> Saudi Sadiq,
> Assistant Lecturer, English Department,
> Faculty of Al-Alsun,Minia University,
> Minia City, Egypt &
> PhD Student, Language and Linguistic Science Department,
> University of York, York, North Yorkshire, UK,
> YO10 5DD
> http://york.academia.edu/SaudiSadiq
> https://www.researchgate.net/profile/Saudi_Sadiq
> Certified Interpreter by Pearl Linguistics
> 
> Forum for Arabic Linguistics conference ???? ???????
> 28-30th July 2015 - call for papers now open
> https://sites.google.com/a/york.ac.uk/fal2015/


From roger.bos at rothschild.com  Thu Jul  2 15:54:46 2015
From: roger.bos at rothschild.com (Bos, Roger)
Date: Thu, 2 Jul 2015 13:54:46 +0000
Subject: [R] as.numeric looses precision
Message-ID: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>

I have a string that contains a number and when I convert it to a number I loose precision and I would like to know if there is a way to avoid that.  Here is my example:

p <- "1087.003489"
as.numeric(p, digits=6)

R gives me 1087.003:

> p <- "1087.003489"
> as.numeric(p, digits=6)
[1] 1087.003

I would be nice if I could keep all the decimal places after the conversion.

Thanks,

Roger





***************************************************************
This message and any attachments are for the intended recipient's use only.
This message may contain confidential, proprietary or legally privileged
information. No right to confidential or privileged treatment
of this message is waived or lost by an error in transmission.
If you have received this message in error, please immediately
notify the sender by e-mail, delete the message, any attachments and all
copies from your system and destroy any hard copies.  You must
not, directly or indirectly, use, disclose, distribute,
print or copy any part of this message or any attachments if you are not
the intended recipient.


From kehld at ktk.pte.hu  Thu Jul  2 16:06:00 2015
From: kehld at ktk.pte.hu (=?iso-8859-2?Q?Kehl_D=E1niel?=)
Date: Thu, 2 Jul 2015 14:06:00 +0000
Subject: [R] as.numeric looses precision
In-Reply-To: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <33D76D77E9AC4B438DA38B348ED6890D14469431@EMAIL.ktkdom.pte.hu>

Hi Roger,

I think it does! Try multiplying with 1000000 or similar. It is just a matter of displaying your result.

best
d
________________________________________
Felad?: R-help [r-help-bounces at r-project.org] ; meghatalmaz&#243;: Bos, Roger [roger.bos at rothschild.com]
K?ldve: 2015. j?lius 2. 15:54
To: r-help at r-project.org
T?rgy: [R] as.numeric looses precision

I have a string that contains a number and when I convert it to a number I loose precision and I would like to know if there is a way to avoid that.  Here is my example:

p <- "1087.003489"
as.numeric(p, digits=6)

R gives me 1087.003:

> p <- "1087.003489"
> as.numeric(p, digits=6)
[1] 1087.003

I would be nice if I could keep all the decimal places after the conversion.

Thanks,

Roger





***************************************************************
This message and any attachments are for the intended recipient's use only.
This message may contain confidential, proprietary or legally privileged
information. No right to confidential or privileged treatment
of this message is waived or lost by an error in transmission.
If you have received this message in error, please immediately
notify the sender by e-mail, delete the message, any attachments and all
copies from your system and destroy any hard copies.  You must
not, directly or indirectly, use, disclose, distribute,
print or copy any part of this message or any attachments if you are not
the intended recipient.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Thu Jul  2 16:16:16 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 2 Jul 2015 14:16:16 +0000
Subject: [R] as.numeric looses precision
In-Reply-To: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3341D@SRVEXCHMBX.precheza.cz>

Hi

As already mentioned it is a matter of printing result on your console.

> p <- "1087.003489"
> as.numeric(p, digits=6)
[1] 1087.003
> options(digits=20)
> as.numeric(p)
[1] 1087.0034889999999

The other problem is that when using PC you run into FAQ 7.31 decimal issue.

"1087.003489" number can not be represented precisely in binary arithmetic.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bos,
> Roger
> Sent: Thursday, July 02, 2015 3:55 PM
> To: r-help at r-project.org
> Subject: [R] as.numeric looses precision
>
> I have a string that contains a number and when I convert it to a
> number I loose precision and I would like to know if there is a way to
> avoid that.  Here is my example:
>
> p <- "1087.003489"
> as.numeric(p, digits=6)
>
> R gives me 1087.003:
>
> > p <- "1087.003489"
> > as.numeric(p, digits=6)
> [1] 1087.003
>
> I would be nice if I could keep all the decimal places after the
> conversion.
>
> Thanks,
>
> Roger
>
>
>
>
>
> ***************************************************************
> This message and any attachments are for the intended recipient's use
> only.
> This message may contain confidential, proprietary or legally
> privileged information. No right to confidential or privileged
> treatment of this message is waived or lost by an error in
> transmission.
> If you have received this message in error, please immediately notify
> the sender by e-mail, delete the message, any attachments and all
> copies from your system and destroy any hard copies.  You must not,
> directly or indirectly, use, disclose, distribute, print or copy any
> part of this message or any attachments if you are not the intended
> recipient.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From sarah.goslee at gmail.com  Thu Jul  2 16:27:06 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 2 Jul 2015 10:27:06 -0400
Subject: [R] as.numeric looses precision
In-Reply-To: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <CAM_vjunW0OO27TBGYsFV-F=MjG3uVYUv3zMMpjWa+yyYVQZH-g@mail.gmail.com>

Hi Roger,

You're mixing up storage and display:

p <- "1087.003489"
p <- as.numeric(p)

> p
[1] 1087.003 # the default value for digits = 7, which can be changed
with option()
> print(p, digits=4)
[1] 1087
> print(p, digits=10)
[1] 1087.003489

But note also:

> print(p, digits=20)
[1] 1087.003488999999945
(And see FAQ 7.31 if you don't understand why.)

Sarah

On Thu, Jul 2, 2015 at 9:54 AM, Bos, Roger <roger.bos at rothschild.com> wrote:
> I have a string that contains a number and when I convert it to a number I loose precision and I would like to know if there is a way to avoid that.  Here is my example:
>
> p <- "1087.003489"
> as.numeric(p, digits=6)
>
> R gives me 1087.003:
>
>> p <- "1087.003489"
>> as.numeric(p, digits=6)
> [1] 1087.003
>
> I would be nice if I could keep all the decimal places after the conversion.
>
> Thanks,
>
> Roger
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From rshepard at appl-ecosys.com  Thu Jul  2 16:46:53 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Thu, 2 Jul 2015 07:46:53 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and box.umbrella
Message-ID: <alpine.LNX.2.11.1507020737190.9672@localhost>

   Lattice's bwplot() displays and prints (using pdf()) the box.rectangle and
box.umbrella in a pale blue that is a pale gray on b&w laser printer output.
I would like to set default options so the box and whiskers are displayed
and printed in black by modifying ~/.Rprofile by adding a .First() function.

   Reading Chapter 7 in Deepayan's book and the ?Startup man page did not
result in my understanding the correct syntax for
lattice.options(default.args = list(...)) in ~/.Rprofile. Web searches did
not find guidance, either.

   Would appreciate help on learning how to set these options so they are
effective when R is invoked.

Rich


From macqueen1 at llnl.gov  Thu Jul  2 16:50:21 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 2 Jul 2015 14:50:21 +0000
Subject: [R] as.numeric looses precision
Message-ID: <D1BA9C39.130566%macqueen1@llnl.gov>

Although Kehl and Petr have already answered the question, I would suggest
this as the simplest way to understand what is going on:

> p <- "1087.003489"
> print( as.numeric(p) , digits=16)
[1] 1087.003489

When R prints numbers, it follows various rules regarding how many decimal
places to display. You assumed it would print all six decimal places, but
that was not the case. The were there in the number itself, but not
printed.


-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/2/15, 6:54 AM, "R-help on behalf of Bos, Roger"
<r-help-bounces at r-project.org on behalf of roger.bos at rothschild.com> wrote:

>I have a string that contains a number and when I convert it to a number
>I loose precision and I would like to know if there is a way to avoid
>that.  Here is my example:
>
>p <- "1087.003489"
>as.numeric(p, digits=6)
>
>R gives me 1087.003:
>
>> p <- "1087.003489"
>> as.numeric(p, digits=6)
>[1] 1087.003
>
>I would be nice if I could keep all the decimal places after the
>conversion.
>
>Thanks,
>
>Roger
>
>
>
>
>
>***************************************************************
>This message and any attachments are for the intended recipient's use
>only.
>This message may contain confidential, proprietary or legally privileged
>information. No right to confidential or privileged treatment
>of this message is waived or lost by an error in transmission.
>If you have received this message in error, please immediately
>notify the sender by e-mail, delete the message, any attachments and all
>copies from your system and destroy any hard copies.  You must
>not, directly or indirectly, use, disclose, distribute,
>print or copy any part of this message or any attachments if you are not
>the intended recipient.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Jul  2 18:25:41 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 2 Jul 2015 09:25:41 -0700
Subject: [R] Lattice: set col = "black" for box.rectangle and
	box.umbrella
In-Reply-To: <alpine.LNX.2.11.1507020737190.9672@localhost>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
Message-ID: <CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>


On Jul 2, 2015, at 7:46 AM, Rich Shepard wrote:

>  Lattice's bwplot() displays and prints (using pdf()) the box.rectangle and
> box.umbrella in a pale blue that is a pale gray on b&w laser printer output.
> I would like to set default options so the box and whiskers are displayed
> and printed in black by modifying ~/.Rprofile by adding a .First() function.
> 
>  Reading Chapter 7 in Deepayan's book and the ?Startup man page did not
> result in my understanding the correct syntax for
> lattice.options(default.args = list(...)) in ~/.Rprofile. Web searches did
> not find guidance, either.
> 
>  Would appreciate help on learning how to set these options so they are
> effective when R is invoked.

The lead-in to section 7.2 mentions `trellis.par.get`,  but in order to change anything you need to use `trellis.par.set` as was illustrated in the pages leading up to that section:


trellis.par.set(box.umbrella=list(col="black"))
bwplot(decrease ~ treatment, OrchardSprays)
trellis.par.set(box.rectangle=list(col="black"))
bwplot(decrease ~ treatment, OrchardSprays)

Could do it all at once with:

trellis.par.set(list( 
              box.umbrella=list(col="black"),
              box.rectangle=list(col="black")
                  )

And Sarkar mentions that this form can be used to create a theme. You may want to investigate the function `standard.theme` and read section 7.1.4 of that chapter again.

-- 

David Winsemius
Alameda, CA, USA


From davies.trevor at gmail.com  Thu Jul  2 18:25:27 2015
From: davies.trevor at gmail.com (Trevor Davies)
Date: Thu, 2 Jul 2015 09:25:27 -0700
Subject: [R] Fread: add one to skip string identifier
In-Reply-To: <CA+8X3fW7XCEq5KmZM8C9p_4Ugi5PLN2X414=P+t+JPMt_gr3tQ@mail.gmail.com>
References: <CAJhyqVhXDFuhrUrL3LTo4mMC-dLUD-0ObwbK071VtAO1hQMr9w@mail.gmail.com>
	<CA+8X3fW7XCEq5KmZM8C9p_4Ugi5PLN2X414=P+t+JPMt_gr3tQ@mail.gmail.com>
Message-ID: <CAJhyqVhZFqC_1K_wwSpCfGE+4YB3r2apqwWtiVHuqwk6uFrECw@mail.gmail.com>

Hi Jim,

Thanks,Jim.  I couldn't get fread to work either for this.

I ended up with this:

txt<- readLines('test_fread.dat');
data.frame1 <- read.table(text=txt, sep='', header=FALSE,
skip=grep('*END*', txt))

BTW, i use that Map Rose function you wrote a few years back quite often.
Thanks for the contributions!
Trevor

On Sun, Jun 28, 2015 at 2:21 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Trevor,
> I couldn't work out how to do it with just fread, but perhaps this will
> help:
>
> # create a test file
> sink("test_fread.dat")
> cat("This is the header of a file\n")
> cat("that ends with the word *end*\n")
> cat("Col1 Col2\n1 2\n3 4\n5 6\n")
> sink()
> # try to read it
> test_con<-file("test_fread.dat","rt")
> header_lines<-1
> while(header_lines) {
>  nextline<-readLines(test_con,1)
>  header_lines<-length(grep("*end",nextline,fixed=TRUE))==0
> }
> fread_dat<-read.table(test_con,header=TRUE)
>
> Jim
>
> On Sat, Jun 27, 2015 at 2:16 AM, Trevor Davies <davies.trevor at gmail.com>
> wrote:
> > I'm trying to read in a file using the function fread.
> >
> > The file that I'm trying to read in has about 100 lines of information I
> > don't want prior to getting to my matrix of data that I do want.  On the
> > line prior to the data I want there is always a string identifier "*end*"
> >
> > The following fread call:
> >
> > impcoord <-
> fread('H:/SBE19plus_01907535_2015_06_17_0093.cnv',skip="*END*")
> >
> > almost gets me there but it starts reading AT *END* and I'd like to have
> it
> > start the line after.  I can't figure out how to make this work.
> >
> > I know I could have a two step function where I have a function scan the
> > file to Identify the line that has *END* but I thought I could just do it
> > with one fread() call.
> >
> > thanks for the help.
> > Trevor
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ss1272 at york.ac.uk  Thu Jul  2 12:58:38 2015
From: ss1272 at york.ac.uk (Saudi Sadiq)
Date: Thu, 2 Jul 2015 11:58:38 +0100
Subject: [R] checkConv and as.data.frame.default problems in R
In-Reply-To: <web-564539582@cgpsrv2.cis.mcmaster.ca>
References: <CAGpVz+jYLGCJT5woKjsWHjNp=4-JuQTSRSh3x64dm5vtr-O=fg@mail.gmail.com>
	<web-564539582@cgpsrv2.cis.mcmaster.ca>
Message-ID: <CAGpVz+jqwxgY+NOZ6rFa2wc+7XCiay6Nd3d-gC6BFTMazs1=PA@mail.gmail.com>

Hi Jon,
Thanks for your reply. I will try again, after correcting mistakes. Then,
if the model does not converge, I will come back to you with datasets
attached.
Best

On 1 July 2015 at 21:01, John Fox <jfox at mcmaster.ca> wrote:

> Dear Saudi Sadiq,
>
> If you want answers to most of your questions, you'll likely have to
> provide your data so that people can reproduce the errors. I can, however,
> answer part of your last question without the data:
>
> plot(allEffects(modelqaaf1)) fails because the data set is named q, and
> there is a standard R function named q(). Obviously there is a scoping
> issue that confuses allEffects() for a reason I don't yet understand.
>
> Here's a simple example that reproduces this error:
>
> ---------- snip ------------
>
> > library(lme4)
> Loading required package: Matrix
> Loading required package: Rcpp
> > library(effects)
> > fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), cake)
> > allEffects(fm1)
>  model: angle ~ recipe * temperature
>
>  recipe*temperature effect
>       temperature
> recipe      175      185      195      205      215      225
>      A 29.13333 31.53333 30.80000 33.53333 38.66667 35.06667
>      B 26.86667 29.40000 31.73333 32.13333 34.46667 35.26667
>      C 27.93333 28.93333 31.73333 30.86667 34.40000 35.73333
> > q <- cake
> > fm1 <- lmer(angle ~ recipe * temperature + (1|recipe:replicate), data=q)
> > allEffects(fm1)
>
>  Error in as.data.frame.default(data, optional = TRUE) :
>   cannot coerce class ""function"" to a data.frame
>
> ------------- snip -------------
>
> Why, as you claim, you get the same error with
> plot(allEffects(modelqaaf)), where the data set is qaaf rather than q, is
> hard to understand, and perhaps you're mistaken.
>
> I hope this helps,
>  John
>
> ------------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
>
>
> On Wed, 1 Jul 2015 15:40:48 +0100
>  Saudi Sadiq <ss1272 at york.ac.uk> wrote:
> > Hi All,
> >
> > I have two datasets, vowels and qaaf, and both have 8 columns clarified
> as
> > follows:
> >
> > 1.  convergence: DV (whether participants succeeded to use CA (Cairene
> > Arabic) or fail to do so; hence, they use MA (Minia Arabic)
> >
> > 2.  speaker: 62 participants
> >
> > 3.  lexical.item: as pronounced
> >
> > 4.  style: careful and casual
> >
> > 5.  gender: males and females
> >
> > 6.  age: continues variable
> >
> > 7.  residence: urbanite, migrant to town or villager
> >
> > 8.  education: secondary or below, university or postgraduate
> >
> > The only difference between the two datasets is the number of items. With
> > the vowels dataset, there are 1339 items; in the qaaf dataset there are
> > 4064 items.
> >
> > The aim of the test done was to know which independent variable is more
> > responsible for using CA forms. I used the lme4 package, function glmer.
> >
> > I ran the model:
> >
> > 1.  modelvowels <- glmer(convergence ~ gender + age + residence +
> > education +  style+ (1|lexical.item) + (1|speaker), data=vowels,
> > family='binomial')
> >
> > The message came on the screen:
> >
> > 2.  Warning message:
> >
> > In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> >
> >   Model failed to converge with max|grad| = 0.00210845 (tol = 0.001,
> > component 1)
> >
> > Then I ran the model after removing STYLE as follows:
> >
> > 3.  modelvowels <- glmer(convergence ~ gender + age + residence +
> > education + (1|lexical.item) + (1|speaker), data=vowels,
> > family='binomial')
> >
> > This produced a result. Then, I ran
> >
> > 4.  plot(allEffects(modelvowels))
> >
> >
> >
> > and this gave four charts (for the four independent variables: gender,
> age,
> > residence and education).
> >
> > Then, I moved to the qaaf dataset (4064 items) and ran the same model
> >
> > 5.  modelqaaf <- glmer(convergence ~ gender + age + residence +
> > education + (1|lexical.item) + (1|speaker), data=qaaf,
> > family='binomial')
> >
> > which gave results with the vowels dataset but there was a warning
> message
> > this time
> >
> > 6.  Warning message:
> >
> > In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :
> >
> >   Model failed to converge with max|grad| = 0.429623 (tol = 0.001,
> component 8)
> >
> > So, I removed one independent variable (residence) and ran this model
> again:
> >
> > 7.  modelqaaf <- glmer(convergence ~ gender + age + education +
> > (1|lexical.item) + (1|speaker), data=qaaf, family='binomial')
> >
> > This gave a result. I removed another independent variable (gender) after
> > returning (residence) and ran the model:
> >
> > 8.  modelqaaf1 <- glmer(convergence ~ residence + age + education +
> > (1|lexical.item) + (1|speaker), data=q, family='binomial')
> >
> > This gave a result as well. Then, I tried to create some graphs using
> >
> > 9.  plot(allEffects(modelqaaf)) and
> >
> > 10. plot(allEffects(modelqaaf1))
> >
> >
> >
> > but there was the same error for both
> >
> > 11. Error in as.data.frame.default(data, optional = TRUE) :
> >
> >   cannot coerce class ""function"" to a data.frame
> >
> >  Now, my questions:
> >
> > a.  why 1 did not work, why 3 worked, why 5 did not work though it has
> > the same four IVs of 3, why 7 and 8 worked with only three IVs, and
> > why 9 and 10 did not work though they are like 4 which worked well.
> >
> >
> >
> > b.  What are the packages that must be installed with, before or after
> > the lme4 package?
> >
> >
> >
> >  Best
> >
> > --
> > Saudi Sadiq,
> > Assistant Lecturer, English Department,
> > Faculty of Al-Alsun,Minia University,
> > Minia City, Egypt &
> > PhD Student, Language and Linguistic Science Department,
> > University of York, York, North Yorkshire, UK,
> > YO10 5DD
> > http://york.academia.edu/SaudiSadiq
> > https://www.researchgate.net/profile/Saudi_Sadiq
> > Certified Interpreter by Pearl Linguistics
> >
> > Forum for Arabic Linguistics conference ???? ???????
> > 28-30th July 2015 - call for papers now open
> > https://sites.google.com/a/york.ac.uk/fal2015/
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>


-- 
Saudi Sadiq,
Assistant Lecturer, English Department,
Faculty of Al-Alsun,Minia University,
Minia City, Egypt &
PhD Student, Language and Linguistic Science Department,
University of York, York, North Yorkshire, UK,
YO10 5DD
http://york.academia.edu/SaudiSadiq
https://www.researchgate.net/profile/Saudi_Sadiq
Certified Interpreter by Pearl Linguistics

Forum for Arabic Linguistics conference ???? ???????
28-30th July 2015 - call for papers now open
https://sites.google.com/a/york.ac.uk/fal2015/

	[[alternative HTML version deleted]]


From ss1272 at york.ac.uk  Thu Jul  2 13:37:40 2015
From: ss1272 at york.ac.uk (Saudi Sadiq)
Date: Thu, 2 Jul 2015 12:37:40 +0100
Subject: [R] checkConv problems in R
Message-ID: <CAGpVz+ha8qUz1=rTiwJi7m8SKUS-+kKiR_9FFJ0NcwOmOvC47w@mail.gmail.com>

Hi All,

I hope you will give me a hand with the checkConv problems.  have two
datasets, vowels and qaaf, and both have many columns. I am interested in
these 8 columns clarified as follows:

1.         convergence: DV (whether participants succeeded to use CA (Cairo
Arabic) instead of MA (Minia Arabic)

2.         speaker: 62 participants

3.         item: as pronounced

4.         style: careful/casual

5.         gender: males/females

6.         age: continues variable

7.         residence: urbanite/rural migrant/villager

8.         education: secondary or below/university/postgraduate



The only difference between the two datasets is the number of items. With
the vowels dataset, there are 1339 items; in the qaaf dataset there are
4064 items.

The aim of the test done was to know which independent variable is more
responsible for using CA forms. I used the lme4 package, function glmer.

I ran the model:

A.     modelvowels <- glmer(convergence ~ gender + age + residence +
education +  style+ (1|lexical.item) + (1|speaker), data=vowels,
family='binomial')

The message came on the screen:

B.     Warning message:

In checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv,  :

  Model failed to converge with max|grad| = 0.00210845 (tol = 0.001,
component 1)

Then I ran the model after removing STYLE as follows:

C.     modelvowels <- glmer(convergence ~ gender + age + residence +
education + (1|lexical.item) + (1|speaker), data=vowels, family='binomial')

This produced a result. Then, I ran

D.     plot(allEffects(modelvowels))

and this gave four charts (for the four independent variables: gender, age,
residence and education).

Then, I moved to the qaaf dataset (4064 items) and ran the same model

E.      modelqaaf <- glmer(convergence ~ gender + real.age + residence +
education +

                            (1|lexical.item) + (1|speaker), data=qaaf,
family='binomial')

which gave results with the vowels dataset but there was a warning message
this time

F.      Warning message:

            In checkConv(attr(opt, "derivs"), opt$par, ctrl =
control$checkConv,  :

            Model failed to converge with max|grad| = 0.429623 (tol =
0.001, component 8)

So, I removed one independent variable (residence) and ran this model again:

G.     modelqaaf <- glmer(convergence ~ gender + real.age + education +

                            (1|lexical.item) + (1|speaker), data=qaaf,
family='binomial')

This gave a result. I removed another independent variable (gender) after
returning (residence) and ran the model:

H.     modelqaaf1<- glmer(convergence ~  real.age + residence + education +

                            (1|lexical.item) + (1|speaker), data=qaaf,
family='binomial')

It also worked well and produced a result.


Now, my questions:

?     Why did not A work, why did C work, why did not E work though it has
the same four predictors of C,  why G and H worked with only three
predictors?

?     What are the packages that must be installed with, before or after
the lme4 package?

 Please, find attached the datasets.


 Best

-- 
Saudi Sadiq,
Assistant Lecturer, English Department,
Faculty of Al-Alsun,Minia University,
Minia City, Egypt &
PhD Student, Language and Linguistic Science Department,
University of York, York, North Yorkshire, UK,
YO10 5DD
http://york.academia.edu/SaudiSadiq
https://www.researchgate.net/profile/Saudi_Sadiq
Certified Interpreter by Pearl Linguistics

Forum for Arabic Linguistics conference ???? ???????
28-30th July 2015 - call for papers now open
https://sites.google.com/a/york.ac.uk/fal2015/

From ravi.varadhan at jhu.edu  Thu Jul  2 16:28:19 2015
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Thu, 2 Jul 2015 14:28:19 +0000
Subject: [R] Ramanujan and the accuracy of floating point computations -
 using Rmpfr in R
Message-ID: <9ae1fd306f07419182b467128429e45c@DOM-EB1-2013.win.ad.jhu.edu>

Hi,

Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).

If I compute this using the Wolfram alpha engine, I get:
262537412640768743.99999999999925007259719818568887935385...

When I do this in R 3.1.1 (64-bit windows), I get:
262537412640768256.0000

The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).

In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:

library(Rmpfr)


> exp(sqrt(163) * mpfr(pi, 120))

1 'mpfr' number of precision  120   bits

[1] 262537412640767837.08771354274620169031

The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?

Thank you,
Ravi



	[[alternative HTML version deleted]]


From prophp at gmail.com  Thu Jul  2 16:05:19 2015
From: prophp at gmail.com (Calin Uioreanu)
Date: Thu, 02 Jul 2015 14:05:19 +0000
Subject: [R] as.numeric looses precision
In-Reply-To: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D81EEE3094@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <CAOXqi8T-R4Fny0L5q8HAOBtbmY4pSNLAhR01590nP+TJ-696gA@mail.gmail.com>

Hello,

There's a difference between the displayed value and the internally stored.
You could use options(digits=10), but this is a global option, and will
affect all future numerical output. default value is 7

> p <- "1087.003489"
> as.numeric(p, digits=6)
[1] 1087.003
> options(digits=10) # set output precision
> as.numeric(p, digits=6)
[1] 1087.003489
>

for more information look here:
http://stackoverflow.com/questions/4540649/retain-numerical-precision-in-an-r-data-frame

regards calin


On Thu, Jul 2, 2015 at 3:56 PM Bos, Roger <roger.bos at rothschild.com> wrote:

> I have a string that contains a number and when I convert it to a number I
> loose precision and I would like to know if there is a way to avoid that.
> Here is my example:
>
> p <- "1087.003489"
> as.numeric(p, digits=6)
>
> R gives me 1087.003:
>
> > p <- "1087.003489"
> > as.numeric(p, digits=6)
> [1] 1087.003
>
> I would be nice if I could keep all the decimal places after the
> conversion.
>
> Thanks,
>
> Roger
>
>
>
>
>
> ***************************************************************
> This message and any attachments are for the intended recipient's use only.
> This message may contain confidential, proprietary or legally privileged
> information. No right to confidential or privileged treatment
> of this message is waived or lost by an error in transmission.
> If you have received this message in error, please immediately
> notify the sender by e-mail, delete the message, any attachments and all
> copies from your system and destroy any hard copies.  You must
> not, directly or indirectly, use, disclose, distribute,
> print or copy any part of this message or any attachments if you are not
> the intended recipient.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From peterenos at ymail.com  Thu Jul  2 16:51:07 2015
From: peterenos at ymail.com (Peter Tuju)
Date: Thu, 2 Jul 2015 14:51:07 +0000 (UTC)
Subject: [R] Extracting data from a file containing data
In-Reply-To: <CAAxdm-7r5aJ-_DnuHYpaaB_33auTmkb-f3uAMYJ3kFSAmHi4Kw@mail.gmail.com>
References: <CAAxdm-7r5aJ-_DnuHYpaaB_33auTmkb-f3uAMYJ3kFSAmHi4Kw@mail.gmail.com>
Message-ID: <2035622469.1202770.1435848667146.JavaMail.yahoo@mail.yahoo.com>

Dear Jim,Thank you very much indeed. Its a great job you have done and I really appreciate.Thank you so much.
?_____________
Peter? E. Tuju
Dar es Salaam
T A N Z A N I A
----------------------
      From: jim holtman <jholtman at gmail.com>
 To: Peter Tuju <peterenos at ymail.com> 
Cc: "r-help at r-project.org" <r-help at r-project.org> 
 Sent: Thursday, July 2, 2015 3:49 AM
 Subject: Re: [R] Extracting data from a file containing data
   
Here is a way to do case I.? It uses the 'tidyr' package and produces results like:
> case1[[1]]? ? ?YR JF-R_NINO1.2 MAM-R_NINO1.2 JJA-R_NINO1.2 OND-R_NINO1.21 ?1982 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ME ? ? ? ? ? ?SE2 ?1983 ? ? ? ? ? SE ? ? ? ? ? ?SE ? ? ? ? ? ?SE ? ? ? ? ? ?ME3 ?1984 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML4 ?1985 ? ? ? ? ? SL ? ? ? ? ? ?SL ? ? ? ? ? ?SL ? ? ? ? ? ?ML5 ?1986 ? ? ? ? ? ME ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ME6 ?1987 ? ? ? ? ? ME ? ? ? ? ? ?SE ? ? ? ? ? ?SE ? ? ? ? ? ?SE7 ?1988 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?SL ? ? ? ? ? ?SL8 ?1989 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML9 ?1990 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML10 1991 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ME ? ? ? ? ? ?ME11 1992 ? ? ? ? ? ME ? ? ? ? ? ?SE ? ? ? ? ? ?ME ? ? ? ? ? ?ML12 1993 ? ? ? ? ? ME ? ? ? ? ? ?ME ? ? ? ? ? ?ME ? ? ? ? ? ?ME13 1994 ? ? ? ? ? ML ? ? ? ? ? ?SL ? ? ? ? ? ?ML ? ? ? ? ? ?ME14 1995 ? ? ? ? ? ME ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML15 1996 ? ? ? ? ? ML ? ? ? ? ? ?SL ? ? ? ? ? ?SL ? ? ? ? ? ?SL16 1997 ? ? ? ? ? ML ? ? ? ? ? ?SE ? ? ? ? ? ?SE ? ? ? ? ? ?SE17 1998 ? ? ? ? ? SE ? ? ? ? ? ?SE ? ? ? ? ? ?SE ? ? ? ? ? ?ML18 1999 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?SL ? ? ? ? ? ?ML19 2000 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML20 2001 ? ? ? ? ? ML ? ? ? ? ? ?ME ? ? ? ? ? ?ML ? ? ? ? ? ?SL21 2002 ? ? ? ? ? ML ? ? ? ? ? ?ME ? ? ? ? ? ?ML ? ? ? ? ? ?ME22 2003 ? ? ? ? ? ML ? ? ? ? ? ?SL ? ? ? ? ? ?ML ? ? ? ? ? ?ME23 2004 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?SL ? ? ? ? ? ?ME24 2005 ? ? ? ? ? ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML ? ? ? ? ? ?ML25 2006 ? ? ? ? ? ME ? ? ? ? ? ?ML ? ? ? ? ? ?ME ? ? ? ? ? ?SE26 2007 ? ? ? ? ? ME ? ? ? ? ? ?SL ? ? ? ? ? ?SL ? ? ? ? ? ?SL27 2008 ? ? ? ? ? ML ? ? ? ? ? ?ME ? ? ? ? ? ?ME ? ? ? ? ? ?ML28 2009 ? ? ? ? ? ML ? ? ? ? ? ?ME ? ? ? ? ? ?ME ? ? ? ? ? ?ME29 2010 ? ? ? ? ? ME ? ? ? ? ? ?ME ? ? ? ? ? ?SL ? ? ? ? ? ?SL30 2011 ? ? ? ? ? ML ? ? ? ? ? ?ME ? ? ? ? ? ?ME ? ? ? ? ? ?ML31 2012 ? ? ? ? ? ML ? ? ? ? ? ?ME ? ? ? ? ? ?ME ? ? ? ? ? ?ML



Jim Holtman
Data Munger Guru
?
What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.
On Fri, Jun 26, 2015 at 5:27 AM, Peter Tuju <peterenos at ymail.com> wrote:

Dear Jim Holtman,
Thank you very much for your help. 
Theproblem I'm trying to solve is ?To determine weather the evolutionof ENSO can influence rainfall over Tanzania?. In this study I havetwo types of data, ie Rainfall data (for 23 stations) and Ninoindices data, both spanning a period of 31 years (1982-2012). 
CASEI:1.In ?Nino.indices.txt? data for all columns of the ninoregions (both for anomalies and SST), to calculate the Season means"January & February (JF)", ?March, April and may(MAM)", "June, July & August (JJA)" and "October,November and December (OND" for each year. and have the outputin table form as;
Ninoindices Mean
|  Years  |  JF SST Mean NINO1+2  |  JF ANOM Mean NINO1+2  |  MAM SST Mean NINO3  |  MAM ANOM Mean NINO3  |  JJA SST Mean NINO4  |  JJA ANOM Mean NINO4  |  OND SST Mean NINO3.4  |  OND SST Mean NINO3.4  |
|  1982  |  
   |  
   |  
   |  
   |  
   |  
   |  
   |  
   |
|  1983  |  
   |  
   |  
   |  
   |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  
   |  
   |  
   |  
   |  
   |  
   |
|  - - - -    |  
   |  
   |  
   |  
   |  
   |  
   |  
   |  
   |
|   2012  |  
   |  
   |  
   |  
   |  
   |  
   |  
   |  
   |


2.To use the Yearly anomalies for each column in nino regions toclassify the events as;(i). IfANOM Mean>1, then I assign itto ?SE? (Being as Strong El-nino)(ii). If0<ANOMMean<=1 , then Iassign it to ?ME? (Being as Moderate El-nino)(iii). IfANOM==0,then I assign it to ?NT? (Being as NeutralCondition)(iv). If ANOMMean< (-1),then I assign it to ?SL? (Being as Strong La-nina)(v). If-1<=ANOMMean< 0, then I assign it to ?ML? (Being as Moderate La-nina)Theoutput have to be in table form as;
FORNINO1+2
|  Years  |  JF ANOM Mean NINO1+2  |  MAM ANOM Mean NINO1+2  |  JJA ANOM Mean NINO1+2  |  OND SST Mean NINO1+2  |
|  1982    |  SE    |  
   |  
   |  
   |
|  1983  |  
   |  
   |  SL  |  
   |
|  - - - -  |  
   |  
   |  
   |  ML  |
|  - - - -  |  
   |  ME  |  
   |  
   |
|  2012  |  
   |  
   |  
   |   SL  |


FORNINO3
|  Years  |  JF ANOM Mean NINO3  |  MAM ANOM Mean NINO3  |  JJA ANOM Mean NINO3  |  OND SST Mean NINO3  |
|  1982    |  SE    |  
   |  
   |  
   |
|  1983  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  ME  |  
   |  
   |
|  2012  |  
   |  
   |  
   |   SL  |


FORNINO4
|  Years  |  JF ANOM Mean NINO4  |  MAM ANOM Mean NINO4  |  JJA ANOM Mean NINO4  |  OND SST Mean NINO4  |
|  1982    |  SE    |  
   |  
   |  
   |
|  1983  |  
   |  
   |  
   |  
   |
|  - - - -  |  ML  |  
   |  
   |  SL  |
|  - - - -  |  
   |  ME  |  
   |  
   |
|  2012  |  
   |  
   |  
   |   SL  |



FORNINO3.4
|  Years  |  JF ANOM Mean NINO3.4  |  MAM ANOM Mean NINO3.4  |  JJA ANOM Mean NINO3.4  |  OND SST Mean NINO3.4  |
|  1982    |  SE    |  SL  |  
   |  
   |
|  1983  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  ML  |  
   |
|  - - - -  |  
   |  ME  |  
   |  
   |
|  2012  |  
   |  
   |  
   |   SL  |



3.To plot the time series graph for each nino regions using the YearlyAnomalies.

CASEII:Consider the Rainfall station data;1. In some files containing the data there are missing data labeledby variable ?m?. I want to substitute these missing data withlong term mean.2. Find the rowSum and anomalies of each file containing the data.3. To find the cumsum of the rowSum of each file containing the data.4. Plot the single mass curves ie. Plot(Year, cumsum) for each fileand name its title as the name of the corresponding file name.5. Plot the time series graphs for seasons JF, MAM, JJA and OND foreach file and name give its name as ?Time series graph for ?nameof the file??6. To find the seasonal correlations for JF, MAM, JJA and OND usingthe anomalies of the rainfall station data and that of each ninoregion indices, and have the results in table form as;
CORRELATIONSOF RAINFALL AND NINO1+2 ANOMALIES
|  Years  |  JF  |  MAM  |  JJA  |  OND  |
|  1982    |  
   |  
   |  
   |  
   |
|  1983  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  
   |  
   |
|  2012  |  
   |  
   |  
   |  
   |


CORRELATIONSOF RAINFALL AND NINO3ANOMALIES
|  Years  |  JF  |  MAM  |  JJA  |  OND  |
|  1982    |  
   |  
   |  
   |  
   |
|  1983  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  
   |  
   |
|  - - - -  |  
   |  
   |  
   |  
   |
|  2012  |  
   |  
   |  
   |  
   |


CORRELATIONSOF RAINFALL AND NINO4ANOMALIES
|  Years  |  JF  |  MAM  |  JJA  |  OND  |
|  1982    |  
   |  |

...

[Message clipped]??



  
	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Thu Jul  2 18:57:52 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Thu, 2 Jul 2015 09:57:52 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and
 box.umbrella
In-Reply-To: <CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
Message-ID: <alpine.LNX.2.11.1507020954360.9672@localhost>

On Thu, 2 Jul 2015, David Winsemius wrote:

> The lead-in to section 7.2 mentions `trellis.par.get`,  but in order to
> change anything you need to use `trellis.par.set` as was illustrated in
> the pages leading up to that section:

David,

   I saw `trellis.par.set` but did not know how to use that in ~/.Rprofile.
>
> trellis.par.set(box.umbrella=list(col="black"))
> bwplot(decrease ~ treatment, OrchardSprays)
> trellis.par.set(box.rectangle=list(col="black"))
> bwplot(decrease ~ treatment, OrchardSprays)
>
> Could do it all at once with:
>
> trellis.par.set(list(
>              box.umbrella=list(col="black"),
>              box.rectangle=list(col="black")
>                  )
>
> And Sarkar mentions that this form can be used to create a theme. You may
> want to investigate the function `standard.theme` and read section 7.1.4
> of that chapter again.

   Yes. Modifying the standard.theme would do the job, too.

Many thanks,

Rich


From dwinsemius at comcast.net  Thu Jul  2 19:07:48 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 2 Jul 2015 10:07:48 -0700
Subject: [R] Lattice: set col = "black" for box.rectangle and
	box.umbrella
In-Reply-To: <alpine.LNX.2.11.1507020954360.9672@localhost>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
Message-ID: <8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>


On Jul 2, 2015, at 9:57 AM, Rich Shepard wrote:

> On Thu, 2 Jul 2015, David Winsemius wrote:
> 
>> The lead-in to section 7.2 mentions `trellis.par.get`,  but in order to
>> change anything you need to use `trellis.par.set` as was illustrated in
>> the pages leading up to that section:
> 
> David,
> 
> I saw `trellis.par.set` but did not know how to use that in ~/.Rprofile.

My .Rprofile has this (among other items):

lattice.options(default.args = list(page = function(n) {
  panel.text(lab = sprintf("%s", date()), x = 0.01, y = 0.01, adj = 0, srt=90)
}))

It prints a "timestamp" along any Lattice plot I make. I think I picked it several years ago from an R-help posting bt someclever person whose name I am not remembering at the moment.

-- 
David.

>> 
>> trellis.par.set(box.umbrella=list(col="black"))
>> bwplot(decrease ~ treatment, OrchardSprays)
>> trellis.par.set(box.rectangle=list(col="black"))
>> bwplot(decrease ~ treatment, OrchardSprays)
>> 
>> Could do it all at once with:
>> 
>> trellis.par.set(list(
>>            box.umbrella=list(col="black"),
>>            box.rectangle=list(col="black")
>>                )
>> 
>> And Sarkar mentions that this form can be used to create a theme. You may
>> want to investigate the function `standard.theme` and read section 7.1.4
>> of that chapter again.
> 
> Yes. Modifying the standard.theme would do the job, too.
> 
> Many thanks,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From boris.steipe at utoronto.ca  Thu Jul  2 19:18:22 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 2 Jul 2015 13:18:22 -0400
Subject: [R] Ramanujan and the accuracy of floating point computations -
	using Rmpfr in R
In-Reply-To: <9ae1fd306f07419182b467128429e45c@DOM-EB1-2013.win.ad.jhu.edu>
References: <9ae1fd306f07419182b467128429e45c@DOM-EB1-2013.win.ad.jhu.edu>
Message-ID: <306FF6D4-18AF-418D-A4F0-7EE0C41319BA@utoronto.ca>

Just a wild guess, but did you check exactly which operations are actually done to high precision? Obviously you will need high-resolution representations of pi and e to get an improved result.

B.



On Jul 2, 2015, at 10:28 AM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:

> Hi,
> 
> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
> 
> If I compute this using the Wolfram alpha engine, I get:
> 262537412640768743.99999999999925007259719818568887935385...
> 
> When I do this in R 3.1.1 (64-bit windows), I get:
> 262537412640768256.0000
> 
> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
> 
> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
> 
> library(Rmpfr)
> 
> 
>> exp(sqrt(163) * mpfr(pi, 120))
> 
> 1 'mpfr' number of precision  120   bits
> 
> [1] 262537412640767837.08771354274620169031
> 
> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
> 
> Thank you,
> Ravi
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Thu Jul  2 19:28:00 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 02 Jul 2015 10:28:00 -0700
Subject: [R] Ramanujan and the accuracy of floating point computations -
	using Rmpfr in R
In-Reply-To: <9ae1fd306f07419182b467128429e45c@DOM-EB1-2013.win.ad.jhu.edu>
References: <9ae1fd306f07419182b467128429e45c@DOM-EB1-2013.win.ad.jhu.edu>
Message-ID: <E2B85EE0-4CAF-4320-B6DC-B43C5E7440E5@dcn.davis.CA.us>

I don't know much about Rmpfr, but it doesn't look like your "pi" or "sqrt" or "exp" are being handled by that package, so I am not really seeing why your result should be more accurate when you have loaded that package.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 2, 2015 7:28:19 AM PDT, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
>Hi,
>
>Ramanujan supposedly discovered that the number, 163, has this
>interesting property that exp(sqrt(163)*pi), which is obviously a
>transcendental number, is real close to an integer (close to 10^(-12)).
>
>If I compute this using the Wolfram alpha engine, I get:
>262537412640768743.99999999999925007259719818568887935385...
>
>When I do this in R 3.1.1 (64-bit windows), I get:
>262537412640768256.0000
>
>The absolute error between the exact and R's value is 488, with a
>relative error of about 1.9x10^(-15).
>
>In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but
>I am unable to get accurate results:
>
>library(Rmpfr)
>
>
>> exp(sqrt(163) * mpfr(pi, 120))
>
>1 'mpfr' number of precision  120   bits
>
>[1] 262537412640767837.08771354274620169031
>
>The above answer is not only inaccurate, but it is actually worse than
>the answer using the usual double precision.  Any thoughts as to what I
>am doing wrong?
>
>Thank you,
>Ravi
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From aps6dl at yahoo.com  Thu Jul  2 19:38:57 2015
From: aps6dl at yahoo.com (Aditya Singh)
Date: Thu, 2 Jul 2015 10:38:57 -0700
Subject: [R] Ramanujan and the accuracy of floating point computations -
	using Rmpfr in R
Message-ID: <1435858737.57954.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>


Ravi

1. You may want to check the sqrt too.

2. Why not take log and try?

Aditya



------------------------------
On Thu 2 Jul, 2015 10:18 AM PDT Boris Steipe wrote:

>Just a wild guess, but did you check exactly which operations are actually done to high precision? Obviously you will need high-resolution representations of pi and e to get an improved result.
>
>B.
>
>
>
>On Jul 2, 2015, at 10:28 AM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
>
>> Hi,
>> 
>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>> 
>> If I compute this using the Wolfram alpha engine, I get:
>> 262537412640768743.99999999999925007259719818568887935385...
>> 
>> When I do this in R 3.1.1 (64-bit windows), I get:
>> 262537412640768256.0000
>> 
>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>> 
>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>> 
>> library(Rmpfr)
>> 
>> 
>> exp(sqrt(163) * mpfr(pi, 120))
>> 
>> 1 'mpfr' number of precision  120   bits
>> 
>> [1] 262537412640767837.08771354274620169031
>> 
>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>> 
>> Thank you,
>> Ravi
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Thu Jul  2 19:50:03 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Thu, 2 Jul 2015 10:50:03 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and
 box.umbrella
In-Reply-To: <8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
Message-ID: <alpine.LNX.2.11.1507021036250.9672@localhost>

On Thu, 2 Jul 2015, David Winsemius wrote:

> My .Rprofile has this (among other items):

   ...

David,

   My attempts at adding lattice.options to .Rprofile fail. In that file is

lattice.options(trellis.par.set(list(
                 box.umbrella=list(col="black"),
   		box.rectangle=list(col="black")
                      )
 		))

and invoking R within emacs displays this warning:

Warning message:
In trellis.par.set(list(box.umbrella = list(col = "black"), box.rectangle = list(col = "black"))) :
   Note: The default device has been opened to honour attempt to modify trellis settings
[Previously saved workspace restored]

   Removing the enclosing lattice.options() produces the same error. In
section 7.1.4 I'm not groking the correct syntax to change the color of both
box items within ~.Rprofile.

Rich


From aps6dl at yahoo.com  Thu Jul  2 20:02:20 2015
From: aps6dl at yahoo.com (Aditya Singh)
Date: Thu, 2 Jul 2015 11:02:20 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
Message-ID: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>


Ravi

I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?

Aditya



------------------------------
On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:

>Hi,
>
>Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>
>If I compute this using the Wolfram alpha engine, I get:
>262537412640768743.99999999999925007259719818568887935385...
>
>When I do this in R 3.1.1 (64-bit windows), I get:
>262537412640768256.0000
>
>The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>
>In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>
>library(Rmpfr)
>
>
>> exp(sqrt(163) * mpfr(pi, 120))
>
>1 'mpfr' number of precision  120   bits
>
>[1] 262537412640767837.08771354274620169031
>
>The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>
>Thank you,
>Ravi
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Thu Jul  2 20:04:23 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Thu, 2 Jul 2015 11:04:23 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and
 box.umbrella
In-Reply-To: <alpine.LNX.2.11.1507021036250.9672@localhost>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
Message-ID: <alpine.LNX.2.11.1507021101040.9672@localhost>

On Thu, 2 Jul 2015, Rich Shepard wrote:

> ... and invoking R within emacs ...

   Trying a different approach:

bwplot(quant ~ param, data = b, main = 'Stream B Constituents', 
ylab = 'Concentration (mg/L)', xlab = 'Constituent', par.settings =
simpleTheme(box.rectangle(col = 'black'), box.umbrella(col='black'), box.dot(col= 'red')))
Error in setValue(col, "col", 1:6) :
   could not find function "box.rectangle"

I'm stymied,

Rich


From jholtman at gmail.com  Thu Jul  2 20:34:52 2015
From: jholtman at gmail.com (jim holtman)
Date: Thu, 2 Jul 2015 14:34:52 -0400
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
Message-ID: <CAAxdm-7qAjOCPVRmVXrfekHg=Ksmk=8Ud-gKz7J=H5Rsb9YjKQ@mail.gmail.com>

This is the standard FAQ 7.31 and then read in detail the referenced paper.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <
r-help at r-project.org> wrote:

>
> Ravi
>
> I am a chemical engineer by training. Is there not something like law of
> corresponding states in numerical analysis?
>
> Aditya
>
>
>
> ------------------------------
> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>
> >Hi,
> >
> >Ramanujan supposedly discovered that the number, 163, has this
> interesting property that exp(sqrt(163)*pi), which is obviously a
> transcendental number, is real close to an integer (close to 10^(-12)).
> >
> >If I compute this using the Wolfram alpha engine, I get:
> >262537412640768743.99999999999925007259719818568887935385...
> >
> >When I do this in R 3.1.1 (64-bit windows), I get:
> >262537412640768256.0000
> >
> >The absolute error between the exact and R's value is 488, with a
> relative error of about 1.9x10^(-15).
> >
> >In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I
> am unable to get accurate results:
> >
> >library(Rmpfr)
> >
> >
> >> exp(sqrt(163) * mpfr(pi, 120))
> >
> >1 'mpfr' number of precision  120   bits
> >
> >[1] 262537412640767837.08771354274620169031
> >
> >The above answer is not only inaccurate, but it is actually worse than
> the answer using the usual double precision.  Any thoughts as to what I am
> doing wrong?
> >
> >Thank you,
> >Ravi
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Jul  2 20:46:39 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 2 Jul 2015 11:46:39 -0700
Subject: [R] Lattice: set col = "black" for box.rectangle and
	box.umbrella
In-Reply-To: <alpine.LNX.2.11.1507021101040.9672@localhost>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
	<alpine.LNX.2.11.1507021101040.9672@localhost>
Message-ID: <CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>

1. box.rectangle, etc. are not functions;  they are (lists of)
parameters that are sublists of the par.settings list.

2. Read the Help for simpleTheme() -- it has a specific list of
parameters which is nothing like what you show.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 2, 2015 at 11:04 AM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> On Thu, 2 Jul 2015, Rich Shepard wrote:
>
>> ... and invoking R within emacs ...
>
>
>   Trying a different approach:
>
> bwplot(quant ~ param, data = b, main = 'Stream B Constituents', ylab =
> 'Concentration (mg/L)', xlab = 'Constituent', par.settings =
> simpleTheme(box.rectangle(col = 'black'), box.umbrella(col='black'),
> box.dot(col= 'red')))
> Error in setValue(col, "col", 1:6) :
>   could not find function "box.rectangle"
>
> I'm stymied,
>
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Thu Jul  2 20:51:55 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Thu, 2 Jul 2015 11:51:55 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and
 box.umbrella
In-Reply-To: <CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
	<alpine.LNX.2.11.1507021101040.9672@localhost>
	<CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507021150100.12452@localhost>

On Thu, 2 Jul 2015, Bert Gunter wrote:

> 1. box.rectangle, etc. are not functions;  they are (lists of)
> parameters that are sublists of the par.settings list.

Bert,

   Thought so ...

> 2. Read the Help for simpleTheme() -- it has a specific list of
> parameters which is nothing like what you show.

   found the simpleTheme example on a web forum thread. Mea culpa! Will read
the help page. Still desire to have the changes set in .Rprofile rather than
for each plot.

Thanks,

Rich


From bgunter.4567 at gmail.com  Thu Jul  2 20:57:27 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 2 Jul 2015 11:57:27 -0700
Subject: [R] Lattice: set col = "black" for box.rectangle and
	box.umbrella
In-Reply-To: <CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
	<alpine.LNX.2.11.1507021101040.9672@localhost>
	<CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
Message-ID: <CAGxFJbTVMXGUW5w_sK4aYxgxCdvC=sLEnT6-CbnvuEA+-Z4yZA@mail.gmail.com>

Let me rephrase my 2nd point:

simpleTheme() has a specific list of **arguments** which is nothing
like you show. As the Help says, its value is a list of parameter
settings that can be used by par.settings.

-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 2, 2015 at 11:46 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 1. box.rectangle, etc. are not functions;  they are (lists of)
> parameters that are sublists of the par.settings list.
>
> 2. Read the Help for simpleTheme() -- it has a specific list of
> parameters which is nothing like what you show.
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Jul 2, 2015 at 11:04 AM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>> On Thu, 2 Jul 2015, Rich Shepard wrote:
>>
>>> ... and invoking R within emacs ...
>>
>>
>>   Trying a different approach:
>>
>> bwplot(quant ~ param, data = b, main = 'Stream B Constituents', ylab =
>> 'Concentration (mg/L)', xlab = 'Constituent', par.settings =
>> simpleTheme(box.rectangle(col = 'black'), box.umbrella(col='black'),
>> box.dot(col= 'red')))
>> Error in setValue(col, "col", 1:6) :
>>   could not find function "box.rectangle"
>>
>> I'm stymied,
>>
>>
>> Rich
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From lid.zigh at gmail.com  Thu Jul  2 18:48:12 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Thu, 2 Jul 2015 11:48:12 -0500
Subject: [R] question
In-Reply-To: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
Message-ID: <CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>

Thank you so much for replying me!
for better understanding my problem, I explain my problem more:

I have a 682*1 matrix called "met" , the first 5 rows similar below:

>  rownames(met)[1:5]

[1]  "glycine_imp"
[2]  "Nacetylglycine_imp"
[3]  "sarcosine_imp"
[4]  "dimethylglycine_imp"
[5]  "betaine_imp"

and I have a function in R that each time use one of the row names of "met"
matrix and create a new object file and I should save the objects!

my function is  "
Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
" that each time just I should change the met[i] and replace by row names
"met" one by one and for each of them I should rename the function and
after that I should save each object!
for example for first row of "met" I have

>   prep1<- Scores(Z=metalofGT,formula="glycine_imp~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
#creat the object file for first row and called prep1###

>   save(prep1, file="prep1.RData", compress="bzip2")      ##save the
object file as "prep1.RData"#####

I should do this process for 682 row names of "met" matrix and at the end I
should have    "prep1.RData"  ,   "prep2.RData"   , "prep3.RData"

so, would you please help me how to do it?

Many Thanks,
Ati

On Wed, Jul 1, 2015 at 1:07 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:

> I have 682 variables in a data frame , and a function that  I should feed
> 682 variables in this function one by one and each time save the file as a
> special name!
> for emaple:
> my data frame file includes 682 names :
> 1  aaa
> 2  bbb
> 3  dfdsfg
> 4 fghh
> .
>
> 682 fgfhg
> and a function like prep(Z, aaa, .....) and each time I should change the
> variable name in this function and read the variable from the data frame
> and each time I should save the file as a special name such as:
>
> prep1<- prep(z, aaa,...)
> prep2<- prep(z, bbb,...)
> prep3<- prep(z, dfdsfg,..)
> Prep4<- prep(z, fghh,...)
>
> How can I use loop function in R to that?
>
> Thanks
>

	[[alternative HTML version deleted]]


From karraspito at yahoo.es  Thu Jul  2 19:14:16 2015
From: karraspito at yahoo.es (Iker Vaquero Alba)
Date: Thu, 2 Jul 2015 17:14:16 +0000 (UTC)
Subject: [R] no slot of name "fixef" for this object of class "lmerMod"
Message-ID: <280816082.1981016.1435857256421.JavaMail.yahoo@mail.yahoo.com>

?? Hello everyone.
?? I am trying to re-analyse some data with an R function I last used in 2011. Everything seemed to work fine then, but now, using the same code, it gives me this error:
?? Error in R.pe(y, group1, group2, returnR = FALSE) : 
? no slot of name "fixef" for this object of class "lmerMod"
?? This is the part of the function that I think is relevant for the problem:
?? # preparation
??? #n <- rowSums(y)
??? N <- length(y)
??? k <- length(unique(group1)) # clone
??? g <- length(unique(group2)) # round
??? #
??? require(lme4)
??? # functions
??? R.pe <- function(y, group1, group2, returnR=TRUE) { 
??? ?mod <- lmer(y ~ 1 + (1|group1)+(1|group2),verbose=FALSE) 
??? ??? VarComp? <- lme4::VarCorr(mod)
??? ??? beta0??? <- as.numeric(mod at fixef)
??? ??? var.e <- attr(VarComp, "sc")^2 # residual variance
??? ??? var.a1 <- (as.numeric(VarComp[1])) # e.g. get clone R
??? ??? var.a2 <- (as.numeric(VarComp[2])) # e.g. get round R
??? ??? ??? R.group1? <- var.a1/(var.a1+var.e) # clone level
????????????????? R.group2? <- var.a2/(var.a2+var.e)
????????????????? R.groupr? <- var.a1/(var.a1+var.e+var.a2)
??? ??? if(returnR) return(list(R.group1=R.group1,R.group2=R.group2,R.groupr=R.groupr))
??? ??? else return(list(beta0=beta0, var.e=var.e, var.a1=var.a1, var.a2=var.a2)) 
??? }


?? I would appreciate any help about this. Let me know if you need more code, the function is obviously longer.
?? Thank you very much in advance.
?? Iker
__________________________________________________________________

?? Dr. Iker Vaquero-Alba
 ?? Daphne du Maurier
 ?? Centre for Ecology and Conservation
 ?? College of Life and Environmental Sciences
 ?? University of Exeter, Cornwall Campus
 ?? TR10 9FE 
 ?? Penryn
 ?? U.K.

 ?? http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/

 ?? https://eric.exeter.ac.uk/repository/handle/10036/3381


	[[alternative HTML version deleted]]


From rmh at temple.edu  Fri Jul  3 00:29:55 2015
From: rmh at temple.edu (Richard M. Heiberger)
Date: Thu, 2 Jul 2015 18:29:55 -0400
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
Message-ID: <CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>

There is a precedence error in your R attempt.  You need to convert
163 to 120 bits first, before taking
its square root.

> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
1 'mpfr' number of precision  120   bits
[1] 262537412640768333.51635812597335712954

## just the last four characters to the left of the decimal point.
> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
> tmp-tmp[2]
     baseR    Wolfram      Rmpfr wrongRmpfr
      -488          0       -411       -907
>

You didn't give the Wolfram alpha code you used.  There is no way of
verifying the correct value from your email.
Please check that you didn't have a similar precedence error in that code.

Rich


On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help
<r-help at r-project.org> wrote:
>
> Ravi
>
> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>
> Aditya
>
>
>
> ------------------------------
> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>
>>Hi,
>>
>>Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>
>>If I compute this using the Wolfram alpha engine, I get:
>>262537412640768743.99999999999925007259719818568887935385...
>>
>>When I do this in R 3.1.1 (64-bit windows), I get:
>>262537412640768256.0000
>>
>>The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>
>>In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>
>>library(Rmpfr)
>>
>>
>>> exp(sqrt(163) * mpfr(pi, 120))
>>
>>1 'mpfr' number of precision  120   bits
>>
>>[1] 262537412640767837.08771354274620169031
>>
>>The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>
>>Thank you,
>>Ravi
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmh at temple.edu  Fri Jul  3 00:51:55 2015
From: rmh at temple.edu (Richard M. Heiberger)
Date: Fri, 3 Jul 2015 00:51:55 +0200
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <14ad39aaf6a542849bbf3f62a0c2f38f@DOM-EB1-2013.win.ad.jhu.edu>
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
	<CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>
	<14ad39aaf6a542849bbf3f62a0c2f38f@DOM-EB1-2013.win.ad.jhu.edu>
Message-ID: <CAGx1TMCjMhijsrY7k-y+_e6Gn5Z06na6Mao1zvz+P0u_ptCh9g@mail.gmail.com>

precedence does matter in this example.  the square
root was taken of a doubleprecision (53 bit) number.  my revision
takes the square root of a 120 bit number.

> sqrt(mpfr(pi, 120))
1 'mpfr' number of precision  120   bits
[1] 1.7724538509055159927515191031392484397
> mpfr(sqrt(pi), 120)
1 'mpfr' number of precision  120   bits
[1] 1.772453850905515881919427556567825377
> print(sqrt(pi), digits=20)
[1] 1.7724538509055158819
>

Sent from my iPhone

> On Jul 3, 2015, at 00:38, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
>
> Hi Rich,
>
> The Wolfram answer is correct.
> http://mathworld.wolfram.com/RamanujanConstant.html
>
> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
> http://www.wolframalpha.com/
>
> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>
> Thanks,
> Ravi
>
> -----Original Message-----
> From: Richard M. Heiberger [mailto:rmh at temple.edu]
> Sent: Thursday, July 02, 2015 6:30 PM
> To: Aditya Singh
> Cc: Ravi Varadhan; r-help
> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>
> There is a precedence error in your R attempt.  You need to convert
> 163 to 120 bits first, before taking
> its square root.
>
>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
> 1 'mpfr' number of precision  120   bits
> [1] 262537412640768333.51635812597335712954
>
> ## just the last four characters to the left of the decimal point.
>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>> tmp-tmp[2]
>     baseR    Wolfram      Rmpfr wrongRmpfr
>      -488          0       -411       -907
>
> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
> Please check that you didn't have a similar precedence error in that code.
>
> Rich
>
>
>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>
>> Ravi
>>
>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>
>> Aditya
>>
>>
>>
>> ------------------------------
>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>
>>> Hi,
>>>
>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>
>>> If I compute this using the Wolfram alpha engine, I get:
>>> 262537412640768743.99999999999925007259719818568887935385...
>>>
>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>> 262537412640768256.0000
>>>
>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>
>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>
>>> library(Rmpfr)
>>>
>>>
>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>
>>> 1 'mpfr' number of precision  120   bits
>>>
>>> [1] 262537412640767837.08771354274620169031
>>>
>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>
>>> Thank you,
>>> Ravi
>>>
>>>
>>>
>>>      [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Fri Jul  3 01:13:20 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 02 Jul 2015 16:13:20 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
In-Reply-To: <CAGx1TMCjMhijsrY7k-y+_e6Gn5Z06na6Mao1zvz+P0u_ptCh9g@mail.gmail.com>
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
	<CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>
	<14ad39aaf6a542849bbf3f62a0c2f38f@DOM-EB1-2013.win.ad.jhu.edu>
	<CAGx1TMCjMhijsrY7k-y+_e6Gn5Z06na6Mao1zvz+P0u_ptCh9g@mail.gmail.com>
Message-ID: <FDB6AA0E-A814-49AE-8D0A-27EA5A46C93F@dcn.davis.CA.us>

But not 120 bits of pi... just 120 bits of the double precision version of pi.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 2, 2015 3:51:55 PM PDT, "Richard M. Heiberger" <rmh at temple.edu> wrote:
>precedence does matter in this example.  the square
>root was taken of a doubleprecision (53 bit) number.  my revision
>takes the square root of a 120 bit number.
>
>> sqrt(mpfr(pi, 120))
>1 'mpfr' number of precision  120   bits
>[1] 1.7724538509055159927515191031392484397
>> mpfr(sqrt(pi), 120)
>1 'mpfr' number of precision  120   bits
>[1] 1.772453850905515881919427556567825377
>> print(sqrt(pi), digits=20)
>[1] 1.7724538509055158819
>>
>
>Sent from my iPhone
>
>> On Jul 3, 2015, at 00:38, Ravi Varadhan <ravi.varadhan at jhu.edu>
>wrote:
>>
>> Hi Rich,
>>
>> The Wolfram answer is correct.
>> http://mathworld.wolfram.com/RamanujanConstant.html
>>
>> There is no code for Wolfram alpha.  You just go to their web engine
>and plug in the expression and it will give you the answer.
>> http://www.wolframalpha.com/
>>
>> I am not sure that the precedence matters in Rmpfr.  Even if it does,
>the answer you get is still wrong as you showed.
>>
>> Thanks,
>> Ravi
>>
>> -----Original Message-----
>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>> Sent: Thursday, July 02, 2015 6:30 PM
>> To: Aditya Singh
>> Cc: Ravi Varadhan; r-help
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>computations - using Rmpfr in R
>>
>> There is a precedence error in your R attempt.  You need to convert
>> 163 to 120 bits first, before taking
>> its square root.
>>
>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>> 1 'mpfr' number of precision  120   bits
>> [1] 262537412640768333.51635812597335712954
>>
>> ## just the last four characters to the left of the decimal point.
>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>> tmp-tmp[2]
>>     baseR    Wolfram      Rmpfr wrongRmpfr
>>      -488          0       -411       -907
>>
>> You didn't give the Wolfram alpha code you used.  There is no way of
>verifying the correct value from your email.
>> Please check that you didn't have a similar precedence error in that
>code.
>>
>> Rich
>>
>>
>>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help
><r-help at r-project.org> wrote:
>>>
>>> Ravi
>>>
>>> I am a chemical engineer by training. Is there not something like
>law of corresponding states in numerical analysis?
>>>
>>> Aditya
>>>
>>>
>>>
>>> ------------------------------
>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>
>>>> Hi,
>>>>
>>>> Ramanujan supposedly discovered that the number, 163, has this
>interesting property that exp(sqrt(163)*pi), which is obviously a
>transcendental number, is real close to an integer (close to 10^(-12)).
>>>>
>>>> If I compute this using the Wolfram alpha engine, I get:
>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>
>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>> 262537412640768256.0000
>>>>
>>>> The absolute error between the exact and R's value is 488, with a
>relative error of about 1.9x10^(-15).
>>>>
>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr"
>but I am unable to get accurate results:
>>>>
>>>> library(Rmpfr)
>>>>
>>>>
>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>
>>>> 1 'mpfr' number of precision  120   bits
>>>>
>>>> [1] 262537412640767837.08771354274620169031
>>>>
>>>> The above answer is not only inaccurate, but it is actually worse
>than the answer using the usual double precision.  Any thoughts as to
>what I am doing wrong?
>>>>
>>>> Thank you,
>>>> Ravi
>>>>
>>>>
>>>>
>>>>      [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From NordlDJ at dshs.wa.gov  Fri Jul  3 02:48:25 2015
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Fri, 3 Jul 2015 00:48:25 +0000
Subject: [R] : Ramanujan and the accuracy of floating point
	computations	- using Rmpfr in R
In-Reply-To: <FDB6AA0E-A814-49AE-8D0A-27EA5A46C93F@dcn.davis.CA.us>
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
	<CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>
	<14ad39aaf6a542849bbf3f62a0c2f38f@DOM-EB1-2013.win.ad.jhu.edu>
	<CAGx1TMCjMhijsrY7k-y+_e6Gn5Z06na6Mao1zvz+P0u_ptCh9g@mail.gmail.com>
	<FDB6AA0E-A814-49AE-8D0A-27EA5A46C93F@dcn.davis.CA.us>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662ED5FA55@WAXMXOLYMB025.WAX.wa.lcl>

Ravi,

Take a look at the following link.  

https://code.google.com/p/r-bc/

I followed the instructions to get a Windows version of the 'nix utility program , bc (a high precision calculator), and the source for an R to bc interface.  After installing them, I executed

exp(sqrt(bc(163))*4*atan(bc(1)))

in R and got this result

"262537412640768743.9999999999992500725971981856888793538563373369908627075374103782106479101186073116295306145602054347"


I don't know if this is helpful, but ...

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jeff Newmiller
Sent: Thursday, July 02, 2015 4:13 PM
To: Richard M. Heiberger; Ravi Varadhan
Cc: r-help
Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R

But not 120 bits of pi... just 120 bits of the double precision version of pi.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------
Sent from my phone. Please excuse my brevity.

On July 2, 2015 3:51:55 PM PDT, "Richard M. Heiberger" <rmh at temple.edu> wrote:
>precedence does matter in this example.  the square root was taken of a 
>doubleprecision (53 bit) number.  my revision takes the square root of 
>a 120 bit number.
>
>> sqrt(mpfr(pi, 120))
>1 'mpfr' number of precision  120   bits
>[1] 1.7724538509055159927515191031392484397
>> mpfr(sqrt(pi), 120)
>1 'mpfr' number of precision  120   bits
>[1] 1.772453850905515881919427556567825377
>> print(sqrt(pi), digits=20)
>[1] 1.7724538509055158819
>>
>
>Sent from my iPhone
>
>> On Jul 3, 2015, at 00:38, Ravi Varadhan <ravi.varadhan at jhu.edu>
>wrote:
>>
>> Hi Rich,
>>
>> The Wolfram answer is correct.
>> http://mathworld.wolfram.com/RamanujanConstant.html
>>
>> There is no code for Wolfram alpha.  You just go to their web engine
>and plug in the expression and it will give you the answer.
>> http://www.wolframalpha.com/
>>
>> I am not sure that the precedence matters in Rmpfr.  Even if it does,
>the answer you get is still wrong as you showed.
>>
>> Thanks,
>> Ravi
>>
>> -----Original Message-----
>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>> Sent: Thursday, July 02, 2015 6:30 PM
>> To: Aditya Singh
>> Cc: Ravi Varadhan; r-help
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>computations - using Rmpfr in R
>>
>> There is a precedence error in your R attempt.  You need to convert
>> 163 to 120 bits first, before taking
>> its square root.
>>
>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>> 1 'mpfr' number of precision  120   bits
>> [1] 262537412640768333.51635812597335712954
>>
>> ## just the last four characters to the left of the decimal point.
>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>> tmp-tmp[2]
>>     baseR    Wolfram      Rmpfr wrongRmpfr
>>      -488          0       -411       -907
>>
>> You didn't give the Wolfram alpha code you used.  There is no way of
>verifying the correct value from your email.
>> Please check that you didn't have a similar precedence error in that
>code.
>>
>> Rich
>>
>>
>>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help
><r-help at r-project.org> wrote:
>>>
>>> Ravi
>>>
>>> I am a chemical engineer by training. Is there not something like
>law of corresponding states in numerical analysis?
>>>
>>> Aditya
>>>
>>>
>>>
>>> ------------------------------
>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>
>>>> Hi,
>>>>
>>>> Ramanujan supposedly discovered that the number, 163, has this
>interesting property that exp(sqrt(163)*pi), which is obviously a 
>transcendental number, is real close to an integer (close to 10^(-12)).
>>>>
>>>> If I compute this using the Wolfram alpha engine, I get:
>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>
>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>> 262537412640768256.0000
>>>>
>>>> The absolute error between the exact and R's value is 488, with a
>relative error of about 1.9x10^(-15).
>>>>
>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr"
>but I am unable to get accurate results:
>>>>
>>>> library(Rmpfr)
>>>>
>>>>
>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>
>>>> 1 'mpfr' number of precision  120   bits
>>>>
>>>> [1] 262537412640767837.08771354274620169031
>>>>
>>>> The above answer is not only inaccurate, but it is actually worse
>than the answer using the usual double precision.  Any thoughts as to 
>what I am doing wrong?
>>>>
>>>> Thank you,
>>>> Ravi
>>>>
>>>>
>>>>
>>>>      [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Fri Jul  3 03:49:55 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Fri, 3 Jul 2015 11:49:55 +1000
Subject: [R] Lattice: set col = "black" for box.rectangle and
	box.umbrella
In-Reply-To: <alpine.LNX.2.11.1507021150100.12452@localhost>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>	<alpine.LNX.2.11.1507020954360.9672@localhost>	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>	<alpine.LNX.2.11.1507021036250.9672@localhost>	<alpine.LNX.2.11.1507021101040.9672@localhost>	<CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
	<alpine.LNX.2.11.1507021150100.12452@localhost>
Message-ID: <000301d0b532$904a11f0$b0de35d0$@bigpond.com>

Hi

I usually avoid global settings as they may change between graphs therefore
I use the par.settings in each plot

My settings are

       par.settings = list(fontsize = list(text = 10.5,
                                           points = 8),
                           strip.background = list(col = "transparent"),
                           box.dot = list(col = "#FF0000", # red
                                          pch = "|"),
                           box.rectangle = list(col = "#000000",
                                                lty = 1),
                           box.umbrella = list(col = "#000000",
                                               lty = 1),
                           plot.symbol = list(alpha = 1,
                                              col = "#000000",
                                              cex = 0.7,
                                              pch = 20)
                  ),

This gives a red line instead of the large dot: it avoids "bulls-eyes"
across the panel when there are a lot of factors

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Rich Shepard
Sent: Friday, 3 July 2015 04:52
To: r-help at r-project.org
Subject: Re: [R] Lattice: set col = "black" for box.rectangle and
box.umbrella

On Thu, 2 Jul 2015, Bert Gunter wrote:

> 1. box.rectangle, etc. are not functions;  they are (lists of)
> parameters that are sublists of the par.settings list.

Bert,

   Thought so ...

> 2. Read the Help for simpleTheme() -- it has a specific list of
> parameters which is nothing like what you show.

   found the simpleTheme example on a web forum thread. Mea culpa! Will read
the help page. Still desire to have the changes set in .Rprofile rather than
for each plot.

Thanks,

Rich

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From tcmuigai at gmail.com  Fri Jul  3 07:23:28 2015
From: tcmuigai at gmail.com (Charles Thuo)
Date: Fri, 3 Jul 2015 08:23:28 +0300
Subject: [R] Removing rows in a data frame
Message-ID: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>

I have a data frame whose rows are 678013 . I would like to  remove rows
from 30696 to 678013 and then attach a new column with a length of 30595.


I tried

Y<- X[-30595:678013,] and its not working

In addition how do i add a new column

Kindly assist.

Charles

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Jul  3 07:44:03 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 2 Jul 2015 22:44:03 -0700
Subject: [R] Removing rows in a data frame
In-Reply-To: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>
References: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>
Message-ID: <CAGxFJbT-pbeJpJRJ1z-Ea9ZyckrJU0+QrsP6O7K6XQ3BJ4+jqQ@mail.gmail.com>

?precedence

-5:10 is (-5):10

-- Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 2, 2015 at 10:23 PM, Charles Thuo <tcmuigai at gmail.com> wrote:
> I have a data frame whose rows are 678013 . I would like to  remove rows
> from 30696 to 678013 and then attach a new column with a length of 30595.
>
>
> I tried
>
> Y<- X[-30595:678013,] and its not working
>
> In addition how do i add a new column
>
> Kindly assist.
>
> Charles
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From loris.bennett at fu-berlin.de  Fri Jul  3 09:21:02 2015
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Fri, 3 Jul 2015 09:21:02 +0200
Subject: [R] igraph plot slowness
Message-ID: <87h9plogf5.fsf@hornfels.zedat.fu-berlin.de>

Hi,

With the following data

ibcore01	ibswitch01
ibcore01	ibswitch02
ibcore01	ibswitch03
ibcore02	ibswitch01
ibcore02	ibswitch02
ibcore02	ibswitch03
ibswitch01	node001
ibswitch01	node002
ibswitch01	node003
ibswitch02	node004
ibswitch02	node005
ibswitch02	node006
ibswitch03	node007
ibswitch03	node008
ibswitch03	node009

in the file "topology.txt"

and the following code:

library("igraph")
topo_data <- read.csv(file="topology.txt",head=FALSE,sep="\t")
network_data <-graph.data.frame(topo_data, directed=F)
plot(network_data)

it takes about 5 seconds for the plot to be drawn with R 3.2.0 on a
12-core 2.67 GHz Xeon X5650 server with no other CPU-intensive processes
running.

This strikes me as rather slow, particularly as my full network has over
120 components and the plot takes around 50 seconds.

Am I doing anything wrong?

(I am working over an ssh connection with X forwarding, but plotting to
a PDF file on the server does not seem to be faster.)

Cheers,

Loris

-- 
This signature is currently under construction.


From rainer.schuermann at gmx.net  Fri Jul  3 09:55:42 2015
From: rainer.schuermann at gmx.net (Rainer Schuermann)
Date: Fri, 3 Jul 2015 09:55:42 +0200
Subject: [R] Removing rows in a data frame
In-Reply-To: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>
References: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>
Message-ID: <201507030955.42992.rainer.schuermann@gmx.net>

Try

y <- x[ -( 30596:678013 ), ]

Please note that I have replaced 30595 with 30596 which is I think what you mean.

You can add a new column with

y$new <- new_column   # this is your vector of length 30595

Good luck,
Rainer


On Friday 03 July 2015 07:23:28 Charles Thuo wrote:
> I have a data frame whose rows are 678013 . I would like to  remove rows
> from 30696 to 678013 and then attach a new column with a length of 30595.
> 
> 
> I tried
> 
> Y<- X[-30595:678013,] and its not working
> 
> In addition how do i add a new column
> 
> Kindly assist.
> 
> Charles
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From sergio.fonda99 at gmail.com  Fri Jul  3 10:09:31 2015
From: sergio.fonda99 at gmail.com (Sergio Fonda)
Date: Fri, 3 Jul 2015 10:09:31 +0200
Subject: [R] Removing rows in a data frame
In-Reply-To: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>
References: <CAAJc=rOD4fd9KVPBAZowV-K-g=bWgPHTVyjXh4rCavQYsZ=Gsw@mail.gmail.com>
Message-ID: <CAJRuHormwi97zqovckY+SgYEtD2R8RewTvgcR5-TmEYCCboHFw@mail.gmail.com>

In my experience package "dplyr" has all functions to deal with this kind
of problems in a simple and compact way
Sergio
Il 03/lug/2015 07:26, "Charles Thuo" <tcmuigai at gmail.com> ha scritto:

> I have a data frame whose rows are 678013 . I would like to  remove rows
> from 30696 to 678013 and then attach a new column with a length of 30595.
>
>
> I tried
>
> Y<- X[-30595:678013,] and its not working
>
> In addition how do i add a new column
>
> Kindly assist.
>
> Charles
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From federico.calboli at helsinki.fi  Fri Jul  3 10:09:54 2015
From: federico.calboli at helsinki.fi (Federico Calboli)
Date: Fri, 3 Jul 2015 11:09:54 +0300
Subject: [R] what constitutes a 'complete sentence'?
Message-ID: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>

Hi All,

I am upgrading a package for CRAN, and I get this note:

checking DESCRIPTION meta-information ... NOTE
Malformed Description field: should contain one or more complete sentences.

This is puzzling because:

cat DESCRIPTION

...
Description: Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.
...

In my understanding "Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.? *is* a complete sentence.  So, what is complete sentence in the opinion of whomever coded that check?

Best

F

--
Federico Calboli
Ecological Genetics Research Unit
Department of Biosciences
PO Box 65 (Biocenter 3, Viikinkaari 1)
FIN-00014 University of Helsinki
Finland

federico.calboli at helsinki.fi


From ravi.varadhan at jhu.edu  Fri Jul  3 00:38:45 2015
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Thu, 2 Jul 2015 22:38:45 +0000
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
	<CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>
Message-ID: <14ad39aaf6a542849bbf3f62a0c2f38f@DOM-EB1-2013.win.ad.jhu.edu>

Hi Rich,

The Wolfram answer is correct.  
http://mathworld.wolfram.com/RamanujanConstant.html 

There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
http://www.wolframalpha.com/ 

I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.

Thanks,
Ravi

-----Original Message-----
From: Richard M. Heiberger [mailto:rmh at temple.edu] 
Sent: Thursday, July 02, 2015 6:30 PM
To: Aditya Singh
Cc: Ravi Varadhan; r-help
Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R

There is a precedence error in your R attempt.  You need to convert
163 to 120 bits first, before taking
its square root.

> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
1 'mpfr' number of precision  120   bits
[1] 262537412640768333.51635812597335712954

## just the last four characters to the left of the decimal point.
> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837) 
> tmp-tmp[2]
     baseR    Wolfram      Rmpfr wrongRmpfr
      -488          0       -411       -907
>

You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
Please check that you didn't have a similar precedence error in that code.

Rich


On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>
> Ravi
>
> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>
> Aditya
>
>
>
> ------------------------------
> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>
>>Hi,
>>
>>Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>
>>If I compute this using the Wolfram alpha engine, I get:
>>262537412640768743.99999999999925007259719818568887935385...
>>
>>When I do this in R 3.1.1 (64-bit windows), I get:
>>262537412640768256.0000
>>
>>The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>
>>In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>
>>library(Rmpfr)
>>
>>
>>> exp(sqrt(163) * mpfr(pi, 120))
>>
>>1 'mpfr' number of precision  120   bits
>>
>>[1] 262537412640767837.08771354274620169031
>>
>>The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>
>>Thank you,
>>Ravi
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide 
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From petr.pikal at precheza.cz  Fri Jul  3 11:14:16 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 3 Jul 2015 09:14:16 +0000
Subject: [R] question
In-Reply-To: <CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
	<CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C335D1@SRVEXCHMBX.precheza.cz>

Hi

I cannot test it in absence of data. Anyway I would not use saving single objects but use list instead.

Something like

lll <- vector(mode="list", nrow(met))

for (i in 1:nrow(met)) {
form <- as.formula(paste(met[i], "~ egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER"))
lll[i] <- Scores(Z=metalofGT,formula=form)
}

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Lida
> Zeighami
> Sent: Thursday, July 02, 2015 6:48 PM
> To: r-help at r-project.org
> Subject: Re: [R] question
>
> Thank you so much for replying me!
> for better understanding my problem, I explain my problem more:
>
> I have a 682*1 matrix called "met" , the first 5 rows similar below:
>
> >  rownames(met)[1:5]
>
> [1]  "glycine_imp"
> [2]  "Nacetylglycine_imp"
> [3]  "sarcosine_imp"
> [4]  "dimethylglycine_imp"
> [5]  "betaine_imp"
>
> and I have a function in R that each time use one of the row names of
> "met"
> matrix and create a new object file and I should save the objects!
>
> my function is  "
> Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+G
> ENDER")
> " that each time just I should change the met[i] and replace by row
> names
> "met" one by one and for each of them I should rename the function and
> after that I should save each object!
> for example for first row of "met" I have
>
> >   prep1<-
> Scores(Z=metalofGT,formula="glycine_imp~egfr_v1_ckdepi+pc1+pc2+pc3+V1AG
> E01+GENDER")
> #creat the object file for first row and called prep1###
>
> >   save(prep1, file="prep1.RData", compress="bzip2")      ##save the
> object file as "prep1.RData"#####
>
> I should do this process for 682 row names of "met" matrix and at the
> end I
> should have    "prep1.RData"  ,   "prep2.RData"   , "prep3.RData"
>
> so, would you please help me how to do it?
>
> Many Thanks,
> Ati
>
> On Wed, Jul 1, 2015 at 1:07 PM, Lida Zeighami <lid.zigh at gmail.com>
> wrote:
>
> > I have 682 variables in a data frame , and a function that  I should
> feed
> > 682 variables in this function one by one and each time save the file
> as a
> > special name!
> > for emaple:
> > my data frame file includes 682 names :
> > 1  aaa
> > 2  bbb
> > 3  dfdsfg
> > 4 fghh
> > .
> >
> > 682 fgfhg
> > and a function like prep(Z, aaa, .....) and each time I should change
> the
> > variable name in this function and read the variable from the data
> frame
> > and each time I should save the file as a special name such as:
> >
> > prep1<- prep(z, aaa,...)
> > prep2<- prep(z, bbb,...)
> > prep3<- prep(z, dfdsfg,..)
> > Prep4<- prep(z, fghh,...)
> >
> > How can I use loop function in R to that?
> >
> > Thanks
> >
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From h.wickham at gmail.com  Fri Jul  3 11:14:06 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Fri, 3 Jul 2015 11:14:06 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
Message-ID: <CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>

It might be a line break problem - I think you want:

Description: Functions designed to test for single gene/phenotype
association and
    for pleiotropy on genetic and genomic data.

Hadley

On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
<federico.calboli at helsinki.fi> wrote:
> Hi All,
>
> I am upgrading a package for CRAN, and I get this note:
>
> checking DESCRIPTION meta-information ... NOTE
> Malformed Description field: should contain one or more complete sentences.
>
> This is puzzling because:
>
> cat DESCRIPTION
>
> ...
> Description: Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.
> ...
>
> In my understanding "Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.? *is* a complete sentence.  So, what is complete sentence in the opinion of whomever coded that check?
>
> Best
>
> F
>
> --
> Federico Calboli
> Ecological Genetics Research Unit
> Department of Biosciences
> PO Box 65 (Biocenter 3, Viikinkaari 1)
> FIN-00014 University of Helsinki
> Finland
>
> federico.calboli at helsinki.fi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://had.co.nz/


From p_connolly at slingshot.co.nz  Fri Jul  3 11:17:49 2015
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Fri, 3 Jul 2015 21:17:49 +1200
Subject: [R] Getting predictions for skewed t-distribution
Message-ID: <20150703091749.GA3496@slingshot.co.nz>

I have a small dataframe xxF, a summary of which looks like this:

> summary(xxF)
       T              Dev          
 Min.   :10.44   Min.   :0.008929  
 1st Qu.:10.44   1st Qu.:0.012048  
 Median :18.61   Median :0.031250  
 Mean   :17.87   Mean   :0.028286  
 3rd Qu.:22.24   3rd Qu.:0.041667  
 Max.   :30.37   Max.   :0.050000  


I managed to make a non-linear fit after a lot of fiddling with
initial values but it looks overly complicated and biologically
unconvincing in part.  The general form of a skewed t-distribution
looks more appropriate so I tried selm from the sn package thus:


>  selmFt <- with(xxF, selm(Dev ~ T, family = "ST", method="MPLE"))
>  coef(selmFt, param.type="DP")
(Intercept.DP)              T          omega          alpha             nu 
  -0.015895099    0.002689226    0.002306132   -5.660870446    1.473210455 

I wish to get predictions for values of T between 10 and 32 but I can't
figure out how to use those coefficients.

With an linear model or glm, even without a prediction method, it's
fairly simple to get predictions from a range of values of the
independent variable/s.  For a skewed-t it's evidently less
straightforward.  Does it have to be done using CP type parameters?


Ideas gratefully accepted



In case it makes any difference....

> sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: i686-pc-linux-gnu (32-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats4    grDevices utils     stats     graphics  methods   base     

other attached packages:
[1] dplyr_0.3.0.2      nlmrt_2013-9.25    RColorBrewer_1.1-2 plyr_1.8.3        
[5] stringr_1.0.0      reshape2_1.4.1     sn_1.2-2           lattice_0.20-31   

loaded via a namespace (and not attached):
 [1] Rcpp_0.11.3       assertthat_0.1    grid_3.2.1        DBI_0.3.1        
 [5] magrittr_1.0.1    stringi_0.4-1     lazyeval_0.1.10   tools_3.2.1      
 [9] numDeriv_2014.2-1 parallel_3.2.1    mnormt_1.5-3 


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From drjimlemon at gmail.com  Fri Jul  3 11:39:53 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 3 Jul 2015 19:39:53 +1000
Subject: [R] question
In-Reply-To: <CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
	<CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
Message-ID: <CA+8X3fXsk0DMdShf2TQMzye0Xp6eYQ0-281WrQVpA0RL2_e=vA@mail.gmail.com>

Hi Ati,
Let's start from the top and see where we finish up. I'll use a
somewhat smaller matrix:

met<-matrix(runif(5),ncol=1)
rownames(met)<-c("glycine_imp","Nacetylglycine_imp","sarcosine_imp",
 "dimethylglycine_imp","betaine_imp")
met
                          [,1]
glycine_imp         0.61532855
Nacetylglycine_imp  0.04294675
sarcosine_imp       0.98840385
dimethylglycine_imp 0.00507230
betaine_imp         0.68528107

In your example, I think you are mixing up the names and the values of
"met". I suspect that you want to use the values for the computation
and the names for the filenames. Also, a one column matrix will act
very much like a vector, but there are a few problems if you treat it
like one. For instance, if you extract a value as though met is a
vector:

met[2]
[1] 0.04294675

you just get the value, not the rowname. Using both indices extracts
both the value and the name.

met[2,1]
Nacetylglycine_imp
        0.04294675

As I don't have any idea what the other values in your formula are, I
will take the liberty of giving them some:

metalofGT<-100
egfr_v1_ckdepi<-1.2
pc1<-2.3
pc2<-3.4
pc3<-4.5
V1AGE01<-27
GENDER<-"F"

Before embarking on the loop, I think you have confused the "name of
the function", which is "Score", with the return value of the
function. I don't think you want to change the function's name or it
won't work unless you change the name in the function call. Therefore,
I am going to make a blind guess and assume that you want the name of
the return value of the function to be the rowname for the value that
is used in the function.

Now you can do something like this:

for(i in 1:nrow(met)) {
 x<-Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
 names(x)<-names(met)[i]
 filename<-paste("prep",i,".Rdata",sep="")
 save(x, file=filename, compress="bzip2")
}

This will produce five files with the names you requested, each
containing whatever value the function "Score" produces. The name of
that value will be the rowname of "met" that produced it.

Jim


On Fri, Jul 3, 2015 at 2:48 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> Thank you so much for replying me!
> for better understanding my problem, I explain my problem more:
>
> I have a 682*1 matrix called "met" , the first 5 rows similar below:
>
>>  rownames(met)[1:5]
>
> [1]  "glycine_imp"
> [2]  "Nacetylglycine_imp"
> [3]  "sarcosine_imp"
> [4]  "dimethylglycine_imp"
> [5]  "betaine_imp"
>
> and I have a function in R that each time use one of the row names of "met"
> matrix and create a new object file and I should save the objects!
>
> my function is  "
> Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> " that each time just I should change the met[i] and replace by row names
> "met" one by one and for each of them I should rename the function and
> after that I should save each object!
> for example for first row of "met" I have
>
>>   prep1<- Scores(Z=metalofGT,formula="glycine_imp~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> #creat the object file for first row and called prep1###
>
>>   save(prep1, file="prep1.RData", compress="bzip2")      ##save the
> object file as "prep1.RData"#####
>
> I should do this process for 682 row names of "met" matrix and at the end I
> should have    "prep1.RData"  ,   "prep2.RData"   , "prep3.RData"
>
> so, would you please help me how to do it?
>
> Many Thanks,
> Ati
>
> On Wed, Jul 1, 2015 at 1:07 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:
>
>> I have 682 variables in a data frame , and a function that  I should feed
>> 682 variables in this function one by one and each time save the file as a
>> special name!
>> for emaple:
>> my data frame file includes 682 names :
>> 1  aaa
>> 2  bbb
>> 3  dfdsfg
>> 4 fghh
>> .
>>
>> 682 fgfhg
>> and a function like prep(Z, aaa, .....) and each time I should change the
>> variable name in this function and read the variable from the data frame
>> and each time I should save the file as a special name such as:
>>
>> prep1<- prep(z, aaa,...)
>> prep2<- prep(z, bbb,...)
>> prep3<- prep(z, dfdsfg,..)
>> Prep4<- prep(z, fghh,...)
>>
>> How can I use loop function in R to that?
>>
>> Thanks
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thierry.onkelinx at inbo.be  Fri Jul  3 12:00:47 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 3 Jul 2015 12:00:47 +0200
Subject: [R] no slot of name "fixef" for this object of class "lmerMod"
In-Reply-To: <280816082.1981016.1435857256421.JavaMail.yahoo@mail.yahoo.com>
References: <280816082.1981016.1435857256421.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAJuCY5wyvLVzOEv=tofQTSzZR2E5jeeqLFAFg66oVNbzRHjKHA@mail.gmail.com>

Dear Iker,

The internals of the mer model have changed. Use fixef(your.model).

Best regards,

?
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
Op 2 jul. 2015 22:17 schreef "Iker Vaquero Alba" <karraspito at yahoo.es>:

>    Hello everyone.
>    I am trying to re-analyse some data with an R function I last used in
> 2011. Everything seemed to work fine then, but now, using the same code, it
> gives me this error:
>    Error in R.pe(y, group1, group2, returnR = FALSE) :
>   no slot of name "fixef" for this object of class "lmerMod"
>    This is the part of the function that I think is relevant for the
> problem:
>    # preparation
>     #n <- rowSums(y)
>     N <- length(y)
>     k <- length(unique(group1)) # clone
>     g <- length(unique(group2)) # round
>     #
>     require(lme4)
>     # functions
>     R.pe <- function(y, group1, group2, returnR=TRUE) {
>      mod <- lmer(y ~ 1 + (1|group1)+(1|group2),verbose=FALSE)
>         VarComp  <- lme4::VarCorr(mod)
>         beta0    <- as.numeric(mod at fixef)
>         var.e <- attr(VarComp, "sc")^2 # residual variance
>         var.a1 <- (as.numeric(VarComp[1])) # e.g. get clone R
>         var.a2 <- (as.numeric(VarComp[2])) # e.g. get round R
>             R.group1  <- var.a1/(var.a1+var.e) # clone level
>                   R.group2  <- var.a2/(var.a2+var.e)
>                   R.groupr  <- var.a1/(var.a1+var.e+var.a2)
>         if(returnR)
> return(list(R.group1=R.group1,R.group2=R.group2,R.groupr=R.groupr))
>         else return(list(beta0=beta0, var.e=var.e, var.a1=var.a1,
> var.a2=var.a2))
>     }
>
>
>    I would appreciate any help about this. Let me know if you need more
> code, the function is obviously longer.
>    Thank you very much in advance.
>    Iker
> __________________________________________________________________
>
>    Dr. Iker Vaquero-Alba
>     Daphne du Maurier
>     Centre for Ecology and Conservation
>     College of Life and Environmental Sciences
>     University of Exeter, Cornwall Campus
>     TR10 9FE
>     Penryn
>     U.K.
>
>
> http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/
>
>     https://eric.exeter.ac.uk/repository/handle/10036/3381
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Fri Jul  3 13:13:33 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Fri, 3 Jul 2015 13:13:33 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
	<55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>
Message-ID: <CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>

In that case, you need to create a minimal reproducible example and make it
publicly available.

Hadley

On Friday, July 3, 2015, Federico Calboli <federico.calboli at helsinki.fi>
wrote:

>
> > On 3 Jul 2015, at 12:14, Hadley Wickham <h.wickham at gmail.com
> <javascript:;>> wrote:
> >
> > It might be a line break problem - I think you want:
> >
> > Description: Functions designed to test for single gene/phenotype
> > association and
> >    for pleiotropy on genetic and genomic data.
>
> Tried this and unfortunately it does not help.
>
> BW
>
> F
>
>
> >
> > Hadley
> >
> > On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
> > <federico.calboli at helsinki.fi <javascript:;>> wrote:
> >> Hi All,
> >>
> >> I am upgrading a package for CRAN, and I get this note:
> >>
> >> checking DESCRIPTION meta-information ... NOTE
> >> Malformed Description field: should contain one or more complete
> sentences.
> >>
> >> This is puzzling because:
> >>
> >> cat DESCRIPTION
> >>
> >> ...
> >> Description: Functions designed to test for single gene/phenotype
> association and for pleiotropy on genetic and genomic data.
> >> ...
> >>
> >> In my understanding "Functions designed to test for single
> gene/phenotype association and for pleiotropy on genetic and genomic data.?
> *is* a complete sentence.  So, what is complete sentence in the opinion of
> whomever coded that check?
> >>
> >> Best
> >>
> >> F
> >>
> >> --
> >> Federico Calboli
> >> Ecological Genetics Research Unit
> >> Department of Biosciences
> >> PO Box 65 (Biocenter 3, Viikinkaari 1)
> >> FIN-00014 University of Helsinki
> >> Finland
> >>
> >> federico.calboli at helsinki.fi <javascript:;>
> >>
> >> ______________________________________________
> >> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > http://had.co.nz/
>
>
> --
> Federico Calboli
> Ecological Genetics Research Unit
> Department of Biosciences
> PO Box 65 (Biocenter 3, Viikinkaari 1)
> FIN-00014 University of Helsinki
> Finland
>
> federico.calboli at helsinki.fi <javascript:;>
>
>
>
>
>
>
>

-- 
http://had.co.nz/

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Jul  3 14:14:10 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 3 Jul 2015 12:14:10 +0000
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
	<55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>
	<CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C336A7@SRVEXCHMBX.precheza.cz>

Hi

without going deep into this matter, what if you remove "/" from your sentence.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Hadley
> Wickham
> Sent: Friday, July 03, 2015 1:14 PM
> To: Federico Calboli
> Cc: R-help
> Subject: Re: [R] what constitutes a 'complete sentence'?
>
> In that case, you need to create a minimal reproducible example and
> make it publicly available.
>
> Hadley
>
> On Friday, July 3, 2015, Federico Calboli
> <federico.calboli at helsinki.fi>
> wrote:
>
> >
> > > On 3 Jul 2015, at 12:14, Hadley Wickham <h.wickham at gmail.com
> > <javascript:;>> wrote:
> > >
> > > It might be a line break problem - I think you want:
> > >
> > > Description: Functions designed to test for single gene/phenotype
> > > association and
> > >    for pleiotropy on genetic and genomic data.
> >
> > Tried this and unfortunately it does not help.
> >
> > BW
> >
> > F
> >
> >
> > >
> > > Hadley
> > >
> > > On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
> > > <federico.calboli at helsinki.fi <javascript:;>> wrote:
> > >> Hi All,
> > >>
> > >> I am upgrading a package for CRAN, and I get this note:
> > >>
> > >> checking DESCRIPTION meta-information ... NOTE Malformed
> > >> Description field: should contain one or more complete
> > sentences.
> > >>
> > >> This is puzzling because:
> > >>
> > >> cat DESCRIPTION
> > >>
> > >> ...
> > >> Description: Functions designed to test for single gene/phenotype
> > association and for pleiotropy on genetic and genomic data.
> > >> ...
> > >>
> > >> In my understanding "Functions designed to test for single
> > gene/phenotype association and for pleiotropy on genetic and genomic
> data.?
> > *is* a complete sentence.  So, what is complete sentence in the
> > opinion of whomever coded that check?
> > >>
> > >> Best
> > >>
> > >> F
> > >>
> > >> --
> > >> Federico Calboli
> > >> Ecological Genetics Research Unit
> > >> Department of Biosciences
> > >> PO Box 65 (Biocenter 3, Viikinkaari 1)
> > >> FIN-00014 University of Helsinki
> > >> Finland
> > >>
> > >> federico.calboli at helsinki.fi <javascript:;>
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
> > >> and
> > more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> > > --
> > > http://had.co.nz/
> >
> >
> > --
> > Federico Calboli
> > Ecological Genetics Research Unit
> > Department of Biosciences
> > PO Box 65 (Biocenter 3, Viikinkaari 1)
> > FIN-00014 University of Helsinki
> > Finland
> >
> > federico.calboli at helsinki.fi <javascript:;>
> >
> >
> >
> >
> >
> >
> >
>
> --
> http://had.co.nz/
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From rshepard at appl-ecosys.com  Fri Jul  3 15:14:09 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 3 Jul 2015 06:14:09 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and
 box.umbrella
In-Reply-To: <000301d0b532$904a11f0$b0de35d0$@bigpond.com>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
	<alpine.LNX.2.11.1507021101040.9672@localhost>
	<CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
	<alpine.LNX.2.11.1507021150100.12452@localhost>
	<000301d0b532$904a11f0$b0de35d0$@bigpond.com>
Message-ID: <alpine.LNX.2.11.1507030610340.20592@localhost>

On Fri, 3 Jul 2015, Duncan Mackay wrote:

> I usually avoid global settings as they may change between graphs therefore
> I use the par.settings in each plot

> My settings are

    ...

Duncan,

   Thank you very much. Sound advice and excellent example.

   I suspect that part of my issues with trying to do this is that Deepayan's
book is now about 8 years past when he wrote it and the lattice package has
evolved quite a bit since then. So, while I read the on-line man pages I did
not see the answers I needed with sufficient clarity.

Regards,

Rich


From r.turner at auckland.ac.nz  Fri Jul  3 15:43:33 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 4 Jul 2015 01:43:33 +1200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
Message-ID: <55969185.5000604@auckland.ac.nz>

On 03/07/15 20:09, Federico Calboli wrote:
> Hi All,
>
> I am upgrading a package for CRAN, and I get this note:
>
> checking DESCRIPTION meta-information ... NOTE Malformed Description
> field: should contain one or more complete sentences.
>
> This is puzzling because:
>
> cat DESCRIPTION
>
> ... Description: Functions designed to test for single gene/phenotype
> association and for pleiotropy on genetic and genomic data. ...
>
> In my understanding "Functions designed to test for single
> gene/phenotype association and for pleiotropy on genetic and genomic
> data.? *is* a complete sentence.  So, what is complete sentence in
> the opinion of whomever coded that check?


If that is your understanding you need to go back to school and learn 
some grammar.  What you have is a noun ("Functions") modified by an 
adjectival clause.  No verb in sight.  Ergo *not* a complete sentence.

OTOH you are probably in good company in not knowing your grammar.  The 
CRAN folks most likely don't know grammar either.  I suspect that they 
*don't* actually demand a complete sentence.  Such a demand would in 
fact be rather pedantic.  Moreover I really can't see how the package 
checker could possibly check for complete sentences.  This would require 
some very sophisticated programming, it seems to me.

If it turns out that you *really* need a complete sentence, you could 
say (for instance):

This package consists of functions designed to test for single 
gene/phenotype association and for pleiotropy on genetic and genomic data.

The foregoing *is* a complete sentence.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jholtman at gmail.com  Fri Jul  3 16:43:25 2015
From: jholtman at gmail.com (jim holtman)
Date: Fri, 3 Jul 2015 10:43:25 -0400
Subject: [R] igraph plot slowness
In-Reply-To: <87h9plogf5.fsf@hornfels.zedat.fu-berlin.de>
References: <87h9plogf5.fsf@hornfels.zedat.fu-berlin.de>
Message-ID: <CAAxdm-5YiwPjVa5m_fZtL-+onGvdoKUP=o99jJ3ZPBCG7ipzZQ@mail.gmail.com>

Here is what it does locally on my PC:

> library("igraph")
>  topo_data <- read.table(text = "ibcore01        ibswitch01
+  ibcore01        ibswitch02
+  ibcore01        ibswitch03
+  ibcore02        ibswitch01
+  ibcore02        ibswitch02
+  ibcore02        ibswitch03
+  ibswitch01      node001
+  ibswitch01      node002
+  ibswitch01      node003
+  ibswitch02      node004
+  ibswitch02      node005
+  ibswitch02      node006
+  ibswitch03      node007
+  ibswitch03      node008
+  ibswitch03      node009" ,head=FALSE)
>  system.time({
+  network_data <-graph.data.frame(topo_data, directed=F)
+  plot(network_data)
+ })
   user  system elapsed
   0.01    0.01    0.03
>
>

Does not seem too slow.  Creating a PDF file takes a little longer:

> library("igraph")
>  topo_data <- read.table(text = "ibcore01        ibswitch01
+  ibcore01        ibswitch02
+  ibcore01        ibswitch03
+  ibcore02        ibswitch01
+  ibcore02        ibswitch02
+  ibcore02        ibswitch03
+  ibswitch01      node001
+  ibswitch01      node002
+  ibswitch01      node003
+  ibswitch02      node004
+  ibswitch02      node005
+  ibswitch02      node006
+  ibswitch03      node007
+  ibswitch03      node008
+  ibswitch03      node009" ,head=FALSE)
>  system.time({
+  network_data <-graph.data.frame(topo_data, directed=F)
+  pdf('test.pdf')
+  plot(network_data)
+  dev.off()
+ })
   user  system elapsed
   0.09    0.00    0.16

The PDF file is attached.  So maybe it is something with your remote
connection.




Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Fri, Jul 3, 2015 at 3:21 AM, Loris Bennett <loris.bennett at fu-berlin.de>
wrote:

> Hi,
>
> With the following data
>
> ibcore01        ibswitch01
> ibcore01        ibswitch02
> ibcore01        ibswitch03
> ibcore02        ibswitch01
> ibcore02        ibswitch02
> ibcore02        ibswitch03
> ibswitch01      node001
> ibswitch01      node002
> ibswitch01      node003
> ibswitch02      node004
> ibswitch02      node005
> ibswitch02      node006
> ibswitch03      node007
> ibswitch03      node008
> ibswitch03      node009
>
> in the file "topology.txt"
>
> and the following code:
>
> library("igraph")
> topo_data <- read.csv(file="topology.txt",head=FALSE,sep="\t")
> network_data <-graph.data.frame(topo_data, directed=F)
> plot(network_data)
>
> it takes about 5 seconds for the plot to be drawn with R 3.2.0 on a
> 12-core 2.67 GHz Xeon X5650 server with no other CPU-intensive processes
> running.
>
> This strikes me as rather slow, particularly as my full network has over
> 120 components and the plot takes around 50 seconds.
>
> Am I doing anything wrong?
>
> (I am working over an ssh connection with X forwarding, but plotting to
> a PDF file on the server does not seem to be faster.)
>
> Cheers,
>
> Loris
>
> --
> This signature is currently under construction.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.pdf
Type: application/pdf
Size: 7445 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150703/23571280/attachment.pdf>

From richard.perry3 at gmail.com  Fri Jul  3 17:06:56 2015
From: richard.perry3 at gmail.com (Richard Perry)
Date: Fri, 3 Jul 2015 16:06:56 +0100
Subject: [R] Variance estimates for survreg vs. lm
Message-ID: <CA+YFvuaAVgkNH2FAT6xywxct+6SBn1J_H56v9HMh2pzDoXwjoA@mail.gmail.com>

I would like help understanding why a survival regression with no censored
data-points does not give the same variance estimates as a linear model
(see code below).

I think it must be something to do with the fact that the variance is an
actual parameter in the survival version via the log(scale), and possibly
that different assumptions are made about the distribution of the variance.
But I really don't know, I'm just guessing.

The reason I ask is because I am moving a process, that has always been
modelled using a linear model, to a survival model (because there are
sometimes a few censored data points). In the past, the censored data
points have been treated as missing which imparts bias. The variance of the
estimates in this process is key, so I need to know why they are changing
in this systematic way?!



library(survival)

ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
ctl.surv <- Surv(ctl)

trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)

lmod <- lm     (ctl      ~ trt                )
smod <- survreg(ctl.surv ~ trt,dist="gaussian")

coef(lmod)
coef(smod) # same

vcov(lmod)
vcov(smod) # smod is smaller

diag(vcov(lmod))     /
diag(vcov(smod))[1:2]  # 1.25 == 0.5*(n/(n-1))

( summary(lmod)$coef [   ,"Std. Error"] /
  summary(smod)$table[1:2,"Std. Error"]   )^2    # 1.25 = 0.5*(n/(n-1))

	[[alternative HTML version deleted]]


From federico.calboli at helsinki.fi  Fri Jul  3 11:28:07 2015
From: federico.calboli at helsinki.fi (Federico Calboli)
Date: Fri, 3 Jul 2015 12:28:07 +0300
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
Message-ID: <55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>


> On 3 Jul 2015, at 12:14, Hadley Wickham <h.wickham at gmail.com> wrote:
> 
> It might be a line break problem - I think you want:
> 
> Description: Functions designed to test for single gene/phenotype
> association and
>    for pleiotropy on genetic and genomic data.

Tried this and unfortunately it does not help.

BW

F


> 
> Hadley
> 
> On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
> <federico.calboli at helsinki.fi> wrote:
>> Hi All,
>> 
>> I am upgrading a package for CRAN, and I get this note:
>> 
>> checking DESCRIPTION meta-information ... NOTE
>> Malformed Description field: should contain one or more complete sentences.
>> 
>> This is puzzling because:
>> 
>> cat DESCRIPTION
>> 
>> ...
>> Description: Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.
>> ...
>> 
>> In my understanding "Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.? *is* a complete sentence.  So, what is complete sentence in the opinion of whomever coded that check?
>> 
>> Best
>> 
>> F
>> 
>> --
>> Federico Calboli
>> Ecological Genetics Research Unit
>> Department of Biosciences
>> PO Box 65 (Biocenter 3, Viikinkaari 1)
>> FIN-00014 University of Helsinki
>> Finland
>> 
>> federico.calboli at helsinki.fi
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> -- 
> http://had.co.nz/


--
Federico Calboli
Ecological Genetics Research Unit
Department of Biosciences
PO Box 65 (Biocenter 3, Viikinkaari 1)
FIN-00014 University of Helsinki
Finland

federico.calboli at helsinki.fi


From ulas at envs.au.dk  Fri Jul  3 12:20:59 2015
From: ulas at envs.au.dk (ulasim77)
Date: Fri, 3 Jul 2015 03:20:59 -0700 (PDT)
Subject: [R] timePlot legend
Message-ID: <1435918859040-4709366.post@n4.nabble.com>

Dear all

I am plotting a time series using time Plot function. All goes well until i
try to modify the legend by taking it from the standard location at the
bottom, to the right side in a vertical way. 

How can i do this?

This is my code:

filename <- sprintf('%s/TS_CO_all.png',folderPLOTS)
y_lab <- sprintf('CO (ug/m3)')
tit <- sprintf('EU Receptors 2010')
png(filename, width = 18 * 360, height = 9 * 360, res = 360, pointsize=240)
timePlot(data2,pollutant=models, group=TRUE, y.relation="same", avg.time
="month", 
         lwd = 3, ylab = y_lab, main = tit) #pLOT THE DATA AS IS
dev.off()

Best regards



--
View this message in context: http://r.789695.n4.nabble.com/timePlot-legend-tp4709366.html
Sent from the R help mailing list archive at Nabble.com.


From federico.calboli at helsinki.fi  Fri Jul  3 13:23:44 2015
From: federico.calboli at helsinki.fi (Federico Calboli)
Date: Fri, 3 Jul 2015 14:23:44 +0300
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
	<55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>
	<CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>
Message-ID: <E9D527E5-4DC4-4E22-A34D-8E83753DA179@helsinki.fi>

That exists already:  last slide here ? it looks like it is a know issue.

BW

F


> On 3 Jul 2015, at 14:13, Hadley Wickham <h.wickham at gmail.com> wrote:
> 
> In that case, you need to create a minimal reproducible example and make it publicly available. 
> 
> Hadley
> 
> On Friday, July 3, 2015, Federico Calboli <federico.calboli at helsinki.fi> wrote:
> 
> > On 3 Jul 2015, at 12:14, Hadley Wickham <h.wickham at gmail.com> wrote:
> >
> > It might be a line break problem - I think you want:
> >
> > Description: Functions designed to test for single gene/phenotype
> > association and
> >    for pleiotropy on genetic and genomic data.
> 
> Tried this and unfortunately it does not help.
> 
> BW
> 
> F
> 
> 
> >
> > Hadley
> >
> > On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
> > <federico.calboli at helsinki.fi> wrote:
> >> Hi All,
> >>
> >> I am upgrading a package for CRAN, and I get this note:
> >>
> >> checking DESCRIPTION meta-information ... NOTE
> >> Malformed Description field: should contain one or more complete sentences.
> >>
> >> This is puzzling because:
> >>
> >> cat DESCRIPTION
> >>
> >> ...
> >> Description: Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.
> >> ...
> >>
> >> In my understanding "Functions designed to test for single gene/phenotype association and for pleiotropy on genetic and genomic data.? *is* a complete sentence.  So, what is complete sentence in the opinion of whomever coded that check?
> >>
> >> Best
> >>
> >> F
> >>
> >> --
> >> Federico Calboli
> >> Ecological Genetics Research Unit
> >> Department of Biosciences
> >> PO Box 65 (Biocenter 3, Viikinkaari 1)
> >> FIN-00014 University of Helsinki
> >> Finland
> >>
> >> federico.calboli at helsinki.fi
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > http://had.co.nz/
> 
> 
> --
> Federico Calboli
> Ecological Genetics Research Unit
> Department of Biosciences
> PO Box 65 (Biocenter 3, Viikinkaari 1)
> FIN-00014 University of Helsinki
> Finland
> 
> federico.calboli at helsinki.fi
> 
> 
> 
> 
> 
> 
> 
> 
> -- 
> http://had.co.nz/


--
Federico Calboli
Ecological Genetics Research Unit
Department of Biosciences
PO Box 65 (Biocenter 3, Viikinkaari 1)
FIN-00014 University of Helsinki
Finland

federico.calboli at helsinki.fi


From karraspito at yahoo.es  Fri Jul  3 13:59:05 2015
From: karraspito at yahoo.es (Iker Vaquero Alba)
Date: Fri, 3 Jul 2015 11:59:05 +0000 (UTC)
Subject: [R] no slot of name "fixef" for this object of class "lmerMod"
In-Reply-To: <CAJuCY5wyvLVzOEv=tofQTSzZR2E5jeeqLFAFg66oVNbzRHjKHA@mail.gmail.com>
References: <CAJuCY5wyvLVzOEv=tofQTSzZR2E5jeeqLFAFg66oVNbzRHjKHA@mail.gmail.com>
Message-ID: <1169967506.2682803.1435924745113.JavaMail.yahoo@mail.yahoo.com>


?? Dear Thierry,
?? Thank you very much. Do you mean I should add a line to the function with the code "fixef(my.model)", retaining everything else, even the "beta0??? <- as.numeric(mod at fixef)" line? Or should I replace something??? Also, when you say "fixef(my.model), do you mean "fixef(mod)" (as in my case "mod <- lmer(y ~ 1 + (1|group1)+(1|group2),verbose=FALSE)"?
?? Thank you very much again.?? Iker
?__________________________________________________________________

?? Dr. Iker Vaquero-Alba
 ?? Daphne du Maurier
 ?? Centre for Ecology and Conservation
 ?? College of Life and Environmental Sciences
 ?? University of Exeter, Cornwall Campus
 ?? TR10 9FE 
 ?? Penryn
 ?? U.K.

 ?? http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/

 ?? https://eric.exeter.ac.uk/repository/handle/10036/3381


      De: Thierry Onkelinx <thierry.onkelinx at inbo.be>
 Para: Iker Vaquero Alba <karraspito at yahoo.es> 
CC: r-help at r-project.org 
 Enviado: Viernes 3 de julio de 2015 11:00
 Asunto: Re: [R] no slot of name "fixef" for this object of class "lmerMod"
   
Dear Iker,The internals of the mer model have changed. Use fixef(your.model). Best regards,?
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest 
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance 
Kliniekstraat 25
1070 Anderlecht
BelgiumTo call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner 
The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. ~ John TukeyOp 2 jul. 2015 22:17 schreef "Iker Vaquero Alba" <karraspito at yahoo.es>:



?? Hello everyone.
?? I am trying to re-analyse some data with an R function I last used in 2011. Everything seemed to work fine then, but now, using the same code, it gives me this error:
?? Error in R.pe(y, group1, group2, returnR = FALSE) :
? no slot of name "fixef" for this object of class "lmerMod"
?? This is the part of the function that I think is relevant for the problem:
?? # preparation
??? #n <- rowSums(y)
??? N <- length(y)
??? k <- length(unique(group1)) # clone
??? g <- length(unique(group2)) # round
??? #
??? require(lme4)
??? # functions
??? R.pe <- function(y, group1, group2, returnR=TRUE) {
??? ?mod <- lmer(y ~ 1 + (1|group1)+(1|group2),verbose=FALSE)
??? ??? VarComp? <- lme4::VarCorr(mod)
??? ??? beta0??? <- as.numeric(mod at fixef)
??? ??? var.e <- attr(VarComp, "sc")^2 # residual variance
??? ??? var.a1 <- (as.numeric(VarComp[1])) # e.g. get clone R
??? ??? var.a2 <- (as.numeric(VarComp[2])) # e.g. get round R
??? ??? ??? R.group1? <- var.a1/(var.a1+var.e) # clone level
????????????????? R.group2? <- var.a2/(var.a2+var.e)
????????????????? R.groupr? <- var.a1/(var.a1+var.e+var.a2)
??? ??? if(returnR) return(list(R.group1=R.group1,R.group2=R.group2,R.groupr=R.groupr))
??? ??? else return(list(beta0=beta0, var.e=var.e, var.a1=var.a1, var.a2=var.a2))
??? }


?? I would appreciate any help about this. Let me know if you need more code, the function is obviously longer.
?? Thank you very much in advance.
?? Iker
__________________________________________________________________

?? Dr. Iker Vaquero-Alba
??? Daphne du Maurier
??? Centre for Ecology and Conservation
??? College of Life and Environmental Sciences
??? University of Exeter, Cornwall Campus
??? TR10 9FE
??? Penryn
??? U.K.

??? http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/

??? https://eric.exeter.ac.uk/repository/handle/10036/3381


? ? ? ? [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From federico.calboli at helsinki.fi  Fri Jul  3 14:18:01 2015
From: federico.calboli at helsinki.fi (Federico Calboli)
Date: Fri, 3 Jul 2015 15:18:01 +0300
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C336A7@SRVEXCHMBX.precheza.cz>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
	<55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>
	<CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C336A7@SRVEXCHMBX.precheza.cz>
Message-ID: <711CBDBE-F910-459A-9999-619705EF722F@helsinki.fi>

As I said, I found a formulation that pleased the check and that?s it for me.  I am befuddled by the check in the first place though.

BW

F


> On 3 Jul 2015, at 15:14, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> 
> Hi
> 
> without going deep into this matter, what if you remove "/" from your sentence.
> 
> Cheers
> Petr
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Hadley
>> Wickham
>> Sent: Friday, July 03, 2015 1:14 PM
>> To: Federico Calboli
>> Cc: R-help
>> Subject: Re: [R] what constitutes a 'complete sentence'?
>> 
>> In that case, you need to create a minimal reproducible example and
>> make it publicly available.
>> 
>> Hadley
>> 
>> On Friday, July 3, 2015, Federico Calboli
>> <federico.calboli at helsinki.fi>
>> wrote:
>> 
>>> 
>>>> On 3 Jul 2015, at 12:14, Hadley Wickham <h.wickham at gmail.com
>>> <javascript:;>> wrote:
>>>> 
>>>> It might be a line break problem - I think you want:
>>>> 
>>>> Description: Functions designed to test for single gene/phenotype
>>>> association and
>>>>   for pleiotropy on genetic and genomic data.
>>> 
>>> Tried this and unfortunately it does not help.
>>> 
>>> BW
>>> 
>>> F
>>> 
>>> 
>>>> 
>>>> Hadley
>>>> 
>>>> On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
>>>> <federico.calboli at helsinki.fi <javascript:;>> wrote:
>>>>> Hi All,
>>>>> 
>>>>> I am upgrading a package for CRAN, and I get this note:
>>>>> 
>>>>> checking DESCRIPTION meta-information ... NOTE Malformed
>>>>> Description field: should contain one or more complete
>>> sentences.
>>>>> 
>>>>> This is puzzling because:
>>>>> 
>>>>> cat DESCRIPTION
>>>>> 
>>>>> ...
>>>>> Description: Functions designed to test for single gene/phenotype
>>> association and for pleiotropy on genetic and genomic data.
>>>>> ...
>>>>> 
>>>>> In my understanding "Functions designed to test for single
>>> gene/phenotype association and for pleiotropy on genetic and genomic
>> data.?
>>> *is* a complete sentence.  So, what is complete sentence in the
>>> opinion of whomever coded that check?
>>>>> 
>>>>> Best
>>>>> 
>>>>> F
>>>>> 
>>>>> --
>>>>> Federico Calboli
>>>>> Ecological Genetics Research Unit
>>>>> Department of Biosciences
>>>>> PO Box 65 (Biocenter 3, Viikinkaari 1)
>>>>> FIN-00014 University of Helsinki
>>>>> Finland
>>>>> 
>>>>> federico.calboli at helsinki.fi <javascript:;>
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
>>>>> and
>>> more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>>>> 
>>>> --
>>>> http://had.co.nz/
>>> 
>>> 
>>> --
>>> Federico Calboli
>>> Ecological Genetics Research Unit
>>> Department of Biosciences
>>> PO Box 65 (Biocenter 3, Viikinkaari 1)
>>> FIN-00014 University of Helsinki
>>> Finland
>>> 
>>> federico.calboli at helsinki.fi <javascript:;>
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>>> 
>> 
>> --
>> http://had.co.nz/
>> 
>>      [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> 
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
> 
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
> 
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


--
Federico Calboli
Ecological Genetics Research Unit
Department of Biosciences
PO Box 65 (Biocenter 3, Viikinkaari 1)
FIN-00014 University of Helsinki
Finland

federico.calboli at helsinki.fi


From arkay7777 at gmail.com  Fri Jul  3 16:16:27 2015
From: arkay7777 at gmail.com (RK)
Date: Fri, 3 Jul 2015 14:16:27 +0000
Subject: [R]
	=?utf-8?q?=3A_Ramanujan_and_the_accuracy_of_floating_point=09?=
	=?utf-8?q?computations=09-_using_Rmpfr_in_R?=
References: <1435860140.7586.BPMail_high_carrier@web142703.mail.bf1.yahoo.com>
	<CAGx1TMDUZmdBQiDgQJsjbaXhVqaSGgKjJ-vJXEqW8=eXs6zu-g@mail.gmail.com>
	<14ad39aaf6a542849bbf3f62a0c2f38f@DOM-EB1-2013.win.ad.jhu.edu>
	<CAGx1TMCjMhijsrY7k-y+_e6Gn5Z06na6Mao1zvz+P0u_ptCh9g@mail.gmail.com>
	<FDB6AA0E-A814-49AE-8D0A-27EA5A46C93F@dcn.davis.CA.us>
	<F7E6D18CC2877149AB5296CE54EA27662ED5FA55@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <loom.20150703T155915-380@post.gmane.org>

Also when I try the following with Rmpfr, it works jut fine.

> exp(sqrt(mpfr(163, 120)) * Const("pi", 120))
1 'mpfr' number of precision  120   bits 
[1] 262537412640768743.99999999999925007601

and

> exp(sqrt(mpfr(163, 400)) * Const("pi", 400))
1 'mpfr' number of precision  400   bits 
[1] 
262537412640768743.99999999999925007259719818568887935385633733699086270
753741037821064791011860731295118134618606450419548

Which compares very nicely with the following:

In[10]:= N[Exp[Sqrt[163] Pi], 125]

Out[10]= 
2.6253741264076874399999999999925007259719818568887935385633733699086270
753741037821064791011860731295118134618606450419308389*10^17


In the multiprecision business, you can never be too certain that you 
are using the right precision throughout your calculations.


Nordlund, Dan (DSHS/RDA <NordlDJ <at> dshs.wa.gov> writes:

> 
> Ravi,
> 
> Take a look at the following link.  
> 
> https://code.google.com/p/r-bc/
> 
> I followed the instructions to get a Windows version of the 'nix 
utility program , bc (a high precision
> calculator), and the source for an R to bc interface.  After 
installing them, I executed
> 
> exp(sqrt(bc(163))*4*atan(bc(1)))
> 
> in R and got this result
> 
> 
"262537412640768743.9999999999992500725971981856888793538563373369908627
075374103782106479101186073116295306145602054347"
> 
> I don't know if this is helpful, but ...
> 
> Dan
> 
> Daniel Nordlund, PhD
> Research and Data Analysis Division
> Services & Enterprise Support Administration
> Washington State Department of Social and Health Services
>


From joscha.zander at roche.com  Fri Jul  3 16:33:32 2015
From: joscha.zander at roche.com (Zander, Joscha)
Date: Fri, 3 Jul 2015 16:33:32 +0200
Subject: [R] matrix -> delete last row -> list
Message-ID: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>

Good day R-community,

i just wondered if it is a bug or a feature...

When i have a matrix "mat" with one column and i delete the last row with

mat <- mat[-nrow(mat),] the result is a list.

So my next call mat[10,] will throw an "wrong dimension" error.
The proper call must be:

mat <- as.matrix(mat[-nrow(mat),])

So is this desired behavior or a bug?

I use R-version 2.15.3, but reconstructed this behavior in 3.2.0 as well.

greetings

-- 
 *Joscha Zander*

Roche Diagnostics GmbH
DXRDDD..6164
Sandhofer Strasse 116
68305 Mannheim / Germany

mailto:joscha.zander at roche.com <joscha.zander at roche.com>

 *Roche Diagnostics GmbH*
Sandhofer Stra?e 116; D?68305 Mannheim; Telefon +49?621?759?0;
Telefax +49?621?759?2890
Sitz der Gesellschaft: Mannheim - Registergericht: AG Mannheim HRB 3962 -
Gesch?ftsf?hrung: Dr. Ursula Redeker, Sprecherin; Edgar Vieth -
Aufsichtsratsvorsitzender: Dr. Severin Schwan

 *Confidentiality Note*
This message is intended only for the use of the named recipient(s) and may
contain confidential and/or privileged information. If you are not the
intended recipient, please contact the sender and delete the message. Any
unauthorized use of the information contained in this message is prohibited.

	[[alternative HTML version deleted]]


From John.Nash at uottawa.ca  Fri Jul  3 17:08:07 2015
From: John.Nash at uottawa.ca (John Nash)
Date: Fri, 3 Jul 2015 15:08:07 +0000
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <5596A3E5.1070103@uottawa.ca>
References: <5596A3E5.1070103@uottawa.ca>
Message-ID: <5596A557.2040802@uottawa.ca>




Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
traffic.

In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
so possibly something changed enough). Trying again.

JN

I got the Wolfram answer as follows:

library(Rmpfr)
n163 <- mpfr(163, 500)
n163
pi500 <- mpfr(pi, 500)
pi500
pitan <- mpfr(4, 500)*atan(mpfr(1,500))
pitan
pitan-pi500
r500 <- exp(sqrt(n163)*pitan)
r500
check <- "262537412640768743.99999999999925007259719818568887935385..."
savehistory("jnramanujan.R")

Note that I used 4*atan(1) to get pi. It seems that may be important,
rather than converting.

JN

On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:

> Message: 40
> Date: Thu, 2 Jul 2015 22:38:45 +0000
> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
> 	<aps6dl at yahoo.com>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] : Ramanujan and the accuracy of floating point
> 	computations - using Rmpfr in R
> Message-ID:
> 	<14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
> Content-Type: text/plain; charset="utf-8"
>
> Hi Rich,
>
> The Wolfram answer is correct.  
> http://mathworld.wolfram.com/RamanujanConstant.html 
>
> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
> http://www.wolframalpha.com/ 
>
> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>
> Thanks,
> Ravi
>
> -----Original Message-----
> From: Richard M. Heiberger [mailto:rmh at temple.edu] 
> Sent: Thursday, July 02, 2015 6:30 PM
> To: Aditya Singh
> Cc: Ravi Varadhan; r-help
> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>
> There is a precedence error in your R attempt.  You need to convert
> 163 to 120 bits first, before taking
> its square root.
>
>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
> 1 'mpfr' number of precision  120   bits
> [1] 262537412640768333.51635812597335712954
>
> ## just the last four characters to the left of the decimal point.
>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837) 
>>> tmp-tmp[2]
>      baseR    Wolfram      Rmpfr wrongRmpfr
>       -488          0       -411       -907
> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
> Please check that you didn't have a similar precedence error in that code.
>
> Rich
>
>
> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>> Ravi
>>>
>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>
>>> Aditya
>>>
>>>
>>>
>>> ------------------------------
>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>
>>>>> Hi,
>>>>>
>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>
>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>
>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>> 262537412640768256.0000
>>>>>
>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>
>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>
>>>>> library(Rmpfr)
>>>>>
>>>>>
>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>> 1 'mpfr' number of precision  120   bits
>>>>>
>>>>> [1] 262537412640767837.08771354274620169031
>>>>>
>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>
>>>>> Thank you,
>>>>> Ravi
>>>>>
>>>>>
>>>>>
>>>>>       [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide 
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> ------------------------------



From bhh at xs4all.nl  Fri Jul  3 17:35:24 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 3 Jul 2015 17:35:24 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <711CBDBE-F910-459A-9999-619705EF722F@helsinki.fi>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<CABdHhvE5n9nEkufEb4dXd_rv74e11BNfS+d=VpyZdfd1j14A+g@mail.gmail.com>
	<55EA9549-BE49-431D-8225-3D4E011A5526@helsinki.fi>
	<CABdHhvHvf-vpv900XKrb5LNyFLxU-3zvHhYKTv=x5bpgEcVPag@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C336A7@SRVEXCHMBX.precheza.cz>
	<711CBDBE-F910-459A-9999-619705EF722F@helsinki.fi>
Message-ID: <A52408CA-2978-4E49-93FE-C0FDC8164466@xs4all.nl>


> On 03-07-2015, at 14:18, Federico Calboli <federico.calboli at helsinki.fi> wrote:
> 
> As I said, I found a formulation that pleased the check and that?s it for me.  I am befuddled by the check in the first place though.
> 

And what would that formulation be (replacing the original one) ?

Berend

> BW
> 
> F
> 
> 
>> On 3 Jul 2015, at 15:14, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>> 
>> Hi
>> 
>> without going deep into this matter, what if you remove "/" from your sentence.
>> 
>> Cheers
>> Petr
>> 
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Hadley
>>> Wickham
>>> Sent: Friday, July 03, 2015 1:14 PM
>>> To: Federico Calboli
>>> Cc: R-help
>>> Subject: Re: [R] what constitutes a 'complete sentence'?
>>> 
>>> In that case, you need to create a minimal reproducible example and
>>> make it publicly available.
>>> 
>>> Hadley
>>> 
>>> On Friday, July 3, 2015, Federico Calboli
>>> <federico.calboli at helsinki.fi>
>>> wrote:
>>> 
>>>> 
>>>>> On 3 Jul 2015, at 12:14, Hadley Wickham <h.wickham at gmail.com
>>>> <javascript:;>> wrote:
>>>>> 
>>>>> It might be a line break problem - I think you want:
>>>>> 
>>>>> Description: Functions designed to test for single gene/phenotype
>>>>> association and
>>>>>  for pleiotropy on genetic and genomic data.
>>>> 
>>>> Tried this and unfortunately it does not help.
>>>> 
>>>> BW
>>>> 
>>>> F
>>>> 
>>>> 
>>>>> 
>>>>> Hadley
>>>>> 
>>>>> On Fri, Jul 3, 2015 at 10:09 AM, Federico Calboli
>>>>> <federico.calboli at helsinki.fi <javascript:;>> wrote:
>>>>>> Hi All,
>>>>>> 
>>>>>> I am upgrading a package for CRAN, and I get this note:
>>>>>> 
>>>>>> checking DESCRIPTION meta-information ... NOTE Malformed
>>>>>> Description field: should contain one or more complete
>>>> sentences.
>>>>>> 
>>>>>> This is puzzling because:
>>>>>> 
>>>>>> cat DESCRIPTION
>>>>>> 
>>>>>> ...
>>>>>> Description: Functions designed to test for single gene/phenotype
>>>> association and for pleiotropy on genetic and genomic data.
>>>>>> ...
>>>>>> 
>>>>>> In my understanding "Functions designed to test for single
>>>> gene/phenotype association and for pleiotropy on genetic and genomic
>>> data.?
>>>> *is* a complete sentence.  So, what is complete sentence in the
>>>> opinion of whomever coded that check?
>>>>>> 
>>>>>> Best
>>>>>> 
>>>>>> F
>>>>>> 
>>>>>> --
>>>>>> Federico Calboli
>>>>>> Ecological Genetics Research Unit
>>>>>> Department of Biosciences
>>>>>> PO Box 65 (Biocenter 3, Viikinkaari 1)
>>>>>> FIN-00014 University of Helsinki
>>>>>> Finland
>>>>>> 
>>>>>> federico.calboli at helsinki.fi <javascript:;>
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
>>>>>> and
>>>> more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> 
>>>>> 
>>>>> --
>>>>> http://had.co.nz/
>>>> 
>>>> 
>>>> --
>>>> Federico Calboli
>>>> Ecological Genetics Research Unit
>>>> Department of Biosciences
>>>> PO Box 65 (Biocenter 3, Viikinkaari 1)
>>>> FIN-00014 University of Helsinki
>>>> Finland
>>>> 
>>>> federico.calboli at helsinki.fi <javascript:;>
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>>> 
>>> 
>>> --
>>> http://had.co.nz/
>>> 
>>>     [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ________________________________
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>> 
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>> 
>> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
>> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>> 
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
>> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> 
> 
> --
> Federico Calboli
> Ecological Genetics Research Unit
> Department of Biosciences
> PO Box 65 (Biocenter 3, Viikinkaari 1)
> FIN-00014 University of Helsinki
> Finland
> 
> federico.calboli at helsinki.fi
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Fri Jul  3 17:41:22 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 03 Jul 2015 10:41:22 -0500
Subject: [R] matrix -> delete last row -> list
In-Reply-To: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>
References: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>
Message-ID: <DF494620-A9F0-44B7-9C5A-697DACE156CD@me.com>


> On Jul 3, 2015, at 9:33 AM, Zander, Joscha <joscha.zander at roche.com> wrote:
> 
> Good day R-community,
> 
> i just wondered if it is a bug or a feature...
> 
> When i have a matrix "mat" with one column and i delete the last row with
> 
> mat <- mat[-nrow(mat),] the result is a list.
> 
> So my next call mat[10,] will throw an "wrong dimension" error.
> The proper call must be:
> 
> mat <- as.matrix(mat[-nrow(mat),])
> 
> So is this desired behavior or a bug?
> 
> I use R-version 2.15.3, but reconstructed this behavior in 3.2.0 as well.
> 
> greetings
> 
> -- 
> *Joscha Zander*


See:

  http://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-do-my-matrices-lose-dimensions_003f


mat <- matrix(1:12, ncol = 1)

> str(mat)
 int [1:12, 1] 1 2 3 4 5 6 7 8 9 10 ...

> mat[-nrow(mat), ]
 [1]  1  2  3  4  5  6  7  8  9 10 11

# This is a vector, not a list
> str(mat[-nrow(mat), ])
 int [1:11] 1 2 3 4 5 6 7 8 9 10 ?

> mat[-nrow(mat), , drop = FALSE]
      [,1]
 [1,]    1
 [2,]    2
 [3,]    3
 [4,]    4
 [5,]    5
 [6,]    6
 [7,]    7
 [8,]    8
 [9,]    9
[10,]   10
[11,]   11

# This is a matrix
> str(mat[-nrow(mat), , drop = FALSE])
 int [1:11, 1] 1 2 3 4 5 6 7 8 9 10 ?


Regards,

Marc Schwartz

P.S. are you restricted in being able to upgrade from a version of R that is two years old?


From sarah.goslee at gmail.com  Fri Jul  3 17:43:21 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 3 Jul 2015 11:43:21 -0400
Subject: [R] matrix -> delete last row -> list
In-Reply-To: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>
References: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>
Message-ID: <CAM_vjuk18-E16E2=zd47++mg=SOjbfAZz2kLLXEho6TQBm0HtA@mail.gmail.com>

Hi,

On Fri, Jul 3, 2015 at 10:33 AM, Zander, Joscha <joscha.zander at roche.com>
wrote:
> Good day R-community,
>
> i just wondered if it is a bug or a feature...
>
> When i have a matrix "mat" with one column and i delete the last row with
>
> mat <- mat[-nrow(mat),] the result is a list.

I have no idea how you're getting a list from a matrix (see below). Perhaps
you mean a data frame?


> So my next call mat[10,] will throw an "wrong dimension" error.
> The proper call must be:
>
> mat <- as.matrix(mat[-nrow(mat),])
>
> So is this desired behavior or a bug?

If you check
?"["
you'll see the drop argument, which is what I guess you want. Compare:

> mat <- matrix(1:6, nrow=2)
> mat <- mat[-nrow(mat), ]
> class(mat) # not a list
[1] "integer"
> dim(mat)
NULL
> is.list(mat) # see? really not a list
[1] FALSE
> mat
[1] 1 3 5


> mat <- matrix(1:6, nrow=2)
> mat <- mat[-nrow(mat), , drop=FALSE]
> class(mat)
[1] "matrix"
> dim(mat)
[1] 1 3
> mat
     [,1] [,2] [,3]
[1,]    1    3    5



> I use R-version 2.15.3, but reconstructed this behavior in 3.2.0 as well.
>
> greetings
>
> --
>  *Joscha Zander*
>--
Sarah Goslee
http://www.functionaldiversity.org

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Jul  3 19:53:22 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 3 Jul 2015 19:53:22 +0200
Subject: [R] no slot of name "fixef" for this object of class "lmerMod"
In-Reply-To: <1169967506.2682803.1435924745113.JavaMail.yahoo@mail.yahoo.com>
References: <CAJuCY5wyvLVzOEv=tofQTSzZR2E5jeeqLFAFg66oVNbzRHjKHA@mail.gmail.com>
	<1169967506.2682803.1435924745113.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAJuCY5yCAyWA6EA1DmuA8O1UM_xAeW3f62SF4jW9nkdKVbq9Rw@mail.gmail.com>

Yes, use fixef(mod) instead of mod at fixef. It is recommended to use the
accessor functions instead of reading the slots with the @.

?
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
Op 3 jul. 2015 13:59 schreef "Iker Vaquero Alba" <karraspito at yahoo.es>:

>
>    Dear Thierry,
>
>    Thank you very much. Do you mean I should *add* a line to the function
> with the code "fixef(my.model)", retaining everything else, even the
> "beta0    <- as.numeric(mod at fixef)" line? Or should I replace something?
>    Also, when you say "fixef(my.model), do you mean "fixef(mod)" (as in my
> case "mod <- lmer(y ~ 1 + (1|group1)+(1|group2),verbose=FALSE)"?
>
>    Thank you very much again.
>    Iker
>
> __________________________________________________________________
>
>    Dr. Iker Vaquero-Alba
>    Daphne du Maurier
>    Centre for Ecology and Conservation
>    College of Life and Environmental Sciences
>    University of Exeter, Cornwall Campus
>    TR10 9FE
>    Penryn
>    U.K.
>
>
> http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/
>
>    https://eric.exeter.ac.uk/repository/handle/10036/3381
>
>
>   ------------------------------
>  *De:* Thierry Onkelinx <thierry.onkelinx at inbo.be>
> *Para:* Iker Vaquero Alba <karraspito at yahoo.es>
> *CC:* r-help at r-project.org
> *Enviado:* Viernes 3 de julio de 2015 11:00
> *Asunto:* Re: [R] no slot of name "fixef" for this object of class
> "lmerMod"
>
> Dear Iker,
> The internals of the mer model have changed. Use fixef(your.model).
> Best regards,
> ?
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> Op 2 jul. 2015 22:17 schreef "Iker Vaquero Alba" <karraspito at yahoo.es>:
>
>
>    Hello everyone.
>    I am trying to re-analyse some data with an R function I last used in
> 2011. Everything seemed to work fine then, but now, using the same code, it
> gives me this error:
>    Error in R.pe(y, group1, group2, returnR = FALSE) :
>   no slot of name "fixef" for this object of class "lmerMod"
>    This is the part of the function that I think is relevant for the
> problem:
>    # preparation
>     #n <- rowSums(y)
>     N <- length(y)
>     k <- length(unique(group1)) # clone
>     g <- length(unique(group2)) # round
>     #
>     require(lme4)
>     # functions
>     R.pe <- function(y, group1, group2, returnR=TRUE) {
>      mod <- lmer(y ~ 1 + (1|group1)+(1|group2),verbose=FALSE)
>         VarComp  <- lme4::VarCorr(mod)
>         beta0    <- as.numeric(mod at fixef)
>         var.e <- attr(VarComp, "sc")^2 # residual variance
>         var.a1 <- (as.numeric(VarComp[1])) # e.g. get clone R
>         var.a2 <- (as.numeric(VarComp[2])) # e.g. get round R
>             R.group1  <- var.a1/(var.a1+var.e) # clone level
>                   R.group2  <- var.a2/(var.a2+var.e)
>                   R.groupr  <- var.a1/(var.a1+var.e+var.a2)
>         if(returnR)
> return(list(R.group1=R.group1,R.group2=R.group2,R.groupr=R.groupr))
>         else return(list(beta0=beta0, var.e=var.e, var.a1=var.a1,
> var.a2=var.a2))
>     }
>
>
>    I would appreciate any help about this. Let me know if you need more
> code, the function is obviously longer.
>    Thank you very much in advance.
>    Iker
> __________________________________________________________________
>
>    Dr. Iker Vaquero-Alba
>     Daphne du Maurier
>     Centre for Ecology and Conservation
>     College of Life and Environmental Sciences
>     University of Exeter, Cornwall Campus
>     TR10 9FE
>     Penryn
>     U.K.
>
>
> http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/
>
>     https://eric.exeter.ac.uk/repository/handle/10036/3381
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jul  3 20:06:07 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 3 Jul 2015 11:06:07 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
In-Reply-To: <5596A557.2040802@uottawa.ca>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
Message-ID: <1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>


> On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
> 
> 
> 
> 
> Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
> traffic.
> 
> In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
> so possibly something changed enough). Trying again.
> 
> JN
> 
> I got the Wolfram answer as follows:
> 
> library(Rmpfr)
> n163 <- mpfr(163, 500)
> n163
> pi500 <- mpfr(pi, 500)
> pi500
> pitan <- mpfr(4, 500)*atan(mpfr(1,500))
> pitan
> pitan-pi500
> r500 <- exp(sqrt(n163)*pitan)
> r500
> check <- "262537412640768743.99999999999925007259719818568887935385..."
> savehistory("jnramanujan.R")
> 
> Note that I used 4*atan(1) to get pi.

RK got it right by following the example in the help page for mpfr:

Const("pi", 120)

The R `pi` constant is not recognized by mpfr as being anything other than another double .


There are four special values that mpfr recognizes.

? 
Best;
David


> It seems that may be important,
> rather than converting.
> 
> JN
> 
> On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
> 
>> Message: 40
>> Date: Thu, 2 Jul 2015 22:38:45 +0000
>> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
>> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
>> 	<aps6dl at yahoo.com>
>> Cc: r-help <r-help at r-project.org>
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>> 	computations - using Rmpfr in R
>> Message-ID:
>> 	<14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
>> Content-Type: text/plain; charset="utf-8"
>> 
>> Hi Rich,
>> 
>> The Wolfram answer is correct.  
>> http://mathworld.wolfram.com/RamanujanConstant.html 
>> 
>> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
>> http://www.wolframalpha.com/ 
>> 
>> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>> 
>> Thanks,
>> Ravi
>> 
>> -----Original Message-----
>> From: Richard M. Heiberger [mailto:rmh at temple.edu] 
>> Sent: Thursday, July 02, 2015 6:30 PM
>> To: Aditya Singh
>> Cc: Ravi Varadhan; r-help
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>> 
>> There is a precedence error in your R attempt.  You need to convert
>> 163 to 120 bits first, before taking
>> its square root.
>> 
>>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>> 1 'mpfr' number of precision  120   bits
>> [1] 262537412640768333.51635812597335712954
>> 
>> ## just the last four characters to the left of the decimal point.
>>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837) 
>>>> tmp-tmp[2]
>>     baseR    Wolfram      Rmpfr wrongRmpfr
>>      -488          0       -411       -907
>> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
>> Please check that you didn't have a similar precedence error in that code.
>> 
>> Rich
>> 
>> 
>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>>> Ravi
>>>> 
>>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>> 
>>>> Aditya
>>>> 
>>>> 
>>>> 
>>>> ------------------------------
>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>> 
>>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>> 
>>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>>> 262537412640768256.0000
>>>>>> 
>>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>> 
>>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>> 
>>>>>> library(Rmpfr)
>>>>>> 
>>>>>> 
>>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>> 
>>>>>> [1] 262537412640767837.08771354274620169031
>>>>>> 
>>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>> 
>>>>>> Thank you,
>>>>>> Ravi
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>>      [[alternative HTML version deleted]]
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide 
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide 
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> ------------------------------
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius, MD
Alameda, CA, USA


From xie at yihui.name  Fri Jul  3 20:27:28 2015
From: xie at yihui.name (Yihui Xie)
Date: Fri, 3 Jul 2015 13:27:28 -0500
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <55969185.5000604@auckland.ac.nz>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
Message-ID: <CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>

Sigh, how natural it is to say "This package ...", but you probably
don't know a package can be easily rejected by CRAN simply because of
this phrase "This package" (it has been clearly stated in the R-exts
manual).

I don't think the grammar is the problem here. When in doubt, I always
check what MASS does:
http://cran.rstudio.com/web/packages/MASS/index.html Turns out its
description is not a complete sentence, either.

Sounds like R has become a language for statistical computing and
graphics, plus English grammar since 3.0.x.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Fri, Jul 3, 2015 at 8:43 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> On 03/07/15 20:09, Federico Calboli wrote:
>>
>> Hi All,
>>
>> I am upgrading a package for CRAN, and I get this note:
>>
>> checking DESCRIPTION meta-information ... NOTE Malformed Description
>> field: should contain one or more complete sentences.
>>
>> This is puzzling because:
>>
>> cat DESCRIPTION
>>
>> ... Description: Functions designed to test for single gene/phenotype
>> association and for pleiotropy on genetic and genomic data. ...
>>
>> In my understanding "Functions designed to test for single
>> gene/phenotype association and for pleiotropy on genetic and genomic
>> data.? *is* a complete sentence.  So, what is complete sentence in
>> the opinion of whomever coded that check?
>
>
>
> If that is your understanding you need to go back to school and learn some
> grammar.  What you have is a noun ("Functions") modified by an adjectival
> clause.  No verb in sight.  Ergo *not* a complete sentence.
>
> OTOH you are probably in good company in not knowing your grammar.  The CRAN
> folks most likely don't know grammar either.  I suspect that they *don't*
> actually demand a complete sentence.  Such a demand would in fact be rather
> pedantic.  Moreover I really can't see how the package checker could
> possibly check for complete sentences.  This would require some very
> sophisticated programming, it seems to me.
>
> If it turns out that you *really* need a complete sentence, you could say
> (for instance):
>
> This package consists of functions designed to test for single
> gene/phenotype association and for pleiotropy on genetic and genomic data.
>
> The foregoing *is* a complete sentence.
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Sat Jul  4 00:26:01 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 4 Jul 2015 10:26:01 +1200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
Message-ID: <55970BF9.400@auckland.ac.nz>

On 04/07/15 06:27, Yihui Xie wrote:
> Sigh, how natural it is to say "This package ...", but you probably
> don't know a package can be easily rejected by CRAN simply because of
> this phrase "This package" (it has been clearly stated in the R-exts
> manual).

Urrrkkkk!  I *did* "know" that, but had forgotten.  Apologies for my 
wrong-headed suggestion.  Thanks for pointing out my error.

> I don't think the grammar is the problem here. When in doubt, I always
> check what MASS does:
> http://cran.rstudio.com/web/packages/MASS/index.html Turns out its
> description is not a complete sentence, either.
>
> Sounds like R has become a language for statistical computing and
> graphics, plus English grammar since 3.0.x.

The CRAN guidelines should be rewritten so that they say what they 
*mean*.  If a complete sentence is not actually required --- and it 
seems abundantly clear that it is not --- then guidelines should not say 
so.  Rather they should say, clearly and comprehensibly, what actually 
*is* required.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Sat Jul  4 00:35:37 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 4 Jul 2015 10:35:37 +1200
Subject: [R] [FORGED] Re:  matrix -> delete last row -> list
In-Reply-To: <CAM_vjuk18-E16E2=zd47++mg=SOjbfAZz2kLLXEho6TQBm0HtA@mail.gmail.com>
References: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>
	<CAM_vjuk18-E16E2=zd47++mg=SOjbfAZz2kLLXEho6TQBm0HtA@mail.gmail.com>
Message-ID: <55970E39.2080005@auckland.ac.nz>

On 04/07/15 03:43, Sarah Goslee wrote:
> Hi,
>
> On Fri, Jul 3, 2015 at 10:33 AM, Zander, Joscha <joscha.zander at roche.com>
> wrote:
>> Good day R-community,
>>
>> i just wondered if it is a bug or a feature...
>>
>> When i have a matrix "mat" with one column and i delete the last row with
>>
>> mat <- mat[-nrow(mat),] the result is a list.
>
> I have no idea how you're getting a list from a matrix (see below). Perhaps
> you mean a data frame?

<SNIP>

No.  The Zander person just means a vector.  Psigh.

Note to the Zander person:  It is important in R programming to get your 
concepts straight.  It is also important in communicating with others to 
get your terminology straight.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ravi.varadhan at jhu.edu  Fri Jul  3 21:01:52 2015
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Fri, 3 Jul 2015 19:01:52 +0000
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>,
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
Message-ID: <1435950099369.58224@jhu.edu>

Thank you all.  I did think about declaring `pi' as a special constant, but for some reason didn't actually try it.  
Would it be easy to have the mpfr() written such that its argument is automatically of extended precision? In other words, if I just called:  mpfr(exp(sqrt(163)*pi, 120), then all the constants, 163, pi, are automatically of 120 bits precision.

Is this easy to do?

Best,
Ravi
 ________________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Friday, July 3, 2015 2:06 PM
To: John Nash
Cc: r-help; Ravi Varadhan
Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R

> On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
>
>
>
>
> Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
> traffic.
>
> In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
> so possibly something changed enough). Trying again.
>
> JN
>
> I got the Wolfram answer as follows:
>
> library(Rmpfr)
> n163 <- mpfr(163, 500)
> n163
> pi500 <- mpfr(pi, 500)
> pi500
> pitan <- mpfr(4, 500)*atan(mpfr(1,500))
> pitan
> pitan-pi500
> r500 <- exp(sqrt(n163)*pitan)
> r500
> check <- "262537412640768743.99999999999925007259719818568887935385..."
> savehistory("jnramanujan.R")
>
> Note that I used 4*atan(1) to get pi.

RK got it right by following the example in the help page for mpfr:

Const("pi", 120)

The R `pi` constant is not recognized by mpfr as being anything other than another double .


There are four special values that mpfr recognizes.

?
Best;
David


> It seems that may be important,
> rather than converting.
>
> JN
>
> On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
>
>> Message: 40
>> Date: Thu, 2 Jul 2015 22:38:45 +0000
>> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
>> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
>>      <aps6dl at yahoo.com>
>> Cc: r-help <r-help at r-project.org>
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>>      computations - using Rmpfr in R
>> Message-ID:
>>      <14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Hi Rich,
>>
>> The Wolfram answer is correct.
>> http://mathworld.wolfram.com/RamanujanConstant.html
>>
>> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
>> http://www.wolframalpha.com/
>>
>> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>>
>> Thanks,
>> Ravi
>>
>> -----Original Message-----
>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>> Sent: Thursday, July 02, 2015 6:30 PM
>> To: Aditya Singh
>> Cc: Ravi Varadhan; r-help
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>
>> There is a precedence error in your R attempt.  You need to convert
>> 163 to 120 bits first, before taking
>> its square root.
>>
>>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>> 1 'mpfr' number of precision  120   bits
>> [1] 262537412640768333.51635812597335712954
>>
>> ## just the last four characters to the left of the decimal point.
>>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>>> tmp-tmp[2]
>>     baseR    Wolfram      Rmpfr wrongRmpfr
>>      -488          0       -411       -907
>> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
>> Please check that you didn't have a similar precedence error in that code.
>>
>> Rich
>>
>>
>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>>> Ravi
>>>>
>>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>>
>>>> Aditya
>>>>
>>>>
>>>>
>>>> ------------------------------
>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>
>>>>>> Hi,
>>>>>>
>>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>>
>>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>>
>>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>>> 262537412640768256.0000
>>>>>>
>>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>>
>>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>>
>>>>>> library(Rmpfr)
>>>>>>
>>>>>>
>>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>>
>>>>>> [1] 262537412640767837.08771354274620169031
>>>>>>
>>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>>
>>>>>> Thank you,
>>>>>> Ravi
>>>>>>
>>>>>>
>>>>>>
>>>>>>      [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>> ------------------------------
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius, MD
Alameda, CA, USA



From dwinsemius at comcast.net  Sat Jul  4 02:08:03 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 3 Jul 2015 17:08:03 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
In-Reply-To: <1435950099369.58224@jhu.edu>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
Message-ID: <3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>



It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.

? 
David

> On Jul 3, 2015, at 12:01 PM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
> 
> Thank you all.  I did think about declaring `pi' as a special constant, but for some reason didn't actually try it.  
> Would it be easy to have the mpfr() written such that its argument is automatically of extended precision? In other words, if I just called:  mpfr(exp(sqrt(163)*pi, 120), then all the constants, 163, pi, are automatically of 120 bits precision.
> 
> Is this easy to do?
> 
> Best,
> Ravi
> ________________________________________
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Friday, July 3, 2015 2:06 PM
> To: John Nash
> Cc: r-help; Ravi Varadhan
> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
> 
>> On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
>> 
>> 
>> 
>> 
>> Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
>> traffic.
>> 
>> In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
>> so possibly something changed enough). Trying again.
>> 
>> JN
>> 
>> I got the Wolfram answer as follows:
>> 
>> library(Rmpfr)
>> n163 <- mpfr(163, 500)
>> n163
>> pi500 <- mpfr(pi, 500)
>> pi500
>> pitan <- mpfr(4, 500)*atan(mpfr(1,500))
>> pitan
>> pitan-pi500
>> r500 <- exp(sqrt(n163)*pitan)
>> r500
>> check <- "262537412640768743.99999999999925007259719818568887935385..."
>> savehistory("jnramanujan.R")
>> 
>> Note that I used 4*atan(1) to get pi.
> 
> RK got it right by following the example in the help page for mpfr:
> 
> Const("pi", 120)
> 
> The R `pi` constant is not recognized by mpfr as being anything other than another double .
> 
> 
> There are four special values that mpfr recognizes.
> 
> ?
> Best;
> David
> 
> 
>> It seems that may be important,
>> rather than converting.
>> 
>> JN
>> 
>> On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
>> 
>>> Message: 40
>>> Date: Thu, 2 Jul 2015 22:38:45 +0000
>>> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
>>> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
>>>     <aps6dl at yahoo.com>
>>> Cc: r-help <r-help at r-project.org>
>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>>>     computations - using Rmpfr in R
>>> Message-ID:
>>>     <14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
>>> Content-Type: text/plain; charset="utf-8"
>>> 
>>> Hi Rich,
>>> 
>>> The Wolfram answer is correct.
>>> http://mathworld.wolfram.com/RamanujanConstant.html
>>> 
>>> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
>>> http://www.wolframalpha.com/
>>> 
>>> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>>> 
>>> Thanks,
>>> Ravi
>>> 
>>> -----Original Message-----
>>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>>> Sent: Thursday, July 02, 2015 6:30 PM
>>> To: Aditya Singh
>>> Cc: Ravi Varadhan; r-help
>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>> 
>>> There is a precedence error in your R attempt.  You need to convert
>>> 163 to 120 bits first, before taking
>>> its square root.
>>> 
>>>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>>> 1 'mpfr' number of precision  120   bits
>>> [1] 262537412640768333.51635812597335712954
>>> 
>>> ## just the last four characters to the left of the decimal point.
>>>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>>>> tmp-tmp[2]
>>>    baseR    Wolfram      Rmpfr wrongRmpfr
>>>     -488          0       -411       -907
>>> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
>>> Please check that you didn't have a similar precedence error in that code.
>>> 
>>> Rich
>>> 
>>> 
>>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>>>> Ravi
>>>>> 
>>>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>>> 
>>>>> Aditya
>>>>> 
>>>>> 
>>>>> 
>>>>> ------------------------------
>>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>> 
>>>>>>> Hi,
>>>>>>> 
>>>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>>> 
>>>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>>> 
>>>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>>>> 262537412640768256.0000
>>>>>>> 
>>>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>>> 
>>>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>>> 
>>>>>>> library(Rmpfr)
>>>>>>> 
>>>>>>> 
>>>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>>> 
>>>>>>> [1] 262537412640767837.08771354274620169031
>>>>>>> 
>>>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>>> 
>>>>>>> Thank you,
>>>>>>> Ravi
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>>     [[alternative HTML version deleted]]
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>> ------------------------------
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius, MD
> Alameda, CA, USA
> 

David Winsemius, MD
Alameda, CA, USA


From dulcalma at bigpond.com  Sat Jul  4 02:23:52 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sat, 4 Jul 2015 10:23:52 +1000
Subject: [R] Lattice: set col = "black" for box.rectangle and
	box.umbrella
In-Reply-To: <alpine.LNX.2.11.1507030853140.20592@localhost>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
	<alpine.LNX.2.11.1507021101040.9672@localhost>
	<CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
	<alpine.LNX.2.11.1507021150100.12452@localhost>
	<000301d0b532$904a11f0$b0de35d0$@bigpond.com>
	<alpine.LNX.2.11.1507030853140.20592@localhost>
Message-ID: <000601d0b5ef$b5772540$20656fc0$@bigpond.com>

Rich

Just a thought
Have you set pch colour values somewhere else?
see also https://stat.ethz.ch/pipermail/r-help/2010-March/230329.html

Duncan

-----Original Message-----
From: rshepard at localhost.appl-ecosys.com
[mailto:rshepard at localhost.appl-ecosys.com] On Behalf Of Rich Shepard
Sent: Saturday, 4 July 2015 01:58
To: Duncan Mackay
Subject: Re: [R] Lattice: set col = "black" for box.rectangle and
box.umbrella

On Fri, 3 Jul 2015, Duncan Mackay wrote:

> My settings are
>
>       par.settings = list(fontsize = list(text = 10.5,
>                                           points = 8),
>                           strip.background = list(col = "transparent"),
>                           box.dot = list(col = "#FF0000", # red

Duncan,

   This is interesting: the color of the horizontal line remains black,
rather than displaying in red whether I use the numeric code or the word. I
need to explore why.

Thanks again,

Rich

-- 
Richard B. Shepard, Ph.D.
Applied Ecosystem Services, Inc.  	       Troutdale, OR 97060 USA
www[dot]appl-ecosys[dot]com   Voice: 503-667-4517    Fax: 503-667-8863


From dwinsemius at comcast.net  Sat Jul  4 03:45:18 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 3 Jul 2015 18:45:18 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
In-Reply-To: <3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
Message-ID: <FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>


> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> 
> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.

This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )

Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
            spi <- paste0("Const('pi',",prec,?)?)
            sexpr <- gsub("pi", spi, sexpr)
            print( eval(parse(text=sexpr))) }

# Very minimal testing
> Pre(pi,120)
1 'mpfr' number of precision  120   bits 
[1] 3.1415926535897932384626433832795028847
> Pre(exp(pi),120)
1 'mpfr' number of precision  120   bits 
[1] 23.140692632779269005729086367948547394
> Pre(log(exp(pi)),120)
1 'mpfr' number of precision  120   bits 
[1] 3.1415926535897932384626433832795028847

At the moment it still needs to ahve other numbers mpfr-ified to succeed:

> Pre(pi-4*atan(1),120)
1 'mpfr' number of precision  120   bits 
[1] 1.2246467991473531772315719253130892016e-16
> Pre(pi-4*atan(mpfr(1,120)),120)
1 'mpfr' number of precision  120   bits 
[1] 0

Best;
David.

> 

> ? 
> David
> 
>> On Jul 3, 2015, at 12:01 PM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
>> 
>> Thank you all.  I did think about declaring `pi' as a special constant, but for some reason didn't actually try it.  
>> Would it be easy to have the mpfr() written such that its argument is automatically of extended precision? In other words, if I just called:  mpfr(exp(sqrt(163)*pi, 120), then all the constants, 163, pi, are automatically of 120 bits precision.
>> 
>> Is this easy to do?
>> 
>> Best,
>> Ravi
>> ________________________________________
>> From: David Winsemius <dwinsemius at comcast.net>
>> Sent: Friday, July 3, 2015 2:06 PM
>> To: John Nash
>> Cc: r-help; Ravi Varadhan
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>> 
>>> On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
>>> 
>>> 
>>> 
>>> 
>>> Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
>>> traffic.
>>> 
>>> In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
>>> so possibly something changed enough). Trying again.
>>> 
>>> JN
>>> 
>>> I got the Wolfram answer as follows:
>>> 
>>> library(Rmpfr)
>>> n163 <- mpfr(163, 500)
>>> n163
>>> pi500 <- mpfr(pi, 500)
>>> pi500
>>> pitan <- mpfr(4, 500)*atan(mpfr(1,500))
>>> pitan
>>> pitan-pi500
>>> r500 <- exp(sqrt(n163)*pitan)
>>> r500
>>> check <- "262537412640768743.99999999999925007259719818568887935385..."
>>> savehistory("jnramanujan.R")
>>> 
>>> Note that I used 4*atan(1) to get pi.
>> 
>> RK got it right by following the example in the help page for mpfr:
>> 
>> Const("pi", 120)
>> 
>> The R `pi` constant is not recognized by mpfr as being anything other than another double .
>> 
>> 
>> There are four special values that mpfr recognizes.
>> 
>> ?
>> Best;
>> David
>> 
>> 
>>> It seems that may be important,
>>> rather than converting.
>>> 
>>> JN
>>> 
>>> On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
>>> 
>>>> Message: 40
>>>> Date: Thu, 2 Jul 2015 22:38:45 +0000
>>>> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
>>>> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
>>>>    <aps6dl at yahoo.com>
>>>> Cc: r-help <r-help at r-project.org>
>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>>>>    computations - using Rmpfr in R
>>>> Message-ID:
>>>>    <14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
>>>> Content-Type: text/plain; charset="utf-8"
>>>> 
>>>> Hi Rich,
>>>> 
>>>> The Wolfram answer is correct.
>>>> http://mathworld.wolfram.com/RamanujanConstant.html
>>>> 
>>>> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
>>>> http://www.wolframalpha.com/
>>>> 
>>>> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>>>> 
>>>> Thanks,
>>>> Ravi
>>>> 
>>>> -----Original Message-----
>>>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>>>> Sent: Thursday, July 02, 2015 6:30 PM
>>>> To: Aditya Singh
>>>> Cc: Ravi Varadhan; r-help
>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>>> 
>>>> There is a precedence error in your R attempt.  You need to convert
>>>> 163 to 120 bits first, before taking
>>>> its square root.
>>>> 
>>>>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>>>> 1 'mpfr' number of precision  120   bits
>>>> [1] 262537412640768333.51635812597335712954
>>>> 
>>>> ## just the last four characters to the left of the decimal point.
>>>>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>>>>> tmp-tmp[2]
>>>>   baseR    Wolfram      Rmpfr wrongRmpfr
>>>>    -488          0       -411       -907
>>>> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
>>>> Please check that you didn't have a similar precedence error in that code.
>>>> 
>>>> Rich
>>>> 
>>>> 
>>>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>>>>> Ravi
>>>>>> 
>>>>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>>>> 
>>>>>> Aditya
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> ------------------------------
>>>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>>> 
>>>>>>>> Hi,
>>>>>>>> 
>>>>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>>>> 
>>>>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>>>> 
>>>>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>>>>> 262537412640768256.0000
>>>>>>>> 
>>>>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>>>> 
>>>>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>>>> 
>>>>>>>> library(Rmpfr)
>>>>>>>> 
>>>>>>>> 
>>>>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>>>> 
>>>>>>>> [1] 262537412640767837.08771354274620169031
>>>>>>>> 
>>>>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>>>> 
>>>>>>>> Thank you,
>>>>>>>> Ravi
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>>    [[alternative HTML version deleted]]
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> ------------------------------
>>> 
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius, MD
>> Alameda, CA, USA
>> 
> 
> David Winsemius, MD
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius, MD
Alameda, CA, USA


From jun.shen.ut at gmail.com  Sat Jul  4 05:05:44 2015
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Fri, 3 Jul 2015 23:05:44 -0400
Subject: [R] How to feed graphic device (png,
	jpeg etc) with different files names
Message-ID: <CAMCXXmoaSBRb8ezG+rxBDkMdnkzvsUXKOg-ab-EoRB1eFA6hjw@mail.gmail.com>

Dear list,

I define a function to export a bunch of plots. A sample code is something
like the follow

export.plots<-function(export.type='pdf',...){

match.fun(export.type) (...)

print(plot.func1(...))
print(plot.func2(...))
print(plot.func3(...))
...
print(plot.funcn(...))

dev.off()

}

If I do pdf , everything can be exported into one file. But for other
devices, each graph is one file. Since I have so many graphs to export, I
can't tell which is what from the file names. I was thinking if I can
construct a file name for each of the 'print" function then to feed the
information to the graphic device to have an informative file name. Is it
possible? Thanks.

Jun

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Sat Jul  4 05:30:04 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 03 Jul 2015 20:30:04 -0700
Subject: [R] How to feed graphic device (png,
	jpeg etc) with different files names
In-Reply-To: <CAMCXXmoaSBRb8ezG+rxBDkMdnkzvsUXKOg-ab-EoRB1eFA6hjw@mail.gmail.com>
References: <CAMCXXmoaSBRb8ezG+rxBDkMdnkzvsUXKOg-ab-EoRB1eFA6hjw@mail.gmail.com>
Message-ID: <44EDEDF4-7317-428C-8075-2940A5148288@dcn.davis.CA.us>

Use dev.off and re-open the device.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 3, 2015 8:05:44 PM PDT, Jun Shen <jun.shen.ut at gmail.com> wrote:
>Dear list,
>
>I define a function to export a bunch of plots. A sample code is
>something
>like the follow
>
>export.plots<-function(export.type='pdf',...){
>
>match.fun(export.type) (...)
>
>print(plot.func1(...))
>print(plot.func2(...))
>print(plot.func3(...))
>...
>print(plot.funcn(...))
>
>dev.off()
>
>}
>
>If I do pdf , everything can be exported into one file. But for other
>devices, each graph is one file. Since I have so many graphs to export,
>I
>can't tell which is what from the file names. I was thinking if I can
>construct a file name for each of the 'print" function then to feed the
>information to the graphic device to have an informative file name. Is
>it
>possible? Thanks.
>
>Jun
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From msharp at txbiomed.org  Sat Jul  4 06:38:13 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Sat, 4 Jul 2015 04:38:13 +0000
Subject: [R] question
In-Reply-To: <CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
	<CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
Message-ID: <9B873725-D55A-47A0-84D0-6CA9568A4780@txbiomed.org>

Lida,

I expect that there is a better way to solve your problem than the process you propose.

However, something like this may do what you want.

###
## met <- read.csv("your_met_file?)
## Since I do not have your file a made a small 5*1 character vector.
met <- c("glycine_imp",
            "Nacetylglycine_imp",
            "sarcosine_imp",
            "dimethylglycine_imp",
            "betaine_imp")

for (i in seq_along(met)) {
  my_formula <- paste0(met[i], "~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
  prep <- Scores(Z=metalofGT, formula = my_formula)
  save(prep, file = paste0("prep", i))
}
###

Mark


R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org

> On Jul 2, 2015, at 11:48 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> 
> Thank you so much for replying me!
> for better understanding my problem, I explain my problem more:
> 
> I have a 682*1 matrix called "met" , the first 5 rows similar below:
> 
>> rownames(met)[1:5]
> 
> [1]  "glycine_imp"
> [2]  "Nacetylglycine_imp"
> [3]  "sarcosine_imp"
> [4]  "dimethylglycine_imp"
> [5]  "betaine_imp"
> 
> and I have a function in R that each time use one of the row names of "met"
> matrix and create a new object file and I should save the objects!
> 
> my function is  "
> Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> " that each time just I should change the met[i] and replace by row names
> "met" one by one and for each of them I should rename the function and
> after that I should save each object!
> for example for first row of "met" I have
> 
>>  prep1<- Scores(Z=metalofGT,formula="glycine_imp~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> #creat the object file for first row and called prep1###
> 
>>  save(prep1, file="prep1.RData", compress="bzip2")      ##save the
> object file as "prep1.RData"#####
> 
> I should do this process for 682 row names of "met" matrix and at the end I
> should have    "prep1.RData"  ,   "prep2.RData"   , "prep3.RData"
> 
> so, would you please help me how to do it?
> 
> Many Thanks,
> Ati
> 
> On Wed, Jul 1, 2015 at 1:07 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> 
>> I have 682 variables in a data frame , and a function that  I should feed
>> 682 variables in this function one by one and each time save the file as a
>> special name!
>> for emaple:
>> my data frame file includes 682 names :
>> 1  aaa
>> 2  bbb
>> 3  dfdsfg
>> 4 fghh
>> .
>> 
>> 682 fgfhg
>> and a function like prep(Z, aaa, .....) and each time I should change the
>> variable name in this function and read the variable from the data frame
>> and each time I should save the file as a special name such as:
>> 
>> prep1<- prep(z, aaa,...)
>> prep2<- prep(z, bbb,...)
>> prep3<- prep(z, dfdsfg,..)
>> Prep4<- prep(z, fghh,...)
>> 
>> How can I use loop function in R to that?
>> 
>> Thanks
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.








From murdoch.duncan at gmail.com  Sat Jul  4 07:41:35 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 04 Jul 2015 07:41:35 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <55970BF9.400@auckland.ac.nz>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>	<55969185.5000604@auckland.ac.nz>	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
Message-ID: <5597720F.3040603@gmail.com>

On 04/07/2015 12:26 AM, Rolf Turner wrote:
> On 04/07/15 06:27, Yihui Xie wrote:
>> Sigh, how natural it is to say "This package ...", but you probably
>> don't know a package can be easily rejected by CRAN simply because of
>> this phrase "This package" (it has been clearly stated in the R-exts
>> manual).
> 
> Urrrkkkk!  I *did* "know" that, but had forgotten.  Apologies for my 
> wrong-headed suggestion.  Thanks for pointing out my error.
> 
>> I don't think the grammar is the problem here. When in doubt, I always
>> check what MASS does:
>> http://cran.rstudio.com/web/packages/MASS/index.html Turns out its
>> description is not a complete sentence, either.
>>
>> Sounds like R has become a language for statistical computing and
>> graphics, plus English grammar since 3.0.x.
> 
> The CRAN guidelines should be rewritten so that they say what they 
> *mean*.  If a complete sentence is not actually required --- and it 
> seems abundantly clear that it is not --- then guidelines should not say 
> so.  Rather they should say, clearly and comprehensibly, what actually 
> *is* required.

There's often a difference between a requirement and the test for it.
If you meet the requirement, you should pass the test, but you can often
pass the test without meeting the requirement, and then you may find
that the test is improved in a later version.  (Requirements may also be
changed, of course.)

Duncan Murdoch


From murdoch.duncan at gmail.com  Sat Jul  4 08:05:20 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 04 Jul 2015 08:05:20 +0200
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
References: <5596A3E5.1070103@uottawa.ca>
	<5596A557.2040802@uottawa.ca>	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>	<1435950099369.58224@jhu.edu>	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
	<FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
Message-ID: <559777A0.9060802@gmail.com>

On 04/07/2015 3:45 AM, David Winsemius wrote:
> 
>> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>
>>
>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
> 
> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
> 
> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )

Why deparse?  That's almost never a good idea.  I can't try your code (I
don't have mpfr available), but it would be much better to modify the
expression than the text representation of it.  For example, I think
your code would modify strings containing "pi", or variables with those
letters in them, etc.  If you used substitute(expr) without the
deparse(), you could replace the symbol "pi" with the call to the Const
function, and be more robust.

Duncan Murdoch


>             spi <- paste0("Const('pi',",prec,?)?)
>             sexpr <- gsub("pi", spi, sexpr)
>             print( eval(parse(text=sexpr))) }
> 
> # Very minimal testing
>> Pre(pi,120)
> 1 'mpfr' number of precision  120   bits 
> [1] 3.1415926535897932384626433832795028847
>> Pre(exp(pi),120)
> 1 'mpfr' number of precision  120   bits 
> [1] 23.140692632779269005729086367948547394
>> Pre(log(exp(pi)),120)
> 1 'mpfr' number of precision  120   bits 
> [1] 3.1415926535897932384626433832795028847
> 
> At the moment it still needs to ahve other numbers mpfr-ified to succeed:
> 
>> Pre(pi-4*atan(1),120)
> 1 'mpfr' number of precision  120   bits 
> [1] 1.2246467991473531772315719253130892016e-16
>> Pre(pi-4*atan(mpfr(1,120)),120)
> 1 'mpfr' number of precision  120   bits 
> [1] 0
> 
> Best;
> David.
> 
>>
> 
>> ? 
>> David
>>
>>> On Jul 3, 2015, at 12:01 PM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
>>>
>>> Thank you all.  I did think about declaring `pi' as a special constant, but for some reason didn't actually try it.  
>>> Would it be easy to have the mpfr() written such that its argument is automatically of extended precision? In other words, if I just called:  mpfr(exp(sqrt(163)*pi, 120), then all the constants, 163, pi, are automatically of 120 bits precision.
>>>
>>> Is this easy to do?
>>>
>>> Best,
>>> Ravi
>>> ________________________________________
>>> From: David Winsemius <dwinsemius at comcast.net>
>>> Sent: Friday, July 3, 2015 2:06 PM
>>> To: John Nash
>>> Cc: r-help; Ravi Varadhan
>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>>
>>>> On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
>>>>
>>>>
>>>>
>>>>
>>>> Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
>>>> traffic.
>>>>
>>>> In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
>>>> so possibly something changed enough). Trying again.
>>>>
>>>> JN
>>>>
>>>> I got the Wolfram answer as follows:
>>>>
>>>> library(Rmpfr)
>>>> n163 <- mpfr(163, 500)
>>>> n163
>>>> pi500 <- mpfr(pi, 500)
>>>> pi500
>>>> pitan <- mpfr(4, 500)*atan(mpfr(1,500))
>>>> pitan
>>>> pitan-pi500
>>>> r500 <- exp(sqrt(n163)*pitan)
>>>> r500
>>>> check <- "262537412640768743.99999999999925007259719818568887935385..."
>>>> savehistory("jnramanujan.R")
>>>>
>>>> Note that I used 4*atan(1) to get pi.
>>>
>>> RK got it right by following the example in the help page for mpfr:
>>>
>>> Const("pi", 120)
>>>
>>> The R `pi` constant is not recognized by mpfr as being anything other than another double .
>>>
>>>
>>> There are four special values that mpfr recognizes.
>>>
>>> ?
>>> Best;
>>> David
>>>
>>>
>>>> It seems that may be important,
>>>> rather than converting.
>>>>
>>>> JN
>>>>
>>>> On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
>>>>
>>>>> Message: 40
>>>>> Date: Thu, 2 Jul 2015 22:38:45 +0000
>>>>> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
>>>>> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
>>>>>    <aps6dl at yahoo.com>
>>>>> Cc: r-help <r-help at r-project.org>
>>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>>>>>    computations - using Rmpfr in R
>>>>> Message-ID:
>>>>>    <14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
>>>>> Content-Type: text/plain; charset="utf-8"
>>>>>
>>>>> Hi Rich,
>>>>>
>>>>> The Wolfram answer is correct.
>>>>> http://mathworld.wolfram.com/RamanujanConstant.html
>>>>>
>>>>> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
>>>>> http://www.wolframalpha.com/
>>>>>
>>>>> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>>>>>
>>>>> Thanks,
>>>>> Ravi
>>>>>
>>>>> -----Original Message-----
>>>>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>>>>> Sent: Thursday, July 02, 2015 6:30 PM
>>>>> To: Aditya Singh
>>>>> Cc: Ravi Varadhan; r-help
>>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>>>>
>>>>> There is a precedence error in your R attempt.  You need to convert
>>>>> 163 to 120 bits first, before taking
>>>>> its square root.
>>>>>
>>>>>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>>>>> 1 'mpfr' number of precision  120   bits
>>>>> [1] 262537412640768333.51635812597335712954
>>>>>
>>>>> ## just the last four characters to the left of the decimal point.
>>>>>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>>>>>> tmp-tmp[2]
>>>>>   baseR    Wolfram      Rmpfr wrongRmpfr
>>>>>    -488          0       -411       -907
>>>>> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
>>>>> Please check that you didn't have a similar precedence error in that code.
>>>>>
>>>>> Rich
>>>>>
>>>>>
>>>>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>>>>>> Ravi
>>>>>>>
>>>>>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>>>>>
>>>>>>> Aditya
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> ------------------------------
>>>>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>>>>
>>>>>>>>> Hi,
>>>>>>>>>
>>>>>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>>>>>
>>>>>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>>>>>
>>>>>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>>>>>> 262537412640768256.0000
>>>>>>>>>
>>>>>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>>>>>
>>>>>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>>>>>
>>>>>>>>> library(Rmpfr)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>>>>>
>>>>>>>>> [1] 262537412640767837.08771354274620169031
>>>>>>>>>
>>>>>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>>>>>
>>>>>>>>> Thank you,
>>>>>>>>> Ravi
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>    [[alternative HTML version deleted]]
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> ------------------------------
>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> David Winsemius, MD
>>> Alameda, CA, USA
>>>
>>
>> David Winsemius, MD
>> Alameda, CA, USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius, MD
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Sat Jul  4 08:21:29 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 3 Jul 2015 23:21:29 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
In-Reply-To: <559777A0.9060802@gmail.com>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
	<FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
	<559777A0.9060802@gmail.com>
Message-ID: <89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>


> On Jul 3, 2015, at 11:05 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 04/07/2015 3:45 AM, David Winsemius wrote:
>> 
>>> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>> 
>>> 
>>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
>> 
>> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
>> 
>> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
> 
> Why deparse?  That's almost never a good idea.  I can't try your code (I
> don't have mpfr available), but it would be much better to modify the
> expression than the text representation of it.  For example, I think
> your code would modify strings containing "pi", or variables with those
> letters in them, etc.  If you used substitute(expr) without the
> deparse(), you could replace the symbol "pi" with the call to the Const
> function, and be more robust.
> 

Really? I did try. I was  fairly sure that someone could do better but I don?t see an open path along the lines you suggest. I?m pretty sure I tried `substitute(expr, list(pi= pi))` when `expr` had been the formal argument and got disappointed because there is no `pi` in the expression `expr`. I _thought_ the problem was that `substitute` does not evaluate its first argument, but I do admit to be pretty much of a klutz with this sort of programming. I don?t think you need to have mpfr installed in order to demonstrate this.

 testf <- function(expr) new <- substitute(expr, list(pi = 3.121450))
 sth<-testf(exp(pi))
 sth
#expr

? 
David.

> Duncan Murdoch
> 
> 
>>            spi <- paste0("Const('pi',",prec,?)?)
>>            sexpr <- gsub("pi", spi, sexpr)
>>            print( eval(parse(text=sexpr))) }
>> 
>> # Very minimal testing
>>> Pre(pi,120)
>> 1 'mpfr' number of precision  120   bits 
>> [1] 3.1415926535897932384626433832795028847
>>> Pre(exp(pi),120)
>> 1 'mpfr' number of precision  120   bits 
>> [1] 23.140692632779269005729086367948547394
>>> Pre(log(exp(pi)),120)
>> 1 'mpfr' number of precision  120   bits 
>> [1] 3.1415926535897932384626433832795028847
>> 
>> At the moment it still needs to ahve other numbers mpfr-ified to succeed:
>> 
>>> Pre(pi-4*atan(1),120)
>> 1 'mpfr' number of precision  120   bits 
>> [1] 1.2246467991473531772315719253130892016e-16
>>> Pre(pi-4*atan(mpfr(1,120)),120)
>> 1 'mpfr' number of precision  120   bits 
>> [1] 0
>> 
>> Best;
>> David.
>> 
>>> 
>> 
>>> ? 
>>> David
>>> 
>>>> On Jul 3, 2015, at 12:01 PM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
>>>> 
>>>> Thank you all.  I did think about declaring `pi' as a special constant, but for some reason didn't actually try it.  
>>>> Would it be easy to have the mpfr() written such that its argument is automatically of extended precision? In other words, if I just called:  mpfr(exp(sqrt(163)*pi, 120), then all the constants, 163, pi, are automatically of 120 bits precision.
>>>> 
>>>> Is this easy to do?
>>>> 
>>>> Best,
>>>> Ravi
>>>> ________________________________________
>>>> From: David Winsemius <dwinsemius at comcast.net>
>>>> Sent: Friday, July 3, 2015 2:06 PM
>>>> To: John Nash
>>>> Cc: r-help; Ravi Varadhan
>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>>> 
>>>>> On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
>>>>> traffic.
>>>>> 
>>>>> In case anyone gets a duplicate, R-help bounced my msg "from non-member" (I changed server, but not email yesterday,
>>>>> so possibly something changed enough). Trying again.
>>>>> 
>>>>> JN
>>>>> 
>>>>> I got the Wolfram answer as follows:
>>>>> 
>>>>> library(Rmpfr)
>>>>> n163 <- mpfr(163, 500)
>>>>> n163
>>>>> pi500 <- mpfr(pi, 500)
>>>>> pi500
>>>>> pitan <- mpfr(4, 500)*atan(mpfr(1,500))
>>>>> pitan
>>>>> pitan-pi500
>>>>> r500 <- exp(sqrt(n163)*pitan)
>>>>> r500
>>>>> check <- "262537412640768743.99999999999925007259719818568887935385..."
>>>>> savehistory("jnramanujan.R")
>>>>> 
>>>>> Note that I used 4*atan(1) to get pi.
>>>> 
>>>> RK got it right by following the example in the help page for mpfr:
>>>> 
>>>> Const("pi", 120)
>>>> 
>>>> The R `pi` constant is not recognized by mpfr as being anything other than another double .
>>>> 
>>>> 
>>>> There are four special values that mpfr recognizes.
>>>> 
>>>> ?
>>>> Best;
>>>> David
>>>> 
>>>> 
>>>>> It seems that may be important,
>>>>> rather than converting.
>>>>> 
>>>>> JN
>>>>> 
>>>>> On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
>>>>> 
>>>>>> Message: 40
>>>>>> Date: Thu, 2 Jul 2015 22:38:45 +0000
>>>>>> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
>>>>>> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
>>>>>>   <aps6dl at yahoo.com>
>>>>>> Cc: r-help <r-help at r-project.org>
>>>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point
>>>>>>   computations - using Rmpfr in R
>>>>>> Message-ID:
>>>>>>   <14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
>>>>>> Content-Type: text/plain; charset="utf-8"
>>>>>> 
>>>>>> Hi Rich,
>>>>>> 
>>>>>> The Wolfram answer is correct.
>>>>>> http://mathworld.wolfram.com/RamanujanConstant.html
>>>>>> 
>>>>>> There is no code for Wolfram alpha.  You just go to their web engine and plug in the expression and it will give you the answer.
>>>>>> http://www.wolframalpha.com/
>>>>>> 
>>>>>> I am not sure that the precedence matters in Rmpfr.  Even if it does, the answer you get is still wrong as you showed.
>>>>>> 
>>>>>> Thanks,
>>>>>> Ravi
>>>>>> 
>>>>>> -----Original Message-----
>>>>>> From: Richard M. Heiberger [mailto:rmh at temple.edu]
>>>>>> Sent: Thursday, July 02, 2015 6:30 PM
>>>>>> To: Aditya Singh
>>>>>> Cc: Ravi Varadhan; r-help
>>>>>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>>>>> 
>>>>>> There is a precedence error in your R attempt.  You need to convert
>>>>>> 163 to 120 bits first, before taking
>>>>>> its square root.
>>>>>> 
>>>>>>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>> [1] 262537412640768333.51635812597335712954
>>>>>> 
>>>>>> ## just the last four characters to the left of the decimal point.
>>>>>>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
>>>>>>>> tmp-tmp[2]
>>>>>>  baseR    Wolfram      Rmpfr wrongRmpfr
>>>>>>   -488          0       -411       -907
>>>>>> You didn't give the Wolfram alpha code you used.  There is no way of verifying the correct value from your email.
>>>>>> Please check that you didn't have a similar precedence error in that code.
>>>>>> 
>>>>>> Rich
>>>>>> 
>>>>>> 
>>>>>> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <r-help at r-project.org> wrote:
>>>>>>>> Ravi
>>>>>>>> 
>>>>>>>> I am a chemical engineer by training. Is there not something like law of corresponding states in numerical analysis?
>>>>>>>> 
>>>>>>>> Aditya
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> ------------------------------
>>>>>>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
>>>>>>>> 
>>>>>>>>>> Hi,
>>>>>>>>>> 
>>>>>>>>>> Ramanujan supposedly discovered that the number, 163, has this interesting property that exp(sqrt(163)*pi), which is obviously a transcendental number, is real close to an integer (close to 10^(-12)).
>>>>>>>>>> 
>>>>>>>>>> If I compute this using the Wolfram alpha engine, I get:
>>>>>>>>>> 262537412640768743.99999999999925007259719818568887935385...
>>>>>>>>>> 
>>>>>>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
>>>>>>>>>> 262537412640768256.0000
>>>>>>>>>> 
>>>>>>>>>> The absolute error between the exact and R's value is 488, with a relative error of about 1.9x10^(-15).
>>>>>>>>>> 
>>>>>>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr" but I am unable to get accurate results:
>>>>>>>>>> 
>>>>>>>>>> library(Rmpfr)
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
>>>>>>>>>> 1 'mpfr' number of precision  120   bits
>>>>>>>>>> 
>>>>>>>>>> [1] 262537412640767837.08771354274620169031
>>>>>>>>>> 
>>>>>>>>>> The above answer is not only inaccurate, but it is actually worse than the answer using the usual double precision.  Any thoughts as to what I am doing wrong?
>>>>>>>>>> 
>>>>>>>>>> Thank you,
>>>>>>>>>> Ravi
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>   [[alternative HTML version deleted]]
>>>>>>>>>> 
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> ------------------------------
>>>>> 
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> David Winsemius, MD
>>>> Alameda, CA, USA
>>>> 
>>> 
>>> David Winsemius, MD
>>> Alameda, CA, USA
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius, MD
>> Alameda, CA, USA
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 

David Winsemius, MD
Alameda, CA, USA


From h.wickham at gmail.com  Sat Jul  4 08:44:21 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Sat, 4 Jul 2015 08:44:21 +0200
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
	<FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
	<559777A0.9060802@gmail.com>
	<89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>
Message-ID: <CABdHhvHN2Mad0X4Vo0LGd5=AH4w5__jQRPWzaG0fCCDzT1sj3w@mail.gmail.com>

>>>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
>>>
>>> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
>>>
>>> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
>>
>> Why deparse?  That's almost never a good idea.  I can't try your code (I
>> don't have mpfr available), but it would be much better to modify the
>> expression than the text representation of it.  For example, I think
>> your code would modify strings containing "pi", or variables with those
>> letters in them, etc.  If you used substitute(expr) without the
>> deparse(), you could replace the symbol "pi" with the call to the Const
>> function, and be more robust.
>>
>
> Really? I did try. I was  fairly sure that someone could do better but I don?t see an open path along the lines you suggest. I?m pretty sure I tried `substitute(expr, list(pi= pi))` when `expr` had been the formal argument and got disappointed because there is no `pi` in the expression `expr`. I _thought_ the problem was that `substitute` does not evaluate its first argument, but I do admit to be pretty much of a klutz with this sort of programming. I don?t think you need to have mpfr installed in order to demonstrate this.

You might want to read http://adv-r.had.co.nz/Expressions.html - it's
my best attempt at explaining how to modify call trees in R.

Hadley

-- 
http://had.co.nz/


From murdoch.duncan at gmail.com  Sat Jul  4 09:20:49 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 04 Jul 2015 09:20:49 +0200
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
	<FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
	<559777A0.9060802@gmail.com>
	<89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>
Message-ID: <55978951.1050103@gmail.com>

On 04/07/2015 8:21 AM, David Winsemius wrote:
> 
>> On Jul 3, 2015, at 11:05 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> On 04/07/2015 3:45 AM, David Winsemius wrote:
>>>
>>>> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>
>>>>
>>>>
>>>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
>>>
>>> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
>>>
>>> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
>>
>> Why deparse?  That's almost never a good idea.  I can't try your code (I
>> don't have mpfr available), but it would be much better to modify the
>> expression than the text representation of it.  For example, I think
>> your code would modify strings containing "pi", or variables with those
>> letters in them, etc.  If you used substitute(expr) without the
>> deparse(), you could replace the symbol "pi" with the call to the Const
>> function, and be more robust.
>>
> 
> Really? I did try. I was  fairly sure that someone could do better but I don?t see an open path along the lines you suggest. I?m pretty sure I tried `substitute(expr, list(pi= pi))` when `expr` had been the formal argument and got disappointed because there is no `pi` in the expression `expr`. I _thought_ the problem was that `substitute` does not evaluate its first argument, but I do admit to be pretty much of a klutz with this sort of programming. I don?t think you need to have mpfr installed in order to demonstrate this.

The substitute() function really does two different things.
substitute(expr) (with no second argument) grabs the underlying
expression out of a promise.  substitute(expr, list(pi = pi)) tries to
make the substitution in the expression "expr", so it doesn't see "pi".

This should work:

do.call(substitute, list(expr = substitute(expr), env=list(pi =
Const(pi, 120))))

(but I can't evaluate the Const function to test it).

This works:

do.call(substitute, list(expr = substitute(expr), env = list(pi =
quote(Const(pi, 120)))))

because it never evaluates the Const function, it just returns some code
that you could modify more if you liked.

Duncan Murdoch


From drjimlemon at gmail.com  Sat Jul  4 10:51:41 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 4 Jul 2015 18:51:41 +1000
Subject: [R] timePlot legend
In-Reply-To: <1435918859040-4709366.post@n4.nabble.com>
References: <1435918859040-4709366.post@n4.nabble.com>
Message-ID: <CA+8X3fVSCxwtd1mGgsQ4jd5pvmweuzmYBFOjs5QPbS8=MaqqjA@mail.gmail.com>

Hi ulasim77,
At a guess, what you want is to move the legend from beneath the plot
to the right side. As none of the examples from the "plotTime"
function seem to work, I'll have to use something simpler:

par(mar=c(5,4,4,8))
plot(1:5)
legend(5.4,3.2,c("First","Second","Third"),lty=1:3,col=1:3,pch=1:3,xpd=TRUE)

Jim


On Fri, Jul 3, 2015 at 8:20 PM, ulasim77 <ulas at envs.au.dk> wrote:
> Dear all
>
> I am plotting a time series using time Plot function. All goes well until i
> try to modify the legend by taking it from the standard location at the
> bottom, to the right side in a vertical way.
>
> How can i do this?
>
> This is my code:
>
> filename <- sprintf('%s/TS_CO_all.png',folderPLOTS)
> y_lab <- sprintf('CO (ug/m3)')
> tit <- sprintf('EU Receptors 2010')
> png(filename, width = 18 * 360, height = 9 * 360, res = 360, pointsize=240)
> timePlot(data2,pollutant=models, group=TRUE, y.relation="same", avg.time
> ="month",
>          lwd = 3, ylab = y_lab, main = tit) #pLOT THE DATA AS IS
> dev.off()
>
> Best regards
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/timePlot-legend-tp4709366.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mdsumner at gmail.com  Sat Jul  4 13:12:13 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Sat, 4 Jul 2015 21:12:13 +1000
Subject: [R] matrix -> delete last row -> list
In-Reply-To: <55970E39.2080005@auckland.ac.nz>
References: <CAABLZfrH=MqtNA_J0bsLmyMcPFJ6_jW_ce9rnA=KnXugo=6a8Q@mail.gmail.com>
	<CAM_vjuk18-E16E2=zd47++mg=SOjbfAZz2kLLXEho6TQBm0HtA@mail.gmail.com>
	<55970E39.2080005@auckland.ac.nz>
Message-ID: <CAAcGz994ZMyuB6t-2uL7=_9BLb2tWzCCQ=SB7QHwEXHNFCSTPw@mail.gmail.com>

Well 'list' in R is pretty naturally the same as R's 'atomic vector' in
scads of languages. In R the term needs special care since it's still a
'vector'.

'Degenerate dimension' is probably a helpful phrase for understanding what
is happening here.

Cheers, Mike

On Saturday, July 4, 2015, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> On 04/07/15 03:43, Sarah Goslee wrote:
>>
>> Hi,
>>
>> On Fri, Jul 3, 2015 at 10:33 AM, Zander, Joscha <joscha.zander at roche.com>
>> wrote:
>>>
>>> Good day R-community,
>>>
>>> i just wondered if it is a bug or a feature...
>>>
>>> When i have a matrix "mat" with one column and i delete the last row
with
>>>
>>> mat <- mat[-nrow(mat),] the result is a list.
>>
>> I have no idea how you're getting a list from a matrix (see below).
Perhaps
>> you mean a data frame?
>
> <SNIP>
>
> No.  The Zander person just means a vector.  Psigh.
>
> Note to the Zander person:  It is important in R programming to get your
concepts straight.  It is also important in communicating with others to
get your terminology straight.
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael Sumner
Software and Database Engineer
Australian Antarctic Division
Hobart, Australia
e-mail: mdsumner at gmail.com

	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Sat Jul  4 15:04:34 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 4 Jul 2015 06:04:34 -0700 (PDT)
Subject: [R] Lattice: set col = "black" for box.rectangle and
 box.umbrella
In-Reply-To: <000601d0b5ef$b5772540$20656fc0$@bigpond.com>
References: <alpine.LNX.2.11.1507020737190.9672@localhost>
	<CAE82007-4F1C-4D05-82C1-F2EA8E8F0CBF@comcast.net>
	<alpine.LNX.2.11.1507020954360.9672@localhost>
	<8373BA97-9C7B-4492-9A87-8B094AD73612@comcast.net>
	<alpine.LNX.2.11.1507021036250.9672@localhost>
	<alpine.LNX.2.11.1507021101040.9672@localhost>
	<CAGxFJbTeDCszet1pd+-Xy9d48Wc5a7z60PykJpL3rUxy_VEg6w@mail.gmail.com>
	<alpine.LNX.2.11.1507021150100.12452@localhost>
	<000301d0b532$904a11f0$b0de35d0$@bigpond.com>
	<alpine.LNX.2.11.1507030853140.20592@localhost>
	<000601d0b5ef$b5772540$20656fc0$@bigpond.com>
Message-ID: <alpine.LNX.2.11.1507040604010.32104@localhost>

On Sat, 4 Jul 2015, Duncan Mackay wrote:

> Just a thought
> Have you set pch colour values somewhere else?

Duncan,

   Not on purpose.

> see also https://stat.ethz.ch/pipermail/r-help/2010-March/230329.html

   Will do.

Thanks,

Rich


From pmaclean2011 at yahoo.com  Sat Jul  4 07:57:17 2015
From: pmaclean2011 at yahoo.com (Peter Maclean)
Date: Sat, 4 Jul 2015 05:57:17 +0000 (UTC)
Subject: [R] Downloading xls with active batons
Message-ID: <1551465646.1977570.1435989437753.JavaMail.yahoo@mail.yahoo.com>

I am trying to download data from here:
www.bls.gov/emp/ep_table_109.htm
I have tried using some packages (such as gdata, xlsx, XLConnext, and r2excell) that read xls files and all failed. The error is related to the file format or the presence of active batons in the header. Reading the file from row 6 does not help. I will appreciate any solution by either reading the files from the website or saving the file and loading individual files to R. I have already downloaded each individual file. I need the data to compare professional requirement (by industry) between a developing and a developed country.
       

Peter Maclean
Department of Economics
University of Dar-es-Salaam, Tanzania.


From dumboisverydumb at gmail.com  Sat Jul  4 09:36:11 2015
From: dumboisverydumb at gmail.com (Alex Kim)
Date: Sat, 4 Jul 2015 03:36:11 -0400
Subject: [R] Matrix Manipulation R
Message-ID: <CAMy2ZoN54z5=jFUo-1gMC7Oj0Roed4OrpAAMqQWWOAQXGRLhBw@mail.gmail.com>

Hi guys,

Suppose I have an extremely large data frame with 2 columns and .5 mil
rows. For example, the last 6 rows may look like this:
.
..
...
89         100
93         120
95         125
101        NA
115        NA
123        NA
124        NA

I would like to manipulate this data frame to output a data frame that
looks like:,

100        89, 93, 95
120        101, 115
125        123, 124

What would be the absolute quickest way to do this, given that there are
many rows? Currently I have this:

# m is the large two column data frame
end <- na.omit(m[,'V2']);
out <- data.frame(End=end,
Start=unname(sapply(split(m[,'V1'],findInterval(m[,'V1'],end))[as.character(0:c(length(end)-1))],paste,collapse='.')))

However this is taking a little bit too long.

Thank you for your help!

	[[alternative HTML version deleted]]


From alexkim205 at yahoo.com  Sat Jul  4 12:05:04 2015
From: alexkim205 at yahoo.com (Alex Kim)
Date: Sat, 4 Jul 2015 06:05:04 -0400
Subject: [R] Matrix Manipulation
Message-ID: <CAMy2ZoMrKc0oU6GU3d4gMjgoB6Uc9Srg8tezXEYH5RB=ZppGwg@mail.gmail.com>

Hi guys,

Suppose I have an extremely large data frame with 2 columns and .5 mil
rows. For example, the last 6 rows may look like this:
.
..
...
89         100
93         120
95         125
101        NA
115        NA
123        NA
124        NA

I would like to manipulate this data frame to output a data frame that
looks like:,

100        89, 93, 95
120        101, 115
125        123, 124

What would be the absolute quickest way to do this, given that there are
many rows? Currently I have this:

# m is the large two column data frame
end <- na.omit(m[,'V2']);
out <- data.frame(End=end,
Start=unname(sapply(split(m[,'V1'],findInterval(m[,'V1'],end))[as.character(0:c(length(end)-1))],paste,collapse='.')))

However this is taking a little bit too long.

Thank you for your help!

	[[alternative HTML version deleted]]


From dumboisverydumb at gmail.com  Sat Jul  4 12:09:39 2015
From: dumboisverydumb at gmail.com (Alex Kim)
Date: Sat, 4 Jul 2015 06:09:39 -0400
Subject: [R] Matrix Manipulation R
Message-ID: <CAMy2ZoMBNLpv7c2LEQd6kfQEzabFLUtOtfQPsM_2UbJ9KSScVg@mail.gmail.com>

Hi guys,

Suppose I have an extremely large data frame with 2 columns and .5 mil
rows. For example, the last 6 rows may look like this:
.
..
...
89         100
93         120
95         125
101        NA
115        NA
123        NA
124        NA

I would like to manipulate this data frame to output a data frame that
looks like:,

100        89, 93, 95
120        101, 115
125        123, 124

What would be the absolute quickest way to do this, given that there are
many rows? Currently I have this:

# m is the large two column data frame
end <- na.omit(m[,'V2']);
out <- data.frame(End=end,
Start=unname(sapply(split(m[,'V1'],findInterval(m[,'V1'],end))[as.character(0:c(length(end)-1))],paste,collapse='.')))

However this is taking a little bit too long.

Thank you for your help!

	[[alternative HTML version deleted]]


From mxkuhn at gmail.com  Sat Jul  4 15:09:10 2015
From: mxkuhn at gmail.com (mxkuhn)
Date: Sat, 4 Jul 2015 09:09:10 -0400
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <5597720F.3040603@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz> <5597720F.3040603@gmail.com>
Message-ID: <C63266F7-E9E2-41D2-85FF-8A94C553DD18@gmail.com>

I encountered this a few months ago and, in my case, the sentence had a noun and verb but lacked a period at the end of the sentence. I tested that 'blah blah blah.' would have passed in that version of R-devel. 

Whenever I find a new rule or test with R CMD check, I tell myself that it must be there because of some previous issue, i.e. they probably had a good reason. I can't imagine what damage an incomplete sentence caused beyond a bruised aura. 

> On Jul 4, 2015, at 1:41 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
>> On 04/07/2015 12:26 AM, Rolf Turner wrote:
>>> On 04/07/15 06:27, Yihui Xie wrote:
>>> Sigh, how natural it is to say "This package ...", but you probably
>>> don't know a package can be easily rejected by CRAN simply because of
>>> this phrase "This package" (it has been clearly stated in the R-exts
>>> manual).
>> 
>> Urrrkkkk!  I *did* "know" that, but had forgotten.  Apologies for my 
>> wrong-headed suggestion.  Thanks for pointing out my error.
>> 
>>> I don't think the grammar is the problem here. When in doubt, I always
>>> check what MASS does:
>>> http://cran.rstudio.com/web/packages/MASS/index.html Turns out its
>>> description is not a complete sentence, either.
>>> 
>>> Sounds like R has become a language for statistical computing and
>>> graphics, plus English grammar since 3.0.x.
>> 
>> The CRAN guidelines should be rewritten so that they say what they 
>> *mean*.  If a complete sentence is not actually required --- and it 
>> seems abundantly clear that it is not --- then guidelines should not say 
>> so.  Rather they should say, clearly and comprehensibly, what actually 
>> *is* required.
> 
> There's often a difference between a requirement and the test for it.
> If you meet the requirement, you should pass the test, but you can often
> pass the test without meeting the requirement, and then you may find
> that the test is improved in a later version.  (Requirements may also be
> changed, of course.)
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ggrothendieck at gmail.com  Sat Jul  4 16:42:43 2015
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 4 Jul 2015 10:42:43 -0400
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <1435950099369.58224@jhu.edu>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
Message-ID: <CAP01uRkbujT5LUoiFVFnFxfsqrLBHguKC-i3ExNt5sWtPGhLig@mail.gmail.com>

You can do that with bc if you pass the entire expression to bc(...) in
quotes but in that case you will have to use bc notation, not R notation
so, for example, exp is e and atan is a.

> library(bc)
> bc("e(sqrt(163)*4*a(1))")
[1]
"262537412640768743.9999999999992500725971981856888793538563373369908627075374103782106479101186073116295306145602054347"


On Fri, Jul 3, 2015 at 3:01 PM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:

> Thank you all.  I did think about declaring `pi' as a special constant,
> but for some reason didn't actually try it.
> Would it be easy to have the mpfr() written such that its argument is
> automatically of extended precision? In other words, if I just called:
> mpfr(exp(sqrt(163)*pi, 120), then all the constants, 163, pi, are
> automatically of 120 bits precision.
>
> Is this easy to do?
>
> Best,
> Ravi
>  ________________________________________
> From: David Winsemius <dwinsemius at comcast.net>
> Sent: Friday, July 3, 2015 2:06 PM
> To: John Nash
> Cc: r-help; Ravi Varadhan
> Subject: Re: [R] : Ramanujan and the accuracy of floating point
> computations - using Rmpfr in R
>
> > On Jul 3, 2015, at 8:08 AM, John Nash <John.Nash at uottawa.ca> wrote:
> >
> >
> >
> >
> > Third try -- I unsubscribed and re-subscribed. Sorry to Ravi for extra
> > traffic.
> >
> > In case anyone gets a duplicate, R-help bounced my msg "from non-member"
> (I changed server, but not email yesterday,
> > so possibly something changed enough). Trying again.
> >
> > JN
> >
> > I got the Wolfram answer as follows:
> >
> > library(Rmpfr)
> > n163 <- mpfr(163, 500)
> > n163
> > pi500 <- mpfr(pi, 500)
> > pi500
> > pitan <- mpfr(4, 500)*atan(mpfr(1,500))
> > pitan
> > pitan-pi500
> > r500 <- exp(sqrt(n163)*pitan)
> > r500
> > check <- "262537412640768743.99999999999925007259719818568887935385..."
> > savehistory("jnramanujan.R")
> >
> > Note that I used 4*atan(1) to get pi.
>
> RK got it right by following the example in the help page for mpfr:
>
> Const("pi", 120)
>
> The R `pi` constant is not recognized by mpfr as being anything other than
> another double .
>
>
> There are four special values that mpfr recognizes.
>
> ?
> Best;
> David
>
>
> > It seems that may be important,
> > rather than converting.
> >
> > JN
> >
> > On 15-07-03 06:00 AM, r-help-request at r-project.org wrote:
> >
> >> Message: 40
> >> Date: Thu, 2 Jul 2015 22:38:45 +0000
> >> From: Ravi Varadhan <ravi.varadhan at jhu.edu>
> >> To: "'Richard M. Heiberger'" <rmh at temple.edu>, Aditya Singh
> >>      <aps6dl at yahoo.com>
> >> Cc: r-help <r-help at r-project.org>
> >> Subject: Re: [R] : Ramanujan and the accuracy of floating point
> >>      computations - using Rmpfr in R
> >> Message-ID:
> >>      <14ad39aaf6a542849bbf3f62a0c2f38f at DOM-EB1-2013.win.ad.jhu.edu>
> >> Content-Type: text/plain; charset="utf-8"
> >>
> >> Hi Rich,
> >>
> >> The Wolfram answer is correct.
> >> http://mathworld.wolfram.com/RamanujanConstant.html
> >>
> >> There is no code for Wolfram alpha.  You just go to their web engine
> and plug in the expression and it will give you the answer.
> >> http://www.wolframalpha.com/
> >>
> >> I am not sure that the precedence matters in Rmpfr.  Even if it does,
> the answer you get is still wrong as you showed.
> >>
> >> Thanks,
> >> Ravi
> >>
> >> -----Original Message-----
> >> From: Richard M. Heiberger [mailto:rmh at temple.edu]
> >> Sent: Thursday, July 02, 2015 6:30 PM
> >> To: Aditya Singh
> >> Cc: Ravi Varadhan; r-help
> >> Subject: Re: [R] : Ramanujan and the accuracy of floating point
> computations - using Rmpfr in R
> >>
> >> There is a precedence error in your R attempt.  You need to convert
> >> 163 to 120 bits first, before taking
> >> its square root.
> >>
> >>>> exp(sqrt(mpfr(163, 120)) * mpfr(pi, 120))
> >> 1 'mpfr' number of precision  120   bits
> >> [1] 262537412640768333.51635812597335712954
> >>
> >> ## just the last four characters to the left of the decimal point.
> >>>> tmp <- c(baseR=8256, Wolfram=8744, Rmpfr=8333, wrongRmpfr=7837)
> >>>> tmp-tmp[2]
> >>     baseR    Wolfram      Rmpfr wrongRmpfr
> >>      -488          0       -411       -907
> >> You didn't give the Wolfram alpha code you used.  There is no way of
> verifying the correct value from your email.
> >> Please check that you didn't have a similar precedence error in that
> code.
> >>
> >> Rich
> >>
> >>
> >> On Thu, Jul 2, 2015 at 2:02 PM, Aditya Singh via R-help <
> r-help at r-project.org> wrote:
> >>>> Ravi
> >>>>
> >>>> I am a chemical engineer by training. Is there not something like law
> of corresponding states in numerical analysis?
> >>>>
> >>>> Aditya
> >>>>
> >>>>
> >>>>
> >>>> ------------------------------
> >>>> On Thu 2 Jul, 2015 7:28 AM PDT Ravi Varadhan wrote:
> >>>>
> >>>>>> Hi,
> >>>>>>
> >>>>>> Ramanujan supposedly discovered that the number, 163, has this
> interesting property that exp(sqrt(163)*pi), which is obviously a
> transcendental number, is real close to an integer (close to 10^(-12)).
> >>>>>>
> >>>>>> If I compute this using the Wolfram alpha engine, I get:
> >>>>>> 262537412640768743.99999999999925007259719818568887935385...
> >>>>>>
> >>>>>> When I do this in R 3.1.1 (64-bit windows), I get:
> >>>>>> 262537412640768256.0000
> >>>>>>
> >>>>>> The absolute error between the exact and R's value is 488, with a
> relative error of about 1.9x10^(-15).
> >>>>>>
> >>>>>> In order to replicate Wolfram Alpha, I tried doing this in "Rmfpr"
> but I am unable to get accurate results:
> >>>>>>
> >>>>>> library(Rmpfr)
> >>>>>>
> >>>>>>
> >>>>>>>> exp(sqrt(163) * mpfr(pi, 120))
> >>>>>> 1 'mpfr' number of precision  120   bits
> >>>>>>
> >>>>>> [1] 262537412640767837.08771354274620169031
> >>>>>>
> >>>>>> The above answer is not only inaccurate, but it is actually worse
> than the answer using the usual double precision.  Any thoughts as to what
> I am doing wrong?
> >>>>>>
> >>>>>> Thank you,
> >>>>>> Ravi
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>>      [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >> ------------------------------
> >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius, MD
> Alameda, CA, USA
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Sat Jul  4 19:12:07 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 4 Jul 2015 10:12:07 -0700
Subject: [R] : Ramanujan and the accuracy of floating point computations
	- using Rmpfr in R
In-Reply-To: <55978951.1050103@gmail.com>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
	<FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
	<559777A0.9060802@gmail.com>
	<89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>
	<55978951.1050103@gmail.com>
Message-ID: <8071107C-C766-4A1C-9B22-6F43F48F1F32@comcast.net>


> On Jul 4, 2015, at 12:20 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 04/07/2015 8:21 AM, David Winsemius wrote:
>> 
>>> On Jul 3, 2015, at 11:05 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>> 
>>> On 04/07/2015 3:45 AM, David Winsemius wrote:
>>>> 
>>>>> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>> 
>>>>> 
>>>>> 
>>>>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
>>>> 
>>>> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
>>>> 
>>>> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
>>> 
>>> Why deparse?  That's almost never a good idea.  I can't try your code (I
>>> don't have mpfr available), but it would be much better to modify the
>>> expression than the text representation of it.  For example, I think
>>> your code would modify strings containing "pi", or variables with those
>>> letters in them, etc.  If you used substitute(expr) without the
>>> deparse(), you could replace the symbol "pi" with the call to the Const
>>> function, and be more robust.
>>> 
>> 
>> Really? I did try. I was  fairly sure that someone could do better but I don?t see an open path along the lines you suggest. I?m pretty sure I tried `substitute(expr, list(pi= pi))` when `expr` had been the formal argument and got disappointed because there is no `pi` in the expression `expr`. I _thought_ the problem was that `substitute` does not evaluate its first argument, but I do admit to be pretty much of a klutz with this sort of programming. I don?t think you need to have mpfr installed in order to demonstrate this.
> 
> The substitute() function really does two different things.
> substitute(expr) (with no second argument) grabs the underlying
> expression out of a promise.  substitute(expr, list(pi = pi)) tries to
> make the substitution in the expression "expr", so it doesn't see "pi?.

Thank you. That was really helpful. I hope it ?sticks? to  sufficiently durable set of neurons.
> 

> This should work:
> 
> do.call(substitute, list(expr = substitute(expr), env=list(pi =
> Const(pi, 120))))
> 
> (but I can't evaluate the Const function to test it).

The expression `pi` as the argument to Const only needed to be character()-ized and then evaluation on that result succeeded:

library(mpfr)
#  Pre <- function(expr, prec){ do.call(substitute, 
                                        list(expr = substitute(expr),
                                             env=list(pi = Const(pi, prec)))) }

> Pre(exp(pi),120)
Error in Const(pi, prec) : 
  'name' must be one of 'pi', 'gamma', 'catalan', 'log2'


 Pre <- function(expr, prec){ do.call(substitute, 
                                     list(expr = substitute(expr), 
                                          env=list(pi = Const('pi', prec)))) }

> Pre(exp(pi),120)
exp(list(<S4 object of class "mpfr1">))
> eval( Pre(exp(pi),120) )
1 'mpfr' number of precision  120   bits 
[1] 23.140692632779269005729086367948547394

So the next step for delivering Ravi?s request would be to add the rest of the ?Const? options:

Pre <- function(expr, prec){ do.call(substitute, 
                                     list(expr = substitute(expr), 
                                         env=list(pi = Const('pi', prec),
                                         catalan=Const('catalan', prec), 
                                         log2=Const('log2', prec), 
                                         gamma=Const('gamma', prec)))) }


> eval(Pre(exp(gamma)))
Error in stopifnot(is.numeric(prec)) : 
  argument "prec" is missing, with no default
> eval(Pre(exp(gamma), 120))
1 'mpfr' number of precision  120   bits 
[1] 1.781072417990197985236504103107179549

> 
> This works:
> 
> do.call(substitute, list(expr = substitute(expr), env = list(pi =
> quote(Const(pi, 120)))))
> 
> because it never evaluates the Const function, it just returns some code
> that you could modify more if you liked.
> 
> Duncan Murdoch

David Winsemius, MD
Alameda, CA, USA


From dwinsemius at comcast.net  Sat Jul  4 18:40:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 4 Jul 2015 09:40:53 -0700
Subject: [R] Matrix Manipulation R
In-Reply-To: <CAMy2ZoMBNLpv7c2LEQd6kfQEzabFLUtOtfQPsM_2UbJ9KSScVg@mail.gmail.com>
References: <CAMy2ZoMBNLpv7c2LEQd6kfQEzabFLUtOtfQPsM_2UbJ9KSScVg@mail.gmail.com>
Message-ID: <AB8DBD47-D2DC-4ED5-B2DB-9306BA46CB21@comcast.net>


> On Jul 4, 2015, at 3:09 AM, Alex Kim <dumboisverydumb at gmail.com> wrote:
> 
> Hi guys,
> 
> Suppose I have an extremely large data frame with 2 columns and .5 mil
> rows. For example, the last 6 rows may look like this:
> .
> ..
> ...
> 89         100
> 93         120
> 95         125
> 101        NA
> 115        NA
> 123        NA
> 124        NA
> 
> I would like to manipulate this data frame to output a data frame that
> looks like:,
> 
> 100        89, 93, 95
> 120        101, 115
> 125        123, 124
> 

> What would be the absolute quickest way to do this, given that there are
> many rows? Currently I have this:
> 
> # m is the large two column data frame
> end <- na.omit(m[,'V2']);
> out <- data.frame(End=end,
> Start=unname(sapply(split(m[,'V1'],findInterval(m[,'V1'],end))[as.character(0:c(length(end)-1))],paste,collapse='.')))
> 

This might be a little faster. It skips some of the steps in your version:

 dput(m)
structure(list(V1 = c(89, 93, 95, 101, 115, 123, 124), V2 = c(100, 
120, 125, NA, NA, NA, NA)), .Names = c("V1", "V2"), row.names = c(NA, 
-7L), class = "data.frame")

end <- na.omit(m[,'V2?])
# this will only work if that vector is sorted
data.frame(End = end,
           Start = sapply( split( m$V1, 
                                 findInterval(m$V1, c(-Inf, end))), 
                          paste,collapse="," ) )
  End    Start
1 100 89,93,95
2 120  101,115
3 125  123,124


> However this is taking a little bit too long.
> 
> Thank you for your help!
> 
> 	[[alternative HTML version deleted]]

This is a plain-text mailing list and posting triplicate questions is poor form.

Do read the posting guide.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

? 
David Winsemius, MD
Alameda, CA, USA


From r.turner at auckland.ac.nz  Sun Jul  5 00:55:15 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 5 Jul 2015 10:55:15 +1200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <C63266F7-E9E2-41D2-85FF-8A94C553DD18@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz> <5597720F.3040603@gmail.com>
	<C63266F7-E9E2-41D2-85FF-8A94C553DD18@gmail.com>
Message-ID: <55986453.4060306@auckland.ac.nz>


On 05/07/15 01:09, mxkuhn wrote:

<SNIP>

> Whenever I find a new rule or test with R CMD check, I tell myself
> that it must be there because of some previous issue, i.e. they
> probably had a good reason. I can't imagine what damage an incomplete
> sentence caused beyond a bruised aura.

Fortune nomination.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ravi.varadhan at jhu.edu  Sat Jul  4 23:10:58 2015
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Sat, 4 Jul 2015 21:10:58 +0000
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <8071107C-C766-4A1C-9B22-6F43F48F1F32@comcast.net>
References: <5596A3E5.1070103@uottawa.ca> <5596A557.2040802@uottawa.ca>
	<1F8D736E-D8A4-49CF-9B90-44954344C90E@comcast.net>
	<1435950099369.58224@jhu.edu>
	<3959F4E8-F51E-4D37-9D02-28310E356FD7@comcast.net>
	<FCFF6EB8-7264-4260-8CEC-D942B980964F@comcast.net>
	<559777A0.9060802@gmail.com>
	<89B6E516-41CF-4A56-82A4-CD4EFA613192@comcast.net>
	<55978951.1050103@gmail.com>,
	<8071107C-C766-4A1C-9B22-6F43F48F1F32@comcast.net>
Message-ID: <1436044246163.29398@jhu.edu>


What about numeric constants, like `163'?  

eval(Pre(exp(sqrt(163)*pi), 120))    does not work.

Thanks,
Ravi
________________________________________
From: David Winsemius <dwinsemius at comcast.net>
Sent: Saturday, July 4, 2015 1:12 PM
To: Duncan Murdoch
Cc: r-help; John Nash; Ravi Varadhan
Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R

> On Jul 4, 2015, at 12:20 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> On 04/07/2015 8:21 AM, David Winsemius wrote:
>>
>>> On Jul 3, 2015, at 11:05 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>>
>>> On 04/07/2015 3:45 AM, David Winsemius wrote:
>>>>
>>>>> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>
>>>>>
>>>>>
>>>>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
>>>>
>>>> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
>>>>
>>>> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
>>>
>>> Why deparse?  That's almost never a good idea.  I can't try your code (I
>>> don't have mpfr available), but it would be much better to modify the
>>> expression than the text representation of it.  For example, I think
>>> your code would modify strings containing "pi", or variables with those
>>> letters in them, etc.  If you used substitute(expr) without the
>>> deparse(), you could replace the symbol "pi" with the call to the Const
>>> function, and be more robust.
>>>
>>
>> Really? I did try. I was  fairly sure that someone could do better but I don?t see an open path along the lines you suggest. I?m pretty sure I tried `substitute(expr, list(pi= pi))` when `expr` had been the formal argument and got disappointed because there is no `pi` in the expression `expr`. I _thought_ the problem was that `substitute` does not evaluate its first argument, but I do admit to be pretty much of a klutz with this sort of programming. I don?t think you need to have mpfr installed in order to demonstrate this.
>
> The substitute() function really does two different things.
> substitute(expr) (with no second argument) grabs the underlying
> expression out of a promise.  substitute(expr, list(pi = pi)) tries to
> make the substitution in the expression "expr", so it doesn't see "pi?.

Thank you. That was really helpful. I hope it ?sticks? to  sufficiently durable set of neurons.
>

> This should work:
>
> do.call(substitute, list(expr = substitute(expr), env=list(pi =
> Const(pi, 120))))
>
> (but I can't evaluate the Const function to test it).

The expression `pi` as the argument to Const only needed to be character()-ized and then evaluation on that result succeeded:

library(mpfr)
#  Pre <- function(expr, prec){ do.call(substitute,
                                        list(expr = substitute(expr),
                                             env=list(pi = Const(pi, prec)))) }

> Pre(exp(pi),120)
Error in Const(pi, prec) :
  'name' must be one of 'pi', 'gamma', 'catalan', 'log2'


 Pre <- function(expr, prec){ do.call(substitute,
                                     list(expr = substitute(expr),
                                          env=list(pi = Const('pi', prec)))) }

> Pre(exp(pi),120)
exp(list(<S4 object of class "mpfr1">))
> eval( Pre(exp(pi),120) )
1 'mpfr' number of precision  120   bits
[1] 23.140692632779269005729086367948547394

So the next step for delivering Ravi?s request would be to add the rest of the ?Const? options:

Pre <- function(expr, prec){ do.call(substitute,
                                     list(expr = substitute(expr),
                                         env=list(pi = Const('pi', prec),
                                         catalan=Const('catalan', prec),
                                         log2=Const('log2', prec),
                                         gamma=Const('gamma', prec)))) }


> eval(Pre(exp(gamma)))
Error in stopifnot(is.numeric(prec)) :
  argument "prec" is missing, with no default
> eval(Pre(exp(gamma), 120))
1 'mpfr' number of precision  120   bits
[1] 1.781072417990197985236504103107179549

>
> This works:
>
> do.call(substitute, list(expr = substitute(expr), env = list(pi =
> quote(Const(pi, 120)))))
>
> because it never evaluates the Const function, it just returns some code
> that you could modify more if you liked.
>
> Duncan Murdoch

David Winsemius, MD
Alameda, CA, USA



From naresh_gurbuxani at hotmail.com  Sun Jul  5 03:53:56 2015
From: naresh_gurbuxani at hotmail.com (Naresh Gurbuxani)
Date: Sat, 4 Jul 2015 21:53:56 -0400
Subject: [R] lattice strip.custom function
Message-ID: <SNT150-W680E18249C8838126D9C44FA940@phx.gbl>

I would like to use a mapping to name panel strips.  Directly using mapping table does not work.  What is the problem here?
my.df <- data.frame(x = rnorm(100, mean = 0, sd = 2), name = "A")
my.df <- rbind(my.df, data.frame(x = rnorm(100, mean = 0, sd = 4), name = "B"))
my.df <- rbind(my.df, data.frame(x = rt(100, 3), name = "C"))
my.df <- rbind(my.df, data.frame(x = runif(100, min = -3, max = 3), name = "D"))


library(lattice)
histogram(~x | name, data = my.df)
name.dist <- data.frame(name = c("A", "B", "C", "D"), dist = c("normal", "normal", "t", "uniform"))
dist.names <- name.dist$dist

# Below code workshistogram(~x | name, data = my.df, strip = strip.custom(factor.levels = c("normal", "normal", "t", "uniform")))
# Below code does not workhistogram(~x | name, data = my.df, strip = strip.custom(factor.levels = dist.names)) 		 	   		  
	[[alternative HTML version deleted]]


From profjcnash at gmail.com  Sun Jul  5 03:42:27 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Sat, 04 Jul 2015 21:42:27 -0400
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <559883DA.8090209@gmail.com>
References: <559883DA.8090209@gmail.com>
Message-ID: <55988B83.1050805@gmail.com>


n163 <- mpfr(163, 500)

is how I set up the number.

JN


On 15-07-04 05:10 PM, Ravi Varadhan wrote:
>> What about numeric constants, like `163'?  
>>
>> eval(Pre(exp(sqrt(163)*pi), 120))    does not work.
>>
>> Thanks,
>> Ravi
>> ________________________________________
>> From: David Winsemius <dwinsemius at comcast.net>
>> Sent: Saturday, July 4, 2015 1:12 PM
>> To: Duncan Murdoch
>> Cc: r-help; John Nash; Ravi Varadhan
>> Subject: Re: [R] : Ramanujan and the accuracy of floating point computations - using Rmpfr in R
>>
>>> On Jul 4, 2015, at 12:20 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>>
>>> On 04/07/2015 8:21 AM, David Winsemius wrote:
>>>>> On Jul 3, 2015, at 11:05 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>>>>
>>>>> On 04/07/2015 3:45 AM, David Winsemius wrote:
>>>>>>> On Jul 3, 2015, at 5:08 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> It doesn?t appear to me that mpfr was ever designed to handle expressions as the first argument.
>>>>>> This could be a start. Obviously one would wnat to include code to do other substitutions probably using the all.vars function to pull out the other ?constants? and ?numeric? values to make them of equivalent precision. I?m guessing you want to follow the parse-tree and then testing the numbers for integer-ness and then replacing by paste0( ?mpfr(?, val, ?L, ?, prec,?)? )
>>>>>>
>>>>>> Pre <- function(expr, prec){ sexpr <- deparse(substitute(expr) )
>>>>> Why deparse?  That's almost never a good idea.  I can't try your code (I
>>>>> don't have mpfr available), but it would be much better to modify the
>>>>> expression than the text representation of it.  For example, I think
>>>>> your code would modify strings containing "pi", or variables with those
>>>>> letters in them, etc.  If you used substitute(expr) without the
>>>>> deparse(), you could replace the symbol "pi" with the call to the Const
>>>>> function, and be more robust.
>>>>>
>>>> Really? I did try. I was  fairly sure that someone could do better but I don?t see an open path along the lines you suggest. I?m pretty sure I tried `substitute(expr, list(pi= pi))` when `expr` had been the formal argument and got disappointed because there is no `pi` in the expression `expr`. I _thought_ the problem was that `substitute` does not evaluate its first argument, but I do admit to be pretty much of a klutz with this sort of programming. I don?t think you need to have mpfr installed in order to demonstrate this.
>>> The substitute() function really does two different things.
>>> substitute(expr) (with no second argument) grabs the underlying
>>> expression out of a promise.  substitute(expr, list(pi = pi)) tries to
>>> make the substitution in the expression "expr", so it doesn't see "pi?.
>> Thank you. That was really helpful. I hope it ?sticks? to  sufficiently durable set of neurons.
>>> This should work:
>>>
>>> do.call(substitute, list(expr = substitute(expr), env=list(pi =
>>> Const(pi, 120))))
>>>
>>> (but I can't evaluate the Const function to test it).
>> The expression `pi` as the argument to Const only needed to be character()-ized and then evaluation on that result succeeded:
>>
>> library(mpfr)
>> #  Pre <- function(expr, prec){ do.call(substitute,
>>                                         list(expr = substitute(expr),
>>                                              env=list(pi = Const(pi, prec)))) }
>>
>>> Pre(exp(pi),120)
>> Error in Const(pi, prec) :
>>   'name' must be one of 'pi', 'gamma', 'catalan', 'log2'
>>
>>
>>  Pre <- function(expr, prec){ do.call(substitute,
>>                                      list(expr = substitute(expr),
>>                                           env=list(pi = Const('pi', prec)))) }
>>
>>> Pre(exp(pi),120)
>> exp(list(<S4 object of class "mpfr1">))
>>> eval( Pre(exp(pi),120) )
>> 1 'mpfr' number of precision  120   bits
>> [1] 23.140692632779269005729086367948547394
>>
>> So the next step for delivering Ravi?s request would be to add the rest of the ?Const? options:
>>
>> Pre <- function(expr, prec){ do.call(substitute,
>>                                      list(expr = substitute(expr),
>>                                          env=list(pi = Const('pi', prec),
>>                                          catalan=Const('catalan', prec),
>>                                          log2=Const('log2', prec),
>>                                          gamma=Const('gamma', prec)))) }
>>
>>
>>> eval(Pre(exp(gamma)))
>> Error in stopifnot(is.numeric(prec)) :
>>   argument "prec" is missing, with no default
>>> eval(Pre(exp(gamma), 120))
>> 1 'mpfr' number of precision  120   bits
>> [1] 1.781072417990197985236504103107179549
>>
>>> This works:
>>>
>>> do.call(substitute, list(expr = substitute(expr), env = list(pi =
>>> quote(Const(pi, 120)))))
>>>
>>> because it never evaluates the Const function, it just returns some code
>>> that you could modify more if you liked.
>>>
>>> Duncan Murdoch
>> David Winsemius, MD
>> Alameda, CA, USA
>>


From bgunter.4567 at gmail.com  Sun Jul  5 06:31:30 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 4 Jul 2015 21:31:30 -0700
Subject: [R] lattice strip.custom function
In-Reply-To: <SNT150-W680E18249C8838126D9C44FA940@phx.gbl>
References: <SNT150-W680E18249C8838126D9C44FA940@phx.gbl>
Message-ID: <CAGxFJbQHY5JNhKfTg_9xpiadfM3LVK9A1zWY3fBmaAS90Z3K1A@mail.gmail.com>

1. Please do not post in HTML. This is a plain text list and it can get mangled.

2. Thank you for the example data and code. The problem is -- see the
Help for strip.custom()  -- that strip.custom() wants a character
vector or expression for the factor.levels argument. But...

> str(dist.names)
 Factor w/ 3 levels "normal","t","uniform": 1 1 2 3

So dist.names is a factor, not a character vector. (I presume you know
the difference! If not, you need to do your homework and read an R
tutorial or two). Thus, this works:

histogram(~x | name, data = my.df,
      strip = strip.custom(factor.levels=as.character(dist.names)))

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 4, 2015 at 6:53 PM, Naresh Gurbuxani
<naresh_gurbuxani at hotmail.com> wrote:
> I would like to use a mapping to name panel strips.  Directly using mapping table does not work.  What is the problem here?
> my.df <- data.frame(x = rnorm(100, mean = 0, sd = 2), name = "A")
> my.df <- rbind(my.df, data.frame(x = rnorm(100, mean = 0, sd = 4), name = "B"))
> my.df <- rbind(my.df, data.frame(x = rt(100, 3), name = "C"))
> my.df <- rbind(my.df, data.frame(x = runif(100, min = -3, max = 3), name = "D"))
>
>
> library(lattice)
> histogram(~x | name, data = my.df)
> name.dist <- data.frame(name = c("A", "B", "C", "D"), dist = c("normal", "normal", "t", "uniform"))
> dist.names <- name.dist$dist
>
> # Below code workshistogram(~x | name, data = my.df, strip = strip.custom(factor.levels = c("normal", "normal", "t", "uniform")))
> # Below code does not workhistogram(~x | name, data = my.df, strip = strip.custom(factor.levels = dist.names))
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jwd at surewest.net  Sun Jul  5 11:13:42 2015
From: jwd at surewest.net (jwd)
Date: Sun, 5 Jul 2015 02:13:42 -0700
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
Message-ID: <20150705021342.24d76d70@Draco.site>

On Fri, 3 Jul 2015 11:09:54 +0300
Federico Calboli <federico.calboli at helsinki.fi> wrote:

> Hi All,
> 
> I am upgrading a package for CRAN, and I get this note:
> 
> checking DESCRIPTION meta-information ... NOTE
> Malformed Description field: should contain one or more complete
> sentences.
> 
> This is puzzling because:
> 
> cat DESCRIPTION
> 
> ...
> Description: Functions designed to test for single gene/phenotype
> association and for pleiotropy on genetic and genomic data. ...
> 
> In my understanding "Functions designed to test for single
> gene/phenotype association and for pleiotropy on genetic and genomic
> data.? *is* a complete sentence.  So, what is complete sentence in
> the opinion of whomever coded that check?
> 
> Best
> 
> F

Well, no.  It isn't a complete sentence which requires a subject, and
object and a verb.  However, there are not many package Descriptions
that ARE complete sentences.  Some could be reworded awkwardly into a
functional sentence.  You could for instance try:

 "[Library/Package name] is a collection of functions designed to test
 for pleiotropy on genetic and genomic data."

That is a complete sentence as it contains a form of the
verb "to be," to whit "is."  By using the package name you also avoid
the stricture against "The package consists ..."

JWdougherty


From pdalgd at gmail.com  Sun Jul  5 12:47:21 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 5 Jul 2015 12:47:21 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <20150705021342.24d76d70@Draco.site>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<20150705021342.24d76d70@Draco.site>
Message-ID: <8DEC59E0-9EA7-4954-82B8-0F3880E8192C@gmail.com>

A couple of pointers:

(a) R-package-devel exists and is right ------> thataway
(b) R is open source, and QC.R is pretty easy to find. Reading the source, it appears that the check is only for whether the Description: field is terminated by punctuation, possibly followed by a quote.
(c) I'd try read.dcf("DESCRIPTION")[,"Description"] and check that it contains what you think it would contain.

-pd

> On 05 Jul 2015, at 11:13 , jwd <jwd at surewest.net> wrote:
> 
> On Fri, 3 Jul 2015 11:09:54 +0300
> Federico Calboli <federico.calboli at helsinki.fi> wrote:
> 
>> Hi All,
>> 
>> I am upgrading a package for CRAN, and I get this note:
>> 
>> checking DESCRIPTION meta-information ... NOTE
>> Malformed Description field: should contain one or more complete
>> sentences.
>> 
>> This is puzzling because:
>> 
>> cat DESCRIPTION
>> 
>> ...
>> Description: Functions designed to test for single gene/phenotype
>> association and for pleiotropy on genetic and genomic data. ...
>> 
>> In my understanding "Functions designed to test for single
>> gene/phenotype association and for pleiotropy on genetic and genomic
>> data.? *is* a complete sentence.  So, what is complete sentence in
>> the opinion of whomever coded that check?
>> 
>> Best
>> 
>> F
> 
> Well, no.  It isn't a complete sentence which requires a subject, and
> object and a verb.  However, there are not many package Descriptions
> that ARE complete sentences.  Some could be reworded awkwardly into a
> functional sentence.  You could for instance try:
> 
> "[Library/Package name] is a collection of functions designed to test
> for pleiotropy on genetic and genomic data."
> 
> That is a complete sentence as it contains a form of the
> verb "to be," to whit "is."  By using the package name you also avoid
> the stricture against "The package consists ..."
> 
> JWdougherty
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From JSorkin at grecc.umaryland.edu  Sun Jul  5 21:29:18 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 05 Jul 2015 15:29:18 -0400
Subject: [R] Greek letters in column titles.
Message-ID: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>

I would like to include a Greek letter in a column title, viz.  pB where B should be the Greek letter beta, but I don't know how to do this. I know about dimnames, but I don't know how to create the Greek letter.
Thank you,
John

John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 


Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From istazahn at gmail.com  Sun Jul  5 22:37:07 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Sun, 5 Jul 2015 16:37:07 -0400
Subject: [R] Greek letters in column titles.
In-Reply-To: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>
References: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>
Message-ID: <CA+vqiLHqrNAMTF29NYC=mS_n8DHh0Us60Kv_TWMknV5Cz+GGmg@mail.gmail.com>

x = data.frame( ? =1)

seems to work fine for me. Perhaps you need to give some more details about
what exactly the problem is.

Best,
Ista
On Jul 5, 2015 3:36 PM, "John Sorkin" <JSorkin at grecc.umaryland.edu> wrote:

> I would like to include a Greek letter in a column title, viz.  pB where B
> should be the Greek letter beta, but I don't know how to do this. I know
> about dimnames, but I don't know how to create the Greek letter.
> Thank you,
> John
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From bgunter.4567 at gmail.com  Sun Jul  5 23:07:03 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 5 Jul 2015 14:07:03 -0700
Subject: [R] Greek letters in column titles.
In-Reply-To: <CA+vqiLHqrNAMTF29NYC=mS_n8DHh0Us60Kv_TWMknV5Cz+GGmg@mail.gmail.com>
References: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>
	<CA+vqiLHqrNAMTF29NYC=mS_n8DHh0Us60Kv_TWMknV5Cz+GGmg@mail.gmail.com>
Message-ID: <CAGxFJbReZiqjoyTYGzhZMc-Kh_t3c65XUo9kobEeD_vR_M3sJA@mail.gmail.com>

Warning: The following may be baloney, so confirmation or correction
by those more knowledgeable is required. But maybe this will get you
started/prove useful. Just beware.

1. First of all, as Ista said, this can be done, but it is heavily
system dependent as to how. Basically I think you want access to UTF-8
encodings, which any modern OS should support, but that access is
dependent on your system.

2. It also depends on where you want to display the results -- on
screen, into a pdf, or ? .

3. See also The man pages on ?unicode and ?iconv , as these may also
be relevant.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Jul 5, 2015 at 1:37 PM, Ista Zahn <istazahn at gmail.com> wrote:
> x = data.frame( ? =1)
>
> seems to work fine for me. Perhaps you need to give some more details about
> what exactly the problem is.
>
> Best,
> Ista
> On Jul 5, 2015 3:36 PM, "John Sorkin" <JSorkin at grecc.umaryland.edu> wrote:
>
>> I would like to include a Greek letter in a column title, viz.  pB where B
>> should be the Greek letter beta, but I don't know how to do this. I know
>> about dimnames, but I don't know how to create the Greek letter.
>> Thank you,
>> John
>>
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and
>> Geriatric Medicine
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> (Phone) 410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>
>>
>> Confidentiality Statement:
>> This email message, including any attachments, is for ...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marongiu.luigi at gmail.com  Mon Jul  6 01:55:41 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Mon, 6 Jul 2015 00:55:41 +0100
Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
 alternative for Lattice)
Message-ID: <CAMk+s2QqBw_Xjp+nUtxakFgRHU=EcqxyJ22taeq-t_wAKE4jyw@mail.gmail.com>

Dear all,
I would like to add an outer strip or something like that on a lattice
plot I am making. Such plot contains 384 cells and, since I am not
interested in the axis values, I set:
           scales = list(
               x = list(draw = FALSE),
               y = list(draw = FALSE),
               relation="same"
               ),
on a xyplot from the LATTICE package.
Nevertheless there are axis labels which run like:
           ylab= "Y axis",
           xlab= "X axis",
I would like to place some more information regarding the individual
cells thus I would like to draw a sort of extra axis labels that are
similar to the outer strip of the LATTICE_EXTRA package, that is
markers placed between the axis labels and the axis values and
centered for each cells, typically placed on the top and left sides of
the plot. This is performed by the useOuterStrips function but:
a) LatticeExtra is not in the CRAN repository thus I have to install
it through a more laborious approach which makes LatticeExtra less
direct than Lattice
b)  useOuterStrips uses information directly from the data whereas I
will have to provide the extra information from ad hoc vectors not
present in the data set.

The question therefore is: is there a way to write text from a vector
in the top and left corners of a lattice xyplot and place the
individual elements at the centre of the rows and columns that compose
the graph?

Many thanks,
Luigi


From jsorkin at grecc.umaryland.edu  Mon Jul  6 01:57:53 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 05 Jul 2015 19:57:53 -0400
Subject: [R] Greek letters in column titles.
In-Reply-To: <CAGxFJbReZiqjoyTYGzhZMc-Kh_t3c65XUo9kobEeD_vR_M3sJA@mail.gmail.com>
References: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>
	<CA+vqiLHqrNAMTF29NYC=mS_n8DHh0Us60Kv_TWMknV5Cz+GGmg@mail.gmail.com>
	<CAGxFJbReZiqjoyTYGzhZMc-Kh_t3c65XUo9kobEeD_vR_M3sJA@mail.gmail.com>
Message-ID: <55998C41020000CB00131E5C@smtp.medicine.umaryland.edu>



  To clarify, what I would like to do is have the letter b in the column
titles below print as the Greek letter beta.


  dimnames(results) <-
list(dimnames(results)[[1]],c("b0","SEb0","pb0","bRM0","SERMb0","pRMb0"))
 
  print(results)


  John



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and
Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> Bert Gunter <bgunter.4567 at gmail.com> 07/05/15 5:07 PM >>>
Warning: The following may be baloney, so confirmation or correction
by those more knowledgeable is required. But maybe this will get you
started/prove useful. Just beware.

1. First of all, as Ista said, this can be done, but it is heavily
system dependent as to how. Basically I think you want access to UTF-8
encodings, which any modern OS should support, but that access is
dependent on your system.

2. It also depends on where you want to display the results -- on
screen, into a pdf, or ? .

3. See also The man pages on ?unicode and ?iconv , as these may also
be relevant.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Jul 5, 2015 at 1:37 PM, Ista Zahn <istazahn at gmail.com> wrote:
> x = data.frame( ? =1)
>
> seems to work fine for me. Perhaps you need to give some more details
about
> what exactly the problem is.
>
> Best,
> Ista
> On Jul 5, 2015 3:36 PM, "John Sorkin" <JSorkin at grecc.umaryland.edu>
wrote:
>
>> I would like to include a Greek letter in a column title, viz.  pB
where B
>> should be the Greek letter beta, but I don't know how to do this. I
know
>> about dimnames, but I don't know how to create the Greek letter.
>> Thank you,
>> John
>>
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and
>> Geriatric Medicine
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> (Phone) 410-605-7119410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>
>>
>> Confidentiality Statement:
>> This email message, including any attachments, is for
...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Call
Send SMS
Call from mobile
Add to Skype
You'll need Skype CreditFree via Skype


Confidentiality Statement:
This email message, including any attachments, is for the sole use of
the intended recipient(s) and may contain confidential and privileged
information. Any unauthorized use, disclosure or distribution is
prohibited. If you are not the intended recipient, please contact the
sender by reply email and destroy all copies of the original message. 

From jdnewmil at dcn.davis.CA.us  Mon Jul  6 02:06:27 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 05 Jul 2015 17:06:27 -0700
Subject: [R] Greek letters in column titles.
In-Reply-To: <55998C41020000CB00131E5C@smtp.medicine.umaryland.edu>
References: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>
	<CA+vqiLHqrNAMTF29NYC=mS_n8DHh0Us60Kv_TWMknV5Cz+GGmg@mail.gmail.com>
	<CAGxFJbReZiqjoyTYGzhZMc-Kh_t3c65XUo9kobEeD_vR_M3sJA@mail.gmail.com>
	<55998C41020000CB00131E5C@smtp.medicine.umaryland.edu>
Message-ID: <AD91399F-83D5-4BE5-A33F-457A736E0617@dcn.davis.CA.us>

This does not clarify. Please re-read Bert's response and try again.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 5, 2015 4:57:53 PM PDT, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
>
>
> To clarify, what I would like to do is have the letter b in the column
>titles below print as the Greek letter beta.
>
>
>  dimnames(results) <-
>list(dimnames(results)[[1]],c("b0","SEb0","pb0","bRM0","SERMb0","pRMb0"))
> 
>  print(results)
>
>
>  John
>
>
>
>John David Sorkin M.D., Ph.D.
>Professor of Medicine
>Chief, Biostatistics and Informatics
>University of Maryland School of Medicine Division of Gerontology and
>Geriatric Medicine
>Baltimore VA Medical Center
>10 North Greene Street
>GRECC (BT/18/GR)
>Baltimore, MD 21201-1524
>(Phone) 410-605-7119
>(Fax) 410-605-7913 (Please call phone number above prior to faxing) 
>
>>>> Bert Gunter <bgunter.4567 at gmail.com> 07/05/15 5:07 PM >>>
>Warning: The following may be baloney, so confirmation or correction
>by those more knowledgeable is required. But maybe this will get you
>started/prove useful. Just beware.
>
>1. First of all, as Ista said, this can be done, but it is heavily
>system dependent as to how. Basically I think you want access to UTF-8
>encodings, which any modern OS should support, but that access is
>dependent on your system.
>
>2. It also depends on where you want to display the results -- on
>screen, into a pdf, or ? .
>
>3. See also The man pages on ?unicode and ?iconv , as these may also
>be relevant.
>
>Cheers,
>Bert
>Bert Gunter
>
>"Data is not information. Information is not knowledge. And knowledge
>is certainly not wisdom."
>   -- Clifford Stoll
>
>
>On Sun, Jul 5, 2015 at 1:37 PM, Ista Zahn <istazahn at gmail.com> wrote:
>> x = data.frame( ? =1)
>>
>> seems to work fine for me. Perhaps you need to give some more details
>about
>> what exactly the problem is.
>>
>> Best,
>> Ista
>> On Jul 5, 2015 3:36 PM, "John Sorkin" <JSorkin at grecc.umaryland.edu>
>wrote:
>>
>>> I would like to include a Greek letter in a column title, viz.  pB
>where B
>>> should be the Greek letter beta, but I don't know how to do this. I
>know
>>> about dimnames, but I don't know how to create the Greek letter.
>>> Thank you,
>>> John
>>>
>>> John David Sorkin M.D., Ph.D.
>>> Professor of Medicine
>>> Chief, Biostatistics and Informatics
>>> University of Maryland School of Medicine Division of Gerontology
>and
>>> Geriatric Medicine
>>> Baltimore VA Medical Center
>>> 10 North Greene Street
>>> GRECC (BT/18/GR)
>>> Baltimore, MD 21201-1524
>>> (Phone) 410-605-7119410-605-7119
>>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>>
>>>
>>> Confidentiality Statement:
>>> This email message, including any attachments, is for
>...{{dropped:16}}
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>Call
>Send SMS
>Call from mobile
>Add to Skype
>You'll need Skype CreditFree via Skype
>
>
>Confidentiality Statement:
>This email message, including any attachments, is for t...{{dropped:12}}


From erinm.hodgess at gmail.com  Mon Jul  6 03:14:11 2015
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Sun, 5 Jul 2015 20:14:11 -0500
Subject: [R]  Rgui not working in Windows Version 8.1
Message-ID: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>

Hello everyone!

I have been having trouble with Rgui on a Windows machine running Version
8.1.  I download the binary from CRAN.  It installs fine.  But when I try
to run it, the console opens for a moment, and then it disappears.

I believe this was on the Windows FAQ list.  I was wondering if there was
any fix/patch please.

This is true for R Version 3.2.0 and 3.2.1.

Thank you,
Sincerely,
Erin


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From erinm.hodgess at gmail.com  Mon Jul  6 03:48:00 2015
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Sun, 5 Jul 2015 20:48:00 -0500
Subject: [R] Obtaining the transformation matrix from the "persp" function
 without plotting
Message-ID: <CACxE24kpwR=7NFVR0d=sorNsebSf4ULOzEK2RxDwuCOb6J2Dtg@mail.gmail.com>

Hello again.

I am looking at the "persp" function and would like to obtain the the
transformation matrix value that is output rather than the plot itself.  I
will use this matrix to pass into 3D plotting functions.

This is what I am doing:

library(rsm)
library(rgl)
tool.df <- read.table("tool.txt",header=TRUE)
tool.code <- coded.data(tool.df,x1~(speed-150)/25,x2~(angle-20)/5)
tool.rsm <- rsm(life ~ SO(x1,x2), data=tool.code)
summary(tool.rsm)

Note:  the tool.txt file is attached.

Update the model:

tool2.rsm = update(tool.rsm,  . ~ . + I(x1^2*x2) + I(x1*x2^2) +
I(x1^2*x2^2))

Now plot:

persp3d.rsm(tool2.rsm, x1~x2, bounds=list(x1=c(-1,1),x2=c(-1,1)),
zlim=c(-6,6),
        contour=TRUE)

Similarly, the persp3d.rsm.R file is attached.

As you can see, about half way in the persp3d.rsm function, I try to run a
regular persp function to get the transformation matrix:

transf2 = persp(dat$x, dat$y, dat$z, zlim = dat$zlim,
                  theta = theta, phi = phi, r = r, col = NA, border = NA,
                   box = FALSE, ...)

However, this produces a set of contour lines on a separate plot, rather
than the 3D plot.

I tried to used "plot=FALSE" in the persp function call, but that gave me a
warning message that the parameter did not exist.

This is from Russell Lenth's work.  Thanks to him for the rsm package.

Thank you for any suggestions.

Sincerely,
Erin



-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com
-------------- next part --------------
 speed angle life
    125    15   -2
    150    15   -3
    175    15    2
    125    15   -1
    150    15    0
    175    15    3
    125    20    0
    150    20    1
    175    20    4
    125    20    2
    150    20    3
    175    20    6
    125    25   -1
    150    25    5
    175    25    0
    125    25    0
    150    25    6
    175    25   -1

From chl948 at mail.usask.ca  Mon Jul  6 03:50:33 2015
From: chl948 at mail.usask.ca (Chel Hee Lee)
Date: Sun, 5 Jul 2015 19:50:33 -0600
Subject: [R] Translation of R Manuals
Message-ID: <5599DEE9.9000000@mail.usask.ca>

Dear the members in the R-help mailing list,

R version 2.1.0 and later support translations of program messages. The 
continuous efforts have been made by R Translation Teams 
<http://developer.r-project.org/TranslationTeams.html>.  I appreciate 
the leadership shown by Prof. Duncan Murdoch in this field.  Also, I'd 
like to express thanks to other R core members regarding the support of 
R message translation.

 From this point, I would like to share my experience on the translation 
of R manuals.  The R Documentation files are licensed under the General 
Public License, version 2 or 3.  This means that the pilot project to 
translate them into other languages has permission to reproduce them and 
translate them.  I have worked to build a framework that allows us to 
keep translation up-to-dated along with the development of R.  You can 
see the prototype of this work in the section "Language Support for R - 
Translation of Messages and Documentation" at 
"http://homepage.usask.ca/~chl948/".

You will see "title of manual", "English text", "Template for other 
languages", "Verbatim process", and "translated manual".  GNU 'gettext' 
utilities are employed for this work.  The portable object template is 
updated a weekly basis or whenever changes are necessary.   The work 
process is identical to the process of translating R messages.

I would appreciate if you are interested in this project.  Please 
contact me if you voluntarily participate in or offer your help with 
this project of translating R manuals.  Comments and corrections is of 
course most welcome.

Chel Hee Lee


From loris.bennett at fu-berlin.de  Mon Jul  6 10:58:44 2015
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Mon, 6 Jul 2015 10:58:44 +0200
Subject: [R] igraph plot slowness
References: <87h9plogf5.fsf@hornfels.zedat.fu-berlin.de>
	<CAAxdm-5YiwPjVa5m_fZtL-+onGvdoKUP=o99jJ3ZPBCG7ipzZQ@mail.gmail.com>
Message-ID: <87zj39u0fv.fsf@hornfels.zedat.fu-berlin.de>

Hi Jim,

jim holtman <jholtman at gmail.com> writes:

> Here is what it does locally on my PC:
>
>> library("igraph")
>>  topo_data <- read.table(text = "ibcore01        ibswitch01
> +  ibcore01        ibswitch02
> +  ibcore01        ibswitch03
> +  ibcore02        ibswitch01
> +  ibcore02        ibswitch02
> +  ibcore02        ibswitch03
> +  ibswitch01      node001
> +  ibswitch01      node002
> +  ibswitch01      node003
> +  ibswitch02      node004
> +  ibswitch02      node005
> +  ibswitch02      node006
> +  ibswitch03      node007
> +  ibswitch03      node008
> +  ibswitch03      node009" ,head=FALSE)
>>  system.time({
> +  network_data <-graph.data.frame(topo_data, directed=F)
> +  plot(network_data)
> + })
>    user  system elapsed
>    0.01    0.01    0.03
>>
>>
>
> Does not seem too slow.  Creating a PDF file takes a little longer:
>
>> library("igraph")
>>  topo_data <- read.table(text = "ibcore01        ibswitch01
> +  ibcore01        ibswitch02
> +  ibcore01        ibswitch03
> +  ibcore02        ibswitch01
> +  ibcore02        ibswitch02
> +  ibcore02        ibswitch03
> +  ibswitch01      node001
> +  ibswitch01      node002
> +  ibswitch01      node003
> +  ibswitch02      node004
> +  ibswitch02      node005
> +  ibswitch02      node006
> +  ibswitch03      node007
> +  ibswitch03      node008
> +  ibswitch03      node009" ,head=FALSE)
>>  system.time({
> +  network_data <-graph.data.frame(topo_data, directed=F)
> +  pdf('test.pdf')
> +  plot(network_data)
> +  dev.off()
> + })
>    user  system elapsed
>    0.09    0.00    0.16
>
> The PDF file is attached.  So maybe it is something with your remote
> connection.

You're right.  Running locally even the plot of complete network takes
less than 0.2 seconds via the X11 device.  I'll have a closer look at
the connection.

Thanks,

Loris

> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Fri, Jul 3, 2015 at 3:21 AM, Loris Bennett <loris.bennett at fu-berlin.de>
> wrote:
>
>> Hi,
>>
>> With the following data
>>
>> ibcore01        ibswitch01
>> ibcore01        ibswitch02
>> ibcore01        ibswitch03
>> ibcore02        ibswitch01
>> ibcore02        ibswitch02
>> ibcore02        ibswitch03
>> ibswitch01      node001
>> ibswitch01      node002
>> ibswitch01      node003
>> ibswitch02      node004
>> ibswitch02      node005
>> ibswitch02      node006
>> ibswitch03      node007
>> ibswitch03      node008
>> ibswitch03      node009
>>
>> in the file "topology.txt"
>>
>> and the following code:
>>
>> library("igraph")
>> topo_data <- read.csv(file="topology.txt",head=FALSE,sep="\t")
>> network_data <-graph.data.frame(topo_data, directed=F)
>> plot(network_data)
>>
>> it takes about 5 seconds for the plot to be drawn with R 3.2.0 on a
>> 12-core 2.67 GHz Xeon X5650 server with no other CPU-intensive processes
>> running.
>>
>> This strikes me as rather slow, particularly as my full network has over
>> 120 components and the plot takes around 50 seconds.
>>
>> Am I doing anything wrong?
>>
>> (I am working over an ssh connection with X forwarding, but plotting to
>> a PDF file on the server does not seem to be faster.)
>>
>> Cheers,
>>
>> Loris
>>
>> --
>> This signature is currently under construction.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
Dr. Loris Bennett (Mr.)
ZEDAT, Freie Universit?t Berlin         Email loris.bennett at fu-berlin.de


From carlos.nasher at googlemail.com  Mon Jul  6 12:03:22 2015
From: carlos.nasher at googlemail.com (Carlos Nasher)
Date: Mon, 6 Jul 2015 12:03:22 +0200
Subject: [R] Hypergeometric Function seems to give wrong results
Message-ID: <CAP=BVWNeusOReNv-sSppN0mL3TMB4199PYsL9uMiiuANM-y0vQ@mail.gmail.com>

Hello R helpers,

I need to evaluate the Hypergeometric Function of the 2nd kind (Tricomi
confluent hypergeometric function). Therefore I'm using the kummerU
function from the fAsianOptions package. It seems to me that kummerU is
giving wrong results. Here's an example:

library("fAsianOptions")
kummerU(a=19, b=19, x = 10)

R gives 1838.298 for the real part.

If I use Mathematica via the wolfram site (
http://functions.wolfram.com/webMathematica/FunctionEvaluation.jsp?name=HypergeometricU)
the result is 3.52603e-20 which is more reasonable in the context of my
analysis.

Can anyone help how to compute the correct values within R?

Best regards,
Carlos

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Mon Jul  6 14:30:54 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Mon, 06 Jul 2015 07:30:54 -0500
Subject: [R] Variance estimates for survreg vs. lm
In-Reply-To: <mailman.3.1436004001.2234.r-help@r-project.org>
References: <mailman.3.1436004001.2234.r-help@r-project.org>
Message-ID: <2f3a88$v7qmv@ironport10.mayo.edu>

The difference is that survreg is using a maximum likelihood estimate (MLE) of the 
variance and that lm is using the unbiased (MVUE) estimate of variance.  For simple linear 
regression, the former divides by "n" and the latter by "n-p".  The difference in your 
variances is exactly n/(n-p) = 10/8.

With censored data the MLE estimate is still clear, but what exactly "n" is is not so 
obvious (does a censored datum count as a whole observation?), so a simple (n-p)/n 
variance correction is also not obvious.  I would not be surprised if someone, somewhere 
has hammered out a correction for "unbiased" variance; I've never looked for it.  I'm not 
convinced that it is worthwhile though.

Terry Therneau


On 07/04/2015 05:00 AM, r-help-request at r-project.org wrote:
> I would like help understanding why a survival regression with no censored
> data-points does not give the same variance estimates as a linear model
> (see code below).
>
> I think it must be something to do with the fact that the variance is an
> actual parameter in the survival version via the log(scale), and possibly
> that different assumptions are made about the distribution of the variance.
> But I really don't know, I'm just guessing.
>
> The reason I ask is because I am moving a process, that has always been
> modelled using a linear model, to a survival model (because there are
> sometimes a few censored data points). In the past, the censored data
> points have been treated as missing which imparts bias. The variance of the
> estimates in this process is key, so I need to know why they are changing
> in this systematic way?!
>
>
>
> library(survival)
>
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> ctl.surv <- Surv(ctl)
>
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>
> lmod <- lm     (ctl      ~ trt                )
> smod <- survreg(ctl.surv ~ trt,dist="gaussian")
>
> coef(lmod)
> coef(smod) # same
>
> vcov(lmod)
> vcov(smod) # smod is smaller
>
> diag(vcov(lmod))     /
> diag(vcov(smod))[1:2]  # 1.25 == 0.5*(n/(n-1))
>
> ( summary(lmod)$coef [   ,"Std. Error"] /
>    summary(smod)$table[1:2,"Std. Error"]   )^2    # 1.25 = 0.5*(n/(n-1))


From frainj at gmail.com  Mon Jul  6 15:22:49 2015
From: frainj at gmail.com (John C Frain)
Date: Mon, 6 Jul 2015 14:22:49 +0100
Subject: [R] Rgui not working in Windows Version 8.1
In-Reply-To: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>
References: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>
Message-ID: <CAHrK515d6v3q_-GpG7KL6ZNAg92H5tBY2XPQcbb2j8zj47dagA@mail.gmail.com>

I am running

R version 3.2.1 Patched (2015-07-02 r68625) -- "World-Famous Astronaut"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

and do not have any problems with Rgui. The output from Sys.info() is

> Sys.info()
       sysname        release        version       nodename        machine
     "Windows"        "7 x64"   "build 9200"      "DELL745"       "x86-64"

I don't know why the release is given as  "7 x64" because I am running
Windows 8.1 Professional on this PC.  If you have not already done so I
would uninstall R and reinstall it to see if that solves the problem.


John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

On 6 July 2015 at 02:14, Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> Hello everyone!
>
> I have been having trouble with Rgui on a Windows machine running Version
> 8.1.  I download the binary from CRAN.  It installs fine.  But when I try
> to run it, the console opens for a moment, and then it disappears.
>
> I believe this was on the Windows FAQ list.  I was wondering if there was
> any fix/patch please.
>
> This is true for R Version 3.2.0 and 3.2.1.
>
> Thank you,
> Sincerely,
> Erin
>
>
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From msharp at txbiomed.org  Mon Jul  6 16:56:18 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Mon, 6 Jul 2015 14:56:18 +0000
Subject: [R] question
In-Reply-To: <CAMqbV1DcP7j1hoToiuugMn+-rgpTNih9Xh91obQ0t7XuKSZ4ng@mail.gmail.com>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
	<CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
	<9B873725-D55A-47A0-84D0-6CA9568A4780@txbiomed.org>
	<CAMqbV1DcP7j1hoToiuugMn+-rgpTNih9Xh91obQ0t7XuKSZ4ng@mail.gmail.com>
Message-ID: <A1AC5565-6C1F-48C1-9E12-6F7270A045C0@txbiomed.org>

Lida,

Please send the code that you ran. It is almost certain that you have a vector being formed that is the wrong length. There should be only one vector that you use to increment through the loop and that is the character vector formed when you read in your_met_file to form the met character vector.

Mark
R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org







> On Jul 6, 2015, at 9:05 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> 
> Hi Mark,
> 
> Thank you so much for your help.
> 
> I run your code and it works great for 5*1 character vector but if I run it for 682*1 character vector it has some error like this:
> 
> Error in model.frame.default(formula = formula, ...) : 
>   variable lengths differ (found for 'egfr_v1_ckdepi')
> 
> Would you please let me know what's the reason is?
> 
> Many Thanks,
> Lida
> 
> On Fri, Jul 3, 2015 at 11:38 PM, Mark Sharp <msharp at txbiomed.org> wrote:
> Lida,
> 
> I expect that there is a better way to solve your problem than the process you propose.
> 
> However, something like this may do what you want.
> 
> ###
> ## met <- read.csv("your_met_file?)
> ## Since I do not have your file a made a small 5*1 character vector.
> met <- c("glycine_imp",
>             "Nacetylglycine_imp",
>             "sarcosine_imp",
>             "dimethylglycine_imp",
>             "betaine_imp")
> 
> for (i in seq_along(met)) {
>   my_formula <- paste0(met[i], "~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
>   prep <- Scores(Z=metalofGT, formula = my_formula)
>   save(prep, file = paste0("prep", i))
> }
> ###
> 
> Mark
> 
> 
> R. Mark Sharp, Ph.D.
> Director of Primate Records Database
> Southwest National Primate Research Center
> Texas Biomedical Research Institute
> P.O. Box 760549
> San Antonio, TX 78245-0549
> Telephone: (210)258-9476
> e-mail: msharp at TxBiomed.org
> 
> > On Jul 2, 2015, at 11:48 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> >
> > Thank you so much for replying me!
> > for better understanding my problem, I explain my problem more:
> >
> > I have a 682*1 matrix called "met" , the first 5 rows similar below:
> >
> >> rownames(met)[1:5]
> >
> > [1]  "glycine_imp"
> > [2]  "Nacetylglycine_imp"
> > [3]  "sarcosine_imp"
> > [4]  "dimethylglycine_imp"
> > [5]  "betaine_imp"
> >
> > and I have a function in R that each time use one of the row names of "met"
> > matrix and create a new object file and I should save the objects!
> >
> > my function is  "
> > Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> > " that each time just I should change the met[i] and replace by row names
> > "met" one by one and for each of them I should rename the function and
> > after that I should save each object!
> > for example for first row of "met" I have
> >
> >>  prep1<- Scores(Z=metalofGT,formula="glycine_imp~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> > #creat the object file for first row and called prep1###
> >
> >>  save(prep1, file="prep1.RData", compress="bzip2")      ##save the
> > object file as "prep1.RData"#####
> >
> > I should do this process for 682 row names of "met" matrix and at the end I
> > should have    "prep1.RData"  ,   "prep2.RData"   , "prep3.RData"
> >
> > so, would you please help me how to do it?
> >
> > Many Thanks,
> > Ati
> >
> > On Wed, Jul 1, 2015 at 1:07 PM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> >
> >> I have 682 variables in a data frame , and a function that  I should feed
> >> 682 variables in this function one by one and each time save the file as a
> >> special name!
> >> for emaple:
> >> my data frame file includes 682 names :
> >> 1  aaa
> >> 2  bbb
> >> 3  dfdsfg
> >> 4 fghh
> >> .
> >>
> >> 682 fgfhg
> >> and a function like prep(Z, aaa, .....) and each time I should change the
> >> variable name in this function and read the variable from the data frame
> >> and each time I should save the file as a special name such as:
> >>
> >> prep1<- prep(z, aaa,...)
> >> prep2<- prep(z, bbb,...)
> >> prep3<- prep(z, dfdsfg,..)
> >> Prep4<- prep(z, fghh,...)
> >>
> >> How can I use loop function in R to that?
> >>
> >> Thanks
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 
> 
> 
> 
> 


From Paul.Domaskis at gmail.com  Mon Jul  6 17:10:48 2015
From: Paul.Domaskis at gmail.com (Paul)
Date: Mon, 6 Jul 2015 15:10:48 +0000
Subject: [R] Invalid URL for R documentation
Message-ID: <loom.20150706T170610-303@post.gmane.org>

I tried accessing http://cran.r-project.org/doc/manuals/r-release/R-
intro.html, which is linked to from http://cran.r-
project.org/manuals.html.  I get the message that the URL could not be 
found by the server. Just wondering if I'm doing something stunningly un-
smart (it would not be the first time).


From john.archie.mckown at gmail.com  Mon Jul  6 17:18:36 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 6 Jul 2015 10:18:36 -0500
Subject: [R] Invalid URL for R documentation
In-Reply-To: <loom.20150706T170610-303@post.gmane.org>
References: <loom.20150706T170610-303@post.gmane.org>
Message-ID: <CAAJSdjj+UnROM6D7DyoeqnF=oSp2NOvkA10y1_7x29Rb9U+Dxg@mail.gmail.com>

I get a 404 on that page as well. Curiously, the R-patched and R-devel
links work. Looks like the webmaster has some work to do!

The pages are still available on the Revolution Analytics mirror site:
http://cran.revolutionanalytics.com/

On Mon, Jul 6, 2015 at 10:10 AM, Paul <Paul.Domaskis at gmail.com> wrote:

> I tried accessing http://cran.r-project.org/doc/manuals/r-release/R-
> intro.html, which is linked to from http://cran.r-
> project.org/manuals.html.  I get the message that the URL could not be
> found by the server. Just wondering if I'm doing something stunningly un-
> smart (it would not be the first time).
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Jul  6 17:22:18 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 6 Jul 2015 11:22:18 -0400
Subject: [R] Invalid URL for R documentation
In-Reply-To: <loom.20150706T170610-303@post.gmane.org>
References: <loom.20150706T170610-303@post.gmane.org>
Message-ID: <559A9D2A.5060902@gmail.com>

On 06/07/2015 11:10 AM, Paul wrote:
> I tried accessing http://cran.r-project.org/doc/manuals/r-release/R-
> intro.html, which is linked to from http://cran.r-
> project.org/manuals.html.  I get the message that the URL could not be
> found by the server. Just wondering if I'm doing something stunningly un-
> smart (it would not be the first time).

That's strange:  the URL works for me.   Perhaps you were unlucky and 
tried while it was being replaced?

Duncan Murdoch


From dcarlson at tamu.edu  Mon Jul  6 17:22:35 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 6 Jul 2015 15:22:35 +0000
Subject: [R] Greek letters in column titles.
In-Reply-To: <55998C41020000CB00131E5C@smtp.medicine.umaryland.edu>
References: <55994D4E020000CB00131E1A@smtp.medicine.umaryland.edu>
	<CA+vqiLHqrNAMTF29NYC=mS_n8DHh0Us60Kv_TWMknV5Cz+GGmg@mail.gmail.com>
	<CAGxFJbReZiqjoyTYGzhZMc-Kh_t3c65XUo9kobEeD_vR_M3sJA@mail.gmail.com>
	<55998C41020000CB00131E5C@smtp.medicine.umaryland.edu>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6A026E@mb02.ads.tamu.edu>

Expanding slightly on Ista's answer, this may help if you are using Windows and want the symbol to appear in the console or in plots

> beta <- intToUtf8(0x03B2) # or intToUtf8(946)
> a <- 0:9
> names(a) <- paste0(beta, a)
> a
?0 ?1 ?2 ?3 ?4 ?5 ?6 ?7 ?8 ?9 
 0  1  2  3  4  5  6  7  8  9
> plot(0:9, 0:9, typ="n")
> text(0:9, 0:9, names(a))

You just have to figure out the utf8 code for the symbol you want. Windows Character Map works for this and lets you insert the code directly as Ista did, or any of the various web pages listing the codes.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John Sorkin
Sent: Sunday, July 5, 2015 6:58 PM
To: bgunter.4567 at gmail.com; istazahn at gmail.com
Cc: r-help at r-project.org
Subject: Re: [R] Greek letters in column titles.



  To clarify, what I would like to do is have the letter b in the column
titles below print as the Greek letter beta.


  dimnames(results) <-
list(dimnames(results)[[1]],c("b0","SEb0","pb0","bRM0","SERMb0","pRMb0"))
 
  print(results)


  John



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and
Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

>>> Bert Gunter <bgunter.4567 at gmail.com> 07/05/15 5:07 PM >>>
Warning: The following may be baloney, so confirmation or correction
by those more knowledgeable is required. But maybe this will get you
started/prove useful. Just beware.

1. First of all, as Ista said, this can be done, but it is heavily
system dependent as to how. Basically I think you want access to UTF-8
encodings, which any modern OS should support, but that access is
dependent on your system.

2. It also depends on where you want to display the results -- on
screen, into a pdf, or ? .

3. See also The man pages on ?unicode and ?iconv , as these may also
be relevant.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Jul 5, 2015 at 1:37 PM, Ista Zahn <istazahn at gmail.com> wrote:
> x = data.frame( ? =1)
>
> seems to work fine for me. Perhaps you need to give some more details
about
> what exactly the problem is.
>
> Best,
> Ista
> On Jul 5, 2015 3:36 PM, "John Sorkin" <JSorkin at grecc.umaryland.edu>
wrote:
>
>> I would like to include a Greek letter in a column title, viz.  pB
where B
>> should be the Greek letter beta, but I don't know how to do this. I
know
>> about dimnames, but I don't know how to create the Greek letter.
>> Thank you,
>> John
>>
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and
>> Geriatric Medicine
>> Baltimore VA Medical Center
>> 10 North Greene Street
>> GRECC (BT/18/GR)
>> Baltimore, MD 21201-1524
>> (Phone) 410-605-7119410-605-7119
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>
>>
>> Confidentiality Statement:
>> This email message, including any attachments, is for
...{{dropped:16}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


Call
Send SMS
Call from mobile
Add to Skype
You'll need Skype CreditFree via Skype


Confidentiality Statement:
This email message, including any attachments, is for the sole use of
the intended recipient(s) and may contain confidential and privileged
information. Any unauthorized use, disclosure or distribution is
prohibited. If you are not the intended recipient, please contact the
sender by reply email and destroy all copies of the original message. 
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From Paul.Domaskis at gmail.com  Mon Jul  6 17:24:49 2015
From: Paul.Domaskis at gmail.com (Paul)
Date: Mon, 6 Jul 2015 15:24:49 +0000
Subject: [R] Invalid URL for R documentation
References: <loom.20150706T170610-303@post.gmane.org>
	<CAAJSdjj+UnROM6D7DyoeqnF=oSp2NOvkA10y1_7x29Rb9U+Dxg@mail.gmail.com>
Message-ID: <loom.20150706T172129-351@post.gmane.org>

On Mon, Jul 6, 2015 at 10:10 AM, Paul <Paul.Domaskis <at> gmail.com>
wrote:
> ...http://cran.r-project.org/doc/manuals/r-release/R-intro.html

John McKown <john.archie.mckown <at> gmail.com> writes:
> I get a 404 on that page as well. Curiously, the R-patched and
> R-devel links work. Looks like the webmaster has some work to do!
>
> The pages are still available on the Revolution Analytics mirror
> site: http://cran.revolutionanalytics.com

Thanks, John.  The initial reason I was trying to access the manual
was to get at
http://cran.r-project.org/doc/manuals/R-lang.html#Indexing (as
specified on
http://stackoverflow.com/questions/1169456/in-r-what-is-the-difference-
between-the-and-notations-for-accessing-the).
Looks like I can still form the right URL using your URL and changing
the last piece to .../R-lang.html#Indexing.


From kunalshah305 at gmail.com  Mon Jul  6 06:28:19 2015
From: kunalshah305 at gmail.com (Kunal Shah)
Date: Mon, 6 Jul 2015 09:58:19 +0530
Subject: [R] Web/Server Interface for R files
Message-ID: <CAOPdpkZ0cM-uakPHWmXWoVreQ=_KMWPpzBEkwJshA5Ge5+i2CQ@mail.gmail.com>

Hello,

I have the following code

#### From excel.link package

xl.workbook.activate("ADSnippetXLS.xlsx")
source('ActiveDeltaSnippet2.R')

#### "ActiveDeltaSnippet2.R" takes data from ADSnippetXLS, performs some
computations on the data and outputs the data back into "ADSnippetXLS.xlsx"


I dont want that the file "ActiveDeltaSnippet2.R" to be stored on the same
PC. In fact I want to call this R file which might be stored on any
server/Web

I tried to search Shiny and ROOK but got lost in the literature.
Any help will be appreciated

Regards

	[[alternative HTML version deleted]]


From Mohan.Radhakrishnan at cognizant.com  Sun Jul  5 15:17:36 2015
From: Mohan.Radhakrishnan at cognizant.com (Mohan.Radhakrishnan at cognizant.com)
Date: Sun, 5 Jul 2015 13:17:36 +0000
Subject: [R] Task estimation - Monte Carlo
Message-ID: <E1B160F4999FD6449524E16C2CB94E030773D0A2@CTSINCHNSXMBP.cts.com>

Hi

   I am trying to  simulate task estimation person days using this type of R code. But I am not sure about reasoning here. Should the distribution be beta or triangular or something else ? How do we get the values of mu,z and s here ? Are there any explanations available ? Sections of some book ?

I have the book about monte carlo analysis using R but that looks like the next step for me. I am at a preliminary stage.


taskestimation <- function( low , high, ci=0.9, n=10000) {
mu = mean(c(low,high))
z = qnorm(1-(1-ci)/2)
s = (high - mu)/z
rnorm(n, mu, s)
}

result = taskestimation(10,80)


#calculate the percentage of cases below certain number of days
length(result[result < 50])/length(result)

I am able to plot a density curve showing the percentage of completion below '50' days that the simulation predicts.

Thanks,
Mohan
This e-mail and any files transmitted with it are for the sole use of the intended recipient(s) and may contain confidential and privileged information. If you are not the intended recipient(s), please reply to the sender and destroy all copies of the original message. Any unauthorized review, use, disclosure, dissemination, forwarding, printing or copying of this email, and/or any action taken in reliance on the contents of this e-mail is strictly prohibited and may be unlawful. Where permitted by applicable law, this e-mail and other e-mail communications sent to and from Cognizant e-mail addresses may be monitored.

	[[alternative HTML version deleted]]


From marammagdysalem at gmx.com  Mon Jul  6 02:29:56 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Mon, 6 Jul 2015 02:29:56 +0200
Subject: [R] NaN produced from log() with positive input
Message-ID: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>

Dear All
I'm trying to find the maximum likelihood estimator  of a certain distribution based on the newton raphson method using maxLik package. I wrote the log-likelihood , gradient, and hessian functionsusing the following code.
 
#Step 1: Creating the theta vector
 theta <-vector(mode = "numeric", length = 3)
# Step 2: Setting the values of r and n
r<- 17
n <-30
 # Step 3: Creating the T vector
T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
# Step 4: Creating the C vector
C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
# The  loglik. func.
loglik <- function(param) {
 theta[1]<- param[1]
 theta[2]<- param[2]
 theta[3]<- param[3]
 l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+ (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+ (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
return(l)
 }
# Step 5: Creating the gradient vector and calculating its inputs
U <- vector(mode="numeric",length=3)
gradlik<-function(param = theta,n, T,C)
 {
U <- vector(mode="numeric",length=3)
theta[1] <- param[1]
theta[2] <- param[2]
theta[3] <- param[3]
r<- 17
n <-30
T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
 U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+( -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+ (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+ (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
return(U)
}
# Step 6: Creating the G (Hessian) matrix and Calculating its inputs
hesslik<-function(param=theta,n,T,C)
{
theta[1] <- param[1]
theta[2] <- param[2]
theta[3] <- param[3]
r<- 17
n <-30
T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
G<- matrix(nrow=3,ncol=3)
G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+ (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+ (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+ (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
G[2,1]<-G[1,2]
G[1,3]<-(n/theta[1])+(-1)*sum( (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
G[3,1]<-G[1,3] 
G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+ (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
G[3,2]<-G[2,3]                 
G[3,3]<-((-1*r)/(theta[3])^2)
return(G)
}
mle<-maxLik(loglik, grad = gradlik, hess = hesslik, start=c(40,50,2))
There were 50 or more warnings (use warnings() to see the first 50)
 
warnings ()
Warning messages:
1: In log(theta[3]) : NaNs produced
2: In log(theta[1] + theta[2]) : NaNs produced
3: In log(theta[1]) : NaNs produced
4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs produced
 and so on .......
 
Although when I evaluate, for example, log(theta[3])  it gives me a number. and the same applies for the other warnings.
 
Then when I used summary (mle), I got
 
 
Maximum Likelihood estimation
Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-Likelihood: -55.89012 
3  free parameters
Estimates:
     Estimate Std. error t value Pr(> t)
[1,]   11.132        Inf       0       1
[2,]   47.618        Inf       0       1
[3,]    1.293        Inf       0       1
--------------------------------------------


Where the estimates are far away from the starting values and they have infinite standard errors. I think there is a problem with my gradlik or hesslik functions, but I can't figure it out.
Any help?
Thank you in advance.

Maram 



	[[alternative HTML version deleted]]


From chernandez at Gmri.org  Mon Jul  6 14:08:11 2015
From: chernandez at Gmri.org (Christina Hernandez)
Date: Mon, 6 Jul 2015 12:08:11 +0000
Subject: [R] opendap accessibility on Windows
Message-ID: <BY2PR0401MB10166FCF0AC1E3E757986F4AB9930@BY2PR0401MB1016.namprd04.prod.outlook.com>

Hello,

I am working on a project where we are accessing large data files via opendap. Myself and two of the higher-ups on the project are working on Windows machines, and we have been unable to access remote files using the ncdf4 package. We have followed the package author's notes for installation on a Windows machine, and we have successfully installed the netcdf-4 libraries and the ncdf4 R package. However, the program still works only on local files. It is not an issue with connectivity of my machine since I am able to remotely access these same data files using Matlab.
I then attempted to follow the package author's notes on cross-compiling in a Linux environment, I was using a Ubuntu Virtual Box. These notes are available here: http://cirrus.ucsd.edu/~pierce/ncdf/how_to_build_on_windows.html
However, I do not really know what I'm doing and I was unable to successfully cross-compile the dependencies. Has anyone dealt with this on a Windows machine? Is there a different way to access remote files through R? \We have considered writing a Python script that can be run from within an R program, so that users that are only familiar with R can still access remote files without downloading a lot of large files.
Any advice is appreciated.


Christina Hernandez
Research Assistant
Ecosystem Modeling Lab
Gulf of Maine Research Institute
chernandez at gmri.org


	[[alternative HTML version deleted]]


From aurora.gonzalez2 at um.es  Mon Jul  6 14:24:14 2015
From: aurora.gonzalez2 at um.es (AURORA GONZALEZ VIDAL)
Date: Mon, 06 Jul 2015 14:24:14 +0200
Subject: [R] Rmarkdown / knitr naming the output file
Message-ID: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>

Hello.
I have a question for Rmarkdown users.

Is there any way to give a name to the output document inside the Rmd?

For example, my rmd's name is "bb.Rmd" but when I knitr to pdf I want it to
name the pdf differently than "bb.pdf", for example, "doc1.pdf". Is there
any way to do this?

Thank you very much


------
Aurora Gonz?lez Vidal

Secci?n Apoyo Estad?stico.
Servicio de Apoyo a la Investigaci?n (SAI).
Vicerrectorado de Investigaci?n.
Universidad de Murcia
Edif. SACE . Campus de Espinardo.
30100 Murcia

@. aurora.gonzalez2 at um.es
T. 868 88 7315
F. 868 88 7302
www.um.es/sai
www.um.es/ae

	[[alternative HTML version deleted]]


From angelo.arcadi at virgilio.it  Mon Jul  6 16:00:35 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Mon, 6 Jul 2015 16:00:35 +0200 (CEST)
Subject: [R] Bonferroni post hoc test in R for repeated measure ANOVA with
 mixed within and between subjects design
Message-ID: <14e63ab1bc0.angelo.arcadi@virgilio.it>

Dear List Members, 



I need to perform a Bonferroni post hoc test in R on a table with 
three within subjects factors (Emotion, having 5 levels, Material, 
having 4 levels, Shoes, having 2 levels) and one between subject factor 
(Musician, having 2 levels). 


I normally use the Tukey method with the following formula

require(nlme)
lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd, random = ~1|Subject)
require(multcomp)
summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))



I am not able to find any reference that explains with an example of R
 code how to perform a post hoc test with the Bonferroni procedure. 
Can anyone provide an example to perform the same post hoc test in the 
code above but with Bonferroni instead of Tukey?


Thank you in advance



Angelo

       
	[[alternative HTML version deleted]]


From angelo.arcadi at virgilio.it  Mon Jul  6 16:04:58 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Mon, 6 Jul 2015 16:04:58 +0200 (CEST)
Subject: [R] Sphericity for a repeated measure ANOVA with whithin and
 between subjects design, using R
Message-ID: <14e63af1f5b.angelo.arcadi@virgilio.it>

 Dear List Members,



I need to perform a four-way ANOVA with repeated measures and to 
calculate the sphericity for eventual correction of the degree of 
freedom of the F ratio.
I have three within subjects factors (Emotion, having 5 levels, 
Material, having 4 levels, Shoes, having 2 levels) and a between subject
 factor (Musician, having 2 levels). Without considering the sphericity I
 use the following formula

aov_SPL = aov(SPL ~ Emotion*Material*Shoes*Musician + Error(Subject/(Emotion*Material*Shoes)), data=scrd)



Unfortunately after having read lot of material online I did not 
arrive to a solution about how to calculate for my case the epsilon of 
the Greenhouse-Geisser method for degree of freedom adjustment.


I load my data in R with this formula:

scrd<- read.csv(file='/path to data/data.csv',sep=',',header=T)



and this is the structure of the imported table:

> head(scrd)
    Subject Material       Shoes    Emotion  H2H H2H_factor SPL_factor SPL_variation     SPL Musician Weight Height
1  Subject1   Gravel dress_shoes AGGRESSIVE  468      0.736     11.591        21.283  97.383       no     90    183
2  Subject1   Gravel dress_shoes      HAPPY  719      1.129      3.188        10.071  86.171       no     90    183
3  Subject1   Gravel dress_shoes     TENDER 1129      1.774      5.114        14.176  90.276       no     90    183
4  Subject1   Gravel dress_shoes        SAD 1010      1.587     13.102        22.347  98.447       no     90    183
5  Subject1   Gravel dress_shoes    NEUTRAL  736      1.156      3.161         9.995  86.095       no     90    183
6 Subject10   Gravel dress_shoes AGGRESSIVE  635      0.998     15.849        24.000 100.100      yes     70    178
> 



I noticed that in the car package there is the Anova() function that 
comes with the Maulchy's sphericity test, but it does not take as an 
input a model generated with aov(). I need to use lm() instead, but 
since
I am not really proficient in R I do not know how to use the lm() 
starting from the loaded data and how to use Anova(). In addition, being
 a mixed design involving within and between subjects factors I wonder 
if Anova() is the most appropriate function to use for my case.


Is there anyone who could provide me with the R code to calculate the
 Maulchy's test and the epsilon of Greenhouse-Geisser for my case?


Thank you in advance
Best

Angelo


       
	[[alternative HTML version deleted]]


From lid.zigh at gmail.com  Mon Jul  6 16:05:19 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Mon, 6 Jul 2015 09:05:19 -0500
Subject: [R] question
In-Reply-To: <9B873725-D55A-47A0-84D0-6CA9568A4780@txbiomed.org>
References: <CAMqbV1Dwe=tgmF3rEki2VXLyxiAoTGQMoDmeS72vaK4f=qh_rA@mail.gmail.com>
	<CAMqbV1B2eU=WBXvVLsZi6a13DDNTseYeOAKHDnrZHK2SRYRguw@mail.gmail.com>
	<9B873725-D55A-47A0-84D0-6CA9568A4780@txbiomed.org>
Message-ID: <CAMqbV1DcP7j1hoToiuugMn+-rgpTNih9Xh91obQ0t7XuKSZ4ng@mail.gmail.com>

Hi Mark,

Thank you so much for your help.

I run your code and it works great for 5*1 character vector but if I run it
for 682*1 character vector it has some error like this:

Error in model.frame.default(formula = formula, ...) :
  variable lengths differ (found for 'egfr_v1_ckdepi')

Would you please let me know what's the reason is?

Many Thanks,
Lida

On Fri, Jul 3, 2015 at 11:38 PM, Mark Sharp <msharp at txbiomed.org> wrote:

> Lida,
>
> I expect that there is a better way to solve your problem than the process
> you propose.
>
> However, something like this may do what you want.
>
> ###
> ## met <- read.csv("your_met_file?)
> ## Since I do not have your file a made a small 5*1 character vector.
> met <- c("glycine_imp",
>             "Nacetylglycine_imp",
>             "sarcosine_imp",
>             "dimethylglycine_imp",
>             "betaine_imp")
>
> for (i in seq_along(met)) {
>   my_formula <- paste0(met[i],
> "~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
>   prep <- Scores(Z=metalofGT, formula = my_formula)
>   save(prep, file = paste0("prep", i))
> }
> ###
>
> Mark
>
>
> R. Mark Sharp, Ph.D.
> Director of Primate Records Database
> Southwest National Primate Research Center
> Texas Biomedical Research Institute
> P.O. Box 760549
> San Antonio, TX 78245-0549
> Telephone: (210)258-9476
> e-mail: msharp at TxBiomed.org
>
> > On Jul 2, 2015, at 11:48 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> >
> > Thank you so much for replying me!
> > for better understanding my problem, I explain my problem more:
> >
> > I have a 682*1 matrix called "met" , the first 5 rows similar below:
> >
> >> rownames(met)[1:5]
> >
> > [1]  "glycine_imp"
> > [2]  "Nacetylglycine_imp"
> > [3]  "sarcosine_imp"
> > [4]  "dimethylglycine_imp"
> > [5]  "betaine_imp"
> >
> > and I have a function in R that each time use one of the row names of
> "met"
> > matrix and create a new object file and I should save the objects!
> >
> > my function is  "
> >
> Scores(Z=metalofGT,formula="met[i]~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> > " that each time just I should change the met[i] and replace by row names
> > "met" one by one and for each of them I should rename the function and
> > after that I should save each object!
> > for example for first row of "met" I have
> >
> >>  prep1<-
> Scores(Z=metalofGT,formula="glycine_imp~egfr_v1_ckdepi+pc1+pc2+pc3+V1AGE01+GENDER")
> > #creat the object file for first row and called prep1###
> >
> >>  save(prep1, file="prep1.RData", compress="bzip2")      ##save the
> > object file as "prep1.RData"#####
> >
> > I should do this process for 682 row names of "met" matrix and at the
> end I
> > should have    "prep1.RData"  ,   "prep2.RData"   , "prep3.RData"
> >
> > so, would you please help me how to do it?
> >
> > Many Thanks,
> > Ati
> >
> > On Wed, Jul 1, 2015 at 1:07 PM, Lida Zeighami <lid.zigh at gmail.com>
> wrote:
> >
> >> I have 682 variables in a data frame , and a function that  I should
> feed
> >> 682 variables in this function one by one and each time save the file
> as a
> >> special name!
> >> for emaple:
> >> my data frame file includes 682 names :
> >> 1  aaa
> >> 2  bbb
> >> 3  dfdsfg
> >> 4 fghh
> >> .
> >>
> >> 682 fgfhg
> >> and a function like prep(Z, aaa, .....) and each time I should change
> the
> >> variable name in this function and read the variable from the data frame
> >> and each time I should save the file as a special name such as:
> >>
> >> prep1<- prep(z, aaa,...)
> >> prep2<- prep(z, bbb,...)
> >> prep3<- prep(z, dfdsfg,..)
> >> Prep4<- prep(z, fghh,...)
> >>
> >> How can I use loop function in R to that?
> >>
> >> Thanks
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Mon Jul  6 17:34:35 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 6 Jul 2015 10:34:35 -0500
Subject: [R] Invalid URL for R documentation
In-Reply-To: <loom.20150706T170610-303@post.gmane.org>
References: <loom.20150706T170610-303@post.gmane.org>
Message-ID: <D2D04D61-2CE4-43AF-8F5D-F78A5A64C333@me.com>


> On Jul 6, 2015, at 10:10 AM, Paul <Paul.Domaskis at gmail.com> wrote:
> 
> I tried accessing http://cran.r-project.org/doc/manuals/r-release/R-
> intro.html, which is linked to from http://cran.r-
> project.org/manuals.html.  I get the message that the URL could not be 
> found by the server. Just wondering if I'm doing something stunningly un-
> smart (it would not be the first time).


It would appear that the HTML and EPUB files for the Release versions of the manuals are not present, unless they are in the process of being generated right now. The PDFs, dated July 6, 2015, are present:

  http://cran.r-project.org/doc/manuals/r-release/

They appear to be ok for patched and devel:

  http://cran.r-project.org/doc/manuals/r-patched/
  http://cran.r-project.org/doc/manuals/r-devel/

which are dated July 4 and 5, respectively.

Checking a mirror, the HTML and EPUB files appear to be present for the July 3, 2015 dated file versions. So something happened over the weekend on the main CRAN server it would seem, depending upon the regen cycle timing.

I tried two different browsers, with refreshes, in case there was a caching issue of sorts.

Regards,

Marc Schwartz


From Paul.Domaskis at gmail.com  Mon Jul  6 17:40:53 2015
From: Paul.Domaskis at gmail.com (Paul)
Date: Mon, 6 Jul 2015 15:40:53 +0000
Subject: [R] Invalid URL for R documentation
References: <loom.20150706T170610-303@post.gmane.org>
	<D2D04D61-2CE4-43AF-8F5D-F78A5A64C333@me.com>
Message-ID: <loom.20150706T173851-56@post.gmane.org>

Marc Schwartz <marc_schwartz <at> me.com> writes:
| It would appear that the HTML and EPUB files for the Release
| versions of the manuals are not present, unless they are in the
| process of being generated right now. The PDFs, dated July 6, 2015,
| are present:
|
|   http://cran.r-project.org/doc/manuals/r-release/
|
| They appear to be ok for patched and devel:
|
|   http://cran.r-project.org/doc/manuals/r-patched/
|   http://cran.r-project.org/doc/manuals/r-devel/
|
| which are dated July 4 and 5, respectively.
|
| Checking a mirror, the HTML and EPUB files appear to be present for
| the July 3, 2015 dated file versions. So something happened over the
| weekend on the main CRAN server it would seem, depending upon the
| regen cycle timing.
|
| I tried two different browsers, with refreshes, in case there was a
| caching issue of sorts.

Thanks for investigating, Marc.  Interesting that I can actually see
the directory in which the files belong.  Been a long time since I
encountered a server that lets you do that.


From roy.mendelssohn at noaa.gov  Mon Jul  6 17:48:57 2015
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Mon, 6 Jul 2015 08:48:57 -0700
Subject: [R] opendap accessibility on Windows
In-Reply-To: <BY2PR0401MB10166FCF0AC1E3E757986F4AB9930@BY2PR0401MB1016.namprd04.prod.outlook.com>
References: <BY2PR0401MB10166FCF0AC1E3E757986F4AB9930@BY2PR0401MB1016.namprd04.prod.outlook.com>
Message-ID: <5E376486-C235-4F1E-9C6C-141F3A7C1DDB@noaa.gov>

Hi Christina:

My memory is that Pierce had problems getting the netcdf4 library to work with OpeNDAP included on Windows.  OPeNDAP does work with ncdf4 on a Mac or a Linux box.

So one option is to run on a Linux VM on Windows.  There may be more direct options, depending on what datasets you are trying to access. Can you send me the URLs you are trying to access?

Thanks,

-Roy



> On Jul 6, 2015, at 5:08 AM, Christina Hernandez <chernandez at Gmri.org> wrote:
> 
> Hello,
> 
> I am working on a project where we are accessing large data files via opendap. Myself and two of the higher-ups on the project are working on Windows machines, and we have been unable to access remote files using the ncdf4 package. We have followed the package author's notes for installation on a Windows machine, and we have successfully installed the netcdf-4 libraries and the ncdf4 R package. However, the program still works only on local files. It is not an issue with connectivity of my machine since I am able to remotely access these same data files using Matlab.
> I then attempted to follow the package author's notes on cross-compiling in a Linux environment, I was using a Ubuntu Virtual Box. These notes are available here: http://cirrus.ucsd.edu/~pierce/ncdf/how_to_build_on_windows.html
> However, I do not really know what I'm doing and I was unable to successfully cross-compile the dependencies. Has anyone dealt with this on a Windows machine? Is there a different way to access remote files through R? \We have considered writing a Python script that can be run from within an R program, so that users that are only familiar with R can still access remote files without downloading a lot of large files.
> Any advice is appreciated.
> 
> 
> Christina Hernandez
> Research Assistant
> Ecosystem Modeling Lab
> Gulf of Maine Research Institute
> chernandez at gmri.org
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From meyners.m at pg.com  Mon Jul  6 17:52:42 2015
From: meyners.m at pg.com (Meyners, Michael)
Date: Mon, 6 Jul 2015 15:52:42 +0000
Subject: [R] Bonferroni post hoc test in R for repeated measure ANOVA
 with mixed within and between subjects design
In-Reply-To: <14e63ab1bc0.angelo.arcadi@virgilio.it>
References: <14e63ab1bc0.angelo.arcadi@virgilio.it>
Message-ID: <AE85F2AAC98B094483D12EEA72230EBD72EB27CE@GADC-EMB019.na.pg.com>

Untested, but if anything, your best bet is likely something like 

summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")), test=adjusted("bonferroni"))

should work (despite the question why you'd want to use Bonferroni rather than Tukey
For a reference, see the book on the topic by the package authors. Might be in the paper, too, which is given by

citation("multcomp")

HTH, Michael


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> angelo.arcadi at virgilio.it
> Sent: Montag, 6. Juli 2015 16:01
> To: r-help at r-project.org
> Subject: [R] Bonferroni post hoc test in R for repeated measure ANOVA with
> mixed within and between subjects design
> 
> Dear List Members,
> 
> 
> 
> I need to perform a Bonferroni post hoc test in R on a table with three within
> subjects factors (Emotion, having 5 levels, Material, having 4 levels, Shoes,
> having 2 levels) and one between subject factor (Musician, having 2 levels).
> 
> 
> I normally use the Tukey method with the following formula
> 
> require(nlme)
> lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd,
> random = ~1|Subject)
> require(multcomp)
> summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))
> 
> 
> 
> I am not able to find any reference that explains with an example of R  code
> how to perform a post hoc test with the Bonferroni procedure.
> Can anyone provide an example to perform the same post hoc test in the
> code above but with Bonferroni instead of Tukey?
> 
> 
> Thank you in advance
> 
> 
> 
> Angelo
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thierry.onkelinx at inbo.be  Mon Jul  6 18:02:10 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 6 Jul 2015 18:02:10 +0200
Subject: [R] Rmarkdown / knitr naming the output file
In-Reply-To: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>
References: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>
Message-ID: <CAJuCY5zW-_QoKK-q2r0p=iAceqGMTrtiL0OOVzbkdzKdYm1X-A@mail.gmail.com>

Dear Aurora,

You acn try specifying a YAML block in the Rmd file and then render() it
with the rmarkdown package.

---
title: "Your title"
output:
  pdf_document:
    pandoc_args: [
      "--output=doc1.pdf"
    ]
---

Note that I haven't tested it.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-06 14:24 GMT+02:00 AURORA GONZALEZ VIDAL <aurora.gonzalez2 at um.es>:

> Hello.
> I have a question for Rmarkdown users.
>
> Is there any way to give a name to the output document inside the Rmd?
>
> For example, my rmd's name is "bb.Rmd" but when I knitr to pdf I want it to
> name the pdf differently than "bb.pdf", for example, "doc1.pdf". Is there
> any way to do this?
>
> Thank you very much
>
>
> ------
> Aurora Gonz?lez Vidal
>
> Secci?n Apoyo Estad?stico.
> Servicio de Apoyo a la Investigaci?n (SAI).
> Vicerrectorado de Investigaci?n.
> Universidad de Murcia
> Edif. SACE . Campus de Espinardo.
> 30100 Murcia
>
> @. aurora.gonzalez2 at um.es
> T. 868 88 7315
> F. 868 88 7302
> www.um.es/sai
> www.um.es/ae
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Mon Jul  6 18:17:35 2015
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 6 Jul 2015 12:17:35 -0400
Subject: [R] Sphericity for a repeated measure ANOVA with whithin and
	between subjects design, using R
In-Reply-To: <14e63af1f5b.angelo.arcadi@virgilio.it>
References: <14e63af1f5b.angelo.arcadi@virgilio.it>
Message-ID: <000901d0b807$454b9ed0$cfe2dc70$@mcmaster.ca>

Dear Angelo,

One way to do this is with the Anova() function in the car package; see the
following article in the R Journal:

@Article{RJournal:2013-1:fox-friendly-weisberg,
  author       = {John Fox and Michael Friendly and Sanford Weisberg}, 
  title        = {Hypothesis Tests for Multivariate Linear Models Using the
{car} Package}, 
  journal      = {The R Journal},
  year         = 2013,
  volume       = 5,
  number       = 1,
  pages        = {39--53},
  month        = jun,
  url          =
{http://journal.r-project.org/archive/2013-1/RJournal_2013-1_fox-friendly-we
isberg.pdf}

I hope this helps,
 John


-----------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.socsci.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> angelo.arcadi at virgilio.it
> Sent: July-06-15 10:05 AM
> To: r-help at r-project.org
> Subject: [R] Sphericity for a repeated measure ANOVA with whithin and
> between subjects design, using R
> 
>  Dear List Members,
> 
> 
> 
> I need to perform a four-way ANOVA with repeated measures and to
> calculate the sphericity for eventual correction of the degree of
> freedom of the F ratio.
> I have three within subjects factors (Emotion, having 5 levels,
> Material, having 4 levels, Shoes, having 2 levels) and a between subject
>  factor (Musician, having 2 levels). Without considering the sphericity
> I
>  use the following formula
> 
> aov_SPL = aov(SPL ~ Emotion*Material*Shoes*Musician +
> Error(Subject/(Emotion*Material*Shoes)), data=scrd)
> 
> 
> 
> Unfortunately after having read lot of material online I did not
> arrive to a solution about how to calculate for my case the epsilon of
> the Greenhouse-Geisser method for degree of freedom adjustment.
> 
> 
> I load my data in R with this formula:
> 
> scrd<- read.csv(file='/path to data/data.csv',sep=',',header=T)
> 
> 
> 
> and this is the structure of the imported table:
> 
> > head(scrd)
>     Subject Material       Shoes    Emotion  H2H H2H_factor SPL_factor
> SPL_variation     SPL Musician Weight Height
> 1  Subject1   Gravel dress_shoes AGGRESSIVE  468      0.736     11.591
> 21.283  97.383       no     90    183
> 2  Subject1   Gravel dress_shoes      HAPPY  719      1.129      3.188
> 10.071  86.171       no     90    183
> 3  Subject1   Gravel dress_shoes     TENDER 1129      1.774      5.114
> 14.176  90.276       no     90    183
> 4  Subject1   Gravel dress_shoes        SAD 1010      1.587     13.102
> 22.347  98.447       no     90    183
> 5  Subject1   Gravel dress_shoes    NEUTRAL  736      1.156      3.161
> 9.995  86.095       no     90    183
> 6 Subject10   Gravel dress_shoes AGGRESSIVE  635      0.998     15.849
> 24.000 100.100      yes     70    178
> >
> 
> 
> 
> I noticed that in the car package there is the Anova() function that
> comes with the Maulchy's sphericity test, but it does not take as an
> input a model generated with aov(). I need to use lm() instead, but
> since
> I am not really proficient in R I do not know how to use the lm()
> starting from the loaded data and how to use Anova(). In addition, being
>  a mixed design involving within and between subjects factors I wonder
> if Anova() is the most appropriate function to use for my case.
> 
> 
> Is there anyone who could provide me with the R code to calculate the
>  Maulchy's test and the epsilon of Greenhouse-Geisser for my case?
> 
> 
> Thank you in advance
> Best
> 
> Angelo
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From profjcnash at gmail.com  Mon Jul  6 17:36:44 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Mon, 06 Jul 2015 11:36:44 -0400
Subject: [R] Invalid URL for R documentation
In-Reply-To: <loom.20150706T172129-351@post.gmane.org>
References: <loom.20150706T170610-303@post.gmane.org>	<CAAJSdjj+UnROM6D7DyoeqnF=oSp2NOvkA10y1_7x29Rb9U+Dxg@mail.gmail.com>
	<loom.20150706T172129-351@post.gmane.org>
Message-ID: <559AA08C.6080505@gmail.com>


I'm also getting a 404. Tried https just in case.

But

http://cran.utstat.utoronto.ca/manuals.html

works and I can copy the link for R-intro and it works there.

http://cran.utstat.utoronto.ca/doc/manuals/r-release/R-intro.html

Very odd.

JN


On 15-07-06 11:24 AM, Paul wrote:
>> http://cran.r-project.org/doc/manuals/r-release/R-intro.html


From Manuel.Weinkauf at gmx.de  Mon Jul  6 17:52:38 2015
From: Manuel.Weinkauf at gmx.de (Manuel Weinkauf)
Date: Mon, 06 Jul 2015 17:52:38 +0200
Subject: [R] PCAgrid scores cannot be predicted correctly
Message-ID: <559AA446.2050401@gmx.de>

I have been running into a problem with the PCAgrid() function of the 
R-package pcaPP. When trying to predict the scores of new data using the 
resulting PCA, the predicted score values are wrong. This is true for 
both, using the predict() function and for calculating the scores 
manually after scaling the data.

The example below illustrates that: In princomp(), when I either use 
predict() or the manual calculation of the scores of the data originally 
used for the PCA, the predicted points coincide exactly with the PCA 
scores (as it should be, as they are using the same raw data). However, 
when doing this with the princomp-object calculated with the pcaPP 
package, the predicted values are nowhere close to where they should be. 
Note that the results of predict() exactly match the results calculated 
manually, so it is not that predict() could not handle this object 
correctly.

Can anybody explain this weird behaviour to me? Thanks a lot.


##EXAMPLE##
library(pcaPP)

#Create data
t1<-rnorm(10, 10, 1)
t2<-rnorm(10, 8, 2)
t3<-rnorm(10, 60, 4)
t4<-rnorm(10, 1, 0.05)
Dat<-matrix(c(t1, t2, t3 , t4), ncol=4)
colnames(Dat)<-paste("Var", 1:4, sep=".")

win.graph(20, 10, 10)
layout(matrix(c(1, 2), 1, 2))
#Normal PCA
PCA<-princomp(Dat)
PCA.pred<-predict(PCA, Dat)
Dat.Scale<-scale(Dat, center=PCA$center, scale=PCA$scale)
Load<-PCA$loadings
PCA.man<-matrix(NA, nrow(PCA$scores), 2)
for (i in 1:nrow(PCA.man)) {
PCA.man[i,1]<-Dat.Scale[i,1]*Load[1,1]+Dat.Scale[i,2]*Load[2,1]+Dat.Scale[i,3]*Load[3,1]+Dat.Scale[i,4]*Load[4,1]
PCA.man[i,2]<-Dat.Scale[i,1]*Load[1,2]+Dat.Scale[i,2]*Load[2,2]+Dat.Scale[i,3]*Load[3,2]+Dat.Scale[i,4]*Load[4,2]
}
plot(PCA$scores[,1:2], pch=1, cex=2, main="Normal PCA")
points(PCA.pred, pch=16)
points(PCA.man, pch=0, col="red", cex=3)
legend("topleft", pch=c(1, 16, 0), col=c("black", "black", "red"), 
legend=c("Ordination", "Prediction", "Manual prediction"))

#robust PCA
rPCA<-PCAgrid(Dat, k=4, center=median, method="qn", scale=mad)
rPCA.pred<-predict(rPCA, Dat)
Dat.Scale<-scale(Dat, center=rPCA$center, scale=rPCA$scale)
Load<-rPCA$loadings
rPCA.man<-matrix(NA, nrow(rPCA$scores), 2)
for (i in 1:nrow(rPCA.man)) {
rPCA.man[i,1]<-Dat.Scale[i,1]*Load[1,1]+Dat.Scale[i,2]*Load[2,1]+Dat.Scale[i,3]*Load[3,1]+Dat.Scale[i,4]*Load[4,1]
rPCA.man[i,2]<-Dat.Scale[i,1]*Load[1,2]+Dat.Scale[i,2]*Load[2,2]+Dat.Scale[i,3]*Load[3,2]+Dat.Scale[i,4]*Load[4,2]
}
plot(rPCA$scores[,1:2], pch=1, cex=2, main="Robust PCA")
points(rPCA.pred, pch=16)
points(rPCA.man, pch=0, col="red", cex=3)
legend("topleft", pch=c(1, 16, 0), col=c("black", "black", "red"), 
legend=c("Ordination", "Prediction", "Manual prediction"))



-- 
Dr Manuel Weinkauf
MARUM Bremen
Room MARUM II?2050
Leobener Stra?e
28359 Bremen
Germany
e-mail: mweinkauf at marum.de


From vyshnnaviammu at gmail.com  Mon Jul  6 17:56:52 2015
From: vyshnnaviammu at gmail.com (Vyshnnavi Parthasarathy)
Date: Mon, 6 Jul 2015 08:56:52 -0700
Subject: [R] Leave one out procedure - R
Message-ID: <CADeBCTn8H6DhcbiEEq=E5zbBrCKaJsjm6vZ=oFqbDvB8eqVBMQ@mail.gmail.com>

Hello,
I am still in the process of familiarizing myself with R, pardon me if this
is basic. I want to run a leave one out procedure for a 40 member dataset.
At the moment I am doing it via a simple for loop. I wanted to know if
there is a superior way to do it. I read about the loocv command here -
http://artax.karlin.mff.cuni.cz/r-help/library/DMwR/html/loocv.html. Since
I need to save the output generated in each iteration, I am not very sure
how exactly to implement the same using the loocv command. If you could
give me an insight into how to do it/ suggest quicker ways to do leave one
out cross validation, it would be really helpful!

Thanks,
Vyshnnavi

	[[alternative HTML version deleted]]


From erinm.hodgess at gmail.com  Mon Jul  6 18:37:19 2015
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Mon, 6 Jul 2015 11:37:19 -0500
Subject: [R] Rgui not working in Windows Version 8.1
In-Reply-To: <CAHrK515d6v3q_-GpG7KL6ZNAg92H5tBY2XPQcbb2j8zj47dagA@mail.gmail.com>
References: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>
	<CAHrK515d6v3q_-GpG7KL6ZNAg92H5tBY2XPQcbb2j8zj47dagA@mail.gmail.com>
Message-ID: <CACxE24mqxFb0wa2m7SUY=Wf++dSSX=Ki53peNs56xb86Oa0Mbw@mail.gmail.com>

I have tried that already...no luck.

Thank you though!
Sincerely,
Erin


On Mon, Jul 6, 2015 at 8:22 AM, John C Frain <frainj at gmail.com> wrote:

> I am running
>
> R version 3.2.1 Patched (2015-07-02 r68625) -- "World-Famous Astronaut"
> Copyright (C) 2015 The R Foundation for Statistical Computing
> Platform: x86_64-w64-mingw32/x64 (64-bit)
>
> and do not have any problems with Rgui. The output from Sys.info() is
>
> > Sys.info()
>        sysname        release        version       nodename        machine
>      "Windows"        "7 x64"   "build 9200"      "DELL745"       "x86-64"
>
> I don't know why the release is given as  "7 x64" because I am running
> Windows 8.1 Professional on this PC.  If you have not already done so I
> would uninstall R and reinstall it to see if that solves the problem.
>
>
> John C Frain
> 3 Aranleigh Park
> Rathfarnham
> Dublin 14
> Ireland
> www.tcd.ie/Economics/staff/frainj/home.html
> mailto:frainj at tcd.ie
> mailto:frainj at gmail.com
>
> On 6 July 2015 at 02:14, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>
>> Hello everyone!
>>
>> I have been having trouble with Rgui on a Windows machine running Version
>> 8.1.  I download the binary from CRAN.  It installs fine.  But when I try
>> to run it, the console opens for a moment, and then it disappears.
>>
>> I believe this was on the Windows FAQ list.  I was wondering if there was
>> any fix/patch please.
>>
>> This is true for R Version 3.2.0 and 3.2.1.
>>
>> Thank you,
>> Sincerely,
>> Erin
>>
>>
>> --
>> Erin Hodgess
>> Associate Professor
>> Department of Mathematical and Statistics
>> University of Houston - Downtown
>> mailto: erinm.hodgess at gmail.com
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Mon Jul  6 18:51:15 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Mon, 6 Jul 2015 12:51:15 -0400
Subject: [R] Web/Server Interface for R files
In-Reply-To: <CAOPdpkZ0cM-uakPHWmXWoVreQ=_KMWPpzBEkwJshA5Ge5+i2CQ@mail.gmail.com>
References: <CAOPdpkZ0cM-uakPHWmXWoVreQ=_KMWPpzBEkwJshA5Ge5+i2CQ@mail.gmail.com>
Message-ID: <CA+vqiLH6+myb9nwjNSFfBAM_MZFc5-0Y7EZjfTd+dny1USyaDw@mail.gmail.com>

souce() works for URLs, so this should "just work" for http. Did you try it?

Best,
Ista

On Mon, Jul 6, 2015 at 12:28 AM, Kunal Shah <kunalshah305 at gmail.com> wrote:
> Hello,
>
> I have the following code
>
> #### From excel.link package
>
> xl.workbook.activate("ADSnippetXLS.xlsx")
> source('ActiveDeltaSnippet2.R')
>
> #### "ActiveDeltaSnippet2.R" takes data from ADSnippetXLS, performs some
> computations on the data and outputs the data back into "ADSnippetXLS.xlsx"
>
>
> I dont want that the file "ActiveDeltaSnippet2.R" to be stored on the same
> PC. In fact I want to call this R file which might be stored on any
> server/Web
>
> I tried to search Shiny and ROOK but got lost in the literature.
> Any help will be appreciated
>
> Regards
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Mon Jul  6 20:30:04 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 6 Jul 2015 20:30:04 +0200
Subject: [R] Rgui not working in Windows Version 8.1
In-Reply-To: <CACxE24mqxFb0wa2m7SUY=Wf++dSSX=Ki53peNs56xb86Oa0Mbw@mail.gmail.com>
References: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>
	<CAHrK515d6v3q_-GpG7KL6ZNAg92H5tBY2XPQcbb2j8zj47dagA@mail.gmail.com>
	<CACxE24mqxFb0wa2m7SUY=Wf++dSSX=Ki53peNs56xb86Oa0Mbw@mail.gmail.com>
Message-ID: <5CF74BD7-58F6-46C7-85ED-978AAB3A16D7@gmail.com>

Hm, could you perhaps see whether R or RGui is the culprit by trying to start R in a Terminal ("DOS box"). If such a thing still exists in Windows 8.1, that is (I don't _do_ windows...).

-Peter

> On 06 Jul 2015, at 18:37 , Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> 
> I have tried that already...no luck.
> 
> Thank you though!
> Sincerely,
> Erin
> 
> 
> On Mon, Jul 6, 2015 at 8:22 AM, John C Frain <frainj at gmail.com> wrote:
> 
>> I am running
>> 
>> R version 3.2.1 Patched (2015-07-02 r68625) -- "World-Famous Astronaut"
>> Copyright (C) 2015 The R Foundation for Statistical Computing
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> 
>> and do not have any problems with Rgui. The output from Sys.info() is
>> 
>>> Sys.info()
>>       sysname        release        version       nodename        machine
>>     "Windows"        "7 x64"   "build 9200"      "DELL745"       "x86-64"
>> 
>> I don't know why the release is given as  "7 x64" because I am running
>> Windows 8.1 Professional on this PC.  If you have not already done so I
>> would uninstall R and reinstall it to see if that solves the problem.
>> 
>> 
>> John C Frain
>> 3 Aranleigh Park
>> Rathfarnham
>> Dublin 14
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.html
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>> 
>> On 6 July 2015 at 02:14, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>> 
>>> Hello everyone!
>>> 
>>> I have been having trouble with Rgui on a Windows machine running Version
>>> 8.1.  I download the binary from CRAN.  It installs fine.  But when I try
>>> to run it, the console opens for a moment, and then it disappears.
>>> 
>>> I believe this was on the Windows FAQ list.  I was wondering if there was
>>> any fix/patch please.
>>> 
>>> This is true for R Version 3.2.0 and 3.2.1.
>>> 
>>> Thank you,
>>> Sincerely,
>>> Erin
>>> 
>>> 
>>> --
>>> Erin Hodgess
>>> Associate Professor
>>> Department of Mathematical and Statistics
>>> University of Houston - Downtown
>>> mailto: erinm.hodgess at gmail.com
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 
> 
> 
> -- 
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From erinm.hodgess at gmail.com  Mon Jul  6 20:39:04 2015
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Mon, 6 Jul 2015 13:39:04 -0500
Subject: [R] Rgui not working in Windows Version 8.1
In-Reply-To: <5CF74BD7-58F6-46C7-85ED-978AAB3A16D7@gmail.com>
References: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>
	<CAHrK515d6v3q_-GpG7KL6ZNAg92H5tBY2XPQcbb2j8zj47dagA@mail.gmail.com>
	<CACxE24mqxFb0wa2m7SUY=Wf++dSSX=Ki53peNs56xb86Oa0Mbw@mail.gmail.com>
	<5CF74BD7-58F6-46C7-85ED-978AAB3A16D7@gmail.com>
Message-ID: <CACxE24kDKXVPDeuJLvnj9pLHxUwhHpEqpvytwEOZEKcUuUS_Lw@mail.gmail.com>

I'll try that next.

Love the "I don't do Windows".  Do they give you a pane?


On Mon, Jul 6, 2015 at 1:30 PM, peter dalgaard <pdalgd at gmail.com> wrote:

> Hm, could you perhaps see whether R or RGui is the culprit by trying to
> start R in a Terminal ("DOS box"). If such a thing still exists in Windows
> 8.1, that is (I don't _do_ windows...).
>
> -Peter
>
> > On 06 Jul 2015, at 18:37 , Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> >
> > I have tried that already...no luck.
> >
> > Thank you though!
> > Sincerely,
> > Erin
> >
> >
> > On Mon, Jul 6, 2015 at 8:22 AM, John C Frain <frainj at gmail.com> wrote:
> >
> >> I am running
> >>
> >> R version 3.2.1 Patched (2015-07-02 r68625) -- "World-Famous Astronaut"
> >> Copyright (C) 2015 The R Foundation for Statistical Computing
> >> Platform: x86_64-w64-mingw32/x64 (64-bit)
> >>
> >> and do not have any problems with Rgui. The output from Sys.info() is
> >>
> >>> Sys.info()
> >>       sysname        release        version       nodename
> machine
> >>     "Windows"        "7 x64"   "build 9200"      "DELL745"
>  "x86-64"
> >>
> >> I don't know why the release is given as  "7 x64" because I am running
> >> Windows 8.1 Professional on this PC.  If you have not already done so I
> >> would uninstall R and reinstall it to see if that solves the problem.
> >>
> >>
> >> John C Frain
> >> 3 Aranleigh Park
> >> Rathfarnham
> >> Dublin 14
> >> Ireland
> >> www.tcd.ie/Economics/staff/frainj/home.html
> >> mailto:frainj at tcd.ie
> >> mailto:frainj at gmail.com
> >>
> >> On 6 July 2015 at 02:14, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> >>
> >>> Hello everyone!
> >>>
> >>> I have been having trouble with Rgui on a Windows machine running
> Version
> >>> 8.1.  I download the binary from CRAN.  It installs fine.  But when I
> try
> >>> to run it, the console opens for a moment, and then it disappears.
> >>>
> >>> I believe this was on the Windows FAQ list.  I was wondering if there
> was
> >>> any fix/patch please.
> >>>
> >>> This is true for R Version 3.2.0 and 3.2.1.
> >>>
> >>> Thank you,
> >>> Sincerely,
> >>> Erin
> >>>
> >>>
> >>> --
> >>> Erin Hodgess
> >>> Associate Professor
> >>> Department of Mathematical and Statistics
> >>> University of Houston - Downtown
> >>> mailto: erinm.hodgess at gmail.com
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>
> >
> >
> > --
> > Erin Hodgess
> > Associate Professor
> > Department of Mathematical and Statistics
> > University of Houston - Downtown
> > mailto: erinm.hodgess at gmail.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From frainj at gmail.com  Mon Jul  6 21:06:58 2015
From: frainj at gmail.com (John C Frain)
Date: Mon, 6 Jul 2015 20:06:58 +0100
Subject: [R] Rgui not working in Windows Version 8.1
In-Reply-To: <CACxE24mqxFb0wa2m7SUY=Wf++dSSX=Ki53peNs56xb86Oa0Mbw@mail.gmail.com>
References: <CACxE24meN3LZSZ8B3+6GN-rE4C8PBzDM6OmBjeVw+MqaYRPhtw@mail.gmail.com>
	<CAHrK515d6v3q_-GpG7KL6ZNAg92H5tBY2XPQcbb2j8zj47dagA@mail.gmail.com>
	<CACxE24mqxFb0wa2m7SUY=Wf++dSSX=Ki53peNs56xb86Oa0Mbw@mail.gmail.com>
Message-ID: <CAHrK515yXkCLn8-Q5SjvPfbOn3ANTeL3kFOUoX6S_WUFrY2O7Q@mail.gmail.com>

You might try starting Rterm or Rgui with the option --vanilla. If this
starts the you have a corrupted .Rdata file or there is a problem with
something in your environment. Have a look at appendix B of an introduction
to R and you may be able to work out what is wrong. Check that some part of
your current R system is not using something from an earlier version. While
this should not happen I did manage something like it some time ago. I
don't recall the exact problem but it can happen.

@Peter There still are terminal programs in Windows 8.1. I use an interface
Conemu64 which is an interface to the windows console. I was working with
gcc sometime ago and have recently discovered that many unix shell commands
now work in conemu64. This rather proves your point.

John C Frain
3 Aranleigh Park
Rathfarnham
Dublin 14
Ireland
www.tcd.ie/Economics/staff/frainj/home.html
mailto:frainj at tcd.ie
mailto:frainj at gmail.com

On 6 July 2015 at 17:37, Erin Hodgess <erinm.hodgess at gmail.com> wrote:

> I have tried that already...no luck.
>
> Thank you though!
> Sincerely,
> Erin
>
>
> On Mon, Jul 6, 2015 at 8:22 AM, John C Frain <frainj at gmail.com> wrote:
>
>> I am running
>>
>> R version 3.2.1 Patched (2015-07-02 r68625) -- "World-Famous Astronaut"
>> Copyright (C) 2015 The R Foundation for Statistical Computing
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>>
>> and do not have any problems with Rgui. The output from Sys.info() is
>>
>> > Sys.info()
>>        sysname        release        version       nodename
>>  machine
>>      "Windows"        "7 x64"   "build 9200"      "DELL745"
>> "x86-64"
>>
>> I don't know why the release is given as  "7 x64" because I am running
>> Windows 8.1 Professional on this PC.  If you have not already done so I
>> would uninstall R and reinstall it to see if that solves the problem.
>>
>>
>> John C Frain
>> 3 Aranleigh Park
>> Rathfarnham
>> Dublin 14
>> Ireland
>> www.tcd.ie/Economics/staff/frainj/home.html
>> mailto:frainj at tcd.ie
>> mailto:frainj at gmail.com
>>
>> On 6 July 2015 at 02:14, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
>>
>>> Hello everyone!
>>>
>>> I have been having trouble with Rgui on a Windows machine running Version
>>> 8.1.  I download the binary from CRAN.  It installs fine.  But when I try
>>> to run it, the console opens for a moment, and then it disappears.
>>>
>>> I believe this was on the Windows FAQ list.  I was wondering if there was
>>> any fix/patch please.
>>>
>>> This is true for R Version 3.2.0 and 3.2.1.
>>>
>>> Thank you,
>>> Sincerely,
>>> Erin
>>>
>>>
>>> --
>>> Erin Hodgess
>>> Associate Professor
>>> Department of Mathematical and Statistics
>>> University of Houston - Downtown
>>> mailto: erinm.hodgess at gmail.com
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>
>
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Jul  6 21:10:24 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 6 Jul 2015 12:10:24 -0700
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <55970BF9.400@auckland.ac.nz>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
Message-ID: <CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>

> The CRAN guidelines should be rewritten so that they say what they *mean*.
> If a complete sentence is not actually required --- and it seems
abundantly clear
> that it is not --- then guidelines should not say so.  Rather they should
say,
> clearly and comprehensibly, what actually *is* required.

This may be true, but also think of the user when you write the description.
If you are scanning a long list of descriptions looking for a package to
use,
seeing a description that starts with 'A package for' just slows you down.
Seeing a description that includes 'designed to' leaves you wondering if the
implementation is woefully incomplete.  You want to go beyond what CRAN
can test for.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Jul 3, 2015 at 3:26 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:

> On 04/07/15 06:27, Yihui Xie wrote:
>
>> Sigh, how natural it is to say "This package ...", but you probably
>> don't know a package can be easily rejected by CRAN simply because of
>> this phrase "This package" (it has been clearly stated in the R-exts
>> manual).
>>
>
> Urrrkkkk!  I *did* "know" that, but had forgotten.  Apologies for my
> wrong-headed suggestion.  Thanks for pointing out my error.
>
>  I don't think the grammar is the problem here. When in doubt, I always
>> check what MASS does:
>> http://cran.rstudio.com/web/packages/MASS/index.html Turns out its
>> description is not a complete sentence, either.
>>
>> Sounds like R has become a language for statistical computing and
>> graphics, plus English grammar since 3.0.x.
>>
>
> The CRAN guidelines should be rewritten so that they say what they
> *mean*.  If a complete sentence is not actually required --- and it seems
> abundantly clear that it is not --- then guidelines should not say so.
> Rather they should say, clearly and comprehensibly, what actually *is*
> required.
>
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From xie at yihui.name  Mon Jul  6 21:24:34 2015
From: xie at yihui.name (Yihui Xie)
Date: Mon, 6 Jul 2015 14:24:34 -0500
Subject: [R] Rmarkdown / knitr naming the output file
In-Reply-To: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>
References: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>
Message-ID: <CANROs4e3XBz=FQ8wEvLvsEv4pHD10GLS-euxGqWs0KeaZ4=YXg@mail.gmail.com>

There is no direct way to get it, but you can get 1) the input
filename via knitr::current_input(), and 2) the output format via
knitr::opts_knit$get('rmarkdown.pandoc.to'). You may be able to figure
out the output filename based on these two pieces of information.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Mon, Jul 6, 2015 at 7:24 AM, AURORA GONZALEZ VIDAL
<aurora.gonzalez2 at um.es> wrote:
> Hello.
> I have a question for Rmarkdown users.
>
> Is there any way to give a name to the output document inside the Rmd?
>
> For example, my rmd's name is "bb.Rmd" but when I knitr to pdf I want it to
> name the pdf differently than "bb.pdf", for example, "doc1.pdf". Is there
> any way to do this?
>
> Thank you very much
>
>
> ------
> Aurora Gonz?lez Vidal
>
> Secci?n Apoyo Estad?stico.
> Servicio de Apoyo a la Investigaci?n (SAI).
> Vicerrectorado de Investigaci?n.
> Universidad de Murcia
> Edif. SACE . Campus de Espinardo.
> 30100 Murcia
>
> @. aurora.gonzalez2 at um.es
> T. 868 88 7315
> F. 868 88 7302
> www.um.es/sai
> www.um.es/ae


From r.turner at auckland.ac.nz  Mon Jul  6 23:09:54 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 7 Jul 2015 09:09:54 +1200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
Message-ID: <559AEEA2.2020805@auckland.ac.nz>

On 07/07/15 07:10, William Dunlap wrote:

[Rolf Turner wrote.]

>> The CRAN guidelines should be rewritten so that they say what they *mean*.
>> If a complete sentence is not actually required --- and it seems abundantly clear
>> that it is not --- then guidelines should not say so.  Rather they should say,
>> clearly and comprehensibly, what actually *is* required.
>
> This may be true, but also think of the user when you write the description.
> If you are scanning a long list of descriptions looking for a package to
> use,
> seeing a description that starts with 'A package for' just slows you down.
> Seeing a description that includes 'designed to' leaves you wondering if the
> implementation is woefully incomplete.  You want to go beyond what CRAN
> can test for.

All very true and sound and wise, but what has this got to do with 
complete sentences?  The package checker issues a message saying that it 
wants a complete sentence when this has nothing to do with what it 
*really* wants.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From murdoch.duncan at gmail.com  Mon Jul  6 23:19:45 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 06 Jul 2015 17:19:45 -0400
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <559AEEA2.2020805@auckland.ac.nz>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>	<55969185.5000604@auckland.ac.nz>	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>	<55970BF9.400@auckland.ac.nz>	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz>
Message-ID: <559AF0F1.9010902@gmail.com>

On 06/07/2015 5:09 PM, Rolf Turner wrote:
> On 07/07/15 07:10, William Dunlap wrote:
> 
> [Rolf Turner wrote.]
> 
>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
>>> If a complete sentence is not actually required --- and it seems abundantly clear
>>> that it is not --- then guidelines should not say so.  Rather they should say,
>>> clearly and comprehensibly, what actually *is* required.
>>
>> This may be true, but also think of the user when you write the description.
>> If you are scanning a long list of descriptions looking for a package to
>> use,
>> seeing a description that starts with 'A package for' just slows you down.
>> Seeing a description that includes 'designed to' leaves you wondering if the
>> implementation is woefully incomplete.  You want to go beyond what CRAN
>> can test for.
> 
> All very true and sound and wise, but what has this got to do with 
> complete sentences?  The package checker issues a message saying that it 
> wants a complete sentence when this has nothing to do with what it 
> *really* wants.

That's false.  If you haven't given a complete sentence, you might still
pass, but if you have, you will pass.  That's not "nothing to do" with
what it really wants, it's just an imperfect test that fails to detect
violations of the guidelines.

As we've seen, it sometimes also makes mistakes in the other direction.
 I'd say those are more serious.

Duncan Murdoch


From mdsumner at gmail.com  Mon Jul  6 23:40:49 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Mon, 06 Jul 2015 21:40:49 +0000
Subject: [R] opendap accessibility on Windows
In-Reply-To: <BY2PR0401MB10166FCF0AC1E3E757986F4AB9930@BY2PR0401MB1016.namprd04.prod.outlook.com>
References: <BY2PR0401MB10166FCF0AC1E3E757986F4AB9930@BY2PR0401MB1016.namprd04.prod.outlook.com>
Message-ID: <CAAcGz9_AGAQ5Nz+nPpZ_Gd2__FYAzCntj8XSjX832uK-84P7yA@mail.gmail.com>

On Tue, 7 Jul 2015 at 01:38 Christina Hernandez <chernandez at gmri.org> wrote:

> Hello,
>
> I am working on a project where we are accessing large data files via
> opendap. Myself and two of the higher-ups on the project are working on
> Windows machines, and we have been unable to access remote files using the
> ncdf4 package. We have followed the package author's notes for installation
> on a Windows machine, and we have successfully installed the netcdf-4
> libraries and the ncdf4 R package.


I've done this on Windows by using Unidata's DLLs that they provide for
download and including them in the build process for ncdf4:

http://www.unidata.ucar.edu/software/netcdf/docs/winbin.html

But this leaves you several branches out with an unsupported package build
(by CRAN) and with Microsoft compiled opendap files.

Please note that you don't need ncdf4 for NetCDF4 faciilties, RNetCDF (and
ncdf) can be built with that version and many features (including remote
file access) work fine.

I would aim for working on Linux, it's much more straightforward to
build/get these tools there. It would be excellent if CRAN's win builder
had these standard libraries, and others like HDF4/5,  but I don't know if
that is possible.

cheers, Mike.

However, the program still works only on local files. It is not an issue
> with connectivity of my machine since I am able to remotely access these
> same data files using Matlab.
> I then attempted to follow the package author's notes on cross-compiling
> in a Linux environment, I was using a Ubuntu Virtual Box. These notes are
> available here:
> http://cirrus.ucsd.edu/~pierce/ncdf/how_to_build_on_windows.html
> However, I do not really know what I'm doing and I was unable to
> successfully cross-compile the dependencies. Has anyone dealt with this on
> a Windows machine? Is there a different way to access remote files through
> R? \We have considered writing a Python script that can be run from within
> an R program, so that users that are only familiar with R can still access
> remote files without downloading a lot of large files.
> Any advice is appreciated.
>
>
> Christina Hernandez
> Research Assistant
> Ecosystem Modeling Lab
> Gulf of Maine Research Institute
> chernandez at gmri.org
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From johannes at huesing.name  Mon Jul  6 21:40:26 2015
From: johannes at huesing.name (Johannes Huesing)
Date: Mon, 6 Jul 2015 21:40:26 +0200
Subject: [R] problem understanding grid coordinate systems
Message-ID: <20150706194026.GB27545@huesing.name>

According to "R Graphics" by Paul Murrell, the coordinates that were used for the
most recent lattics plot can be retrieved with "native" units.

I have difficulties to access these coordinates. The following code 
renders the following results:

> library(grid)
> library(lattice)
> pl <- xyplot(1:10 ~ 10:1)
> print(pl)
> convertX(unit(.9, "npc"), "native")
[1] 605.7native
> R.version
                _                           
platform       i686-pc-linux-gnu           
arch           i686                        
os             linux-gnu                   
system         i686, linux-gnu             
status                                     
major          3                           
minor          0.2                         
year           2013                        
month          09                          
day            25                          
svn rev        63987                       
language       R                           
version.string R version 3.0.2 (2013-09-25)
nickname       Frisbee Sailing 

I had expected something around 9.5 in native coordinates. Which coordinate
system do I have to address instead?

-- 
Johannes H?sing               There is something fascinating about science. 
                               One gets such wholesale returns of conjecture 
mailto:johannes at huesing.name  from such a trifling investment of fact.                
http://derwisch.wikidot.com         (Mark Twain, "Life on the Mississippi")


From pdalgd at gmail.com  Tue Jul  7 00:12:25 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 7 Jul 2015 00:12:25 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <559AF0F1.9010902@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
Message-ID: <CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>


> On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 06/07/2015 5:09 PM, Rolf Turner wrote:
>> On 07/07/15 07:10, William Dunlap wrote:
>> 
>> [Rolf Turner wrote.]
>> 
>>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
>>>> If a complete sentence is not actually required --- and it seems abundantly clear
>>>> that it is not --- then guidelines should not say so.  Rather they should say,
>>>> clearly and comprehensibly, what actually *is* required.
>>> 
>>> This may be true, but also think of the user when you write the description.
>>> If you are scanning a long list of descriptions looking for a package to
>>> use,
>>> seeing a description that starts with 'A package for' just slows you down.
>>> Seeing a description that includes 'designed to' leaves you wondering if the
>>> implementation is woefully incomplete.  You want to go beyond what CRAN
>>> can test for.
>> 
>> All very true and sound and wise, but what has this got to do with 
>> complete sentences?  The package checker issues a message saying that it 
>> wants a complete sentence when this has nothing to do with what it 
>> *really* wants.
> 
> That's false.  If you haven't given a complete sentence, you might still
> pass, but if you have, you will pass.  That's not "nothing to do" with
> what it really wants, it's just an imperfect test that fails to detect
> violations of the guidelines.
> 
> As we've seen, it sometimes also makes mistakes in the other direction.
> I'd say those are more serious.
> 
> Duncan Murdoch
> 

Ackchewly....

I don't think what we want is what we say that we want. A quick check suggests that many/most packages use "headline speech", as in "Provides functions for analysis of foo, with special emphasis on bar.", which seems perfectly ok.  As others have indicated, prefixing with "This package" would be rather useless. However, I'm at a loss as to how to describe what it is that we want, much less how to translate it to a dozen other languages. 

-pd
-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From angelo.arcadi at virgilio.it  Tue Jul  7 00:50:49 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 7 Jul 2015 00:50:49 +0200 (CEST)
Subject: [R] R: RE: Bonferroni post hoc test in R for repeated measure ANOVA
 with mixed within and between subjects design
Message-ID: <14e65909091.angelo.arcadi@virgilio.it>

Dear Michael,
thank you for your answer, however, I am not asking for the tukey
with the bonferroni adjustment, but doing the post hoc with the bonferroni method.
Apparently this is done easily in SPSS, I am wondering whether it is possible with R.

Can anyone help me?

Thanks in advance


Angelo



----Messaggio originale----
Da: meyners.m at pg.com
Data: 6-lug-2015 17.52
A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, "r-help at r-project.org"<r-help at r-project.org>
Ogg: RE: [R] Bonferroni post hoc test in R for repeated measure ANOVA with mixed within and between subjects design

Untested, but if anything, your best bet is likely something like 

summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")), test=adjusted("bonferroni"))

should work (despite the question why you'd want to use Bonferroni rather than Tukey
For a reference, see the book on the topic by the package authors. Might be in the paper, too, which is given by

citation("multcomp")

HTH, Michael


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> angelo.arcadi at virgilio.it
> Sent: Montag, 6. Juli 2015 16:01
> To: r-help at r-project.org
> Subject: [R] Bonferroni post hoc test in R for repeated measure ANOVA with
> mixed within and between subjects design
> 
> Dear List Members,
> 
> 
> 
> I need to perform a Bonferroni post hoc test in R on a table with three within
> subjects factors (Emotion, having 5 levels, Material, having 4 levels, Shoes,
> having 2 levels) and one between subject factor (Musician, having 2 levels).
> 
> 
> I normally use the Tukey method with the following formula
> 
> require(nlme)
> lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd,
> random = ~1|Subject)
> require(multcomp)
> summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))
> 
> 
> 
> I am not able to find any reference that explains with an example of R  code
> how to perform a post hoc test with the Bonferroni procedure.
> Can anyone provide an example to perform the same post hoc test in the
> code above but with Bonferroni instead of Tukey?
> 
> 
> Thank you in advance
> 
> 
> 
> Angelo
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.



   
	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Tue Jul  7 01:19:52 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Tue, 7 Jul 2015 09:19:52 +1000
Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
	alternative for Lattice)
In-Reply-To: <CAMk+s2QqBw_Xjp+nUtxakFgRHU=EcqxyJ22taeq-t_wAKE4jyw@mail.gmail.com>
References: <CAMk+s2QqBw_Xjp+nUtxakFgRHU=EcqxyJ22taeq-t_wAKE4jyw@mail.gmail.com>
Message-ID: <000001d0b842$43905870$cab10950$@bigpond.com>

Hi Luigi

Not exactly sure what you want

Have a look at 
https://stat.ethz.ch/pipermail/r-help/2007-May/132785.html
and
https://stat.ethz.ch/pipermail/r-help/2007-July/135551.html

otherwise have a look at ?trellis.focus and
https://stat.ethz.ch/pipermail/r-help/2006-July/109585.html

failing that ?gridRect and ?gridText from library(grid)

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
Marongiu
Sent: Monday, 6 July 2015 09:56
To: r-help
Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
alternative for Lattice)

Dear all,
I would like to add an outer strip or something like that on a lattice
plot I am making. Such plot contains 384 cells and, since I am not
interested in the axis values, I set:
           scales = list(
               x = list(draw = FALSE),
               y = list(draw = FALSE),
               relation="same"
               ),
on a xyplot from the LATTICE package.
Nevertheless there are axis labels which run like:
           ylab= "Y axis",
           xlab= "X axis",
I would like to place some more information regarding the individual
cells thus I would like to draw a sort of extra axis labels that are
similar to the outer strip of the LATTICE_EXTRA package, that is
markers placed between the axis labels and the axis values and
centered for each cells, typically placed on the top and left sides of
the plot. This is performed by the useOuterStrips function but:
a) LatticeExtra is not in the CRAN repository thus I have to install
it through a more laborious approach which makes LatticeExtra less
direct than Lattice
b)  useOuterStrips uses information directly from the data whereas I
will have to provide the extra information from ad hoc vectors not
present in the data set.

The question therefore is: is there a way to write text from a vector
in the top and left corners of a lattice xyplot and place the
individual elements at the centre of the rows and columns that compose
the graph?

Many thanks,
Luigi

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Tue Jul  7 01:52:51 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 6 Jul 2015 15:52:51 -0800
Subject: [R] Leave one out procedure - R
In-Reply-To: <CADeBCTn8H6DhcbiEEq=E5zbBrCKaJsjm6vZ=oFqbDvB8eqVBMQ@mail.gmail.com>
Message-ID: <61AF2D1D50D.00000BB3jrkrideau@inbox.com>


Hi Vyshnnavi,

I'd suggest having a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html for some hints on how to ask questions on the R-help List.

At the moment I don't think you have supplied enough information, code, and data for people to easily answer your questions.

BTW if you are supplying sample data the best way is to use the dput() function. It provides an exact copy of the data set that you are working with. 

Sorry not to be of any substantive help.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: vyshnnaviammu at gmail.com
> Sent: Mon, 6 Jul 2015 08:56:52 -0700
> To: r-help at r-project.org
> Subject: [R] Leave one out procedure - R
> 
> Hello,
> I am still in the process of familiarizing myself with R, pardon me if
> this
> is basic. I want to run a leave one out procedure for a 40 member
> dataset.
> At the moment I am doing it via a simple for loop. I wanted to know if
> there is a superior way to do it. I read about the loocv command here -
> http://artax.karlin.mff.cuni.cz/r-help/library/DMwR/html/loocv.html.
> Since
> I need to save the output generated in each iteration, I am not very sure
> how exactly to implement the same using the loocv command. If you could
> give me an insight into how to do it/ suggest quicker ways to do leave
> one
> out cross validation, it would be really helpful!
> 
> Thanks,
> Vyshnnavi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
GET FREE 5GB EMAIL - Check out spam free email with many cool features!
Visit http://www.inbox.com/email to find out more!


From jfox at mcmaster.ca  Tue Jul  7 03:23:30 2015
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 06 Jul 2015 21:23:30 -0400
Subject: [R] R: RE: Bonferroni post hoc test in R for repeated measure
	ANOVA with mixed within and between subjects design
In-Reply-To: <14e65909091.angelo.arcadi@virgilio.it>
References: <14e65909091.angelo.arcadi@virgilio.it>
Message-ID: <web-564884568@cgpsrv2.cis.mcmaster.ca>

Dear Angelo,

The Bonferroni p-value is just the ordinary p-value times the number of tests, so, since R supports multiplication, you can apply the Bonferroni adjustment in R. Because Bonferroni tests for multiple comparisons can be very conservative, asking why you want to use them is a fair question.

Best,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	
	
	
On Tue, 7 Jul 2015 00:50:49 +0200 (CEST)
 "angelo.arcadi at virgilio.it" <angelo.arcadi at virgilio.it> wrote:
> Dear Michael,
> thank you for your answer, however, I am not asking for the tukey
> with the bonferroni adjustment, but doing the post hoc with the bonferroni method.
> Apparently this is done easily in SPSS, I am wondering whether it is possible with R.
> 
> Can anyone help me?
> 
> Thanks in advance
> 
> 
> Angelo
> 
> 
> 
> ----Messaggio originale----
> Da: meyners.m at pg.com
> Data: 6-lug-2015 17.52
> A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, "r-help at r-project.org"<r-help at r-project.org>
> Ogg: RE: [R] Bonferroni post hoc test in R for repeated measure ANOVA with mixed within and between subjects design
> 
> Untested, but if anything, your best bet is likely something like 
> 
> summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")), test=adjusted("bonferroni"))
> 
> should work (despite the question why you'd want to use Bonferroni rather than Tukey
> For a reference, see the book on the topic by the package authors. Might be in the paper, too, which is given by
> 
> citation("multcomp")
> 
> HTH, Michael
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > angelo.arcadi at virgilio.it
> > Sent: Montag, 6. Juli 2015 16:01
> > To: r-help at r-project.org
> > Subject: [R] Bonferroni post hoc test in R for repeated measure ANOVA with
> > mixed within and between subjects design
> > 
> > Dear List Members,
> > 
> > 
> > 
> > I need to perform a Bonferroni post hoc test in R on a table with three within
> > subjects factors (Emotion, having 5 levels, Material, having 4 levels, Shoes,
> > having 2 levels) and one between subject factor (Musician, having 2 levels).
> > 
> > 
> > I normally use the Tukey method with the following formula
> > 
> > require(nlme)
> > lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd,
> > random = ~1|Subject)
> > require(multcomp)
> > summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))
> > 
> > 
> > 
> > I am not able to find any reference that explains with an example of R  code
> > how to perform a post hoc test with the Bonferroni procedure.
> > Can anyone provide an example to perform the same post hoc test in the
> > code above but with Bonferroni instead of Tukey?
> > 
> > 
> > Thank you in advance
> > 
> > 
> > 
> > Angelo
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
>    
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Tue Jul  7 03:28:23 2015
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 06 Jul 2015 21:28:23 -0400
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
Message-ID: <web-564884742@cgpsrv2.cis.mcmaster.ca>

Dear Peter,

I think that the grammatical term you're looking for is "verb phrase."

Best,
 John

On Tue, 7 Jul 2015 00:12:25 +0200
 peter dalgaard <pdalgd at gmail.com> wrote:
> 
> > On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > 
> > On 06/07/2015 5:09 PM, Rolf Turner wrote:
> >> On 07/07/15 07:10, William Dunlap wrote:
> >> 
> >> [Rolf Turner wrote.]
> >> 
> >>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
> >>>> If a complete sentence is not actually required --- and it seems abundantly clear
> >>>> that it is not --- then guidelines should not say so.  Rather they should say,
> >>>> clearly and comprehensibly, what actually *is* required.
> >>> 
> >>> This may be true, but also think of the user when you write the description.
> >>> If you are scanning a long list of descriptions looking for a package to
> >>> use,
> >>> seeing a description that starts with 'A package for' just slows you down.
> >>> Seeing a description that includes 'designed to' leaves you wondering if the
> >>> implementation is woefully incomplete.  You want to go beyond what CRAN
> >>> can test for.
> >> 
> >> All very true and sound and wise, but what has this got to do with 
> >> complete sentences?  The package checker issues a message saying that it 
> >> wants a complete sentence when this has nothing to do with what it 
> >> *really* wants.
> > 
> > That's false.  If you haven't given a complete sentence, you might still
> > pass, but if you have, you will pass.  That's not "nothing to do" with
> > what it really wants, it's just an imperfect test that fails to detect
> > violations of the guidelines.
> > 
> > As we've seen, it sometimes also makes mistakes in the other direction.
> > I'd say those are more serious.
> > 
> > Duncan Murdoch
> > 
> 
> Ackchewly....
> 
> I don't think what we want is what we say that we want. A quick check suggests that many/most packages use "headline speech", as in "Provides functions for analysis of foo, with special emphasis on bar.", which seems perfectly ok.  As others have indicated, prefixing with "This package" would be rather useless. However, I'm at a loss as to how to describe what it is that we want, much less how to translate it to a dozen other languages. 
> 
> -pd
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From meyners.m at pg.com  Tue Jul  7 08:58:47 2015
From: meyners.m at pg.com (Meyners, Michael)
Date: Tue, 7 Jul 2015 06:58:47 +0000
Subject: [R] R: RE: Bonferroni post hoc test in R for repeated measure
 ANOVA with mixed within and between subjects design
In-Reply-To: <web-564884568@cgpsrv2.cis.mcmaster.ca>
References: <14e65909091.angelo.arcadi@virgilio.it>
	<web-564884568@cgpsrv2.cis.mcmaster.ca>
Message-ID: <AE85F2AAC98B094483D12EEA72230EBD72EB28DE@GADC-EMB019.na.pg.com>

Angelo,

the conservatism of Bonferroni aside for a moment, I don't really see what you are after. 

mcp(Emotion = "Tukey")

just defines the contrasts to test; in this case, all pairwise comparisons. 

The test=adjusted("bonferroni") argument then says that these contrasts should not be corrected for according to Tukey's procedure, but using Bonferroni adjustment. 

So this should give you tests for all pairwise comparisons with multiplicity correction according to Bonferroni. I wonder how this differs from the results you get from SPSS. 

Seemingly, this is not what you want, so the question is what you really want. "Bonferroni method" does not indicate which comparisons / contrasts to look at, but just takes all those that you are interested in and multiply the corresponding p values with the number of comparisons (and making sure it does not exceed 1). As John indicated, that can easily be handled manually. Yet, you need to create the tests for all comparisons that you are interested in - if not via  linfct=mcp(Emotion = "Tukey"), you need to specify them otherwise (see the three options indicated on ?glht). The code I suggested offers a convenient shortcut in case you are interested in all pairwise comparisons and want them to be corrected according to Bonferroni, but if something else is of interest, you'd need to specify this (and let us know, as "Bonferroni method" does not give a clue about which comparisons to test).

NB: You may want to pay attention to the warning halfway down the helppage for ?mcp; it may not be clear exactly which effects you want to compare; mcp uses just the main effects w/o interactions etc.

Michael


> -----Original Message-----
> From: John Fox [mailto:jfox at mcmaster.ca]
> Sent: Dienstag, 7. Juli 2015 03:24
> To: angelo.arcadi at virgilio.it
> Cc: Meyners, Michael; r-help at r-project.org
> Subject: Re: [R] R: RE: Bonferroni post hoc test in R for repeated measure
> ANOVA with mixed within and between subjects design
> 
> Dear Angelo,
> 
> The Bonferroni p-value is just the ordinary p-value times the number of
> tests, so, since R supports multiplication, you can apply the Bonferroni
> adjustment in R. Because Bonferroni tests for multiple comparisons can be
> very conservative, asking why you want to use them is a fair question.
> 
> Best,
>  John
> 
> ------------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
> 
> 
> 
> On Tue, 7 Jul 2015 00:50:49 +0200 (CEST)  "angelo.arcadi at virgilio.it"
> <angelo.arcadi at virgilio.it> wrote:
> > Dear Michael,
> > thank you for your answer, however, I am not asking for the tukey with
> > the bonferroni adjustment, but doing the post hoc with the bonferroni
> method.
> > Apparently this is done easily in SPSS, I am wondering whether it is possible
> with R.
> >
> > Can anyone help me?
> >
> > Thanks in advance
> >
> >
> > Angelo
> >
> >
> >
> > ----Messaggio originale----
> > Da: meyners.m at pg.com
> > Data: 6-lug-2015 17.52
> > A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>,
> > "r-help at r-project.org"<r-help at r-project.org>
> > Ogg: RE: [R] Bonferroni post hoc test in R for repeated measure ANOVA
> > with mixed within and between subjects design
> >
> > Untested, but if anything, your best bet is likely something like
> >
> > summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")),
> > test=adjusted("bonferroni"))
> >
> > should work (despite the question why you'd want to use Bonferroni
> > rather than Tukey For a reference, see the book on the topic by the
> > package authors. Might be in the paper, too, which is given by
> >
> > citation("multcomp")
> >
> > HTH, Michael
> >
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > > angelo.arcadi at virgilio.it
> > > Sent: Montag, 6. Juli 2015 16:01
> > > To: r-help at r-project.org
> > > Subject: [R] Bonferroni post hoc test in R for repeated measure
> > > ANOVA with mixed within and between subjects design
> > >
> > > Dear List Members,
> > >
> > >
> > >
> > > I need to perform a Bonferroni post hoc test in R on a table with
> > > three within subjects factors (Emotion, having 5 levels, Material,
> > > having 4 levels, Shoes, having 2 levels) and one between subject factor
> (Musician, having 2 levels).
> > >
> > >
> > > I normally use the Tukey method with the following formula
> > >
> > > require(nlme)
> > > lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd,
> > > random = ~1|Subject)
> > > require(multcomp)
> > > summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))
> > >
> > >
> > >
> > > I am not able to find any reference that explains with an example of
> > > R  code how to perform a post hoc test with the Bonferroni procedure.
> > > Can anyone provide an example to perform the same post hoc test in
> > > the code above but with Bonferroni instead of Tukey?
> > >
> > >
> > > Thank you in advance
> > >
> > >
> > >
> > > Angelo
> > >
> > >
> > > 	[[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html and provide commented, minimal, self-contained,
> > > reproducible code.
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Jul  7 09:44:20 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 7 Jul 2015 07:44:20 +0000
Subject: [R] geom_segment drop unused levels
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33A2D@SRVEXCHMBX.precheza.cz>

Dear all.

I want to drop unused (NA) level from geom_segment plot. I tried different combination of drop = TRUE argument in several places of following code but NA level of prace is still present.

p<-ggplot(snimek, aes(x=cas, y=prace, xend=endtime, yend=prace, colour=typ, size=osoba))
p+geom_segment(alpha=.4)+scale_size_discrete(range=c(3,6))+
guides(colour = guide_legend(override.aes = list(size=4)))

Is it possible to remove NA level directly from ggplot call or do I need to discard rows with NA before plotting?

Below are data

Best regards.
Petr

> dput(snimek)
snimek <- structure(list(start = c(7.5, 8, 8.1, 8.4, 9.3, 10.45, 11, 11.35,
11.5, 12.25, 12.4, 13.2, 14.05, 14.2, 14.3, 7.55, 8, 8.1, 8.15,
8.35, 9.1, 9.4, 10, 12.05, 12.25, 12.4, 13.35, 14.05, 14.2, 14.3
), akce = structure(c(9L, 7L, 11L, 4L, 14L, 8L, 6L, 16L, 14L,
2L, 15L, 10L, 2L, 8L, 5L, 12L, 7L, 13L, 7L, 18L, 7L, 3L, 7L,
6L, 2L, 1L, 17L, 2L, 8L, 5L), .Label = c("Administrativa", "Cigareta",
"CL vzorky", "?i?t?n? MM", "Konec", "Ob?d", "Po??ta?", "P?est?vka",
"Satna", "Sklad", "Sva?ina", "?atna", "Tisk dokumentace", "?klid",
"?klid v?robny", "Vymyt? sud?", "V?stra?n? cedule", "Vzorkov?n?"
), class = "factor"), zarizeni = c(NA, NA, NA, NA, NA, NA, NA,
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
NA, NA, NA, NA, NA, NA, NA), typ = structure(c(4L, 5L, 4L, 5L,
5L, 4L, 2L, 5L, 5L, 1L, 5L, 5L, 1L, 4L, NA, 4L, 3L, 5L, 5L, 5L,
5L, 5L, 5L, 2L, 1L, 5L, 5L, 1L, 4L, NA), .Label = c("Kou?en?",
"Ob?d", "Po??ta?", "Prostoj", "Re?ie"), class = "factor"), prace = structure(c(1L,
2L, 1L, 4L, 2L, 1L, 1L, 2L, 2L, 1L, 3L, 2L, 1L, 1L, NA, 1L, 2L,
3L, 2L, 3L, 2L, 3L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, NA), .Label = c("Ne",
"Re?ie", "V?roba", "V?zkum"), class = "factor"), osoba = structure(c(1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Bern?t",
"?ervenka"), class = "factor"), cas = structure(list(sec = c(0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0), min = c(50L, 0L, 10L, 40L, 30L, 45L,
0L, 35L, 50L, 25L, 40L, 20L, 5L, 20L, 30L, 55L, 0L, 10L, 15L,
35L, 10L, 40L, 0L, 5L, 25L, 40L, 35L, 5L, 20L, 30L), hour = c(7L,
8L, 8L, 8L, 9L, 10L, 11L, 11L, 11L, 12L, 12L, 13L, 14L, 14L,
14L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 10L, 12L, 12L, 12L, 13L, 14L,
14L, 14L), mday = c(7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L), mon = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L), year = c(115L, 115L, 115L, 115L, 115L, 115L,
115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
115L, 115L), wday = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
2L, 2L, 2L, 2L), yday = c(187L, 187L, 187L, 187L, 187L, 187L,
187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
187L, 187L), isdst = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L), zone = c("CEST", "CEST", "CEST", "CEST", "CEST",
"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
"CEST"), gmtoff = c(NA_integer_, NA_integer_, NA_integer_, NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
NA_integer_)), .Names = c("sec", "min", "hour", "mday", "mon",
"year", "wday", "yday", "isdst", "zone", "gmtoff"), class = c("POSIXlt",
"POSIXt")), doba = c(10, 10, 30, 50, 75, 15, 35, 15, 35, 15,
40, 45, 15, 10, NA, 5, 10, 5, 20, 35, 30, 20, 125, 20, 15, 55,
30, 15, 10, NA), endtime = structure(c(1436248800, 1436249400,
1436251200, 1436254200, 1436258700, 1436259600, 1436261700, 1436262600,
1436264700, 1436265600, 1436268000, 1436270700, 1436271600, 1436272200,
NA, 1436248800, 1436249400, 1436249700, 1436250900, 1436253000,
1436254800, 1436256000, 1436263500, 1436264700, 1436265600, 1436268900,
1436270700, 1436271600, 1436272200, NA), class = c("POSIXct",
"POSIXt"))), .Names = c("start", "akce", "zarizeni", "typ", "prace",
"osoba", "cas", "doba", "endtime"), row.names = c(NA, -30L), class = "data.frame")



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From spinelli at ciml.univ-mrs.fr  Tue Jul  7 09:42:02 2015
From: spinelli at ciml.univ-mrs.fr (Lionel SPINELLI)
Date: Tue, 7 Jul 2015 07:42:02 +0000
Subject: [R] Common install of R on network disk
Message-ID: <1436255085506.3652@ciml.univ-mrs.fr>

Hello all,


I am looking for information on how to install R on a network disk so that it can be used by several users mounting this network disk from different computers.

We are using a cluster and shared computers in my lab. Those machines can mount network disks. On those disks we have installed several tools, like samtools that can be used from any user and from any machine since they have installs that do no required any system libraries.

We would like to do the same thing with R: have a folder in the network disk that contains R (and its libraries) and that permits to execute it from any machine that mount the network disk. However, it seems R uses system libraries since, once the installation done we obtain the following message when trying to execute it from a different machine than the one from which R was installed:

 error while loading shared libraries: libicuuc.so.52: cannot open shared object file: No such file or directory

This library seems present on the machine executing R but seems not found the same. We did not found any info on how to set a lib path in order to put that library with the R install.

Do you know any reference that explain how to successfully achieved the R install as we would like to?


Thanks a lot in advance




[http://sesame.univ-amu.fr/Logos/logo_sciences.jpg]

Lionel Spinelli - Ing?nieur de recherche en bioinformatique
UMR_S 1090 TAGC  et  UMR_S 1104 CIML

Aix-Marseille Universit? - LUMINY - 163 Avenue de Luminy - 13009 Marseille

T?l: +33(0)4 91 82 87 12 (TAGC) / +33(0)4 91 26 91 90 (CIML)

Site : http://www.univ-amu.fr<http://www.univ-amu.fr/> - Email : lionel.spinelli at univ-amu.fr<mailto:lionel.spinelli at univ-amu.fr>

Afin de respecter l'environnement, merci de n'imprimer cet email que si n?cessaire.


From pdalgd at gmail.com  Tue Jul  7 10:12:54 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 7 Jul 2015 10:12:54 +0200
Subject: [R] Sphericity for a repeated measure ANOVA with whithin and
	between subjects design, using R
In-Reply-To: <000901d0b807$454b9ed0$cfe2dc70$@mcmaster.ca>
References: <14e63af1f5b.angelo.arcadi@virgilio.it>
	<000901d0b807$454b9ed0$cfe2dc70$@mcmaster.ca>
Message-ID: <1D744F14-8A62-41EA-B78B-73EDBCF44CD1@gmail.com>

Another method goes via anova.mlm(), which is pretty straight forward once you figure out how to set up the necessary within-subject projections (contrasts). See

@Article{Rnews:Dalgaard:2007,
 author       = {Peter Dalgaard},
 title        = {New Functions for Multivariate Analysis},
 journal      = {R News},
 year         = 2007,
 volume       = 7,
 number       = 2,
 pages        = {2--7},
}

and ?anova.mlm.

-pd

> On 06 Jul 2015, at 18:17 , John Fox <jfox at mcmaster.ca> wrote:
> 
> Dear Angelo,
> 
> One way to do this is with the Anova() function in the car package; see the
> following article in the R Journal:
> 
> @Article{RJournal:2013-1:fox-friendly-weisberg,
> author       = {John Fox and Michael Friendly and Sanford Weisberg}, 
> title        = {Hypothesis Tests for Multivariate Linear Models Using the
> {car} Package}, 
> journal      = {The R Journal},
> year         = 2013,
> volume       = 5,
> number       = 1,
> pages        = {39--53},
> month        = jun,
> url          =
> {http://journal.r-project.org/archive/2013-1/RJournal_2013-1_fox-friendly-we
> isberg.pdf}
> 
> I hope this helps,
> John
> 
> 
> -----------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.socsci.mcmaster.ca/jfox/
> 
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> angelo.arcadi at virgilio.it
>> Sent: July-06-15 10:05 AM
>> To: r-help at r-project.org
>> Subject: [R] Sphericity for a repeated measure ANOVA with whithin and
>> between subjects design, using R
>> 
>> Dear List Members,
>> 
>> 
>> 
>> I need to perform a four-way ANOVA with repeated measures and to
>> calculate the sphericity for eventual correction of the degree of
>> freedom of the F ratio.
>> I have three within subjects factors (Emotion, having 5 levels,
>> Material, having 4 levels, Shoes, having 2 levels) and a between subject
>> factor (Musician, having 2 levels). Without considering the sphericity
>> I
>> use the following formula
>> 
>> aov_SPL = aov(SPL ~ Emotion*Material*Shoes*Musician +
>> Error(Subject/(Emotion*Material*Shoes)), data=scrd)
>> 
>> 
>> 
>> Unfortunately after having read lot of material online I did not
>> arrive to a solution about how to calculate for my case the epsilon of
>> the Greenhouse-Geisser method for degree of freedom adjustment.
>> 
>> 
>> I load my data in R with this formula:
>> 
>> scrd<- read.csv(file='/path to data/data.csv',sep=',',header=T)
>> 
>> 
>> 
>> and this is the structure of the imported table:
>> 
>>> head(scrd)
>>   Subject Material       Shoes    Emotion  H2H H2H_factor SPL_factor
>> SPL_variation     SPL Musician Weight Height
>> 1  Subject1   Gravel dress_shoes AGGRESSIVE  468      0.736     11.591
>> 21.283  97.383       no     90    183
>> 2  Subject1   Gravel dress_shoes      HAPPY  719      1.129      3.188
>> 10.071  86.171       no     90    183
>> 3  Subject1   Gravel dress_shoes     TENDER 1129      1.774      5.114
>> 14.176  90.276       no     90    183
>> 4  Subject1   Gravel dress_shoes        SAD 1010      1.587     13.102
>> 22.347  98.447       no     90    183
>> 5  Subject1   Gravel dress_shoes    NEUTRAL  736      1.156      3.161
>> 9.995  86.095       no     90    183
>> 6 Subject10   Gravel dress_shoes AGGRESSIVE  635      0.998     15.849
>> 24.000 100.100      yes     70    178
>>> 
>> 
>> 
>> 
>> I noticed that in the car package there is the Anova() function that
>> comes with the Maulchy's sphericity test, but it does not take as an
>> input a model generated with aov(). I need to use lm() instead, but
>> since
>> I am not really proficient in R I do not know how to use the lm()
>> starting from the loaded data and how to use Anova(). In addition, being
>> a mixed design involving within and between subjects factors I wonder
>> if Anova() is the most appropriate function to use for my case.
>> 
>> 
>> Is there anyone who could provide me with the R code to calculate the
>> Maulchy's test and the epsilon of Greenhouse-Geisser for my case?
>> 
>> 
>> Thank you in advance
>> Best
>> 
>> Angelo
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ---
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Tue Jul  7 10:33:58 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 7 Jul 2015 10:33:58 +0200
Subject: [R] Invalid URL for R documentation
In-Reply-To: <559AA08C.6080505@gmail.com>
References: <loom.20150706T170610-303@post.gmane.org>
	<CAAJSdjj+UnROM6D7DyoeqnF=oSp2NOvkA10y1_7x29Rb9U+Dxg@mail.gmail.com>
	<loom.20150706T172129-351@post.gmane.org>
	<559AA08C.6080505@gmail.com>
Message-ID: <EFB40C95-BEDD-417F-BDDD-A1AF49612279@gmail.com>

Some changes were made in connection with the release in order to have better automation of things like this (we now centralize information on the versions in a VERSION-INFO.dcf). 

I expect that something went wrong; maybe one of the periodic CRAN housekeeping scripts doesn't quite do what it should. Have patience and use the R-patched versions in the interim...

- Peter

> On 06 Jul 2015, at 17:36 , ProfJCNash <profjcnash at gmail.com> wrote:
> 
> 
> I'm also getting a 404. Tried https just in case.
> 
> But
> 
> http://cran.utstat.utoronto.ca/manuals.html
> 
> works and I can copy the link for R-intro and it works there.
> 
> http://cran.utstat.utoronto.ca/doc/manuals/r-release/R-intro.html
> 
> Very odd.
> 
> JN
> 
> 
> On 15-07-06 11:24 AM, Paul wrote:
>>> http://cran.r-project.org/doc/manuals/r-release/R-intro.html
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From loris.bennett at fu-berlin.de  Tue Jul  7 11:34:36 2015
From: loris.bennett at fu-berlin.de (Loris Bennett)
Date: Tue, 7 Jul 2015 11:34:36 +0200
Subject: [R] Common install of R on network disk
References: <1436255085506.3652@ciml.univ-mrs.fr>
Message-ID: <87h9pge2fn.fsf@hornfels.zedat.fu-berlin.de>

Hello Lionel,

Lionel SPINELLI <spinelli at ciml.univ-mrs.fr> writes:

> Hello all,
>>
> I am looking for information on how to install R on a network disk so
> that it can be used by several users mounting this network disk from
> different computers.
>
> We are using a cluster and shared computers in my lab. Those machines
> can mount network disks. On those disks we have installed several
> tools, like samtools that can be used from any user and from any
> machine since they have installs that do no required any system
> libraries.
>
> We would like to do the same thing with R: have a folder in the
> network disk that contains R (and its libraries) and that permits to
> execute it from any machine that mount the network disk. However, it
> seems R uses system libraries since, once the installation done we
> obtain the following message when trying to execute it from a
> different machine than the one from which R was installed:
>
>  error while loading shared libraries: libicuuc.so.52: cannot open
>  shared object file: No such file or directory
>
> This library seems present on the machine executing R but seems not
> found the same. We did not found any info on how to set a lib path in
> order to put that library with the R install.
>
> Do you know any reference that explain how to successfully achieved
> the R install as we would like to?
>
>
> Thanks a lot in advance

You could try building a static version, but as described here

https://stat.ethz.ch/pipermail/r-devel/2010-May/057534.html

that may not be very easy.

If you really want to run R across a very heterogeneous cluster, it
might be more practical to just install the same version of R directly
on each machine and the install only R packages to the network share,
e.g.

install.packages("Rmpi","/my/network/share/R/3.2.0/site-library")

You will then need a line like

.libPaths( c( .libPaths(), "/my/network/share/R/3.2.0/site-library" )

in a file such as /usr/lib64/R/etc/Rprofile.site to enable R to find the
packages.

HTH

Loris

-- 
This signature is currently under construction.


From Manuel.Weinkauf at gmx.de  Tue Jul  7 12:19:15 2015
From: Manuel.Weinkauf at gmx.de (Manuel Weinkauf)
Date: Tue, 07 Jul 2015 12:19:15 +0200
Subject: [R]  Addition to: PCAgrid scores cannot be predicted correctly
In-Reply-To: <mailman.5.1436263203.22309.r-help@r-project.org>
References: <mailman.5.1436263203.22309.r-help@r-project.org>
Message-ID: <559BA7A3.2040901@gmx.de>

I played around with the function a bit more and could actually narrow 
down the problem some more. The problem seems to lie in the scale-option 
of PCAgrid. So, for the example below, this code line produces a 
princomp object with which the scores of Dat can be predicted correctly:

rPCA<-PCAgrid(Dat, k=4, center=median, method="qn")

However, when using this code line the predicted scores of Dat are wrong:

rPCA<-PCAgrid(Dat, k=4, center=median, method="qn", scale=mad)

Since rPCA$scale is present in both cases (but naturally 1 for the first 
example) this seems to point into the direction that the scaling factors 
returned by PCAgrid might be wrong!?

Thanks a lot for your help.



On 07.07.2015 12:00, r-help-request at r-project.org wrote:
Message: 25
Date: Mon, 06 Jul 2015 17:52:38 +0200
From: Manuel Weinkauf <Manuel.Weinkauf at gmx.de>
To: R-help at r-project.org
Subject: [R] PCAgrid scores cannot be predicted correctly
Message-ID: <559AA446.2050401 at gmx.de>
Content-Type: text/plain; charset=utf-8; format=flowed

I have been running into a problem with the PCAgrid() function of the
R-package pcaPP. When trying to predict the scores of new data using the
resulting PCA, the predicted score values are wrong. This is true for
both, using the predict() function and for calculating the scores
manually after scaling the data.

The example below illustrates that: In princomp(), when I either use
predict() or the manual calculation of the scores of the data originally
used for the PCA, the predicted points coincide exactly with the PCA
scores (as it should be, as they are using the same raw data). However,
when doing this with the princomp-object calculated with the pcaPP
package, the predicted values are nowhere close to where they should be.
Note that the results of predict() exactly match the results calculated
manually, so it is not that predict() could not handle this object
correctly.

Can anybody explain this weird behaviour to me? Thanks a lot.


##EXAMPLE##
library(pcaPP)

#Create data
t1<-rnorm(10, 10, 1)
t2<-rnorm(10, 8, 2)
t3<-rnorm(10, 60, 4)
t4<-rnorm(10, 1, 0.05)
Dat<-matrix(c(t1, t2, t3 , t4), ncol=4)
colnames(Dat)<-paste("Var", 1:4, sep=".")

win.graph(20, 10, 10)
layout(matrix(c(1, 2), 1, 2))
#Normal PCA
PCA<-princomp(Dat)
PCA.pred<-predict(PCA, Dat)
Dat.Scale<-scale(Dat, center=PCA$center, scale=PCA$scale)
Load<-PCA$loadings
PCA.man<-matrix(NA, nrow(PCA$scores), 2)
for (i in 1:nrow(PCA.man)) {
PCA.man[i,1]<-Dat.Scale[i,1]*Load[1,1]+Dat.Scale[i,2]*Load[2,1]+Dat.Scale[i,3]*Load[3,1]+Dat.Scale[i,4]*Load[4,1]
PCA.man[i,2]<-Dat.Scale[i,1]*Load[1,2]+Dat.Scale[i,2]*Load[2,2]+Dat.Scale[i,3]*Load[3,2]+Dat.Scale[i,4]*Load[4,2]
}
plot(PCA$scores[,1:2], pch=1, cex=2, main="Normal PCA")
points(PCA.pred, pch=16)
points(PCA.man, pch=0, col="red", cex=3)
legend("topleft", pch=c(1, 16, 0), col=c("black", "black", "red"),
legend=c("Ordination", "Prediction", "Manual prediction"))

#robust PCA
rPCA<-PCAgrid(Dat, k=4, center=median, method="qn", scale=mad)
rPCA.pred<-predict(rPCA, Dat)
Dat.Scale<-scale(Dat, center=rPCA$center, scale=rPCA$scale)
Load<-rPCA$loadings
rPCA.man<-matrix(NA, nrow(rPCA$scores), 2)
for (i in 1:nrow(rPCA.man)) {
rPCA.man[i,1]<-Dat.Scale[i,1]*Load[1,1]+Dat.Scale[i,2]*Load[2,1]+Dat.Scale[i,3]*Load[3,1]+Dat.Scale[i,4]*Load[4,1]
rPCA.man[i,2]<-Dat.Scale[i,1]*Load[1,2]+Dat.Scale[i,2]*Load[2,2]+Dat.Scale[i,3]*Load[3,2]+Dat.Scale[i,4]*Load[4,2]
}
plot(rPCA$scores[,1:2], pch=1, cex=2, main="Robust PCA")
points(rPCA.pred, pch=16)
points(rPCA.man, pch=0, col="red", cex=3)
legend("topleft", pch=c(1, 16, 0), col=c("black", "black", "red"),
legend=c("Ordination", "Prediction", "Manual prediction"))

-- 
Dr Manuel Weinkauf
MARUM Bremen
Room MARUM II?2050
Leobener Stra?e
28359 Bremen
Germany
e-mail: mweinkauf at marum.de
phone: +49 421 218 659 75


From pdalgd at gmail.com  Tue Jul  7 12:20:38 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 7 Jul 2015 12:20:38 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <web-564884742@cgpsrv2.cis.mcmaster.ca>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
	<web-564884742@cgpsrv2.cis.mcmaster.ca>
Message-ID: <915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>

...except that there is not necessarily a verb either. What we're looking for is something like "advertisement style" as in 

UGLY MUGS 7.95. 

An invaluable addition to your display cabinet. Comes in an assortment of warts and wrinkles, crafted by professional artist Foo Yung.

However, I'm drawing blanks when searching for an established term for it.

Could we perhaps sidestep the issue by requesting a "single descriptive paragraph, with punctuation" or thereabouts?

----

I'm still puzzled about what threw Federico's example in the first place. The actual code is 

    if(strict && !is.na(val <- db["Description"])
       && !grepl("[.!?]['\")]?$", trimws(val)))
        out$bad_Description <- TRUE

and  I can do this

> strict <- TRUE
> db <- tools:::.read_description("/tmp/dd")
>    if(strict && !is.na(val <- db["Description"])
+        && !grepl("[.!?]['\")]?$", trimws(val)))
+         out$bad_Description <- TRUE
> out
Error: object 'out' not found

I.e., the complaint should _not_ be triggered. I suppose that something like a non-breakable space at the end could confuse trimws(), but beyond that I'm out of ideas.


On 07 Jul 2015, at 03:28 , John Fox <jfox at mcmaster.ca> wrote:

> Dear Peter,
> 
> I think that the grammatical term you're looking for is "verb phrase."
> 
> Best,
> John
> 
> On Tue, 7 Jul 2015 00:12:25 +0200
> peter dalgaard <pdalgd at gmail.com> wrote:
>> 
>>> On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>> 
>>> On 06/07/2015 5:09 PM, Rolf Turner wrote:
>>>> On 07/07/15 07:10, William Dunlap wrote:
>>>> 
>>>> [Rolf Turner wrote.]
>>>> 
>>>>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
>>>>>> If a complete sentence is not actually required --- and it seems abundantly clear
>>>>>> that it is not --- then guidelines should not say so.  Rather they should say,
>>>>>> clearly and comprehensibly, what actually *is* required.
>>>>> 
>>>>> This may be true, but also think of the user when you write the description.
>>>>> If you are scanning a long list of descriptions looking for a package to
>>>>> use,
>>>>> seeing a description that starts with 'A package for' just slows you down.
>>>>> Seeing a description that includes 'designed to' leaves you wondering if the
>>>>> implementation is woefully incomplete.  You want to go beyond what CRAN
>>>>> can test for.
>>>> 
>>>> All very true and sound and wise, but what has this got to do with 
>>>> complete sentences?  The package checker issues a message saying that it 
>>>> wants a complete sentence when this has nothing to do with what it 
>>>> *really* wants.
>>> 
>>> That's false.  If you haven't given a complete sentence, you might still
>>> pass, but if you have, you will pass.  That's not "nothing to do" with
>>> what it really wants, it's just an imperfect test that fails to detect
>>> violations of the guidelines.
>>> 
>>> As we've seen, it sometimes also makes mistakes in the other direction.
>>> I'd say those are more serious.
>>> 
>>> Duncan Murdoch
>>> 
>> 
>> Ackchewly....
>> 
>> I don't think what we want is what we say that we want. A quick check suggests that many/most packages use "headline speech", as in "Provides functions for analysis of foo, with special emphasis on bar.", which seems perfectly ok.  As others have indicated, prefixing with "This package" would be rather useless. However, I'm at a loss as to how to describe what it is that we want, much less how to translate it to a dozen other languages. 
>> 
>> -pd
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From lists at dewey.myzen.co.uk  Tue Jul  7 12:51:13 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 07 Jul 2015 11:51:13 +0100
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>	<55969185.5000604@auckland.ac.nz>	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>	<55970BF9.400@auckland.ac.nz>	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>	<559AEEA2.2020805@auckland.ac.nz>
	<559AF0F1.9010902@gmail.com>	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>	<web-564884742@cgpsrv2.cis.mcmaster.ca>
	<915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>
Message-ID: <559BAF21.9060307@dewey.myzen.co.uk>

In line below

On 07/07/2015 11:20, peter dalgaard wrote:
> ...except that there is not necessarily a verb either. What we're looking for is something like "advertisement style" as in
>
> UGLY MUGS 7.95.
>
> An invaluable addition to your display cabinet. Comes in an assortment of warts and wrinkles, crafted by professional artist Foo Yung.
>
> However, I'm drawing blanks when searching for an established term for it.
>

People who try to measure readability seem to define sentence as 'a 
sequence of words terminated by a stop'. They presumably count question 
marks and exclamation marks as a stop. So Peter's examples above are 
indeed sentences for one meaning of sentence.

> Could we perhaps sidestep the issue by requesting a "single descriptive paragraph, with punctuation" or thereabouts?
>
> ----
>
> I'm still puzzled about what threw Federico's example in the first place. The actual code is
>
>      if(strict && !is.na(val <- db["Description"])
>         && !grepl("[.!?]['\")]?$", trimws(val)))
>          out$bad_Description <- TRUE
>
> and  I can do this
>
>> strict <- TRUE
>> db <- tools:::.read_description("/tmp/dd")
>>     if(strict && !is.na(val <- db["Description"])
> +        && !grepl("[.!?]['\")]?$", trimws(val)))
> +         out$bad_Description <- TRUE
>> out
> Error: object 'out' not found
>
> I.e., the complaint should _not_ be triggered. I suppose that something like a non-breakable space at the end could confuse trimws(), but beyond that I'm out of ideas.
>
>
> On 07 Jul 2015, at 03:28 , John Fox <jfox at mcmaster.ca> wrote:
>
>> Dear Peter,
>>
>> I think that the grammatical term you're looking for is "verb phrase."
>>
>> Best,
>> John
>>
>> On Tue, 7 Jul 2015 00:12:25 +0200
>> peter dalgaard <pdalgd at gmail.com> wrote:
>>>
>>>> On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>>>
>>>> On 06/07/2015 5:09 PM, Rolf Turner wrote:
>>>>> On 07/07/15 07:10, William Dunlap wrote:
>>>>>
>>>>> [Rolf Turner wrote.]
>>>>>
>>>>>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
>>>>>>> If a complete sentence is not actually required --- and it seems abundantly clear
>>>>>>> that it is not --- then guidelines should not say so.  Rather they should say,
>>>>>>> clearly and comprehensibly, what actually *is* required.
>>>>>>
>>>>>> This may be true, but also think of the user when you write the description.
>>>>>> If you are scanning a long list of descriptions looking for a package to
>>>>>> use,
>>>>>> seeing a description that starts with 'A package for' just slows you down.
>>>>>> Seeing a description that includes 'designed to' leaves you wondering if the
>>>>>> implementation is woefully incomplete.  You want to go beyond what CRAN
>>>>>> can test for.
>>>>>
>>>>> All very true and sound and wise, but what has this got to do with
>>>>> complete sentences?  The package checker issues a message saying that it
>>>>> wants a complete sentence when this has nothing to do with what it
>>>>> *really* wants.
>>>>
>>>> That's false.  If you haven't given a complete sentence, you might still
>>>> pass, but if you have, you will pass.  That's not "nothing to do" with
>>>> what it really wants, it's just an imperfect test that fails to detect
>>>> violations of the guidelines.
>>>>
>>>> As we've seen, it sometimes also makes mistakes in the other direction.
>>>> I'd say those are more serious.
>>>>
>>>> Duncan Murdoch
>>>>
>>>
>>> Ackchewly....
>>>
>>> I don't think what we want is what we say that we want. A quick check suggests that many/most packages use "headline speech", as in "Provides functions for analysis of foo, with special emphasis on bar.", which seems perfectly ok.  As others have indicated, prefixing with "This package" would be rather useless. However, I'm at a loss as to how to describe what it is that we want, much less how to translate it to a dozen other languages.
>>>
>>> -pd
>>> --
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> 	
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pdalgd at gmail.com  Tue Jul  7 13:28:27 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 7 Jul 2015 13:28:27 +0200
Subject: [R] Invalid URL for R documentation
In-Reply-To: <EFB40C95-BEDD-417F-BDDD-A1AF49612279@gmail.com>
References: <loom.20150706T170610-303@post.gmane.org>
	<CAAJSdjj+UnROM6D7DyoeqnF=oSp2NOvkA10y1_7x29Rb9U+Dxg@mail.gmail.com>
	<loom.20150706T172129-351@post.gmane.org>
	<559AA08C.6080505@gmail.com>
	<EFB40C95-BEDD-417F-BDDD-A1AF49612279@gmail.com>
Message-ID: <B0C97E9C-9280-42EE-AFD2-4ED118910FBD@gmail.com>

Wrong diagnosis. The actual issue is a version-compare blunder for texinfo in the configure script. This has been mended for r-patched, but r-release requires a workaround.

-pd

On 07 Jul 2015, at 10:33 , peter dalgaard <pdalgd at gmail.com> wrote:

> Some changes were made in connection with the release in order to have better automation of things like this (we now centralize information on the versions in a VERSION-INFO.dcf). 
> 
> I expect that something went wrong; maybe one of the periodic CRAN housekeeping scripts doesn't quite do what it should. Have patience and use the R-patched versions in the interim...
> 
> - Peter
> 
>> On 06 Jul 2015, at 17:36 , ProfJCNash <profjcnash at gmail.com> wrote:
>> 
>> 
>> I'm also getting a 404. Tried https just in case.
>> 
>> But
>> 
>> http://cran.utstat.utoronto.ca/manuals.html
>> 
>> works and I can copy the link for R-intro and it works there.
>> 
>> http://cran.utstat.utoronto.ca/doc/manuals/r-release/R-intro.html
>> 
>> Very odd.
>> 
>> JN
>> 
>> 
>> On 15-07-06 11:24 AM, Paul wrote:
>>>> http://cran.r-project.org/doc/manuals/r-release/R-intro.html
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jfox at mcmaster.ca  Tue Jul  7 14:19:26 2015
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 07 Jul 2015 08:19:26 -0400
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
	<web-564884742@cgpsrv2.cis.mcmaster.ca>
	<915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>
Message-ID: <web-564904636@cgpsrv2.cis.mcmaster.ca>

Dear Peter,

You're correct that these examples aren't verb phrases (though the second one contains a verb phrase). I don't want to make the discussion even more pedantic (moving it in this direction was my fault), but "Paragraph" isn't quite right, unless explained, because conventionally a paragraph consists of sentences. 

How about something like this? "One can use several complete sentences or punctuated telegraphic phrases, but only one paragraph (that is, block of continuous text with no intervening blank lines). The description should end with a full stop (period)."

It would likely be helpful to add some examples of good and bad descriptions, and to explain how the check actually works.

Best,
 John

On Tue, 7 Jul 2015 12:20:38 +0200
 peter dalgaard <pdalgd at gmail.com> wrote:
> ...except that there is not necessarily a verb either. What we're looking for is something like "advertisement style" as in 
> 
> UGLY MUGS 7.95. 
> 
> An invaluable addition to your display cabinet. Comes in an assortment of warts and wrinkles, crafted by professional artist Foo Yung.
> 
> However, I'm drawing blanks when searching for an established term for it.
> 
> Could we perhaps sidestep the issue by requesting a "single descriptive paragraph, with punctuation" or thereabouts?
> 
> ----
> 
> I'm still puzzled about what threw Federico's example in the first place. The actual code is 
> 
>     if(strict && !is.na(val <- db["Description"])
>        && !grepl("[.!?]['\")]?$", trimws(val)))
>         out$bad_Description <- TRUE
> 
> and  I can do this
> 
> > strict <- TRUE
> > db <- tools:::.read_description("/tmp/dd")
> >    if(strict && !is.na(val <- db["Description"])
> +        && !grepl("[.!?]['\")]?$", trimws(val)))
> +         out$bad_Description <- TRUE
> > out
> Error: object 'out' not found
> 
> I.e., the complaint should _not_ be triggered. I suppose that something like a non-breakable space at the end could confuse trimws(), but beyond that I'm out of ideas.
> 
> 
> On 07 Jul 2015, at 03:28 , John Fox <jfox at mcmaster.ca> wrote:
> 
> > Dear Peter,
> > 
> > I think that the grammatical term you're looking for is "verb phrase."
> > 
> > Best,
> > John
> > 
> > On Tue, 7 Jul 2015 00:12:25 +0200
> > peter dalgaard <pdalgd at gmail.com> wrote:
> >> 
> >>> On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >>> 
> >>> On 06/07/2015 5:09 PM, Rolf Turner wrote:
> >>>> On 07/07/15 07:10, William Dunlap wrote:
> >>>> 
> >>>> [Rolf Turner wrote.]
> >>>> 
> >>>>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
> >>>>>> If a complete sentence is not actually required --- and it seems abundantly clear
> >>>>>> that it is not --- then guidelines should not say so.  Rather they should say,
> >>>>>> clearly and comprehensibly, what actually *is* required.
> >>>>> 
> >>>>> This may be true, but also think of the user when you write the description.
> >>>>> If you are scanning a long list of descriptions looking for a package to
> >>>>> use,
> >>>>> seeing a description that starts with 'A package for' just slows you down.
> >>>>> Seeing a description that includes 'designed to' leaves you wondering if the
> >>>>> implementation is woefully incomplete.  You want to go beyond what CRAN
> >>>>> can test for.
> >>>> 
> >>>> All very true and sound and wise, but what has this got to do with 
> >>>> complete sentences?  The package checker issues a message saying that it 
> >>>> wants a complete sentence when this has nothing to do with what it 
> >>>> *really* wants.
> >>> 
> >>> That's false.  If you haven't given a complete sentence, you might still
> >>> pass, but if you have, you will pass.  That's not "nothing to do" with
> >>> what it really wants, it's just an imperfect test that fails to detect
> >>> violations of the guidelines.
> >>> 
> >>> As we've seen, it sometimes also makes mistakes in the other direction.
> >>> I'd say those are more serious.
> >>> 
> >>> Duncan Murdoch
> >>> 
> >> 
> >> Ackchewly....
> >> 
> >> I don't think what we want is what we say that we want. A quick check suggests that many/most packages use "headline speech", as in "Provides functions for analysis of foo, with special emphasis on bar.", which seems perfectly ok.  As others have indicated, prefixing with "This package" would be rather useless. However, I'm at a loss as to how to describe what it is that we want, much less how to translate it to a dozen other languages. 
> >> 
> >> -pd
> >> -- 
> >> Peter Dalgaard, Professor,
> >> Center for Statistics, Copenhagen Business School
> >> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >> Phone: (+45)38153501
> >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >> 
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> > 
> > 
> > 	
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/


From john.archie.mckown at gmail.com  Tue Jul  7 14:44:45 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Tue, 7 Jul 2015 07:44:45 -0500
Subject: [R] Common install of R on network disk
In-Reply-To: <1436255085506.3652@ciml.univ-mrs.fr>
References: <1436255085506.3652@ciml.univ-mrs.fr>
Message-ID: <CAAJSdjjycg6FxRhUKsqYCT8mA3_qXHT8=Dk5GofdoUeeL7Nd=w@mail.gmail.com>

On Tue, Jul 7, 2015 at 2:42 AM, Lionel SPINELLI <spinelli at ciml.univ-mrs.fr>
wrote:

> Hello all,
>
>
> I am looking for information on how to install R on a network disk so that
> it can be used by several users mounting this network disk from different
> computers.
>
> We are using a cluster and shared computers in my lab. Those machines can
> mount network disks. On those disks we have installed several tools, like
> samtools that can be used from any user and from any machine since they
> have installs that do no required any system libraries.
>
> We would like to do the same thing with R: have a folder in the network
> disk that contains R (and its libraries) and that permits to execute it
> from any machine that mount the network disk. However, it seems R uses
> system libraries since, once the installation done we obtain the following
> message when trying to execute it from a different machine than the one
> from which R was installed:
>
>  error while loading shared libraries: libicuuc.so.52: cannot open shared
> object file: No such file or directory
>
> This library seems present on the machine executing R but seems not found
> the same. We did not found any info on how to set a lib path in order to
> put that library with the R install.
>
> Do you know any reference that explain how to successfully achieved the R
> install as we would like to?
>
>
> Thanks a lot in advance
>
>
First, I am assuming that all the clients are running the same OS at the
same, or compatible, levels. I am also assuming Linux, but that may be
pushing it. What I _think_ will work (I can't test it where I am now) is to
set the LD_LIBRARY_PATH environment to include the directory which contains
the libicuuc.so file.

For a possible example, suppose libicucc.so is in the directory
/network/disk/R-installation/lib, then try the command (I am using BASH):

LD_LIBRARY_PATH="${LD_LIBRARY_PATH}${LD_LIBRARY_PATH+:}/network/disk/R-installation/lib"
R

Yes, the above is _ugly_. An easier way might be do to the following (as
root) on all the systems which want to run R (# is the command prompt for
"root", not something you type in!)

# echo "/netwrok/disk/R-installation/lib" >>/etc/ld.so.conf.d/R-network.conf
# ldconfig

What the above does is add the directory /network/disk/R-installation/lib
(which contains the R shared libraries) to the directories automatically
searched for shared libraries. You can add more that one directory name,
each on a separate line, into the R-network.conf file. Oh, the name of the
file can be anything you like but must be in the directory
/etc/ld.so.conf.d and end in ".conf". E.g. R-network.conf or
myIdiotBrother.conf would both work.

# echo "/network/disk1/R-installation/lib"
>>/etc/ld.so.conf.d/R-network.conf
# echo "/network/disk2/R-2nd/lib" >>/etc/ld.so.conf.d/R-network.conf
# echo "/some/other/directory/entirely" >>/etc/ld.so.conf.d/R-network.conf
# ldconfig

would add the three mentioned directories.


This stackoverflow discussion might be of some help as well:
http://stackoverflow.com/questions/13428910/how-to-set-the-environmental-variable-ld-library-path-in-linux





>
> [http://sesame.univ-amu.fr/Logos/logo_sciences.jpg]
>
> Lionel Spinelli - Ing?nieur de recherche en bioinformatique
> UMR_S 1090 TAGC  et  UMR_S 1104 CIML
>
> Aix-Marseille Universit? - LUMINY - 163 Avenue de Luminy - 13009 Marseille
>
> T?l: +33(0)4 91 82 87 12 (TAGC) / +33(0)4 91 26 91 90 (CIML)
>
> Site : http://www.univ-amu.fr<http://www.univ-amu.fr/> - Email :
> lionel.spinelli at univ-amu.fr<mailto:lionel.spinelli at univ-amu.fr>
>
>
-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From marammagdysalem at gmx.com  Tue Jul  7 14:46:53 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Tue, 7 Jul 2015 14:46:53 +0200
Subject: [R] Can't get a reasonable the maximum likelihood  estimator
References: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>
Message-ID: <829ECDEB-6C8A-46A0-B794-6A9FFBF2A7F5@gmx.com>



Sent from my iPhone

Begin forwarded message:

> From: "Maram Salem" <marammagdysalem at gmx.com>
> Date: July 6, 2015 at 2:29:56 AM GMT+2
> To: "r-help at r-project.org" <r-help at r-project.org>
> Subject: [R] NaN produced from log() with positive input
> 
> Dear All
> I'm trying to find the maximum likelihood estimator  of a certain distribution based on the newton raphson method using maxLik package. I wrote the log-likelihood , gradient, and hessian functions using the following code.
> 
> #Step 1: Creating the theta vector
> theta <-vector(mode = "numeric", length = 3)
> # Step 2: Setting the values of r and n
> r<- 17
> n <-30
> # Step 3: Creating the T vector
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> # Step 4: Creating the C vector
> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> # The  loglik. func.
> loglik <- function(param) {
> theta[1]<- param[1]
> theta[2]<- param[2]
> theta[3]<- param[3]
> l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+ (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+ (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
> return(l)
> }
> # Step 5: Creating the gradient vector and calculating its inputs
> U <- vector(mode="numeric",length=3)
> gradlik<-function(param = theta,n, T,C)
> {
> U <- vector(mode="numeric",length=3)
> theta[1] <- param[1]
> theta[2] <- param[2]
> theta[3] <- param[3]
> r<- 17
> n <-30
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+( -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+ (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+ (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
> return(U)
> }
> # Step 6: Creating the G (Hessian) matrix and Calculating its inputs
> hesslik<-function(param=theta,n,T,C)
> {
> theta[1] <- param[1]
> theta[2] <- param[2]
> theta[3] <- param[3]
> r<- 17
> n <-30
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> G<- matrix(nrow=3,ncol=3)
> G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+ (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+ (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+ (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[2,1]<-G[1,2]
> G[1,3]<-(n/theta[1])+(-1)*sum( (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> G[3,1]<-G[1,3] 
> G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+ (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> G[3,2]<-G[2,3]                 
> G[3,3]<-((-1*r)/(theta[3])^2)
> return(G)
> }
> mle<-maxLik(loglik, grad = gradlik, hess = hesslik, start=c(40,50,2))
> There were 50 or more warnings (use warnings() to see the first 50)
> 
> warnings ()
> Warning messages:
> 1: In log(theta[3]) : NaNs produced
> 2: In log(theta[1] + theta[2]) : NaNs produced
> 3: In log(theta[1]) : NaNs produced
> 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs produced
> and so on .......
> 
> Although when I evaluate, for example, log(theta[3])  it gives me a number. and the same applies for the other warnings.
> 
> Then when I used summary (mle), I got
> 
> 
> Maximum Likelihood estimation
> Newton-Raphson maximisation, 7 iterations
> Return code 1: gradient close to zero
> Log-Likelihood: -55.89012 
> 3  free parameters
> Estimates:
>     Estimate Std. error t value Pr(> t)
> [1,]   11.132        Inf       0       1
> [2,]   47.618        Inf       0       1
> [3,]    1.293        Inf       0       1
> --------------------------------------------
> 
> 
> Where the estimates are far away from the starting values and they have infinite standard errors. I think there is a problem with my gradlik or hesslik functions, but I can't figure it out.
> Any help?
> Thank you in advance.
> 
> Maram 
> 
> 
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Tue Jul  7 14:56:33 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 07 Jul 2015 05:56:33 -0700
Subject: [R] geom_segment drop unused levels
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33A2D@SRVEXCHMBX.precheza.cz>
References: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33A2D@SRVEXCHMBX.precheza.cz>
Message-ID: <61CB9399-4EF9-4F6E-9A03-C8CA5F9A1F31@dcn.davis.CA.us>

Not reproducible.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 7, 2015 12:44:20 AM PDT, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>Dear all.
>
>I want to drop unused (NA) level from geom_segment plot. I tried
>different combination of drop = TRUE argument in several places of
>following code but NA level of prace is still present.
>
>p<-ggplot(snimek, aes(x=cas, y=prace, xend=endtime, yend=prace,
>colour=typ, size=osoba))
>p+geom_segment(alpha=.4)+scale_size_discrete(range=c(3,6))+
>guides(colour = guide_legend(override.aes = list(size=4)))
>
>Is it possible to remove NA level directly from ggplot call or do I
>need to discard rows with NA before plotting?
>
>Below are data
>
>Best regards.
>Petr
>
>> dput(snimek)
>snimek <- structure(list(start = c(7.5, 8, 8.1, 8.4, 9.3, 10.45, 11,
>11.35,
>11.5, 12.25, 12.4, 13.2, 14.05, 14.2, 14.3, 7.55, 8, 8.1, 8.15,
>8.35, 9.1, 9.4, 10, 12.05, 12.25, 12.4, 13.35, 14.05, 14.2, 14.3
>), akce = structure(c(9L, 7L, 11L, 4L, 14L, 8L, 6L, 16L, 14L,
>2L, 15L, 10L, 2L, 8L, 5L, 12L, 7L, 13L, 7L, 18L, 7L, 3L, 7L,
>6L, 2L, 1L, 17L, 2L, 8L, 5L), .Label = c("Administrativa", "Cigareta",
>"CL vzorky", "?i?t?n? MM", "Konec", "Ob?d", "Po??ta?", "P?est?vka",
>"Satna", "Sklad", "Sva?ina", "?atna", "Tisk dokumentace", "?klid",
>"?klid v?robny", "Vymyt? sud?", "V?stra?n? cedule", "Vzorkov?n?"
>), class = "factor"), zarizeni = c(NA, NA, NA, NA, NA, NA, NA,
>NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>NA, NA, NA, NA, NA, NA, NA), typ = structure(c(4L, 5L, 4L, 5L,
>5L, 4L, 2L, 5L, 5L, 1L, 5L, 5L, 1L, 4L, NA, 4L, 3L, 5L, 5L, 5L,
>5L, 5L, 5L, 2L, 1L, 5L, 5L, 1L, 4L, NA), .Label = c("Kou?en?",
>"Ob?d", "Po??ta?", "Prostoj", "Re?ie"), class = "factor"), prace =
>structure(c(1L,
>2L, 1L, 4L, 2L, 1L, 1L, 2L, 2L, 1L, 3L, 2L, 1L, 1L, NA, 1L, 2L,
>3L, 2L, 3L, 2L, 3L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, NA), .Label = c("Ne",
>"Re?ie", "V?roba", "V?zkum"), class = "factor"), osoba =
>structure(c(1L,
>1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L,
>2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label =
>c("Bern?t",
>"?ervenka"), class = "factor"), cas = structure(list(sec = c(0,
>0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>0, 0, 0, 0, 0, 0, 0, 0), min = c(50L, 0L, 10L, 40L, 30L, 45L,
>0L, 35L, 50L, 25L, 40L, 20L, 5L, 20L, 30L, 55L, 0L, 10L, 15L,
>35L, 10L, 40L, 0L, 5L, 25L, 40L, 35L, 5L, 20L, 30L), hour = c(7L,
>8L, 8L, 8L, 9L, 10L, 11L, 11L, 11L, 12L, 12L, 13L, 14L, 14L,
>14L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 10L, 12L, 12L, 12L, 13L, 14L,
>14L, 14L), mday = c(7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>7L, 7L, 7L), mon = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>6L, 6L, 6L, 6L), year = c(115L, 115L, 115L, 115L, 115L, 115L,
>115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
>115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
>115L, 115L), wday = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>2L, 2L, 2L, 2L), yday = c(187L, 187L, 187L, 187L, 187L, 187L,
>187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
>187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
>187L, 187L), isdst = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>1L, 1L, 1L, 1L), zone = c("CEST", "CEST", "CEST", "CEST", "CEST",
>"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
>"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
>"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
>"CEST"), gmtoff = c(NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>NA_integer_)), .Names = c("sec", "min", "hour", "mday", "mon",
>"year", "wday", "yday", "isdst", "zone", "gmtoff"), class =
>c("POSIXlt",
>"POSIXt")), doba = c(10, 10, 30, 50, 75, 15, 35, 15, 35, 15,
>40, 45, 15, 10, NA, 5, 10, 5, 20, 35, 30, 20, 125, 20, 15, 55,
>30, 15, 10, NA), endtime = structure(c(1436248800, 1436249400,
>1436251200, 1436254200, 1436258700, 1436259600, 1436261700, 1436262600,
>1436264700, 1436265600, 1436268000, 1436270700, 1436271600, 1436272200,
>NA, 1436248800, 1436249400, 1436249700, 1436250900, 1436253000,
>1436254800, 1436256000, 1436263500, 1436264700, 1436265600, 1436268900,
>1436270700, 1436271600, 1436272200, NA), class = c("POSIXct",
>"POSIXt"))), .Names = c("start", "akce", "zarizeni", "typ", "prace",
>"osoba", "cas", "doba", "endtime"), row.names = c(NA, -30L), class =
>"data.frame")
>
>
>
>________________________________
>Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>ur?eny pouze jeho adres?t?m.
>Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
>kopie vyma?te ze sv?ho syst?mu.
>Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
>modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
>V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky
>ze strany p??jemce s dodatkem ?i odchylkou.
>- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
>zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
>adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
>p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
>zn?m?.
>
>This e-mail and any documents attached to it may be confidential and
>are intended only for its intended recipients.
>If you received this e-mail by mistake, please immediately inform its
>sender. Delete the contents of this e-mail with all attachments and its
>copies from your system.
>If you are not the intended recipient of this e-mail, you are not
>authorized to use, disseminate, copy or disclose this e-mail in any
>manner.
>The sender of this e-mail shall not be liable for any possible damage
>caused by modifications of the e-mail or by delay with transfer of the
>email.
>
>In case that this e-mail forms part of business dealings:
>- the sender reserves the right to end negotiations about entering into
>a contract in any time, for any reason, and without stating any
>reasoning.
>- if the e-mail contains an offer, the recipient is entitled to
>immediately accept such offer; The sender of this e-mail (offer)
>excludes any acceptance of the offer on the part of the recipient
>containing any amendment or variation.
>- the sender insists on that the respective contract is concluded only
>upon an express mutual agreement on all its aspects.
>- the sender of this e-mail informs that he/she is not authorized to
>enter into any contracts on behalf of the company except for cases in
>which he/she is expressly authorized to do so in writing, and such
>authorization or power of attorney is submitted to the recipient or the
>person represented by the recipient, or the existence of such
>authorization is known to the recipient of the person represented by
>the recipient.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From thierry.onkelinx at inbo.be  Tue Jul  7 15:09:01 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 7 Jul 2015 15:09:01 +0200
Subject: [R] geom_segment drop unused levels
In-Reply-To: <61CB9399-4EF9-4F6E-9A03-C8CA5F9A1F31@dcn.davis.CA.us>
References: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33A2D@SRVEXCHMBX.precheza.cz>
	<61CB9399-4EF9-4F6E-9A03-C8CA5F9A1F31@dcn.davis.CA.us>
Message-ID: <CAJuCY5zGcNPyvdcWKXimDh=j14mUYCzsSg9ZLhLBQPZ_B2_=VQ@mail.gmail.com>

I disagree with you Jeff. I can reproduce it without any problem.

I see no other option than to remove the offending missing data prior to
plotting.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-07 14:56 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:

> Not reproducible.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 7, 2015 12:44:20 AM PDT, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> >Dear all.
> >
> >I want to drop unused (NA) level from geom_segment plot. I tried
> >different combination of drop = TRUE argument in several places of
> >following code but NA level of prace is still present.
> >
> >p<-ggplot(snimek, aes(x=cas, y=prace, xend=endtime, yend=prace,
> >colour=typ, size=osoba))
> >p+geom_segment(alpha=.4)+scale_size_discrete(range=c(3,6))+
> >guides(colour = guide_legend(override.aes = list(size=4)))
> >
> >Is it possible to remove NA level directly from ggplot call or do I
> >need to discard rows with NA before plotting?
> >
> >Below are data
> >
> >Best regards.
> >Petr
> >
> >> dput(snimek)
> >snimek <- structure(list(start = c(7.5, 8, 8.1, 8.4, 9.3, 10.45, 11,
> >11.35,
> >11.5, 12.25, 12.4, 13.2, 14.05, 14.2, 14.3, 7.55, 8, 8.1, 8.15,
> >8.35, 9.1, 9.4, 10, 12.05, 12.25, 12.4, 13.35, 14.05, 14.2, 14.3
> >), akce = structure(c(9L, 7L, 11L, 4L, 14L, 8L, 6L, 16L, 14L,
> >2L, 15L, 10L, 2L, 8L, 5L, 12L, 7L, 13L, 7L, 18L, 7L, 3L, 7L,
> >6L, 2L, 1L, 17L, 2L, 8L, 5L), .Label = c("Administrativa", "Cigareta",
> >"CL vzorky", "?i?t?n? MM", "Konec", "Ob?d", "Po??ta?", "P?est?vka",
> >"Satna", "Sklad", "Sva?ina", "?atna", "Tisk dokumentace", "?klid",
> >"?klid v?robny", "Vymyt? sud?", "V?stra?n? cedule", "Vzorkov?n?"
> >), class = "factor"), zarizeni = c(NA, NA, NA, NA, NA, NA, NA,
> >NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >NA, NA, NA, NA, NA, NA, NA), typ = structure(c(4L, 5L, 4L, 5L,
> >5L, 4L, 2L, 5L, 5L, 1L, 5L, 5L, 1L, 4L, NA, 4L, 3L, 5L, 5L, 5L,
> >5L, 5L, 5L, 2L, 1L, 5L, 5L, 1L, 4L, NA), .Label = c("Kou?en?",
> >"Ob?d", "Po??ta?", "Prostoj", "Re?ie"), class = "factor"), prace =
> >structure(c(1L,
> >2L, 1L, 4L, 2L, 1L, 1L, 2L, 2L, 1L, 3L, 2L, 1L, 1L, NA, 1L, 2L,
> >3L, 2L, 3L, 2L, 3L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, NA), .Label = c("Ne",
> >"Re?ie", "V?roba", "V?zkum"), class = "factor"), osoba =
> >structure(c(1L,
> >1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L,
> >2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label =
> >c("Bern?t",
> >"?ervenka"), class = "factor"), cas = structure(list(sec = c(0,
> >0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> >0, 0, 0, 0, 0, 0, 0, 0), min = c(50L, 0L, 10L, 40L, 30L, 45L,
> >0L, 35L, 50L, 25L, 40L, 20L, 5L, 20L, 30L, 55L, 0L, 10L, 15L,
> >35L, 10L, 40L, 0L, 5L, 25L, 40L, 35L, 5L, 20L, 30L), hour = c(7L,
> >8L, 8L, 8L, 9L, 10L, 11L, 11L, 11L, 12L, 12L, 13L, 14L, 14L,
> >14L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 10L, 12L, 12L, 12L, 13L, 14L,
> >14L, 14L), mday = c(7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
> >7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
> >7L, 7L, 7L), mon = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> >6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> >6L, 6L, 6L, 6L), year = c(115L, 115L, 115L, 115L, 115L, 115L,
> >115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
> >115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
> >115L, 115L), wday = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >2L, 2L, 2L, 2L), yday = c(187L, 187L, 187L, 187L, 187L, 187L,
> >187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
> >187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
> >187L, 187L), isdst = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> >1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> >1L, 1L, 1L, 1L), zone = c("CEST", "CEST", "CEST", "CEST", "CEST",
> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
> >"CEST"), gmtoff = c(NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >NA_integer_)), .Names = c("sec", "min", "hour", "mday", "mon",
> >"year", "wday", "yday", "isdst", "zone", "gmtoff"), class =
> >c("POSIXlt",
> >"POSIXt")), doba = c(10, 10, 30, 50, 75, 15, 35, 15, 35, 15,
> >40, 45, 15, 10, NA, 5, 10, 5, 20, 35, 30, 20, 125, 20, 15, 55,
> >30, 15, 10, NA), endtime = structure(c(1436248800, 1436249400,
> >1436251200, 1436254200, 1436258700, 1436259600, 1436261700, 1436262600,
> >1436264700, 1436265600, 1436268000, 1436270700, 1436271600, 1436272200,
> >NA, 1436248800, 1436249400, 1436249700, 1436250900, 1436253000,
> >1436254800, 1436256000, 1436263500, 1436264700, 1436265600, 1436268900,
> >1436270700, 1436271600, 1436272200, NA), class = c("POSIXct",
> >"POSIXt"))), .Names = c("start", "akce", "zarizeni", "typ", "prace",
> >"osoba", "cas", "doba", "endtime"), row.names = c(NA, -30L), class =
> >"data.frame")
> >
> >
> >
> >________________________________
> >Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> >ur?eny pouze jeho adres?t?m.
> >Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> >neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho
> >kopie vyma?te ze sv?ho syst?mu.
> >Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
> >email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> >Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> >modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >
> >V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> >- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> >smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> >- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> >p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky
> >ze strany p??jemce s dodatkem ?i odchylkou.
> >- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> >v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> >- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> >spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
> >zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
> >adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
> >p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen?
> >zn?m?.
> >
> >This e-mail and any documents attached to it may be confidential and
> >are intended only for its intended recipients.
> >If you received this e-mail by mistake, please immediately inform its
> >sender. Delete the contents of this e-mail with all attachments and its
> >copies from your system.
> >If you are not the intended recipient of this e-mail, you are not
> >authorized to use, disseminate, copy or disclose this e-mail in any
> >manner.
> >The sender of this e-mail shall not be liable for any possible damage
> >caused by modifications of the e-mail or by delay with transfer of the
> >email.
> >
> >In case that this e-mail forms part of business dealings:
> >- the sender reserves the right to end negotiations about entering into
> >a contract in any time, for any reason, and without stating any
> >reasoning.
> >- if the e-mail contains an offer, the recipient is entitled to
> >immediately accept such offer; The sender of this e-mail (offer)
> >excludes any acceptance of the offer on the part of the recipient
> >containing any amendment or variation.
> >- the sender insists on that the respective contract is concluded only
> >upon an express mutual agreement on all its aspects.
> >- the sender of this e-mail informs that he/she is not authorized to
> >enter into any contracts on behalf of the company except for cases in
> >which he/she is expressly authorized to do so in writing, and such
> >authorization or power of attorney is submitted to the recipient or the
> >person represented by the recipient, or the existence of such
> >authorization is known to the recipient of the person represented by
> >the recipient.
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From angelo.arcadi at virgilio.it  Tue Jul  7 15:16:53 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 7 Jul 2015 15:16:53 +0200 (CEST)
Subject: [R] R: Re: Sphericity for a repeated measure ANOVA with whithin and
 between subjects design, using R
Message-ID: <14e68a97822.angelo.arcadi@virgilio.it>

Thank you John, so far the easiest way is to use ezanova() since I do not have to reorder the data
and create iframe etc.


Best





----Messaggio originale----
Da: pdalgd at gmail.com
Data: 7-lug-2015 10.12
A: "John Fox"<jfox at mcmaster.ca>
Cc: <angelo.arcadi at virgilio.it>, <r-help at r-project.org>
Ogg: Re: [R] Sphericity for a repeated measure ANOVA with whithin and between subjects design, using R

Another method goes via anova.mlm(), which is pretty straight forward once you figure out how to set up the necessary within-subject projections (contrasts). See

@Article{Rnews:Dalgaard:2007,
 author       = {Peter Dalgaard},
 title        = {New Functions for Multivariate Analysis},
 journal      = {R News},
 year         = 2007,
 volume       = 7,
 number       = 2,
 pages        = {2--7},
}

and ?anova.mlm.

-pd

> On 06 Jul 2015, at 18:17 , John Fox <jfox at mcmaster.ca> wrote:
> 
> Dear Angelo,
> 
> One way to do this is with the Anova() function in the car package; see the
> following article in the R Journal:
> 
> @Article{RJournal:2013-1:fox-friendly-weisberg,
> author       = {John Fox and Michael Friendly and Sanford Weisberg}, 
> title        = {Hypothesis Tests for Multivariate Linear Models Using the
> {car} Package}, 
> journal      = {The R Journal},
> year         = 2013,
> volume       = 5,
> number       = 1,
> pages        = {39--53},
> month        = jun,
> url          =
> {http://journal.r-project.org/archive/2013-1/RJournal_2013-1_fox-friendly-we
> isberg.pdf}
> 
> I hope this helps,
> John
> 
> 
> -----------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.socsci.mcmaster.ca/jfox/
> 
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> angelo.arcadi at virgilio.it
>> Sent: July-06-15 10:05 AM
>> To: r-help at r-project.org
>> Subject: [R] Sphericity for a repeated measure ANOVA with whithin and
>> between subjects design, using R
>> 
>> Dear List Members,
>> 
>> 
>> 
>> I need to perform a four-way ANOVA with repeated measures and to
>> calculate the sphericity for eventual correction of the degree of
>> freedom of the F ratio.
>> I have three within subjects factors (Emotion, having 5 levels,
>> Material, having 4 levels, Shoes, having 2 levels) and a between subject
>> factor (Musician, having 2 levels). Without considering the sphericity
>> I
>> use the following formula
>> 
>> aov_SPL = aov(SPL ~ Emotion*Material*Shoes*Musician +
>> Error(Subject/(Emotion*Material*Shoes)), data=scrd)
>> 
>> 
>> 
>> Unfortunately after having read lot of material online I did not
>> arrive to a solution about how to calculate for my case the epsilon of
>> the Greenhouse-Geisser method for degree of freedom adjustment.
>> 
>> 
>> I load my data in R with this formula:
>> 
>> scrd<- read.csv(file='/path to data/data.csv',sep=',',header=T)
>> 
>> 
>> 
>> and this is the structure of the imported table:
>> 
>>> head(scrd)
>>   Subject Material       Shoes    Emotion  H2H H2H_factor SPL_factor
>> SPL_variation     SPL Musician Weight Height
>> 1  Subject1   Gravel dress_shoes AGGRESSIVE  468      0.736     11.591
>> 21.283  97.383       no     90    183
>> 2  Subject1   Gravel dress_shoes      HAPPY  719      1.129      3.188
>> 10.071  86.171       no     90    183
>> 3  Subject1   Gravel dress_shoes     TENDER 1129      1.774      5.114
>> 14.176  90.276       no     90    183
>> 4  Subject1   Gravel dress_shoes        SAD 1010      1.587     13.102
>> 22.347  98.447       no     90    183
>> 5  Subject1   Gravel dress_shoes    NEUTRAL  736      1.156      3.161
>> 9.995  86.095       no     90    183
>> 6 Subject10   Gravel dress_shoes AGGRESSIVE  635      0.998     15.849
>> 24.000 100.100      yes     70    178
>>> 
>> 
>> 
>> 
>> I noticed that in the car package there is the Anova() function that
>> comes with the Maulchy's sphericity test, but it does not take as an
>> input a model generated with aov(). I need to use lm() instead, but
>> since
>> I am not really proficient in R I do not know how to use the lm()
>> starting from the loaded data and how to use Anova(). In addition, being
>> a mixed design involving within and between subjects factors I wonder
>> if Anova() is the most appropriate function to use for my case.
>> 
>> 
>> Is there anyone who could provide me with the R code to calculate the
>> Maulchy's test and the epsilon of Greenhouse-Geisser for my case?
>> 
>> 
>> Thank you in advance
>> Best
>> 
>> Angelo
>> 
>> 
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ---
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com











   
	[[alternative HTML version deleted]]


From angelo.arcadi at virgilio.it  Tue Jul  7 15:20:19 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 7 Jul 2015 15:20:19 +0200 (CEST)
Subject: [R] R: Re: R: RE: Bonferroni post hoc test in R for repeated
 measure ANOVA with mixed within and between subjects design
Message-ID: <14e68ac9bb0.angelo.arcadi@virgilio.it>

Dear John,
thank you, I normally use Tuckey but I was told that Bonferroni is better, so I wanted to try.

For number of tests what do you mean exactly? How do I calculate it?

In the end is it sufficient to add  test=adjusted("bonferroni") as previously suggested?


Best

A.




----Messaggio originale----
Da: jfox at mcmaster.ca
Data: 7-lug-2015 3.23
A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>
Cc: <meyners.m at pg.com>, "r-help at r-project.org"<r-help at r-project.org>
Ogg: Re: [R] R: RE: Bonferroni post hoc test in R for repeated measure ANOVA with mixed within and between subjects design

Dear Angelo,

The Bonferroni p-value is just the ordinary p-value times the number of tests, so, since R supports multiplication, you can apply the Bonferroni adjustment in R. Because Bonferroni tests for multiple comparisons can be very conservative, asking why you want to use them is a fair question.

Best,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	
	
	
On Tue, 7 Jul 2015 00:50:49 +0200 (CEST)
 "angelo.arcadi at virgilio.it" <angelo.arcadi at virgilio.it> wrote:
> Dear Michael,
> thank you for your answer, however, I am not asking for the tukey
> with the bonferroni adjustment, but doing the post hoc with the bonferroni method.
> Apparently this is done easily in SPSS, I am wondering whether it is possible with R.
> 
> Can anyone help me?
> 
> Thanks in advance
> 
> 
> Angelo
> 
> 
> 
> ----Messaggio originale----
> Da: meyners.m at pg.com
> Data: 6-lug-2015 17.52
> A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, "r-help at r-project.org"<r-help at r-project.org>
> Ogg: RE: [R] Bonferroni post hoc test in R for repeated measure ANOVA with mixed within and between subjects design
> 
> Untested, but if anything, your best bet is likely something like 
> 
> summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")), test=adjusted("bonferroni"))
> 
> should work (despite the question why you'd want to use Bonferroni rather than Tukey
> For a reference, see the book on the topic by the package authors. Might be in the paper, too, which is given by
> 
> citation("multcomp")
> 
> HTH, Michael
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > angelo.arcadi at virgilio.it
> > Sent: Montag, 6. Juli 2015 16:01
> > To: r-help at r-project.org
> > Subject: [R] Bonferroni post hoc test in R for repeated measure ANOVA with
> > mixed within and between subjects design
> > 
> > Dear List Members,
> > 
> > 
> > 
> > I need to perform a Bonferroni post hoc test in R on a table with three within
> > subjects factors (Emotion, having 5 levels, Material, having 4 levels, Shoes,
> > having 2 levels) and one between subject factor (Musician, having 2 levels).
> > 
> > 
> > I normally use the Tukey method with the following formula
> > 
> > require(nlme)
> > lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd,
> > random = ~1|Subject)
> > require(multcomp)
> > summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))
> > 
> > 
> > 
> > I am not able to find any reference that explains with an example of R  code
> > how to perform a post hoc test with the Bonferroni procedure.
> > Can anyone provide an example to perform the same post hoc test in the
> > code above but with Bonferroni instead of Tukey?
> > 
> > 
> > Thank you in advance
> > 
> > 
> > 
> > Angelo
> > 
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
>    
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



   
	[[alternative HTML version deleted]]


From angelo.arcadi at virgilio.it  Tue Jul  7 15:23:52 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 7 Jul 2015 15:23:52 +0200 (CEST)
Subject: [R] R: RE: R: RE: Bonferroni post hoc test in R for repeated
 measure ANOVA with mixed within and between subjects design
Message-ID: <14e68afdad4.angelo.arcadi@virgilio.it>

Dear Michael,
I am very grateful to you for the detailed explanation, now everything is clear. In the end I will opt for the Tuckey method then.

Thanks

Best

A.




----Messaggio originale----
Da: meyners.m at pg.com
Data: 7-lug-2015 8.58
A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>
Cc: "r-help at r-project.org"<r-help at r-project.org>, "John Fox"<jfox at mcmaster.ca>
Ogg: RE: [R] R: RE: Bonferroni post hoc test in R for repeated measure ANOVA with mixed within and between subjects design

Angelo,

the conservatism of Bonferroni aside for a moment, I don't really see what you are after. 

mcp(Emotion = "Tukey")

just defines the contrasts to test; in this case, all pairwise comparisons. 

The test=adjusted("bonferroni") argument then says that these contrasts should not be corrected for according to Tukey's procedure, but using Bonferroni adjustment. 

So this should give you tests for all pairwise comparisons with multiplicity correction according to Bonferroni. I wonder how this differs from the results you get from SPSS. 

Seemingly, this is not what you want, so the question is what you really want. "Bonferroni method" does not indicate which comparisons / contrasts to look at, but just takes all those that you are interested in and multiply the corresponding p values with the number of comparisons (and making sure it does not exceed 1). As John indicated, that can easily be handled manually. Yet, you need to create the tests for all comparisons that you are interested in - if not via  linfct=mcp(Emotion = "Tukey"), you need to specify them otherwise (see the three options indicated on ?glht). The code I suggested offers a convenient shortcut in case you are interested in all pairwise comparisons and want them to be corrected according to Bonferroni, but if something else is of interest, you'd need to specify this (and let us know, as "Bonferroni method" does not give a clue about which comparisons to test).

NB: You may want to pay attention to the warning halfway down the helppage for ?mcp; it may not be clear exactly which effects you want to compare; mcp uses just the main effects w/o interactions etc.

Michael


> -----Original Message-----
> From: John Fox [mailto:jfox at mcmaster.ca]
> Sent: Dienstag, 7. Juli 2015 03:24
> To: angelo.arcadi at virgilio.it
> Cc: Meyners, Michael; r-help at r-project.org
> Subject: Re: [R] R: RE: Bonferroni post hoc test in R for repeated measure
> ANOVA with mixed within and between subjects design
> 
> Dear Angelo,
> 
> The Bonferroni p-value is just the ordinary p-value times the number of
> tests, so, since R supports multiplication, you can apply the Bonferroni
> adjustment in R. Because Bonferroni tests for multiple comparisons can be
> very conservative, asking why you want to use them is a fair question.
> 
> Best,
>  John
> 
> ------------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
> 
> 
> 
> On Tue, 7 Jul 2015 00:50:49 +0200 (CEST)  "angelo.arcadi at virgilio.it"
> <angelo.arcadi at virgilio.it> wrote:
> > Dear Michael,
> > thank you for your answer, however, I am not asking for the tukey with
> > the bonferroni adjustment, but doing the post hoc with the bonferroni
> method.
> > Apparently this is done easily in SPSS, I am wondering whether it is possible
> with R.
> >
> > Can anyone help me?
> >
> > Thanks in advance
> >
> >
> > Angelo
> >
> >
> >
> > ----Messaggio originale----
> > Da: meyners.m at pg.com
> > Data: 6-lug-2015 17.52
> > A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>,
> > "r-help at r-project.org"<r-help at r-project.org>
> > Ogg: RE: [R] Bonferroni post hoc test in R for repeated measure ANOVA
> > with mixed within and between subjects design
> >
> > Untested, but if anything, your best bet is likely something like
> >
> > summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")),
> > test=adjusted("bonferroni"))
> >
> > should work (despite the question why you'd want to use Bonferroni
> > rather than Tukey For a reference, see the book on the topic by the
> > package authors. Might be in the paper, too, which is given by
> >
> > citation("multcomp")
> >
> > HTH, Michael
> >
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > > angelo.arcadi at virgilio.it
> > > Sent: Montag, 6. Juli 2015 16:01
> > > To: r-help at r-project.org
> > > Subject: [R] Bonferroni post hoc test in R for repeated measure
> > > ANOVA with mixed within and between subjects design
> > >
> > > Dear List Members,
> > >
> > >
> > >
> > > I need to perform a Bonferroni post hoc test in R on a table with
> > > three within subjects factors (Emotion, having 5 levels, Material,
> > > having 4 levels, Shoes, having 2 levels) and one between subject factor
> (Musician, having 2 levels).
> > >
> > >
> > > I normally use the Tukey method with the following formula
> > >
> > > require(nlme)
> > > lme_H2H = lme(H2H ~ Emotion*Material*Shoes*Musician, data=scrd,
> > > random = ~1|Subject)
> > > require(multcomp)
> > > summary(glht(lme_H2H, linfct=mcp(Emotion = "Tukey")))
> > >
> > >
> > >
> > > I am not able to find any reference that explains with an example of
> > > R  code how to perform a post hoc test with the Bonferroni procedure.
> > > Can anyone provide an example to perform the same post hoc test in
> > > the code above but with Bonferroni instead of Tukey?
> > >
> > >
> > > Thank you in advance
> > >
> > >
> > >
> > > Angelo
> > >
> > >
> > > 	[[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html and provide commented, minimal, self-contained,
> > > reproducible code.
> >
> >
> >
> >
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.



   
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Tue Jul  7 15:45:09 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 07 Jul 2015 06:45:09 -0700
Subject: [R] geom_segment drop unused levels
In-Reply-To: <CAJuCY5zGcNPyvdcWKXimDh=j14mUYCzsSg9ZLhLBQPZ_B2_=VQ@mail.gmail.com>
References: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33A2D@SRVEXCHMBX.precheza.cz>
	<61CB9399-4EF9-4F6E-9A03-C8CA5F9A1F31@dcn.davis.CA.us>
	<CAJuCY5zGcNPyvdcWKXimDh=j14mUYCzsSg9ZLhLBQPZ_B2_=VQ@mail.gmail.com>
Message-ID: <3B70E693-5F48-4713-91CA-380255817E7A@dcn.davis.CA.us>

Sorry, now it works for me as well. And yes, must drop rows with is.na( prace ) prior to calling ggplot.  Note that the levels of prace  do not contain NA so your subject line is not correct.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 7, 2015 6:09:01 AM PDT, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:
>I disagree with you Jeff. I can reproduce it without any problem.
>
>I see no other option than to remove the offending missing data prior
>to
>plotting.
>
>Best regards,
>
>ir. Thierry Onkelinx
>Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>and
>Forest
>team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>Kliniekstraat 25
>1070 Anderlecht
>Belgium
>
>To call in the statistician after the experiment is done may be no more
>than asking him to perform a post-mortem examination: he may be able to
>say
>what the experiment died of. ~ Sir Ronald Aylmer Fisher
>The plural of anecdote is not data. ~ Roger Brinner
>The combination of some data and an aching desire for an answer does
>not
>ensure that a reasonable answer can be extracted from a given body of
>data.
>~ John Tukey
>
>2015-07-07 14:56 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>
>> Not reproducible.
>>
>---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>>
>---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 7, 2015 12:44:20 AM PDT, PIKAL Petr <petr.pikal at precheza.cz>
>> wrote:
>> >Dear all.
>> >
>> >I want to drop unused (NA) level from geom_segment plot. I tried
>> >different combination of drop = TRUE argument in several places of
>> >following code but NA level of prace is still present.
>> >
>> >p<-ggplot(snimek, aes(x=cas, y=prace, xend=endtime, yend=prace,
>> >colour=typ, size=osoba))
>> >p+geom_segment(alpha=.4)+scale_size_discrete(range=c(3,6))+
>> >guides(colour = guide_legend(override.aes = list(size=4)))
>> >
>> >Is it possible to remove NA level directly from ggplot call or do I
>> >need to discard rows with NA before plotting?
>> >
>> >Below are data
>> >
>> >Best regards.
>> >Petr
>> >
>> >> dput(snimek)
>> >snimek <- structure(list(start = c(7.5, 8, 8.1, 8.4, 9.3, 10.45, 11,
>> >11.35,
>> >11.5, 12.25, 12.4, 13.2, 14.05, 14.2, 14.3, 7.55, 8, 8.1, 8.15,
>> >8.35, 9.1, 9.4, 10, 12.05, 12.25, 12.4, 13.35, 14.05, 14.2, 14.3
>> >), akce = structure(c(9L, 7L, 11L, 4L, 14L, 8L, 6L, 16L, 14L,
>> >2L, 15L, 10L, 2L, 8L, 5L, 12L, 7L, 13L, 7L, 18L, 7L, 3L, 7L,
>> >6L, 2L, 1L, 17L, 2L, 8L, 5L), .Label = c("Administrativa",
>"Cigareta",
>> >"CL vzorky", "?i?t?n? MM", "Konec", "Ob?d", "Po??ta?", "P?est?vka",
>> >"Satna", "Sklad", "Sva?ina", "?atna", "Tisk dokumentace", "?klid",
>> >"?klid v?robny", "Vymyt? sud?", "V?stra?n? cedule", "Vzorkov?n?"
>> >), class = "factor"), zarizeni = c(NA, NA, NA, NA, NA, NA, NA,
>> >NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>> >NA, NA, NA, NA, NA, NA, NA), typ = structure(c(4L, 5L, 4L, 5L,
>> >5L, 4L, 2L, 5L, 5L, 1L, 5L, 5L, 1L, 4L, NA, 4L, 3L, 5L, 5L, 5L,
>> >5L, 5L, 5L, 2L, 1L, 5L, 5L, 1L, 4L, NA), .Label = c("Kou?en?",
>> >"Ob?d", "Po??ta?", "Prostoj", "Re?ie"), class = "factor"), prace =
>> >structure(c(1L,
>> >2L, 1L, 4L, 2L, 1L, 1L, 2L, 2L, 1L, 3L, 2L, 1L, 1L, NA, 1L, 2L,
>> >3L, 2L, 3L, 2L, 3L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, NA), .Label =
>c("Ne",
>> >"Re?ie", "V?roba", "V?zkum"), class = "factor"), osoba =
>> >structure(c(1L,
>> >1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L,
>> >2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label =
>> >c("Bern?t",
>> >"?ervenka"), class = "factor"), cas = structure(list(sec = c(0,
>> >0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>> >0, 0, 0, 0, 0, 0, 0, 0), min = c(50L, 0L, 10L, 40L, 30L, 45L,
>> >0L, 35L, 50L, 25L, 40L, 20L, 5L, 20L, 30L, 55L, 0L, 10L, 15L,
>> >35L, 10L, 40L, 0L, 5L, 25L, 40L, 35L, 5L, 20L, 30L), hour = c(7L,
>> >8L, 8L, 8L, 9L, 10L, 11L, 11L, 11L, 12L, 12L, 13L, 14L, 14L,
>> >14L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 10L, 12L, 12L, 12L, 13L, 14L,
>> >14L, 14L), mday = c(7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>> >7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
>> >7L, 7L, 7L), mon = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>> >6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
>> >6L, 6L, 6L, 6L), year = c(115L, 115L, 115L, 115L, 115L, 115L,
>> >115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
>> >115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
>> >115L, 115L), wday = c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> >2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>> >2L, 2L, 2L, 2L), yday = c(187L, 187L, 187L, 187L, 187L, 187L,
>> >187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
>> >187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
>> >187L, 187L), isdst = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> >1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> >1L, 1L, 1L, 1L), zone = c("CEST", "CEST", "CEST", "CEST", "CEST",
>> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
>> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
>> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
>> >"CEST"), gmtoff = c(NA_integer_, NA_integer_, NA_integer_,
>NA_integer_,
>> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
>> >NA_integer_)), .Names = c("sec", "min", "hour", "mday", "mon",
>> >"year", "wday", "yday", "isdst", "zone", "gmtoff"), class =
>> >c("POSIXlt",
>> >"POSIXt")), doba = c(10, 10, 30, 50, 75, 15, 35, 15, 35, 15,
>> >40, 45, 15, 10, NA, 5, 10, 5, 20, 35, 30, 20, 125, 20, 15, 55,
>> >30, 15, 10, NA), endtime = structure(c(1436248800, 1436249400,
>> >1436251200, 1436254200, 1436258700, 1436259600, 1436261700,
>1436262600,
>> >1436264700, 1436265600, 1436268000, 1436270700, 1436271600,
>1436272200,
>> >NA, 1436248800, 1436249400, 1436249700, 1436250900, 1436253000,
>> >1436254800, 1436256000, 1436263500, 1436264700, 1436265600,
>1436268900,
>> >1436270700, 1436271600, 1436272200, NA), class = c("POSIXct",
>> >"POSIXt"))), .Names = c("start", "akce", "zarizeni", "typ", "prace",
>> >"osoba", "cas", "doba", "endtime"), row.names = c(NA, -30L), class =
>> >"data.frame")
>> >
>> >
>> >
>> >________________________________
>> >Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
>jsou
>> >ur?eny pouze jeho adres?t?m.
>> >Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> >neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a
>jeho
>> >kopie vyma?te ze sv?ho syst?mu.
>> >Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> >email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> >Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
>> >modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>> >
>> >V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> >- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> >smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> >- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> >p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet?
>nab?dky
>> >ze strany p??jemce s dodatkem ?i odchylkou.
>> >- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> >v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> >- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> >spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
>> >zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
>> >adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
>> >p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m
>zastoupen?
>> >zn?m?.
>> >
>> >This e-mail and any documents attached to it may be confidential and
>> >are intended only for its intended recipients.
>> >If you received this e-mail by mistake, please immediately inform
>its
>> >sender. Delete the contents of this e-mail with all attachments and
>its
>> >copies from your system.
>> >If you are not the intended recipient of this e-mail, you are not
>> >authorized to use, disseminate, copy or disclose this e-mail in any
>> >manner.
>> >The sender of this e-mail shall not be liable for any possible
>damage
>> >caused by modifications of the e-mail or by delay with transfer of
>the
>> >email.
>> >
>> >In case that this e-mail forms part of business dealings:
>> >- the sender reserves the right to end negotiations about entering
>into
>> >a contract in any time, for any reason, and without stating any
>> >reasoning.
>> >- if the e-mail contains an offer, the recipient is entitled to
>> >immediately accept such offer; The sender of this e-mail (offer)
>> >excludes any acceptance of the offer on the part of the recipient
>> >containing any amendment or variation.
>> >- the sender insists on that the respective contract is concluded
>only
>> >upon an express mutual agreement on all its aspects.
>> >- the sender of this e-mail informs that he/she is not authorized to
>> >enter into any contracts on behalf of the company except for cases
>in
>> >which he/she is expressly authorized to do so in writing, and such
>> >authorization or power of attorney is submitted to the recipient or
>the
>> >person represented by the recipient, or the existence of such
>> >authorization is known to the recipient of the person represented by
>> >the recipient.
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From fisher at plessthan.com  Tue Jul  7 16:20:54 2015
From: fisher at plessthan.com (Dennis Fisher)
Date: Tue, 7 Jul 2015 07:20:54 -0700
Subject: [R] Implementation of contrasts
Message-ID: <6BB221F9-8708-4D62-BBF7-2D73268AD1FF@plessthan.com>

R 3.2.1
OS X

Colleagues

I am trying to understand ANOVA contrasts and I have encountered some puzzling results.

I have data from five studies with four different combinations of treatments.  I want to make the following comparisons:

Study 1, 3: 	A / B vs. C / D (i.e., mean of A and B vs. mean of C and D)
Study 2: 		A vs. B / C
Study 4: 		A / B / C vs. D
Study 5: 		A vs. B / C / D

With some trial-and-error, I got the following code to yield results matching SAS outputs:
	AB.CD 					<- c(1, 1, -1, -1)/2 
	A.BC 					<- c(1, -1, -1)/2
	ABC.D 					<- c(1, 1, 1, -1)/2
	A.BCD 					<- c(1, -1, -1, -1)/2 
	if (STUDY %in% c(1,3)))		contrasts(DATA$TRT)     <- AB.CD
	if (STUDY == 2) 			contrasts(DATA$TRT)     <- A.BC
	if (STUDY == 4)			contrasts(DATA$TRT)     <- ABC.D
	if (STUDY == 5)			contrasts(DATA$TRT)     <- A.BCD

AB.CD makes sense to me ? take one-half of each of A and B compare to negative one-half of C and D (the contrasts add to zero).
However, I don?t understand how the other contrasts are written (i.e., they don?t add to zero).  For example, I tried:
	A.BC					<- c(2, -1, -1) / 2
	ABC.D 					<- c(1, 1, 1, -3)/3
without success (they yielded results markedly different from SAS)

I have searched the web extensively but the explanations of contrasts in R are not particularly understandable.  Can anyone help me understand the specifics of this situation?  Thanks in advance.

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone: 1-866-PLessThan (1-866-753-7784)
Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From petr.pikal at precheza.cz  Tue Jul  7 16:39:51 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 7 Jul 2015 14:39:51 +0000
Subject: [R] geom_segment drop unused levels
In-Reply-To: <3B70E693-5F48-4713-91CA-380255817E7A@dcn.davis.CA.us>
References: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33A2D@SRVEXCHMBX.precheza.cz>
	<61CB9399-4EF9-4F6E-9A03-C8CA5F9A1F31@dcn.davis.CA.us>
	<CAJuCY5zGcNPyvdcWKXimDh=j14mUYCzsSg9ZLhLBQPZ_B2_=VQ@mail.gmail.com>
	<3B70E693-5F48-4713-91CA-380255817E7A@dcn.davis.CA.us>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33BEB@SRVEXCHMBX.precheza.cz>

Thanks to all who responded. It is easy to remove NA from factor so I will go this way. I just wanted to know that I did not miss some simple canned solution.

Best regards
Petr

> -----Original Message-----
> From: Jeff Newmiller [mailto:jdnewmil at dcn.davis.CA.us]
> Sent: Tuesday, July 07, 2015 3:45 PM
> To: Thierry Onkelinx
> Cc: PIKAL Petr; R-help
> Subject: Re: [R] geom_segment drop unused levels
>
> Sorry, now it works for me as well. And yes, must drop rows with is.na(
> prace ) prior to calling ggplot.  Note that the levels of prace  do not
> contain NA so your subject line is not correct.
> -----------------------------------------------------------------------
> ----
> Jeff Newmiller                        The     .....       .....  Go
> Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..
> Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> -----------------------------------------------------------------------
> ----
> Sent from my phone. Please excuse my brevity.
>
> On July 7, 2015 6:09:01 AM PDT, Thierry Onkelinx
> <thierry.onkelinx at inbo.be> wrote:
> >I disagree with you Jeff. I can reproduce it without any problem.
> >
> >I see no other option than to remove the offending missing data prior
> >to plotting.
> >
> >Best regards,
> >
> >ir. Thierry Onkelinx
> >Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> >and Forest team Biometrie & Kwaliteitszorg / team Biometrics & Quality
> >Assurance Kliniekstraat 25 1070 Anderlecht Belgium
> >
> >To call in the statistician after the experiment is done may be no
> more
> >than asking him to perform a post-mortem examination: he may be able
> to
> >say what the experiment died of. ~ Sir Ronald Aylmer Fisher The plural
> >of anecdote is not data. ~ Roger Brinner The combination of some data
> >and an aching desire for an answer does not ensure that a reasonable
> >answer can be extracted from a given body of data.
> >~ John Tukey
> >
> >2015-07-07 14:56 GMT+02:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
> >
> >> Not reproducible.
> >>
> >----------------------------------------------------------------------
> -
> >----
> >> Jeff Newmiller                        The     .....       .....  Go
> >Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
> Live
> >> Go...
> >>                                       Live:   OO#.. Dead: OO#..
> >Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.
> with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> >rocks...1k
> >>
> >----------------------------------------------------------------------
> -
> >----
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 7, 2015 12:44:20 AM PDT, PIKAL Petr <petr.pikal at precheza.cz>
> >> wrote:
> >> >Dear all.
> >> >
> >> >I want to drop unused (NA) level from geom_segment plot. I tried
> >> >different combination of drop = TRUE argument in several places of
> >> >following code but NA level of prace is still present.
> >> >
> >> >p<-ggplot(snimek, aes(x=cas, y=prace, xend=endtime, yend=prace,
> >> >colour=typ, size=osoba))
> >> >p+geom_segment(alpha=.4)+scale_size_discrete(range=c(3,6))+
> >> >guides(colour = guide_legend(override.aes = list(size=4)))
> >> >
> >> >Is it possible to remove NA level directly from ggplot call or do I
> >> >need to discard rows with NA before plotting?
> >> >
> >> >Below are data
> >> >
> >> >Best regards.
> >> >Petr
> >> >
> >> >> dput(snimek)
> >> >snimek <- structure(list(start = c(7.5, 8, 8.1, 8.4, 9.3, 10.45,
> 11,
> >> >11.35, 11.5, 12.25, 12.4, 13.2, 14.05, 14.2, 14.3, 7.55, 8, 8.1,
> >> >8.15, 8.35, 9.1, 9.4, 10, 12.05, 12.25, 12.4, 13.35, 14.05, 14.2,
> >> >14.3 ), akce = structure(c(9L, 7L, 11L, 4L, 14L, 8L, 6L, 16L, 14L,
> >> >2L, 15L, 10L, 2L, 8L, 5L, 12L, 7L, 13L, 7L, 18L, 7L, 3L, 7L, 6L,
> 2L,
> >> >1L, 17L, 2L, 8L, 5L), .Label = c("Administrativa",
> >"Cigareta",
> >> >"CL vzorky", "?i?t?n? MM", "Konec", "Ob?d", "Po??ta?", "P?est?vka",
> >> >"Satna", "Sklad", "Sva?ina", "?atna", "Tisk dokumentace", "?klid",
> >> >"?klid v?robny", "Vymyt? sud?", "V?stra?n? cedule", "Vzorkov?n?"
> >> >), class = "factor"), zarizeni = c(NA, NA, NA, NA, NA, NA, NA, NA,
> >> >NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
> >> >NA, NA, NA, NA, NA), typ = structure(c(4L, 5L, 4L, 5L, 5L, 4L, 2L,
> >> >5L, 5L, 1L, 5L, 5L, 1L, 4L, NA, 4L, 3L, 5L, 5L, 5L, 5L, 5L, 5L, 2L,
> >> >1L, 5L, 5L, 1L, 4L, NA), .Label = c("Kou?en?", "Ob?d", "Po??ta?",
> >> >"Prostoj", "Re?ie"), class = "factor"), prace = structure(c(1L, 2L,
> >> >1L, 4L, 2L, 1L, 1L, 2L, 2L, 1L, 3L, 2L, 1L, 1L, NA, 1L, 2L, 3L, 2L,
> >> >3L, 2L, 3L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, NA), .Label =
> >c("Ne",
> >> >"Re?ie", "V?roba", "V?zkum"), class = "factor"), osoba =
> >> >structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> >> >1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L),
> >> >.Label = c("Bern?t", "?ervenka"), class = "factor"), cas =
> >> >structure(list(sec = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> >> >0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), min = c(50L, 0L, 10L,
> >> >40L, 30L, 45L, 0L, 35L, 50L, 25L, 40L, 20L, 5L, 20L, 30L, 55L, 0L,
> >> >10L, 15L, 35L, 10L, 40L, 0L, 5L, 25L, 40L, 35L, 5L, 20L, 30L), hour
> >> >= c(7L, 8L, 8L, 8L, 9L, 10L, 11L, 11L, 11L, 12L, 12L, 13L, 14L,
> 14L,
> >> >14L, 7L, 8L, 8L, 8L, 8L, 9L, 9L, 10L, 12L, 12L, 12L, 13L, 14L, 14L,
> >> >14L), mday = c(7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
> >> >7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
> 7L),
> >> >mon = c(6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
> >> >6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L), year =
> >> >c(115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
> >> >115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L,
> >> >115L, 115L, 115L, 115L, 115L, 115L, 115L, 115L), wday = c(2L, 2L,
> >> >2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
> >> >2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), yday = c(187L, 187L,
> >> >187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
> >> >187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L, 187L,
> >> >187L, 187L, 187L, 187L, 187L, 187L), isdst = c(1L, 1L, 1L, 1L, 1L,
> >> >1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> >> >1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), zone = c("CEST", "CEST", "CEST",
> >> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
> >> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
> >> >"CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST", "CEST",
> >> >"CEST", "CEST", "CEST"), gmtoff = c(NA_integer_, NA_integer_,
> >> >NA_integer_,
> >NA_integer_,
> >> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >> >NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_,
> >> >NA_integer_)), .Names = c("sec", "min", "hour", "mday", "mon",
> >> >"year", "wday", "yday", "isdst", "zone", "gmtoff"), class =
> >> >c("POSIXlt", "POSIXt")), doba = c(10, 10, 30, 50, 75, 15, 35, 15,
> >> >35, 15, 40, 45, 15, 10, NA, 5, 10, 5, 20, 35, 30, 20, 125, 20, 15,
> >> >55, 30, 15, 10, NA), endtime = structure(c(1436248800, 1436249400,
> >> >1436251200, 1436254200, 1436258700, 1436259600, 1436261700,
> >1436262600,
> >> >1436264700, 1436265600, 1436268000, 1436270700, 1436271600,
> >1436272200,
> >> >NA, 1436248800, 1436249400, 1436249700, 1436250900, 1436253000,
> >> >1436254800, 1436256000, 1436263500, 1436264700, 1436265600,
> >1436268900,
> >> >1436270700, 1436271600, 1436272200, NA), class = c("POSIXct",
> >> >"POSIXt"))), .Names = c("start", "akce", "zarizeni", "typ",
> "prace",
> >> >"osoba", "cas", "doba", "endtime"), row.names = c(NA, -30L), class
> =
> >> >"data.frame")
> >> >
> >> >
> >> >
> >> >________________________________
> >> >Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a
> >jsou
> >> >ur?eny pouze jeho adres?t?m.
> >> >Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> >> >neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a
> >jeho
> >> >kopie vyma?te ze sv?ho syst?mu.
> >> >Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni
> tento
> >> >email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> >> >Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou
> >> >modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
> >> >
> >> >V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> >> >- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> >> >smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> >> >- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
> >> >p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet?
> >nab?dky
> >> >ze strany p??jemce s dodatkem ?i odchylkou.
> >> >- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> >> >v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> >> >- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> >> >spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn?
> >> >zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly
> >> >adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje,
> >> >p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m
> >zastoupen?
> >> >zn?m?.
> >> >
> >> >This e-mail and any documents attached to it may be confidential
> and
> >> >are intended only for its intended recipients.
> >> >If you received this e-mail by mistake, please immediately inform
> >its
> >> >sender. Delete the contents of this e-mail with all attachments and
> >its
> >> >copies from your system.
> >> >If you are not the intended recipient of this e-mail, you are not
> >> >authorized to use, disseminate, copy or disclose this e-mail in any
> >> >manner.
> >> >The sender of this e-mail shall not be liable for any possible
> >damage
> >> >caused by modifications of the e-mail or by delay with transfer of
> >the
> >> >email.
> >> >
> >> >In case that this e-mail forms part of business dealings:
> >> >- the sender reserves the right to end negotiations about entering
> >into
> >> >a contract in any time, for any reason, and without stating any
> >> >reasoning.
> >> >- if the e-mail contains an offer, the recipient is entitled to
> >> >immediately accept such offer; The sender of this e-mail (offer)
> >> >excludes any acceptance of the offer on the part of the recipient
> >> >containing any amendment or variation.
> >> >- the sender insists on that the respective contract is concluded
> >only
> >> >upon an express mutual agreement on all its aspects.
> >> >- the sender of this e-mail informs that he/she is not authorized
> to
> >> >enter into any contracts on behalf of the company except for cases
> >in
> >> >which he/she is expressly authorized to do so in writing, and such
> >> >authorization or power of attorney is submitted to the recipient or
> >the
> >> >person represented by the recipient, or the existence of such
> >> >authorization is known to the recipient of the person represented
> by
> >> >the recipient.
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From lid.zigh at gmail.com  Tue Jul  7 16:42:56 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Tue, 7 Jul 2015 09:42:56 -0500
Subject: [R] add a special column to a matrix
Message-ID: <CAMqbV1BKQuoOBL42Q+mdroYg-4qYLL977pQ7mZ1uDRQA4jAE4w@mail.gmail.com>

Hi there,

I have a two matrices which they have a common column! I want to add the a
column of second to matrix to the equivalent column of first matrix!
my first matrix is
head(mat1)
a       b         c           d
fg    1:23       dfgv       5
pt    10:18     tgtgh      1
wq   15:123    oiljk       6
fg     9:1323    ass       4
yr     12:123    kjjlk       5

my second matrix:
head(mat2)
       e            q
  1:23             0
   10:18           1
  14:455          2
   15:123         2
    9:1323        2
    12:123        1
     5:1548        0

mat1 and mat2 have a common column (b and e) but is not in the same order,
I want to add the q column from mat2 to be added to mat1
my output will be:

head(mat)
a       b         c           d         q
fg    1:23       dfgv       5        0
pt    10:18     tgtgh      1        1
wq   15:123    oiljk       6        2
fg     9:1323    ass       4        2
yr     12:123    kjjlk       5        1

would you please let me know how to do it?

Thanks

	[[alternative HTML version deleted]]


From mxkuhn at gmail.com  Tue Jul  7 16:52:38 2015
From: mxkuhn at gmail.com (Max Kuhn)
Date: Tue, 7 Jul 2015 10:52:38 -0400
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <web-564904636@cgpsrv2.cis.mcmaster.ca>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
	<web-564884742@cgpsrv2.cis.mcmaster.ca>
	<915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>
	<web-564904636@cgpsrv2.cis.mcmaster.ca>
Message-ID: <CAJ9CoWm0KB6d9VPVkf9SUWHpHSG92=7TxuFDgfY55kp12m1vaQ@mail.gmail.com>

On Tue, Jul 7, 2015 at 8:19 AM, John Fox <jfox at mcmaster.ca> wrote:

> Dear Peter,
>
> You're correct that these examples aren't verb phrases (though the second
> one contains a verb phrase). I don't want to make the discussion even more
> pedantic (moving it in this direction was my fault), but "Paragraph" isn't
> quite right, unless explained, because conventionally a paragraph consists
> of sentences.
>
> How about something like this? "One can use several complete sentences or
> punctuated telegraphic phrases, but only one paragraph (that is, block of
> continuous text with no intervening blank lines). The description should
> end with a full stop (period)."
>
>
Before we start crafting better definitions of the rule, it seems important
to understand what issue we are trying to solve. I don't see any place
where this has been communicated. As I said previously, I usually give them
the benefit of the doubt. However, this requirement is poorly implemented
and we need to know more.

For example, does CRAN need to parse the text and the code failed because
there was no period? It seems plausible that someone could have worded that
requirement in the current form, but it is poorly written (which is
unusual).

If the goal is to improve the quality of the description text, then that is
a more difficult issue to define. and good luck coding your way into a
lucid and effective set of rules. It also seems a bit over the top to me
and a poor choice of where everyone should be spending their time.

What are we trying to fix?

It would likely be helpful to add some examples of good and bad
> descriptions, and to explain how the check actually works.
>
> Best,
>  John
>
> On Tue, 7 Jul 2015 12:20:38 +0200
>  peter dalgaard <pdalgd at gmail.com> wrote:
> > ...except that there is not necessarily a verb either. What we're
> looking for is something like "advertisement style" as in
> >
> > UGLY MUGS 7.95.
> >
> > An invaluable addition to your display cabinet. Comes in an assortment
> of warts and wrinkles, crafted by professional artist Foo Yung.
> >
> > However, I'm drawing blanks when searching for an established term for
> it.
> >
> > Could we perhaps sidestep the issue by requesting a "single descriptive
> paragraph, with punctuation" or thereabouts?
> >
> > ----
> >
> > I'm still puzzled about what threw Federico's example in the first
> place. The actual code is
> >
> >     if(strict && !is.na(val <- db["Description"])
> >        && !grepl("[.!?]['\")]?$", trimws(val)))
> >         out$bad_Description <- TRUE
> >
> > and  I can do this
> >
> > > strict <- TRUE
> > > db <- tools:::.read_description("/tmp/dd")
> > >    if(strict && !is.na(val <- db["Description"])
> > +        && !grepl("[.!?]['\")]?$", trimws(val)))
> > +         out$bad_Description <- TRUE
> > > out
> > Error: object 'out' not found
> >
> > I.e., the complaint should _not_ be triggered. I suppose that something
> like a non-breakable space at the end could confuse trimws(), but beyond
> that I'm out of ideas.
> >
> >
> > On 07 Jul 2015, at 03:28 , John Fox <jfox at mcmaster.ca> wrote:
> >
> > > Dear Peter,
> > >
> > > I think that the grammatical term you're looking for is "verb phrase."
> > >
> > > Best,
> > > John
> > >
> > > On Tue, 7 Jul 2015 00:12:25 +0200
> > > peter dalgaard <pdalgd at gmail.com> wrote:
> > >>
> > >>> On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
> > >>>
> > >>> On 06/07/2015 5:09 PM, Rolf Turner wrote:
> > >>>> On 07/07/15 07:10, William Dunlap wrote:
> > >>>>
> > >>>> [Rolf Turner wrote.]
> > >>>>
> > >>>>>> The CRAN guidelines should be rewritten so that they say what
> they *mean*.
> > >>>>>> If a complete sentence is not actually required --- and it seems
> abundantly clear
> > >>>>>> that it is not --- then guidelines should not say so.  Rather
> they should say,
> > >>>>>> clearly and comprehensibly, what actually *is* required.
> > >>>>>
> > >>>>> This may be true, but also think of the user when you write the
> description.
> > >>>>> If you are scanning a long list of descriptions looking for a
> package to
> > >>>>> use,
> > >>>>> seeing a description that starts with 'A package for' just slows
> you down.
> > >>>>> Seeing a description that includes 'designed to' leaves you
> wondering if the
> > >>>>> implementation is woefully incomplete.  You want to go beyond what
> CRAN
> > >>>>> can test for.
> > >>>>
> > >>>> All very true and sound and wise, but what has this got to do with
> > >>>> complete sentences?  The package checker issues a message saying
> that it
> > >>>> wants a complete sentence when this has nothing to do with what it
> > >>>> *really* wants.
> > >>>
> > >>> That's false.  If you haven't given a complete sentence, you might
> still
> > >>> pass, but if you have, you will pass.  That's not "nothing to do"
> with
> > >>> what it really wants, it's just an imperfect test that fails to
> detect
> > >>> violations of the guidelines.
> > >>>
> > >>> As we've seen, it sometimes also makes mistakes in the other
> direction.
> > >>> I'd say those are more serious.
> > >>>
> > >>> Duncan Murdoch
> > >>>
> > >>
> > >> Ackchewly....
> > >>
> > >> I don't think what we want is what we say that we want. A quick check
> suggests that many/most packages use "headline speech", as in "Provides
> functions for analysis of foo, with special emphasis on bar.", which seems
> perfectly ok.  As others have indicated, prefixing with "This package"
> would be rather useless. However, I'm at a loss as to how to describe what
> it is that we want, much less how to translate it to a dozen other
> languages.
> > >>
> > >> -pd
> > >> --
> > >> Peter Dalgaard, Professor,
> > >> Center for Statistics, Copenhagen Business School
> > >> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > >> Phone: (+45)38153501
> > >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School
> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
>
> ------------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Tue Jul  7 16:53:16 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 7 Jul 2015 14:53:16 +0000
Subject: [R] add a special column to a matrix
In-Reply-To: <CAMqbV1BKQuoOBL42Q+mdroYg-4qYLL977pQ7mZ1uDRQA4jAE4w@mail.gmail.com>
References: <CAMqbV1BKQuoOBL42Q+mdroYg-4qYLL977pQ7mZ1uDRQA4jAE4w@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33C1A@SRVEXCHMBX.precheza.cz>

Hi

No HTML posting please, sometimes the mail is unreadable

It is a work for merge.

mat3 <- merge(mat1, mat2, all=TRUE)

shall do the trick (untested)

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Lida
> Zeighami
> Sent: Tuesday, July 07, 2015 4:43 PM
> To: r-help at r-project.org
> Subject: [R] add a special column to a matrix
>
> Hi there,
>
> I have a two matrices which they have a common column! I want to add
> the a column of second to matrix to the equivalent column of first
> matrix!
> my first matrix is
> head(mat1)
> a       b         c           d
> fg    1:23       dfgv       5
> pt    10:18     tgtgh      1
> wq   15:123    oiljk       6
> fg     9:1323    ass       4
> yr     12:123    kjjlk       5
>
> my second matrix:
> head(mat2)
>        e            q
>   1:23             0
>    10:18           1
>   14:455          2
>    15:123         2
>     9:1323        2
>     12:123        1
>      5:1548        0
>
> mat1 and mat2 have a common column (b and e) but is not in the same
> order, I want to add the q column from mat2 to be added to mat1 my
> output will be:
>
> head(mat)
> a       b         c           d         q
> fg    1:23       dfgv       5        0
> pt    10:18     tgtgh      1        1
> wq   15:123    oiljk       6        2
> fg     9:1323    ass       4        2
> yr     12:123    kjjlk       5        1
>
> would you please let me know how to do it?
>
> Thanks
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From federico.calboli at helsinki.fi  Tue Jul  7 10:53:03 2015
From: federico.calboli at helsinki.fi (Federico Calboli)
Date: Tue, 7 Jul 2015 11:53:03 +0300
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
Message-ID: <FBE9C8ED-16CD-42A7-AE77-59F246D319EA@helsinki.fi>


> On 7 Jul 2015, at 01:12, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
>> On 06 Jul 2015, at 23:19 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>> 
>> On 06/07/2015 5:09 PM, Rolf Turner wrote:
>>> On 07/07/15 07:10, William Dunlap wrote:
>>> 
>>> [Rolf Turner wrote.]
>>> 
>>>>> The CRAN guidelines should be rewritten so that they say what they *mean*.
>>>>> If a complete sentence is not actually required --- and it seems abundantly clear
>>>>> that it is not --- then guidelines should not say so.  Rather they should say,
>>>>> clearly and comprehensibly, what actually *is* required.
>>>> 
>>>> This may be true, but also think of the user when you write the description.
>>>> If you are scanning a long list of descriptions looking for a package to
>>>> use,
>>>> seeing a description that starts with 'A package for' just slows you down.
>>>> Seeing a description that includes 'designed to' leaves you wondering if the
>>>> implementation is woefully incomplete.  You want to go beyond what CRAN
>>>> can test for.
>>> 
>>> All very true and sound and wise, but what has this got to do with 
>>> complete sentences?  The package checker issues a message saying that it 
>>> wants a complete sentence when this has nothing to do with what it 
>>> *really* wants.
>> 
>> That's false.  If you haven't given a complete sentence, you might still
>> pass, but if you have, you will pass.  That's not "nothing to do" with
>> what it really wants, it's just an imperfect test that fails to detect
>> violations of the guidelines.
>> 
>> As we've seen, it sometimes also makes mistakes in the other direction.
>> I'd say those are more serious.
>> 
>> Duncan Murdoch
>> 
> 
> Ackchewly....
> 
> I don't think what we want is what we say that we want. A quick check suggests that many/most packages use "headline speech", as in "Provides functions for analysis of foo, with special emphasis on bar.", which seems perfectly ok.  As others have indicated, prefixing with "This package" would be rather useless. However, I'm at a loss as to how to describe what it is that we want, much less how to translate it to a dozen other languages. 

You are hitting the nail on the head ? R asks for a *description* without defining any grammatical rule for it aside from the nebulous ?complete sentence? (nebulous because of how it is enforced) and ?this package':

"The mandatory ?Description? field should give a comprehensive description of what the package does. One can use several (complete) sentences, but only one paragraph. It should be intelligible to all the intended readership (e.g. for a CRAN package to all CRAN users). It is good practice not to start with the package name, ?This package? or similar."

I am puzzled by the idea that people that deal with stats, maths and computers should define what is a grammatically acceptable description, as opposed to a description.  If I describe my package poorly it might not be used as much, and thus it might represent a wasted effort for *me*.  Incidentally, not being able to use ?pkgname' or ?this package? decreases the chances of successfully deploying a subject-verb-object sentence.

BW

F





> 
> -pd
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 


--
Federico Calboli
Ecological Genetics Research Unit
Department of Biosciences
PO Box 65 (Biocenter 3, Viikinkaari 1)
FIN-00014 University of Helsinki
Finland

federico.calboli at helsinki.fi


From dheeraj2444 at gmail.com  Tue Jul  7 11:30:21 2015
From: dheeraj2444 at gmail.com (Dheeraj Singh)
Date: Tue, 7 Jul 2015 15:00:21 +0530
Subject: [R] Building Intercative Dashboard using shiny in R
Message-ID: <CAM=ndZ-9gDDzouembeHP1xr=9CUaSh9AYowR1K8qtNFEx2WkLQ@mail.gmail.com>

Hi All,

I have just begun working on shiny package in R and have been building a
dashboard using tutorials and documentation abvailable online. I  am stuck
in the middle.

It would be of great help if you could help me out with my code. I want to
build a dashboard that would take 'date range' and 'variable' to be
displayed as user input and then display the plot. It takes the data from
the csv file.

Currently, the issue is when I change the variable the plot gets changed
but when I change date range, nothing happens to the plot. Please help me
out with this. I have posted the question in stackoverflow as well. Here's
the link: question
<http://stackoverflow.com/questions/31246498/plots-dont-change-with-changing-dates-in-shiny-dashborad-in-r>


Here is the source code:

*ui.R*

library("shiny")
library("shinydashboard")
dashboardPage(

dashboardHeader(title = "Dashboard"),

dashboardSidebar(img(src="jpeg.jpg", length = 75, width = 230),
               dateRangeInput("daterange", "Select Date range:",
                              start = "2014-06-01",
                              end = "2015-06-30"),
               selectInput("var", "Choose a variable to display",
                           choices = c('Total_Orders' = 'Total_Orders',
                                       'Delivered_orders' = 'Delivered_orders',
                                       'Cancelled_orders' = 'Cancelled_orders',
                                       'NEW_USERS_COUNT' = 'NEW_USERS_COUNT'),
                           selected = "Total_Orders")

               ),
dashboardBody(

fluidRow(
  box(title = "plot", status = "primary", solidHeader = TRUE,
      collapsible = TRUE, plotOutput("chart1", height = 250, width = 500))
  )
  ))


*server.R:*

library("shiny")
library("data.table")
file <-     read.csv('filepath.csv', header = TRUE)
setnames(file, old = c('X_id'), new = c('date'))
file$date <- as.Date(strptime(file$date, "%Y-%m-%d" ))

shinyServer(function(input, output)
{output$chart1 <- renderPlot({

               start_date <- input$daterange[1]
               end_date <- input$daterange[2]

               subset(file, date >= start_date & date   <= start_date)

               point <- switch(input$var,
               'Total_Orders' = file$Total_Orders,
               'Cancelled_orders' = file$Cancelled_orders,
               'Delivered_orders' = file$Delivered_ordes,
               'NEW_USERS_COUNT' = file$NEW_USERS_COUNT
               )
               plot(file$date, point, type = "b")
 }) })





?*Sample Data:*

Date        Total_Orders   Delivered_orders   Cancelled_orders
NEW_USERS_COUNT 2015-04-30     23             12               2
           212015-05-01     43             21               12
        322015-05-02     32             13               10
     302015-05-03     43             32               7
  142015-05-04     43             22               3
212015-05-05     32             21               1                  22


?Thanks in advance :)

Thanks!?

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Jul  7 17:19:52 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 7 Jul 2015 15:19:52 +0000
Subject: [R] Leave one out procedure - R
In-Reply-To: <61AF2D1D50D.00000BB3jrkrideau@inbox.com>
References: <CADeBCTn8H6DhcbiEEq=E5zbBrCKaJsjm6vZ=oFqbDvB8eqVBMQ@mail.gmail.com>
	<61AF2D1D50D.00000BB3jrkrideau@inbox.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6A086D@mb02.ads.tamu.edu>

As John suggests, more information and some sample data would help. I haven't used loocv, but here is a simple example of leave one out cross validation with simple linear regression. This does not eliminate the loop, but uses lapply() which makes specifying the loop simpler. The results of the 40 regressions are stored in a list called results and results.coef contains the intercept and slope for each regression in a matrix.

> set.seed(42)
> x <- rnorm(40, 10, 2)
> y <- x + rnorm(40, 0, 2)
> results <- lapply(1:40, function(i) lm(y[-i] ~ x[-i]))
> results.coef <- t(sapply(results, coef))
> summary(results.coef)
  (Intercept)          x[-i]       
 Min.   :-0.1477   Min.   :0.8408  
 1st Qu.: 0.2032   1st Qu.:0.9830  
 Median : 0.2613   Median :0.9883  
 Mean   : 0.2933   Mean   :0.9866  
 3rd Qu.: 0.3240   3rd Qu.:0.9959  
 Max.   : 1.9160   Max.   :1.0245  

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John Kane
Sent: Monday, July 6, 2015 6:53 PM
To: Vyshnnavi Parthasarathy; r-help at r-project.org
Subject: Re: [R] Leave one out procedure - R


Hi Vyshnnavi,

I'd suggest having a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html for some hints on how to ask questions on the R-help List.

At the moment I don't think you have supplied enough information, code, and data for people to easily answer your questions.

BTW if you are supplying sample data the best way is to use the dput() function. It provides an exact copy of the data set that you are working with.

Sorry not to be of any substantive help.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: vyshnnaviammu at gmail.com
> Sent: Mon, 6 Jul 2015 08:56:52 -0700
> To: r-help at r-project.org
> Subject: [R] Leave one out procedure - R
> 
> Hello,
> I am still in the process of familiarizing myself with R, pardon me if
> this
> is basic. I want to run a leave one out procedure for a 40 member
> dataset.
> At the moment I am doing it via a simple for loop. I wanted to know if
> there is a superior way to do it. I read about the loocv command here -
> http://artax.karlin.mff.cuni.cz/r-help/library/DMwR/html/loocv.html.
> Since
> I need to save the output generated in each iteration, I am not very sure
> how exactly to implement the same using the loocv command. If you could
> give me an insight into how to do it/ suggest quicker ways to do leave
> one
> out cross validation, it would be really helpful!
> 
> Thanks,
> Vyshnnavi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
GET FREE 5GB EMAIL - Check out spam free email with many cool features!
Visit http://www.inbox.com/email to find out more!

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From JSorkin at grecc.umaryland.edu  Tue Jul  7 17:40:12 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 07 Jul 2015 11:40:12 -0400
Subject: [R] degrees of freedom (and hence p values) from lme and lmer don't
 agree . . . Why?????????
Message-ID: <559BBA9C020000CB001320D6@smtp.medicine.umaryland.edu>

I am trying to fit data from 23 subjects using random effects
regression, and am comparing the results of lme and lmer. The point
estimates and the SEs are the same in both models, however the degrees
of freedom are widely different. lme reports 88 DF, lmer approximately
22. Can someone help me understand why the DFs are not the same? I have
23 subjects, each of whom is studied in up to five different
experimental conditions (i.e. Amp). For each condition multiple
measurements are made for each subject (i.e. X).
Thank you,
John
 
 

# lme: Random intercept, random slope.
cat("********This analysis has 88 degrees of freedom\n")
fit0X.new <- groupedData(X~Amp|SS,data=data,order.groups=FALSE)
xx <- lme(fit0X.new,random=~1+Amp)
summary(xx)
cat("\n\n")
 
 
# lmer: Random intercept, random slope.
cat("*********This analysis has ~22 degrees of freedom\n")
fit0X <- lmer(X~Amp+(1+Amp|SS),data=data)
print(summary(fit0X))
fit0XSum<-summary(fit0X)$coefficients
 
 
 
********This analysis has 88 degrees of freedom
Linear mixed-effects model fit by REML
 Data: fit0X.new 
       AIC      BIC    logLik
  331.7688 347.9717 -159.8844
Random effects:
 Formula: ~1 + Amp | SS
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 1.3515911 (Intr)
Amp         2.5619953 -0.366
Residual    0.6139429       
Fixed effects: X ~ Amp 
               Value Std.Error DF   t-value p-value
(Intercept) 1.718376 0.3609133 88  4.761188       0
Amp         6.890429 0.5978236 88 11.525856       0
 Correlation: 
    (Intr)
Amp -0.526
Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.2177007 -0.5770388 -0.1249565  0.5247444  4.1150164 
Number of Observations: 112
Number of Groups: 23 

*********This analysis has ~22 degrees of freedom
Linear mixed model fit by REML t-tests use Satterthwaite approximations
to degrees of freedom [merModLmerTest]
Formula: X ~ Amp + (1 + Amp | SS)
   Data: data
REML criterion at convergence: 319.8
Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2177 -0.5770 -0.1250  0.5247  4.1150 
Random effects:
 Groups   Name        Variance Std.Dev. Corr 
 SS       (Intercept) 1.8268   1.3516        
          Amp         6.5638   2.5620   -0.37
 Residual             0.3769   0.6139        
Number of obs: 112, groups:  SS, 23
Fixed effects:
            Estimate Std. Error      df t value Pr(>|t|)    
(Intercept)   1.7184     0.3609 21.1150   4.761 0.000104 ***
Amp           6.8904     0.5978 22.0460  11.526 8.37e-11 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
Correlation of Fixed Effects:
    (Intr)
Amp -0.526
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and
Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of
the intended recipient(s) and may contain confidential and privileged
information. Any unauthorized use, disclosure or distribution is
prohibited. If you are not the intended recipient, please contact the
sender by reply email and destroy all copies of the original message. 

From jfox at mcmaster.ca  Tue Jul  7 17:51:15 2015
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 7 Jul 2015 11:51:15 -0400
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CAJ9CoWm0KB6d9VPVkf9SUWHpHSG92=7TxuFDgfY55kp12m1vaQ@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>	<55969185.5000604@auckland.ac.nz>	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>	<55970BF9.400@auckland.ac.nz>	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>	<559AEEA2.2020805@auckland.ac.nz>
	<559AF0F1.9010902@gmail.com>	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>	<web-564884742@cgpsrv2.cis.mcmaster.ca>	<915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>	<web-564904636@cgpsrv2.cis.mcmaster.ca>
	<CAJ9CoWm0KB6d9VPVkf9SUWHpHSG92=7TxuFDgfY55kp12m1vaQ@mail.gmail.com>
Message-ID: <001901d0b8cc$c373a6c0$4a5af440$@mcmaster.ca>

Dear Max,

I think that the object is to describe clearly what CRAN wants in the
description field so that package authors don't write description fields
that are unacceptable to CRAN. Clear criteria would save both package
authors' and CRAN maintainers' time. Although a mechanical check can find
some problems with descriptions, I doubt whether it's possible to write a
mechanical check that will fully implement what CRAN wants.

John

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Max Kuhn
> Sent: July-07-15 10:53 AM
> To: John Fox
> Cc: R Help; Federico Calboli; peter dalgaard
> Subject: Re: [R] what constitutes a 'complete sentence'?
> 
> On Tue, Jul 7, 2015 at 8:19 AM, John Fox <jfox at mcmaster.ca> wrote:
> 
> > Dear Peter,
> >
> > You're correct that these examples aren't verb phrases (though the
> second
> > one contains a verb phrase). I don't want to make the discussion even
> more
> > pedantic (moving it in this direction was my fault), but "Paragraph"
> isn't
> > quite right, unless explained, because conventionally a paragraph
> consists
> > of sentences.
> >
> > How about something like this? "One can use several complete sentences
> or
> > punctuated telegraphic phrases, but only one paragraph (that is, block
> of
> > continuous text with no intervening blank lines). The description
> should
> > end with a full stop (period)."
> >
> >
> Before we start crafting better definitions of the rule, it seems
> important
> to understand what issue we are trying to solve. I don't see any place
> where this has been communicated. As I said previously, I usually give
> them
> the benefit of the doubt. However, this requirement is poorly
> implemented
> and we need to know more.
> 
> For example, does CRAN need to parse the text and the code failed
> because
> there was no period? It seems plausible that someone could have worded
> that
> requirement in the current form, but it is poorly written (which is
> unusual).
> 
> If the goal is to improve the quality of the description text, then that
> is
> a more difficult issue to define. and good luck coding your way into a
> lucid and effective set of rules. It also seems a bit over the top to me
> and a poor choice of where everyone should be spending their time.
> 
> What are we trying to fix?
> 
> It would likely be helpful to add some examples of good and bad
> > descriptions, and to explain how the check actually works.
> >
> > Best,
> >  John
> >
> > On Tue, 7 Jul 2015 12:20:38 +0200
> >  peter dalgaard <pdalgd at gmail.com> wrote:
> > > ...except that there is not necessarily a verb either. What we're
> > looking for is something like "advertisement style" as in
> > >
> > > UGLY MUGS 7.95.
> > >
> > > An invaluable addition to your display cabinet. Comes in an
> assortment
> > of warts and wrinkles, crafted by professional artist Foo Yung.
> > >
> > > However, I'm drawing blanks when searching for an established term
> for
> > it.
> > >
> > > Could we perhaps sidestep the issue by requesting a "single
> descriptive
> > paragraph, with punctuation" or thereabouts?
> > >
> > > ----
> > >
> > > I'm still puzzled about what threw Federico's example in the first
> > place. The actual code is
> > >
> > >     if(strict && !is.na(val <- db["Description"])
> > >        && !grepl("[.!?]['\")]?$", trimws(val)))
> > >         out$bad_Description <- TRUE
> > >
> > > and  I can do this
> > >
> > > > strict <- TRUE
> > > > db <- tools:::.read_description("/tmp/dd")
> > > >    if(strict && !is.na(val <- db["Description"])
> > > +        && !grepl("[.!?]['\")]?$", trimws(val)))
> > > +         out$bad_Description <- TRUE
> > > > out
> > > Error: object 'out' not found
> > >
> > > I.e., the complaint should _not_ be triggered. I suppose that
> something
> > like a non-breakable space at the end could confuse trimws(), but
> beyond
> > that I'm out of ideas.
> > >
> > >
> > > On 07 Jul 2015, at 03:28 , John Fox <jfox at mcmaster.ca> wrote:
> > >
> > > > Dear Peter,
> > > >
> > > > I think that the grammatical term you're looking for is "verb
> phrase."
> > > >
> > > > Best,
> > > > John
> > > >
> > > > On Tue, 7 Jul 2015 00:12:25 +0200
> > > > peter dalgaard <pdalgd at gmail.com> wrote:
> > > >>
> > > >>> On 06 Jul 2015, at 23:19 , Duncan Murdoch
> <murdoch.duncan at gmail.com>
> > wrote:
> > > >>>
> > > >>> On 06/07/2015 5:09 PM, Rolf Turner wrote:
> > > >>>> On 07/07/15 07:10, William Dunlap wrote:
> > > >>>>
> > > >>>> [Rolf Turner wrote.]
> > > >>>>
> > > >>>>>> The CRAN guidelines should be rewritten so that they say what
> > they *mean*.
> > > >>>>>> If a complete sentence is not actually required --- and it
> seems
> > abundantly clear
> > > >>>>>> that it is not --- then guidelines should not say so.  Rather
> > they should say,
> > > >>>>>> clearly and comprehensibly, what actually *is* required.
> > > >>>>>
> > > >>>>> This may be true, but also think of the user when you write
> the
> > description.
> > > >>>>> If you are scanning a long list of descriptions looking for a
> > package to
> > > >>>>> use,
> > > >>>>> seeing a description that starts with 'A package for' just
> slows
> > you down.
> > > >>>>> Seeing a description that includes 'designed to' leaves you
> > wondering if the
> > > >>>>> implementation is woefully incomplete.  You want to go beyond
> what
> > CRAN
> > > >>>>> can test for.
> > > >>>>
> > > >>>> All very true and sound and wise, but what has this got to do
> with
> > > >>>> complete sentences?  The package checker issues a message
> saying
> > that it
> > > >>>> wants a complete sentence when this has nothing to do with what
> it
> > > >>>> *really* wants.
> > > >>>
> > > >>> That's false.  If you haven't given a complete sentence, you
> might
> > still
> > > >>> pass, but if you have, you will pass.  That's not "nothing to
> do"
> > with
> > > >>> what it really wants, it's just an imperfect test that fails to
> > detect
> > > >>> violations of the guidelines.
> > > >>>
> > > >>> As we've seen, it sometimes also makes mistakes in the other
> > direction.
> > > >>> I'd say those are more serious.
> > > >>>
> > > >>> Duncan Murdoch
> > > >>>
> > > >>
> > > >> Ackchewly....
> > > >>
> > > >> I don't think what we want is what we say that we want. A quick
> check
> > suggests that many/most packages use "headline speech", as in
> "Provides
> > functions for analysis of foo, with special emphasis on bar.", which
> seems
> > perfectly ok.  As others have indicated, prefixing with "This package"
> > would be rather useless. However, I'm at a loss as to how to describe
> what
> > it is that we want, much less how to translate it to a dozen other
> > languages.
> > > >>
> > > >> -pd
> > > >> --
> > > >> Peter Dalgaard, Professor,
> > > >> Center for Statistics, Copenhagen Business School
> > > >> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > > >> Phone: (+45)38153501
> > > >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > > >>
> > > >> ______________________________________________
> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > >> and provide commented, minimal, self-contained, reproducible
> code.
> > > >
> > > >
> > > >
> > >
> > > --
> > > Peter Dalgaard, Professor,
> > > Center for Statistics, Copenhagen Business School
> > > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> > > Phone: (+45)38153501
> > > Office: A 4.23
> > > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> >
> > ------------------------------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario, Canada
> > http://socserv.mcmaster.ca/jfox/
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From bgunter.4567 at gmail.com  Tue Jul  7 17:57:03 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 7 Jul 2015 08:57:03 -0700
Subject: [R] degrees of freedom (and hence p values) from lme and lmer
 don't agree . . . Why?????????
In-Reply-To: <559BBA9C020000CB001320D6@smtp.medicine.umaryland.edu>
References: <559BBA9C020000CB001320D6@smtp.medicine.umaryland.edu>
Message-ID: <CAGxFJbSVTArz81_vhA6q4O1HVb+r35mk7y9nbJj6fNiejKrFWw@mail.gmail.com>

This is a **highly technical** statistical issue, not an R-Help topic.
I strongly suggest that you post to the R-sig-mixed-models list
instead.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Jul 7, 2015 at 8:40 AM, John Sorkin <JSorkin at grecc.umaryland.edu> wrote:
> I am trying to fit data from 23 subjects using random effects
> regression, and am comparing the results of lme and lmer. The point
> estimates and the SEs are the same in both models, however the degrees
> of freedom are widely different. lme reports 88 DF, lmer approximately
> 22. Can someone help me understand why the DFs are not the same? I have
> 23 subjects, each of whom is studied in up to five different
> experimental conditions (i.e. Amp). For each condition multiple
> measurements are made for each subject (i.e. X).
> Thank you,
> John
>
>
>
> # lme: Random intercept, random slope.
> cat("********This analysis has 88 degrees of freedom\n")
> fit0X.new <- groupedData(X~Amp|SS,data=data,order.groups=FALSE)
> xx <- lme(fit0X.new,random=~1+Amp)
> summary(xx)
> cat("\n\n")
>
>
> # lmer: Random intercept, random slope.
> cat("*********This analysis has ~22 degrees of freedom\n")
> fit0X <- lmer(X~Amp+(1+Amp|SS),data=data)
> print(summary(fit0X))
> fit0XSum<-summary(fit0X)$coefficients
>
>
>
> ********This analysis has 88 degrees of freedom
> Linear mixed-effects model fit by REML
>  Data: fit0X.new
>        AIC      BIC    logLik
>   331.7688 347.9717 -159.8844
> Random effects:
>  Formula: ~1 + Amp | SS
>  Structure: General positive-definite, Log-Cholesky parametrization
>             StdDev    Corr
> (Intercept) 1.3515911 (Intr)
> Amp         2.5619953 -0.366
> Residual    0.6139429
> Fixed effects: X ~ Amp
>                Value Std.Error DF   t-value p-value
> (Intercept) 1.718376 0.3609133 88  4.761188       0
> Amp         6.890429 0.5978236 88 11.525856       0
>  Correlation:
>     (Intr)
> Amp -0.526
> Standardized Within-Group Residuals:
>        Min         Q1        Med         Q3        Max
> -2.2177007 -0.5770388 -0.1249565  0.5247444  4.1150164
> Number of Observations: 112
> Number of Groups: 23
>
> *********This analysis has ~22 degrees of freedom
> Linear mixed model fit by REML t-tests use Satterthwaite approximations
> to degrees of freedom [merModLmerTest]
> Formula: X ~ Amp + (1 + Amp | SS)
>    Data: data
> REML criterion at convergence: 319.8
> Scaled residuals:
>     Min      1Q  Median      3Q     Max
> -2.2177 -0.5770 -0.1250  0.5247  4.1150
> Random effects:
>  Groups   Name        Variance Std.Dev. Corr
>  SS       (Intercept) 1.8268   1.3516
>           Amp         6.5638   2.5620   -0.37
>  Residual             0.3769   0.6139
> Number of obs: 112, groups:  SS, 23
> Fixed effects:
>             Estimate Std. Error      df t value Pr(>|t|)
> (Intercept)   1.7184     0.3609 21.1150   4.761 0.000104 ***
> Amp           6.8904     0.5978 22.0460  11.526 8.37e-11 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> Correlation of Fixed Effects:
>     (Intr)
> Amp -0.526
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:12}}


From vyshnnaviammu at gmail.com  Tue Jul  7 19:18:42 2015
From: vyshnnaviammu at gmail.com (Vyshnnavi Parthasarathy)
Date: Tue, 7 Jul 2015 10:18:42 -0700
Subject: [R] Leave one out procedure - R
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6A086D@mb02.ads.tamu.edu>
References: <CADeBCTn8H6DhcbiEEq=E5zbBrCKaJsjm6vZ=oFqbDvB8eqVBMQ@mail.gmail.com>
	<61AF2D1D50D.00000BB3jrkrideau@inbox.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6A086D@mb02.ads.tamu.edu>
Message-ID: <CADeBCTn+0bN4e9Gzz=ZQeauYwQCWiNZp7NYXUUXc2318npafbQ@mail.gmail.com>

Hello,
Thanks John and David. I ll look into your suggestions and I ll ensure I
post more details the next time.

Thanks,
Vyshnnavi
On Jul 7, 2015 8:19 AM, "David L Carlson" <dcarlson at tamu.edu> wrote:

> As John suggests, more information and some sample data would help. I
> haven't used loocv, but here is a simple example of leave one out cross
> validation with simple linear regression. This does not eliminate the loop,
> but uses lapply() which makes specifying the loop simpler. The results of
> the 40 regressions are stored in a list called results and results.coef
> contains the intercept and slope for each regression in a matrix.
>
> > set.seed(42)
> > x <- rnorm(40, 10, 2)
> > y <- x + rnorm(40, 0, 2)
> > results <- lapply(1:40, function(i) lm(y[-i] ~ x[-i]))
> > results.coef <- t(sapply(results, coef))
> > summary(results.coef)
>   (Intercept)          x[-i]
>  Min.   :-0.1477   Min.   :0.8408
>  1st Qu.: 0.2032   1st Qu.:0.9830
>  Median : 0.2613   Median :0.9883
>  Mean   : 0.2933   Mean   :0.9866
>  3rd Qu.: 0.3240   3rd Qu.:0.9959
>  Max.   : 1.9160   Max.   :1.0245
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John Kane
> Sent: Monday, July 6, 2015 6:53 PM
> To: Vyshnnavi Parthasarathy; r-help at r-project.org
> Subject: Re: [R] Leave one out procedure - R
>
>
> Hi Vyshnnavi,
>
> I'd suggest having a look at
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> and http://adv-r.had.co.nz/Reproducibility.html for some hints on how to
> ask questions on the R-help List.
>
> At the moment I don't think you have supplied enough information, code,
> and data for people to easily answer your questions.
>
> BTW if you are supplying sample data the best way is to use the dput()
> function. It provides an exact copy of the data set that you are working
> with.
>
> Sorry not to be of any substantive help.
>
>
>
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: vyshnnaviammu at gmail.com
> > Sent: Mon, 6 Jul 2015 08:56:52 -0700
> > To: r-help at r-project.org
> > Subject: [R] Leave one out procedure - R
> >
> > Hello,
> > I am still in the process of familiarizing myself with R, pardon me if
> > this
> > is basic. I want to run a leave one out procedure for a 40 member
> > dataset.
> > At the moment I am doing it via a simple for loop. I wanted to know if
> > there is a superior way to do it. I read about the loocv command here -
> > http://artax.karlin.mff.cuni.cz/r-help/library/DMwR/html/loocv.html.
> > Since
> > I need to save the output generated in each iteration, I am not very sure
> > how exactly to implement the same using the loocv command. If you could
> > give me an insight into how to do it/ suggest quicker ways to do leave
> > one
> > out cross validation, it would be really helpful!
> >
> > Thanks,
> > Vyshnnavi
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> GET FREE 5GB EMAIL - Check out spam free email with many cool features!
> Visit http://www.inbox.com/email to find out more!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From arne.henningsen at gmail.com  Tue Jul  7 21:55:08 2015
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Tue, 7 Jul 2015 21:55:08 +0200
Subject: [R] NaN produced from log() with positive input
In-Reply-To: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>
References: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>
Message-ID: <CAMTWbJhOfaSvmvbKfeg4r8Vnytwy0h86ACJZNe++FBExFinRyA@mail.gmail.com>

Dear Maram

Please do NOT post your message twice!

The warning messages occur each time, when maxLik() tries to calculate
the logLik value for theta[1] <= 0, theta[1] + theta[2] <= 0, theta[3]
<= 0 or something similar. According to the log-likelihood function,
it seems that the parameters theta[1], theta[2], and theta[3] must be
strictly positive. I suggest to re-parameterise your model so that the
estimated parameters can take any values between minus infinity and
infinity, e.g. by theta[1] <- exp( param[1] ); theta[2] <- exp(
param[2] ); theta[3] <- exp( param[3] ) so that your estimated
parameter vector 'param' consists of log( theta[1] ), log( theta[2] ),
and log( theta[3] ). After the estimation, you can obtain the
estimated values of the thetas by exp( param[1] ), exp( param[2] ),
and exp( param[3] ) .

Best regards,
Arne



2015-07-06 2:29 GMT+02:00 Maram Salem <marammagdysalem at gmx.com>:
> Dear All
> I'm trying to find the maximum likelihood estimator  of a certain distribution based on the newton raphson method using maxLik package. I wrote the log-likelihood , gradient, and hessian functionsusing the following code.
>
> #Step 1: Creating the theta vector
>  theta <-vector(mode = "numeric", length = 3)
> # Step 2: Setting the values of r and n
> r<- 17
> n <-30
>  # Step 3: Creating the T vector
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> # Step 4: Creating the C vector
> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> # The  loglik. func.
> loglik <- function(param) {
>  theta[1]<- param[1]
>  theta[2]<- param[2]
>  theta[3]<- param[3]
>  l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+ (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+ (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
> return(l)
>  }
> # Step 5: Creating the gradient vector and calculating its inputs
> U <- vector(mode="numeric",length=3)
> gradlik<-function(param = theta,n, T,C)
>  {
> U <- vector(mode="numeric",length=3)
> theta[1] <- param[1]
> theta[2] <- param[2]
> theta[3] <- param[3]
> r<- 17
> n <-30
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>  U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+( -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+ (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+ (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
> return(U)
> }
> # Step 6: Creating the G (Hessian) matrix and Calculating its inputs
> hesslik<-function(param=theta,n,T,C)
> {
> theta[1] <- param[1]
> theta[2] <- param[2]
> theta[3] <- param[3]
> r<- 17
> n <-30
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> G<- matrix(nrow=3,ncol=3)
> G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+ (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+ (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+ (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[2,1]<-G[1,2]
> G[1,3]<-(n/theta[1])+(-1)*sum( (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> G[3,1]<-G[1,3]
> G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+ (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> G[3,2]<-G[2,3]
> G[3,3]<-((-1*r)/(theta[3])^2)
> return(G)
> }
> mle<-maxLik(loglik, grad = gradlik, hess = hesslik, start=c(40,50,2))
> There were 50 or more warnings (use warnings() to see the first 50)
>
> warnings ()
> Warning messages:
> 1: In log(theta[3]) : NaNs produced
> 2: In log(theta[1] + theta[2]) : NaNs produced
> 3: In log(theta[1]) : NaNs produced
> 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs produced
>  and so on .......
>
> Although when I evaluate, for example, log(theta[3])  it gives me a number. and the same applies for the other warnings.
>
> Then when I used summary (mle), I got
>
>
> Maximum Likelihood estimation
> Newton-Raphson maximisation, 7 iterations
> Return code 1: gradient close to zero
> Log-Likelihood: -55.89012
> 3  free parameters
> Estimates:
>      Estimate Std. error t value Pr(> t)
> [1,]   11.132        Inf       0       1
> [2,]   47.618        Inf       0       1
> [3,]    1.293        Inf       0       1
> --------------------------------------------
>
>
> Where the estimates are far away from the starting values and they have infinite standard errors. I think there is a problem with my gradlik or hesslik functions, but I can't figure it out.
> Any help?
> Thank you in advance.
>
> Maram
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Arne Henningsen
http://www.arne-henningsen.name


From marammagdysalem at gmx.com  Tue Jul  7 22:29:56 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Tue, 7 Jul 2015 22:29:56 +0200
Subject: [R] NaN produced from log() with positive input
In-Reply-To: <CAMTWbJhOfaSvmvbKfeg4r8Vnytwy0h86ACJZNe++FBExFinRyA@mail.gmail.com>
References: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>
	<CAMTWbJhOfaSvmvbKfeg4r8Vnytwy0h86ACJZNe++FBExFinRyA@mail.gmail.com>
Message-ID: <36A078A0-ADD6-4588-895F-BCBDFB8C6FE8@gmx.com>

Dear Arne,

Sorry for posting my mail twice and thanks a lot for your help.

Best regards,
Maram 

Sent from my iPhone

> On Jul 7, 2015, at 9:55 PM, Arne Henningsen <arne.henningsen at gmail.com> wrote:
> 
> Dear Maram
> 
> Please do NOT post your message twice!
> 
> The warning messages occur each time, when maxLik() tries to calculate
> the logLik value for theta[1] <= 0, theta[1] + theta[2] <= 0, theta[3]
> <= 0 or something similar. According to the log-likelihood function,
> it seems that the parameters theta[1], theta[2], and theta[3] must be
> strictly positive. I suggest to re-parameterise your model so that the
> estimated parameters can take any values between minus infinity and
> infinity, e.g. by theta[1] <- exp( param[1] ); theta[2] <- exp(
> param[2] ); theta[3] <- exp( param[3] ) so that your estimated
> parameter vector 'param' consists of log( theta[1] ), log( theta[2] ),
> and log( theta[3] ). After the estimation, you can obtain the
> estimated values of the thetas by exp( param[1] ), exp( param[2] ),
> and exp( param[3] ) .
> 
> Best regards,
> Arne
> 
> 
> 
> 2015-07-06 2:29 GMT+02:00 Maram Salem <marammagdysalem at gmx.com>:
>> Dear All
>> I'm trying to find the maximum likelihood estimator  of a certain distribution based on the newton raphson method using maxLik package. I wrote the log-likelihood , gradient, and hessian functionsusing the following code.
>> 
>> #Step 1: Creating the theta vector
>> theta <-vector(mode = "numeric", length = 3)
>> # Step 2: Setting the values of r and n
>> r<- 17
>> n <-30
>> # Step 3: Creating the T vector
>> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
>> # Step 4: Creating the C vector
>> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>> # The  loglik. func.
>> loglik <- function(param) {
>> theta[1]<- param[1]
>> theta[2]<- param[2]
>> theta[3]<- param[3]
>> l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+ (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+ (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
>> return(l)
>> }
>> # Step 5: Creating the gradient vector and calculating its inputs
>> U <- vector(mode="numeric",length=3)
>> gradlik<-function(param = theta,n, T,C)
>> {
>> U <- vector(mode="numeric",length=3)
>> theta[1] <- param[1]
>> theta[2] <- param[2]
>> theta[3] <- param[3]
>> r<- 17
>> n <-30
>> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
>> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>> U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+( -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+ (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+ (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+ (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
>> return(U)
>> }
>> # Step 6: Creating the G (Hessian) matrix and Calculating its inputs
>> hesslik<-function(param=theta,n,T,C)
>> {
>> theta[1] <- param[1]
>> theta[2] <- param[2]
>> theta[3] <- param[3]
>> r<- 17
>> n <-30
>> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
>> C<- c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>> G<- matrix(nrow=3,ncol=3)
>> G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+ (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
>> G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+ (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+ (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
>> G[2,1]<-G[1,2]
>> G[1,3]<-(n/theta[1])+(-1)*sum( (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> G[3,1]<-G[1,3]
>> G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+ (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+( theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
>> G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> G[3,2]<-G[2,3]
>> G[3,3]<-((-1*r)/(theta[3])^2)
>> return(G)
>> }
>> mle<-maxLik(loglik, grad = gradlik, hess = hesslik, start=c(40,50,2))
>> There were 50 or more warnings (use warnings() to see the first 50)
>> 
>> warnings ()
>> Warning messages:
>> 1: In log(theta[3]) : NaNs produced
>> 2: In log(theta[1] + theta[2]) : NaNs produced
>> 3: In log(theta[1]) : NaNs produced
>> 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs produced
>> and so on .......
>> 
>> Although when I evaluate, for example, log(theta[3])  it gives me a number. and the same applies for the other warnings.
>> 
>> Then when I used summary (mle), I got
>> 
>> 
>> Maximum Likelihood estimation
>> Newton-Raphson maximisation, 7 iterations
>> Return code 1: gradient close to zero
>> Log-Likelihood: -55.89012
>> 3  free parameters
>> Estimates:
>>     Estimate Std. error t value Pr(> t)
>> [1,]   11.132        Inf       0       1
>> [2,]   47.618        Inf       0       1
>> [3,]    1.293        Inf       0       1
>> --------------------------------------------
>> 
>> 
>> Where the estimates are far away from the starting values and they have infinite standard errors. I think there is a problem with my gradlik or hesslik functions, but I can't figure it out.
>> Any help?
>> Thank you in advance.
>> 
>> Maram
>> 
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> -- 
> Arne Henningsen
> http://www.arne-henningsen.name


From karl.schilling at uni-bonn.de  Tue Jul  7 20:00:55 2015
From: karl.schilling at uni-bonn.de (Karl Schilling)
Date: Tue, 7 Jul 2015 20:00:55 +0200
Subject: [R] problem with using seq() or rep() inside a for loop
Message-ID: <559C13D7.4030303@uni-bonn.de>

Dear All:

I want to use seq() inside a for-loop and use the looping counter i as 
the "by" argument in seq(). Here is some exemplary data and code:

# set up some data
distances <- c(0, NA, NA, NA, NA, NA,
                5, 0, NA, NA, NA, NA,
                5, 2, 0, NA, NA, NA,
                18, 5, 5, 0, NA, NA,
                25, 10, 8, 1, 0, NA,
                41, 20, 18, 5, 2, 0)

MD_dist <- matrix(distances, ncol = 6)

MEAN <- numeric(nrow(MD_dist) - 1) # just to set up a vector

# loop to add (subsets of) off-diagonal diagonal values
for(i in 1: ncol(MD_dist) - 1){
diagonal <- as.vector(MD_dist[row(MD_dist) == (col(MD_dist) - i)])
# the following line extracts every i-th element from "diagonal"
diagonal.2 <- diagonal[seq(1, to = length(diagonal), by = i)]
MEAN[i] <- mean(diagonal.2)
}

However, I keep getting the following error message:

Error in seq.default(1, to = length(diagonal), by = i) :
   invalid (to - from)/by in seq(.)

May I add that if I run the loop "by hand" - i.e. by just setting i = 1, 
2... etc and then running the core without the for {...} , everything 
works fine.

Further, when I use rep(..) instead of seq(...) to extract the desired 
values from "diagonal", I have the very same problem - it works outside 
a for loop, but fails inside.

I am using R 3.2.1. (x64) under Win 7 Professional on a 64 bit machine.

Any suggestion would be appreciated.

Thank you so much.

Karl

-- 
Karl Schilling


From jun.shen.ut at gmail.com  Tue Jul  7 22:53:07 2015
From: jun.shen.ut at gmail.com (Jun Shen)
Date: Tue, 7 Jul 2015 16:53:07 -0400
Subject: [R] How to assign value to a variable dynamically constructed
Message-ID: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>

Dear list,

Let's say we have a variable (id), whose name is dynamically constructed.
This variable represents a vector or data frame with many elements. Now I
want to specifically assign a value to one of the elements. I couldn't get
it right.

test <- 'id' # "id" is dynamically constructed through paste()

id <- 1:4

# I can get the element by doing

get(test)[2]

# Now I want to assign a value to the second element of this dynamical
variable.

get(test)[2] <- 5  # doesn't work.

Thanks a lot.

Jun Shen

	[[alternative HTML version deleted]]


From dnbarron at gmail.com  Tue Jul  7 23:11:54 2015
From: dnbarron at gmail.com (David Barron)
Date: Tue, 7 Jul 2015 22:11:54 +0100
Subject: [R] problem with using seq() or rep() inside a for loop
In-Reply-To: <559C13D7.4030303@uni-bonn.de>
References: <559C13D7.4030303@uni-bonn.de>
Message-ID: <CAHuze_KukP87CJoWU2wxjS1Tn5_oNJJ7Cks4GpNJWAj5XpUQ-g@mail.gmail.com>

You need to put the expression on the right of the colon in your for
statement in parenthesis:

for (i in 1:(ncol(MD_dist) - 1)){
...
}

On 7 July 2015 at 19:00, Karl Schilling <karl.schilling at uni-bonn.de> wrote:
> Dear All:
>
> I want to use seq() inside a for-loop and use the looping counter i as the
> "by" argument in seq(). Here is some exemplary data and code:
>
> # set up some data
> distances <- c(0, NA, NA, NA, NA, NA,
>                5, 0, NA, NA, NA, NA,
>                5, 2, 0, NA, NA, NA,
>                18, 5, 5, 0, NA, NA,
>                25, 10, 8, 1, 0, NA,
>                41, 20, 18, 5, 2, 0)
>
> MD_dist <- matrix(distances, ncol = 6)
>
> MEAN <- numeric(nrow(MD_dist) - 1) # just to set up a vector
>
> # loop to add (subsets of) off-diagonal diagonal values
> for(i in 1: ncol(MD_dist) - 1){
> diagonal <- as.vector(MD_dist[row(MD_dist) == (col(MD_dist) - i)])
> # the following line extracts every i-th element from "diagonal"
> diagonal.2 <- diagonal[seq(1, to = length(diagonal), by = i)]
> MEAN[i] <- mean(diagonal.2)
> }
>
> However, I keep getting the following error message:
>
> Error in seq.default(1, to = length(diagonal), by = i) :
>   invalid (to - from)/by in seq(.)
>
> May I add that if I run the loop "by hand" - i.e. by just setting i = 1,
> 2... etc and then running the core without the for {...} , everything works
> fine.
>
> Further, when I use rep(..) instead of seq(...) to extract the desired
> values from "diagonal", I have the very same problem - it works outside a
> for loop, but fails inside.
>
> I am using R 3.2.1. (x64) under Win 7 Professional on a 64 bit machine.
>
> Any suggestion would be appreciated.
>
> Thank you so much.
>
> Karl
>
> --
> Karl Schilling
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From boris.steipe at utoronto.ca  Tue Jul  7 23:12:55 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Tue, 7 Jul 2015 17:12:55 -0400
Subject: [R] problem with using seq() or rep() inside a for loop
In-Reply-To: <559C13D7.4030303@uni-bonn.de>
References: <559C13D7.4030303@uni-bonn.de>
Message-ID: <7F442222-B34B-4D4A-B31F-5896FC18C4E0@utoronto.ca>

The fact that it works when you pass your i values "by hand" should alert you that perhaps your loop control does not do what you think it does.

Consider: 
Your version:
for(i in 1:6 - 1)

What you probably meant:
for(i in 1:(6 - 1)) print(i)

The error is created on the first iteration of your loop where you set i to 0. "by" can't be 0.




Cheers,
Boris

On Jul 7, 2015, at 2:00 PM, Karl Schilling <karl.schilling at uni-bonn.de> wrote:

> Dear All:
> 
> I want to use seq() inside a for-loop and use the looping counter i as the "by" argument in seq(). Here is some exemplary data and code:
> 
> # set up some data
> distances <- c(0, NA, NA, NA, NA, NA,
>               5, 0, NA, NA, NA, NA,
>               5, 2, 0, NA, NA, NA,
>               18, 5, 5, 0, NA, NA,
>               25, 10, 8, 1, 0, NA,
>               41, 20, 18, 5, 2, 0)
> 
> MD_dist <- matrix(distances, ncol = 6)
> 
> MEAN <- numeric(nrow(MD_dist) - 1) # just to set up a vector
> 
> # loop to add (subsets of) off-diagonal diagonal values
> for(i in 1: ncol(MD_dist) - 1){
> diagonal <- as.vector(MD_dist[row(MD_dist) == (col(MD_dist) - i)])
> # the following line extracts every i-th element from "diagonal"
> diagonal.2 <- diagonal[seq(1, to = length(diagonal), by = i)]
> MEAN[i] <- mean(diagonal.2)
> }
> 
> However, I keep getting the following error message:
> 
> Error in seq.default(1, to = length(diagonal), by = i) :
>  invalid (to - from)/by in seq(.)
> 
> May I add that if I run the loop "by hand" - i.e. by just setting i = 1, 2... etc and then running the core without the for {...} , everything works fine.
> 
> Further, when I use rep(..) instead of seq(...) to extract the desired values from "diagonal", I have the very same problem - it works outside a for loop, but fails inside.
> 
> I am using R 3.2.1. (x64) under Win 7 Professional on a 64 bit machine.
> 
> Any suggestion would be appreciated.
> 
> Thank you so much.
> 
> Karl
> 
> -- 
> Karl Schilling
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Jul  7 23:23:57 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 7 Jul 2015 14:23:57 -0700
Subject: [R] How to assign value to a variable dynamically constructed
In-Reply-To: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
References: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
Message-ID: <CAGxFJbTQ4hi10CLN-CgFXwzogyTfZ+NMyypyiJwsdtRmDY+PHg@mail.gmail.com>

Try reading the Help files before posting here. That's what they're for.

?get

provides the answer in a note in the Help page.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Jul 7, 2015 at 1:53 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> Dear list,
>
> Let's say we have a variable (id), whose name is dynamically constructed.
> This variable represents a vector or data frame with many elements. Now I
> want to specifically assign a value to one of the elements. I couldn't get
> it right.
>
> test <- 'id' # "id" is dynamically constructed through paste()
>
> id <- 1:4
>
> # I can get the element by doing
>
> get(test)[2]
>
> # Now I want to assign a value to the second element of this dynamical
> variable.
>
> get(test)[2] <- 5  # doesn't work.
>
> Thanks a lot.
>
> Jun Shen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Tue Jul  7 23:26:39 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 7 Jul 2015 14:26:39 -0700
Subject: [R] problem with using seq() or rep() inside a for loop
In-Reply-To: <CAHuze_KukP87CJoWU2wxjS1Tn5_oNJJ7Cks4GpNJWAj5XpUQ-g@mail.gmail.com>
References: <559C13D7.4030303@uni-bonn.de>
	<CAHuze_KukP87CJoWU2wxjS1Tn5_oNJJ7Cks4GpNJWAj5XpUQ-g@mail.gmail.com>
Message-ID: <CAF8bMcbawF2eY-nO+1G5hZc0xvPe760Y_wfPw+RzF36nncX0MQ@mail.gmail.com>

   for (i in 1:(ncol(MD_dist) - 1)){
   ...
   }

Even better, replace
   1:(n-1)
with
   seq_len(n-1)
The latter does what you want (and empty integer vector) when n is 1;
the former would give c(1,0).


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jul 7, 2015 at 2:11 PM, David Barron <dnbarron at gmail.com> wrote:

> You need to put the expression on the right of the colon in your for
> statement in parenthesis:
>
> for (i in 1:(ncol(MD_dist) - 1)){
> ...
> }
>
> On 7 July 2015 at 19:00, Karl Schilling <karl.schilling at uni-bonn.de>
> wrote:
> > Dear All:
> >
> > I want to use seq() inside a for-loop and use the looping counter i as
> the
> > "by" argument in seq(). Here is some exemplary data and code:
> >
> > # set up some data
> > distances <- c(0, NA, NA, NA, NA, NA,
> >                5, 0, NA, NA, NA, NA,
> >                5, 2, 0, NA, NA, NA,
> >                18, 5, 5, 0, NA, NA,
> >                25, 10, 8, 1, 0, NA,
> >                41, 20, 18, 5, 2, 0)
> >
> > MD_dist <- matrix(distances, ncol = 6)
> >
> > MEAN <- numeric(nrow(MD_dist) - 1) # just to set up a vector
> >
> > # loop to add (subsets of) off-diagonal diagonal values
> > for(i in 1: ncol(MD_dist) - 1){
> > diagonal <- as.vector(MD_dist[row(MD_dist) == (col(MD_dist) - i)])
> > # the following line extracts every i-th element from "diagonal"
> > diagonal.2 <- diagonal[seq(1, to = length(diagonal), by = i)]
> > MEAN[i] <- mean(diagonal.2)
> > }
> >
> > However, I keep getting the following error message:
> >
> > Error in seq.default(1, to = length(diagonal), by = i) :
> >   invalid (to - from)/by in seq(.)
> >
> > May I add that if I run the loop "by hand" - i.e. by just setting i = 1,
> > 2... etc and then running the core without the for {...} , everything
> works
> > fine.
> >
> > Further, when I use rep(..) instead of seq(...) to extract the desired
> > values from "diagonal", I have the very same problem - it works outside a
> > for loop, but fails inside.
> >
> > I am using R 3.2.1. (x64) under Win 7 Professional on a 64 bit machine.
> >
> > Any suggestion would be appreciated.
> >
> > Thank you so much.
> >
> > Karl
> >
> > --
> > Karl Schilling
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Wed Jul  8 00:22:46 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 8 Jul 2015 00:22:46 +0200
Subject: [R] Implementation of contrasts
In-Reply-To: <6BB221F9-8708-4D62-BBF7-2D73268AD1FF@plessthan.com>
References: <6BB221F9-8708-4D62-BBF7-2D73268AD1FF@plessthan.com>
Message-ID: <237545C5-8B2A-42A0-9F79-401866ED3014@gmail.com>


> On 07 Jul 2015, at 16:20 , Dennis Fisher <fisher at plessthan.com> wrote:
> 
> R 3.2.1
> OS X
> 
> Colleagues
> 
> I am trying to understand ANOVA contrasts and I have encountered some puzzling results.
> 
> I have data from five studies with four different combinations of treatments.  I want to make the following comparisons:
> 
> Study 1, 3: 	A / B vs. C / D (i.e., mean of A and B vs. mean of C and D)
> Study 2: 		A vs. B / C
> Study 4: 		A / B / C vs. D
> Study 5: 		A vs. B / C / D
> 
> With some trial-and-error, I got the following code to yield results matching SAS outputs:
> 	AB.CD 					<- c(1, 1, -1, -1)/2 
> 	A.BC 					<- c(1, -1, -1)/2
> 	ABC.D 					<- c(1, 1, 1, -1)/2
> 	A.BCD 					<- c(1, -1, -1, -1)/2 
> 	if (STUDY %in% c(1,3)))		contrasts(DATA$TRT)     <- AB.CD
> 	if (STUDY == 2) 			contrasts(DATA$TRT)     <- A.BC
> 	if (STUDY == 4)			contrasts(DATA$TRT)     <- ABC.D
> 	if (STUDY == 5)			contrasts(DATA$TRT)     <- A.BCD
> 
> AB.CD makes sense to me ? take one-half of each of A and B compare to negative one-half of C and D (the contrasts add to zero).
> However, I don?t understand how the other contrasts are written (i.e., they don?t add to zero).  For example, I tried:
> 	A.BC					<- c(2, -1, -1) / 2
> 	ABC.D 					<- c(1, 1, 1, -3)/3
> without success (they yielded results markedly different from SAS)
> 
> I have searched the web extensively but the explanations of contrasts in R are not particularly understandable.  Can anyone help me understand the specifics of this situation?  Thanks in advance.

This is a standard pitfall. You have to distinguish between contrasts and contrast parametrizations. One is the contrast parameter as a function of the expected values, the other states the expected values as a function of the contrast parameters. Formulated that way, it is pretty obvious that they shouldn't be identical, but in some cases they actually are, at least kind of...

So e.g. 

ABC.D <- c(1, 1, 1, -1)/2

states that the means of group A, B, and C are all at +a/2 but D is at -a/2 so the difference is a. You can orthogonalize the contrast by subtracting the mean, yielding

> x <- c(1,1,1,-1)/2
> x - mean(x)
[1]  0.25  0.25  0.25 -0.75

so now the three groups are at a/4 and D at -3a/4, the difference still being a. However, now they sum to zero and you will notice that they are proportional to (1,1,1,-3)/3. In fact,

> xx <- x - mean(x)
> xx/sum(xx^2)
[1]  0.3333333  0.3333333  0.3333333 -1.0000000

The whole thing is related to the familiar (X'X)^-1 X'Y formula. With the c(1, 1, 1, -1)/2 parametrization, we can do

> cbind(x,1)
        x  
[1,]  0.5 1
[2,]  0.5 1
[3,]  0.5 1
[4,] -0.5 1
> X <- cbind(x,1)
> solve(crossprod(X),t(X))
       [,1]      [,2]      [,3] [,4]
x 0.3333333 0.3333333 0.3333333 -1.0
  0.1666667 0.1666667 0.1666667  0.5

with an orthogonal parametrization, crossprod(X) becomes diagonal and the rows of solve(crossprod(X),t(X)) become proportional to corresponding rows of X.

> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone: 1-866-PLessThan (1-866-753-7784)
> Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jrkrideau at inbox.com  Wed Jul  8 01:23:34 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 7 Jul 2015 15:23:34 -0800
Subject: [R] add a special column to a matrix
In-Reply-To: <CAMqbV1BKQuoOBL42Q+mdroYg-4qYLL977pQ7mZ1uDRQA4jAE4w@mail.gmail.com>
Message-ID: <6E00630D1D4.00000A3Fjrkrideau@inbox.com>

?merge should do it.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: lid.zigh at gmail.com
> Sent: Tue, 7 Jul 2015 09:42:56 -0500
> To: r-help at r-project.org
> Subject: [R] add a special column to a matrix
> 
> Hi there,
> 
> I have a two matrices which they have a common column! I want to add the
> a
> column of second to matrix to the equivalent column of first matrix!
> my first matrix is
> head(mat1)
> a       b         c           d
> fg    1:23       dfgv       5
> pt    10:18     tgtgh      1
> wq   15:123    oiljk       6
> fg     9:1323    ass       4
> yr     12:123    kjjlk       5
> 
> my second matrix:
> head(mat2)
>        e            q
>   1:23             0
>    10:18           1
>   14:455          2
>    15:123         2
>     9:1323        2
>     12:123        1
>      5:1548        0
> 
> mat1 and mat2 have a common column (b and e) but is not in the same
> order,
> I want to add the q column from mat2 to be added to mat1
> my output will be:
> 
> head(mat)
> a       b         c           d         q
> fg    1:23       dfgv       5        0
> pt    10:18     tgtgh      1        1
> wq   15:123    oiljk       6        2
> fg     9:1323    ass       4        2
> yr     12:123    kjjlk       5        1
> 
> would you please let me know how to do it?
> 
> Thanks
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From rosita21 at gmail.com  Tue Jul  7 23:45:27 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Tue, 7 Jul 2015 22:45:27 +0100
Subject: [R] multiple graphs with a single legend and trellis graph
Message-ID: <5AA641EE-7369-496A-86C0-3A6066EDE16A@gmail.com>

Iam trying to plot 6 graphs in one single plot and I was able to, nonetheless I wanted that all graphs had just 1 common legend, as the legend is the same for all the 6 graphs and there is no sense in repeating it 6 times and even more, the legends in each graph sometimes don?t fit the graph.

Is there a way to put just one legend for all the 6 graphs ate the same time?

I was told to use a trellis graph, but after days of trying to do that I wasn?t able to.

Can anyone help me? 


library(ggplot2)
library(reshape)
library(lattice)

par(mfrow=c(2,3))
mse.alpha1 <-read.csv(file="graphs_mse_alpha1.csv",head=TRUE,sep=",")
attach(mse.alpha1)
names(mse1000.alpha1)
mse.alpha2 <-read.csv(file="graphs_mse_alpha2.csv",head=TRUE,sep=",")
attach(mse.alpha2)
names(mse.alpha2)
nsample==50

plot(mse.alpha1$lambda[mse.alpha1$nsample==50],
mse.alpha1$mse.naive[mse.alpha1$nsample==50],
xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
)
lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.RegCal[mse.alpha1$nsample==50],col=2,lty=2)
lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.PL[mse.alpha1$nsample==50],col=3,lty=3)
title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
title("\n\n sample size=50")
legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))

plot(mse.alpha1$lambda[mse.alpha1$nsample==250],
mse.alpha1$mse.naive[mse.alpha1$nsample==250],
xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
)
lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.RegCal[mse.alpha1$nsample==250],col=2,lty=2)
lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.PL[mse.alpha1$nsample==250],col=3,lty=3)
title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
title("\n\n sample size=250")
legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))


plot(mse.alpha1$lambda[mse.alpha1$nsample==1000],
mse.alpha1$mse.naive[mse.alpha1$nsample== 1000],
xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
)
lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.RegCal[mse.alpha1$nsample== 1000],col=2,lty=2)
lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.PL[mse.alpha1$nsample== 1000],col=3,lty=3)
title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
title("\n\n sample size=1000")
legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))

plot(mse.alpha2$lambda[mse.alpha2$nsample==50],
mse.alpha2$mse.naive[mse.alpha2$nsample==50],
xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
)
lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.RegCal[mse.alpha2$nsample==50],col=2,lty=2)
lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.PL[mse.alpha2$nsample==50],col=3,lty=3)
title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
title("\n\n sample size=50")
legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))


plot(mse.alpha2$lambda[mse.alpha2$nsample==250],
mse.alpha2$mse.naive[mse.alpha2$nsample==250],
xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
)
lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.PL[mse.alpha2$nsample==250],col=3,lty=3)
title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
title("\n\n sample size=250")
legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))

plot(mse.alpha2$lambda[mse.alpha2$nsample==1000],
mse.alpha2$mse.naive[mse.alpha2$nsample== 1000],
xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
)
lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.PL[mse.alpha2$nsample== 1000],col=3,lty=3)
title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
title("\n\n sample size=1000")
legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))










Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates


From dwinsemius at comcast.net  Wed Jul  8 05:05:31 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 7 Jul 2015 20:05:31 -0700
Subject: [R] multiple graphs with a single legend and trellis graph
In-Reply-To: <5AA641EE-7369-496A-86C0-3A6066EDE16A@gmail.com>
References: <5AA641EE-7369-496A-86C0-3A6066EDE16A@gmail.com>
Message-ID: <B7E977BA-153A-42D6-8549-A5A3BD3F4FCC@comcast.net>


On Jul 7, 2015, at 2:45 PM, Rosa Oliveira wrote:

> Iam trying to plot 6 graphs in one single plot and I was able to, nonetheless I wanted that all graphs had just 1 common legend, as the legend is the same for all the 6 graphs and there is no sense in repeating it 6 times and even more, the legends in each graph sometimes don?t fit the graph.
> 
> Is there a way to put just one legend for all the 6 graphs ate the same time?
> 
> I was told to use a trellis graph, but after days of trying to do that I wasn?t able to.
> 
> Can anyone help me? 
> 
> 
> library(ggplot2)
> library(reshape)


> library(lattice)

Why did you load those packages above? As far as I can see you did not use any lattice or ggplot2 functions. (Also see no reshape or reshape2 functions in use.)

> par(mfrow=c(2,3))
> mse.alpha1 <-read.csv(file="graphs_mse_alpha1.csv",head=TRUE,sep=",")

And there you lose us. We are unable to see your data. The `legend` function can put the legend anywhere. You may need to set par(xpd=TRUE) if the location is outside the current plot area. If you wnated just one legend then you mus ask yourself why you are issuing multiple legend calls. I see the token-`legend` a total of 12 times in the code below.


> attach(mse.alpha1)
> names(mse1000.alpha1)
> mse.alpha2 <-read.csv(file="graphs_mse_alpha2.csv",head=TRUE,sep=",")
> attach(mse.alpha2)
> names(mse.alpha2)
> nsample==50
> 
> plot(mse.alpha1$lambda[mse.alpha1$nsample==50],
> mse.alpha1$mse.naive[mse.alpha1$nsample==50],
> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
> )
> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.RegCal[mse.alpha1$nsample==50],col=2,lty=2)
> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.PL[mse.alpha1$nsample==50],col=3,lty=3)
> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
> title("\n\n sample size=50")
> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
> 
> plot(mse.alpha1$lambda[mse.alpha1$nsample==250],
> mse.alpha1$mse.naive[mse.alpha1$nsample==250],
> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
> )
> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.RegCal[mse.alpha1$nsample==250],col=2,lty=2)
> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.PL[mse.alpha1$nsample==250],col=3,lty=3)
> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
> title("\n\n sample size=250")
> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
> 
> 
> plot(mse.alpha1$lambda[mse.alpha1$nsample==1000],
> mse.alpha1$mse.naive[mse.alpha1$nsample== 1000],
> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
> )
> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.RegCal[mse.alpha1$nsample== 1000],col=2,lty=2)
> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.PL[mse.alpha1$nsample== 1000],col=3,lty=3)
> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
> title("\n\n sample size=1000")
> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
> 
> plot(mse.alpha2$lambda[mse.alpha2$nsample==50],
> mse.alpha2$mse.naive[mse.alpha2$nsample==50],
> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
> )
> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.RegCal[mse.alpha2$nsample==50],col=2,lty=2)
> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.PL[mse.alpha2$nsample==50],col=3,lty=3)
> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
> title("\n\n sample size=50")
> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
> 
> 
> plot(mse.alpha2$lambda[mse.alpha2$nsample==250],
> mse.alpha2$mse.naive[mse.alpha2$nsample==250],
> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
> )
> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.PL[mse.alpha2$nsample==250],col=3,lty=3)
> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
> title("\n\n sample size=250")
> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
> 
> plot(mse.alpha2$lambda[mse.alpha2$nsample==1000],
> mse.alpha2$mse.naive[mse.alpha2$nsample== 1000],
> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
> )
> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.PL[mse.alpha2$nsample== 1000],col=3,lty=3)
> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
> title("\n\n sample size=1000")
> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
> 



David Winsemius
Alameda, CA, USA


From steve.taylor at aut.ac.nz  Wed Jul  8 06:18:58 2015
From: steve.taylor at aut.ac.nz (Steve Taylor)
Date: Wed, 8 Jul 2015 04:18:58 +0000
Subject: [R] valid LRT between MASS::polr and nnet::multinom
Message-ID: <CCE952776B6679469977532BD863C39CB2B2EB20@Lewis.autuni.aut.ac.nz>

Dear R-helpers,

Does anyone know if the likelihoods calculated by these two packages are comparable in this way?  

That is, is this a valid likelihood ratio test?

# Reproducable example:
library(MASS)
library(nnet)
data(housing)
polr1 = MASS::polr(Sat ~ Infl + Type + Cont, weights=Freq, data=housing)
mnom1 = nnet::multinom(Sat ~ Infl + Type + Cont, weights=Freq, data=housing)
pll = logLik(polr1)
mll = logLik(mnom1)
res = data.frame(
  model = c('Proportional odds','Multinomial'),
  Function = c('MASS::polr','nnet::multinom'),
  nobs = c(attr(pll, 'nobs'), attr(mll, 'nobs')),
  df = c(attr(pll, 'df'), attr(mll, 'df')),
  logLik = c(pll,mll),
  deviance = c(deviance(polr1), deviance(mnom1)),
  AIC = c(AIC(polr1), AIC(mnom1)),
  stringsAsFactors = FALSE
)
res[3,1:2] = c("Difference","")
res[3,3:7] = apply(res[,3:7],2,diff)[1,]
print(res)
mytest = structure(
  list(
    statistic = setNames(res$logLik[3], "X-squared"),
    parameter = setNames(res$df[3],"df"),
    p.value = pchisq(res$logLik[3], res$df[3], lower.tail = FALSE),
    method = "Likelihood ratio test",
    data.name = "housing"
  ),
  class='htest'
)
print(mytest)

# If you want to see the fitted results:
library(effects)
plot(allEffects(polr1), layout=c(3,1), ylim=0:1)
plot(allEffects(mnom1), layout=c(3,1), ylim=0:1)

many thanks,
    Steve


From petr.pikal at precheza.cz  Wed Jul  8 08:06:20 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 8 Jul 2015 06:06:20 +0000
Subject: [R] add a special column to a matrix
In-Reply-To: <CAMqbV1CX67o6S4i_N_3wd9L7p00pxVULuCX8DKbiZq10-+TT+w@mail.gmail.com>
References: <CAMqbV1BKQuoOBL42Q+mdroYg-4qYLL977pQ7mZ1uDRQA4jAE4w@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33C1A@SRVEXCHMBX.precheza.cz>
	<CAMqbV1D7fW+86e3Ysr5ODnyXcrgOdXJ3wm8_zCzusjfM_JTNXg@mail.gmail.com>
	<CAMqbV1CX67o6S4i_N_3wd9L7p00pxVULuCX8DKbiZq10-+TT+w@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33D10@SRVEXCHMBX.precheza.cz>

Hi

See answers in line

From: Lida Zeighami [mailto:lid.zigh at gmail.com]
Sent: Tuesday, July 07, 2015 10:20 PM
To: PIKAL Petr
Subject: Re: [R] add a special column to a matrix


Hi Petr,

Thanks, I can solve it!
On Jul 7, 2015 11:05 AM, "Lida Zeighami" <lid.zigh at gmail.com<mailto:lid.zigh at gmail.com>> wrote:
Hi Petr,

Thank you so much for replying!
I have two problems:


1.      I couldn't understand what you mean "No HTML posting ", would you please explain more about that?

> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


2. still my problem hasn't been solved! I don't want to add the columns and rows of two matrices together!
just I want  to add the q column from mat2 to mat1  ( column b in mat1 and the rownames in mat1 are the same but not in a same order! )

I should keep the mat1 and just add the value of rownames of mat2 to the equivalent elements in column b( I should add column q from mat2 to mat1 regarding the column b)


Did you try read help page of merge command?

Did you even try merge? If yes please describe what you do not like about it.

merge(mat1, mat2, all.x=TRUE)

shall take all lines of mat1 and add corresponding columns of mat2 to appropriate rows.

However we do not know anything about your data. From your posting it seems to me that

dim(mat1) results in c(n,4) and dim(mat2) results in (n,1) and what do you consider as a column in mat2 are row names. In that case you need to add a column of row names, maybe

mat2$b <- rownames(mat2)

before calling merge

Cheers
Petr



head(mat1)
a       b         c           d
fg    1:23       dfgv       5
pt    10:18     tgtgh      1
wq   15:123    oiljk       6
fg     9:1323    ass       4
yr     12:123    kjjlk       5

my second matrix:
head(mat2)
                     q
  1:23             0
   10:18           1
  14:455          2
   15:123         2
    9:1323        2
    12:123        1
     5:1548        0

output should be :
head(mat)
a       b         c           d         q
fg    1:23       dfgv       5        0
pt    10:18     tgtgh      1        1
wq   15:123    oiljk       6        2
fg     9:1323    ass       4        2
yr     12:123    kjjlk       5        1

Thanks again,
Lida

On Tue, Jul 7, 2015 at 9:53 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

No HTML posting please, sometimes the mail is unreadable

It is a work for merge.

mat3 <- merge(mat1, mat2, all=TRUE)

shall do the trick (untested)

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Lida
> Zeighami
> Sent: Tuesday, July 07, 2015 4:43 PM
> To: r-help at r-project.org<mailto:r-help at r-project.org>
> Subject: [R] add a special column to a matrix
>
> Hi there,
>
> I have a two matrices which they have a common column! I want to add
> the a column of second to matrix to the equivalent column of first
> matrix!
> my first matrix is
> head(mat1)
> a       b         c           d
> fg    1:23       dfgv       5
> pt    10:18     tgtgh      1
> wq   15:123    oiljk       6
> fg     9:1323    ass       4
> yr     12:123    kjjlk       5
>
> my second matrix:
> head(mat2)
>        e            q
>   1:23             0
>    10:18           1
>   14:455          2
>    15:123         2
>     9:1323        2
>     12:123        1
>      5:1548        0
>
> mat1 and mat2 have a common column (b and e) but is not in the same
> order, I want to add the q column from mat2 to be added to mat1 my
> output will be:
>
> head(mat)
> a       b         c           d         q
> fg    1:23       dfgv       5        0
> pt    10:18     tgtgh      1        1
> wq   15:123    oiljk       6        2
> fg     9:1323    ass       4        2
> yr     12:123    kjjlk       5        1
>
> would you please let me know how to do it?
>
> Thanks
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Wed Jul  8 11:53:36 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 8 Jul 2015 11:53:36 +0200
Subject: [R] what constitutes a 'complete sentence'?
In-Reply-To: <CAJ9CoWm0KB6d9VPVkf9SUWHpHSG92=7TxuFDgfY55kp12m1vaQ@mail.gmail.com>
References: <D14340C3-D045-4B0F-9DCA-287E82A71D65@helsinki.fi>
	<55969185.5000604@auckland.ac.nz>
	<CANROs4eymNLgKxWQn2Pyv50VY3BaymKXPW2upKNGh1RAFpZt6g@mail.gmail.com>
	<55970BF9.400@auckland.ac.nz>
	<CAF8bMcZZjyRqS7A1mrCO--gfcEvtzjefMgqhAHpk34kKFioa=w@mail.gmail.com>
	<559AEEA2.2020805@auckland.ac.nz> <559AF0F1.9010902@gmail.com>
	<CDB4EC38-1494-4AF0-97E7-0B94BD8E0E5B@gmail.com>
	<web-564884742@cgpsrv2.cis.mcmaster.ca>
	<915D87EA-1017-4AAE-860B-FCC7E6C4A5F7@gmail.com>
	<web-564904636@cgpsrv2.cis.mcmaster.ca>
	<CAJ9CoWm0KB6d9VPVkf9SUWHpHSG92=7TxuFDgfY55kp12m1vaQ@mail.gmail.com>
Message-ID: <9A08DB0D-E0FF-42E6-BF4B-3F6B5D983DC7@gmail.com>


On 07 Jul 2015, at 16:52 , Max Kuhn <mxkuhn at gmail.com> wrote:

> 
> What are we trying to fix?
> 
> 

Two things, actually.

(1) An error message that sends the package developer on a wild goose chase, because it is both out of sync both with what is wanted, and what is checked for.
(2) The description in the WRE manual is somewhat self-contradicting, and could probably be improved.

To my mind, the tricky one is (1). (2) should be easier because there is space to elaborate.
(1) needs to be succinct, which is hard if there is no single term for what is being required. I'm beginning to think that the best we can do is along the lines of "Malformed Description field: Missing end punctuation.". If more checks are devised, add more messages.
 
-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jfox at mcmaster.ca  Wed Jul  8 15:25:10 2015
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 08 Jul 2015 09:25:10 -0400
Subject: [R] valid LRT between MASS::polr and nnet::multinom
In-Reply-To: <CCE952776B6679469977532BD863C39CB2B2EB20@Lewis.autuni.aut.ac.nz>
References: <CCE952776B6679469977532BD863C39CB2B2EB20@Lewis.autuni.aut.ac.nz>
Message-ID: <web-565027131@cgpsrv2.cis.mcmaster.ca>

Dear Steve,

The short answer is "no," but the test you propose is in my experience usually a close approximation to a valid test.

The proportional-odds and multinomial-logistic regression models differ in two ways: the po model has an equal-coefficients (parallel-regressions) assumptions (except for intercepts) and so has fewer parameters; the po model models cumulative logits, while the mnl model models individual-category logits. As a consequence of the latter difference, the po model is not a specialization of the mnl model, as required by the LR test. A proper test of the po (equal-coefficients) assumption is to fit a cumulative-logit model with this constraint. You can do this with the vglm() function in the VGAM package using the cumulative family.

Here is an example:

---------- snip ---------

> library(effects) # for WVS data
> library(nnet) # for multinom()
> library(MASS) # for polr()
> library(VGAM) # for vglm()
Loading required package: stats4
Loading required package: splines
> 
> mod.polr <- polr(poverty ~ gender + religion + degree + country*poly(age,3),
+                            data=WVS)
> coef(mod.polr)
                 gendermale                 religionyes                   degreeyes 
                  0.1691953                   0.1684846                   0.1413380 
              countryNorway               countrySweden                  countryUSA 
                 -0.3217821                  -0.5732783                   0.6040006 
              poly(age, 3)1               poly(age, 3)2               poly(age, 3)3 
                 19.9101983                 -10.2208416                   6.1157062 
countryNorway:poly(age, 3)1 countrySweden:poly(age, 3)1    countryUSA:poly(age, 3)1 
                -17.0042706                  -9.4160841                   1.5577738 
countryNorway:poly(age, 3)2 countrySweden:poly(age, 3)2    countryUSA:poly(age, 3)2 
                 17.3824147                  17.3856575                  10.1575695 
countryNorway:poly(age, 3)3 countrySweden:poly(age, 3)3    countryUSA:poly(age, 3)3 
                  3.5181428                   2.3652443                  -8.4004861 
> logLik(mod.polr)
'log Lik.' -5182.602 (df=20)
> 
> mod.vglm.p <- vgam(poverty ~ gender + religion + degree + country*poly(age,3),
+                    data=WVS, family=cumulative(parallel=TRUE)) 
> coef(mod.vglm.p) # same within rounding as polr except for sign
              (Intercept):1               (Intercept):2                  gendermale 
                  0.2139034                   2.0267774                  -0.1691716 
                religionyes                   degreeyes               countryNorway 
                 -0.1684541                  -0.1413316                   0.3218158 
              countrySweden                  countryUSA               poly(age, 3)1 
                  0.5733987                  -0.6040348                 -19.9340368 
              poly(age, 3)2               poly(age, 3)3 countryNorway:poly(age, 3)1 
                 10.2120529                  -6.1558215                  17.0535729 
countrySweden:poly(age, 3)1    countryUSA:poly(age, 3)1 countryNorway:poly(age, 3)2 
                  9.4712722                  -1.5238183                 -17.3344400 
countrySweden:poly(age, 3)2    countryUSA:poly(age, 3)2 countryNorway:poly(age, 3)3 
                -17.3422095                 -10.1469673                  -3.4087977 
countrySweden:poly(age, 3)3    countryUSA:poly(age, 3)3 
                 -2.2649306                   8.4423869 
> logLik(mod.vglm.p) # same (within rounding error)
[1] -5182.601
> 
> mod.multinom <- multinom(poverty ~ gender + religion + degree + country*poly(age,3),
+                          data=WVS)
# weights:  60 (38 variable)
initial  value 5911.632725 
iter  10 value 5162.749080
iter  20 value 5007.247684
iter  30 value 4995.375350
iter  40 value 4989.909216
iter  50 value 4987.650806
iter  60 value 4987.190310
iter  70 value 4987.131548
iter  80 value 4987.075422
final  value 4987.073531 
converged
> logLik(mod.multinom)
'log Lik.' -4987.074 (df=38)
> length(coef(mod.multinom))
[1] 38
> 
> mod.vglm.np <- vgam(poverty ~ gender + religion + degree + country*poly(age,3),
+                    data=WVS, family=cumulative(parallel=FALSE))
> logLik(mod.vglm.np) # close but not the same as multinom
[1] -4988.865
> length(coef(mod.vglm.np)) # same no. of coefs
[1] 38

---------------- snip --------------

Best,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	

On Wed, 8 Jul 2015 04:18:58 +0000
 Steve Taylor <steve.taylor at aut.ac.nz> wrote:
> Dear R-helpers,
> 
> Does anyone know if the likelihoods calculated by these two packages are comparable in this way?  
> 
> That is, is this a valid likelihood ratio test?
> 
> # Reproducable example:
> library(MASS)
> library(nnet)
> data(housing)
> polr1 = MASS::polr(Sat ~ Infl + Type + Cont, weights=Freq, data=housing)
> mnom1 = nnet::multinom(Sat ~ Infl + Type + Cont, weights=Freq, data=housing)
> pll = logLik(polr1)
> mll = logLik(mnom1)
> res = data.frame(
>   model = c('Proportional odds','Multinomial'),
>   Function = c('MASS::polr','nnet::multinom'),
>   nobs = c(attr(pll, 'nobs'), attr(mll, 'nobs')),
>   df = c(attr(pll, 'df'), attr(mll, 'df')),
>   logLik = c(pll,mll),
>   deviance = c(deviance(polr1), deviance(mnom1)),
>   AIC = c(AIC(polr1), AIC(mnom1)),
>   stringsAsFactors = FALSE
> )
> res[3,1:2] = c("Difference","")
> res[3,3:7] = apply(res[,3:7],2,diff)[1,]
> print(res)
> mytest = structure(
>   list(
>     statistic = setNames(res$logLik[3], "X-squared"),
>     parameter = setNames(res$df[3],"df"),
>     p.value = pchisq(res$logLik[3], res$df[3], lower.tail = FALSE),
>     method = "Likelihood ratio test",
>     data.name = "housing"
>   ),
>   class='htest'
> )
> print(mytest)
> 
> # If you want to see the fitted results:
> library(effects)
> plot(allEffects(polr1), layout=c(3,1), ylim=0:1)
> plot(allEffects(mnom1), layout=c(3,1), ylim=0:1)
> 
> many thanks,
>     Steve
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bbolker at gmail.com  Wed Jul  8 16:18:28 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 8 Jul 2015 14:18:28 +0000
Subject: [R] Hypergeometric Function seems to give wrong results
References: <CAP=BVWNeusOReNv-sSppN0mL3TMB4199PYsL9uMiiuANM-y0vQ@mail.gmail.com>
Message-ID: <loom.20150708T161647-96@post.gmane.org>

Carlos Nasher via R-help <r-help <at> r-project.org> writes:

>

[snip]

> I need to evaluate the Hypergeometric Function of the 2nd kind (Tricomi
> confluent hypergeometric function). Therefore I'm using the kummerU
> function from the fAsianOptions package. It seems to me that kummerU is
> giving wrong results. Here's an example:
> 
> library("fAsianOptions")
> kummerU(a=19, b=19, x = 10)
> 
> R gives 1838.298 for the real part.
> 
> If I use Mathematica via the wolfram site (
> http://functions.wolfram.com/webMathematica
   /FunctionEvaluation.jsp?name=HypergeometricU)
> the result is 3.52603e-20 which is more reasonable in the context of my
> analysis.
> 

 [snip]

  Your best bet is probably to contact the package maintainer
(use maintainer("fAsianOptions") to see who it is, or look on 
the CRAN page for the package).  If this functionality is very
commonly used in finance you might try the r-sig-finance mailing
list (indicating that you've already tried here).

  good luck
    Ben Bolker


From Gilfred.Tan at aberdeen-asset.com  Wed Jul  8 11:43:40 2015
From: Gilfred.Tan at aberdeen-asset.com (Gilfred Tan)
Date: Wed, 8 Jul 2015 09:43:40 +0000
Subject: [R] R-project on App-V 5
Message-ID: <7A764ECA291DFD4688EA52B398B54AAFBD4B14@SINGMBX01.aberdeen.aberdeen-asset.com>


Hi

Here at Aberdeen Asset Management .  We are in the process of looking to move to a VDI environment (based on Win7) and are wanting to deploy the R-project via Microsoft App-V 5.  Based around this I have a series of technical queries regarding your application.

Firstly can I ask you to please provide me (or point me towards a download location) the current version of software, including an installation guide, and any updates / patches needed?

The company that is packaging our applications into the App-V environment have asked us the following questions (feel free to point me to the install guide where applicable on these queries):

  - are there any pre-requisites (such as Java for instance) or dependant applications?
  - is the app 32-bit or 64-bit?
  - is there any 16-bit code in the app?
  - is it compatible with Win7 64 bit?.
  - does is support RDSH (64bit XenApp 7.6)
  - does it support App-V 5?
  -  any other vendor technical support contact.
  - how are updates managed and what is their typical frequency?
  - Are there licensing details regarding the product, and if so please advise on how it is controlled / locked down.

Please come back to me with any questions you may have on my requests, otherwise thanks in advance for the assistance.

Regards

Gilfred Tan
Discovery Engineer
Aberdeen Asset Management Asia Limited (Reg. No. 199105448E)
Tel: +65 6395 2642
aberdeen-asset.com<http://www.aberdeen-asset.com/>
Find out how aspects of everyday life embody our philosophy at Aberdeen
simplyaberdeen.com<http://www.simplyaberdeen.com>


This email and any attachment are confidential and may contain privileged and copyright information. It is intended solely for the addressee. If you are not the intended recipient, please notify the sender immediately and delete this email. In accordance with good business practice and applicable regulations, all electronic communications with the Aberdeen Asset Management Group of companies may be monitored and retained. Aberdeen Asset Management PLC, Company Number: SC82015, Registered Office: Ten Queen's Terrace, Aberdeen AB10 1YG Scotland. 
For further information please visit our website: http://www.aberdeen-asset.com and www.aberdeen-asset.com/aam.nsf/AAM/privacy.
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Jul  8 12:20:05 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 8 Jul 2015 20:20:05 +1000
Subject: [R] multiple graphs with a single legend and trellis graph
In-Reply-To: <B7E977BA-153A-42D6-8549-A5A3BD3F4FCC@comcast.net>
References: <5AA641EE-7369-496A-86C0-3A6066EDE16A@gmail.com>
	<B7E977BA-153A-42D6-8549-A5A3BD3F4FCC@comcast.net>
Message-ID: <CA+8X3fVrkw0AicWORb7uhByugxPZKHL2tvSBqCkDc8aW07AgWg@mail.gmail.com>

Hi Rosa,
As you are using base graphics, here is an example that might be of
use. As we don't have access to your data, I have used something
similar to the toy data in the example for the "panes" function. This
could be done better using the "split.screen" function, so let me know
if you would like an example using that.

 library(plotrix)
 # start a wide plotting device
 x11(width=10,height=4)
 y<-runif(100)
 oldpar<-panes(matrix(1:6,nrow=2,byrow=TRUE),widths=c(1,1,1.7))
 par(mar=c(0,2,1.8,0))
 boxplot(y,axes=FALSE)
 axis(2)
 box()
 par(mar=c(0,0,1.8,0))
 tab.title("Boxplot of y",tab.col="#88dd88",cex=1)
 y_hist<-hist(y,axes=FALSE,breaks=seq(0,1,length.out=5))
 box()
 tab.title("Histogram of y",tab.col="#dd8800",cex=1)
 par(mar=c(0,0,1.8,12))
 pie(y_hist$counts,col=2:9)
 tab.title("Pie chart of y categories",tab.col="#8888dd",cex=1)
 box()
 par(mar=c(2,2,1.8,0))
 plot(y,xaxs="i",xlim=c(0,101),axes=FALSE,col=2:9)
 axis(2)
 box()
 tab.title("Scatterplot of y",tab.col="#aabbcc",cex=1)
 par(mar=c(2,0,1.8,0))
 plot(sort(y),xaxs="i",xlim=c(0,101),axes=FALSE,col=2:9)
 box()
 tab.title("Scatterplot of y sorted",tab.col="#ddbc44",cex=1)
 # center the title at the middle of the fifth plot
 mtext("Overall title of plot",side=1,line=0.8,cex=1.5)
 par(mar=c(2,0,1.8,12))
 plot(diff(y),xaxs="i",xlim=c(0,100),axes=FALSE,col=2:9)
 axis(4)
 box()
 tab.title("Scatterplot of diff(y)",tab.col="#ff33cc",cex=1)
 legend(115,1.8,
  c("Boxplot","Histogram","Pie chart","Scatterplot","Sort","Diff"),
  fill=c("#88dd88","#dd8800","#8888dd","#aabbcc","#ddbc44","#ff33cc"),
  xpd=NA)

Jim




On Wed, Jul 8, 2015 at 1:05 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> On Jul 7, 2015, at 2:45 PM, Rosa Oliveira wrote:
>
>> Iam trying to plot 6 graphs in one single plot and I was able to, nonetheless I wanted that all graphs had just 1 common legend, as the legend is the same for all the 6 graphs and there is no sense in repeating it 6 times and even more, the legends in each graph sometimes don?t fit the graph.
>>
>> Is there a way to put just one legend for all the 6 graphs ate the same time?
>>
>> I was told to use a trellis graph, but after days of trying to do that I wasn?t able to.
>>
>> Can anyone help me?
>>
>>
>> library(ggplot2)
>> library(reshape)
>
>
>> library(lattice)
>
> Why did you load those packages above? As far as I can see you did not use any lattice or ggplot2 functions. (Also see no reshape or reshape2 functions in use.)
>
>> par(mfrow=c(2,3))
>> mse.alpha1 <-read.csv(file="graphs_mse_alpha1.csv",head=TRUE,sep=",")
>
> And there you lose us. We are unable to see your data. The `legend` function can put the legend anywhere. You may need to set par(xpd=TRUE) if the location is outside the current plot area. If you wnated just one legend then you mus ask yourself why you are issuing multiple legend calls. I see the token-`legend` a total of 12 times in the code below.
>
>
>> attach(mse.alpha1)
>> names(mse1000.alpha1)
>> mse.alpha2 <-read.csv(file="graphs_mse_alpha2.csv",head=TRUE,sep=",")
>> attach(mse.alpha2)
>> names(mse.alpha2)
>> nsample==50
>>
>> plot(mse.alpha1$lambda[mse.alpha1$nsample==50],
>> mse.alpha1$mse.naive[mse.alpha1$nsample==50],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>> )
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.RegCal[mse.alpha1$nsample==50],col=2,lty=2)
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.PL[mse.alpha1$nsample==50],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>> title("\n\n sample size=50")
>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>
>> plot(mse.alpha1$lambda[mse.alpha1$nsample==250],
>> mse.alpha1$mse.naive[mse.alpha1$nsample==250],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>> )
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.RegCal[mse.alpha1$nsample==250],col=2,lty=2)
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.PL[mse.alpha1$nsample==250],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>> title("\n\n sample size=250")
>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>
>>
>> plot(mse.alpha1$lambda[mse.alpha1$nsample==1000],
>> mse.alpha1$mse.naive[mse.alpha1$nsample== 1000],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>> )
>> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.RegCal[mse.alpha1$nsample== 1000],col=2,lty=2)
>> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.PL[mse.alpha1$nsample== 1000],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>> title("\n\n sample size=1000")
>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>
>> plot(mse.alpha2$lambda[mse.alpha2$nsample==50],
>> mse.alpha2$mse.naive[mse.alpha2$nsample==50],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>> )
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.RegCal[mse.alpha2$nsample==50],col=2,lty=2)
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.PL[mse.alpha2$nsample==50],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>> title("\n\n sample size=50")
>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>
>>
>> plot(mse.alpha2$lambda[mse.alpha2$nsample==250],
>> mse.alpha2$mse.naive[mse.alpha2$nsample==250],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>> )
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.PL[mse.alpha2$nsample==250],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>> title("\n\n sample size=250")
>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>
>> plot(mse.alpha2$lambda[mse.alpha2$nsample==1000],
>> mse.alpha2$mse.naive[mse.alpha2$nsample== 1000],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>> )
>> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
>> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.PL[mse.alpha2$nsample== 1000],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>> title("\n\n sample size=1000")
>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>
>
>
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Wed Jul  8 12:32:16 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 8 Jul 2015 20:32:16 +1000
Subject: [R] add a special column to a matrix
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33D10@SRVEXCHMBX.precheza.cz>
References: <CAMqbV1BKQuoOBL42Q+mdroYg-4qYLL977pQ7mZ1uDRQA4jAE4w@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33C1A@SRVEXCHMBX.precheza.cz>
	<CAMqbV1D7fW+86e3Ysr5ODnyXcrgOdXJ3wm8_zCzusjfM_JTNXg@mail.gmail.com>
	<CAMqbV1CX67o6S4i_N_3wd9L7p00pxVULuCX8DKbiZq10-+TT+w@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C33D10@SRVEXCHMBX.precheza.cz>
Message-ID: <CA+8X3fVf+--LDeTwZe9faxNetWOUjQEujOw==w57VqiiCb5y3w@mail.gmail.com>

Hi Lida,
I think what you want is:

mat1<-read.table(text="a    b    c     d
fg    1:23       dfgv       5
pt    10:18     tgtgh      1
wq   15:123    oiljk       6
fg     9:1323    ass       4
yr     12:123    kjjlk       5",header=TRUE)

mat2<-read.table(text="b     q
 1:23      0
 10:18     1
 14:455    2
 15:123    2
 9:1323    2
 12:123    1
 5:1548    0",header=TRUE)

merge(mat1,mat2,by="b")

You don't seem to have a column name for the first column in mat2, so
I have added one. If it is different from mat1, you will have to use a
slightly different "b" argument.

Jim

On Wed, Jul 8, 2015 at 4:06 PM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> Hi
>
> See answers in line
>
> From: Lida Zeighami [mailto:lid.zigh at gmail.com]
> Sent: Tuesday, July 07, 2015 10:20 PM
> To: PIKAL Petr
> Subject: Re: [R] add a special column to a matrix
>
>
> Hi Petr,
>
> Thanks, I can solve it!
> On Jul 7, 2015 11:05 AM, "Lida Zeighami" <lid.zigh at gmail.com<mailto:lid.zigh at gmail.com>> wrote:
> Hi Petr,
>
> Thank you so much for replying!
> I have two problems:
>
>
> 1.      I couldn't understand what you mean "No HTML posting ", would you please explain more about that?
>
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
> 2. still my problem hasn't been solved! I don't want to add the columns and rows of two matrices together!
> just I want  to add the q column from mat2 to mat1  ( column b in mat1 and the rownames in mat1 are the same but not in a same order! )
>
> I should keep the mat1 and just add the value of rownames of mat2 to the equivalent elements in column b( I should add column q from mat2 to mat1 regarding the column b)
>
>
> Did you try read help page of merge command?
>
> Did you even try merge? If yes please describe what you do not like about it.
>
> merge(mat1, mat2, all.x=TRUE)
>
> shall take all lines of mat1 and add corresponding columns of mat2 to appropriate rows.
>
> However we do not know anything about your data. From your posting it seems to me that
>
> dim(mat1) results in c(n,4) and dim(mat2) results in (n,1) and what do you consider as a column in mat2 are row names. In that case you need to add a column of row names, maybe
>
> mat2$b <- rownames(mat2)
>
> before calling merge
>
> Cheers
> Petr
>
>
>
> head(mat1)
> a       b         c           d
> fg    1:23       dfgv       5
> pt    10:18     tgtgh      1
> wq   15:123    oiljk       6
> fg     9:1323    ass       4
> yr     12:123    kjjlk       5
>
> my second matrix:
> head(mat2)
>                      q
>   1:23             0
>    10:18           1
>   14:455          2
>    15:123         2
>     9:1323        2
>     12:123        1
>      5:1548        0
>
> output should be :
> head(mat)
> a       b         c           d         q
> fg    1:23       dfgv       5        0
> pt    10:18     tgtgh      1        1
> wq   15:123    oiljk       6        2
> fg     9:1323    ass       4        2
> yr     12:123    kjjlk       5        1
>
> Thanks again,
> Lida
>
> On Tue, Jul 7, 2015 at 9:53 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
> Hi
>
> No HTML posting please, sometimes the mail is unreadable
>
> It is a work for merge.
>
> mat3 <- merge(mat1, mat2, all=TRUE)
>
> shall do the trick (untested)
>
> Cheers
> Petr
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Lida
>> Zeighami
>> Sent: Tuesday, July 07, 2015 4:43 PM
>> To: r-help at r-project.org<mailto:r-help at r-project.org>
>> Subject: [R] add a special column to a matrix
>>
>> Hi there,
>>
>> I have a two matrices which they have a common column! I want to add
>> the a column of second to matrix to the equivalent column of first
>> matrix!
>> my first matrix is
>> head(mat1)
>> a       b         c           d
>> fg    1:23       dfgv       5
>> pt    10:18     tgtgh      1
>> wq   15:123    oiljk       6
>> fg     9:1323    ass       4
>> yr     12:123    kjjlk       5
>>
>> my second matrix:
>> head(mat2)
>>        e            q
>>   1:23             0
>>    10:18           1
>>   14:455          2
>>    15:123         2
>>     9:1323        2
>>     12:123        1
>>      5:1548        0
>>
>> mat1 and mat2 have a common column (b and e) but is not in the same
>> order, I want to add the q column from mat2 to be added to mat1 my
>> output will be:
>>
>> head(mat)
>> a       b         c           d         q
>> fg    1:23       dfgv       5        0
>> pt    10:18     tgtgh      1        1
>> wq   15:123    oiljk       6        2
>> fg     9:1323    ass       4        2
>> yr     12:123    kjjlk       5        1
>>
>> would you please let me know how to do it?
>>
>> Thanks
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
>
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rosita21 at gmail.com  Wed Jul  8 14:11:34 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Wed, 8 Jul 2015 13:11:34 +0100
Subject: [R] multiple graphs with a single legend and trellis graph
Message-ID: <0ADDFB05-FC82-46E9-868A-A5E1653F6516@gmail.com>

Dear Jim,

first of all, thank you very much :) 
when I run the code:

myDF <- rbind(mse.alpha1, mse.alpha2)  # assumes both data frames have the same variables in the same order


myDF$ID <- factor(rep(c("alpha1", "alpha2"), times =
c(nrow(mse.alpha1), nrow(mse.alpha2))) )   


library(reshape2)
myDF.melt <- melt(myDF, id = c("ID", "nsample", "lambda"), measure =
c("mse.naive", "mse.RegCal", "mse.PL"))  #Melt the three columns to be plotted, something like

library(ggplot2)
ggplot(myDF.melt, aes(x = lambda, y = value, color = variable)) +
   geom_point() + geom_line() +
   facet_grid(ID ~ nsample) +
   labs(x = expression(paste(lambda)), y = "MSE", color = "Type?)



 I get the attached graph. I also attach my data, so you can see :)

I?m I able to resize the graphs differently? I.e. for alpha1: ylim=ylim=c(0,.6) and for alpha2: ylim=c(0,.17)? 
I reshaped, a new variable in sample was created, NA, Was it me that made something wrong?

I?m very naive and new in R :(

Thanks again ;)







Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates


From rosita21 at gmail.com  Wed Jul  8 15:14:14 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Wed, 8 Jul 2015 14:14:14 +0100
Subject: [R] multiple graphs with a single legend and trellis graph
In-Reply-To: <CADv2QyHvJ1Wt-Hq8e0xBc0xQ7-YCgBJ6Z7UqTBpMv+brQc50AQ@mail.gmail.com>
References: <5AA641EE-7369-496A-86C0-3A6066EDE16A@gmail.com>
	<CADv2QyHvJ1Wt-Hq8e0xBc0xQ7-YCgBJ6Z7UqTBpMv+brQc50AQ@mail.gmail.com>
Message-ID: <A7062029-8DC0-4AF4-BE87-759FC58E6BFA@gmail.com>

Dear Jim,

first of all, thank you very much :) 


can you please explain me how to use split.screen?


I attach my previous graphs and my data, so you can see :)









I?m very naive and new in R :(

I really tried:

library(plotrix)
# start a wide plotting device
x11(width=10,height=4)
y<-runif(100)
oldpar<-panes(matrix(1:6,nrow=2,byrow=TRUE),widths=c(1,1,1.7))
par(mar=c(0,2,1.8,0))

mse <- plot(mse.alpha1$lambda[mse.alpha1$nsample==50],
     mse.alpha1$mse.naive[mse.alpha1$nsample==50],
     xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
     xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
     #,main="yaxs default"
)
lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.RegCal[mse.alpha1$nsample==50],col=2,lty=2)
lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.PL[mse.alpha1$nsample==50],col=3,lty=3)
tab.title("alpha 1 N sample=50",tab.col="#88dd88",cex=1)
# tab.title("\n\n sample size=50")
# problem: when I run: tab.title("Mean squared error for ", paste(alpha[1]))",tab.col="#88dd88",cex=1)
# I get an error message: unexpected string constant in "tab.title("Mean squared error for ", paste(alpha[1]))",tab.col=""
# I searched but wasn't able to fix this one, neither the other "subtitle":
# tab.title("\n\n sample size=50")
box()


par(mar=c(0,0,1.8,0))
plot(mse.alpha1$lambda[mse.alpha1$nsample==250],
     mse.alpha1$mse.naive[mse.alpha1$nsample==250],
     xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
     xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
     #,main="yaxs default"
)
lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.RegCal[mse.alpha1$nsample==250],col=2,lty=2)
lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.PL[mse.alpha1$nsample==250],col=3,lty=3)
tab.title ( "alpha 1 N sample=250",tab.col="#dd8800",cex=1)
box()


plot(mse.alpha1$lambda[mse.alpha1$nsample==1000],
     mse.alpha1$mse.naive[mse.alpha1$nsample== 1000],
     xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
     xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
     #,main="yaxs default"
)
lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.RegCal[mse.alpha1$nsample== 1000],col=2,lty=2)
lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.PL[mse.alpha1$nsample== 1000],col=3,lty=3)
tab.title("alpha 1 N sample=1000",tab.col="#8888dd",cex=1)
box()



par(mar=c(2,2,1.8,0))
plot(mse.alpha2$lambda[mse.alpha2$nsample==50],
     mse.alpha2$mse.naive[mse.alpha2$nsample==50],
     xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
     xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
     #,main="yaxs default"
)
lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.RegCal[mse.alpha2$nsample==50],col=2,lty=2)
lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.PL[mse.alpha2$nsample==50],col=3,lty=3)
box()
tab.title("alpha 2 N sample=50",tab.col="#aabbcc",cex=1)

par(mar=c(2,0,1.8,0))
plot(mse.alpha2$lambda[mse.alpha2$nsample==250],
     mse.alpha2$mse.naive[mse.alpha2$nsample==250],
     xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
     xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
     #,main="yaxs default"
)
lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.PL[mse.alpha2$nsample==250],col=3,lty=3)
box()
tab.title("alpha 2 N sample=250",tab.col="#ddbc44",cex=1)


# center the title at the middle of the fifth plot
mtext("Mean Squared Error",side=1,line=0.8,cex=1.5)


par(mar=c(2,0,1.8,12))
plot(mse.alpha2$lambda[mse.alpha2$nsample==1000],
     mse.alpha2$mse.naive[mse.alpha2$nsample== 1000],
     xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4, 
     xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
     #,main="yaxs default"
)
lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.PL[mse.alpha2$nsample== 1000],col=3,lty=3)
box()
tab.title("alpha 2 N sample=1000",tab.col="#ff33cc",cex=1)

legend(115,1.8,
       c("alpha 1 N sample=50","alpha 1 N sample=250","alpha 1 N sample=1000",
         "alpha 2 N sample=50","alpha 2 N sample=250","alpha 2 N sample=1000"),
       fill=c("#88dd88","#dd8800","#8888dd","#aabbcc","#ddbc44","#ff33cc"),
       xpd=NA)


legend(115,1.8,
       c("Naive", "Regression Calibration", "Pseudo Likelihood"), 
       bty = "n",col=c(4,2,3),lty=c(4,2,3),
       xpd=NA)



and I got:








Thanks again for your help ;)

Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates

> On 08 Jul 2015, at 11:35, Dennis Murphy <djmuser at gmail.com> wrote:
> 
> Hi:
> 
> The general process for doing this kind of thing in either ggplot2 or
> lattice is:
> 
> (1) Row concatenate the data frames, assuming they all have the same
> structure. It's a good idea to have a number or character string that
> identifies each data frame.
> (2) After concatenating the data frames, convert the identifier to a factor.
> (3) Using ggplot2 or lattice, create the core features of your plot.
> In ggplot2, the data frame identifier would be a faceting variable; in
> lattice, it would be a conditioning variable in the plot formula: y ~
> x | identifier
> 
> I didn't take the time to look at your code carefully, but it appears
> this could be done pretty easily in ggplot2 and probably in lattice as
> well. One difference is that it typically takes less code to create a
> ggplot2 legend than it does to create one in lattice.
> 
> Here's a reproducible toy example to illustrate how one could do this
> in principle with ggplot2.
> 
> DF <- data.frame(id = factor(rep(LETTERS[1:6], each = 10)),
>                  v = factor(sample(1:2, 60, replace = TRUE)),
>                  x = seq_len(10),
>                  y = 0.2 + rep(seq(0.1, 0.6, by = 0.1), each = 10) *
>                            seq_len(10) + rnorm(60))
> 
> Think of id as the row concatenation of your data frames, where it is
> assumed that each data frame has the same variables and the same
> structure. The variable v is a manufactured factor to show that you
> can make multiple plots and return one legend. The rest is simply fake
> data to fit regression lines in each sub-data frame identified with
> the same id with different true slopes in each subset but the same
> intercept. The code for the plot is
> 
> library(ggplot2)
> ggplot(DF, aes(x = x, y = y)) +
>    geom_point(aes(color = v)) +
>    geom_smooth(method = "lm", color = "blue", se = FALSE) +
>    facet_wrap(~ id, ncol = 2)
> 
> The feature of interest in the produced graph is that there are six
> panels (produced by the call to facet_wrap()) but only one legend. A
> similar graphic in lattice would be
> 
> library(lattice)
> xyplot(y ~ x | id, data = DF, type = c("p", "r"),
>         as.table = TRUE, col = DF$v,
>         auto.key = list(text = levels(DF$v),
>                         space = "right",
>                         points = TRUE))
> 
> The problem with the legend in the lattice plot is that the non-red
> color in the graph is black but the corresponding color in the legend
> is blue. More work is required in lattice to accurately map the
> colors, shapes, etc. between the graph and legend. It's not worth
> going there at this point.
> 
> In ggplot2, guides (axes or legends) are mappings of aesthetics (in
> this case, x, y and color) to variables (x, y and v, respectively). By
> default, ggplot2 produces a guide for each mapped aesthetic, which is
> defined within the function aes().
> When aes() is used in the ggplot() function, the mappings are global -
> i.e., they apply to all subsequent layers produced by a geom or stat
> function. When aes() is called inside a geom or stat function, the
> mapping is local to the layer produced by the geom/stat call. Wherever
> an aesthetic mapping is declared, a guide will be produced unless it
> is turned off. With this convention, it is much easier to produce
> legend guides in ggplot2 than in lattice. If you want to fine tune the
> contents of a legend, there exist functions of the form
> scale_<aesthetic>_<type>() that give you more freedom in defining
> specific colors, point shapes, etc., as well as labels in the legend.
> 
> Since you did not supply a reproducible example, I would follow steps
> (1) and (2) above to arrange your data, and then follow the guidelines
> from my fake data example to produce the plot. The trick, especially
> with ggplot2, is to figure out how to produce one graph, and then use
> faceting to map that structure to all groups.
> 
> From what I can tell, you'll probably need to do some type of melting
> operation with reshape2::melt() to stack the three columns Naive,
> RegCal and PL, since that is apparently what you want for the legend.
> You'll need two faceting variables: nsample and data set identifier.
> It appears as though you want to plot points and lines in each panel,
> so if you're using ggplot2, you'd use geom_point() and geom_line().
> The titles are not really necessary, as you can produce the
> information you need in the strip panels.
> 
> If I read this right, you have:
> 
> * two data sets
> * nsample as a faceting/conditioning variable within sample
> * lambda as the x-variable
> * points and lines for three types of response variables (MSEs) that
> are to be compared and labeled in a legend
> 
> If that is correct, I'd suggest the following:
> 
> 1. myDF <- rbind(mse.alpha1, mse.alpha2)  # assumes both data frames
> have the same variables in the same order
> 2. myDF$ID <- factor(rep(c("alpha1", "alpha2"), times =
> c(nrow(mse.alpha1), nrow(mse.alpha2)))
> 3. Melt the three columns to be plotted, something like
> 
> library(reshape2)
> myDF.melt <- melt(myDF, id = c("ID", "nsample", "lambda"), measure =
> c("Naive", "RegCal", "PL"))
> 
> This will return a data frame with 3 * nrow(DF) rows and 5 columns.
> The two derived columns are named variable and value, the first of
> which returns the variable names as factor levels and the second of
> which contains the corresponding values. Then,
> 
> library(ggplot2)
> ggplot(myDF.melt, aes(x = lambda, y = value, color = variable)) +
>    geom_point() + geom_line() +
>    facet_grid(ID ~ nsample) +
>    labs(x = expression(lambda), y = "MSE", color = "Type")
> 
> That's the best I can do without access to the data, so you're on your
> own to figure this out. If this doesn't work or I misperceived certain
> elements of your desired graph (a distinct possibility), then please
> re-post a _reproducible example_ back to R-help, which contains both
> data and code. You want the example to be minimal so that potential
> helpers can quickly see the problem and respond with useful
> suggestions. The length of my response in an attempt to divine the
> structure of your data punctuates the need for you to provide data +
> code + expected outcome. By helping potential helpers to help you, you
> are likely to get better and more accurate replies in much less time.
> 
> HTH,
> Dennis
> 
> On Tue, Jul 7, 2015 at 2:45 PM, Rosa Oliveira <rosita21 at gmail.com> wrote:
>> Iam trying to plot 6 graphs in one single plot and I was able to, nonetheless I wanted that all graphs had just 1 common legend, as the legend is the same for all the 6 graphs and there is no sense in repeating it 6 times and even more, the legends in each graph sometimes don?t fit the graph.
>> 
>> Is there a way to put just one legend for all the 6 graphs ate the same time?
>> 
>> I was told to use a trellis graph, but after days of trying to do that I wasn?t able to.
>> 
>> Can anyone help me?
>> 
>> 
>> library(ggplot2)
>> library(reshape)
>> library(lattice)
>> 
>> par(mfrow=c(2,3))
>> mse.alpha1 <-read.csv(file="graphs_mse_alpha1.csv",head=TRUE,sep=",")
>> attach(mse.alpha1)
>> names(mse1000.alpha1)
>> mse.alpha2 <-read.csv(file="graphs_mse_alpha2.csv",head=TRUE,sep=",")
>> attach(mse.alpha2)
>> names(mse.alpha2)
>> nsample==50
>> 
>> plot(mse.alpha1$lambda[mse.alpha1$nsample==50],
>> mse.alpha1$mse.naive[mse.alpha1$nsample==50],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>> )
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.RegCal[mse.alpha1$nsample==50],col=2,lty=2)
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.PL[mse.alpha1$nsample==50],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>> title("\n\n sample size=50")
>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>> 
>> plot(mse.alpha1$lambda[mse.alpha1$nsample==250],
>> mse.alpha1$mse.naive[mse.alpha1$nsample==250],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>> )
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.RegCal[mse.alpha1$nsample==250],col=2,lty=2)
>> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.PL[mse.alpha1$nsample==250],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>> title("\n\n sample size=250")
>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>> 
>> 
>> plot(mse.alpha1$lambda[mse.alpha1$nsample==1000],
>> mse.alpha1$mse.naive[mse.alpha1$nsample== 1000],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>> )
>> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.RegCal[mse.alpha1$nsample== 1000],col=2,lty=2)
>> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.PL[mse.alpha1$nsample== 1000],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>> title("\n\n sample size=1000")
>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>> 
>> plot(mse.alpha2$lambda[mse.alpha2$nsample==50],
>> mse.alpha2$mse.naive[mse.alpha2$nsample==50],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>> )
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.RegCal[mse.alpha2$nsample==50],col=2,lty=2)
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.PL[mse.alpha2$nsample==50],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>> title("\n\n sample size=50")
>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>> 
>> 
>> plot(mse.alpha2$lambda[mse.alpha2$nsample==250],
>> mse.alpha2$mse.naive[mse.alpha2$nsample==250],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>> )
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
>> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.PL[mse.alpha2$nsample==250],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>> title("\n\n sample size=250")
>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>> 
>> plot(mse.alpha2$lambda[mse.alpha2$nsample==1000],
>> mse.alpha2$mse.naive[mse.alpha2$nsample== 1000],
>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>> )
>> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
>> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.PL[mse.alpha2$nsample== 1000],col=3,lty=3)
>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>> title("\n\n sample size=1000")
>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> Atenciosamente,
>> Rosa Oliveira
>> 
>> --
>> ____________________________________________________________________________
>> 
>> 
>> Rosa Celeste dos Santos Oliveira,
>> 
>> E-mail: rosita21 at gmail.com
>> Tlm: +351 939355143
>> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
>> ____________________________________________________________________________
>> "Many admire, few know"
>> Hippocrates
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From rosita21 at gmail.com  Wed Jul  8 15:27:35 2015
From: rosita21 at gmail.com (Rosa Oliveira)
Date: Wed, 8 Jul 2015 14:27:35 +0100
Subject: [R] multiple graphs with a single legend and trellis graph
In-Reply-To: <CA+8X3fVrkw0AicWORb7uhByugxPZKHL2tvSBqCkDc8aW07AgWg@mail.gmail.com>
References: <5AA641EE-7369-496A-86C0-3A6066EDE16A@gmail.com>
	<B7E977BA-153A-42D6-8549-A5A3BD3F4FCC@comcast.net>
	<CA+8X3fVrkw0AicWORb7uhByugxPZKHL2tvSBqCkDc8aW07AgWg@mail.gmail.com>
Message-ID: <1D8785E3-D2E5-42E8-85D0-40305991588D@gmail.com>

Dear Jim,

first of all, thank you very much :) 


can you please explain me how to use split.screen?

I?m felling so silly, I could not run your example because of x11(width=10,height=4). I already installed package XQuartz because X11 library was missing , nonetheless, after I have installed it, ? Error in .External2(C_X11, d$display, d$width, d$height, d$pointsize,  : 
  unable to start device X11
In addition: Warning message:
In x11(width = 10, height = 4) :
  unable to open connection to X11 display ?'" 


I didn?t started the x11 server on my mac ;) 

Now I got the graphs ;)

I attach my previous graphs and my data, so you can see :)









I?m very naive and new in R :(

Thanks again for your help ;)

Atenciosamente,
Rosa Oliveira
Atenciosamente,
Rosa Oliveira

-- 
____________________________________________________________________________


Rosa Celeste dos Santos Oliveira, 

E-mail: rosita21 at gmail.com
Tlm: +351 939355143 
Linkedin: https://pt.linkedin.com/in/rosacsoliveira
____________________________________________________________________________
"Many admire, few know"
Hippocrates

> On 08 Jul 2015, at 11:20, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Rosa,
> As you are using base graphics, here is an example that might be of
> use. As we don't have access to your data, I have used something
> similar to the toy data in the example for the "panes" function. This
> could be done better using the "split.screen" function, so let me know
> if you would like an example using that.
> 
> library(plotrix)
> # start a wide plotting device
> x11(width=10,height=4)
> y<-runif(100)
> oldpar<-panes(matrix(1:6,nrow=2,byrow=TRUE),widths=c(1,1,1.7))
> par(mar=c(0,2,1.8,0))
> boxplot(y,axes=FALSE)
> axis(2)
> box()
> par(mar=c(0,0,1.8,0))
> tab.title("Boxplot of y",tab.col="#88dd88",cex=1)
> y_hist<-hist(y,axes=FALSE,breaks=seq(0,1,length.out=5))
> box()
> tab.title("Histogram of y",tab.col="#dd8800",cex=1)
> par(mar=c(0,0,1.8,12))
> pie(y_hist$counts,col=2:9)
> tab.title("Pie chart of y categories",tab.col="#8888dd",cex=1)
> box()
> par(mar=c(2,2,1.8,0))
> plot(y,xaxs="i",xlim=c(0,101),axes=FALSE,col=2:9)
> axis(2)
> box()
> tab.title("Scatterplot of y",tab.col="#aabbcc",cex=1)
> par(mar=c(2,0,1.8,0))
> plot(sort(y),xaxs="i",xlim=c(0,101),axes=FALSE,col=2:9)
> box()
> tab.title("Scatterplot of y sorted",tab.col="#ddbc44",cex=1)
> # center the title at the middle of the fifth plot
> mtext("Overall title of plot",side=1,line=0.8,cex=1.5)
> par(mar=c(2,0,1.8,12))
> plot(diff(y),xaxs="i",xlim=c(0,100),axes=FALSE,col=2:9)
> axis(4)
> box()
> tab.title("Scatterplot of diff(y)",tab.col="#ff33cc",cex=1)
> legend(115,1.8,
>  c("Boxplot","Histogram","Pie chart","Scatterplot","Sort","Diff"),
>  fill=c("#88dd88","#dd8800","#8888dd","#aabbcc","#ddbc44","#ff33cc"),
>  xpd=NA)
> 
> Jim
> 
> 
> 
> 
> On Wed, Jul 8, 2015 at 1:05 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> On Jul 7, 2015, at 2:45 PM, Rosa Oliveira wrote:
>> 
>>> Iam trying to plot 6 graphs in one single plot and I was able to, nonetheless I wanted that all graphs had just 1 common legend, as the legend is the same for all the 6 graphs and there is no sense in repeating it 6 times and even more, the legends in each graph sometimes don?t fit the graph.
>>> 
>>> Is there a way to put just one legend for all the 6 graphs ate the same time?
>>> 
>>> I was told to use a trellis graph, but after days of trying to do that I wasn?t able to.
>>> 
>>> Can anyone help me?
>>> 
>>> 
>>> library(ggplot2)
>>> library(reshape)
>> 
>> 
>>> library(lattice)
>> 
>> Why did you load those packages above? As far as I can see you did not use any lattice or ggplot2 functions. (Also see no reshape or reshape2 functions in use.)
>> 
>>> par(mfrow=c(2,3))
>>> mse.alpha1 <-read.csv(file="graphs_mse_alpha1.csv",head=TRUE,sep=",")
>> 
>> And there you lose us. We are unable to see your data. The `legend` function can put the legend anywhere. You may need to set par(xpd=TRUE) if the location is outside the current plot area. If you wnated just one legend then you mus ask yourself why you are issuing multiple legend calls. I see the token-`legend` a total of 12 times in the code below.
>> 
>> 
>>> attach(mse.alpha1)
>>> names(mse1000.alpha1)
>>> mse.alpha2 <-read.csv(file="graphs_mse_alpha2.csv",head=TRUE,sep=",")
>>> attach(mse.alpha2)
>>> names(mse.alpha2)
>>> nsample==50
>>> 
>>> plot(mse.alpha1$lambda[mse.alpha1$nsample==50],
>>> mse.alpha1$mse.naive[mse.alpha1$nsample==50],
>>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>>> )
>>> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.RegCal[mse.alpha1$nsample==50],col=2,lty=2)
>>> lines(mse.alpha1$lambda[mse.alpha1$nsample==50],mse.alpha1$mse.PL[mse.alpha1$nsample==50],col=3,lty=3)
>>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>>> title("\n\n sample size=50")
>>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>> 
>>> plot(mse.alpha1$lambda[mse.alpha1$nsample==250],
>>> mse.alpha1$mse.naive[mse.alpha1$nsample==250],
>>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>>> )
>>> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.RegCal[mse.alpha1$nsample==250],col=2,lty=2)
>>> lines(mse.alpha1$lambda[mse.alpha1$nsample==250],mse.alpha1$mse.PL[mse.alpha1$nsample==250],col=3,lty=3)
>>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>>> title("\n\n sample size=250")
>>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>> 
>>> 
>>> plot(mse.alpha1$lambda[mse.alpha1$nsample==1000],
>>> mse.alpha1$mse.naive[mse.alpha1$nsample== 1000],
>>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>>> xlim=c(.6,1), ylim=c(0,1), cex.lab=1.5
>>> )
>>> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.RegCal[mse.alpha1$nsample== 1000],col=2,lty=2)
>>> lines(mse.alpha1$lambda[mse.alpha1$nsample== 1000],mse.alpha1$mse.PL[mse.alpha1$nsample== 1000],col=3,lty=3)
>>> title ( expression (paste ("Mean squared error for ", alpha[1])), cex.main=1.5)
>>> title("\n\n sample size=1000")
>>> legend(.7,1, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>> 
>>> plot(mse.alpha2$lambda[mse.alpha2$nsample==50],
>>> mse.alpha2$mse.naive[mse.alpha2$nsample==50],
>>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>>> )
>>> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.RegCal[mse.alpha2$nsample==50],col=2,lty=2)
>>> lines(mse.alpha2$lambda[mse.alpha2$nsample==50],mse.alpha2$mse.PL[mse.alpha2$nsample==50],col=3,lty=3)
>>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>>> title("\n\n sample size=50")
>>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>> 
>>> 
>>> plot(mse.alpha2$lambda[mse.alpha2$nsample==250],
>>> mse.alpha2$mse.naive[mse.alpha2$nsample==250],
>>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>>> )
>>> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
>>> lines(mse.alpha2$lambda[mse.alpha2$nsample==250],mse.alpha2$mse.PL[mse.alpha2$nsample==250],col=3,lty=3)
>>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>>> title("\n\n sample size=250")
>>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>> 
>>> plot(mse.alpha2$lambda[mse.alpha2$nsample==1000],
>>> mse.alpha2$mse.naive[mse.alpha2$nsample== 1000],
>>> xlab=expression(paste(lambda)),ylab="MSE",type="l",col=4,lty=4,
>>> xlim=c(.6,1), ylim=c(0,.17), cex.lab=1.5
>>> )
>>> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.RegCal[mse.alpha2$nsample==250],col=2,lty=2)
>>> lines(mse.alpha2$lambda[mse.alpha2$nsample== 1000],mse.alpha2$mse.PL[mse.alpha2$nsample== 1000],col=3,lty=3)
>>> title ( expression (paste ("Mean squared error for ", alpha[2])), cex.main=1.5)
>>> title("\n\n sample size=1000")
>>> legend(.7,.17, legend= c("Naive", "Regression Calibration", "Pseudo Likelihood"), bty = "n",col=c(4,2,3),lty=c(4,2,3))
>>> 
>> 
>> 
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From John.Szumiloski at bms.com  Wed Jul  8 15:46:41 2015
From: John.Szumiloski at bms.com (Szumiloski, John)
Date: Wed, 8 Jul 2015 13:46:41 +0000
Subject: [R] lattice::subscripts[?] usage question
Message-ID: <7d95473e5f0d411c81e3c41a9e5af7bc@CO2PR26MB0011.067d.mgd.msft.net>

Dear useRs,

I have a question regarding panel functions in the lattice package, in particular how to customize the plotting colors in xyplot.

I have longitudinal data from several treatments (TRT) with subjects (SUBJ) nested in treatment.  Each subject has several time points; the time points are nearly balanced across subjects, although not completely.  At each sample (SAMP) at each time point (TIME), two replicates (REP) are taken.   Let's call the response Y and the whole data.frame DAT.

My goal is to make a trellis plot with each treatment in its own panel, each subject having its own color and lines connecting the REPs within SAMP (after adding jitter along the TIME axis to distinguish SUBJs).  Let me switch over to an explicit example with artificial data.

   set.seed(695442223)

   DAT <- data.frame(TRT=factor(rep(c("Trt1","Trt2"), each=12)),              # 2 treatments
                     SUBJ=factor(paste('subj', rep(seq(4), each=6), sep='')), # 4 subjects, 2 w/in each TRT
                     SAMP=factor(rep(seq(12), each=2)),                       # 4x3 = 12 samples
                     REP=factor(c(1,2)),                                      # 2 replicates
                     TIME=rep(c(4,4,8,8,12,12), times=4),                     # 3 arbitrary times
                     Y=round(rep(rgamma(n=12, shape=47500, scale=1/500), each=2) +
                              rep(rgamma(n=12, shape=4, scale=1/8), each=2) * c(-1,1),
                             2)   # arbitrary Y but very easy to view
   )

   DAT <- transform(DAT,
                       # horizontal jitter
                    plotX=TIME+ifelse(SUBJ %in% c("subj1","subj3"), -1,1)/10,
                       # unique plotting color for each subject
                    COLS=factor(c( "green", "black", "blue", "red"))[as.numeric(SUBJ)]
   )

The canonical example is:

xyplot(Y ~ plotX | TRT, data = DAT, groups = SAMP, type="b")

Of course, this connects the REPs within SAMP, since SAMP is the "groups" variable.  But each SAMP is now independently colored, recycled from (if I understand correctly)  trellis.par.get('superpose.<symbol/line>')$col, instead of each SUBJ being independently colored from the COLS variable.

In Section 5.2 in Deepayan Sarkar's "Lattice: Multivariate Data Visualization with R" (Springer 2008), the suggestion in Section 5.2 is to use a panel function which gets passed an explicit "subscripts" argument, to distinguish the groups indexing from the color indexing.  Starting with the book's code to plot Figure 5.4, I hacked around to get this:

    xyplot(Y ~ plotX | TRT, data = DAT, groups = SAMP, type="b",
              # the full vector of colors
          cols = DAT[['COLS']],
          panel = function(x, y, ..., cols, groups, subscripts) {
                           colSUBJ <- cols[subscripts]
                           panel.xyplot(x, y, groups=groups, subscripts=subscripts,
                                        ..., col.line=colSUBJ, col.symbol=colSUBJ)
                          }
         )


Now this gets each treatment TRT with its own color, but not each subject SUBJ.  I am indeed not exactly sure what this code does specifically, with the subscripts argument.  But nearly every other permutation of arguments I could think off either gave a totally wrong plot, or an error.

How do I write a panel function to join reps in the same sample with the groups argument, but give each subject a unique color? (an alternate chore is to use only unique colors within each treatment, or repeating colors in different treatments).  And what is a good pointer or two to good resources explaining the use of the subscripts argument in panel functions?  Sarkar's book is fantastic but its treatment of this issue is quite thin.

Thanks,
John Szumiloski

John Szumiloski, Ph.D.
Principal Scientist, Statistician
Analytical and Bioanalytical Development
NBR105-1-1411

Bristol-Myers Squibb
P.O. Box 191
1 Squibb Drive
New Brunswick, NJ
08903-0191

(732) 227-7167





________________________________
 This message (including any attachments) may contain co...{{dropped:8}}


From bgunter.4567 at gmail.com  Wed Jul  8 16:30:19 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 8 Jul 2015 07:30:19 -0700
Subject: [R] R-project on App-V 5
In-Reply-To: <7A764ECA291DFD4688EA52B398B54AAFBD4B14@SINGMBX01.aberdeen.aberdeen-asset.com>
References: <7A764ECA291DFD4688EA52B398B54AAFBD4B14@SINGMBX01.aberdeen.aberdeen-asset.com>
Message-ID: <CAGxFJbT+np5GgUH_4MW=MvOLYkW0qMr97Z+G8wYHsQL-2BJ0rA@mail.gmail.com>

http://cran.r-project.org/

and do not post further to this list, please.


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Jul 8, 2015 at 2:43 AM, Gilfred Tan
<Gilfred.Tan at aberdeen-asset.com> wrote:
>
> Hi
>
> Here at Aberdeen Asset Management .  We are in the process of looking to move to a VDI environment (based on Win7) and are wanting to deploy the R-project via Microsoft App-V 5.  Based around this I have a series of technical queries regarding your application.
>
> Firstly can I ask you to please provide me (or point me towards a download location) the current version of software, including an installation guide, and any updates / patches needed?
>
> The company that is packaging our applications into the App-V environment have asked us the following questions (feel free to point me to the install guide where applicable on these queries):
>
>   - are there any pre-requisites (such as Java for instance) or dependant applications?
>   - is the app 32-bit or 64-bit?
>   - is there any 16-bit code in the app?
>   - is it compatible with Win7 64 bit?.
>   - does is support RDSH (64bit XenApp 7.6)
>   - does it support App-V 5?
>   -  any other vendor technical support contact.
>   - how are updates managed and what is their typical frequency?
>   - Are there licensing details regarding the product, and if so please advise on how it is controlled / locked down.
>
> Please come back to me with any questions you may have on my requests, otherwise thanks in advance for the assistance.
>
> Regards
>
> Gilfred Tan
> Discovery Engineer
> Aberdeen Asset Management Asia Limited (Reg. No. 199105448E)
> Tel: +65 6395 2642
> aberdeen-asset.com<http://www.aberdeen-asset.com/>
> Find out how aspects of everyday life embody our philosophy at Aberdeen
> simplyaberdeen.com<http://www.simplyaberdeen.com>
>
>
> This email and any attachment are confidential and may contain privileged and copyright information. It is intended solely for the addressee. If you are not the intended recipient, please notify the sender immediately and delete this email. In accordance with good business practice and applicable regulations, all electronic communications with the Aberdeen Asset Management Group of companies may be monitored and retained. Aberdeen Asset Management PLC, Company Number: SC82015, Registered Office: Ten Queen's Terrace, Aberdeen AB10 1YG Scotland.
> For further information please visit our website: http://www.aberdeen-asset.com and www.aberdeen-asset.com/aam.nsf/AAM/privacy.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From thierry.onkelinx at inbo.be  Wed Jul  8 16:30:39 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 8 Jul 2015 16:30:39 +0200
Subject: [R] R-project on App-V 5
In-Reply-To: <7A764ECA291DFD4688EA52B398B54AAFBD4B14@SINGMBX01.aberdeen.aberdeen-asset.com>
References: <7A764ECA291DFD4688EA52B398B54AAFBD4B14@SINGMBX01.aberdeen.aberdeen-asset.com>
Message-ID: <CAJuCY5xLvPJsVrvJ=HeE4D70y1B+Us731eZ_xqEJ4MCB2HGAbg@mail.gmail.com>

Please do read the posting guide (
http://www.R-project.org/posting-guide.html
<http://www.r-project.org/posting-guide.html>). It points to the FAQ which
answers much of your questions (http://cran.r-project.org/faqs.html).

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-08 11:43 GMT+02:00 Gilfred Tan <Gilfred.Tan at aberdeen-asset.com>:

>
> Hi
>
> Here at Aberdeen Asset Management .  We are in the process of looking to
> move to a VDI environment (based on Win7) and are wanting to deploy the
> R-project via Microsoft App-V 5.  Based around this I have a series of
> technical queries regarding your application.
>
> Firstly can I ask you to please provide me (or point me towards a download
> location) the current version of software, including an installation guide,
> and any updates / patches needed?
>
> The company that is packaging our applications into the App-V environment
> have asked us the following questions (feel free to point me to the install
> guide where applicable on these queries):
>
>   - are there any pre-requisites (such as Java for instance) or dependant
> applications?
>   - is the app 32-bit or 64-bit?
>   - is there any 16-bit code in the app?
>   - is it compatible with Win7 64 bit?.
>   - does is support RDSH (64bit XenApp 7.6)
>   - does it support App-V 5?
>   -  any other vendor technical support contact.
>   - how are updates managed and what is their typical frequency?
>   - Are there licensing details regarding the product, and if so please
> advise on how it is controlled / locked down.
>
> Please come back to me with any questions you may have on my requests,
> otherwise thanks in advance for the assistance.
>
> Regards
>
> Gilfred Tan
> Discovery Engineer
> Aberdeen Asset Management Asia Limited (Reg. No. 199105448E)
> Tel: +65 6395 2642
> aberdeen-asset.com<http://www.aberdeen-asset.com/>
> Find out how aspects of everyday life embody our philosophy at Aberdeen
> simplyaberdeen.com<http://www.simplyaberdeen.com>
>
>
> This email and any attachment are confidential and may contain privileged
> and copyright information. It is intended solely for the addressee. If you
> are not the intended recipient, please notify the sender immediately and
> delete this email. In accordance with good business practice and applicable
> regulations, all electronic communications with the Aberdeen Asset
> Management Group of companies may be monitored and retained. Aberdeen Asset
> Management PLC, Company Number: SC82015, Registered Office: Ten Queen's
> Terrace, Aberdeen AB10 1YG Scotland.
> For further information please visit our website:
> http://www.aberdeen-asset.com and
> www.aberdeen-asset.com/aam.nsf/AAM/privacy.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lid.zigh at gmail.com  Wed Jul  8 17:37:54 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Wed, 8 Jul 2015 10:37:54 -0500
Subject: [R] select a subset of a matrix which one of its column meet a
	condition
Message-ID: <CAMqbV1DVhhPu1Laano=PhWzuV0auFz9etCwYFoAst3_xy7qcng@mail.gmail.com>

Hi there,

I have a matrix and I want to get a subset from that which one of its
matrix meet a condition,

my matrix is
met
     Row.names             Name
maf                                   caf
1 10:100003915      10:1000039              0.0003782148
0.0003782148
2 10:100008738      10:100008738           0.0003759398
 0.0003759398
3 10:100011321      10:100011321           0.0003762227
 0.0003762227
4 10:100012219      10:100012219           0.0007518797
0.0007518797
5 10:100013325      10:100013325          0.0000000000
 0.0000000000
6 10:100015404      10:100015404          0.0000000000
0.0000000000

I want to choose a subset from "met" which maf values are between 0 and
0.05 (0,0.05)

I wrote this code, but seem doesn't work properly:

> maf<- met[which(c(met[,5]) %in% c(0,0.05)),]

the dim(maf)  is 0 so seems my code didn't work!
would please let me know how to correct it?

Thanks

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Wed Jul  8 19:06:19 2015
From: 538280 at gmail.com (Greg Snow)
Date: Wed, 8 Jul 2015 11:06:19 -0600
Subject: [R] How to assign value to a variable dynamically constructed
In-Reply-To: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
References: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
Message-ID: <CAFEqCdyF82JcOQ7p4UmaH7S1wS81M7tBPuPT1sz9Loqck2jJdA@mail.gmail.com>

This is FAQ 7.21.

The most important part of the answer in FAQ 7.21 is the last section
where it states that it is often easier to use a list rather than
messing around with trying to dynamically name global variables.

If you tell us what you are trying to accomplish then we may have
better advice.  The route you are headed down now usually leads to
inefficient code and hard to find bugs.

On Tue, Jul 7, 2015 at 2:53 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> Dear list,
>
> Let's say we have a variable (id), whose name is dynamically constructed.
> This variable represents a vector or data frame with many elements. Now I
> want to specifically assign a value to one of the elements. I couldn't get
> it right.
>
> test <- 'id' # "id" is dynamically constructed through paste()
>
> id <- 1:4
>
> # I can get the element by doing
>
> get(test)[2]
>
> # Now I want to assign a value to the second element of this dynamical
> variable.
>
> get(test)[2] <- 5  # doesn't work.
>
> Thanks a lot.
>
> Jun Shen
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From ruipbarradas at sapo.pt  Wed Jul  8 19:06:44 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Wed, 08 Jul 2015 18:06:44 +0100
Subject: [R] select a subset of a matrix which one of its column meet a
 condition
In-Reply-To: <CAMqbV1DVhhPu1Laano=PhWzuV0auFz9etCwYFoAst3_xy7qcng@mail.gmail.com>
References: <CAMqbV1DVhhPu1Laano=PhWzuV0auFz9etCwYFoAst3_xy7qcng@mail.gmail.com>
Message-ID: <559D58A4.8030501@sapo.pt>

Hello,

Your matrix has only 4 columns but you refer to met[,5]. I'll assume 
you're refering to the last column, met[,4]. Try reading the help page 
for ?%in%, it doesn't do what you seem to think it does. And try using <=.

maf <- met[0 <= met[, 4] & met[, 4] <= 0.05, ]

Hope this helps,

Rui Barradas

Em 08-07-2015 16:37, Lida Zeighami escreveu:
> Hi there,
>
> I have a matrix and I want to get a subset from that which one of its
> matrix meet a condition,
>
> my matrix is
> met
>       Row.names             Name
> maf                                   caf
> 1 10:100003915      10:1000039              0.0003782148
> 0.0003782148
> 2 10:100008738      10:100008738           0.0003759398
>   0.0003759398
> 3 10:100011321      10:100011321           0.0003762227
>   0.0003762227
> 4 10:100012219      10:100012219           0.0007518797
> 0.0007518797
> 5 10:100013325      10:100013325          0.0000000000
>   0.0000000000
> 6 10:100015404      10:100015404          0.0000000000
> 0.0000000000
>
> I want to choose a subset from "met" which maf values are between 0 and
> 0.05 (0,0.05)
>
> I wrote this code, but seem doesn't work properly:
>
>> maf<- met[which(c(met[,5]) %in% c(0,0.05)),]
>
> the dim(maf)  is 0 so seems my code didn't work!
> would please let me know how to correct it?
>
> Thanks
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From wdunlap at tibco.com  Wed Jul  8 19:28:02 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 8 Jul 2015 10:28:02 -0700
Subject: [R] How to assign value to a variable dynamically constructed
In-Reply-To: <CAFEqCdyF82JcOQ7p4UmaH7S1wS81M7tBPuPT1sz9Loqck2jJdA@mail.gmail.com>
References: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
	<CAFEqCdyF82JcOQ7p4UmaH7S1wS81M7tBPuPT1sz9Loqck2jJdA@mail.gmail.com>
Message-ID: <CAF8bMcZyBF1RgVvC=M_iU+QToBK0_FuDozTw7w0qk70WZ4BLaA@mail.gmail.com>

You can use an environment instead of a list using the same [[ syntax.  It
is like 'get0(..., inherit=FALSE)' on the left side of the <- and like
'assign(...)' on the right side.   E.g.,
   myData <- new.env()
   varName <- "v1"
   myData[[varName]] <- 1:10
   myData[[varName]][4] <- myData[[varName]][4] * 100
   myData[[varName]]
   #  [1]   1   2   3 400   5   6   7   8   9  10
   names(myData)
   # [1] "v1"
(Before R-3.2.0 or so, you had to use objects(myData,all=TRUE) if
myData was an environment and names(myData) if it was a list.  Now
names() works for environments.)

It is better to use a dedicated environment (or list) for each set of
related
variables so that name collisions do not cause problems.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jul 8, 2015 at 10:06 AM, Greg Snow <538280 at gmail.com> wrote:

> This is FAQ 7.21.
>
> The most important part of the answer in FAQ 7.21 is the last section
> where it states that it is often easier to use a list rather than
> messing around with trying to dynamically name global variables.
>
> If you tell us what you are trying to accomplish then we may have
> better advice.  The route you are headed down now usually leads to
> inefficient code and hard to find bugs.
>
> On Tue, Jul 7, 2015 at 2:53 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> > Dear list,
> >
> > Let's say we have a variable (id), whose name is dynamically constructed.
> > This variable represents a vector or data frame with many elements. Now I
> > want to specifically assign a value to one of the elements. I couldn't
> get
> > it right.
> >
> > test <- 'id' # "id" is dynamically constructed through paste()
> >
> > id <- 1:4
> >
> > # I can get the element by doing
> >
> > get(test)[2]
> >
> > # Now I want to assign a value to the second element of this dynamical
> > variable.
> >
> > get(test)[2] <- 5  # doesn't work.
> >
> > Thanks a lot.
> >
> > Jun Shen
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From markleeds2 at gmail.com  Wed Jul  8 22:33:48 2015
From: markleeds2 at gmail.com (Mark Leeds)
Date: Wed, 8 Jul 2015 16:33:48 -0400
Subject: [R] norm's book
Message-ID: <CAHz+bWbPa=YeP5-zR+Fm2xbWVWaHoix6CbJCUSOftYwQDQz8cg@mail.gmail.com>

Hi All: In case anyone is interested,  Norm's new book, "parallel computing
for data science" is out on amazon.  It already got raving reviews from
Dave Giles  who runs a popular econometrics blog.


Mark

	[[alternative HTML version deleted]]


From annemarie_dh at hotmail.com  Wed Jul  8 22:22:57 2015
From: annemarie_dh at hotmail.com (Annemarie Fischer)
Date: Wed, 8 Jul 2015 20:22:57 +0000
Subject: [R] Maxent Jarfile
Message-ID: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>

Hi,
I have been trying to solve the below problem for 2 days with no success. Hopefully you can help as i can find no assistance online.
I am attempting to run the niche.equivalency.test and the bg.similarity.test using RStudio and Maxent. I keep getting the error:
Error: Unable to access jarfile C:/ProgramError in file(fname, "r") : cannot open the connectionIn addition: Warning messages:1: running command 'java -jar C:/Program Files/R/R-3.1.3/library/dismo/java/maxent.jar -e  R.phyloclim.temp/background.csv -s  R.phyloclim.temp/samples.csv -j  R.phyloclim.temp/proj/ -o  R.phyloclim.temp/out/  -r removeduplicates nopictures autorun' had status 1 2: In file(fname, "r") :  cannot open file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or directory
I suspect the issue is that the file directory doesnt have "", but i have no idea how to add these in, as in RStudio values, the "" does appear.
My code is:
# load required packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)    library(rJava)library(maptools)library(rasterVis)library(phyloclim)
# path to MAXENT# --------------maxent.exe <- paste(system.file(package="dismo"),                     "/java/maxent.jar", sep = "")
# a data frame of coordinates where two species # have been detected ('presence points') and# a raster stack of environmental covariables# --------------------------------------
###Change to correct species usedfile <- paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv", sep="")# this is the file we will use:file
#save(file, file="Molossidae_rarefied_points.rda")
#data()#data(package = .packages(all.available = TRUE))
#myData <- read.csv("file", header=TRUE, nrows=10000)
Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
species <- c("Rhinolophus blasii", "Rhinolophus clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species, collapse = "|"), Rhinolophus_species$Spp), ]data.path <- system.file("extdata", package = "phyloclim")preds <- list.files(path = data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep = "/")preds <- stack(lapply(X = preds, FUN = raster))
# testing against 9 permutations of the data# -------------------------------------------reps <- 1000
# run hypothesis tests# --------------------if (file.exists(maxent.exe)){  net <- niche.equivalency.test(samples, preds, reps, maxent.exe)  net; plot(net)  bst <- bg.similarity.test(samples, preds, reps, app = maxent.exe)  bst; plot(bst)} else {  message("get a copy of MAXENT (see Details)")} 		 	   		  
	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Thu Jul  9 00:15:45 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 8 Jul 2015 14:15:45 -0800
Subject: [R] multiple graphs with a single legend and trellis graph
In-Reply-To: <0ADDFB05-FC82-46E9-868A-A5E1653F6516@gmail.com>
Message-ID: <79FB6E9835E.00000338jrkrideau@inbox.com>

Hi Rosa,
Neither the graph nor the data arrived.  R-help can be very fussy about what attached files it will accept. Usually pdf, txt and png, I think, will come through.

In any case it is much better to supply data using the dput() function See ?dput or have a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html for how to do this.

It is best to use dput() rather than supplying a data file because dput() ensures that we get an exact copy of the data as it exists on your computer.  There is no real guarantee that I will read in a data file the same way as you have.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: rosita21 at gmail.com
> Sent: Wed, 8 Jul 2015 13:11:34 +0100
> To: djmuser at gmail.com, drjimlemon at gmail.com, r-help at r-project.org
> Subject: Re: [R] multiple graphs with a single legend and trellis graph
> 
> Dear Jim,
> 
> first of all, thank you very much :)
> when I run the code:
> 
> myDF <- rbind(mse.alpha1, mse.alpha2)  # assumes both data frames have
> the same variables in the same order
> 
> 
> myDF$ID <- factor(rep(c("alpha1", "alpha2"), times =
> c(nrow(mse.alpha1), nrow(mse.alpha2))) )
> 
> 
> library(reshape2)
> myDF.melt <- melt(myDF, id = c("ID", "nsample", "lambda"), measure =
> c("mse.naive", "mse.RegCal", "mse.PL"))  #Melt the three columns to be
> plotted, something like
> 
> library(ggplot2)
> ggplot(myDF.melt, aes(x = lambda, y = value, color = variable)) +
>    geom_point() + geom_line() +
>    facet_grid(ID ~ nsample) +
>    labs(x = expression(paste(lambda)), y = "MSE", color = "Type?)
> 
> 
> 
>  I get the attached graph. I also attach my data, so you can see :)
> 
> I?m I able to resize the graphs differently? I.e. for alpha1:
> ylim=ylim=c(0,.6) and for alpha2: ylim=c(0,.17)?
> I reshaped, a new variable in sample was created, NA, Was it me that made
> something wrong?
> 
> I?m very naive and new in R :(
> 
> Thanks again ;)
> 
> 
> 
> 
> 
> 
> 
> Atenciosamente,
> Rosa Oliveira
> 
> --
> ____________________________________________________________________________
> 
> 
> Rosa Celeste dos Santos Oliveira,
> 
> E-mail: rosita21 at gmail.com
> Tlm: +351 939355143
> Linkedin: https://pt.linkedin.com/in/rosacsoliveira
> ____________________________________________________________________________
> "Many admire, few know"
> Hippocrates
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jrkrideau at inbox.com  Thu Jul  9 00:22:48 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 8 Jul 2015 14:22:48 -0800
Subject: [R] Maxent Jarfile
In-Reply-To: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>
Message-ID: <7A0B317AB6E.0000034Fjrkrideau@inbox.com>

Hi Annemarie,
You have sent the email in HTML and it is very close to  unreadable. Could you please resubmit the message in plain text.  R-help does not accept HTML and, as happened here, the text gets seriously mangled.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: annemarie_dh at hotmail.com
> Sent: Wed, 8 Jul 2015 20:22:57 +0000
> To: r-help at r-project.org
> Subject: [R] Maxent Jarfile
> 
> Hi,
> I have been trying to solve the below problem for 2 days with no success.
> Hopefully you can help as i can find no assistance online.
> I am attempting to run the niche.equivalency.test and the
> bg.similarity.test using RStudio and Maxent. I keep getting the error:
> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
> cannot open the connectionIn addition: Warning messages:1: running
> command 'java -jar C:/Program
> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
> R.phyloclim.temp/background.csv -s  R.phyloclim.temp/samples.csv -j
> R.phyloclim.temp/proj/ -o  R.phyloclim.temp/out/  -r removeduplicates
> nopictures autorun' had status 1 2: In file(fname, "r") :  cannot open
> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
> directory
> I suspect the issue is that the file directory doesnt have "", but i have
> no idea how to add these in, as in RStudio values, the "" does appear.
> My code is:
> # load required
packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
> # path to MAXENT# --------------maxent.exe <-
> paste(system.file(package="dismo"),
> "/java/maxent.jar", sep = "")
> # a data frame of coordinates where two species # have been detected
> ('presence points') and# a raster stack of environmental covariables#
> --------------------------------------
> ###Change to correct species usedfile <-
> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
> sep="")# this is the file we will use:file
> #save(file, file="Molossidae_rarefied_points.rda")
> #data()#data(package = .packages(all.available = TRUE))
> #myData <- read.csv("file", header=TRUE, nrows=10000)
> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
> species <- c("Rhinolophus blasii", "Rhinolophus
> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
> system.file("extdata", package = "phyloclim")preds <- list.files(path =
> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
> "/")preds <- stack(lapply(X = preds, FUN = raster))
> # testing against 9 permutations of the data#
> -------------------------------------------reps <- 1000
> # run hypothesis tests# --------------------if (file.exists(maxent.exe)){
> net <- niche.equivalency.test(samples, preds, reps, maxent.exe)  net;
> plot(net)  bst <- bg.similarity.test(samples, preds, reps, app =
> maxent.exe)  bst; plot(bst)} else {  message("get a copy of MAXENT (see
> Details)")}
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From marongiu.luigi at gmail.com  Thu Jul  9 01:22:10 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Thu, 9 Jul 2015 00:22:10 +0100
Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
 alternative for Lattice)
In-Reply-To: <000001d0b842$43905870$cab10950$@bigpond.com>
References: <CAMk+s2QqBw_Xjp+nUtxakFgRHU=EcqxyJ22taeq-t_wAKE4jyw@mail.gmail.com>
	<000001d0b842$43905870$cab10950$@bigpond.com>
Message-ID: <CAMk+s2TCoN=vFv1p37o42RDG2=3CBPR1cu2J_yQO78Q729EXuA@mail.gmail.com>

In relation to this question I have prepared a workable example. First
I prepare a dataframe with three variables (Cycle, Target, Rn), then I
plot the results with lattice's xyplot(). I won't use the scales but
only the labels and the panels are NOT indicated by the variable Well.
What I would need to use are instead the vectors row.name and col.name
that can identify each column and row of the plot.
Secondly I create replicates of the row.name and col.name in order to
fit the data and create a second dataframe, then I plot using lattice
extra's useOuterStrips().
However (a) I think the call is wrong anyway, (b) I obtain "Error:
length(dimx) == 2 is not TRUE" (c) I need a package on top of lattice
(d) I might introduce errors during the creation of the second
dataframe.
The requirements remains to create a strip on the top and left side of
the plot to allocate the elements of row.name and col.name possibly
using lattice only.
Thank you for your help.
Luigi

>>>
Line<-c(    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
  11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
 21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
41,    42,    43,    44,    45,    46,    47,    48,    49,    50,
51,    52,    53,    54,    55,    56,    57,    58,    59,    60,
61,    62,    63,    64,    65,    66,    67,    68,    69,    70,
71,    72,    73,    74,    75,    76,    77,    78,    79,    80,
81,    82,    83,    84,    85,    86,    87,    88,    89,    90,
91,    92,    93,    94,    95,    96,    97,    98,    99,    100,
101,    102,    103,    104,    105,    106,    107,    108,    109,
 110,    111,    112,    113,    114,    115,    116,    117,    118,
  119,    120,    121,    122,    123,    124,    125,    126,    127,
   128,    129,    130,    131,    132,    133,    134,    135,
136,    137,    138,    139,    140,    141,    142,    143,    144,
 145,    146,    147,    148,    149,    150,    151,    152,    153,
  154,    155,    156,    157,    158,    159,    160,    161,    162,
   163,    164,    165,    166,    167,    168,    169,    170,
171,    172,    173,    174,    175,    176,    177,    178,    179,
 180,    181,    182,    183,    184,    185,    186,    187,    188,
  189,    190,    191,    192,    193,    194,    195,    196,    197,
   198,    199,    200,    201,    202,    203,    204,    205,
206,    207,    208,    209,    210,    211,    212,    213,    214,
 215,    216,    217,    218,    219,    220,    221,    222,    223,
  224,    225,    226,    227,    228,    229,    230,    231,    232,
   233,    234,    235,    236,    237,    238,    239,    240,
241,    242,    243,    244,    245,    246,    247,    248,    249,
 250,    251,    252,    253,    254,    255,    256,    257,    258,
  259,    260,    261,    262,    263,    264,    265,    266,    267,
   268,    269,    270,    271,    272,    273,    274,    275,
276,    277,    278,    279,    280,    281,    282,    283,    284,
 285,    286,    287,    288,    289,    290,    291,    292,    293,
  294,    295,    296,    297,    298,    299,    300,    301,    302,
   303,    304,    305,    306,    307,    308,    309,    310,
311,    312,    313,    314,    315,    316,    317,    318,    319,
 320,    321,    322,    323,    324,    325,    326,    327,    328,
  329,    330,    331,    332,    333,    334,    335,    336,    337,
   338,    339,    340,    341,    342,    343,    344,    345,
346,    347,    348,    349,    350,    351,    352,    353,    354,
 355,    356,    357,    358,    359,    360,    361,    362,    363,
  364,    365,    366,    367,    368,    369,    370,    371,    372,
   373,    374,    375,    376,    377,    378,    379,    380,
381,    382,    383,    384,    385,    386,    387,    388,    389,
 390,    391,    392,    393,    394,    395,    396,    397,    398,
  399,    400,    401,    402,    403,    404,    405,    406,    407,
   408,    409,    410,    411,    412,    413,    414,    415,
416,    417,    418,    419,    420,    421,    422,    423,    424,
 425,    426,    427,    428,    429,    430,    431,    432,    433,
  434,    435,    436,    437,    438,    439,    440,    441,    442,
   443,    444,    445,    446,    447,    448,    449,    450,
451,    452,    453,    454,    455,    456,    457,    458,    459,
 460,    461,    462,    463,    464,    465,    466,    467,    468,
  469,    470,    471,    472,    473,    474,    475,    476,    477,
   478,    479,    480,    481,    482,    483,    484,    485,
486,    487,    488,    489,    490,    491,    492,    493,    494,
 495,    496,    497,    498,    499,    500,    501,    502,    503,
  504,    505,    506,    507,    508,    509,    510,    511,    512,
   513,    514,    515,    516,    517,    518,    519,    520,
521,    522,    523,    524,    525,    526,    527,    528,    529,
 530,    531,    532,    533,    534,    535,    536,    537,    538,
  539,    540)
Well<-c(    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
 1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
  1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
 2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
  2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
   2,    2,    2,    2,    2,    2,    2,    2,    2,    3,    3,
3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
 3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
  3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
   3,    3,    3,    3,    3,    3,    3,    4,    4,    4,    4,
4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
 4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
  4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
   4,    4,    4,    4,    4,    5,    5,    5,    5,    5,    5,
5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
 5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
  5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
   5,    5,    5,    6,    6,    6,    6,    6,    6,    6,    6,
6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
 6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
  6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
   6,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
 7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
  7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    8,
   8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
 8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
  8,    8,    8,    8,    8,    8,    8,    8,    8,    9,    9,    9,
   9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
 9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
  9,    9,    9,    9,    9,    9,    9,    10,    10,    10,    10,
 10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12)
Cycle<-c(    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
   11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
  21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
 31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
41,    42,    43,    44,    45,    1,    2,    3,    4,    5,    6,
7,    8,    9,    10,    11,    12,    13,    14,    15,    16,    17,
   18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
  28,    29,    30,    31,    32,    33,    34,    35,    36,    37,
 38,    39,    40,    41,    42,    43,    44,    45,    1,    2,
3,    4,    5,    6,    7,    8,    9,    10,    11,    12,    13,
14,    15,    16,    17,    18,    19,    20,    21,    22,    23,
24,    25,    26,    27,    28,    29,    30,    31,    32,    33,
34,    35,    36,    37,    38,    39,    40,    41,    42,    43,
44,    45,    1,    2,    3,    4,    5,    6,    7,    8,    9,
10,    11,    12,    13,    14,    15,    16,    17,    18,    19,
20,    21,    22,    23,    24,    25,    26,    27,    28,    29,
30,    31,    32,    33,    34,    35,    36,    37,    38,    39,
40,    41,    42,    43,    44,    45,    1,    2,    3,    4,    5,
 6,    7,    8,    9,    10,    11,    12,    13,    14,    15,    16,
   17,    18,    19,    20,    21,    22,    23,    24,    25,    26,
  27,    28,    29,    30,    31,    32,    33,    34,    35,    36,
 37,    38,    39,    40,    41,    42,    43,    44,    45,    1,
2,    3,    4,    5,    6,    7,    8,    9,    10,    11,    12,
13,    14,    15,    16,    17,    18,    19,    20,    21,    22,
23,    24,    25,    26,    27,    28,    29,    30,    31,    32,
33,    34,    35,    36,    37,    38,    39,    40,    41,    42,
43,    44,    45,    1,    2,    3,    4,    5,    6,    7,    8,
9,    10,    11,    12,    13,    14,    15,    16,    17,    18,
19,    20,    21,    22,    23,    24,    25,    26,    27,    28,
29,    30,    31,    32,    33,    34,    35,    36,    37,    38,
39,    40,    41,    42,    43,    44,    45,    1,    2,    3,    4,
  5,    6,    7,    8,    9,    10,    11,    12,    13,    14,    15,
   16,    17,    18,    19,    20,    21,    22,    23,    24,    25,
  26,    27,    28,    29,    30,    31,    32,    33,    34,    35,
 36,    37,    38,    39,    40,    41,    42,    43,    44,    45,
1,    2,    3,    4,    5,    6,    7,    8,    9,    10,    11,
12,    13,    14,    15,    16,    17,    18,    19,    20,    21,
22,    23,    24,    25,    26,    27,    28,    29,    30,    31,
32,    33,    34,    35,    36,    37,    38,    39,    40,    41,
42,    43,    44,    45,    1,    2,    3,    4,    5,    6,    7,
8,    9,    10,    11,    12,    13,    14,    15,    16,    17,
18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
28,    29,    30,    31,    32,    33,    34,    35,    36,    37,
38,    39,    40,    41,    42,    43,    44,    45,    1,    2,    3,
   4,    5,    6,    7,    8,    9,    10,    11,    12,    13,    14,
   15,    16,    17,    18,    19,    20,    21,    22,    23,    24,
  25,    26,    27,    28,    29,    30,    31,    32,    33,    34,
 35,    36,    37,    38,    39,    40,    41,    42,    43,    44,
45,    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
41,    42,    43,    44,    45)
Target <-c(
    rep("alpha",45),
    rep("beta", 45),
    rep("gamma", 45),
    rep("delta", 45),
    rep("epsilon", 45),
    rep("zeta", 45),
    rep("eta", 45),
    rep("theta", 45),
    rep("iota", 45),
    rep("kappa", 45),
    rep("lamba", 45),
    rep("mu", 45)
)
Rn<-c(    0.728,    0.735,    0.749,    0.758,    0.77,    0.778,
0.78,    0.784,    0.786,    0.785,    0.786,    0.786,    0.785,
0.785,    0.784,    0.783,    0.784,    0.786,    0.786,    0.787,
0.789,    0.786,    0.784,    0.786,    0.786,    0.785,    0.784,
0.785,    0.786,    0.784,    0.784,    0.78,    0.781,    0.779,
0.78,    0.78,    0.78,    0.781,    0.781,    0.78,    0.78,
0.781,    0.781,    0.78,    0.781,    0.695,    0.712,    0.751,
0.784,    0.81,    0.831,    0.852,    0.867,    0.877,    0.889,
0.896,    0.902,    0.908,    0.912,    0.912,    0.915,    0.919,
0.916,    0.918,    0.92,    0.917,    0.914,    0.917,    0.914,
0.914,    0.913,    0.913,    0.91,    0.908,    0.906,    0.902,
0.9,    0.901,    0.897,    0.896,    0.895,    0.896,    0.892,
0.89,    0.889,    0.89,    0.888,    0.889,    0.885,    0.886,
1.701,    1.702,    1.69,    1.678,    1.666,    1.65,    1.642,
1.632,    1.623,    1.616,    1.605,    1.598,    1.591,    1.582,
1.575,    1.568,    1.561,    1.556,    1.553,    1.549,    1.546,
1.541,    1.536,    1.531,    1.529,    1.526,    1.524,    1.522,
1.52,    1.517,    1.516,    1.514,    1.512,    1.512,    1.509,
1.509,    1.506,    1.505,    1.505,    1.508,    1.513,    1.508,
1.506,    1.507,    1.503,    0.761,    0.774,    0.797,    0.817,
0.833,    0.844,    0.85,    0.856,    0.864,    0.867,    0.869,
0.873,    0.874,    0.875,    0.873,    0.872,    0.872,    0.87,
0.866,    0.865,    0.864,    0.864,    0.86,    0.857,    0.855,
0.852,    0.851,    0.849,    0.845,    0.843,    0.842,    0.84,
0.835,    0.833,    0.83,    0.827,    0.825,    0.826,    0.824,
0.821,    0.82,    0.818,    0.817,    0.816,    0.813,    0.982,
0.988,    0.998,    1.009,    1.015,    1.018,    1.021,    1.023,
1.023,    1.02,    1.016,    1.015,    1.009,    1.005,    1.003,
1,    0.995,    0.989,    0.985,    0.981,    0.975,    0.969,
0.964,    0.96,    0.956,    0.952,    0.948,    0.944,    0.94,
0.935,    0.932,    0.927,    0.924,    0.921,    0.918,    0.915,
0.91,    0.907,    0.904,    0.901,    0.898,    0.896,    0.892,
0.889,    0.888,    1.14,    1.133,    1.117,    1.105,    1.096,
1.086,    1.074,    1.063,    1.052,    1.042,    1.033,    1.024,
1.015,    1.006,    0.999,    0.993,    0.987,    0.982,    0.975,
0.969,    0.965,    0.96,    0.955,    0.952,    0.947,    0.944,
0.943,    0.939,    0.935,    0.933,    0.93,    0.927,    0.925,
0.921,    0.919,    0.919,    0.917,    0.917,    0.915,    0.912,
0.912,    0.912,    0.909,    0.907,    0.907,    1.304,    1.31,
1.325,    1.338,    1.349,    1.355,    1.359,    1.36,    1.361,
1.362,    1.359,    1.353,    1.344,    1.335,    1.331,    1.323,
1.315,    1.308,    1.3,    1.292,    1.284,    1.276,    1.268,
1.262,    1.256,    1.25,    1.245,    1.241,    1.234,    1.228,
1.222,    1.216,    1.213,    1.208,    1.205,    1.2,    1.197,
1.192,    1.189,    1.186,    1.184,    1.182,    1.181,    1.178,
1.178,    0.802,    0.801,    0.801,    0.8,    0.799,    0.797,
0.794,    0.791,    0.785,    0.781,    0.777,    0.772,    0.766,
0.76,    0.756,    0.753,    0.751,    0.746,    0.742,    0.739,
0.735,    0.732,    0.728,    0.726,    0.725,    0.722,    0.718,
0.717,    0.716,    0.715,    0.71,    0.709,    0.711,    0.71,
0.709,    0.709,    0.709,    0.709,    0.708,    0.709,    0.71,
0.71,    0.711,    0.711,    0.712,    1.209,    1.206,    1.204,
1.202,    1.197,    1.186,    1.175,    1.165,    1.154,    1.143,
1.133,    1.12,    1.11,    1.105,    1.098,    1.091,    1.085,
1.078,    1.072,    1.067,    1.063,    1.054,    1.049,    1.048,
1.04,    1.036,    1.033,    1.029,    1.027,    1.024,    1.021,
1.019,    1.017,    1.013,    1.01,    1.008,    1.006,    1.005,
1.004,    1.002,    1.002,    1.001,    1,    0.998,    0.995,
2.936,    2.942,    2.946,    2.951,    2.956,    2.956,    2.968,
2.964,    2.953,    2.945,    2.939,    2.929,    2.919,    2.909,
2.902,    2.893,    2.882,    2.871,    2.857,    2.847,    2.835,
2.825,    2.819,    2.806,    2.795,    2.787,    2.781,    2.766,
2.761,    2.752,    2.749,    2.74,    2.731,    2.722,    2.718,
2.711,    2.705,    2.7,    2.693,    2.69,    2.686,    2.676,
2.672,    2.668,    2.667,    1.032,    1.033,    1.031,    1.033,
1.031,    1.029,    1.025,    1.02,    1.019,    1.016,    1.012,
1.008,    1.007,    1.011,    1.015,    1.032,    1.068,    1.124,
1.209,    1.327,    1.472,    1.632,    1.8,    1.971,    2.14,
2.302,    2.459,    2.612,    2.754,    2.886,    3.008,    3.122,
3.218,    3.306,    3.39,    3.472,    3.547,    3.613,    3.674,
3.731,    3.772,    3.81,    3.84,    3.86,    3.882,    0.808,
0.808,    0.808,    0.807,    0.805,    0.804,    0.802,    0.801,
0.798,    0.796,    0.794,    0.79,    0.788,    0.785,    0.781,
0.78,    0.777,    0.774,    0.772,    0.771,    0.769,    0.767,
0.767,    0.766,    0.766,    0.764,    0.764,    0.765,    0.762,
0.762,    0.76,    0.759,    0.759,    0.758,    0.758,    0.758,
0.756,    0.754,    0.754,    0.753,    0.754,    0.755,    0.753,
0.754,    0.753)

my.data <- as.data.frame(cbind(Line, Well, Target, Rn))

L <- c(6,2)
row.name <- c("A", "B")
col.name <- 1:6


library("lattice")
xyplot(Rn ~ Cycle | Well,
       data = my.data,
       groups = Well,
       ylab= "Y axis",
       xlab="X axis",
       main="Title",
       scales = list(
           x = list(draw = FALSE),
           y = list(draw = FALSE),
           relation="same"
       ),
       as.table = TRUE,
       layout = L,
       par.settings = list(
           strip.background=list(col="white"),
           axis.text = list(cex = 0.6),
           par.xlab.text = list(cex = 0.75),
           par.ylab.text = list(cex = 0.75),
           par.main.text = list(cex = 0.8),
           superpose.symbol = list(pch = ".", cex = 1)
       ),
       strip    = FALSE,
       type = "l",
       col = 3,
       panel = panel.superpose
)


ROW <- c(rep(row.name[1], 45*6), rep(row.name[2], 45*6))
CO <- c(rep(col.name[1], 45),
        rep(col.name[2], 45),
        rep(col.name[3], 45),
        rep(col.name[4], 45),
        rep(col.name[5], 45),
        rep(col.name[6], 45)
)
COL <- rep(CO,2)
new.data <- cbind(my.data, ROW, COL)
head(new.data, 200)

useOuterStrips(
    xyplot(Rn ~ Cycle | Well,
       data = my.data,
       groups = Well,
       ylab= "Y axis",
       xlab="X axis",
       main="Title",
       scales = list(
           x = list(draw = FALSE),
           y = list(draw = FALSE),
           relation="same"
       ),
       as.table = TRUE,
       layout = L,
       par.settings = list(
           strip.background=list(col="white"),
           axis.text = list(cex = 0.6),
           par.xlab.text = list(cex = 0.75),
           par.ylab.text = list(cex = 0.75),
           par.main.text = list(cex = 0.8),
           superpose.symbol = list(pch = ".", cex = 1)
       ),
       strip    = FALSE,
       type = "l",
       col = 3,
       panel = panel.superpose
),
c(ROW,COL))

On Tue, Jul 7, 2015 at 12:19 AM, Duncan Mackay <dulcalma at bigpond.com> wrote:
> Hi Luigi
>
> Not exactly sure what you want
>
> Have a look at
> https://stat.ethz.ch/pipermail/r-help/2007-May/132785.html
> and
> https://stat.ethz.ch/pipermail/r-help/2007-July/135551.html
>
> otherwise have a look at ?trellis.focus and
> https://stat.ethz.ch/pipermail/r-help/2006-July/109585.html
>
> failing that ?gridRect and ?gridText from library(grid)
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
> Marongiu
> Sent: Monday, 6 July 2015 09:56
> To: r-help
> Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
> alternative for Lattice)
>
> Dear all,
> I would like to add an outer strip or something like that on a lattice
> plot I am making. Such plot contains 384 cells and, since I am not
> interested in the axis values, I set:
>            scales = list(
>                x = list(draw = FALSE),
>                y = list(draw = FALSE),
>                relation="same"
>                ),
> on a xyplot from the LATTICE package.
> Nevertheless there are axis labels which run like:
>            ylab= "Y axis",
>            xlab= "X axis",
> I would like to place some more information regarding the individual
> cells thus I would like to draw a sort of extra axis labels that are
> similar to the outer strip of the LATTICE_EXTRA package, that is
> markers placed between the axis labels and the axis values and
> centered for each cells, typically placed on the top and left sides of
> the plot. This is performed by the useOuterStrips function but:
> a) LatticeExtra is not in the CRAN repository thus I have to install
> it through a more laborious approach which makes LatticeExtra less
> direct than Lattice
> b)  useOuterStrips uses information directly from the data whereas I
> will have to provide the extra information from ad hoc vectors not
> present in the data set.
>
> The question therefore is: is there a way to write text from a vector
> in the top and left corners of a lattice xyplot and place the
> individual elements at the centre of the rows and columns that compose
> the graph?
>
> Many thanks,
> Luigi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From mccormack at molbio.mgh.harvard.edu  Thu Jul  9 02:00:39 2015
From: mccormack at molbio.mgh.harvard.edu (Matthew)
Date: Wed, 08 Jul 2015 20:00:39 -0400
Subject: [R] tcltk2 entry box
Message-ID: <559DB9A7.40202@molbio.mgh.harvard.edu>

Is anyone familiar enough with the tcltk2 package to know if it is 
possible to have an entry box where a user can enter information (such 
as a path to a file or a number) and then be able to use the entered 
information downstream in a R script ?

The idea is for someone unfamiliar with R to just start an R script that 
would take care of all the commands for them so all they have to do is 
get the script started. However, there is always a couple of pieces of 
information that will change each time the script is used (for example, 
a different file will be processed by the script). So, I would like a 
way for the user to input that information as the script ran.

Matthew McCormack


From jfox at mcmaster.ca  Thu Jul  9 02:37:38 2015
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 8 Jul 2015 20:37:38 -0400
Subject: [R] tcltk2 entry box
In-Reply-To: <559DB9A7.40202@molbio.mgh.harvard.edu>
References: <559DB9A7.40202@molbio.mgh.harvard.edu>
Message-ID: <004f01d0b9df$751d89a0$5f589ce0$@mcmaster.ca>

Dear Matthew,

For file selection, see ?tcltk::tk_choose.files or ?tcltk::tkgetOpenFile . 

You could enter a number in a tk entry widget, but, depending upon the
nature of the number, a slider or other widget might be a better choice. 

For a variety of helpful tcltk examples see
<http://www.sciviews.org/_rgui/tcltk/>, originally by James Wettenhall but
now maintained by Philippe Grosjean (the author of the tcltk2 package). (You
probably don't need tcltk2 for the simple operations that you mention, but
see ?tk2spinbox for an alternative to a slider.)

Best,
 John

-----------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.socsci.mcmaster.ca/jfox/




> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matthew
> Sent: July-08-15 8:01 PM
> To: r-help
> Subject: [R] tcltk2 entry box
> 
> Is anyone familiar enough with the tcltk2 package to know if it is
> possible to have an entry box where a user can enter information (such
> as a path to a file or a number) and then be able to use the entered
> information downstream in a R script ?
> 
> The idea is for someone unfamiliar with R to just start an R script that
> would take care of all the commands for them so all they have to do is
> get the script started. However, there is always a couple of pieces of
> information that will change each time the script is used (for example,
> a different file will be processed by the script). So, I would like a
> way for the user to input that information as the script ran.
> 
> Matthew McCormack
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From andreas.leha at med.uni-goettingen.de  Thu Jul  9 02:41:46 2015
From: andreas.leha at med.uni-goettingen.de (Andreas Leha)
Date: Thu, 9 Jul 2015 01:41:46 +0100
Subject: [R] why must a named colClasses in read.table be in correct order
Message-ID: <olubnfmqi0l.fsf@med.uni-goettingen.de>

Hi all,

Apparently, the colClasses argument to read.table needs to be in the
order of the columns *even when it is named*.  Why is that?  And where
would I find it in the documentation?

Here is a MWE:

--8<---------------cut here---------------start------------->8---
kkk <- c("a\tb",
         "3.14\tx")
read.table(textConnection(kkk),
           sep="\t",
           header = TRUE)

cclasses=c(b="character",
           a="numeric")

read.table(textConnection(kkk),
           sep="\t",
           header = TRUE,
           colClasses = cclasses)              ## <--- error

read.table(textConnection(kkk),
           sep="\t",
           header = TRUE,
           colClasses = cclasses[order(names(cclasses))])
--8<---------------cut here---------------end--------------->8---


Thanks,
Andreas


From lutipilotto at yahoo.com.br  Thu Jul  9 02:43:10 2015
From: lutipilotto at yahoo.com.br (Luciane Maria Pilotto)
Date: Wed, 8 Jul 2015 17:43:10 -0700
Subject: [R] clm funtion and CI
Message-ID: <1436402590.60787.YahooMailBasic@web120203.mail.ne1.yahoo.com>

Hi,

I'm working with ordinal logistic regression and fitting the model with the "clm" funtion of the ordinal package and would like to get the CI. According to the Tutorial on fitting Cumulative Link Models with the ordinal Package, Rune Haubo B Christensen (21 January 2015) you can run the OR, but not CI. The same happens with the "clm2" for partial proportional odds.

I appreciate any help !!


Luciane


From andreas.leha at med.uni-goettingen.de  Thu Jul  9 03:42:24 2015
From: andreas.leha at med.uni-goettingen.de (Andreas Leha)
Date: Thu, 09 Jul 2015 02:42:24 +0100
Subject: [R] why must a named colClasses in read.table be in correct
	order
In-Reply-To: <CAFDcVCTY5ysNc_BK+B2eXnpqz9QZ8o6woVepwucTzYQOC4fWuA@mail.gmail.com>
References: <olubnfmqi0l.fsf@med.uni-goettingen.de>
	<CAFDcVCTY5ysNc_BK+B2eXnpqz9QZ8o6woVepwucTzYQOC4fWuA@mail.gmail.com>
Message-ID: <559DD180.6000200@med.uni-goettingen.de>

Hi Henrik,

Thanks for your reply.

I am not (yet) convinced, though.  The help page for read.table
mentions named colClasses and if I specify colClasses for not all
columns, the names are taken into account:

--8<---------------cut here---------------start------------->8---
kkk <- c("a\tb",
         "3.14\tx")
str(read.table(textConnection(kkk),
           sep="\t",
               header = TRUE))

str(read.table(textConnection(kkk),
               sep="\t",
               header = TRUE,
               colClasses=c(b="character")))
--8<---------------cut here---------------end--------------->8---

What am I missing?

Best,
Andreas



On 09/07/2015 02:21, Henrik Bengtsson wrote:
> read.table() does not make use of names(colClasses) - only its values.
> Because of this, ordering is critical, as you noted. It shouldn't be
> too hard to add support for a named `colClasses` argument of
> utils::read.table(), but someone needs to convince the R core team
> that this is a good idea.
> 
> As an alternative, see R.filesets::readDataFrame() for a
> read.table()-like function that matches names(colClasses) to column
> names, if they exists.
> 
> /Henrik
> (author of R.filesets)
> 
> On Wed, Jul 8, 2015 at 5:41 PM, Andreas Leha
> <andreas.leha at med.uni-goettingen.de> wrote:
>> Hi all,
>>
>> Apparently, the colClasses argument to read.table needs to be in the
>> order of the columns *even when it is named*.  Why is that?  And where
>> would I find it in the documentation?
>>
>> Here is a MWE:
>>
>> --8<---------------cut here---------------start------------->8---
>> kkk <- c("a\tb",
>>          "3.14\tx")
>> read.table(textConnection(kkk),
>>            sep="\t",
>>            header = TRUE)
>>
>> cclasses=c(b="character",
>>            a="numeric")
>>
>> read.table(textConnection(kkk),
>>            sep="\t",
>>            header = TRUE,
>>            colClasses = cclasses)              ## <--- error
>>
>> read.table(textConnection(kkk),
>>            sep="\t",
>>            header = TRUE,
>>            colClasses = cclasses[order(names(cclasses))])
>> --8<---------------cut here---------------end--------------->8---
>>
>>
>> Thanks,
>> Andreas
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From mccormack at molbio.mgh.harvard.edu  Thu Jul  9 03:58:45 2015
From: mccormack at molbio.mgh.harvard.edu (Matthew McCormack)
Date: Wed, 08 Jul 2015 21:58:45 -0400
Subject: [R] tcltk2 entry box
In-Reply-To: <004f01d0b9df$751d89a0$5f589ce0$@mcmaster.ca>
References: <559DB9A7.40202@molbio.mgh.harvard.edu>
	<004f01d0b9df$751d89a0$5f589ce0$@mcmaster.ca>
Message-ID: <559DD555.2020506@molbio.mgh.harvard.edu>

Wow !  Very nice.  Thank you very much, John.  This is very helpful and 
just what I need.
Yes, I can see that I should have paid attention to tcltk before going 
to tcltk2.

Matthew

On 7/8/2015 8:37 PM, John Fox wrote:
> Dear Matthew,
>
> For file selection, see ?tcltk::tk_choose.files or ?tcltk::tkgetOpenFile .
>
> You could enter a number in a tk entry widget, but, depending upon the
> nature of the number, a slider or other widget might be a better choice.
>
> For a variety of helpful tcltk examples see
> <http://www.sciviews.org/_rgui/tcltk/>, originally by James Wettenhall but
> now maintained by Philippe Grosjean (the author of the tcltk2 package). (You
> probably don't need tcltk2 for the simple operations that you mention, but
> see ?tk2spinbox for an alternative to a slider.)
>
> Best,
>   John
>
> -----------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.socsci.mcmaster.ca/jfox/
>
>
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matthew
>> Sent: July-08-15 8:01 PM
>> To: r-help
>> Subject: [R] tcltk2 entry box
>>
>> Is anyone familiar enough with the tcltk2 package to know if it is
>> possible to have an entry box where a user can enter information (such
>> as a path to a file or a number) and then be able to use the entered
>> information downstream in a R script ?
>>
>> The idea is for someone unfamiliar with R to just start an R script that
>> would take care of all the commands for them so all they have to do is
>> get the script started. However, there is always a couple of pieces of
>> information that will change each time the script is used (for example,
>> a different file will be processed by the script). So, I would like a
>> way for the user to input that information as the script ran.
>>
>> Matthew McCormack
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ---
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/antivirus
>


From henrik.bengtsson at ucsf.edu  Thu Jul  9 04:54:01 2015
From: henrik.bengtsson at ucsf.edu (Henrik Bengtsson)
Date: Wed, 8 Jul 2015 19:54:01 -0700
Subject: [R] why must a named colClasses in read.table be in correct
	order
In-Reply-To: <559DD180.6000200@med.uni-goettingen.de>
References: <olubnfmqi0l.fsf@med.uni-goettingen.de>
	<CAFDcVCTY5ysNc_BK+B2eXnpqz9QZ8o6woVepwucTzYQOC4fWuA@mail.gmail.com>
	<559DD180.6000200@med.uni-goettingen.de>
Message-ID: <CAFDcVCQ_kajJEw-xdWEkm7csQn7gWiqC5VvoBYoSKQeFA8jkoA@mail.gmail.com>

Thanks for insisting; I was wrong and I'm happy to see that there is
indeed code intended for named 'colClasses', which even goes back to
2004.   But as you report, then names only work when
length(colClasses) < cols (which also explains why I though it was not
supported).  I'm not sure if that _strictly less than_  test is
intentional or a mistake, but I would propose the following patch:

[HB-X201]{hb}: svn diff src\library\utils\R\readtable.R
Index: src/library/utils/R/readtable.R
===================================================================
--- src/library/utils/R/readtable.R     (revision 68642)
+++ src/library/utils/R/readtable.R     (working copy)
@@ -139,7 +139,7 @@
     if (rlabp) col.names <- c("row.names", col.names)

     nmColClasses <- names(colClasses)
-    if(length(colClasses) < cols)
+    if(length(colClasses) <= cols)
         if(is.null(nmColClasses)) {
             colClasses <- rep_len(colClasses, cols)
         } else {


Your example works with this patch.  I've made it source():able so you
can try it out (if you cannot source() https://, then download the
file an source it locally):

source("https://gist.githubusercontent.com/HenrikBengtsson/ed1eeb41a1b4d6c43b47/raw/ebe58f76e518dd014423bea466a5c93d2efd3c99/readtable-fix.R")

kkk <- c("a\tb",
         "3.14\tx")

colClasses <- c(a="numeric", b="character")
data <- read.table(textConnection(kkk),
                   sep="\t",
                   header = TRUE,
                   colClasses = colClasses)
str(data)
### 'data.frame':   1 obs. of  2 variables:
### $ a: num 3.14
### $ b: chr "x"

## Does not work with utils::read.table(), but with patch
data <- read.table(textConnection(kkk),
                   sep="\t",
                   header = TRUE,
                   colClasses = rev(colClasses))
str(data)
### 'data.frame':   1 obs. of  2 variables:
### $ a: num 3.14
### $ b: chr "x"

Let's hope that the above is a (10-year old) typo, and changing a < to
a <= adds support for named 'colClasses', which is a really useful
functionality.

/Henrik

On Wed, Jul 8, 2015 at 6:42 PM, Andreas Leha
<andreas.leha at med.uni-goettingen.de> wrote:
> Hi Henrik,
>
> Thanks for your reply.
>
> I am not (yet) convinced, though.  The help page for read.table
> mentions named colClasses and if I specify colClasses for not all
> columns, the names are taken into account:
>
> --8<---------------cut here---------------start------------->8---
> kkk <- c("a\tb",
>          "3.14\tx")
> str(read.table(textConnection(kkk),
>            sep="\t",
>                header = TRUE))
>
> str(read.table(textConnection(kkk),
>                sep="\t",
>                header = TRUE,
>                colClasses=c(b="character")))
> --8<---------------cut here---------------end--------------->8---
>
> What am I missing?
>
> Best,
> Andreas
>
>
>
> On 09/07/2015 02:21, Henrik Bengtsson wrote:
>> read.table() does not make use of names(colClasses) - only its values.
>> Because of this, ordering is critical, as you noted. It shouldn't be
>> too hard to add support for a named `colClasses` argument of
>> utils::read.table(), but someone needs to convince the R core team
>> that this is a good idea.
>>
>> As an alternative, see R.filesets::readDataFrame() for a
>> read.table()-like function that matches names(colClasses) to column
>> names, if they exists.
>>
>> /Henrik
>> (author of R.filesets)
>>
>> On Wed, Jul 8, 2015 at 5:41 PM, Andreas Leha
>> <andreas.leha at med.uni-goettingen.de> wrote:
>>> Hi all,
>>>
>>> Apparently, the colClasses argument to read.table needs to be in the
>>> order of the columns *even when it is named*.  Why is that?  And where
>>> would I find it in the documentation?
>>>
>>> Here is a MWE:
>>>
>>> --8<---------------cut here---------------start------------->8---
>>> kkk <- c("a\tb",
>>>          "3.14\tx")
>>> read.table(textConnection(kkk),
>>>            sep="\t",
>>>            header = TRUE)
>>>
>>> cclasses=c(b="character",
>>>            a="numeric")
>>>
>>> read.table(textConnection(kkk),
>>>            sep="\t",
>>>            header = TRUE,
>>>            colClasses = cclasses)              ## <--- error
>>>
>>> read.table(textConnection(kkk),
>>>            sep="\t",
>>>            header = TRUE,
>>>            colClasses = cclasses[order(names(cclasses))])
>>> --8<---------------cut here---------------end--------------->8---
>>>
>>>
>>> Thanks,
>>> Andreas
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Thu Jul  9 04:56:13 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Thu, 9 Jul 2015 12:56:13 +1000
Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
	alternative for Lattice)
In-Reply-To: <CAMk+s2TCoN=vFv1p37o42RDG2=3CBPR1cu2J_yQO78Q729EXuA@mail.gmail.com>
References: <CAMk+s2QqBw_Xjp+nUtxakFgRHU=EcqxyJ22taeq-t_wAKE4jyw@mail.gmail.com>	<000001d0b842$43905870$cab10950$@bigpond.com>
	<CAMk+s2TCoN=vFv1p37o42RDG2=3CBPR1cu2J_yQO78Q729EXuA@mail.gmail.com>
Message-ID: <000401d0b9f2$d1ed7a10$75c86e30$@bigpond.com>

Hi Luigi

str(my.data)
'data.frame':   540 obs. of  4 variables:
 $ Line  : chr  "1" "2" "3" "4" ...
 $ Well  : chr  "1" "1" "1" "1" ...
 $ Target: chr  "alpha" "alpha" "alpha" "alpha" ...
 $ Rn    : chr  "0.728" "0.735" "0.749" "0.758" ...

You didnot create the data.frame properly

mydata <-
data.frame(Line=Line, Well=Well, Target=Target, Rn=Rn,Cycle= Cycle)
str(mydata)
'data.frame':   540 obs. of  4 variables:
 $ Line  : num  1 2 3 4 5 6 7 8 9 10 ...
 $ Well  : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Target: chr  "alpha" "alpha" "alpha" "alpha" ...
 $ Rn    : num  0.728 0.735 0.749 0.758 0.77 0.778 0.78 0.784 0.786 0.785 ...

I was having problems with your script so I started with the basics (before I found the problem with the df)

    xyplot(Rn ~ Cycle | sprintf("%2d",Well), data = mydata,
           as.table = TRUE,
           layout = c(6,2),
           groups = Well)

You are not splitting up the data with groups being the same as the conditioning.
It can be necessary to use this setup in some cases but not this.

latticeExtra is on Cran so you can use install.packages("latticeExtra") or use the menu
I have never heard of lattice_Extra


    mydata$rown <- ifelse(mydata$Well>6,2,1)
    mydata$welln <- rep(1:6, 2)[sapply(mydata$Well, pmatch, 1:12)]


useOuterStrips(strip      = strip.custom(factor.levels = paste("column",1:6),
                                         par.strip.text = list(cex = 0.75)),
               strip.left = strip.custom(factor.levels = paste("row", 1:2),
                                         horizontal = FALSE,
                                         par.strip.text = list(cex = 0.75)),

    xyplot(Rn ~ Cycle | welln*rown, data = mydata,
           as.table = TRUE,
           layout = c(6,2)
          )

) ## useOuterStrips

  ps.Colours <-
  c("#000000","#FF0000","#00FF00","#0000FF","#FFA54F",
    "#00FFFF","#FF00FF","#C00000","#00B414","#FFD18F",
    "#00B2EE","#ffe5cc","#6A5ACD","#C0C0C0","#CCCCCC",
    "#6495ED","#FFFAFA")



useOuterStrips(strip      = strip.custom(factor.levels = paste("column",1:6),
                                         par.strip.text = list(cex = 0.75)),
               strip.left = strip.custom(factor.levels = paste("row", 1:2),
                                         horizontal = FALSE,
                                         par.strip.text = list(cex = 0.75)),

    xyplot(Rn ~ Cycle | welln*rown, data = mydata,
           as.table = TRUE,
           groups = Well,
           col = ps.Colours,
           layout = c(6,2)
          )

) ## useOuterStrips

Duncan

-----Original Message-----
From: Luigi Marongiu [mailto:marongiu.luigi at gmail.com] 
Sent: Thursday, 9 July 2015 09:22
To: Duncan Mackay; Dennis Murphy; r-help
Subject: Re: [R] add outer strip for levels in lattice plot (useOuterStrips alternative for Lattice)

In relation to this question I have prepared a workable example. First
I prepare a dataframe with three variables (Cycle, Target, Rn), then I
plot the results with lattice's xyplot(). I won't use the scales but
only the labels and the panels are NOT indicated by the variable Well.
What I would need to use are instead the vectors row.name and col.name
that can identify each column and row of the plot.
Secondly I create replicates of the row.name and col.name in order to
fit the data and create a second dataframe, then I plot using lattice
extra's useOuterStrips().
However (a) I think the call is wrong anyway, (b) I obtain "Error:
length(dimx) == 2 is not TRUE" (c) I need a package on top of lattice
(d) I might introduce errors during the creation of the second
dataframe.
The requirements remains to create a strip on the top and left side of
the plot to allocate the elements of row.name and col.name possibly
using lattice only.
Thank you for your help.
Luigi

>>>
Line<-c(    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
  11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
 21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
41,    42,    43,    44,    45,    46,    47,    48,    49,    50,
51,    52,    53,    54,    55,    56,    57,    58,    59,    60,
61,    62,    63,    64,    65,    66,    67,    68,    69,    70,
71,    72,    73,    74,    75,    76,    77,    78,    79,    80,
81,    82,    83,    84,    85,    86,    87,    88,    89,    90,
91,    92,    93,    94,    95,    96,    97,    98,    99,    100,
101,    102,    103,    104,    105,    106,    107,    108,    109,
 110,    111,    112,    113,    114,    115,    116,    117,    118,
  119,    120,    121,    122,    123,    124,    125,    126,    127,
   128,    129,    130,    131,    132,    133,    134,    135,
136,    137,    138,    139,    140,    141,    142,    143,    144,
 145,    146,    147,    148,    149,    150,    151,    152,    153,
  154,    155,    156,    157,    158,    159,    160,    161,    162,
   163,    164,    165,    166,    167,    168,    169,    170,
171,    172,    173,    174,    175,    176,    177,    178,    179,
 180,    181,    182,    183,    184,    185,    186,    187,    188,
  189,    190,    191,    192,    193,    194,    195,    196,    197,
   198,    199,    200,    201,    202,    203,    204,    205,
206,    207,    208,    209,    210,    211,    212,    213,    214,
 215,    216,    217,    218,    219,    220,    221,    222,    223,
  224,    225,    226,    227,    228,    229,    230,    231,    232,
   233,    234,    235,    236,    237,    238,    239,    240,
241,    242,    243,    244,    245,    246,    247,    248,    249,
 250,    251,    252,    253,    254,    255,    256,    257,    258,
  259,    260,    261,    262,    263,    264,    265,    266,    267,
   268,    269,    270,    271,    272,    273,    274,    275,
276,    277,    278,    279,    280,    281,    282,    283,    284,
 285,    286,    287,    288,    289,    290,    291,    292,    293,
  294,    295,    296,    297,    298,    299,    300,    301,    302,
   303,    304,    305,    306,    307,    308,    309,    310,
311,    312,    313,    314,    315,    316,    317,    318,    319,
 320,    321,    322,    323,    324,    325,    326,    327,    328,
  329,    330,    331,    332,    333,    334,    335,    336,    337,
   338,    339,    340,    341,    342,    343,    344,    345,
346,    347,    348,    349,    350,    351,    352,    353,    354,
 355,    356,    357,    358,    359,    360,    361,    362,    363,
  364,    365,    366,    367,    368,    369,    370,    371,    372,
   373,    374,    375,    376,    377,    378,    379,    380,
381,    382,    383,    384,    385,    386,    387,    388,    389,
 390,    391,    392,    393,    394,    395,    396,    397,    398,
  399,    400,    401,    402,    403,    404,    405,    406,    407,
   408,    409,    410,    411,    412,    413,    414,    415,
416,    417,    418,    419,    420,    421,    422,    423,    424,
 425,    426,    427,    428,    429,    430,    431,    432,    433,
  434,    435,    436,    437,    438,    439,    440,    441,    442,
   443,    444,    445,    446,    447,    448,    449,    450,
451,    452,    453,    454,    455,    456,    457,    458,    459,
 460,    461,    462,    463,    464,    465,    466,    467,    468,
  469,    470,    471,    472,    473,    474,    475,    476,    477,
   478,    479,    480,    481,    482,    483,    484,    485,
486,    487,    488,    489,    490,    491,    492,    493,    494,
 495,    496,    497,    498,    499,    500,    501,    502,    503,
  504,    505,    506,    507,    508,    509,    510,    511,    512,
   513,    514,    515,    516,    517,    518,    519,    520,
521,    522,    523,    524,    525,    526,    527,    528,    529,
 530,    531,    532,    533,    534,    535,    536,    537,    538,
  539,    540)
Well<-c(    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
 1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
  1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
 2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
  2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
   2,    2,    2,    2,    2,    2,    2,    2,    2,    3,    3,
3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
 3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
  3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
   3,    3,    3,    3,    3,    3,    3,    4,    4,    4,    4,
4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
 4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
  4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
   4,    4,    4,    4,    4,    5,    5,    5,    5,    5,    5,
5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
 5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
  5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
   5,    5,    5,    6,    6,    6,    6,    6,    6,    6,    6,
6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
 6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
  6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
   6,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
 7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
  7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    8,
   8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
 8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
  8,    8,    8,    8,    8,    8,    8,    8,    8,    9,    9,    9,
   9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
 9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
  9,    9,    9,    9,    9,    9,    9,    10,    10,    10,    10,
 10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
10,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
11,    11,    11,    11,    11,    11,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
12)
Cycle<-c(    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
   11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
  21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
 31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
41,    42,    43,    44,    45,    1,    2,    3,    4,    5,    6,
7,    8,    9,    10,    11,    12,    13,    14,    15,    16,    17,
   18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
  28,    29,    30,    31,    32,    33,    34,    35,    36,    37,
 38,    39,    40,    41,    42,    43,    44,    45,    1,    2,
3,    4,    5,    6,    7,    8,    9,    10,    11,    12,    13,
14,    15,    16,    17,    18,    19,    20,    21,    22,    23,
24,    25,    26,    27,    28,    29,    30,    31,    32,    33,
34,    35,    36,    37,    38,    39,    40,    41,    42,    43,
44,    45,    1,    2,    3,    4,    5,    6,    7,    8,    9,
10,    11,    12,    13,    14,    15,    16,    17,    18,    19,
20,    21,    22,    23,    24,    25,    26,    27,    28,    29,
30,    31,    32,    33,    34,    35,    36,    37,    38,    39,
40,    41,    42,    43,    44,    45,    1,    2,    3,    4,    5,
 6,    7,    8,    9,    10,    11,    12,    13,    14,    15,    16,
   17,    18,    19,    20,    21,    22,    23,    24,    25,    26,
  27,    28,    29,    30,    31,    32,    33,    34,    35,    36,
 37,    38,    39,    40,    41,    42,    43,    44,    45,    1,
2,    3,    4,    5,    6,    7,    8,    9,    10,    11,    12,
13,    14,    15,    16,    17,    18,    19,    20,    21,    22,
23,    24,    25,    26,    27,    28,    29,    30,    31,    32,
33,    34,    35,    36,    37,    38,    39,    40,    41,    42,
43,    44,    45,    1,    2,    3,    4,    5,    6,    7,    8,
9,    10,    11,    12,    13,    14,    15,    16,    17,    18,
19,    20,    21,    22,    23,    24,    25,    26,    27,    28,
29,    30,    31,    32,    33,    34,    35,    36,    37,    38,
39,    40,    41,    42,    43,    44,    45,    1,    2,    3,    4,
  5,    6,    7,    8,    9,    10,    11,    12,    13,    14,    15,
   16,    17,    18,    19,    20,    21,    22,    23,    24,    25,
  26,    27,    28,    29,    30,    31,    32,    33,    34,    35,
 36,    37,    38,    39,    40,    41,    42,    43,    44,    45,
1,    2,    3,    4,    5,    6,    7,    8,    9,    10,    11,
12,    13,    14,    15,    16,    17,    18,    19,    20,    21,
22,    23,    24,    25,    26,    27,    28,    29,    30,    31,
32,    33,    34,    35,    36,    37,    38,    39,    40,    41,
42,    43,    44,    45,    1,    2,    3,    4,    5,    6,    7,
8,    9,    10,    11,    12,    13,    14,    15,    16,    17,
18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
28,    29,    30,    31,    32,    33,    34,    35,    36,    37,
38,    39,    40,    41,    42,    43,    44,    45,    1,    2,    3,
   4,    5,    6,    7,    8,    9,    10,    11,    12,    13,    14,
   15,    16,    17,    18,    19,    20,    21,    22,    23,    24,
  25,    26,    27,    28,    29,    30,    31,    32,    33,    34,
 35,    36,    37,    38,    39,    40,    41,    42,    43,    44,
45,    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
41,    42,    43,    44,    45)
Target <-c(
    rep("alpha",45),
    rep("beta", 45),
    rep("gamma", 45),
    rep("delta", 45),
    rep("epsilon", 45),
    rep("zeta", 45),
    rep("eta", 45),
    rep("theta", 45),
    rep("iota", 45),
    rep("kappa", 45),
    rep("lamba", 45),
    rep("mu", 45)
)
Rn<-c(    0.728,    0.735,    0.749,    0.758,    0.77,    0.778,
0.78,    0.784,    0.786,    0.785,    0.786,    0.786,    0.785,
0.785,    0.784,    0.783,    0.784,    0.786,    0.786,    0.787,
0.789,    0.786,    0.784,    0.786,    0.786,    0.785,    0.784,
0.785,    0.786,    0.784,    0.784,    0.78,    0.781,    0.779,
0.78,    0.78,    0.78,    0.781,    0.781,    0.78,    0.78,
0.781,    0.781,    0.78,    0.781,    0.695,    0.712,    0.751,
0.784,    0.81,    0.831,    0.852,    0.867,    0.877,    0.889,
0.896,    0.902,    0.908,    0.912,    0.912,    0.915,    0.919,
0.916,    0.918,    0.92,    0.917,    0.914,    0.917,    0.914,
0.914,    0.913,    0.913,    0.91,    0.908,    0.906,    0.902,
0.9,    0.901,    0.897,    0.896,    0.895,    0.896,    0.892,
0.89,    0.889,    0.89,    0.888,    0.889,    0.885,    0.886,
1.701,    1.702,    1.69,    1.678,    1.666,    1.65,    1.642,
1.632,    1.623,    1.616,    1.605,    1.598,    1.591,    1.582,
1.575,    1.568,    1.561,    1.556,    1.553,    1.549,    1.546,
1.541,    1.536,    1.531,    1.529,    1.526,    1.524,    1.522,
1.52,    1.517,    1.516,    1.514,    1.512,    1.512,    1.509,
1.509,    1.506,    1.505,    1.505,    1.508,    1.513,    1.508,
1.506,    1.507,    1.503,    0.761,    0.774,    0.797,    0.817,
0.833,    0.844,    0.85,    0.856,    0.864,    0.867,    0.869,
0.873,    0.874,    0.875,    0.873,    0.872,    0.872,    0.87,
0.866,    0.865,    0.864,    0.864,    0.86,    0.857,    0.855,
0.852,    0.851,    0.849,    0.845,    0.843,    0.842,    0.84,
0.835,    0.833,    0.83,    0.827,    0.825,    0.826,    0.824,
0.821,    0.82,    0.818,    0.817,    0.816,    0.813,    0.982,
0.988,    0.998,    1.009,    1.015,    1.018,    1.021,    1.023,
1.023,    1.02,    1.016,    1.015,    1.009,    1.005,    1.003,
1,    0.995,    0.989,    0.985,    0.981,    0.975,    0.969,
0.964,    0.96,    0.956,    0.952,    0.948,    0.944,    0.94,
0.935,    0.932,    0.927,    0.924,    0.921,    0.918,    0.915,
0.91,    0.907,    0.904,    0.901,    0.898,    0.896,    0.892,
0.889,    0.888,    1.14,    1.133,    1.117,    1.105,    1.096,
1.086,    1.074,    1.063,    1.052,    1.042,    1.033,    1.024,
1.015,    1.006,    0.999,    0.993,    0.987,    0.982,    0.975,
0.969,    0.965,    0.96,    0.955,    0.952,    0.947,    0.944,
0.943,    0.939,    0.935,    0.933,    0.93,    0.927,    0.925,
0.921,    0.919,    0.919,    0.917,    0.917,    0.915,    0.912,
0.912,    0.912,    0.909,    0.907,    0.907,    1.304,    1.31,
1.325,    1.338,    1.349,    1.355,    1.359,    1.36,    1.361,
1.362,    1.359,    1.353,    1.344,    1.335,    1.331,    1.323,
1.315,    1.308,    1.3,    1.292,    1.284,    1.276,    1.268,
1.262,    1.256,    1.25,    1.245,    1.241,    1.234,    1.228,
1.222,    1.216,    1.213,    1.208,    1.205,    1.2,    1.197,
1.192,    1.189,    1.186,    1.184,    1.182,    1.181,    1.178,
1.178,    0.802,    0.801,    0.801,    0.8,    0.799,    0.797,
0.794,    0.791,    0.785,    0.781,    0.777,    0.772,    0.766,
0.76,    0.756,    0.753,    0.751,    0.746,    0.742,    0.739,
0.735,    0.732,    0.728,    0.726,    0.725,    0.722,    0.718,
0.717,    0.716,    0.715,    0.71,    0.709,    0.711,    0.71,
0.709,    0.709,    0.709,    0.709,    0.708,    0.709,    0.71,
0.71,    0.711,    0.711,    0.712,    1.209,    1.206,    1.204,
1.202,    1.197,    1.186,    1.175,    1.165,    1.154,    1.143,
1.133,    1.12,    1.11,    1.105,    1.098,    1.091,    1.085,
1.078,    1.072,    1.067,    1.063,    1.054,    1.049,    1.048,
1.04,    1.036,    1.033,    1.029,    1.027,    1.024,    1.021,
1.019,    1.017,    1.013,    1.01,    1.008,    1.006,    1.005,
1.004,    1.002,    1.002,    1.001,    1,    0.998,    0.995,
2.936,    2.942,    2.946,    2.951,    2.956,    2.956,    2.968,
2.964,    2.953,    2.945,    2.939,    2.929,    2.919,    2.909,
2.902,    2.893,    2.882,    2.871,    2.857,    2.847,    2.835,
2.825,    2.819,    2.806,    2.795,    2.787,    2.781,    2.766,
2.761,    2.752,    2.749,    2.74,    2.731,    2.722,    2.718,
2.711,    2.705,    2.7,    2.693,    2.69,    2.686,    2.676,
2.672,    2.668,    2.667,    1.032,    1.033,    1.031,    1.033,
1.031,    1.029,    1.025,    1.02,    1.019,    1.016,    1.012,
1.008,    1.007,    1.011,    1.015,    1.032,    1.068,    1.124,
1.209,    1.327,    1.472,    1.632,    1.8,    1.971,    2.14,
2.302,    2.459,    2.612,    2.754,    2.886,    3.008,    3.122,
3.218,    3.306,    3.39,    3.472,    3.547,    3.613,    3.674,
3.731,    3.772,    3.81,    3.84,    3.86,    3.882,    0.808,
0.808,    0.808,    0.807,    0.805,    0.804,    0.802,    0.801,
0.798,    0.796,    0.794,    0.79,    0.788,    0.785,    0.781,
0.78,    0.777,    0.774,    0.772,    0.771,    0.769,    0.767,
0.767,    0.766,    0.766,    0.764,    0.764,    0.765,    0.762,
0.762,    0.76,    0.759,    0.759,    0.758,    0.758,    0.758,
0.756,    0.754,    0.754,    0.753,    0.754,    0.755,    0.753,
0.754,    0.753)

my.data <- as.data.frame(cbind(Line, Well, Target, Rn))

L <- c(6,2)
row.name <- c("A", "B")
col.name <- 1:6


library("lattice")
xyplot(Rn ~ Cycle | Well,
       data = my.data,
       groups = Well,
       ylab= "Y axis",
       xlab="X axis",
       main="Title",
       scales = list(
           x = list(draw = FALSE),
           y = list(draw = FALSE),
           relation="same"
       ),
       as.table = TRUE,
       layout = L,
       par.settings = list(
           strip.background=list(col="white"),
           axis.text = list(cex = 0.6),
           par.xlab.text = list(cex = 0.75),
           par.ylab.text = list(cex = 0.75),
           par.main.text = list(cex = 0.8),
           superpose.symbol = list(pch = ".", cex = 1)
       ),
       strip    = FALSE,
       type = "l",
       col = 3,
       panel = panel.superpose
)


ROW <- c(rep(row.name[1], 45*6), rep(row.name[2], 45*6))
CO <- c(rep(col.name[1], 45),
        rep(col.name[2], 45),
        rep(col.name[3], 45),
        rep(col.name[4], 45),
        rep(col.name[5], 45),
        rep(col.name[6], 45)
)
COL <- rep(CO,2)
new.data <- cbind(my.data, ROW, COL)
head(new.data, 200)

useOuterStrips(
    xyplot(Rn ~ Cycle | Well,
       data = my.data,
       groups = Well,
       ylab= "Y axis",
       xlab="X axis",
       main="Title",
       scales = list(
           x = list(draw = FALSE),
           y = list(draw = FALSE),
           relation="same"
       ),
       as.table = TRUE,
       layout = L,
       par.settings = list(
           strip.background=list(col="white"),
           axis.text = list(cex = 0.6),
           par.xlab.text = list(cex = 0.75),
           par.ylab.text = list(cex = 0.75),
           par.main.text = list(cex = 0.8),
           superpose.symbol = list(pch = ".", cex = 1)
       ),
       strip    = FALSE,
       type = "l",
       col = 3,
       panel = panel.superpose
),
c(ROW,COL))

On Tue, Jul 7, 2015 at 12:19 AM, Duncan Mackay <dulcalma at bigpond.com> wrote:
> Hi Luigi
>
> Not exactly sure what you want
>
> Have a look at
> https://stat.ethz.ch/pipermail/r-help/2007-May/132785.html
> and
> https://stat.ethz.ch/pipermail/r-help/2007-July/135551.html
>
> otherwise have a look at ?trellis.focus and
> https://stat.ethz.ch/pipermail/r-help/2006-July/109585.html
>
> failing that ?gridRect and ?gridText from library(grid)
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home: mackay at northnet.com.au
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
> Marongiu
> Sent: Monday, 6 July 2015 09:56
> To: r-help
> Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
> alternative for Lattice)
>
> Dear all,
> I would like to add an outer strip or something like that on a lattice
> plot I am making. Such plot contains 384 cells and, since I am not
> interested in the axis values, I set:
>            scales = list(
>                x = list(draw = FALSE),
>                y = list(draw = FALSE),
>                relation="same"
>                ),
> on a xyplot from the LATTICE package.
> Nevertheless there are axis labels which run like:
>            ylab= "Y axis",
>            xlab= "X axis",
> I would like to place some more information regarding the individual
> cells thus I would like to draw a sort of extra axis labels that are
> similar to the outer strip of the LATTICE_EXTRA package, that is
> markers placed between the axis labels and the axis values and
> centered for each cells, typically placed on the top and left sides of
> the plot. This is performed by the useOuterStrips function but:
> a) LatticeExtra is not in the CRAN repository thus I have to install
> it through a more laborious approach which makes LatticeExtra less
> direct than Lattice
> b)  useOuterStrips uses information directly from the data whereas I
> will have to provide the extra information from ad hoc vectors not
> present in the data set.
>
> The question therefore is: is there a way to write text from a vector
> in the top and left corners of a lattice xyplot and place the
> individual elements at the centre of the rows and columns that compose
> the graph?
>
> Many thanks,
> Luigi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From andreas.leha at med.uni-goettingen.de  Thu Jul  9 05:15:16 2015
From: andreas.leha at med.uni-goettingen.de (Andreas Leha)
Date: Thu, 9 Jul 2015 04:15:16 +0100
Subject: [R] why must a named colClasses in read.table be in correct
	order
References: <olubnfmqi0l.fsf@med.uni-goettingen.de>
	<CAFDcVCTY5ysNc_BK+B2eXnpqz9QZ8o6woVepwucTzYQOC4fWuA@mail.gmail.com>
	<559DD180.6000200@med.uni-goettingen.de>
	<CAFDcVCQ_kajJEw-xdWEkm7csQn7gWiqC5VvoBYoSKQeFA8jkoA@mail.gmail.com>
Message-ID: <olu7fqaqawr.fsf@med.uni-goettingen.de>

Hi Henrik,

Thank you very much for looking into this.  And thanks for the patch!

Yes, let's hope this is a typo that gets fixed.

Regards,
Andreas

Henrik Bengtsson <henrik.bengtsson at ucsf.edu> writes:
> Thanks for insisting; I was wrong and I'm happy to see that there is
> indeed code intended for named 'colClasses', which even goes back to
> 2004.   But as you report, then names only work when
> length(colClasses) < cols (which also explains why I though it was not
> supported).  I'm not sure if that _strictly less than_  test is
> intentional or a mistake, but I would propose the following patch:
>
> [HB-X201]{hb}: svn diff src\library\utils\R\readtable.R
> Index: src/library/utils/R/readtable.R
> ===================================================================
> --- src/library/utils/R/readtable.R     (revision 68642)
> +++ src/library/utils/R/readtable.R     (working copy)
> @@ -139,7 +139,7 @@
>      if (rlabp) col.names <- c("row.names", col.names)
>
>      nmColClasses <- names(colClasses)
> -    if(length(colClasses) < cols)
> +    if(length(colClasses) <= cols)
>          if(is.null(nmColClasses)) {
>              colClasses <- rep_len(colClasses, cols)
>          } else {
>
>
> Your example works with this patch.  I've made it source():able so you
> can try it out (if you cannot source() https://, then download the
> file an source it locally):
>
> source("https://gist.githubusercontent.com/HenrikBengtsson/ed1eeb41a1b4d6c43b47/raw/ebe58f76e518dd014423bea466a5c93d2efd3c99/readtable-fix.R")
>
> kkk <- c("a\tb",
>          "3.14\tx")
>
> colClasses <- c(a="numeric", b="character")
> data <- read.table(textConnection(kkk),
>                    sep="\t",
>                    header = TRUE,
>                    colClasses = colClasses)
> str(data)
> ### 'data.frame':   1 obs. of  2 variables:
> ### $ a: num 3.14
> ### $ b: chr "x"
>
> ## Does not work with utils::read.table(), but with patch
> data <- read.table(textConnection(kkk),
>                    sep="\t",
>                    header = TRUE,
>                    colClasses = rev(colClasses))
> str(data)
> ### 'data.frame':   1 obs. of  2 variables:
> ### $ a: num 3.14
> ### $ b: chr "x"
>
> Let's hope that the above is a (10-year old) typo, and changing a < to
> a <= adds support for named 'colClasses', which is a really useful
> functionality.
>
> /Henrik
>
> On Wed, Jul 8, 2015 at 6:42 PM, Andreas Leha
> <andreas.leha at med.uni-goettingen.de> wrote:
>> Hi Henrik,
>>
>> Thanks for your reply.
>>
>> I am not (yet) convinced, though.  The help page for read.table
>> mentions named colClasses and if I specify colClasses for not all
>> columns, the names are taken into account:
>>
>> --8<---------------cut here---------------start------------->8---
>> kkk <- c("a\tb",
>>          "3.14\tx")
>> str(read.table(textConnection(kkk),
>>            sep="\t",
>>                header = TRUE))
>>
>> str(read.table(textConnection(kkk),
>>                sep="\t",
>>                header = TRUE,
>>                colClasses=c(b="character")))
>> --8<---------------cut here---------------end--------------->8---
>>
>> What am I missing?
>>
>> Best,
>> Andreas
>>
>>
>>
>> On 09/07/2015 02:21, Henrik Bengtsson wrote:
>>> read.table() does not make use of names(colClasses) - only its values.
>>> Because of this, ordering is critical, as you noted. It shouldn't be
>>> too hard to add support for a named `colClasses` argument of
>>> utils::read.table(), but someone needs to convince the R core team
>>> that this is a good idea.
>>>
>>> As an alternative, see R.filesets::readDataFrame() for a
>>> read.table()-like function that matches names(colClasses) to column
>>> names, if they exists.
>>>
>>> /Henrik
>>> (author of R.filesets)
>>>
>>> On Wed, Jul 8, 2015 at 5:41 PM, Andreas Leha
>>> <andreas.leha at med.uni-goettingen.de> wrote:
>>>> Hi all,
>>>>
>>>> Apparently, the colClasses argument to read.table needs to be in the
>>>> order of the columns *even when it is named*.  Why is that?  And where
>>>> would I find it in the documentation?
>>>>
>>>> Here is a MWE:
>>>>
>>>> --8<---------------cut here---------------start------------->8---
>>>> kkk <- c("a\tb",
>>>>          "3.14\tx")
>>>> read.table(textConnection(kkk),
>>>>            sep="\t",
>>>>            header = TRUE)
>>>>
>>>> cclasses=c(b="character",
>>>>            a="numeric")
>>>>
>>>> read.table(textConnection(kkk),
>>>>            sep="\t",
>>>>            header = TRUE,
>>>>            colClasses = cclasses)              ## <--- error
>>>>
>>>> read.table(textConnection(kkk),
>>>>            sep="\t",
>>>>            header = TRUE,
>>>>            colClasses = cclasses[order(names(cclasses))])
>>>> --8<---------------cut here---------------end--------------->8---
>>>>
>>>>
>>>> Thanks,
>>>> Andreas
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.


From mariana.rrari at comvoce.com  Thu Jul  9 00:50:39 2015
From: mariana.rrari at comvoce.com (Mariana Ferrari)
Date: Wed, 8 Jul 2015 19:50:39 -0300
Subject: [R] Para r-help
Message-ID: <722938301366216e58dc9fd700114b5b@comvoce.com>

Tudo bem?
Sou a Josie, achei voc? na internet e queria aproveitar para fazer o marketing 
do site onde estou trabalhando ;)
Pra mim ? o melhor lugar para fazer amigos, achar um relacionamento e at? um 
encontro assim... mais ?ntimo.

O link ? este: http://goo.gl/DWtKBg
(N?o se preocupe, n?o ? v?rus! Juro!)

Estou cadastrada l? como "josymel". Cadastre-se tamb?m para conversamos por 
l?.

Desculpa te mandar um email assim sem permiss?o ok?
Te encontro l?!

Bjs!!!
Josie



	[[alternative HTML version deleted]]


From johannes at huesing.name  Thu Jul  9 07:44:09 2015
From: johannes at huesing.name (Johannes Huesing)
Date: Thu, 9 Jul 2015 07:44:09 +0200
Subject: [R]  problem understanding grid coordinate systems
Message-ID: <20150709054409.GA31801@huesing.name>

According to "R Graphics" by Paul Murrell, the coordinates that were used for the
most recent lattics plot can be retrieved with "native" units.

I have difficulties to access these coordinates. The following code 
renders the following results:

> library(grid)
> library(lattice)
> pl <- xyplot(1:10 ~ 10:1)
> print(pl)
> convertX(unit(.9, "npc"), "native")
[1] 605.7native
> R.version
                _                           
platform       i686-pc-linux-gnu           
arch           i686
os             linux-gnu                   
system         i686, linux-gnu             
status         
major          3                           
minor          0.2
year           2013                        
month          09
day            25                          
svn rev        63987
language       R                           
version.string R version 3.0.2 (2013-09-25)
nickname       Frisbee Sailing

I had expected something around 9.5 in native coordinates. Which coordinate
system do I have to address instead?

--
Johannes H?sing


From annemariefischer86 at gmail.com  Thu Jul  9 07:25:06 2015
From: annemariefischer86 at gmail.com (annemariefischer86 at gmail.com)
Date: Thu, 09 Jul 2015 07:25:06 +0200
Subject: [R] Maxent Jarfile
In-Reply-To: <7A0B317AB6E.0000034Fjrkrideau@inbox.com>
References: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>
	<7A0B317AB6E.0000034Fjrkrideau@inbox.com>
Message-ID: <20150709052506.5849232.18416.844@gmail.com>

?Hi John,?

Sorry about that. Please find attached the code, error and input file.

Thanks,
Annemarie


? Original Message ?
From: John Kane
Sent: Thursday 9 July 2015 00:26
To: Annemarie Fischer; r-help at r-project.org
Subject: Re: [R] Maxent Jarfile

Hi Annemarie,
You have sent the email in HTML and it is very close to unreadable. Could you please resubmit the message in plain text. R-help does not accept HTML and, as happened here, the text gets seriously mangled.



John Kane
Kingston ON Canada


> -----Original Message-----
> From: annemarie_dh at hotmail.com
> Sent: Wed, 8 Jul 2015 20:22:57 +0000
> To: r-help at r-project.org
> Subject: [R] Maxent Jarfile
> 
> Hi,
> I have been trying to solve the below problem for 2 days with no success.
> Hopefully you can help as i can find no assistance online.
> I am attempting to run the niche.equivalency.test and the
> bg.similarity.test using RStudio and Maxent. I keep getting the error:
> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
> cannot open the connectionIn addition: Warning messages:1: running
> command 'java -jar C:/Program
> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
> R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
> R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
> nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
> directory
> I suspect the issue is that the file directory doesnt have "", but i have
> no idea how to add these in, as in RStudio values, the "" does appear.
> My code is:
> # load required
packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
> # path to MAXENT# --------------maxent.exe <-
> paste(system.file(package="dismo"),
> "/java/maxent.jar", sep = "")
> # a data frame of coordinates where two species # have been detected
> ('presence points') and# a raster stack of environmental covariables#
> --------------------------------------
> ###Change to correct species usedfile <-
> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
> sep="")# this is the file we will use:file
> #save(file, file="Molossidae_rarefied_points.rda")
> #data()#data(package = .packages(all.available = TRUE))
> #myData <- read.csv("file", header=TRUE, nrows=10000)
> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
> species <- c("Rhinolophus blasii", "Rhinolophus
> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
> system.file("extdata", package = "phyloclim")preds <- list.files(path =
> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
> "/")preds <- stack(lapply(X = preds, FUN = raster))
> # testing against 9 permutations of the data#
> -------------------------------------------reps <- 1000
> # run hypothesis tests# --------------------if (file.exists(maxent.exe)){
> net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
> plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
> maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
> Details)")}
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________


______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From marammagdysalem at gmx.com  Thu Jul  9 08:46:52 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Thu, 9 Jul 2015 08:46:52 +0200
Subject: [R] Can't post to the list
Message-ID: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>

Dear all, 
I need help with posting to the list. Whenever i post any message, i get this reply : the message content was explicitly allowed. 
Any help please? 
Thanks a lot.
Maram

Sent from my iPhone

From rni.boh at gmail.com  Thu Jul  9 09:30:14 2015
From: rni.boh at gmail.com (Bob O'Hara)
Date: Thu, 9 Jul 2015 09:30:14 +0200
Subject: [R] Maxent Jarfile
In-Reply-To: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>
References: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>
Message-ID: <CAN-Z0xW0JNdjsNgn0GMEs6VbFSLUCKFiyJAE2gk7_fjz+5Dkcw@mail.gmail.com>

You would do better to contact the package owner for this, as it's a
problem specific to the package. In this case the problem looks to be
in dismo, so you should use packageDescription("dismo") to discover
that Robert Hijmans is the person to contact. I would suggest running
maxent() first to see if it works. If it does then the problem is
elsewhere, so use find("niche.equivalency.test") to find out what
package that function is in, and email the maintainer of that package
for help.

Oh, and also read the posting guide:
<http://www.r-project.org/posting-guide.html> :-)

Bob
(who is not going to wait 15 hours to send this)

On 8 July 2015 at 22:22, Annemarie Fischer <annemarie_dh at hotmail.com> wrote:
> Hi,
> I have been trying to solve the below problem for 2 days with no success. Hopefully you can help as i can find no assistance online.
> I am attempting to run the niche.equivalency.test and the bg.similarity.test using RStudio and Maxent. I keep getting the error:
> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") : cannot open the connectionIn addition: Warning messages:1: running command 'java -jar C:/Program Files/R/R-3.1.3/library/dismo/java/maxent.jar -e  R.phyloclim.temp/background.csv -s  R.phyloclim.temp/samples.csv -j  R.phyloclim.temp/proj/ -o  R.phyloclim.temp/out/  -r removeduplicates nopictures autorun' had status 1 2: In file(fname, "r") :  cannot open file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or directory
> I suspect the issue is that the file directory doesnt have "", but i have no idea how to add these in, as in RStudio values, the "" does appear.
> My code is:
> # load required packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)    library(rJava)library(maptools)library(rasterVis)library(phyloclim)
> # path to MAXENT# --------------maxent.exe <- paste(system.file(package="dismo"),                     "/java/maxent.jar", sep = "")
> # a data frame of coordinates where two species # have been detected ('presence points') and# a raster stack of environmental covariables# --------------------------------------
> ###Change to correct species usedfile <- paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv", sep="")# this is the file we will use:file
> #save(file, file="Molossidae_rarefied_points.rda")
> #data()#data(package = .packages(all.available = TRUE))
> #myData <- read.csv("file", header=TRUE, nrows=10000)
> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
> species <- c("Rhinolophus blasii", "Rhinolophus clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species, collapse = "|"), Rhinolophus_species$Spp), ]data.path <- system.file("extdata", package = "phyloclim")preds <- list.files(path = data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep = "/")preds <- stack(lapply(X = preds, FUN = raster))
> # testing against 9 permutations of the data# -------------------------------------------reps <- 1000
> # run hypothesis tests# --------------------if (file.exists(maxent.exe)){  net <- niche.equivalency.test(samples, preds, reps, maxent.exe)  net; plot(net)  bst <- bg.similarity.test(samples, preds, reps, app = maxent.exe)  bst; plot(bst)} else {  message("get a copy of MAXENT (see Details)")}
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Bob O'Hara

Biodiversity and Climate Research Centre
Senckenberganlage 25
D-60325 Frankfurt am Main,
Germany

Tel: +49 69 798 40226
Mobile: +49 1515 888 5440
WWW:   http://www.bik-f.de/root/index.php?page_id=219
Blog: http://occamstypewriter.org/boboh/
Journal of Negative Results - EEB: www.jnr-eeb.org


From thierry.onkelinx at inbo.be  Thu Jul  9 09:42:48 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 9 Jul 2015 09:42:48 +0200
Subject: [R] Maxent Jarfile
In-Reply-To: <20150709052506.5849232.18416.844@gmail.com>
References: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>
	<7A0B317AB6E.0000034Fjrkrideau@inbox.com>
	<20150709052506.5849232.18416.844@gmail.com>
Message-ID: <CAJuCY5xYXC90j6o-sOy5_RPsS7QhKv6hhDTmEsvzc4YvqYWzwg@mail.gmail.com>

Dear Annemie,

The mailing list accepts only a limited number of extensions. Your
attachment got stripped for the mail. It is probably easier to put the
files on-line and send us the link to the files.

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-09 7:25 GMT+02:00 <annemariefischer86 at gmail.com>:

> ?Hi John,
>
> Sorry about that. Please find attached the code, error and input file.
>
> Thanks,
> Annemarie
>
>
>   Original Message
> From: John Kane
> Sent: Thursday 9 July 2015 00:26
> To: Annemarie Fischer; r-help at r-project.org
> Subject: Re: [R] Maxent Jarfile
>
> Hi Annemarie,
> You have sent the email in HTML and it is very close to unreadable. Could
> you please resubmit the message in plain text. R-help does not accept HTML
> and, as happened here, the text gets seriously mangled.
>
>
>
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: annemarie_dh at hotmail.com
> > Sent: Wed, 8 Jul 2015 20:22:57 +0000
> > To: r-help at r-project.org
> > Subject: [R] Maxent Jarfile
> >
> > Hi,
> > I have been trying to solve the below problem for 2 days with no success.
> > Hopefully you can help as i can find no assistance online.
> > I am attempting to run the niche.equivalency.test and the
> > bg.similarity.test using RStudio and Maxent. I keep getting the error:
> > Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
> > cannot open the connectionIn addition: Warning messages:1: running
> > command 'java -jar C:/Program
> > Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
> > R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
> > R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
> > nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
> > file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
> > directory
> > I suspect the issue is that the file directory doesnt have "", but i have
> > no idea how to add these in, as in RStudio values, the "" does appear.
> > My code is:
> > # load required
>
> packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
> > library(rJava)library(maptools)library(rasterVis)library(phyloclim)
> > # path to MAXENT# --------------maxent.exe <-
> > paste(system.file(package="dismo"),
> > "/java/maxent.jar", sep = "")
> > # a data frame of coordinates where two species # have been detected
> > ('presence points') and# a raster stack of environmental covariables#
> > --------------------------------------
> > ###Change to correct species usedfile <-
> > paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
> > sep="")# this is the file we will use:file
> > #save(file, file="Molossidae_rarefied_points.rda")
> > #data()#data(package = .packages(all.available = TRUE))
> > #myData <- read.csv("file", header=TRUE, nrows=10000)
> > Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
> > species <- c("Rhinolophus blasii", "Rhinolophus
> > clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
> > collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
> > system.file("extdata", package = "phyloclim")preds <- list.files(path =
> > data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
> > "/")preds <- stack(lapply(X = preds, FUN = raster))
> > # testing against 9 permutations of the data#
> > -------------------------------------------reps <- 1000
> > # run hypothesis tests# --------------------if (file.exists(maxent.exe)){
> > net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
> > plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
> > maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
> > Details)")}
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From Bettina.Gruen at jku.at  Wed Jul  1 07:27:33 2015
From: Bettina.Gruen at jku.at (Bettina Gruen)
Date: Wed, 1 Jul 2015 07:27:33 +0200
Subject: [R] The R Journal, Volume 7, Issue 1
Message-ID: <55937A45.8060600@jku.at>

Dear all,

The latest issue of The R Journal is now available at
http://journal.r-project.org/archive/2015-1/

Many thanks to all contributors.

Regards,
Bettina

-- 
-------------------------------------------------------------------
Bettina Gr?n
Institut f?r Angewandte Statistik / IFAS
Johannes Kepler Universit?t Linz
Altenbergerstra?e 69
4040 Linz, Austria

Tel: +43 732 2468-6829
Fax: +43 732 2468-6800
E-Mail: Bettina.Gruen at jku.at
www.ifas.jku.at

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce

From marammagdysalem at gmx.com  Thu Jul  9 09:49:31 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Thu, 9 Jul 2015 09:49:31 +0200
Subject: [R] Can't post to the list
In-Reply-To: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
References: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
Message-ID: <1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>

Sorry I mean NOT explicitly allowed

Sent from my iPhone

> On Jul 9, 2015, at 8:46 AM, Maram Salem <marammagdysalem at gmx.com> wrote:
> 
> Dear all, 
> I need help with posting to the list. Whenever i post any message, i get this reply : the message content was explicitly allowed. 
> Any help please? 
> Thanks a lot.
> Maram
> 
> Sent from my iPhone
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Thu Jul  9 11:36:00 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 9 Jul 2015 21:36:00 +1200
Subject: [R] Can't post to the list
In-Reply-To: <1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>
References: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
	<1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>
Message-ID: <559E4080.4050300@auckland.ac.nz>


On 09/07/15 19:49, Maram Salem wrote:

> Sorry I mean NOT explicitly allowed

You probably mean "explicitly NOT allowed".  The order of the words 
makes a substantial difference to the meaning.

>
> Sent from my iPhone
>
>> On Jul 9, 2015, at 8:46 AM, Maram Salem <marammagdysalem at gmx.com> wrote:
>>
>> Dear all,
>> I need help with posting to the list. Whenever i post any message, i get this reply : the message content was explicitly allowed.
>> Any help please?
>> Thanks a lot.

You have just successfully posted two messages to the list.

So think about the error message that you got.  It told you that the 
content of your (rejected) message was disallowed.  What was it about 
that content that would make the list disallow it?  Then remove the 
offending content.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From marammagdysalem at gmx.com  Thu Jul  9 13:04:44 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Thu, 9 Jul 2015 13:04:44 +0200
Subject: [R] Can't post to the list
In-Reply-To: <559E4080.4050300@auckland.ac.nz>
References: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
	<1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>
	<559E4080.4050300@auckland.ac.nz>
Message-ID: <272193A8-53DD-4A37-8BDE-E2FCFD879FF0@gmx.com>

Dear Rolf, 
I recieved this message exactly:
"The message's content type was not explicitly allowed"

Thanks for your concern,

Maram

Sent from my iPhone

> On Jul 9, 2015, at 11:36 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
>> On 09/07/15 19:49, Maram Salem wrote:
>> 
>> Sorry I mean NOT explicitly allowed
> 
> You probably mean "explicitly NOT allowed".  The order of the words makes a substantial difference to the meaning.
> 
>> 
>> Sent from my iPhone
>> 
>>> On Jul 9, 2015, at 8:46 AM, Maram Salem <marammagdysalem at gmx.com> wrote:
>>> 
>>> Dear all,
>>> I need help with posting to the list. Whenever i post any message, i get this reply : the message content was explicitly allowed.
>>> Any help please?
>>> Thanks a lot.
> 
> You have just successfully posted two messages to the list.
> 
> So think about the error message that you got.  It told you that the content of your (rejected) message was disallowed.  What was it about that content that would make the list disallow it?  Then remove the offending content.
> 
> cheers,
> 
> Rolf Turner
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276


From john.archie.mckown at gmail.com  Thu Jul  9 13:17:34 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Thu, 9 Jul 2015 06:17:34 -0500
Subject: [R] Can't post to the list
In-Reply-To: <272193A8-53DD-4A37-8BDE-E2FCFD879FF0@gmx.com>
References: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
	<1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>
	<559E4080.4050300@auckland.ac.nz>
	<272193A8-53DD-4A37-8BDE-E2FCFD879FF0@gmx.com>
Message-ID: <CAAJSdjhOmiuGS=p-2g20tZCkVSsg9hkFhW4TAtTwaQABbbxVYA@mail.gmail.com>

>
>
> On Thu, Jul 9, 2015 at 6:04 AM, Maram Salem <marammagdysalem at gmx.com>
> wrote:
> >
> > Dear Rolf,
> > I recieved this message exactly:
> > "The message's content type was not explicitly allowed"
>
>
> I'm just guessing here! But this list is "plain text only". I wonder if
> your email client sometimes uses unsupported MIME types. Again, I'm no
> expert, but this might be if it included "screen pictures" in JPG format
> (Windows loves these from the "Print Screen" button). Or maybe BASE64 or
> UUENCODEd somehow. If the listserv software detect this, and postings with
> that type of data are not in the setup as "explicitly allowed", then maybe.
>
> Here's a similar discussion which may be of interest:
>
>
> https://mailman-1.sys.kth.se/pipermail/gromacs.org_gmx-developers/2012-August/006176.html
>
> <quote>
> ...
>
> >
> >                 If you have received rejected mails saying "The
> >                 message's content type was not
> >                 explicitly allowed" it means that you have used in
> >                 your mail special styles or
> >                 formatting (e.g. bold typeface, colors etc.).
> >
>
> ...
>
> </quote>
>
>
>
> >
> >
> > Thanks for your concern,
> Maram
>
> Sent from my iPhone
>
>
-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From marammagdysalem at gmx.com  Thu Jul  9 14:11:58 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Thu, 9 Jul 2015 14:11:58 +0200
Subject: [R] Can't post to the list
In-Reply-To: <CAAJSdjhOmiuGS=p-2g20tZCkVSsg9hkFhW4TAtTwaQABbbxVYA@mail.gmail.com>
References: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
	<1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>
	<559E4080.4050300@auckland.ac.nz>
	<272193A8-53DD-4A37-8BDE-E2FCFD879FF0@gmx.com>
	<CAAJSdjhOmiuGS=p-2g20tZCkVSsg9hkFhW4TAtTwaQABbbxVYA@mail.gmail.com>
Message-ID: <92E01EDB-1CA9-4973-AAE5-59F660F4DD9A@gmx.com>

Thanks a lot John. Will check it out.
Maram

Sent from my iPhone

On Jul 9, 2015, at 1:17 PM, John McKown <john.archie.mckown at gmail.com> wrote:

>> 
>> On Thu, Jul 9, 2015 at 6:04 AM, Maram Salem <marammagdysalem at gmx.com> wrote:
>> >
>> > Dear Rolf,
>> > I recieved this message exactly:
>> > "The message's content type was not explicitly allowed"
>> 
>> 
>> I'm just guessing here! But this list is "plain text only". I wonder if your email client sometimes uses unsupported MIME types. Again, I'm no expert, but this might be if it included "screen pictures" in JPG format (Windows loves these from the "Print Screen" button). Or maybe BASE64 or UUENCODEd somehow. If the listserv software detect this, and postings with that type of data are not in the setup as "explicitly allowed", then maybe.
>> 
>> Here's a similar discussion which may be of interest:
>> 
>> https://mailman-1.sys.kth.se/pipermail/gromacs.org_gmx-developers/2012-August/006176.html
>> 
>> <quote>
>> ...
>> 
>> >
>> >                 If you have received rejected mails saying "The
>> >                 message's content type was not
>> >                 explicitly allowed" it means that you have used in
>> >                 your mail special styles or
>> >                 formatting (e.g. bold typeface, colors etc.).
>> >
>> 
>> ...
>> 
>> </quote>
>> 
>> 
>>  
>> >
>> >
>> > Thanks for your concern,
>> Maram
>> 
>> Sent from my iPhone
> 
> 
> -- 
> 
> Schrodinger's backup: The condition of any backup is unknown until a restore is attempted.
> 
> Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.
> 
> He's about as useful as a wax frying pan.
> 
> 10 to the 12th power microphones = 1 Megaphone
> 
> Maranatha! <><
> John McKown

	[[alternative HTML version deleted]]


From marammagdysalem at gmx.com  Thu Jul  9 14:13:58 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Thu, 9 Jul 2015 14:13:58 +0200
Subject: [R] Can't post to the list
In-Reply-To: <559E4080.4050300@auckland.ac.nz>
References: <DC4E7BCF-72C0-4400-B578-DBE8A64E699B@gmx.com>
	<1B14F8E7-018C-48FD-8A96-34BD0A16F66F@gmx.com>
	<559E4080.4050300@auckland.ac.nz>
Message-ID: <76D97E51-0E08-4F8B-8A0E-7E13D44F14AD@gmx.com>

Dear Rolf,
There isn't any offending content. I think it's related to the styles used on the mail. 
Thanks for helping.
Maram

Sent from my iPhone

> On Jul 9, 2015, at 11:36 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
>> On 09/07/15 19:49, Maram Salem wrote:
>> 
>> Sorry I mean NOT explicitly allowed
> 
> You probably mean "explicitly NOT allowed".  The order of the words makes a substantial difference to the meaning.
> 
>> 
>> Sent from my iPhone
>> 
>>> On Jul 9, 2015, at 8:46 AM, Maram Salem <marammagdysalem at gmx.com> wrote:
>>> 
>>> Dear all,
>>> I need help with posting to the list. Whenever i post any message, i get this reply : the message content was explicitly allowed.
>>> Any help please?
>>> Thanks a lot.
> 
> You have just successfully posted two messages to the list.
> 
> So think about the error message that you got.  It told you that the content of your (rejected) message was disallowed.  What was it about that content that would make the list disallow it?  Then remove the offending content.
> 
> cheers,
> 
> Rolf Turner
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276


From cryan at binghamton.edu  Thu Jul  9 04:48:54 2015
From: cryan at binghamton.edu (Christopher W Ryan)
Date: Wed, 8 Jul 2015 22:48:54 -0400
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
Message-ID: <CAM+rpYnUiP0v=rUjnZp3-Okc7E-eiMbdxMirFzrR_e=mZyN9pg@mail.gmail.com>

Running R 3.1.1 on windows 7

I want to identify as a case any record in a dataframe that contains any
of several keywords in any of several variables.

Example:

# create a dataframe with 4 variables and 10 records
v2 <- c("white bird", "blue bird", "green turtle", "quick brown fox",
"big black dog", "waffle the hamster", "benny likes food a lot", "hello
world", "yellow giraffe with a long neck", "black bear")
v3 <- c("harry potter", "hermione grainger", "ronald weasley", "ginny
weasley", "dudley dursley", "red sparks", "blue sparks", "white dress
robes", "gandalf the white", "gandalf the grey")
zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10, lambda=2),
stringsAsFactors=FALSE)
str(zz)
zz

# here are the keywords
alarm.words <- c("red", "green", "turtle", "gandalf")

# For each row/record, I want to test whether the string in v2 or the
string in v3 contains any of the strings in alarm.words. And then if so,
set zz$v5=TRUE for that record.

# I'm thinking the str_detect function in the stringr package ought to
be able to help, perhaps with some use of apply over the rows, but I
obviously misunderstand something about how str_detect works

library(stringr)

str_detect(zz[,2:3], alarm.words)    # error: the target of the search
                                     # must be a vector, not multiple
                                     # columns

str_detect(zz[1:4,2:3], alarm.words) # same error

str_detect(zz[,2], alarm.words)      # error, length of alarm.words
                                     # is less than the number of
                                     # rows I am using for the
                                     # comparison

str_detect(zz[1:4,2], alarm.words)   # works as hoped when
length(alarm.words)                  # confining nrows
                                     # to the length of alarm.words

str_detect(zz, alarm.words)          # obviously not right

# maybe I need apply() ?
my.f <- function(x){str_detect(x, alarm.words)}

apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
                           # between alarm.words and that
                           # in which I am searching for
                           # matching strings

apply(zz, 2, my.f)         # now I'm getting somewhere
apply(zz[1:4,], 2, my.f)   # but still only works with 4
                           # rows of the dataframe

Appreciate any advice.

-- Chris Ryan


From cryan at binghamton.edu  Thu Jul  9 04:23:37 2015
From: cryan at binghamton.edu (Christopher W. Ryan)
Date: Wed, 08 Jul 2015 22:23:37 -0400
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
Message-ID: <559DDB29.3040803@binghamton.edu>

Running R 3.1.1 on windows 7

I want to identify as a case any record in a dataframe that contains any
of several keywords in any of several variables.

Example:

# create a dataframe with 4 variables and 10 records
v2 <- c("white bird", "blue bird", "green turtle", "quick brown fox",
"big black dog", "waffle the hamster", "benny likes food a lot", "hello
world", "yellow giraffe with a long neck", "black bear")
v3 <- c("harry potter", "hermione grainger", "ronald weasley", "ginny
weasley", "dudley dursley", "red sparks", "blue sparks", "white dress
robes", "gandalf the white", "gandalf the grey")
zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10, lambda=2),
stringsAsFactors=FALSE)
str(zz)
zz

# here are the keywords
alarm.words <- c("red", "green", "turtle", "gandalf")

# For each row/record, I want to test whether the string in v2 or the
string in v3 contains any of the strings in alarm.words. And then if so,
set zz$v5=TRUE for that record.

# I'm thinking the str_detect function in the stringr package ought to
be able to help, perhaps with some use of apply over the rows, but I
obviously misunderstand something about how str_detect works

library(stringr)

str_detect(zz[,2:3], alarm.words)    # error: the target of the search
                                     # must be a vector, not multiple
                                     # columns

str_detect(zz[1:4,2:3], alarm.words) # same error

str_detect(zz[,2], alarm.words)      # error, length of alarm.words
                                     # is less than the number of
                                     # rows I am using for the
                                     # comparison

str_detect(zz[1:4,2], alarm.words)   # works as hoped when
length(alarm.words)                  # confining nrows
                                     # to the length of alarm.words

str_detect(zz, alarm.words)          # obviously not right

# maybe I need apply() ?
my.f <- function(x){str_detect(x, alarm.words)}

apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
                           # between alarm.words and that
                           # in which I am searching for
                           # matching strings

apply(zz, 2, my.f)         # now I'm getting somewhere
apply(zz[1:4,], 2, my.f)   # but still only works with 4
                           # rows of the dataframe


# perhaps %in% could do the job?

Appreciate any advice.

--Chris Ryan


From jrkrideau at inbox.com  Thu Jul  9 15:02:16 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 9 Jul 2015 05:02:16 -0800
Subject: [R] Maxent Jarfile
In-Reply-To: <20150709052506.5849232.18416.844@gmail.com>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
Message-ID: <81B8F5B86EB.000009A4jrkrideau@inbox.com>

Thanks Annemarie,
Things came through okay. I'll defer to the experts on substantive advice.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: annemariefischer86 at gmail.com
> Sent: Thu, 09 Jul 2015 07:25:06 +0200
> To: jrkrideau at inbox.com, annemarie_dh at hotmail.com, r-help at r-project.org
> Subject: Re: [R] Maxent Jarfile
> 
> ?Hi John,
> 
> Sorry about that. Please find attached the code, error and input file.
> 
> Thanks,
> Annemarie
> 
> 
> ? Original Message
> From: John Kane
> Sent: Thursday 9 July 2015 00:26
> To: Annemarie Fischer; r-help at r-project.org
> Subject: Re: [R] Maxent Jarfile
> 
> Hi Annemarie,
> You have sent the email in HTML and it is very close to unreadable. Could
> you please resubmit the message in plain text. R-help does not accept
> HTML and, as happened here, the text gets seriously mangled.
> 
> 
> 
> John Kane
> Kingston ON Canada
> 
> 
>> -----Original Message-----
>> From: annemarie_dh at hotmail.com
>> Sent: Wed, 8 Jul 2015 20:22:57 +0000
>> To: r-help at r-project.org
>> Subject: [R] Maxent Jarfile
>> 
>> Hi,
>> I have been trying to solve the below problem for 2 days with no
>> success.
>> Hopefully you can help as i can find no assistance online.
>> I am attempting to run the niche.equivalency.test and the
>> bg.similarity.test using RStudio and Maxent. I keep getting the error:
>> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
>> cannot open the connectionIn addition: Warning messages:1: running
>> command 'java -jar C:/Program
>> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
>> R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
>> R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
>> nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
>> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
>> directory
>> I suspect the issue is that the file directory doesnt have "", but i
>> have
>> no idea how to add these in, as in RStudio values, the "" does appear.
>> My code is:
>> # load required
> packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
>> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
>> # path to MAXENT# --------------maxent.exe <-
>> paste(system.file(package="dismo"),
>> "/java/maxent.jar", sep = "")
>> # a data frame of coordinates where two species # have been detected
>> ('presence points') and# a raster stack of environmental covariables#
>> --------------------------------------
>> ###Change to correct species usedfile <-
>> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
>> sep="")# this is the file we will use:file
>> #save(file, file="Molossidae_rarefied_points.rda")
>> #data()#data(package = .packages(all.available = TRUE))
>> #myData <- read.csv("file", header=TRUE, nrows=10000)
>> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
>> species <- c("Rhinolophus blasii", "Rhinolophus
>> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
>> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
>> system.file("extdata", package = "phyloclim")preds <- list.files(path =
>> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
>> "/")preds <- stack(lapply(X = preds, FUN = raster))
>> # testing against 9 permutations of the data#
>> -------------------------------------------reps <- 1000
>> # run hypothesis tests# --------------------if
>> (file.exists(maxent.exe)){
>> net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
>> plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
>> maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
>> Details)")}
>> [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ____________________________________________________________
> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From jfox at mcmaster.ca  Thu Jul  9 15:05:03 2015
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 09 Jul 2015 09:05:03 -0400
Subject: [R] detecting any element in a vector of strings,
	appearing anywhere in any of several character variables in a
	dataframe
In-Reply-To: <559DDB29.3040803@binghamton.edu>
References: <559DDB29.3040803@binghamton.edu>
Message-ID: <web-565125331@cgpsrv2.cis.mcmaster.ca>

Dear Chris,

If I understand correctly what you want, how about the following?

> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words, grepl, x=x)))
> zz[rows, ]

          v1                              v2                v3 v4
3  -1.022329                    green turtle    ronald weasley  2
6   0.336599              waffle the hamster        red sparks  1
9  -1.631874 yellow giraffe with a long neck gandalf the white  1
10  1.130622                      black bear  gandalf the grey  2

I hope this helps,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	

On Wed, 08 Jul 2015 22:23:37 -0400
 "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
> Running R 3.1.1 on windows 7
> 
> I want to identify as a case any record in a dataframe that contains any
> of several keywords in any of several variables.
> 
> Example:
> 
> # create a dataframe with 4 variables and 10 records
> v2 <- c("white bird", "blue bird", "green turtle", "quick brown fox",
> "big black dog", "waffle the hamster", "benny likes food a lot", "hello
> world", "yellow giraffe with a long neck", "black bear")
> v3 <- c("harry potter", "hermione grainger", "ronald weasley", "ginny
> weasley", "dudley dursley", "red sparks", "blue sparks", "white dress
> robes", "gandalf the white", "gandalf the grey")
> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10, lambda=2),
> stringsAsFactors=FALSE)
> str(zz)
> zz
> 
> # here are the keywords
> alarm.words <- c("red", "green", "turtle", "gandalf")
> 
> # For each row/record, I want to test whether the string in v2 or the
> string in v3 contains any of the strings in alarm.words. And then if so,
> set zz$v5=TRUE for that record.
> 
> # I'm thinking the str_detect function in the stringr package ought to
> be able to help, perhaps with some use of apply over the rows, but I
> obviously misunderstand something about how str_detect works
> 
> library(stringr)
> 
> str_detect(zz[,2:3], alarm.words)    # error: the target of the search
>                                      # must be a vector, not multiple
>                                      # columns
> 
> str_detect(zz[1:4,2:3], alarm.words) # same error
> 
> str_detect(zz[,2], alarm.words)      # error, length of alarm.words
>                                      # is less than the number of
>                                      # rows I am using for the
>                                      # comparison
> 
> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
> length(alarm.words)                  # confining nrows
>                                      # to the length of alarm.words
> 
> str_detect(zz, alarm.words)          # obviously not right
> 
> # maybe I need apply() ?
> my.f <- function(x){str_detect(x, alarm.words)}
> 
> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>                            # between alarm.words and that
>                            # in which I am searching for
>                            # matching strings
> 
> apply(zz, 2, my.f)         # now I'm getting somewhere
> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>                            # rows of the dataframe
> 
> 
> # perhaps %in% could do the job?
> 
> Appreciate any advice.
> 
> --Chris Ryan
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.otojanov at qmul.ac.uk  Thu Jul  9 15:51:19 2015
From: r.otojanov at qmul.ac.uk (mrrox)
Date: Thu, 9 Jul 2015 06:51:19 -0700 (PDT)
Subject: [R] ca.jo function, urca package, singular matrix problem
Message-ID: <1436449879103-4709635.post@n4.nabble.com>

Hi, I am trying to run a cointegration test with a dummy variable using
`*ca.jo*` function in `*urca*` package. 

*johcoint=ca.jo(Ydata[10:60,1:5],type="trace",ecdet=c("const"),K=2,spec="transitory",dumvar=dumvar)
*
 `*dumvar*` is the binary variable that take 1 and 0 only. the first two
observations are 1 and the rest are 0s. 
when I run the code, I get 

   / Error in solve.default(M11) : 
          Lapack routine dgesv: system is exactly singular: U[1,1] = 0/

I think this is something to do with the invertability of the input matrix,
and this occurs only when I use `*dumvar*` only. The error message
disappears if I add a 1 to the 3rd observation of `dumvar`.

Below is the sample data just for info:
   

              A             B            C             D             E      
dumvar
    1  2.255446 1.688807 1.506579 1.880152 9.575868      1
    2  2.230118 1.578281 1.546805 1.905426 9.545534      1
    3  2.255446 1.688807 1.506579 1.880152 9.575868      0
    4  2.230118 1.578281 1.546805 1.905426 9.545534      0
    5  2.255446 1.688807 1.506579 1.880152 9.575868      0
    6  2.230118 1.578281 1.546805 1.905426 9.545534      0
    7  2.255446 1.688807 1.506579 1.880152 9.575868      0
    8  2.230118 1.578281 1.546805 1.905426 9.545534      0
    9  2.255446 1.688807 1.506579 1.880152 9.575868      0
    10 2.230118 1.578281 1.546805 1.905426 9.545534      0
    11 2.255446 1.688807 1.506579 1.880152 9.575868      0
    12 2.230118 1.578281 1.546805 1.905426 9.545534      0
    13 2.255446 1.688807 1.506579 1.880152 9.575868      0
    14 2.230118 1.578281 1.546805 1.905426 9.545534      0
    15 2.255446 1.688807 1.506579 1.880152 9.575868      0
    16 2.230118 1.578281 1.546805 1.905426 9.545534      0
    17 2.255446 1.688807 1.506579 1.880152 9.575868      0
    18 2.230118 1.578281 1.546805 1.905426 9.545534      0
    19 2.255446 1.688807 1.506579 1.880152 9.575868      0
    20 2.230118 1.578281 1.546805 1.905426 9.545534      0
    21 2.255446 1.688807 1.506579 1.880152 9.575868      0
    22 2.230118 1.578281 1.546805 1.905426 9.545534      0
    23 2.255446 1.688807 1.506579 1.880152 9.575868      0
    24 2.230118 1.578281 1.546805 1.905426 9.545534      0
    25 2.255446 1.688807 1.506579 1.880152 9.575868      0
    26 2.230118 1.578281 1.546805 1.905426 9.545534      0
    27 2.255446 1.688807 1.506579 1.880152 9.575868      0
    28 2.230118 1.578281 1.546805 1.905426 9.545534      0
    29 2.255446 1.688807 1.506579 1.880152 9.575868      0
    30 2.230118 1.578281 1.546805 1.905426 9.545534      0
    31 2.255446 1.688807 1.506579 1.880152 9.575868      0
    32 2.230118 1.578281 1.546805 1.905426 9.545534      0
    33 2.255446 1.688807 1.506579 1.880152 9.575868      0
    34 2.230118 1.578281 1.546805 1.905426 9.545534      0
    35 2.255446 1.688807 1.506579 1.880152 9.575868      0
    36 2.230118 1.578281 1.546805 1.905426 9.545534      0
    37 2.255446 1.688807 1.506579 1.880152 9.575868      0
    38 2.230118 1.578281 1.546805 1.905426 9.545534      0
    39 2.255446 1.688807 1.506579 1.880152 9.575868      0
    40 2.230118 1.578281 1.546805 1.905426 9.545534      0
    41 2.255446 1.688807 1.506579 1.880152 9.575868      0
    42 2.230118 1.578281 1.546805 1.905426 9.545534      0
    43 2.255446 1.688807 1.506579 1.880152 9.575868      0
    44 2.230118 1.578281 1.546805 1.905426 9.545534      0
    45 2.255446 1.688807 1.506579 1.880152 9.575868      0
    46 2.230118 1.578281 1.546805 1.905426 9.545534      0
    47 2.255446 1.688807 1.506579 1.880152 9.575868      0
    48 2.230118 1.578281 1.546805 1.905426 9.545534      0
    49 2.255446 1.688807 1.506579 1.880152 9.575868      0
    50 2.230118 1.578281 1.546805 1.905426 9.545534      0

Thank you!



--
View this message in context: http://r.789695.n4.nabble.com/ca-jo-function-urca-package-singular-matrix-problem-tp4709635.html
Sent from the R help mailing list archive at Nabble.com.


From sa.filahi at gmail.com  Thu Jul  9 14:12:32 2015
From: sa.filahi at gmail.com (Said Filahi)
Date: Thu, 9 Jul 2015 12:12:32 +0000
Subject: [R] portrait diagram
Message-ID: <CABNiEm=yDVHLuZxtko5NcFfKQ6Lit8Sjg0GbHE2u3C0mVS1M5g@mail.gmail.com>

Hello,

I x

	[[alternative HTML version deleted]]


From sa.filahi at gmail.com  Thu Jul  9 14:14:56 2015
From: sa.filahi at gmail.com (Said Filahi)
Date: Thu, 9 Jul 2015 12:14:56 +0000
Subject: [R] portrait diagram
Message-ID: <CABNiEm=WTZ7f8ms8c_0_10T4LNLMpfSaRkiEZnua9vW=jGCt1w@mail.gmail.com>

Hello

i want to make a portrait diagram, i don't know if i can perfom it with a R


thank you


Said

	[[alternative HTML version deleted]]


From j.carreiras at sheffield.ac.uk  Thu Jul  9 15:06:30 2015
From: j.carreiras at sheffield.ac.uk (Joao Carreiras)
Date: Thu, 9 Jul 2015 14:06:30 +0100
Subject: [R] randomForest set.seed()
Message-ID: <CAJskk_8i5yqO25BxsWcKTqTSzfR0WXcZcS0M5t2cH_VUWmHTww@mail.gmail.com>

Dear forum members,

?I wrote a piece of code to test various combinations of mtry and ntree, so
that the best combination (in terms of mse?) could be used. Before each
randomForest command I included a set.seed() command, so that I can keep
track of the seed number and replicate the results. However, when I run the
randomForest command again with the seed number corresponding to the best
combination I get a different mse. Any suggestions?
Thanks and best wishes
Joao

------------
## Define the number of trees to test
nt <- c(100, 200, 300, 400, 500, 700, 900, 1000)
## Define the number of variables to be randomly selected at each node
mt <- c(1, 2, 3)
##
a <- 0
##
## For loop to test the various combinations of ntree and mtry
for (j in 1:8)
   for (i in 1:3)
? ?
{
         b <- 1968 + a
         set.seed(b)
         HH <- randomForest(HH_MEAN ~ ASF + PALU + FC
??
,
??
                            data = model.data,
                            ntree = nt[j],
                            mtry = mt[i],
                            replace = T,
?                     ?
? }?
?----------------?

??

	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Thu Jul  9 17:16:53 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 9 Jul 2015 11:16:53 -0400
Subject: [R] portrait diagram
In-Reply-To: <CABNiEm=WTZ7f8ms8c_0_10T4LNLMpfSaRkiEZnua9vW=jGCt1w@mail.gmail.com>
References: <CABNiEm=WTZ7f8ms8c_0_10T4LNLMpfSaRkiEZnua9vW=jGCt1w@mail.gmail.com>
Message-ID: <0F5E3F35-E037-4AF3-A7B7-26EA6C9F86D8@utoronto.ca>

Read the posting guides and do not post in HTML.

Type ?pdf at the console prompt. Read the help text. The parameter "paper" specifies paper size and orientation. Portrait is the default.



B.


On Jul 9, 2015, at 8:14 AM, Said Filahi <sa.filahi at gmail.com> wrote:

> Hello
> 
> i want to make a portrait diagram, i don't know if i can perfom it with a R
> 
> 
> thank you
> 
> 
> Said
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Thu Jul  9 17:51:10 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 9 Jul 2015 08:51:10 -0700
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <web-565125331@cgpsrv2.cis.mcmaster.ca>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
Message-ID: <CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>

Here's a way to do it that uses %in% (i.e. match() ) and uses only a
single, not a double, loop. It should be more efficient.

> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
+       function(x)any(x %in% alarm.words))

 [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE

The idea is to paste the strings in each row (do.call allows an
arbitrary number of columns) into a single string and then use
strsplit to break the string into individual "words" on whitespace.
Then the matching is vectorized with the any( %in% ... ) call.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
> Dear Chris,
>
> If I understand correctly what you want, how about the following?
>
>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words, grepl, x=x)))
>> zz[rows, ]
>
>           v1                              v2                v3 v4
> 3  -1.022329                    green turtle    ronald weasley  2
> 6   0.336599              waffle the hamster        red sparks  1
> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
> 10  1.130622                      black bear  gandalf the grey  2
>
> I hope this helps,
>  John
>
> ------------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
>
>
> On Wed, 08 Jul 2015 22:23:37 -0400
>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>> Running R 3.1.1 on windows 7
>>
>> I want to identify as a case any record in a dataframe that contains any
>> of several keywords in any of several variables.
>>
>> Example:
>>
>> # create a dataframe with 4 variables and 10 records
>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown fox",
>> "big black dog", "waffle the hamster", "benny likes food a lot", "hello
>> world", "yellow giraffe with a long neck", "black bear")
>> v3 <- c("harry potter", "hermione grainger", "ronald weasley", "ginny
>> weasley", "dudley dursley", "red sparks", "blue sparks", "white dress
>> robes", "gandalf the white", "gandalf the grey")
>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10, lambda=2),
>> stringsAsFactors=FALSE)
>> str(zz)
>> zz
>>
>> # here are the keywords
>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>
>> # For each row/record, I want to test whether the string in v2 or the
>> string in v3 contains any of the strings in alarm.words. And then if so,
>> set zz$v5=TRUE for that record.
>>
>> # I'm thinking the str_detect function in the stringr package ought to
>> be able to help, perhaps with some use of apply over the rows, but I
>> obviously misunderstand something about how str_detect works
>>
>> library(stringr)
>>
>> str_detect(zz[,2:3], alarm.words)    # error: the target of the search
>>                                      # must be a vector, not multiple
>>                                      # columns
>>
>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>
>> str_detect(zz[,2], alarm.words)      # error, length of alarm.words
>>                                      # is less than the number of
>>                                      # rows I am using for the
>>                                      # comparison
>>
>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>> length(alarm.words)                  # confining nrows
>>                                      # to the length of alarm.words
>>
>> str_detect(zz, alarm.words)          # obviously not right
>>
>> # maybe I need apply() ?
>> my.f <- function(x){str_detect(x, alarm.words)}
>>
>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>                            # between alarm.words and that
>>                            # in which I am searching for
>>                            # matching strings
>>
>> apply(zz, 2, my.f)         # now I'm getting somewhere
>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>                            # rows of the dataframe
>>
>>
>> # perhaps %in% could do the job?
>>
>> Appreciate any advice.
>>
>> --Chris Ryan
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From Bernhard_Pfaff at fra.invesco.com  Thu Jul  9 18:06:39 2015
From: Bernhard_Pfaff at fra.invesco.com (Pfaff, Bernhard Dr.)
Date: Thu, 9 Jul 2015 16:06:39 +0000
Subject: [R] ca.jo function, urca package, singular matrix problem
In-Reply-To: <1436449879103-4709635.post@n4.nabble.com>
References: <1436449879103-4709635.post@n4.nabble.com>
Message-ID: <FCD9A33C859ACC469587CB09DD5C6C710EA8E22A@GBLONXMB13.corp.amvescap.net>

Watch out for the pre-sample values (K = 2); hence you have supplied a dumvar consisting of zeros only, in your first example.

Best,
Bernhard

-----Urspr?ngliche Nachricht-----
Von: R-help [mailto:r-help-bounces at r-project.org] Im Auftrag von mrrox
Gesendet: Donnerstag, 9. Juli 2015 15:51
An: r-help at r-project.org
Betreff: [R] ca.jo function, urca package, singular matrix problem

Hi, I am trying to run a cointegration test with a dummy variable using `*ca.jo*` function in `*urca*` package. 

*johcoint=ca.jo(Ydata[10:60,1:5],type="trace",ecdet=c("const"),K=2,spec="transitory",dumvar=dumvar)
*
 `*dumvar*` is the binary variable that take 1 and 0 only. the first two observations are 1 and the rest are 0s. 
when I run the code, I get 

   / Error in solve.default(M11) : 
          Lapack routine dgesv: system is exactly singular: U[1,1] = 0/

I think this is something to do with the invertability of the input matrix, and this occurs only when I use `*dumvar*` only. The error message disappears if I add a 1 to the 3rd observation of `dumvar`.

Below is the sample data just for info:
   

              A             B            C             D             E      
dumvar
    1  2.255446 1.688807 1.506579 1.880152 9.575868      1
    2  2.230118 1.578281 1.546805 1.905426 9.545534      1
    3  2.255446 1.688807 1.506579 1.880152 9.575868      0
    4  2.230118 1.578281 1.546805 1.905426 9.545534      0
    5  2.255446 1.688807 1.506579 1.880152 9.575868      0
    6  2.230118 1.578281 1.546805 1.905426 9.545534      0
    7  2.255446 1.688807 1.506579 1.880152 9.575868      0
    8  2.230118 1.578281 1.546805 1.905426 9.545534      0
    9  2.255446 1.688807 1.506579 1.880152 9.575868      0
    10 2.230118 1.578281 1.546805 1.905426 9.545534      0
    11 2.255446 1.688807 1.506579 1.880152 9.575868      0
    12 2.230118 1.578281 1.546805 1.905426 9.545534      0
    13 2.255446 1.688807 1.506579 1.880152 9.575868      0
    14 2.230118 1.578281 1.546805 1.905426 9.545534      0
    15 2.255446 1.688807 1.506579 1.880152 9.575868      0
    16 2.230118 1.578281 1.546805 1.905426 9.545534      0
    17 2.255446 1.688807 1.506579 1.880152 9.575868      0
    18 2.230118 1.578281 1.546805 1.905426 9.545534      0
    19 2.255446 1.688807 1.506579 1.880152 9.575868      0
    20 2.230118 1.578281 1.546805 1.905426 9.545534      0
    21 2.255446 1.688807 1.506579 1.880152 9.575868      0
    22 2.230118 1.578281 1.546805 1.905426 9.545534      0
    23 2.255446 1.688807 1.506579 1.880152 9.575868      0
    24 2.230118 1.578281 1.546805 1.905426 9.545534      0
    25 2.255446 1.688807 1.506579 1.880152 9.575868      0
    26 2.230118 1.578281 1.546805 1.905426 9.545534      0
    27 2.255446 1.688807 1.506579 1.880152 9.575868      0
    28 2.230118 1.578281 1.546805 1.905426 9.545534      0
    29 2.255446 1.688807 1.506579 1.880152 9.575868      0
    30 2.230118 1.578281 1.546805 1.905426 9.545534      0
    31 2.255446 1.688807 1.506579 1.880152 9.575868      0
    32 2.230118 1.578281 1.546805 1.905426 9.545534      0
    33 2.255446 1.688807 1.506579 1.880152 9.575868      0
    34 2.230118 1.578281 1.546805 1.905426 9.545534      0
    35 2.255446 1.688807 1.506579 1.880152 9.575868      0
    36 2.230118 1.578281 1.546805 1.905426 9.545534      0
    37 2.255446 1.688807 1.506579 1.880152 9.575868      0
    38 2.230118 1.578281 1.546805 1.905426 9.545534      0
    39 2.255446 1.688807 1.506579 1.880152 9.575868      0
    40 2.230118 1.578281 1.546805 1.905426 9.545534      0
    41 2.255446 1.688807 1.506579 1.880152 9.575868      0
    42 2.230118 1.578281 1.546805 1.905426 9.545534      0
    43 2.255446 1.688807 1.506579 1.880152 9.575868      0
    44 2.230118 1.578281 1.546805 1.905426 9.545534      0
    45 2.255446 1.688807 1.506579 1.880152 9.575868      0
    46 2.230118 1.578281 1.546805 1.905426 9.545534      0
    47 2.255446 1.688807 1.506579 1.880152 9.575868      0
    48 2.230118 1.578281 1.546805 1.905426 9.545534      0
    49 2.255446 1.688807 1.506579 1.880152 9.575868      0
    50 2.230118 1.578281 1.546805 1.905426 9.545534      0

Thank you!



--
View this message in context: http://r.789695.n4.nabble.com/ca-jo-function-urca-package-singular-matrix-problem-tp4709635.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
*****************************************************************
Confidentiality Note: The information contained in this ...{{dropped:10}}


From dwinsemius at comcast.net  Thu Jul  9 18:13:02 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 9 Jul 2015 09:13:02 -0700
Subject: [R] Maxent Jarfile
In-Reply-To: <81B8F5B86EB.000009A4jrkrideau@inbox.com>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
Message-ID: <05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>


On Jul 9, 2015, at 6:02 AM, John Kane wrote:

> Thanks Annemarie,
> Things came through okay. I'll defer to the experts on substantive advice.
> 

John;

It only came through to you by way of a copy sent directly to you. It did not come to you from the rhelp-server. If there are any experts out there, then they did not get a copy.

The advice to contact the package maintainer seemed the best avenue to proceed along.

David.


> John Kane
> Kingston ON Canada
> 
> 
>> -----Original Message-----
>> From: annemariefischer86 at gmail.com
>> Sent: Thu, 09 Jul 2015 07:25:06 +0200
>> To: jrkrideau at inbox.com, annemarie_dh at hotmail.com, r-help at r-project.org
>> Subject: Re: [R] Maxent Jarfile
>> 
>> ?Hi John,
>> 
>> Sorry about that. Please find attached the code, error and input file.
>> 
>> Thanks,
>> Annemarie
>> 
>> 
>>   Original Message
>> From: John Kane
>> Sent: Thursday 9 July 2015 00:26
>> To: Annemarie Fischer; r-help at r-project.org
>> Subject: Re: [R] Maxent Jarfile
>> 
>> Hi Annemarie,
>> You have sent the email in HTML and it is very close to unreadable. Could
>> you please resubmit the message in plain text. R-help does not accept
>> HTML and, as happened here, the text gets seriously mangled.
>> 
>> 
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: annemarie_dh at hotmail.com
>>> Sent: Wed, 8 Jul 2015 20:22:57 +0000
>>> To: r-help at r-project.org
>>> Subject: [R] Maxent Jarfile
>>> 
>>> Hi,
>>> I have been trying to solve the below problem for 2 days with no
>>> success.
>>> Hopefully you can help as i can find no assistance online.
>>> I am attempting to run the niche.equivalency.test and the
>>> bg.similarity.test using RStudio and Maxent. I keep getting the error:
>>> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
>>> cannot open the connectionIn addition: Warning messages:1: running
>>> command 'java -jar C:/Program
>>> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
>>> R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
>>> R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
>>> nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
>>> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
>>> directory
>>> I suspect the issue is that the file directory doesnt have "", but i
>>> have
>>> no idea how to add these in, as in RStudio values, the "" does appear.
>>> My code is:
>>> # load required
>> packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
>>> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
>>> # path to MAXENT# --------------maxent.exe <-
>>> paste(system.file(package="dismo"),
>>> "/java/maxent.jar", sep = "")
>>> # a data frame of coordinates where two species # have been detected
>>> ('presence points') and# a raster stack of environmental covariables#
>>> --------------------------------------
>>> ###Change to correct species usedfile <-
>>> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
>>> sep="")# this is the file we will use:file
>>> #save(file, file="Molossidae_rarefied_points.rda")
>>> #data()#data(package = .packages(all.available = TRUE))
>>> #myData <- read.csv("file", header=TRUE, nrows=10000)
>>> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
>>> species <- c("Rhinolophus blasii", "Rhinolophus
>>> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
>>> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
>>> system.file("extdata", package = "phyloclim")preds <- list.files(path =
>>> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
>>> "/")preds <- stack(lapply(X = preds, FUN = raster))
>>> # testing against 9 permutations of the data#
>>> -------------------------------------------reps <- 1000
>>> # run hypothesis tests# --------------------if
>>> (file.exists(maxent.exe)){
>>> net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
>>> plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
>>> maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
>>> Details)")}
>>> [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ____________________________________________________________
>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ____________________________________________________________
> Can't remember your password? Do you need a strong and secure password?
> Use Password manager! It stores your passwords & protects your account.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.CA.us  Thu Jul  9 18:34:28 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 09 Jul 2015 09:34:28 -0700
Subject: [R] detecting any element in a vector of strings,
	appearing anywhere in any of several character variables in a
	dataframe
In-Reply-To: <CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
Message-ID: <60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>

I think grep is better suited to this:

zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call( paste, zz[ , 2:3 ] ) ) )
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 9, 2015 8:51:10 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Here's a way to do it that uses %in% (i.e. match() ) and uses only a
>single, not a double, loop. It should be more efficient.
>
>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>+       function(x)any(x %in% alarm.words))
>
> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>
>The idea is to paste the strings in each row (do.call allows an
>arbitrary number of columns) into a single string and then use
>strsplit to break the string into individual "words" on whitespace.
>Then the matching is vectorized with the any( %in% ... ) call.
>
>Cheers,
>Bert
>Bert Gunter
>
>"Data is not information. Information is not knowledge. And knowledge
>is certainly not wisdom."
>   -- Clifford Stoll
>
>
>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>> Dear Chris,
>>
>> If I understand correctly what you want, how about the following?
>>
>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>grepl, x=x)))
>>> zz[rows, ]
>>
>>           v1                              v2                v3 v4
>> 3  -1.022329                    green turtle    ronald weasley  2
>> 6   0.336599              waffle the hamster        red sparks  1
>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>> 10  1.130622                      black bear  gandalf the grey  2
>>
>> I hope this helps,
>>  John
>>
>> ------------------------------------------------
>> John Fox, Professor
>> McMaster University
>> Hamilton, Ontario, Canada
>> http://socserv.mcmaster.ca/jfox/
>>
>>
>> On Wed, 08 Jul 2015 22:23:37 -0400
>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>> Running R 3.1.1 on windows 7
>>>
>>> I want to identify as a case any record in a dataframe that contains
>any
>>> of several keywords in any of several variables.
>>>
>>> Example:
>>>
>>> # create a dataframe with 4 variables and 10 records
>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>fox",
>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>"hello
>>> world", "yellow giraffe with a long neck", "black bear")
>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>"ginny
>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>dress
>>> robes", "gandalf the white", "gandalf the grey")
>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10, lambda=2),
>>> stringsAsFactors=FALSE)
>>> str(zz)
>>> zz
>>>
>>> # here are the keywords
>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>>
>>> # For each row/record, I want to test whether the string in v2 or
>the
>>> string in v3 contains any of the strings in alarm.words. And then if
>so,
>>> set zz$v5=TRUE for that record.
>>>
>>> # I'm thinking the str_detect function in the stringr package ought
>to
>>> be able to help, perhaps with some use of apply over the rows, but I
>>> obviously misunderstand something about how str_detect works
>>>
>>> library(stringr)
>>>
>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>search
>>>                                      # must be a vector, not
>multiple
>>>                                      # columns
>>>
>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>>
>>> str_detect(zz[,2], alarm.words)      # error, length of alarm.words
>>>                                      # is less than the number of
>>>                                      # rows I am using for the
>>>                                      # comparison
>>>
>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>> length(alarm.words)                  # confining nrows
>>>                                      # to the length of alarm.words
>>>
>>> str_detect(zz, alarm.words)          # obviously not right
>>>
>>> # maybe I need apply() ?
>>> my.f <- function(x){str_detect(x, alarm.words)}
>>>
>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>>                            # between alarm.words and that
>>>                            # in which I am searching for
>>>                            # matching strings
>>>
>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>>                            # rows of the dataframe
>>>
>>>
>>> # perhaps %in% could do the job?
>>>
>>> Appreciate any advice.
>>>
>>> --Chris Ryan
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From annemariefischer86 at gmail.com  Thu Jul  9 18:38:01 2015
From: annemariefischer86 at gmail.com (annemariefischer86 at gmail.com)
Date: Thu, 09 Jul 2015 18:38:01 +0200
Subject: [R] Maxent Jarfile
In-Reply-To: <05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
	<05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
Message-ID: <20150709163801.5849232.67320.898@gmail.com>


Hi John,

I have attempted to add links to the files to the thread directly, but my message was not ?authorized. I'm not sure why.?

?https://www.dropbox.com/s/gqk908gw9ze9553/error.txt?dl=0

?https://www.dropbox.com/s/jffg48y4dojzsb3/R%20Studio%20code.txt?dl=0

?https://www.dropbox.com/s/npyppmm9lv4wy8b/Rhinolophus_species.csv?dl=0

I sincerely hope there are experts out there, as I still am having no luck and as yet no response from the package maintainer.

? Original Message ?
From: David Winsemius
Sent: Thursday 9 July 2015 18:15
To: John Kane
Cc: R. Mailing List
Subject: Re: [R] Maxent Jarfile


On Jul 9, 2015, at 6:02 AM, John Kane wrote:

> Thanks Annemarie,
> Things came through okay. I'll defer to the experts on substantive advice.
> 

John;

It only came through to you by way of a copy sent directly to you. It did not come to you from the rhelp-server. If there are any experts out there, then they did not get a copy.

The advice to contact the package maintainer seemed the best avenue to proceed along.

David.


> John Kane
> Kingston ON Canada
> 
> 
>> -----Original Message-----
>> From: annemariefischer86 at gmail.com
>> Sent: Thu, 09 Jul 2015 07:25:06 +0200
>> To: jrkrideau at inbox.com, annemarie_dh at hotmail.com, r-help at r-project.org
>> Subject: Re: [R] Maxent Jarfile
>> 
>> ?Hi John,
>> 
>> Sorry about that. Please find attached the code, error and input file.
>> 
>> Thanks,
>> Annemarie
>> 
>> 
>> Original Message
>> From: John Kane
>> Sent: Thursday 9 July 2015 00:26
>> To: Annemarie Fischer; r-help at r-project.org
>> Subject: Re: [R] Maxent Jarfile
>> 
>> Hi Annemarie,
>> You have sent the email in HTML and it is very close to unreadable. Could
>> you please resubmit the message in plain text. R-help does not accept
>> HTML and, as happened here, the text gets seriously mangled.
>> 
>> 
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: annemarie_dh at hotmail.com
>>> Sent: Wed, 8 Jul 2015 20:22:57 +0000
>>> To: r-help at r-project.org
>>> Subject: [R] Maxent Jarfile
>>> 
>>> Hi,
>>> I have been trying to solve the below problem for 2 days with no
>>> success.
>>> Hopefully you can help as i can find no assistance online.
>>> I am attempting to run the niche.equivalency.test and the
>>> bg.similarity.test using RStudio and Maxent. I keep getting the error:
>>> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
>>> cannot open the connectionIn addition: Warning messages:1: running
>>> command 'java -jar C:/Program
>>> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
>>> R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
>>> R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
>>> nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
>>> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
>>> directory
>>> I suspect the issue is that the file directory doesnt have "", but i
>>> have
>>> no idea how to add these in, as in RStudio values, the "" does appear.
>>> My code is:
>>> # load required
>> packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
>>> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
>>> # path to MAXENT# --------------maxent.exe <-
>>> paste(system.file(package="dismo"),
>>> "/java/maxent.jar", sep = "")
>>> # a data frame of coordinates where two species # have been detected
>>> ('presence points') and# a raster stack of environmental covariables#
>>> --------------------------------------
>>> ###Change to correct species usedfile <-
>>> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
>>> sep="")# this is the file we will use:file
>>> #save(file, file="Molossidae_rarefied_points.rda")
>>> #data()#data(package = .packages(all.available = TRUE))
>>> #myData <- read.csv("file", header=TRUE, nrows=10000)
>>> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
>>> species <- c("Rhinolophus blasii", "Rhinolophus
>>> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
>>> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
>>> system.file("extdata", package = "phyloclim")preds <- list.files(path =
>>> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
>>> "/")preds <- stack(lapply(X = preds, FUN = raster))
>>> # testing against 9 permutations of the data#
>>> -------------------------------------------reps <- 1000
>>> # run hypothesis tests# --------------------if
>>> (file.exists(maxent.exe)){
>>> net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
>>> plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
>>> maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
>>> Details)")}
>>> [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ____________________________________________________________
>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ____________________________________________________________
> Can't remember your password? Do you need a strong and secure password?
> Use Password manager! It stores your passwords & protects your account.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From roy.mendelssohn at noaa.gov  Thu Jul  9 18:47:56 2015
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 9 Jul 2015 09:47:56 -0700
Subject: [R] Maxent Jarfile
In-Reply-To: <20150709163801.5849232.67320.898@gmail.com>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
	<05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
	<20150709163801.5849232.67320.898@gmail.com>
Message-ID: <5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>

Have you been able to run the example that comes with dismo::maxent such as at the bottom of:

http://www.inside-r.org/packages/cran/dismo/docs/maxent

-Roy

> On Jul 9, 2015, at 9:38 AM, annemariefischer86 at gmail.com wrote:
> 
> 
> Hi John,
> 
> I have attempted to add links to the files to the thread directly, but my message was not ?authorized. I'm not sure why. 
> 
> ?https://www.dropbox.com/s/gqk908gw9ze9553/error.txt?dl=0
> 
> ?https://www.dropbox.com/s/jffg48y4dojzsb3/R%20Studio%20code.txt?dl=0
> 
> ?https://www.dropbox.com/s/npyppmm9lv4wy8b/Rhinolophus_species.csv?dl=0
> 
> I sincerely hope there are experts out there, as I still am having no luck and as yet no response from the package maintainer.
> 
>   Original Message  
> From: David Winsemius
> Sent: Thursday 9 July 2015 18:15
> To: John Kane
> Cc: R. Mailing List
> Subject: Re: [R] Maxent Jarfile
> 
> 
> On Jul 9, 2015, at 6:02 AM, John Kane wrote:
> 
>> Thanks Annemarie,
>> Things came through okay. I'll defer to the experts on substantive advice.
>> 
> 
> John;
> 
> It only came through to you by way of a copy sent directly to you. It did not come to you from the rhelp-server. If there are any experts out there, then they did not get a copy.
> 
> The advice to contact the package maintainer seemed the best avenue to proceed along.
> 
> David.
> 
> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: annemariefischer86 at gmail.com
>>> Sent: Thu, 09 Jul 2015 07:25:06 +0200
>>> To: jrkrideau at inbox.com, annemarie_dh at hotmail.com, r-help at r-project.org
>>> Subject: Re: [R] Maxent Jarfile
>>> 
>>> ?Hi John,
>>> 
>>> Sorry about that. Please find attached the code, error and input file.
>>> 
>>> Thanks,
>>> Annemarie
>>> 
>>> 
>>> Original Message
>>> From: John Kane
>>> Sent: Thursday 9 July 2015 00:26
>>> To: Annemarie Fischer; r-help at r-project.org
>>> Subject: Re: [R] Maxent Jarfile
>>> 
>>> Hi Annemarie,
>>> You have sent the email in HTML and it is very close to unreadable. Could
>>> you please resubmit the message in plain text. R-help does not accept
>>> HTML and, as happened here, the text gets seriously mangled.
>>> 
>>> 
>>> 
>>> John Kane
>>> Kingston ON Canada
>>> 
>>> 
>>>> -----Original Message-----
>>>> From: annemarie_dh at hotmail.com
>>>> Sent: Wed, 8 Jul 2015 20:22:57 +0000
>>>> To: r-help at r-project.org
>>>> Subject: [R] Maxent Jarfile
>>>> 
>>>> Hi,
>>>> I have been trying to solve the below problem for 2 days with no
>>>> success.
>>>> Hopefully you can help as i can find no assistance online.
>>>> I am attempting to run the niche.equivalency.test and the
>>>> bg.similarity.test using RStudio and Maxent. I keep getting the error:
>>>> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
>>>> cannot open the connectionIn addition: Warning messages:1: running
>>>> command 'java -jar C:/Program
>>>> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
>>>> R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
>>>> R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
>>>> nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
>>>> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
>>>> directory
>>>> I suspect the issue is that the file directory doesnt have "", but i
>>>> have
>>>> no idea how to add these in, as in RStudio values, the "" does appear.
>>>> My code is:
>>>> # load required
>>> packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
>>>> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
>>>> # path to MAXENT# --------------maxent.exe <-
>>>> paste(system.file(package="dismo"),
>>>> "/java/maxent.jar", sep = "")
>>>> # a data frame of coordinates where two species # have been detected
>>>> ('presence points') and# a raster stack of environmental covariables#
>>>> --------------------------------------
>>>> ###Change to correct species usedfile <-
>>>> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
>>>> sep="")# this is the file we will use:file
>>>> #save(file, file="Molossidae_rarefied_points.rda")
>>>> #data()#data(package = .packages(all.available = TRUE))
>>>> #myData <- read.csv("file", header=TRUE, nrows=10000)
>>>> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
>>>> species <- c("Rhinolophus blasii", "Rhinolophus
>>>> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
>>>> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
>>>> system.file("extdata", package = "phyloclim")preds <- list.files(path =
>>>> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
>>>> "/")preds <- stack(lapply(X = preds, FUN = raster))
>>>> # testing against 9 permutations of the data#
>>>> -------------------------------------------reps <- 1000
>>>> # run hypothesis tests# --------------------if
>>>> (file.exists(maxent.exe)){
>>>> net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
>>>> plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
>>>> maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
>>>> Details)")}
>>>> [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ____________________________________________________________
>>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ____________________________________________________________
>> Can't remember your password? Do you need a strong and secure password?
>> Use Password manager! It stores your passwords & protects your account.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From annemariefischer86 at gmail.com  Thu Jul  9 19:10:38 2015
From: annemariefischer86 at gmail.com (annemariefischer86 at gmail.com)
Date: Thu, 09 Jul 2015 19:10:38 +0200
Subject: [R] Maxent Jarfile
In-Reply-To: <5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
	<05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
	<20150709163801.5849232.67320.898@gmail.com>
	<5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>
Message-ID: <20150709171038.5849232.49711.902@gmail.com>

?Hi Roy,

This example gives me the following error on trying to run the first line:

Error: unexpected '&' in "jar <- paste(system.file(package="dismo"),
"/java/maxent.jar", sep=&"

I am currently attempting to run using the code given in the below thread:

http://r.789695.n4.nabble.com/phyloclim-help-td4633283.html

? Original Message ?
From: Roy Mendelssohn - NOAA Federal
Sent: Thursday 9 July 2015 18:48
To: annemariefischer86 at gmail.com
Cc: David Winsemius; John Kane; R. Mailing List
Subject: Re: [R] Maxent Jarfile


From bgunter.4567 at gmail.com  Thu Jul  9 19:12:23 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 9 Jul 2015 10:12:23 -0700
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
Message-ID: <CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>

Jeff:

Well, it would be much better (no loops!) except, I think, for one
issue: "red" would match "barred" and I don't think that this is what
is wanted: the matches should be on whole "words" not just string
patterns.

So you would need to fix up the matching pattern to make this work,
but it may be a little tricky, as arbitrary whitespace characters,
e.g. " " or "\n" etc. could be in the strings to be matched separating
the words or ending the "sentence."  I'm sure it can be done, but I'll
leave it to you or others to figure it out.

Of course, if my diagnosis is wrong or silly, please point this out.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> I think grep is better suited to this:
>
> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call( paste, zz[ , 2:3 ] ) ) )
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 9, 2015 8:51:10 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>Here's a way to do it that uses %in% (i.e. match() ) and uses only a
>>single, not a double, loop. It should be more efficient.
>>
>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>>+       function(x)any(x %in% alarm.words))
>>
>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>>
>>The idea is to paste the strings in each row (do.call allows an
>>arbitrary number of columns) into a single string and then use
>>strsplit to break the string into individual "words" on whitespace.
>>Then the matching is vectorized with the any( %in% ... ) call.
>>
>>Cheers,
>>Bert
>>Bert Gunter
>>
>>"Data is not information. Information is not knowledge. And knowledge
>>is certainly not wisdom."
>>   -- Clifford Stoll
>>
>>
>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>>> Dear Chris,
>>>
>>> If I understand correctly what you want, how about the following?
>>>
>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>>grepl, x=x)))
>>>> zz[rows, ]
>>>
>>>           v1                              v2                v3 v4
>>> 3  -1.022329                    green turtle    ronald weasley  2
>>> 6   0.336599              waffle the hamster        red sparks  1
>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>>> 10  1.130622                      black bear  gandalf the grey  2
>>>
>>> I hope this helps,
>>>  John
>>>
>>> ------------------------------------------------
>>> John Fox, Professor
>>> McMaster University
>>> Hamilton, Ontario, Canada
>>> http://socserv.mcmaster.ca/jfox/
>>>
>>>
>>> On Wed, 08 Jul 2015 22:23:37 -0400
>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>>> Running R 3.1.1 on windows 7
>>>>
>>>> I want to identify as a case any record in a dataframe that contains
>>any
>>>> of several keywords in any of several variables.
>>>>
>>>> Example:
>>>>
>>>> # create a dataframe with 4 variables and 10 records
>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>>fox",
>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>>"hello
>>>> world", "yellow giraffe with a long neck", "black bear")
>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>>"ginny
>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>>dress
>>>> robes", "gandalf the white", "gandalf the grey")
>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10, lambda=2),
>>>> stringsAsFactors=FALSE)
>>>> str(zz)
>>>> zz
>>>>
>>>> # here are the keywords
>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>>>
>>>> # For each row/record, I want to test whether the string in v2 or
>>the
>>>> string in v3 contains any of the strings in alarm.words. And then if
>>so,
>>>> set zz$v5=TRUE for that record.
>>>>
>>>> # I'm thinking the str_detect function in the stringr package ought
>>to
>>>> be able to help, perhaps with some use of apply over the rows, but I
>>>> obviously misunderstand something about how str_detect works
>>>>
>>>> library(stringr)
>>>>
>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>>search
>>>>                                      # must be a vector, not
>>multiple
>>>>                                      # columns
>>>>
>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>>>
>>>> str_detect(zz[,2], alarm.words)      # error, length of alarm.words
>>>>                                      # is less than the number of
>>>>                                      # rows I am using for the
>>>>                                      # comparison
>>>>
>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>>> length(alarm.words)                  # confining nrows
>>>>                                      # to the length of alarm.words
>>>>
>>>> str_detect(zz, alarm.words)          # obviously not right
>>>>
>>>> # maybe I need apply() ?
>>>> my.f <- function(x){str_detect(x, alarm.words)}
>>>>
>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>>>                            # between alarm.words and that
>>>>                            # in which I am searching for
>>>>                            # matching strings
>>>>
>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>>>                            # rows of the dataframe
>>>>
>>>>
>>>> # perhaps %in% could do the job?
>>>>
>>>> Appreciate any advice.
>>>>
>>>> --Chris Ryan
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>


From bgunter.4567 at gmail.com  Thu Jul  9 19:24:18 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 9 Jul 2015 10:24:18 -0700
Subject: [R] ca.jo function, urca package, singular matrix problem
In-Reply-To: <1436449879103-4709635.post@n4.nabble.com>
References: <1436449879103-4709635.post@n4.nabble.com>
Message-ID: <CAGxFJbTXcRJt0m82njVKOw4k5qa_xogBcEBdmNewzU9x9fOSKg@mail.gmail.com>

I do know nothing about what you are specifically doing (!!) but...

In general, R uses factors and contrasts not dummy variables to handle
categorical variables in (linear) models, so it might be that your
dummy variable representation is what causes the singularity. If so,
do a web search on "contrasts in R" for background and details. Also
?contrasts and the linked man pages in R.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 9, 2015 at 6:51 AM, mrrox <r.otojanov at qmul.ac.uk> wrote:
> Hi, I am trying to run a cointegration test with a dummy variable using
> `*ca.jo*` function in `*urca*` package.
>
> *johcoint=ca.jo(Ydata[10:60,1:5],type="trace",ecdet=c("const"),K=2,spec="transitory",dumvar=dumvar)
> *
>  `*dumvar*` is the binary variable that take 1 and 0 only. the first two
> observations are 1 and the rest are 0s.
> when I run the code, I get
>
>    / Error in solve.default(M11) :
>           Lapack routine dgesv: system is exactly singular: U[1,1] = 0/
>
> I think this is something to do with the invertability of the input matrix,
> and this occurs only when I use `*dumvar*` only. The error message
> disappears if I add a 1 to the 3rd observation of `dumvar`.
>
> Below is the sample data just for info:
>
>
>               A             B            C             D             E
> dumvar
>     1  2.255446 1.688807 1.506579 1.880152 9.575868      1
>     2  2.230118 1.578281 1.546805 1.905426 9.545534      1
>     3  2.255446 1.688807 1.506579 1.880152 9.575868      0
>     4  2.230118 1.578281 1.546805 1.905426 9.545534      0
>     5  2.255446 1.688807 1.506579 1.880152 9.575868      0
>     6  2.230118 1.578281 1.546805 1.905426 9.545534      0
>     7  2.255446 1.688807 1.506579 1.880152 9.575868      0
>     8  2.230118 1.578281 1.546805 1.905426 9.545534      0
>     9  2.255446 1.688807 1.506579 1.880152 9.575868      0
>     10 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     11 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     12 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     13 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     14 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     15 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     16 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     17 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     18 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     19 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     20 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     21 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     22 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     23 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     24 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     25 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     26 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     27 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     28 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     29 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     30 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     31 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     32 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     33 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     34 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     35 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     36 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     37 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     38 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     39 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     40 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     41 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     42 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     43 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     44 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     45 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     46 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     47 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     48 2.230118 1.578281 1.546805 1.905426 9.545534      0
>     49 2.255446 1.688807 1.506579 1.880152 9.575868      0
>     50 2.230118 1.578281 1.546805 1.905426 9.545534      0
>
> Thank you!
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/ca-jo-function-urca-package-singular-matrix-problem-tp4709635.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Thu Jul  9 19:30:01 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 09 Jul 2015 10:30:01 -0700
Subject: [R] detecting any element in a vector of strings,
	appearing anywhere in any of several character variables in a
	dataframe
In-Reply-To: <CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
Message-ID: <7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>

Just add a word break marker before and after:

zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ), ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Jeff:
>
>Well, it would be much better (no loops!) except, I think, for one
>issue: "red" would match "barred" and I don't think that this is what
>is wanted: the matches should be on whole "words" not just string
>patterns.
>
>So you would need to fix up the matching pattern to make this work,
>but it may be a little tricky, as arbitrary whitespace characters,
>e.g. " " or "\n" etc. could be in the strings to be matched separating
>the words or ending the "sentence."  I'm sure it can be done, but I'll
>leave it to you or others to figure it out.
>
>Of course, if my diagnosis is wrong or silly, please point this out.
>
>Cheers,
>Bert
>
>
>Bert Gunter
>
>"Data is not information. Information is not knowledge. And knowledge
>is certainly not wisdom."
>   -- Clifford Stoll
>
>
>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>> I think grep is better suited to this:
>>
>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call( paste,
>zz[ , 2:3 ] ) ) )
>>
>---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>>
>---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>wrote:
>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only a
>>>single, not a double, loop. It should be more efficient.
>>>
>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>>>+       function(x)any(x %in% alarm.words))
>>>
>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>>>
>>>The idea is to paste the strings in each row (do.call allows an
>>>arbitrary number of columns) into a single string and then use
>>>strsplit to break the string into individual "words" on whitespace.
>>>Then the matching is vectorized with the any( %in% ... ) call.
>>>
>>>Cheers,
>>>Bert
>>>Bert Gunter
>>>
>>>"Data is not information. Information is not knowledge. And knowledge
>>>is certainly not wisdom."
>>>   -- Clifford Stoll
>>>
>>>
>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>>>> Dear Chris,
>>>>
>>>> If I understand correctly what you want, how about the following?
>>>>
>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>>>grepl, x=x)))
>>>>> zz[rows, ]
>>>>
>>>>           v1                              v2                v3 v4
>>>> 3  -1.022329                    green turtle    ronald weasley  2
>>>> 6   0.336599              waffle the hamster        red sparks  1
>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>>>> 10  1.130622                      black bear  gandalf the grey  2
>>>>
>>>> I hope this helps,
>>>>  John
>>>>
>>>> ------------------------------------------------
>>>> John Fox, Professor
>>>> McMaster University
>>>> Hamilton, Ontario, Canada
>>>> http://socserv.mcmaster.ca/jfox/
>>>>
>>>>
>>>> On Wed, 08 Jul 2015 22:23:37 -0400
>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>>>> Running R 3.1.1 on windows 7
>>>>>
>>>>> I want to identify as a case any record in a dataframe that
>contains
>>>any
>>>>> of several keywords in any of several variables.
>>>>>
>>>>> Example:
>>>>>
>>>>> # create a dataframe with 4 variables and 10 records
>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>>>fox",
>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>>>"hello
>>>>> world", "yellow giraffe with a long neck", "black bear")
>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>>>"ginny
>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>>>dress
>>>>> robes", "gandalf the white", "gandalf the grey")
>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
>lambda=2),
>>>>> stringsAsFactors=FALSE)
>>>>> str(zz)
>>>>> zz
>>>>>
>>>>> # here are the keywords
>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>>>>
>>>>> # For each row/record, I want to test whether the string in v2 or
>>>the
>>>>> string in v3 contains any of the strings in alarm.words. And then
>if
>>>so,
>>>>> set zz$v5=TRUE for that record.
>>>>>
>>>>> # I'm thinking the str_detect function in the stringr package
>ought
>>>to
>>>>> be able to help, perhaps with some use of apply over the rows, but
>I
>>>>> obviously misunderstand something about how str_detect works
>>>>>
>>>>> library(stringr)
>>>>>
>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>>>search
>>>>>                                      # must be a vector, not
>>>multiple
>>>>>                                      # columns
>>>>>
>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>>>>
>>>>> str_detect(zz[,2], alarm.words)      # error, length of
>alarm.words
>>>>>                                      # is less than the number of
>>>>>                                      # rows I am using for the
>>>>>                                      # comparison
>>>>>
>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>>>> length(alarm.words)                  # confining nrows
>>>>>                                      # to the length of
>alarm.words
>>>>>
>>>>> str_detect(zz, alarm.words)          # obviously not right
>>>>>
>>>>> # maybe I need apply() ?
>>>>> my.f <- function(x){str_detect(x, alarm.words)}
>>>>>
>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>>>>                            # between alarm.words and that
>>>>>                            # in which I am searching for
>>>>>                            # matching strings
>>>>>
>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>>>>                            # rows of the dataframe
>>>>>
>>>>>
>>>>> # perhaps %in% could do the job?
>>>>>
>>>>> Appreciate any advice.
>>>>>
>>>>> --Chris Ryan
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>______________________________________________
>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide
>>>http://www.R-project.org/posting-guide.html
>>>and provide commented, minimal, self-contained, reproducible code.
>>


From bgunter.4567 at gmail.com  Thu Jul  9 19:52:45 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 9 Jul 2015 10:52:45 -0700
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
	<7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>
Message-ID: <CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>

Yup, that does it. Let grep figure out what's a word rather than doing
it manually. Forgot about "\b"

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 9, 2015 at 10:30 AM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> Just add a word break marker before and after:
>
> zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ), ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>Jeff:
>>
>>Well, it would be much better (no loops!) except, I think, for one
>>issue: "red" would match "barred" and I don't think that this is what
>>is wanted: the matches should be on whole "words" not just string
>>patterns.
>>
>>So you would need to fix up the matching pattern to make this work,
>>but it may be a little tricky, as arbitrary whitespace characters,
>>e.g. " " or "\n" etc. could be in the strings to be matched separating
>>the words or ending the "sentence."  I'm sure it can be done, but I'll
>>leave it to you or others to figure it out.
>>
>>Of course, if my diagnosis is wrong or silly, please point this out.
>>
>>Cheers,
>>Bert
>>
>>
>>Bert Gunter
>>
>>"Data is not information. Information is not knowledge. And knowledge
>>is certainly not wisdom."
>>   -- Clifford Stoll
>>
>>
>>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
>><jdnewmil at dcn.davis.ca.us> wrote:
>>> I think grep is better suited to this:
>>>
>>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call( paste,
>>zz[ , 2:3 ] ) ) )
>>>
>>---------------------------------------------------------------------------
>>> Jeff Newmiller                        The     .....       .....  Go
>>Live...
>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>Go...
>>>                                       Live:   OO#.. Dead: OO#..
>>Playing
>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>rocks...1k
>>>
>>---------------------------------------------------------------------------
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>>wrote:
>>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only a
>>>>single, not a double, loop. It should be more efficient.
>>>>
>>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>>>>+       function(x)any(x %in% alarm.words))
>>>>
>>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>>>>
>>>>The idea is to paste the strings in each row (do.call allows an
>>>>arbitrary number of columns) into a single string and then use
>>>>strsplit to break the string into individual "words" on whitespace.
>>>>Then the matching is vectorized with the any( %in% ... ) call.
>>>>
>>>>Cheers,
>>>>Bert
>>>>Bert Gunter
>>>>
>>>>"Data is not information. Information is not knowledge. And knowledge
>>>>is certainly not wisdom."
>>>>   -- Clifford Stoll
>>>>
>>>>
>>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>>>>> Dear Chris,
>>>>>
>>>>> If I understand correctly what you want, how about the following?
>>>>>
>>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>>>>grepl, x=x)))
>>>>>> zz[rows, ]
>>>>>
>>>>>           v1                              v2                v3 v4
>>>>> 3  -1.022329                    green turtle    ronald weasley  2
>>>>> 6   0.336599              waffle the hamster        red sparks  1
>>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>>>>> 10  1.130622                      black bear  gandalf the grey  2
>>>>>
>>>>> I hope this helps,
>>>>>  John
>>>>>
>>>>> ------------------------------------------------
>>>>> John Fox, Professor
>>>>> McMaster University
>>>>> Hamilton, Ontario, Canada
>>>>> http://socserv.mcmaster.ca/jfox/
>>>>>
>>>>>
>>>>> On Wed, 08 Jul 2015 22:23:37 -0400
>>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>>>>> Running R 3.1.1 on windows 7
>>>>>>
>>>>>> I want to identify as a case any record in a dataframe that
>>contains
>>>>any
>>>>>> of several keywords in any of several variables.
>>>>>>
>>>>>> Example:
>>>>>>
>>>>>> # create a dataframe with 4 variables and 10 records
>>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>>>>fox",
>>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>>>>"hello
>>>>>> world", "yellow giraffe with a long neck", "black bear")
>>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>>>>"ginny
>>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>>>>dress
>>>>>> robes", "gandalf the white", "gandalf the grey")
>>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
>>lambda=2),
>>>>>> stringsAsFactors=FALSE)
>>>>>> str(zz)
>>>>>> zz
>>>>>>
>>>>>> # here are the keywords
>>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>>>>>
>>>>>> # For each row/record, I want to test whether the string in v2 or
>>>>the
>>>>>> string in v3 contains any of the strings in alarm.words. And then
>>if
>>>>so,
>>>>>> set zz$v5=TRUE for that record.
>>>>>>
>>>>>> # I'm thinking the str_detect function in the stringr package
>>ought
>>>>to
>>>>>> be able to help, perhaps with some use of apply over the rows, but
>>I
>>>>>> obviously misunderstand something about how str_detect works
>>>>>>
>>>>>> library(stringr)
>>>>>>
>>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>>>>search
>>>>>>                                      # must be a vector, not
>>>>multiple
>>>>>>                                      # columns
>>>>>>
>>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>>>>>
>>>>>> str_detect(zz[,2], alarm.words)      # error, length of
>>alarm.words
>>>>>>                                      # is less than the number of
>>>>>>                                      # rows I am using for the
>>>>>>                                      # comparison
>>>>>>
>>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>>>>> length(alarm.words)                  # confining nrows
>>>>>>                                      # to the length of
>>alarm.words
>>>>>>
>>>>>> str_detect(zz, alarm.words)          # obviously not right
>>>>>>
>>>>>> # maybe I need apply() ?
>>>>>> my.f <- function(x){str_detect(x, alarm.words)}
>>>>>>
>>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>>>>>                            # between alarm.words and that
>>>>>>                            # in which I am searching for
>>>>>>                            # matching strings
>>>>>>
>>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>>>>>                            # rows of the dataframe
>>>>>>
>>>>>>
>>>>>> # perhaps %in% could do the job?
>>>>>>
>>>>>> Appreciate any advice.
>>>>>>
>>>>>> --Chris Ryan
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>______________________________________________
>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide
>>>>http://www.R-project.org/posting-guide.html
>>>>and provide commented, minimal, self-contained, reproducible code.
>>>
>


From annemariefischer86 at gmail.com  Thu Jul  9 20:00:00 2015
From: annemariefischer86 at gmail.com (annemariefischer86 at gmail.com)
Date: Thu, 09 Jul 2015 20:00:00 +0200
Subject: [R] Maxent Jarfile
In-Reply-To: <5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
	<05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
	<20150709163801.5849232.67320.898@gmail.com>
	<5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>
Message-ID: <20150709180000.5849232.98907.910@gmail.com>

?Hi Roy,

Using the code in the other path also did not work, as I am not sure what to use for the projections or background.


? Original Message ?
From: Roy Mendelssohn - NOAA Federal
Sent: Thursday 9 July 2015 18:48
To: annemariefischer86 at gmail.com
Cc: David Winsemius; John Kane; R. Mailing List
Subject: Re: [R] Maxent Jarfile

Have you been able to run the example that comes with dismo::maxent such as at the bottom of:

http://www.inside-r.org/packages/cran/dismo/docs/maxent

-Roy

> On Jul 9, 2015, at 9:38 AM, annemariefischer86 at gmail.com wrote:
> 
> 
> Hi John,
> 
> I have attempted to add links to the files to the thread directly, but my message was not ?authorized. I'm not sure why. 
> 
> ?https://www.dropbox.com/s/gqk908gw9ze9553/error.txt?dl=0
> 
> ?https://www.dropbox.com/s/jffg48y4dojzsb3/R%20Studio%20code.txt?dl=0
> 
> ?https://www.dropbox.com/s/npyppmm9lv4wy8b/Rhinolophus_species.csv?dl=0
> 
> I sincerely hope there are experts out there, as I still am having no luck and as yet no response from the package maintainer.
> 
> Original Message 
> From: David Winsemius
> Sent: Thursday 9 July 2015 18:15
> To: John Kane
> Cc: R. Mailing List
> Subject: Re: [R] Maxent Jarfile
> 
> 
> On Jul 9, 2015, at 6:02 AM, John Kane wrote:
> 
>> Thanks Annemarie,
>> Things came through okay. I'll defer to the experts on substantive advice.
>> 
> 
> John;
> 
> It only came through to you by way of a copy sent directly to you. It did not come to you from the rhelp-server. If there are any experts out there, then they did not get a copy.
> 
> The advice to contact the package maintainer seemed the best avenue to proceed along.
> 
> David.
> 
> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: annemariefischer86 at gmail.com
>>> Sent: Thu, 09 Jul 2015 07:25:06 +0200
>>> To: jrkrideau at inbox.com, annemarie_dh at hotmail.com, r-help at r-project.org
>>> Subject: Re: [R] Maxent Jarfile
>>> 
>>> ?Hi John,
>>> 
>>> Sorry about that. Please find attached the code, error and input file.
>>> 
>>> Thanks,
>>> Annemarie
>>> 
>>> 
>>> Original Message
>>> From: John Kane
>>> Sent: Thursday 9 July 2015 00:26
>>> To: Annemarie Fischer; r-help at r-project.org
>>> Subject: Re: [R] Maxent Jarfile
>>> 
>>> Hi Annemarie,
>>> You have sent the email in HTML and it is very close to unreadable. Could
>>> you please resubmit the message in plain text. R-help does not accept
>>> HTML and, as happened here, the text gets seriously mangled.
>>> 
>>> 
>>> 
>>> John Kane
>>> Kingston ON Canada
>>> 
>>> 
>>>> -----Original Message-----
>>>> From: annemarie_dh at hotmail.com
>>>> Sent: Wed, 8 Jul 2015 20:22:57 +0000
>>>> To: r-help at r-project.org
>>>> Subject: [R] Maxent Jarfile
>>>> 
>>>> Hi,
>>>> I have been trying to solve the below problem for 2 days with no
>>>> success.
>>>> Hopefully you can help as i can find no assistance online.
>>>> I am attempting to run the niche.equivalency.test and the
>>>> bg.similarity.test using RStudio and Maxent. I keep getting the error:
>>>> Error: Unable to access jarfile C:/ProgramError in file(fname, "r") :
>>>> cannot open the connectionIn addition: Warning messages:1: running
>>>> command 'java -jar C:/Program
>>>> Files/R/R-3.1.3/library/dismo/java/maxent.jar -e
>>>> R.phyloclim.temp/background.csv -s R.phyloclim.temp/samples.csv -j
>>>> R.phyloclim.temp/proj/ -o R.phyloclim.temp/out/ -r removeduplicates
>>>> nopictures autorun' had status 1 2: In file(fname, "r") : cannot open
>>>> file 'R.phyloclim.temp/out/Rhinolophus blasii_proj.asc': No such file or
>>>> directory
>>>> I suspect the issue is that the file directory doesnt have "", but i
>>>> have
>>>> no idea how to add these in, as in RStudio values, the "" does appear.
>>>> My code is:
>>>> # load required
>>> packageslibrary(raster)library(mgcv)library(dismo)library(rgdal)library(ellipse)library(sp)library(proj4)library(rgeos)
>>>> library(rJava)library(maptools)library(rasterVis)library(phyloclim)
>>>> # path to MAXENT# --------------maxent.exe <-
>>>> paste(system.file(package="dismo"),
>>>> "/java/maxent.jar", sep = "")
>>>> # a data frame of coordinates where two species # have been detected
>>>> ('presence points') and# a raster stack of environmental covariables#
>>>> --------------------------------------
>>>> ###Change to correct species usedfile <-
>>>> paste(system.file(package="dismo"), "/ex/Rhinolophus_species.csv",
>>>> sep="")# this is the file we will use:file
>>>> #save(file, file="Molossidae_rarefied_points.rda")
>>>> #data()#data(package = .packages(all.available = TRUE))
>>>> #myData <- read.csv("file", header=TRUE, nrows=10000)
>>>> Rhinolophus_species <- read.table(file, header=TRUE, sep=',')
>>>> species <- c("Rhinolophus blasii", "Rhinolophus
>>>> clivosus")#data(sites)samples <- Rhinolophus_species[grep(paste(species,
>>>> collapse = "|"), Rhinolophus_species$Spp), ]data.path <-
>>>> system.file("extdata", package = "phyloclim")preds <- list.files(path =
>>>> data.path, pattern = "[.]asc")preds <- paste(data.path, preds, sep =
>>>> "/")preds <- stack(lapply(X = preds, FUN = raster))
>>>> # testing against 9 permutations of the data#
>>>> -------------------------------------------reps <- 1000
>>>> # run hypothesis tests# --------------------if
>>>> (file.exists(maxent.exe)){
>>>> net <- niche.equivalency.test(samples, preds, reps, maxent.exe) net;
>>>> plot(net) bst <- bg.similarity.test(samples, preds, reps, app =
>>>> maxent.exe) bst; plot(bst)} else { message("get a copy of MAXENT (see
>>>> Details)")}
>>>> [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> ____________________________________________________________
>>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ____________________________________________________________
>> Can't remember your password? Do you need a strong and secure password?
>> Use Password manager! It stores your passwords & protects your account.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From jdnewmil at dcn.davis.ca.us  Thu Jul  9 20:31:04 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 9 Jul 2015 11:31:04 -0700 (PDT)
Subject: [R] randomForest set.seed()
In-Reply-To: <CAJskk_8i5yqO25BxsWcKTqTSzfR0WXcZcS0M5t2cH_VUWmHTww@mail.gmail.com>
References: <CAJskk_8i5yqO25BxsWcKTqTSzfR0WXcZcS0M5t2cH_VUWmHTww@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1507091126570.36658@pedal.dcn.davis.ca.us>

Please post in plain text, and supply a reproducible example (that 
includes sample data, preferably using the dput function).

The below code obtains repeatable random number sequences in each pass 
through the loop, as confirmed by the first value in each sequence.

##
a <- 0
##
## For loop to test the various combinations of ntree and mtry
for ( j in 1:8 ) {
     for ( i in 1:3 ) {
         b <- 1968 + a
         set.seed( b )
         cat( i, " ", j, " ", runif( 1 ), "\n" )
     }
}


On Thu, 9 Jul 2015, Joao Carreiras wrote:

> Dear forum members,
>
> ?I wrote a piece of code to test various combinations of mtry and ntree, so
> that the best combination (in terms of mse?) could be used. Before each
> randomForest command I included a set.seed() command, so that I can keep
> track of the seed number and replicate the results. However, when I run the
> randomForest command again with the seed number corresponding to the best
> combination I get a different mse. Any suggestions?
> Thanks and best wishes
> Joao
>
> ------------
> ## Define the number of trees to test
> nt <- c(100, 200, 300, 400, 500, 700, 900, 1000)
> ## Define the number of variables to be randomly selected at each node
> mt <- c(1, 2, 3)
> ##
> a <- 0
> ##
> ## For loop to test the various combinations of ntree and mtry
> for (j in 1:8)
>   for (i in 1:3)
> ? ?
> {
>         b <- 1968 + a
>         set.seed(b)
>         HH <- randomForest(HH_MEAN ~ ASF + PALU + FC
> ??
> ,
> ??
>                            data = model.data,
>                            ntree = nt[j],
>                            mtry = mt[i],
>                            replace = T,
> ?                     ?
> ? }?
> ?----------------?
>
> ??
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From roy.mendelssohn at noaa.gov  Thu Jul  9 20:34:30 2015
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Thu, 9 Jul 2015 11:34:30 -0700
Subject: [R] Maxent Jarfile
In-Reply-To: <20150709171038.5849232.49711.902@gmail.com>
References: <7a0b317ab6e.0000034fjrkrideau@inbox.com>
	<dub128-w680b85744a81777d161bfbea910@phx.gbl>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
	<05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
	<20150709163801.5849232.67320.898@gmail.com>
	<5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>
	<20150709171038.5849232.49711.902@gmail.com>
Message-ID: <A0AF1416-DE06-43CA-9AB8-C9BDBAD7C801@noaa.gov>

This suggests that you either have the maxent.jar file in the wrong place or with the wrong name.  I suspect when you get the first two lines of the script to work you will be okay.

-Roy

> On Jul 9, 2015, at 10:10 AM, annemariefischer86 at gmail.com wrote:
> 
> ?Hi Roy,
> 
> This example gives me the following error on trying to run the first line:
> 
> Error: unexpected '&' in "jar <- paste(system.file(package="dismo"),
> "/java/maxent.jar", sep=&"
> 
> I am currently attempting to run using the code given in the below thread:
> 
> http://r.789695.n4.nabble.com/phyloclim-help-td4633283.html
> 
>   Original Message  
> From: Roy Mendelssohn - NOAA Federal
> Sent: Thursday 9 July 2015 18:48
> To: annemariefischer86 at gmail.com
> Cc: David Winsemius; John Kane; R. Mailing List
> Subject: Re: [R] Maxent Jarfile
> 

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From jacksonmrodrigues at gmail.com  Thu Jul  9 21:02:05 2015
From: jacksonmrodrigues at gmail.com (Jackson Rodrigues)
Date: Thu, 9 Jul 2015 21:02:05 +0200
Subject: [R] degree of freedom of smoothing spline in Generalized Additive
	Model (GAM)
Message-ID: <CAPL76w8gbGFSs2RYgsuxc=y7j4Cxc6ZgL1pNj7kVER6WJGgxjg@mail.gmail.com>

Hi all,

I am trying to fit a Generalized Additive Model (GAM) with mgcv package
aiming to identify changes on time series just like Gavin Simpson did (
http://www.fromthebottomoftheheap.net/2014/05/15/identifying-periods-of-change-with-gams/
).

However, depending on value of degree of freedom (K) chosen , the result
change considerably.

I run the following code to fit the GAM
fit <- gamm(Mydata ~ s(Age, k =25), correlation = corARMA(form = ~ Age, p =
1))

The package documentation provides some support in "choose K" section by
exploring p-value and k-index (gam.check function), however, I have gotten
no p-value and K-index either.

Could anyone help me to find the best degree of freedom?


Thank you in advance for any support.

Jackson Rodrigues
-- 

Jackson M. Rodrigues
Department of Palynology and Climate Dynamics
Albrecht-von-Haller-Institute for Plant Sciences
Georg-August-University G?ttingen
Untere Karspuele 2
37073 G?ttingen/Germany
Tel.:   0049 (0) 176 8186 4994

	[[alternative HTML version deleted]]


From frainj at gmail.com  Thu Jul  9 21:23:47 2015
From: frainj at gmail.com (John C Frain)
Date: Thu, 9 Jul 2015 20:23:47 +0100
Subject: [R] ca.jo function, urca package, singular matrix problem
In-Reply-To: <CAGxFJbTXcRJt0m82njVKOw4k5qa_xogBcEBdmNewzU9x9fOSKg@mail.gmail.com>
References: <1436449879103-4709635.post@n4.nabble.com>
	<CAGxFJbTXcRJt0m82njVKOw4k5qa_xogBcEBdmNewzU9x9fOSKg@mail.gmail.com>
Message-ID: <CAHrK514zgEZNKu56=f1J7df5qDqSvaRzCVc_JD8rogrNsU8ZQQ@mail.gmail.com>

There is something very wrong in your data set. You appear to have two
distinct observations of your variables and these are continuously repeated
throughout the sample.
On 9 Jul 2015 18:26, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:

> I do know nothing about what you are specifically doing (!!) but...
>
> In general, R uses factors and contrasts not dummy variables to handle
> categorical variables in (linear) models, so it might be that your
> dummy variable representation is what causes the singularity. If so,
> do a web search on "contrasts in R" for background and details. Also
> ?contrasts and the linked man pages in R.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Jul 9, 2015 at 6:51 AM, mrrox <r.otojanov at qmul.ac.uk> wrote:
> > Hi, I am trying to run a cointegration test with a dummy variable using
> > `*ca.jo*` function in `*urca*` package.
> >
> > *johcoint=ca.jo
> (Ydata[10:60,1:5],type="trace",ecdet=c("const"),K=2,spec="transitory",dumvar=dumvar)
> > *
> >  `*dumvar*` is the binary variable that take 1 and 0 only. the first two
> > observations are 1 and the rest are 0s.
> > when I run the code, I get
> >
> >    / Error in solve.default(M11) :
> >           Lapack routine dgesv: system is exactly singular: U[1,1] = 0/
> >
> > I think this is something to do with the invertability of the input
> matrix,
> > and this occurs only when I use `*dumvar*` only. The error message
> > disappears if I add a 1 to the 3rd observation of `dumvar`.
> >
> > Below is the sample data just for info:
> >
> >
> >               A             B            C             D             E
> > dumvar
> >     1  2.255446 1.688807 1.506579 1.880152 9.575868      1
> >     2  2.230118 1.578281 1.546805 1.905426 9.545534      1
> >     3  2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     4  2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     5  2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     6  2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     7  2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     8  2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     9  2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     10 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     11 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     12 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     13 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     14 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     15 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     16 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     17 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     18 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     19 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     20 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     21 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     22 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     23 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     24 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     25 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     26 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     27 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     28 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     29 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     30 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     31 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     32 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     33 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     34 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     35 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     36 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     37 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     38 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     39 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     40 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     41 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     42 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     43 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     44 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     45 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     46 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     47 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     48 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >     49 2.255446 1.688807 1.506579 1.880152 9.575868      0
> >     50 2.230118 1.578281 1.546805 1.905426 9.545534      0
> >
> > Thank you!
> >
> >
> >
> > --
> > View this message in context:
> http://r.789695.n4.nabble.com/ca-jo-function-urca-package-singular-matrix-problem-tp4709635.html
> > Sent from the R help mailing list archive at Nabble.com.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Thu Jul  9 21:24:00 2015
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 9 Jul 2015 15:24:00 -0400
Subject: [R] detecting any element in a vector of strings,
	appearing anywhere in any of several character variables in a
	dataframe
In-Reply-To: <CAM+rpY=ucmtVQREyhcgoXzc7v5X+kWaej4DiOTbeZNajnB-CSg@mail.gmail.com>
References: <559DDB29.3040803@binghamton.edu>	<web-565125331@cgpsrv2.cis.mcmaster.ca>	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>	<7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>	<CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>
	<CAM+rpY=ucmtVQREyhcgoXzc7v5X+kWaej4DiOTbeZNajnB-CSg@mail.gmail.com>
Message-ID: <003b01d0ba7c$cf953e80$6ebfbb80$@mcmaster.ca>

Dear Christopher,

My usual orientation to this kind of one-off problem is that I'm looking for a simple correct solution. Computing time is usually much smaller than programming time. 

That said, Bert Gunter's solution was about 5 times faster in a simple check that I ran with microbenchmark, and Jeff Newmiller's solution was about 10 times faster. Both Bert's and Jeff's (eventual) solution protect against partial (rather than full-word) matches, while mine doesn't (though it could easily be modified to do that).

Best,
 John

> -----Original Message-----
> From: Christopher W Ryan [mailto:cryan at binghamton.edu]
> Sent: July-09-15 2:49 PM
> To: Bert Gunter
> Cc: Jeff Newmiller; R Help; John Fox
> Subject: Re: [R] detecting any element in a vector of strings, appearing
> anywhere in any of several character variables in a dataframe
> 
> Thanks everyone.  John's original solution worked great.  And with
> 27,000 records, 65 alarm.words, and 6 columns to search, it takes only
> about 15 seconds.  That is certainly adequate for my needs.  But I
> will try out the other strategies too.
> 
> And thanks also for lot's of new R things to learn--grep, grepl,
> do.call . . . that's always a bonus!
> 
> --Chris Ryan
> 
> On Thu, Jul 9, 2015 at 1:52 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> > Yup, that does it. Let grep figure out what's a word rather than doing
> > it manually. Forgot about "\b"
> >
> > Cheers,
> > Bert
> >
> >
> > Bert Gunter
> >
> > "Data is not information. Information is not knowledge. And knowledge
> > is certainly not wisdom."
> >    -- Clifford Stoll
> >
> >
> > On Thu, Jul 9, 2015 at 10:30 AM, Jeff Newmiller
> > <jdnewmil at dcn.davis.ca.us> wrote:
> >> Just add a word break marker before and after:
> >>
> >> zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ),
> ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
> >> ---------------------------------------------------------------------
> ------
> >> Jeff Newmiller                        The     .....       .....  Go
> Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
> >>                                       Live:   OO#.. Dead: OO#..
> Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >> ---------------------------------------------------------------------
> ------
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
> >>>Jeff:
> >>>
> >>>Well, it would be much better (no loops!) except, I think, for one
> >>>issue: "red" would match "barred" and I don't think that this is what
> >>>is wanted: the matches should be on whole "words" not just string
> >>>patterns.
> >>>
> >>>So you would need to fix up the matching pattern to make this work,
> >>>but it may be a little tricky, as arbitrary whitespace characters,
> >>>e.g. " " or "\n" etc. could be in the strings to be matched
> separating
> >>>the words or ending the "sentence."  I'm sure it can be done, but
> I'll
> >>>leave it to you or others to figure it out.
> >>>
> >>>Of course, if my diagnosis is wrong or silly, please point this out.
> >>>
> >>>Cheers,
> >>>Bert
> >>>
> >>>
> >>>Bert Gunter
> >>>
> >>>"Data is not information. Information is not knowledge. And knowledge
> >>>is certainly not wisdom."
> >>>   -- Clifford Stoll
> >>>
> >>>
> >>>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
> >>><jdnewmil at dcn.davis.ca.us> wrote:
> >>>> I think grep is better suited to this:
> >>>>
> >>>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call(
> paste,
> >>>zz[ , 2:3 ] ) ) )
> >>>>
> >>>---------------------------------------------------------------------
> ------
> >>>> Jeff Newmiller                        The     .....       .....  Go
> >>>Live...
> >>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
> Live
> >>>Go...
> >>>>                                       Live:   OO#.. Dead: OO#..
> >>>Playing
> >>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
> with
> >>>> /Software/Embedded Controllers)               .OO#.       .OO#.
> >>>rocks...1k
> >>>>
> >>>---------------------------------------------------------------------
> ------
> >>>> Sent from my phone. Please excuse my brevity.
> >>>>
> >>>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter
> <bgunter.4567 at gmail.com>
> >>>wrote:
> >>>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only
> a
> >>>>>single, not a double, loop. It should be more efficient.
> >>>>>
> >>>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
> >>>>>+       function(x)any(x %in% alarm.words))
> >>>>>
> >>>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
> >>>>>
> >>>>>The idea is to paste the strings in each row (do.call allows an
> >>>>>arbitrary number of columns) into a single string and then use
> >>>>>strsplit to break the string into individual "words" on whitespace.
> >>>>>Then the matching is vectorized with the any( %in% ... ) call.
> >>>>>
> >>>>>Cheers,
> >>>>>Bert
> >>>>>Bert Gunter
> >>>>>
> >>>>>"Data is not information. Information is not knowledge. And
> knowledge
> >>>>>is certainly not wisdom."
> >>>>>   -- Clifford Stoll
> >>>>>
> >>>>>
> >>>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
> >>>>>> Dear Chris,
> >>>>>>
> >>>>>> If I understand correctly what you want, how about the following?
> >>>>>>
> >>>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
> >>>>>grepl, x=x)))
> >>>>>>> zz[rows, ]
> >>>>>>
> >>>>>>           v1                              v2                v3 v4
> >>>>>> 3  -1.022329                    green turtle    ronald weasley  2
> >>>>>> 6   0.336599              waffle the hamster        red sparks  1
> >>>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
> >>>>>> 10  1.130622                      black bear  gandalf the grey  2
> >>>>>>
> >>>>>> I hope this helps,
> >>>>>>  John
> >>>>>>
> >>>>>> ------------------------------------------------
> >>>>>> John Fox, Professor
> >>>>>> McMaster University
> >>>>>> Hamilton, Ontario, Canada
> >>>>>> http://socserv.mcmaster.ca/jfox/
> >>>>>>
> >>>>>>
> >>>>>> On Wed, 08 Jul 2015 22:23:37 -0400
> >>>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
> >>>>>>> Running R 3.1.1 on windows 7
> >>>>>>>
> >>>>>>> I want to identify as a case any record in a dataframe that
> >>>contains
> >>>>>any
> >>>>>>> of several keywords in any of several variables.
> >>>>>>>
> >>>>>>> Example:
> >>>>>>>
> >>>>>>> # create a dataframe with 4 variables and 10 records
> >>>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
> >>>>>fox",
> >>>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
> >>>>>"hello
> >>>>>>> world", "yellow giraffe with a long neck", "black bear")
> >>>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
> >>>>>"ginny
> >>>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
> >>>>>dress
> >>>>>>> robes", "gandalf the white", "gandalf the grey")
> >>>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
> >>>lambda=2),
> >>>>>>> stringsAsFactors=FALSE)
> >>>>>>> str(zz)
> >>>>>>> zz
> >>>>>>>
> >>>>>>> # here are the keywords
> >>>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
> >>>>>>>
> >>>>>>> # For each row/record, I want to test whether the string in v2
> or
> >>>>>the
> >>>>>>> string in v3 contains any of the strings in alarm.words. And
> then
> >>>if
> >>>>>so,
> >>>>>>> set zz$v5=TRUE for that record.
> >>>>>>>
> >>>>>>> # I'm thinking the str_detect function in the stringr package
> >>>ought
> >>>>>to
> >>>>>>> be able to help, perhaps with some use of apply over the rows,
> but
> >>>I
> >>>>>>> obviously misunderstand something about how str_detect works
> >>>>>>>
> >>>>>>> library(stringr)
> >>>>>>>
> >>>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
> >>>>>search
> >>>>>>>                                      # must be a vector, not
> >>>>>multiple
> >>>>>>>                                      # columns
> >>>>>>>
> >>>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
> >>>>>>>
> >>>>>>> str_detect(zz[,2], alarm.words)      # error, length of
> >>>alarm.words
> >>>>>>>                                      # is less than the number
> of
> >>>>>>>                                      # rows I am using for the
> >>>>>>>                                      # comparison
> >>>>>>>
> >>>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
> >>>>>>> length(alarm.words)                  # confining nrows
> >>>>>>>                                      # to the length of
> >>>alarm.words
> >>>>>>>
> >>>>>>> str_detect(zz, alarm.words)          # obviously not right
> >>>>>>>
> >>>>>>> # maybe I need apply() ?
> >>>>>>> my.f <- function(x){str_detect(x, alarm.words)}
> >>>>>>>
> >>>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
> >>>>>>>                            # between alarm.words and that
> >>>>>>>                            # in which I am searching for
> >>>>>>>                            # matching strings
> >>>>>>>
> >>>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
> >>>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
> >>>>>>>                            # rows of the dataframe
> >>>>>>>
> >>>>>>>
> >>>>>>> # perhaps %in% could do the job?
> >>>>>>>
> >>>>>>> Appreciate any advice.
> >>>>>>>
> >>>>>>> --Chris Ryan
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide
> >>>>>http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible
> code.
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>>http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible
> code.
> >>>>>
> >>>>>______________________________________________
> >>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>PLEASE do read the posting guide
> >>>>>http://www.R-project.org/posting-guide.html
> >>>>>and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> > and provide commented, minimal, self-contained, reproducible code.


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From davidsmi at microsoft.com  Thu Jul  9 21:54:06 2015
From: davidsmi at microsoft.com (David Smith)
Date: Thu, 9 Jul 2015 19:54:06 +0000
Subject: [R] Revolutions blog: June roundup
Message-ID: <2E1BD329-F48F-4491-AF54-3F1DCF86E8A8@microsoft.com>

Since 2008, Revolution Analytics (and now Microsoft) staff and guests have written about R every weekday at the Revolutions blog:
 http://blog.revolutionanalytics.com
and every month I post a summary of articles from the previous month of particular interest to readers of r-help. 

In case you missed them, here are some articles related to R from the month of June:

The R Consortium, a trade group dedicated to the support and growth of the R Community, has launched with the R Foundation, Microsoft, RStudio and others as founding members: http://blog.revolutionanalytics.com/2015/06/r-consortium.html

A detailed FAQ for fitting Generalized Linear Models in R: http://blog.revolutionanalytics.com/2015/06/generalized-linear-mixed-models-the-faq.html

My presentation on Microsoft?s embrace of R, both in supporting the open-source R community, and connecting R with Microsoft platforms: http://blog.revolutionanalytics.com/2015/06/r-at-microsoft.html

Packages for analyzing the RStudio CRAN logs, used to calculate the top 100 R packages by downloads: http://blog.revolutionanalytics.com/2015/06/working-with-the-rstudio-cran-logs.html

Counting the number of packages on CRAN by platform: http://blog.revolutionanalytics.com/2015/06/how-many-packages-are-there-really-on-cran.html

Getting data into and out of R applications with DeployR: http://blog.revolutionanalytics.com/2015/06/deployr-data-io.html

A review of the various options for using R with Hadoop: http://blog.revolutionanalytics.com/2015/06/using-hadoop-with-r-it-depends.html

Using R to search for CRAN packages by topic area: http://blog.revolutionanalytics.com/2015/06/fishing-for-packages-in-cran.html

R code to draw the Archimedes Spiral: http://blog.revolutionanalytics.com/2015/06/inspired-by-mathematics-drawing-the-archimedes-spiral.html

A controversial caution about using only pairwise-complete observations when calculating correlation/covariance matrices in R: http://blog.revolutionanalytics.com/2015/06/pairwise-complete-correlation-considered-dangerous.html

You can use the RBlpapi package to access Bloomberg data with R: http://blog.revolutionanalytics.com/2015/06/connect-r-to-bloomberg.html

SparkR, a package to use the Spark distributed-computing framework from R, is now part of the Apache Spark project: http://blog.revolutionanalytics.com/2015/06/sparkr-announcement.html

An interactive map locates the 160+ R user groups around the world: http://blog.revolutionanalytics.com/2015/06/r-user-groups-are-everywhere.html

R has 64-bit objects, but there are constraints having only 32-bit integers: http://blog.revolutionanalytics.com/2015/06/r-in-a-64-bit-world.html

R is sometimes called a quirky language, but I argue that these design decisions have directly led to many innovations in statistical computing: http://blog.revolutionanalytics.com/2015/06/why-has-r-been-so-successful.html

R and BioConductor were featured in ?BUILD? (Microsoft?s developer conference in San Francisco), shown being called on-stage from a mobile app: http://blog.revolutionanalytics.com/2015/06/r-build-keynote.html

A review of some of the presentations at R/Finance 2015 in Chicago: http://blog.revolutionanalytics.com/2015/06/r-finance-2015.html

Using the rpud package to calculate distance matrices the GPU in R: http://blog.revolutionanalytics.com/2015/06/computing-with-gpus-in-r.html

A tutorial on using Azure as a data source for R: http://blog.revolutionanalytics.com/2015/06/using-azure-as-an-r-datasource-part-2-pulling-data-from-mysqlmariadb.html

A comparison of several high-performance computing approaches in R: http://blog.revolutionanalytics.com/2015/06/a-comparison-of-high-performance-computing-techniques-in-r.html

General interest stories (not related to R) in the past month included: planning A/B tests (http://blog.revolutionanalytics.com/2015/06/why-does-planning-something-as-simple-as-an-ab-test-always-end-up-feeling-so-complicated.html), a critique of US state flags (http://blog.revolutionanalytics.com/2015/06/because-its-friday-bad-flags.html), a new type of bearing (http://blog.revolutionanalytics.com/2015/06/because-its-friday-rethinking-bearings.html), a warning about drop bears (http://blog.revolutionanalytics.com/2015/06/because-its-friday-beware-the-drop-bears.html), and a visual comparison of the Game of Thrones books and TV series (http://blog.revolutionanalytics.com/2015/06/because-its-friday-the-adaptation-of-game-of-thrones.html). 

Meeting times for local R user groups (http://blog.revolutionanalytics.com/local-r-groups.html) can be found on the updated R Community Calendar at: http://blog.revolutionanalytics.com/calendar.html

If you're looking for more articles about R, you can find summaries from previous months at http://blog.revolutionanalytics.com/roundups/. You can receive daily blog posts via email using services like blogtrottr.com, or join the Revolution Analytics mailing list at http://revolutionanalytics.com/newsletter to be alerted to new articles on a monthly basis.

As always, thanks for the comments and please keep sending suggestions to me at davidsmi at microsoft.com or via Twitter (I'm @revodavid).

Cheers,
# David

--
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft
Twitter: @revodavid
Blog:  http://blog.revolutionanalytics.com
We?re hiring! http://azuremljobs.github.io/







From Phillip.Schermerhorn at austintexas.gov  Thu Jul  9 16:44:50 2015
From: Phillip.Schermerhorn at austintexas.gov (Schermerhorn, Phillip)
Date: Thu, 9 Jul 2015 14:44:50 +0000
Subject: [R] Compiling on AIX 6.1
Message-ID: <CY1PR09MB04251F526989DB579EEE756D8D900@CY1PR09MB0425.namprd09.prod.outlook.com>

I'm trying to compile R on AIX 6.1.

gcc -v
Using built-in specs.
Target: powerpc-ibm-aix6.1.0.0
Configured with: ../gcc-4.4.5/configure --with-as=/usr/bin/as --with-ld=/usr/bin/ld --enable-languages=c,c++,fortran --prefix=/opt/freeware --enable-threads --enable-version-specific-runtime-libs --disable-nls --enable-decimal-float=dpd --host=powerpc-ibm-aix6.1.0.0
Thread model: aix
gcc version 4.4.5 (GCC)

gfortran -v
Using built-in specs.
Target: powerpc-ibm-aix6.1.0.0
Configured with: ../gcc-4.4.5/configure --with-as=/usr/bin/as --with-ld=/usr/bin/ld --enable-languages=c,c++,fortran --prefix=/opt/freeware --enable-threads --enable-version-specific-runtime-libs --disable-nls --enable-decimal-float=dpd --host=powerpc-ibm-aix6.1.0.0
Thread model: aix
gcc version 4.4.5 (GCC)

No matter what I try, I keep getting the following errors.

ld: 0711-224 WARNING: Duplicate symbol: R_ReadItemDepth
ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more information.
ld: 0711-317 ERROR: Undefined symbol: .__nearbyintl128
ld: 0711-317 ERROR: Undefined symbol: .__log1pl128
collect2: ld returned 8 exit status
make: 1254-004 The error code from the last command is 1.

Any ideas on how to correct this?

Thanks
Phil

Phillip Schermerhorn
Programmer/Analyst
Communication Technology Management -- City of Austin
Office: (512) 974-1433

The latest survey shows that three out of four people make up 75% of the population.




	[[alternative HTML version deleted]]


From John.Szumiloski at bms.com  Thu Jul  9 19:36:55 2015
From: John.Szumiloski at bms.com (Szumiloski, John)
Date: Thu, 9 Jul 2015 17:36:55 +0000
Subject: [R] panel function revisited
Message-ID: <a1f652316ac641569359d84dfa67f5c0@CO2PR26MB0011.067d.mgd.msft.net>

Dear useRs,

I recently posted here (https://stat.ethz.ch/pipermail/r-help/2015-July/430223.html) a query regarding customizing panel functions for lattice::xyplot.  I received some helpful replies offline and while I learned some things, they did not get at my issue.  However there was a consensus that my original example was complex and hard to understand, so I want to revisit the issue with a very simple example.

Let's look at the teeth data in nlme::Orthodont.

    require(lattice)
    require(nlme)
    ?Orthodont

One way to display this data is to condition on the Sex of the Subject, and plot distance as function of age, grouped by Subject.  This plot distinguishes the Sex factor by making two panels, one for each Sex.

    xyplot(distance ~ age | Sex, data=Orthodont, groups=Subject, type='b')

The goal I have is to not use conditioning to differentiate data between the Sexes, but rather separate graphical parameters (I want to use other conditioning variables...).  The problem is normally graphical parameters are associated with the grouping variable, and I want to keep the grouping variable but use another variable (here Sex) to be distinguished graphically.  I can remove the conditioning as follows; the graphical parameters are mapped onto the grouping variable as expected:

    xyplot(distance ~ age, data=Orthodont, groups=Subject, type='b')

This is what I want, except I want only two colors, one for the Male Subjects, and one for the Female ones.  It seems the subscripts argument to panel functions is designed for this type of thing.  Based on the example from Figure 5.4 in Sarkar's lattice book, I thought the following would work:

    xyplot(distance ~ age, data=Orthodont, groups=Subject, type='b',
             # passing the vector of colors
           Cols = Orthodont[['Sex']] ,
           panel = function(x, y, ..., groups, subscripts, Cols){
                            panel.xyplot(x, y, ..., groups=groups, subscripts=subscripts,
                                         col.line=Cols[subscripts], col.symbol=Cols[subscripts]
                                        )
                           }
          )

But all the Subjects are plotted with the same color.

How can I arrange to have different graphical parameters for each level of Sex, without using Sex as either grouping or conditioning variable?

[I am using R version 3.2.1 (release version), revision 68531, and version 0.20-31 of lattice.]

Thanks,
John

John Szumiloski, Ph.D.
Principal Scientist, Statistician
Analytical and Bioanalytical Development
NBR105-1-1411

Bristol-Myers Squibb
P.O. Box 191
1 Squibb Drive
New Brunswick, NJ
08903-0191

(732) 227-7167



________________________________
 This message (including any attachments) may contain co...{{dropped:8}}


From dawn1313 at gmail.com  Thu Jul  9 19:12:30 2015
From: dawn1313 at gmail.com (Dawn)
Date: Thu, 9 Jul 2015 10:12:30 -0700
Subject: [R] sum some columns for each row
Message-ID: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>

Hi,

I have a big dataframe as follows

    109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC    25ABC    25XYZ
   30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC    36SUR
38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM    42SUR
46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC    66XYZ
67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ    76ABC
76XYZ    82ABC    85ABC    POV
Cluster_1                                                        17    1
3    10    14    5    2    2        1    1    1    2
                        2                            TT:61
Cluster_2                    1                                4    20
6    5    3    6    9    9    6        10        1    3    1
                            4                            TT:88
Cluster_3    3        3                            6        4        17
17    18    13    17    19    22    11    5    21    8    5    18    4
7                                        9                            TT:227
........

I want to get two columns, i.e,  one is to sum columns for all including
ABC for each row and the other is  to sum columns for all including XYZ for
each row.

Is there some help? Thank you!
Dawn

	[[alternative HTML version deleted]]


From annemariefischer86 at gmail.com  Thu Jul  9 20:32:50 2015
From: annemariefischer86 at gmail.com (annafischer86)
Date: Thu, 9 Jul 2015 11:32:50 -0700 (PDT)
Subject: [R] Maxent Jarfile
In-Reply-To: <A0AF1416-DE06-43CA-9AB8-C9BDBAD7C801@noaa.gov>
References: <DUB128-W680B85744A81777D161BFBEA910@phx.gbl>
	<7A0B317AB6E.0000034Fjrkrideau@inbox.com>
	<20150709052506.5849232.18416.844@gmail.com>
	<81B8F5B86EB.000009A4jrkrideau@inbox.com>
	<05B5BD69-4A29-4AE7-A9BA-992CA2D349F7@comcast.net>
	<20150709163801.5849232.67320.898@gmail.com>
	<5B341CE6-53B2-4535-ADEE-2DFC82173D6C@noaa.gov>
	<20150709171038.5849232.49711.902@gmail.com>
	<A0AF1416-DE06-43CA-9AB8-C9BDBAD7C801@noaa.gov>
Message-ID: <20150709185542.5849232.72229.915@gmail.com>

Hi Roy, I managed to make the code work, the issue was with underscores in the files. Thanks for your help.                                                                                                                                                                                                                                          From: Roy Mendelssohn - NOAA Federal [via R] Sent: Thursday 9 July 2015 20:36 To: annafischer86 Subject: Re: Maxent Jarfile 




--
View this message in context: http://r.789695.n4.nabble.com/Maxent-Jarfile-tp4709586p4709665.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From chichi.shu at hotmail.com  Thu Jul  9 22:04:12 2015
From: chichi.shu at hotmail.com (Chichi Shu)
Date: Thu, 9 Jul 2015 16:04:12 -0400
Subject: [R] ggmap warning
In-Reply-To: <CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>
References: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
	<CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>
Message-ID: <BLU179-DS144790805832072B29CBD78F900@phx.gbl>

Hi, 

I limit my data to just two fields. Longitude and latitude. I first used is.finite to check each of the long and lat values to make sure they are finite values.

Then when I use ggmap again, it still tells me that it removed the majority of the records bec of non-finite values.

Any other suggestions?

Thanks!

From: Thierry Onkelinx 
Sent: Wednesday, July 01, 2015 2:50 AM
To: Chichi Shu 
Cc: r-help at r-project.org 
Subject: Re: [R] ggmap warning

Your data contains 4945 rows with missing or infinite values. These cannot be handled by stat_density2d and are dropped for that reason.

Best regards,

Thierry


Op 1 jul. 2015 08:43 schreef "Chichi Shu" <chichi.shu at hotmail.com>:

  Dear Listers



  I?ve been using ggmap package to produce crime Heat map. But I?ve noticed the following warning message when executing my code:



  In loop_apply(n, do.ply) :

    Removed 4945 rows containing non-finite values (stat_density2d).



  I?ve googled this message but I couldn?t find any good answers.



  Is this related to ggmap package or one of its depending package?



  What does it mean?



  Thanks!

          [[alternative HTML version deleted]]


  ______________________________________________
  R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
  https://stat.ethz.ch/mailman/listinfo/r-help
  PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
  and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Thu Jul  9 22:46:15 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 09 Jul 2015 13:46:15 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
Message-ID: <33938245-5DD1-4983-BF4C-23794B50F7A5@dcn.davis.CA.us>

Please read the Posting Guide before posting again. Pay particular attention to the guidance to post using plain text. Also use the dput function to provide your sample data so we can more accurately start from where you are starting bad give more targeted answers.

You can use indexing to select the subset of columns to work with, and the rowSums function to do the calculations. Something like

dta$abc <- rowSums( dta[ , grep( "abc", names( dta ) ) ] )

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 9, 2015 10:12:30 AM PDT, Dawn <dawn1313 at gmail.com> wrote:
>Hi,
>
>I have a big dataframe as follows
>
> 109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC    25ABC    25XYZ
>   30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC    36SUR
>38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM    42SUR
>46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC    66XYZ
>67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ    76ABC
>76XYZ    82ABC    85ABC    POV
>Cluster_1                                                        17   
>1
>3    10    14    5    2    2        1    1    1    2
>                        2                            TT:61
>Cluster_2                    1                                4    20
>6    5    3    6    9    9    6        10        1    3    1
>                            4                            TT:88
>Cluster_3    3        3                            6        4        17
>17    18    13    17    19    22    11    5    21    8    5    18    4
>7                                        9                           
>TT:227
>........
>
>I want to get two columns, i.e,  one is to sum columns for all
>including
>ABC for each row and the other is  to sum columns for all including XYZ
>for
>each row.
>
>Is there some help? Thank you!
>Dawn
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Thu Jul  9 22:52:59 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Thu, 09 Jul 2015 21:52:59 +0100
Subject: [R] sum some columns for each row
In-Reply-To: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
Message-ID: <559EDF2B.6030705@sapo.pt>

Hello,

Please use ?dput to give a data example, like this it's completely 
unreadable. If your data.frame is named 'dat' use

dput(head(dat, 30))  # paste the outut of this in your mail


And don't post in html, use plain text only, like the posting guide says.

Rui Barradas

Em 09-07-2015 18:12, Dawn escreveu:
> Hi,
>
> I have a big dataframe as follows
>
>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC    25ABC    25XYZ
>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC    36SUR
> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM    42SUR
> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC    66XYZ
> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ    76ABC
> 76XYZ    82ABC    85ABC    POV
> Cluster_1                                                        17    1
> 3    10    14    5    2    2        1    1    1    2
>                          2                            TT:61
> Cluster_2                    1                                4    20
> 6    5    3    6    9    9    6        10        1    3    1
>                              4                            TT:88
> Cluster_3    3        3                            6        4        17
> 17    18    13    17    19    22    11    5    21    8    5    18    4
> 7                                        9                            TT:227
> ........
>
> I want to get two columns, i.e,  one is to sum columns for all including
> ABC for each row and the other is  to sum columns for all including XYZ for
> each row.
>
> Is there some help? Thank you!
> Dawn
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From thierry.onkelinx at inbo.be  Thu Jul  9 22:57:36 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Thu, 9 Jul 2015 22:57:36 +0200
Subject: [R] ggmap warning
In-Reply-To: <BLU179-DS144790805832072B29CBD78F900@phx.gbl>
References: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
	<CAJuCY5wm5ixM7JqgnqE9W0boT+KE-wMx==j4Jey0ZYb-Xs7VnQ@mail.gmail.com>
	<BLU179-DS144790805832072B29CBD78F900@phx.gbl>
Message-ID: <CAJuCY5zYUEFi-kDLdD1zMLuZ9ZOKYrWZPNqf=+j-GJvV5Zq1MQ@mail.gmail.com>

Yes. Provide a reproducible example of your problem: the code and the data.
Op 9-jul.-2015 22:06 schreef "Chichi Shu" <chichi.shu at hotmail.com>:

> Hi,
>
> I limit my data to just two fields. Longitude and latitude. I first used
> is.finite to check each of the long and lat values to make sure they are
> finite values.
>
> Then when I use ggmap again, it still tells me that it removed the
> majority of the records bec of non-finite values.
>
> Any other suggestions?
>
> Thanks!
>
> From: Thierry Onkelinx
> Sent: Wednesday, July 01, 2015 2:50 AM
> To: Chichi Shu
> Cc: r-help at r-project.org
> Subject: Re: [R] ggmap warning
>
> Your data contains 4945 rows with missing or infinite values. These cannot
> be handled by stat_density2d and are dropped for that reason.
>
> Best regards,
>
> Thierry
>
>
> Op 1 jul. 2015 08:43 schreef "Chichi Shu" <chichi.shu at hotmail.com>:
>
>   Dear Listers
>
>
>
>   I?ve been using ggmap package to produce crime Heat map. But I?ve
> noticed the following warning message when executing my code:
>
>
>
>   In loop_apply(n, do.ply) :
>
>     Removed 4945 rows containing non-finite values (stat_density2d).
>
>
>
>   I?ve googled this message but I couldn?t find any good answers.
>
>
>
>   Is this related to ggmap package or one of its depending package?
>
>
>
>   What does it mean?
>
>
>
>   Thanks!
>
>           [[alternative HTML version deleted]]
>
>
>   ______________________________________________
>   R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>   https://stat.ethz.ch/mailman/listinfo/r-help
>   PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>   and provide commented, minimal, self-contained, reproducible code.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Thu Jul  9 23:03:29 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 09 Jul 2015 14:03:29 -0700
Subject: [R] Compiling on AIX 6.1
In-Reply-To: <CY1PR09MB04251F526989DB579EEE756D8D900@CY1PR09MB0425.namprd09.prod.outlook.com>
References: <CY1PR09MB04251F526989DB579EEE756D8D900@CY1PR09MB0425.namprd09.prod.outlook.com>
Message-ID: <491EF222-44E2-4EB1-8A8A-E1F3238AF346@dcn.davis.CA.us>

Please read and follow the Posting Guide, which indicates that this type of question is off-topic on R-help and should be asked on R-devel. It also warns you that cross-posting is not polite, so this reply is only being sent to R-help.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 9, 2015 7:44:50 AM PDT, "Schermerhorn, Phillip" <Phillip.Schermerhorn at austintexas.gov> wrote:
>I'm trying to compile R on AIX 6.1.
>
>gcc -v
>Using built-in specs.
>Target: powerpc-ibm-aix6.1.0.0
>Configured with: ../gcc-4.4.5/configure --with-as=/usr/bin/as
>--with-ld=/usr/bin/ld --enable-languages=c,c++,fortran
>--prefix=/opt/freeware --enable-threads
>--enable-version-specific-runtime-libs --disable-nls
>--enable-decimal-float=dpd --host=powerpc-ibm-aix6.1.0.0
>Thread model: aix
>gcc version 4.4.5 (GCC)
>
>gfortran -v
>Using built-in specs.
>Target: powerpc-ibm-aix6.1.0.0
>Configured with: ../gcc-4.4.5/configure --with-as=/usr/bin/as
>--with-ld=/usr/bin/ld --enable-languages=c,c++,fortran
>--prefix=/opt/freeware --enable-threads
>--enable-version-specific-runtime-libs --disable-nls
>--enable-decimal-float=dpd --host=powerpc-ibm-aix6.1.0.0
>Thread model: aix
>gcc version 4.4.5 (GCC)
>
>No matter what I try, I keep getting the following errors.
>
>ld: 0711-224 WARNING: Duplicate symbol: R_ReadItemDepth
>ld: 0711-345 Use the -bloadmap or -bnoquiet option to obtain more
>information.
>ld: 0711-317 ERROR: Undefined symbol: .__nearbyintl128
>ld: 0711-317 ERROR: Undefined symbol: .__log1pl128
>collect2: ld returned 8 exit status
>make: 1254-004 The error code from the last command is 1.
>
>Any ideas on how to correct this?
>
>Thanks
>Phil
>
>Phillip Schermerhorn
>Programmer/Analyst
>Communication Technology Management -- City of Austin
>Office: (512) 974-1433
>
>The latest survey shows that three out of four people make up 75% of
>the population.
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Thu Jul  9 23:12:57 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 9 Jul 2015 14:12:57 -0700
Subject: [R] ggmap warning
In-Reply-To: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
References: <BLU179-DS105EEEB47BA347236F93478FA90@phx.gbl>
Message-ID: <CAF8bMcZye_OnPp_Ox=-DWqD9k9mrtKBY-+fRj+MfZ3U-GsfMFw@mail.gmail.com>

Look at your data.frame with str() and see if the variables you think
are numeric are actually factors.  is.finite(factor()) reports TRUE but
lots of functions expecting numeric data will abort when given factor
data.

E.g.
library(ggplot2)
> d <- data.frame(X=factor(round(sin(1:1000),1)),
Y=factor(round(sin((1:1000)*4),1)))
> head(d)
     X    Y
1  0.8 -0.8
2  0.9    1
3  0.1 -0.5
4 -0.8 -0.3
5   -1  0.9
6 -0.3 -0.9
>  ggplot(d, aes(x=X, y=Y)) + geom_point() + geom_density2d()
Error in if (any(h <= 0)) stop("bandwidths must be strictly positive") :
  missing value where TRUE/FALSE needed
> # no contours displayed
>  ggplot(d, aes(x=as.numeric(as.character(X)),
y=as.numeric(as.character(Y)))) + geom_point() + geom_density2d()
> # see contours of 2d density

If you have factors where you expect numbers then try rereading the file
using
read.table(colClasses=c("numeric","numeric",...), ...) and look at the
missing
values to see why they appeared that way.  (E.g., does the text file use
commas
for the decimal points or thousands separators?)


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Jun 30, 2015 at 12:43 PM, Chichi Shu <chichi.shu at hotmail.com> wrote:

> Dear Listers
>
>
>
> I?ve been using ggmap package to produce crime Heat map. But I?ve noticed
> the following warning message when executing my code:
>
>
>
> In loop_apply(n, do.ply) :
>
>   Removed 4945 rows containing non-finite values (stat_density2d).
>
>
>
> I?ve googled this message but I couldn?t find any good answers.
>
>
>
> Is this related to ggmap package or one of its depending package?
>
>
>
> What does it mean?
>
>
>
> Thanks!
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Thu Jul  9 23:31:51 2015
From: 538280 at gmail.com (Greg Snow)
Date: Thu, 9 Jul 2015 15:31:51 -0600
Subject: [R] tcltk2 entry box
In-Reply-To: <559DD555.2020506@molbio.mgh.harvard.edu>
References: <559DB9A7.40202@molbio.mgh.harvard.edu>
	<004f01d0b9df$751d89a0$5f589ce0$@mcmaster.ca>
	<559DD555.2020506@molbio.mgh.harvard.edu>
Message-ID: <CAFEqCdwOnK3gD5WjJRCA6UN3TyiKOWfyiyM3rmWjQUOZ3jM+hw@mail.gmail.com>

If you want you script to wait until you have a value entered then you
can use the tkwait.variable or tkwait.window commands to make the
script wait before continuing (or you can bind the code to a button so
that you enter the value, then click on the button to run the code).

On Wed, Jul 8, 2015 at 7:58 PM, Matthew McCormack
<mccormack at molbio.mgh.harvard.edu> wrote:
> Wow !  Very nice.  Thank you very much, John.  This is very helpful and just
> what I need.
> Yes, I can see that I should have paid attention to tcltk before going to
> tcltk2.
>
> Matthew
>
>
> On 7/8/2015 8:37 PM, John Fox wrote:
>>
>> Dear Matthew,
>>
>> For file selection, see ?tcltk::tk_choose.files or ?tcltk::tkgetOpenFile .
>>
>> You could enter a number in a tk entry widget, but, depending upon the
>> nature of the number, a slider or other widget might be a better choice.
>>
>> For a variety of helpful tcltk examples see
>> <http://www.sciviews.org/_rgui/tcltk/>, originally by James Wettenhall but
>> now maintained by Philippe Grosjean (the author of the tcltk2 package).
>> (You
>> probably don't need tcltk2 for the simple operations that you mention, but
>> see ?tk2spinbox for an alternative to a slider.)
>>
>> Best,
>>   John
>>
>> -----------------------------------------------
>> John Fox, Professor
>> McMaster University
>> Hamilton, Ontario, Canada
>> http://socserv.socsci.mcmaster.ca/jfox/
>>
>>
>>
>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matthew
>>> Sent: July-08-15 8:01 PM
>>> To: r-help
>>> Subject: [R] tcltk2 entry box
>>>
>>> Is anyone familiar enough with the tcltk2 package to know if it is
>>> possible to have an entry box where a user can enter information (such
>>> as a path to a file or a number) and then be able to use the entered
>>> information downstream in a R script ?
>>>
>>> The idea is for someone unfamiliar with R to just start an R script that
>>> would take care of all the commands for them so all they have to do is
>>> get the script started. However, there is always a couple of pieces of
>>> information that will change each time the script is used (for example,
>>> a different file will be processed by the script). So, I would like a
>>> way for the user to input that information as the script ran.
>>>
>>> Matthew McCormack
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ---
>> This email has been checked for viruses by Avast antivirus software.
>> https://www.avast.com/antivirus
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From angel_nauti at yahoo.com  Fri Jul 10 03:08:43 2015
From: angel_nauti at yahoo.com (Angel Marley)
Date: Fri, 10 Jul 2015 01:08:43 +0000 (UTC)
Subject: [R] Quantile regression interval prediction
Message-ID: <1831042018.2311888.1436490523619.JavaMail.yahoo@mail.yahoo.com>

Hi all,I would like to know how to predict a new y value and its confidence interval for the prediction given a new observation x when using a linear(or non-linear) quantile regression model.How it is possible to transform the confidence prediction in to an interval prediction?
Is it correct to assume the same as in a lineal model prediction to transform the confidence prediction into an interval prediction?Is there any code or package to do this?

Here is a simple example.
require(quantreg)
x<-runif(10,0,5)
y<-0.3+ x*05 + rnorm(10, 0,3)

mod1<-lm(y~x)

# Values to get predictions
newval<-data.frame(x=c(3,7))
pred1<-predict(mod1, newval, interval = "prediction")# in lm is ok

mod2<- rq(y~x, tau=0.95)
pred2<-predict(mod2, newval, interval = "confidence")

Many Thanks in advanceAngel



 


	[[alternative HTML version deleted]]


From xphongvn at gmail.com  Fri Jul 10 04:22:20 2015
From: xphongvn at gmail.com (Phong Nguyen)
Date: Fri, 10 Jul 2015 11:22:20 +0900
Subject: [R]  color2D.matplot with anchored color scale
Message-ID: <CABBSeunM4u+GHxZAvZ53VJAoLYPp0h10HJL=S-_pj4dDTrsXzw@mail.gmail.com>

Hello Jim,

I have a several matrices stored in allGrid <- list()

I am using:

      color2D.matplot(grid, extremes = c("green", "red"), vcol = "black",
nslices = 60, show.legend=TRUE,axes=FALSE,show.values=TRUE,
xlab="Columns",ylab="Rows")

for each of "grid" in "allGrid". Each "grid" is a matrix with element range
from 0 to max number of allGrid, say 40.

I want to plot many "grid" with color represents the number (density). But
I cannot fix the color scale and anchor the maximum value to a fixed
number.

I have tried many things but nothing works.

Just the extra is: I cannot make the color from green (small number) ->
yellow (middle number) -> red (big number). Sometimes, the plot turns black
and white.

Please help me.

Phong

	[[alternative HTML version deleted]]


From cryan at binghamton.edu  Thu Jul  9 20:48:46 2015
From: cryan at binghamton.edu (Christopher W Ryan)
Date: Thu, 9 Jul 2015 14:48:46 -0400
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
	<7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>
	<CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>
Message-ID: <CAM+rpY=ucmtVQREyhcgoXzc7v5X+kWaej4DiOTbeZNajnB-CSg@mail.gmail.com>

Thanks everyone.  John's original solution worked great.  And with
27,000 records, 65 alarm.words, and 6 columns to search, it takes only
about 15 seconds.  That is certainly adequate for my needs.  But I
will try out the other strategies too.

And thanks also for lot's of new R things to learn--grep, grepl,
do.call . . . that's always a bonus!

--Chris Ryan

On Thu, Jul 9, 2015 at 1:52 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Yup, that does it. Let grep figure out what's a word rather than doing
> it manually. Forgot about "\b"
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Jul 9, 2015 at 10:30 AM, Jeff Newmiller
> <jdnewmil at dcn.davis.ca.us> wrote:
>> Just add a word break marker before and after:
>>
>> zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ), ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>>                                       Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>Jeff:
>>>
>>>Well, it would be much better (no loops!) except, I think, for one
>>>issue: "red" would match "barred" and I don't think that this is what
>>>is wanted: the matches should be on whole "words" not just string
>>>patterns.
>>>
>>>So you would need to fix up the matching pattern to make this work,
>>>but it may be a little tricky, as arbitrary whitespace characters,
>>>e.g. " " or "\n" etc. could be in the strings to be matched separating
>>>the words or ending the "sentence."  I'm sure it can be done, but I'll
>>>leave it to you or others to figure it out.
>>>
>>>Of course, if my diagnosis is wrong or silly, please point this out.
>>>
>>>Cheers,
>>>Bert
>>>
>>>
>>>Bert Gunter
>>>
>>>"Data is not information. Information is not knowledge. And knowledge
>>>is certainly not wisdom."
>>>   -- Clifford Stoll
>>>
>>>
>>>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
>>><jdnewmil at dcn.davis.ca.us> wrote:
>>>> I think grep is better suited to this:
>>>>
>>>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call( paste,
>>>zz[ , 2:3 ] ) ) )
>>>>
>>>---------------------------------------------------------------------------
>>>> Jeff Newmiller                        The     .....       .....  Go
>>>Live...
>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>Go...
>>>>                                       Live:   OO#.. Dead: OO#..
>>>Playing
>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>rocks...1k
>>>>
>>>---------------------------------------------------------------------------
>>>> Sent from my phone. Please excuse my brevity.
>>>>
>>>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>>>wrote:
>>>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only a
>>>>>single, not a double, loop. It should be more efficient.
>>>>>
>>>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>>>>>+       function(x)any(x %in% alarm.words))
>>>>>
>>>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>>>>>
>>>>>The idea is to paste the strings in each row (do.call allows an
>>>>>arbitrary number of columns) into a single string and then use
>>>>>strsplit to break the string into individual "words" on whitespace.
>>>>>Then the matching is vectorized with the any( %in% ... ) call.
>>>>>
>>>>>Cheers,
>>>>>Bert
>>>>>Bert Gunter
>>>>>
>>>>>"Data is not information. Information is not knowledge. And knowledge
>>>>>is certainly not wisdom."
>>>>>   -- Clifford Stoll
>>>>>
>>>>>
>>>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>>>>>> Dear Chris,
>>>>>>
>>>>>> If I understand correctly what you want, how about the following?
>>>>>>
>>>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>>>>>grepl, x=x)))
>>>>>>> zz[rows, ]
>>>>>>
>>>>>>           v1                              v2                v3 v4
>>>>>> 3  -1.022329                    green turtle    ronald weasley  2
>>>>>> 6   0.336599              waffle the hamster        red sparks  1
>>>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>>>>>> 10  1.130622                      black bear  gandalf the grey  2
>>>>>>
>>>>>> I hope this helps,
>>>>>>  John
>>>>>>
>>>>>> ------------------------------------------------
>>>>>> John Fox, Professor
>>>>>> McMaster University
>>>>>> Hamilton, Ontario, Canada
>>>>>> http://socserv.mcmaster.ca/jfox/
>>>>>>
>>>>>>
>>>>>> On Wed, 08 Jul 2015 22:23:37 -0400
>>>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>>>>>> Running R 3.1.1 on windows 7
>>>>>>>
>>>>>>> I want to identify as a case any record in a dataframe that
>>>contains
>>>>>any
>>>>>>> of several keywords in any of several variables.
>>>>>>>
>>>>>>> Example:
>>>>>>>
>>>>>>> # create a dataframe with 4 variables and 10 records
>>>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>>>>>fox",
>>>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>>>>>"hello
>>>>>>> world", "yellow giraffe with a long neck", "black bear")
>>>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>>>>>"ginny
>>>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>>>>>dress
>>>>>>> robes", "gandalf the white", "gandalf the grey")
>>>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
>>>lambda=2),
>>>>>>> stringsAsFactors=FALSE)
>>>>>>> str(zz)
>>>>>>> zz
>>>>>>>
>>>>>>> # here are the keywords
>>>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>>>>>>
>>>>>>> # For each row/record, I want to test whether the string in v2 or
>>>>>the
>>>>>>> string in v3 contains any of the strings in alarm.words. And then
>>>if
>>>>>so,
>>>>>>> set zz$v5=TRUE for that record.
>>>>>>>
>>>>>>> # I'm thinking the str_detect function in the stringr package
>>>ought
>>>>>to
>>>>>>> be able to help, perhaps with some use of apply over the rows, but
>>>I
>>>>>>> obviously misunderstand something about how str_detect works
>>>>>>>
>>>>>>> library(stringr)
>>>>>>>
>>>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>>>>>search
>>>>>>>                                      # must be a vector, not
>>>>>multiple
>>>>>>>                                      # columns
>>>>>>>
>>>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>>>>>>
>>>>>>> str_detect(zz[,2], alarm.words)      # error, length of
>>>alarm.words
>>>>>>>                                      # is less than the number of
>>>>>>>                                      # rows I am using for the
>>>>>>>                                      # comparison
>>>>>>>
>>>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>>>>>> length(alarm.words)                  # confining nrows
>>>>>>>                                      # to the length of
>>>alarm.words
>>>>>>>
>>>>>>> str_detect(zz, alarm.words)          # obviously not right
>>>>>>>
>>>>>>> # maybe I need apply() ?
>>>>>>> my.f <- function(x){str_detect(x, alarm.words)}
>>>>>>>
>>>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>>>>>>                            # between alarm.words and that
>>>>>>>                            # in which I am searching for
>>>>>>>                            # matching strings
>>>>>>>
>>>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>>>>>>                            # rows of the dataframe
>>>>>>>
>>>>>>>
>>>>>>> # perhaps %in% could do the job?
>>>>>>>
>>>>>>> Appreciate any advice.
>>>>>>>
>>>>>>> --Chris Ryan
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>>______________________________________________
>>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>PLEASE do read the posting guide
>>>>>http://www.R-project.org/posting-guide.html
>>>>>and provide commented, minimal, self-contained, reproducible code.
>>>>
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jul 10 12:53:53 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 10 Jul 2015 20:53:53 +1000
Subject: [R] color2D.matplot with anchored color scale
In-Reply-To: <CABBSeunM4u+GHxZAvZ53VJAoLYPp0h10HJL=S-_pj4dDTrsXzw@mail.gmail.com>
References: <CABBSeunM4u+GHxZAvZ53VJAoLYPp0h10HJL=S-_pj4dDTrsXzw@mail.gmail.com>
Message-ID: <CA+8X3fWLkHY5C0-d2sJnwFTvHnEYKpEY6OhT8jp7S3L=Un2UZQ@mail.gmail.com>

Hi Phong,
This is a common problem with using color to represent numeric values.
What you want is a reference scale. Whenever you call color2D.matplot,
use the "cellcolors" argument to specify the colors of each cell. You
can directly call "color.scale" in this argument, using the "xrange"
argument of that function to ensure that the colors returned are
calculated for the full range:

color.scale(...,xrange=c(0,40),...)

Look at the second example in the help page for "barp" to see how this
is done. Notice that it doesn't matter that the actual data are in
proportions (0 to 1) and the legend is labelled in percentages (0 to
100). The data values are correctly colored for the entire scale,
although none of these values are at the endpoints.

Jim


On Fri, Jul 10, 2015 at 12:22 PM, Phong Nguyen <xphongvn at gmail.com> wrote:
> Hello Jim,
>
> I have a several matrices stored in allGrid <- list()
>
> I am using:
>
>       color2D.matplot(grid, extremes = c("green", "red"), vcol = "black",
> nslices = 60, show.legend=TRUE,axes=FALSE,show.values=TRUE,
> xlab="Columns",ylab="Rows")
>
> for each of "grid" in "allGrid". Each "grid" is a matrix with element range
> from 0 to max number of allGrid, say 40.
>
> I want to plot many "grid" with color represents the number (density). But I
> cannot fix the color scale and anchor the maximum value to a fixed number.
>
> I have tried many things but nothing works.
>
> Just the extra is: I cannot make the color from green (small number) ->
> yellow (middle number) -> red (big number). Sometimes, the plot turns black
> and white.
>
> Please help me.
>
> Phong
>
>


From drjimlemon at gmail.com  Fri Jul 10 13:15:01 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 10 Jul 2015 21:15:01 +1000
Subject: [R] sum some columns for each row
In-Reply-To: <559EDF2B.6030705@sapo.pt>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
Message-ID: <CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>

Hi Dawn,
Your data are a bit messed up, but try the following:

colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)

I'm assuming that you want to discard the NA values.

Jim

On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Please use ?dput to give a data example, like this it's completely
> unreadable. If your data.frame is named 'dat' use
>
> dput(head(dat, 30))  # paste the outut of this in your mail
>
>
> And don't post in html, use plain text only, like the posting guide says.
>
> Rui Barradas
>
>
> Em 09-07-2015 18:12, Dawn escreveu:
>>
>> Hi,
>>
>> I have a big dataframe as follows
>>
>>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC    25ABC
>> 25XYZ
>>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC    36SUR
>> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM    42SUR
>> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC    66XYZ
>> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ    76ABC
>> 76XYZ    82ABC    85ABC    POV
>> Cluster_1                                                        17    1
>> 3    10    14    5    2    2        1    1    1    2
>>                          2                            TT:61
>> Cluster_2                    1                                4    20
>> 6    5    3    6    9    9    6        10        1    3    1
>>                              4                            TT:88
>> Cluster_3    3        3                            6        4        17
>> 17    18    13    17    19    22    11    5    21    8    5    18    4
>> 7                                        9
>> TT:227
>> ........
>>
>> I want to get two columns, i.e,  one is to sum columns for all including
>> ABC for each row and the other is  to sum columns for all including XYZ
>> for
>> each row.
>>
>> Is there some help? Thank you!
>> Dawn
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maitra.mbox.ignored at inbox.com  Fri Jul 10 15:23:49 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Fri, 10 Jul 2015 08:23:49 -0500
Subject: [R] approaches tpmatrix multiplication of each layer of 3-d array
 with different matrix
Message-ID: <20150710082349.54719564cd8cc2a6db42060d@inbox.com>

Dear friends,

I have two 3-d arrays of appropriate dimensions. I want to postmultiply the 2-D matrix layers of the first with the 2-D matrix layers of the second. Can I do this easily in R avoiding loops?

As an example:

--- begin R code ---

AA <- array(1:60, dim = c(5500,44,33))
BB <- array(1:60, dim = c(44,44,33))


arraymatprod <- function(A, B) {
    EE <- array(dim=dim(A));
    for (i in 1:3) EE[,,i] <- A[,,i] %*% B[,,i]
    EE
}

system.time(arraymatprod(AA, BB))

--- end R code ---

So, is there a way to do this without the loop?

Of course, I could abind the arrays and then use apply with an appropriate function which would be as follows:

--- begin R code ---

arraymatrixproduct <- function(A, B) {
	require(abind)
	dA <- dim(A)[1]
	AB <- abind(A, B, along = 1)
	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ] %*% mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
}

system.time(arraymatrixproduct(AA, BB))

--- end R code ---

However, this turns out to be slower -- perhaps because of the use of abind and filling the array inside the function.

I just wanted suggestions to get this operation done more efficiently.

Many thanks and best wishes,
Ranjan

____________________________________________________________
Publish your photos in seconds for FREE
TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4


From maitra.mbox.ignored at inbox.com  Fri Jul 10 15:44:19 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Fri, 10 Jul 2015 08:44:19 -0500
Subject: [R] Corrected: approaches tpmatrix multiplication of each layer of
 3-d array with different matrix
In-Reply-To: <20150710082349.54719564cd8cc2a6db42060d@inbox.com>
References: <20150710082349.54719564cd8cc2a6db42060d@inbox.com>
Message-ID: <20150710084419.dc76c2fdbf40f951458e53ba@inbox.com>

Hi,

Sorry to post again, but there is a careless error in my first R code snippet:

--- begin R code ---
 
AA <- array(1:60, dim = c(5500,44,33))
BB <- array(1:60, dim = c(44,44,33))
 
 
arraymatprod <- function(A, B) {
    EE <- array(dim=dim(A));
    for (i in 1:dim(A)[3]) EE[,,i] <- A[,,i] %*% B[,,i]
    EE
}

system.time(arraymatprod(AA, BB))

--- end R code ---

The second snippet is correct:

--- begin R code ---

arraymatrixproduct <- function(A, B) {
	require(abind)
	dA <- dim(A)[1]
	AB <- abind(A, B, along = 1)
	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ] %*% mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
}

system.time(arraymatrixproduct(AA, BB))

--- end R code ---

However, the second is almost twice as long as the first snippet.

Many thanks,
Ranjan


On Fri, 10 Jul 2015 08:23:49 -0500 Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:

> Dear friends,
> 
> I have two 3-d arrays of appropriate dimensions. I want to postmultiply the 2-D matrix layers of the first with the 2-D matrix layers of the second. Can I do this easily in R avoiding loops?
> 
> As an example:
> 
> --- begin R code ---
> 
> AA <- array(1:60, dim = c(5500,44,33))
> BB <- array(1:60, dim = c(44,44,33))
> 
> 
> arraymatprod <- function(A, B) {
>     EE <- array(dim=dim(A));
>     for (i in 1:3) EE[,,i] <- A[,,i] %*% B[,,i]
>     EE
> }
> 
> system.time(arraymatprod(AA, BB))
> 
> --- end R code ---
> 
> So, is there a way to do this without the loop?
> 
> Of course, I could abind the arrays and then use apply with an appropriate function which would be as follows:
> 
> --- begin R code ---
> 
> arraymatrixproduct <- function(A, B) {
> 	require(abind)
> 	dA <- dim(A)[1]
> 	AB <- abind(A, B, along = 1)
> 	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ] %*% mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
> }
> 
> system.time(arraymatrixproduct(AA, BB))
> 
> --- end R code ---
> 
> However, this turns out to be slower -- perhaps because of the use of abind and filling the array inside the function.
> 
> I just wanted suggestions to get this operation done more efficiently.
> 
> Many thanks and best wishes,
> Ranjan
> 
> ____________________________________________________________
> Publish your photos in seconds for FREE
> TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From jdnewmil at dcn.davis.CA.us  Fri Jul 10 16:07:15 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 10 Jul 2015 07:07:15 -0700
Subject: [R] Corrected: approaches tpmatrix multiplication of each layer
	of 3-d array with different matrix
In-Reply-To: <20150710084419.dc76c2fdbf40f951458e53ba@inbox.com>
References: <20150710082349.54719564cd8cc2a6db42060d@inbox.com>
	<20150710084419.dc76c2fdbf40f951458e53ba@inbox.com>
Message-ID: <40750368-0D34-44CC-89E5-AEF66EDBA294@dcn.davis.CA.us>

Strictly speaking, the answer is yes because you can unroll the loop, but that probably is not what you really want or need to do.

Your example seems about right, but it is not clear how you plan to make 44 conform with 5500.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 10, 2015 6:44:19 AM PDT, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
>Hi,
>
>Sorry to post again, but there is a careless error in my first R code
>snippet:
>
>--- begin R code ---
> 
>AA <- array(1:60, dim = c(5500,44,33))
>BB <- array(1:60, dim = c(44,44,33))
> 
> 
>arraymatprod <- function(A, B) {
>    EE <- array(dim=dim(A));
>    for (i in 1:dim(A)[3]) EE[,,i] <- A[,,i] %*% B[,,i]
>    EE
>}
>
>system.time(arraymatprod(AA, BB))
>
>--- end R code ---
>
>The second snippet is correct:
>
>--- begin R code ---
>
>arraymatrixproduct <- function(A, B) {
>	require(abind)
>	dA <- dim(A)[1]
>	AB <- abind(A, B, along = 1)
>	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ] %*%
>mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
>}
>
>system.time(arraymatrixproduct(AA, BB))
>
>--- end R code ---
>
>However, the second is almost twice as long as the first snippet.
>
>Many thanks,
>Ranjan
>
>
>On Fri, 10 Jul 2015 08:23:49 -0500 Ranjan Maitra
><maitra.mbox.ignored at inbox.com> wrote:
>
>> Dear friends,
>> 
>> I have two 3-d arrays of appropriate dimensions. I want to
>postmultiply the 2-D matrix layers of the first with the 2-D matrix
>layers of the second. Can I do this easily in R avoiding loops?
>> 
>> As an example:
>> 
>> --- begin R code ---
>> 
>> AA <- array(1:60, dim = c(5500,44,33))
>> BB <- array(1:60, dim = c(44,44,33))
>> 
>> 
>> arraymatprod <- function(A, B) {
>>     EE <- array(dim=dim(A));
>>     for (i in 1:3) EE[,,i] <- A[,,i] %*% B[,,i]
>>     EE
>> }
>> 
>> system.time(arraymatprod(AA, BB))
>> 
>> --- end R code ---
>> 
>> So, is there a way to do this without the loop?
>> 
>> Of course, I could abind the arrays and then use apply with an
>appropriate function which would be as follows:
>> 
>> --- begin R code ---
>> 
>> arraymatrixproduct <- function(A, B) {
>> 	require(abind)
>> 	dA <- dim(A)[1]
>> 	AB <- abind(A, B, along = 1)
>> 	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ]
>%*% mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
>> }
>> 
>> system.time(arraymatrixproduct(AA, BB))
>> 
>> --- end R code ---
>> 
>> However, this turns out to be slower -- perhaps because of the use of
>abind and filling the array inside the function.
>> 
>> I just wanted suggestions to get this operation done more
>efficiently.
>> 
>> Many thanks and best wishes,
>> Ranjan
>> 
>> ____________________________________________________________
>> Publish your photos in seconds for FREE
>> TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From csun at cfr.msstate.edu  Fri Jul 10 16:40:59 2015
From: csun at cfr.msstate.edu (Edwin Sun)
Date: Fri, 10 Jul 2015 07:40:59 -0700 (PDT)
Subject: [R] Embed fonts in an R graph
Message-ID: <1436539259216-4709707.post@n4.nabble.com>

Hello all,

I cannot embed a common font type into an R graph. I did it successfully in
December 2014 with the previous R version. However, with R 3.2.1 in July
2015, the following sample codes do not work anymore.

pdf(file = "c:/testA.pdf", family = "serif")
plot(x = 1:10, y = rnorm(10))
dev.off()
embedFonts(file = "c:/testA.pdf", outfile = "c:/testB.pdf")

As a result, both testA.pdf and testB.pdf cannot embed the fonts into the
graph. Specifically, Adobe Acrobt reveals that "Times-Roman" is substituted
by "TimesNewRomanPSMT", and "ZapfDingbats" is substituted by "AdobePiStd". 

Any help is greatly appreciated.

Edwin

<http://r.789695.n4.nabble.com/file/n4709707/Capture.png> 




--
View this message in context: http://r.789695.n4.nabble.com/Embed-fonts-in-an-R-graph-tp4709707.html
Sent from the R help mailing list archive at Nabble.com.


From maitra.mbox.ignored at inbox.com  Fri Jul 10 17:37:37 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Fri, 10 Jul 2015 10:37:37 -0500
Subject: [R] Corrected: approaches to matrix multiplication of each
 layer of 3-d array with different matrix
In-Reply-To: <40750368-0D34-44CC-89E5-AEF66EDBA294@dcn.davis.CA.us>
References: <20150710082349.54719564cd8cc2a6db42060d@inbox.com>
	<20150710084419.dc76c2fdbf40f951458e53ba@inbox.com>
	<40750368-0D34-44CC-89E5-AEF66EDBA294@dcn.davis.CA.us>
Message-ID: <20150710103737.af188bd53acca2a093a1b98b@inbox.com>

What does it mean to unroll the loop?

Thanks!

Ranjan


On Fri, 10 Jul 2015 07:07:15 -0700 Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:

> Strictly speaking, the answer is yes because you can unroll the loop, but that probably is not what you really want or need to do.
> 
> Your example seems about right, but it is not clear how you plan to make 44 conform with 5500.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
> 
> On July 10, 2015 6:44:19 AM PDT, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> >Hi,
> >
> >Sorry to post again, but there is a careless error in my first R code
> >snippet:
> >
> >--- begin R code ---
> > 
> >AA <- array(1:60, dim = c(5500,44,33))
> >BB <- array(1:60, dim = c(44,44,33))
> > 
> > 
> >arraymatprod <- function(A, B) {
> >    EE <- array(dim=dim(A));
> >    for (i in 1:dim(A)[3]) EE[,,i] <- A[,,i] %*% B[,,i]
> >    EE
> >}
> >
> >system.time(arraymatprod(AA, BB))
> >
> >--- end R code ---
> >
> >The second snippet is correct:
> >
> >--- begin R code ---
> >
> >arraymatrixproduct <- function(A, B) {
> >	require(abind)
> >	dA <- dim(A)[1]
> >	AB <- abind(A, B, along = 1)
> >	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ] %*%
> >mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
> >}
> >
> >system.time(arraymatrixproduct(AA, BB))
> >
> >--- end R code ---
> >
> >However, the second is almost twice as long as the first snippet.
> >
> >Many thanks,
> >Ranjan
> >
> >
> >On Fri, 10 Jul 2015 08:23:49 -0500 Ranjan Maitra
> ><maitra.mbox.ignored at inbox.com> wrote:
> >
> >> Dear friends,
> >> 
> >> I have two 3-d arrays of appropriate dimensions. I want to
> >postmultiply the 2-D matrix layers of the first with the 2-D matrix
> >layers of the second. Can I do this easily in R avoiding loops?
> >> 
> >> As an example:
> >> 
> >> --- begin R code ---
> >> 
> >> AA <- array(1:60, dim = c(5500,44,33))
> >> BB <- array(1:60, dim = c(44,44,33))
> >> 
> >> 
> >> arraymatprod <- function(A, B) {
> >>     EE <- array(dim=dim(A));
> >>     for (i in 1:3) EE[,,i] <- A[,,i] %*% B[,,i]
> >>     EE
> >> }
> >> 
> >> system.time(arraymatprod(AA, BB))
> >> 
> >> --- end R code ---
> >> 
> >> So, is there a way to do this without the loop?
> >> 
> >> Of course, I could abind the arrays and then use apply with an
> >appropriate function which would be as follows:
> >> 
> >> --- begin R code ---
> >> 
> >> arraymatrixproduct <- function(A, B) {
> >> 	require(abind)
> >> 	dA <- dim(A)[1]
> >> 	AB <- abind(A, B, along = 1)
> >> 	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ]
> >%*% mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
> >> }
> >> 
> >> system.time(arraymatrixproduct(AA, BB))
> >> 
> >> --- end R code ---
> >> 
> >> However, this turns out to be slower -- perhaps because of the use of
> >abind and filling the array inside the function.
> >> 
> >> I just wanted suggestions to get this operation done more
> >efficiently.
> >> 
> >> Many thanks and best wishes,
> >> Ranjan
> >> 
> >> ____________________________________________________________
> >> Publish your photos in seconds for FREE
> >> TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4
> >> 
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From dwinsemius at comcast.net  Fri Jul 10 17:45:47 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 10 Jul 2015 08:45:47 -0700
Subject: [R] Embed fonts in an R graph
In-Reply-To: <1436539259216-4709707.post@n4.nabble.com>
References: <1436539259216-4709707.post@n4.nabble.com>
Message-ID: <F970CF15-3ABB-4BE9-89DF-681499628A25@comcast.net>


On Jul 10, 2015, at 7:40 AM, Edwin Sun wrote:

> Hello all,
> 
> I cannot embed a common font type into an R graph. I did it successfully in
> December 2014 with the previous R version. However, with R 3.2.1 in July
> 2015, the following sample codes do not work anymore.
> 
> pdf(file = "c:/testA.pdf", family = "serif")
> plot(x = 1:10, y = rnorm(10))
> dev.off()
> embedFonts(file = "c:/testA.pdf", outfile = "c:/testB.pdf")
> 

When you look at ?embedFonts it becomes obvious that this is a process mediated by GhostScript, an external program no under the control of the R Team, but rather a system facility that you are responsible for maintaining on your unspecified (but inferable) operating system.


> As a result, both testA.pdf and testB.pdf cannot embed the fonts into the
> graph.

That sentence did not make sense to me. I would have expected 'testA.pdf" to have been altered at all.

-- 
David.

Since You are posting through Nabble I'd suggest you read the Posting Guide:

R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


> Specifically, Adobe Acrobt reveals that "Times-Roman" is substituted
> by "TimesNewRomanPSMT", and "ZapfDingbats" is substituted by "AdobePiStd". 
> 
> Any help is greatly appreciated.
> 
> Edwin
> 
> <http://r.789695.n4.nabble.com/file/n4709707/Capture.png> 
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Embed-fonts-in-an-R-graph-tp4709707.html
> Sent from the R help mailing list archive at Nabble.com.
-- 

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Fri Jul 10 17:53:12 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 10 Jul 2015 08:53:12 -0700
Subject: [R] Embed fonts in an R graph
In-Reply-To: <F970CF15-3ABB-4BE9-89DF-681499628A25@comcast.net>
References: <1436539259216-4709707.post@n4.nabble.com>
	<F970CF15-3ABB-4BE9-89DF-681499628A25@comcast.net>
Message-ID: <62126A2E-DE06-4320-915D-3BD584E48657@comcast.net>


On Jul 10, 2015, at 8:45 AM, David Winsemius wrote:

> 
> On Jul 10, 2015, at 7:40 AM, Edwin Sun wrote:
> 
>> Hello all,
>> 
>> I cannot embed a common font type into an R graph. I did it successfully in
>> December 2014 with the previous R version. However, with R 3.2.1 in July
>> 2015, the following sample codes do not work anymore.
>> 
>> pdf(file = "c:/testA.pdf", family = "serif")
>> plot(x = 1:10, y = rnorm(10))
>> dev.off()
>> embedFonts(file = "c:/testA.pdf", outfile = "c:/testB.pdf")
>> 
> 
> When you look at ?embedFonts it becomes obvious that this is a process mediated by GhostScript, an external program no under the control of the R Team, but rather a system facility that you are responsible for maintaining on your unspecified (but inferable) operating system.
> 

And don't forget to read your News. In this case, there is an item in the very first entry about embedFonts:

> str(news())
Classes ?news_db_from_Rd?, ?news_db? and 'data.frame':	681 obs. of  4 variables:
 $ Version : chr  "3.1.2" "3.1.2" "3.1.2" "3.1.2" ...
 $ Date    : chr  NA NA NA NA ...
 $ Category: chr  "NEW FEATURES" "NEW FEATURES" "NEW FEATURES" "NEW FEATURES" ...
 $ Text    : chr  "embedFonts() now defaults to format = \"ps2write\" for .ps and .eps\nfiles.  This is available in Ghostscript 9.x (since 2010) "| __truncated__ "For consistency with [dpqr]norm(), [dp]lnorm(sdlog = 0) model a\npoint mass at exp(mulog) rather than return NaN (for an error)"| __truncated__ "capabilities() now reports if ICU is compiled in for use for\ncollation (it is only actually used if a suitable locale is set f"| __truncated__ "(OS X only.) Package tcltk checks when loaded if it is linked\nagainst the CRAN X11-based Tcl/Tk and if so that the Tcl/Tk\ncom"| __truncated__ ...
 - attr(*, "bad")= logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 - attr(*, "package")= chr "R"

Here's a way to search for any others:

> newstuff$Text[ grepl("embed", newstuff$Text) ]
[1] "embedFonts() now defaults to format = \"ps2write\" for .ps and .eps\nfiles.  This is available in Ghostscript 9.x (since 2010) whereas\nthe previous default, format = \"pswrite\", was removed in\nGhostscript 9.10."
[2] "Error messages from bugs in embedded Sexpr code in Sweave documents\nnow report the source location."                                                                                                                 
[3] "read.table(), readLines() and scan() have a new argument to\ninfluence the treatment of embedded nuls."                                                                                                               
[4] "read.table(), readLines() and scan() now warn when an embedded nul\nis found in the input.  (Related to PR#15625 which was puzzled by\nthe behaviour in this unsupported case.)"       




> 
>> As a result, both testA.pdf and testB.pdf cannot embed the fonts into the
>> graph.
> 
> That sentence did not make sense to me. I would have expected 'testA.pdf" to have been altered at all.
> 
> -- 
> David.
> 
> Since You are posting through Nabble I'd suggest you read the Posting Guide:
> 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
>> Specifically, Adobe Acrobt reveals that "Times-Roman" is substituted
>> by "TimesNewRomanPSMT", and "ZapfDingbats" is substituted by "AdobePiStd". 
>> 
>> Any help is greatly appreciated.
>> 
>> Edwin
>> 
>> <http://r.789695.n4.nabble.com/file/n4709707/Capture.png> 
>> 
>> 
>> 
>> 
>> --
>> View this message in context: http://r.789695.n4.nabble.com/Embed-fonts-in-an-R-graph-tp4709707.html
>> Sent from the R help mailing list archive at Nabble.com.
> -- 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From wdunlap at tibco.com  Fri Jul 10 18:58:59 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 10 Jul 2015 09:58:59 -0700
Subject: [R] How to assign value to a variable dynamically constructed
In-Reply-To: <44a899b0-dc83-4dfd-99d5-f6dc91d17474@googlegroups.com>
References: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
	<CAFEqCdyF82JcOQ7p4UmaH7S1wS81M7tBPuPT1sz9Loqck2jJdA@mail.gmail.com>
	<CAF8bMcZyBF1RgVvC=M_iU+QToBK0_FuDozTw7w0qk70WZ4BLaA@mail.gmail.com>
	<44a899b0-dc83-4dfd-99d5-f6dc91d17474@googlegroups.com>
Message-ID: <CAF8bMcbHMtV5KnjZU016+qFheArLDSxR=PEh_Sqz86pk-_YkZw@mail.gmail.com>

Yes, assign() and get() can do this, but I think the [[]] syntax is simpler
and makes it easier to switch between lists and environments for
data organization.

E.g., the translation of
   myData[[varName]][4] <- myData[[varName]][4] * 100
where myData is an environment to the get/assign style
would be something like
   tmp <- get(varName, envir=myData)
   tmp[4] <- tmp[4] * 100
   assign(varName, tmp, envir=myData)

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Jul 10, 2015 at 9:44 AM, Bastien Tran <bastien.tran at gmail.com>
wrote:

> Dear all,
>
> Provided I understood correctly, shouldn't assign() do the trick? Most
> similar threads seem to include this approach (among others, indeed).
>
> Regards,
> Bastien
>
>
> On Wednesday, July 8, 2015 at 7:30:04 PM UTC+2, William Dunlap wrote:
> > You can use an environment instead of a list using the same [[ syntax.
> It
> > is like 'get0(..., inherit=FALSE)' on the left side of the <- and like
> > 'assign(...)' on the right side.   E.g.,
> >    myData <- new.env()
> >    varName <- "v1"
> >    myData[[varName]] <- 1:10
> >    myData[[varName]][4] <- myData[[varName]][4] * 100
> >    myData[[varName]]
> >    #  [1]   1   2   3 400   5   6   7   8   9  10
> >    names(myData)
> >    # [1] "v1"
> > (Before R-3.2.0 or so, you had to use objects(myData,all=TRUE) if
> > myData was an environment and names(myData) if it was a list.  Now
> > names() works for environments.)
> >
> > It is better to use a dedicated environment (or list) for each set of
> > related
> > variables so that name collisions do not cause problems.
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> > On Wed, Jul 8, 2015 at 10:06 AM, Greg Snow <538280 at gmail.com> wrote:
> >
> > > This is FAQ 7.21.
> > >
> > > The most important part of the answer in FAQ 7.21 is the last section
> > > where it states that it is often easier to use a list rather than
> > > messing around with trying to dynamically name global variables.
> > >
> > > If you tell us what you are trying to accomplish then we may have
> > > better advice.  The route you are headed down now usually leads to
> > > inefficient code and hard to find bugs.
> > >
> > > On Tue, Jul 7, 2015 at 2:53 PM, Jun Shen <jun.shen.ut at gmail.com>
> wrote:
> > > > Dear list,
> > > >
> > > > Let's say we have a variable (id), whose name is dynamically
> constructed.
> > > > This variable represents a vector or data frame with many elements.
> Now I
> > > > want to specifically assign a value to one of the elements. I
> couldn't
> > > get
> > > > it right.
> > > >
> > > > test <- 'id' # "id" is dynamically constructed through paste()
> > > >
> > > > id <- 1:4
> > > >
> > > > # I can get the element by doing
> > > >
> > > > get(test)[2]
> > > >
> > > > # Now I want to assign a value to the second element of this
> dynamical
> > > > variable.
> > > >
> > > > get(test)[2] <- 5  # doesn't work.
> > > >
> > > > Thanks a lot.
> > > >
> > > > Jun Shen
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> > > --
> > > Gregory (Greg) L. Snow Ph.D.
> > > 538280 at gmail.com
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Fri Jul 10 19:06:54 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 10 Jul 2015 10:06:54 -0700
Subject: [R] Corrected: approaches to matrix multiplication of each
	layer of 3-d array with different matrix
In-Reply-To: <20150710103737.af188bd53acca2a093a1b98b@inbox.com>
References: <20150710082349.54719564cd8cc2a6db42060d@inbox.com>
	<20150710084419.dc76c2fdbf40f951458e53ba@inbox.com>
	<40750368-0D34-44CC-89E5-AEF66EDBA294@dcn.davis.CA.us>
	<20150710103737.af188bd53acca2a093a1b98b@inbox.com>
Message-ID: <9306B27F-8001-4811-B91A-4D5071FB18BC@dcn.davis.CA.us>

EE[,,1] <- A[,,1] %*% B[,,1]
EE[,,2] <- A[,,2] %*% B[,,2]
EE[,,3] <- A[,,3] %*% B[,,3]
etc.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 10, 2015 8:37:37 AM PDT, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
>What does it mean to unroll the loop?
>
>Thanks!
>
>Ranjan
>
>
>On Fri, 10 Jul 2015 07:07:15 -0700 Jeff Newmiller
><jdnewmil at dcn.davis.ca.us> wrote:
>
>> Strictly speaking, the answer is yes because you can unroll the loop,
>but that probably is not what you really want or need to do.
>> 
>> Your example seems about right, but it is not clear how you plan to
>make 44 conform with 5500.
>>
>---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>>
>---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 10, 2015 6:44:19 AM PDT, Ranjan Maitra
><maitra.mbox.ignored at inbox.com> wrote:
>> >Hi,
>> >
>> >Sorry to post again, but there is a careless error in my first R
>code
>> >snippet:
>> >
>> >--- begin R code ---
>> > 
>> >AA <- array(1:60, dim = c(5500,44,33))
>> >BB <- array(1:60, dim = c(44,44,33))
>> > 
>> > 
>> >arraymatprod <- function(A, B) {
>> >    EE <- array(dim=dim(A));
>> >    for (i in 1:dim(A)[3]) EE[,,i] <- A[,,i] %*% B[,,i]
>> >    EE
>> >}
>> >
>> >system.time(arraymatprod(AA, BB))
>> >
>> >--- end R code ---
>> >
>> >The second snippet is correct:
>> >
>> >--- begin R code ---
>> >
>> >arraymatrixproduct <- function(A, B) {
>> >	require(abind)
>> >	dA <- dim(A)[1]
>> >	AB <- abind(A, B, along = 1)
>> >	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA, ]
>%*%
>> >mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
>> >}
>> >
>> >system.time(arraymatrixproduct(AA, BB))
>> >
>> >--- end R code ---
>> >
>> >However, the second is almost twice as long as the first snippet.
>> >
>> >Many thanks,
>> >Ranjan
>> >
>> >
>> >On Fri, 10 Jul 2015 08:23:49 -0500 Ranjan Maitra
>> ><maitra.mbox.ignored at inbox.com> wrote:
>> >
>> >> Dear friends,
>> >> 
>> >> I have two 3-d arrays of appropriate dimensions. I want to
>> >postmultiply the 2-D matrix layers of the first with the 2-D matrix
>> >layers of the second. Can I do this easily in R avoiding loops?
>> >> 
>> >> As an example:
>> >> 
>> >> --- begin R code ---
>> >> 
>> >> AA <- array(1:60, dim = c(5500,44,33))
>> >> BB <- array(1:60, dim = c(44,44,33))
>> >> 
>> >> 
>> >> arraymatprod <- function(A, B) {
>> >>     EE <- array(dim=dim(A));
>> >>     for (i in 1:3) EE[,,i] <- A[,,i] %*% B[,,i]
>> >>     EE
>> >> }
>> >> 
>> >> system.time(arraymatprod(AA, BB))
>> >> 
>> >> --- end R code ---
>> >> 
>> >> So, is there a way to do this without the loop?
>> >> 
>> >> Of course, I could abind the arrays and then use apply with an
>> >appropriate function which would be as follows:
>> >> 
>> >> --- begin R code ---
>> >> 
>> >> arraymatrixproduct <- function(A, B) {
>> >> 	require(abind)
>> >> 	dA <- dim(A)[1]
>> >> 	AB <- abind(A, B, along = 1)
>> >> 	array(apply(X = AB, MARGIN = 3, FUN = (function(mat) ((mat[1:dA,
>]
>> >%*% mat[(dA+1):(dim(AB)[1]),])))), dim = dim(A))
>> >> }
>> >> 
>> >> system.time(arraymatrixproduct(AA, BB))
>> >> 
>> >> --- end R code ---
>> >> 
>> >> However, this turns out to be slower -- perhaps because of the use
>of
>> >abind and filling the array inside the function.
>> >> 
>> >> I just wanted suggestions to get this operation done more
>> >efficiently.
>> >> 
>> >> Many thanks and best wishes,
>> >> Ranjan
>> >> 
>> >> ____________________________________________________________
>> >> Publish your photos in seconds for FREE
>> >> TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4
>> >> 
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>


From yue-hu-1 at uiowa.edu  Wed Jul  1 23:58:25 2015
From: yue-hu-1 at uiowa.edu (Hu, Yue)
Date: Wed, 1 Jul 2015 21:58:25 +0000
Subject: [R] [R-pkgs] interplot: a new package for visualizing
	interactive	effects
In-Reply-To: <BLUPR04MB006BED96EBDD7488D18FE2280A80@BLUPR04MB006.namprd04.prod.outlook.com>
References: <BLUPR04MB006BED96EBDD7488D18FE2280A80@BLUPR04MB006.namprd04.prod.outlook.com>
Message-ID: <BLUPR04MB0069C4B94C006A47B24755D80A80@BLUPR04MB006.namprd04.prod.outlook.com>


Dear colleagues,

We just published a package, "interplot",  in CRAN. It can be used to plots the conditional coefficients ("marginal effects") of variables included in multiplicative interaction terms for various models.  The installation instruction and more details about the package are available in http://cran.r-project.org/web/packages/interplot/vignettes/interplot-vignette.html.

Please contact me with any questions, bug reports, and comments.

Best,

Hu, Yue
Ph.D. Student, Political Science,
University of Iowa,
313 Schaeffer Hall,
20E Washington St.,
Iowa City, IA, 52242.


	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From p.jagadish at inrhythm-inc.com  Fri Jul 10 14:55:25 2015
From: p.jagadish at inrhythm-inc.com (jagadishpchary)
Date: Fri, 10 Jul 2015 05:55:25 -0700 (PDT)
Subject: [R] Reading the non delimited file with no particular patterns in
 the data to R
In-Reply-To: <1433839253539-4708379.post@n4.nabble.com>
References: <1433839253539-4708379.post@n4.nabble.com>
Message-ID: <1436532925238-4709699.post@n4.nabble.com>

I am beginner in R and I want to read a ASCII file to R environment. However,
the ASCII file is a non delimited and the data is not continuous (have some
blank spaces between the variables) so in order to read the data i have used
the below syntax i.e
test <- read.fwf("D:/R_process/ASCII.txt", width = c(10, 4, 1, 4, 9, 9,
1,1,1,1,1,1,1,3,8))

Now i am able to read it but the data read is wrong. Actually my out put
should have only the applicable variables data but not the blank data.
Attached is the ASCII data. Please let me know how should i write the syntax
to read only the applicable data in the file.

Thanks for your help in advance. test.txt
<http://r.789695.n4.nabble.com/file/n4709699/test.txt>  



--
View this message in context: http://r.789695.n4.nabble.com/Cross-tabulation-with-top-one-variable-and-side-as-multiple-variables-tp4708379p4709699.html
Sent from the R help mailing list archive at Nabble.com.


From Keith.Jewell at campdenbri.co.uk  Fri Jul 10 12:18:20 2015
From: Keith.Jewell at campdenbri.co.uk (Keith Jewell)
Date: Fri, 10 Jul 2015 11:18:20 +0100
Subject: [R] predict.poly for multivariate data
Message-ID: <mno65c$atb$1@ger.gmane.org>

A recent stackoverflow post 
<http://stackoverflow.com/questions/31134985> "How do you make R poly() 
evaluate (or ?predict?) multivariate new data (orthogonal or raw)?" made 
me look at poly and polym again.

predict.poly doesn't work with multivariate data because for such data 
poly calls polym which does not:
a) return an object inheriting from class poly;
b) return the coefficients needed to make predictions;
c) accept coefficients as an argument or include code to make predictions.

This does lead to some wrong answers without warnings. e.g.
################## vanilla poly and polym ###########
library(datasets)
alm <- lm(stack.loss ~ poly(Air.Flow, Water.Temp, degree=3), stackloss)
# "correct" prediction values [1:10]
alm$fitted.values[1:10]
# predict(alldata)[1:10] gives correct values
predict(alm, stackloss)[1:10]
# predict(partdata) gives wrong values
predict(alm, stackloss[1:10,])
#########
I guess - but haven't confirmed - that with multivariate newdata 
predict.lm(alm, newdata) calculates new orthogonal polynomials based on 
newdata rather than applying the original coefficients.

Below I append versions of:
a) polym edited to address the three points above;
b) poly slightly edited to reflect the changes in polym;
c) predict.poly unaltered, just to get it in the same environment as 
polym and poly for testing.
After implementing these the three sets of predictions above all agree.

I'm very ready to believe that I've got the wrong end of the stick 
and/or my suggestions can be improved so I welcome correction.

Otherwise, how do I go about getting these changes implemented?
I see stats is maintained by R Core Team. Are they likely to pick it up 
from here, or do I need to take any other action?

Best regards

Keith Jewell

### polym ##############################################
polym <- function (..., degree = 1, coefs = NULL, raw = FALSE)
# add coefs argument
{
   if(is.null(coefs)) {
     dots <- list(...)
     nd <- length(dots)
     if (nd == 0)
       stop("must supply one or more vectors")
     if (nd == 1)
       return(poly(dots[[1L]], degree, raw = raw))
     n <- sapply(dots, length)
     if (any(n != n[1L]))
       stop("arguments must have the same length")
     z <- do.call("expand.grid", rep.int(list(0:degree), nd))
     s <- rowSums(z)
     ind <- (s > 0) & (s <= degree)
     z <- z[ind, ]
     s <- s[ind]
     aPoly <- poly(dots[[1L]], degree, raw = raw) # avoid 2 calcs
     res <- cbind(1, aPoly)[, 1 + z[, 1]]
# attribute "coefs" = list of coefs from individual variables
     if (!raw) coefs <- list(attr(aPoly, "coefs"))
     for (i in 2:nd) {
       aPoly <- poly(dots[[i]], degree, raw = raw)
       res <- res * cbind(1, aPoly)[, 1 + z[, i]]
       if (!raw) coefs <- c(coefs, list(attr(aPoly, "coefs")))
     }
     colnames(res) <- apply(z, 1L, function(x) paste(x, collapse = "."))
     attr(res, "degree") <- as.vector(s)
     if (!raw) attr(res, "coefs") <- coefs
     class(res) <- c("poly", "matrix") # add poly class
     res
   }
   else
   {
     nd <- length(coefs)    # number of variables
     newdata <- as.data.frame(list(...)) # new data
     if (nd != ncol(newdata)) stop("wrong number of columns in newdata")
     z <- do.call("expand.grid", rep.int(list(0:degree), nd))
     s <- rowSums(z)
     ind <- (s > 0) & (s <= degree)
     z <- z[ind, ]
     res <- cbind(1, poly(newdata[[1]], degree=degree, 
coefs=coefs[[1]]))[, 1 + z[, 1]]
     for (i in 2:nd) res <- res*cbind(1, poly(newdata[[i]], 
degree=degree, coefs=coefs[[i]]))[, 1 + z[, i]]
     colnames(res) <- apply(z, 1L, function(x) paste(x, collapse = "."))
     res
   }
}
######################

#### poly ##################
poly <- function (x, ..., degree = 1, coefs = NULL, raw = FALSE)
{
   dots <- list(...)
   if (nd <- length(dots)) {
     if (nd == 1 && length(dots[[1L]]) == 1L)
       degree <- dots[[1L]]
# pass coefs argument as well
     else return(polym(x, ..., degree = degree, coefs=coefs, raw = raw))
   }
   if (is.matrix(x)) {
     m <- unclass(as.data.frame(cbind(x, ...)))
# pass coefs argument as well
     return(do.call("polym", c(m, degree = degree, raw = raw, 
list(coefs=coefs))))
   }
   if (degree < 1)
     stop("'degree' must be at least 1")
   if (anyNA(x))
     stop("missing values are not allowed in 'poly'")
   n <- degree + 1
   if (raw) {
     Z <- outer(x, 1L:degree, "^")
     colnames(Z) <- 1L:degree
     attr(Z, "degree") <- 1L:degree
     class(Z) <- c("poly", "matrix")
     return(Z)
   }
   if (is.null(coefs)) {
     if (degree >= length(unique(x)))
       stop("'degree' must be less than number of unique points")
     xbar <- mean(x)
     x <- x - xbar
     X <- outer(x, seq_len(n) - 1, "^")
     QR <- qr(X)
     if (QR$rank < degree)
       stop("'degree' must be less than number of unique points")
     z <- QR$qr
     z <- z * (row(z) == col(z))
     raw <- qr.qy(QR, z)
     norm2 <- colSums(raw^2)
     alpha <- (colSums(x * raw^2)/norm2 + xbar)[1L:degree]
     Z <- raw/rep(sqrt(norm2), each = length(x))
     colnames(Z) <- 1L:n - 1L
     Z <- Z[, -1, drop = FALSE]
     attr(Z, "degree") <- 1L:degree
     attr(Z, "coefs") <- list(alpha = alpha, norm2 = c(1,
                                                       norm2))
     class(Z) <- c("poly", "matrix")
   }
   else {
     alpha <- coefs$alpha
     norm2 <- coefs$norm2
     Z <- matrix(, length(x), n)
     Z[, 1] <- 1
     Z[, 2] <- x - alpha[1L]
     if (degree > 1)
       for (i in 2:degree) Z[, i + 1] <- (x - alpha[i]) *
       Z[, i] - (norm2[i + 1]/norm2[i]) * Z[, i - 1]
     Z <- Z/rep(sqrt(norm2[-1L]), each = length(x))
     colnames(Z) <- 0:degree
     Z <- Z[, -1, drop = FALSE]
     attr(Z, "degree") <- 1L:degree
     attr(Z, "coefs") <- list(alpha = alpha, norm2 = norm2)
     class(Z) <- c("poly", "matrix")
   }
   Z
}
################################################

### predict.poly ####
predict.poly <- function (object, newdata, ...)
{
   if (missing(newdata))
     return(object)
   if (is.null(attr(object, "coefs")))
     poly(newdata, degree = max(attr(object, "degree")), raw = TRUE)
   else poly(newdata, degree = max(attr(object, "degree")),
             coefs = attr(object, "coefs"))
}
######################


From 925345468 at qq.com  Fri Jul 10 14:43:53 2015
From: 925345468 at qq.com (=?gb18030?B?anVzdF9yb29raWU=?=)
Date: Fri, 10 Jul 2015 20:43:53 +0800
Subject: [R] =?gb18030?q?how_to_embed_a_3D_plot_created_by_=A1=B0rgl=A1=B1?=
 =?gb18030?q?_into_=22gWidgets_ggraphics_device=A1=B0=3F?=
Message-ID: <tencent_2610C29B02B3E5313D99F015@qq.com>

I created some 3D cubes by using "rgl" package. The next step is to embed the 3D plot("rgl" device) into "ggraphics" device from the "gWidgets" package, but I have no idea how to implement it. Is there a way to implement by "gWidgets" or other package? Any help? Thank you in advance!

My code:
library(rgl)
library(scatterplot3d)
library(gWidgets)
options(guiToolkit="RGtk2")


df<-data.frame(x=c(1,2,3,4),
               y=c(2,4,6,8),
               z=c(3,6,9,12),
               value=c(33,43,75,21))

clr <- df$value/max(df$value)
f <- colorRamp(c("green", "yellow", "purple", "red"))

## 3D PLOT
for(i in 1:length(df$x)){
  shade3d(translate3d(scale3d(cube3d(col=rgb(f(clr[i])/255),alpha=0.15 ),
                                     10.0, 10.0, 2.5),df$x[i],df$y[i],df$z[i]))
} 

window <- gwindow("TEST", width = 800, height= 600, visible = FALSE)
group <- ggroup(cont = window, expand = TRUE)

loadBtn <- gbutton("display", cont = group)

addHandlerChanged(loadBtn, handler = function(h,...){
  sudwin<-gwindow("3D",visible = TRUE)
  dev <- ggraphics(cont=subwin)
  #### DO NOT KNOW HOW TO DO NEXT...
})
visible(window)<-TRUE

From lutipilotto at yahoo.com.br  Fri Jul 10 14:47:49 2015
From: lutipilotto at yahoo.com.br (Luciane Maria Pilotto)
Date: Fri, 10 Jul 2015 05:47:49 -0700
Subject: [R] clm funtion and CI
Message-ID: <1436532469.52981.YahooMailBasic@web120202.mail.ne1.yahoo.com>

I am using the commands bellow. 

##########################################
load(file.choose())#dataframe:"id3.rda"
attach(id3)

dput(head(id3,10))

structure(list(regiao = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L), .Label = c("Norte", "Nordeste", "Sudeste", "Sul", 
"Centro-Oeste"), class = "factor"), estado = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Rondonia", "Acre", 
"Amazonas", "Roraima", "Para", "Amapa", "Tocantins", "Maranhao", 
"Piaui", "Ceara", "Rio Grande Do Norte", "Paraiba", "Pernambuco", 
"Alagoas", "Sergipe", "Bahia", "Minas Gerais", "Espirito Santo", 
"Rio De Janeiro", "Sao Paulo", "Parana", "Santa Catarina", "Rio Grande Do Sul", 
"Mato Grosso Do Sul", "Mato Grosso", "Goias", "Distrito Federal"
), class = "factor"), cod_mun = c(1200401L, 1200401L, 1200401L, 
1200401L, 1200401L, 1200401L, 1200401L, 1200401L, 1200401L, 1200401L
), setor = c("05000056", "05000056", "05000056", "05000056", 
"05000056", "05000056", "05000056", "05000056", "05000062", "05000062"
), cap_int = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L), .Label = c("Interior", "Capital"), class = "factor"), idade = c(15L, 
15L, 17L, 17L, 18L, 18L, 18L, 19L, 19L, 15L), getario = structure(c(3L,
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("5 anos", "12 anos", 
"15 a 19 anos", "35 a 44 anos", "65 a 74 anos"), class = "factor"), 
? ? sexo = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L
? ? ), .Label = c("Male", "Female"), class = "factor"), grp_etni = structure(c(1L, 
? ? 4L, 4L, 4L, 2L, 4L, 4L, 4L, 4L, 4L), .Label = c("Branca", 
? ? "Preta", "Amarela", "Parda", "Indigena", "Sem Informacao"
? ? ), class = "factor"), quest_01 = c(8L, 2L, 4L, 4L, 3L, 4L, 
? ? 3L, 3L, 3L, 4L), quest_02 = c(4L, 4L, 2L, 2L, 4L, 1L, 3L, 
? ? 1L, 3L, 2L), density = c(2, 0.5, 2, 2, 0.75, 4, 1, 3, 1, 
? ? 2), quest_03 = c(3L, 4L, 6L, 5L, 5L, 3L, 3L, 3L, 6L, 5L), 
? ? quest_04 = structure(c(2L, 3L, 3L, 4L, NA, 2L, 1L, NA, 3L, 
? ? 3L), .Label = c("Ate 250", "251 a 500", "501 a 1.500", "1.501 a 2.500", 
? ? "2.501 a 4.500", "4.501 a 9.500", "Mais de 9.500", "Nao sabe/Nao respondeu"
? ? ), class = "factor"), inc_percapita1 = c(46.875, 500, 250, 
? ? 500, NA, 93.75, 41.6666679382324, NA, 333.333343505859, 250
? ? ), inc_percapita2 = c(46.875, 500, 250, 500, NA, 93.75, 41.6666679382324, 
? ? NA, 333.333343505859, 250), inc_sqrt1 = c(132.58251953125, 
? ? 707.106811523438, 500, 1000, NA, 187.5, 72.1687850952148, 
? ? NA, 577.350280761719, 500), inc_sqrt2 = c(132.58251953125, 
? ? 707.106811523438, 500, 1000, NA, 187.5, 72.1687850952148, 
? ? NA, 577.350280761719, 500), q05 = c(11L, 4L, 12L, 6L, 11L, 
? ? 10L, 7L, 12L, 8L, 7L), quest_06 = structure(c(2L, 2L, 2L, 
? ? 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Nao", "Sim", "Nao se aplica", 
? ? "Nao sabe"), class = "factor"), quest_07 = structure(c(1L, 
? ? 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L), .Label = c("Nao", "Sim", 
? ? "Nao se aplica", "Nao sabe"), class = "factor"), quest_08 = c(9L, 
? ? 9L, 9L, 9L, 9L, 9L, 9L, 9L, 3L, 9L), quest_09 = structure(c(1L,
? ? 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Nao", "Sim", 
? ? "Nao se aplica", "Nao sabe"), class = "factor"), quest_10 = structure(c(5L, 
? ? 5L, 2L, 3L, 3L, 2L, 2L, 3L, 3L, 3L), .Label = c("Menos de 1 ano",
? ? "1 a 2 anos", ">3 anos", "Outros", "N?a se aplica", "Nao sabe"
? ? ), class = "factor"), quest_11 = structure(c(5L, 5L, 1L, 
? ? 1L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c("Publico", "Particular",
? ? "Plano de Saude/Convenios", "Outros", "Nao se aplica", "Nao sabe"
? ? ), class = "factor"), quest_12 = structure(c(6L, 6L, 2L, 
? ? 4L, 1L, 3L, 1L, 4L, 2L, 1L), .Label = c("Revisao/Prevencao", 
? ? "Dor", "Extracao", "Tratamento", "Outros", "Nao se aplica", 
? ? "Nao sabe"), class = "factor"), quest_13 = structure(c(NA, 
? ? NA, 2L, 3L, 2L, 3L, 2L, 2L, 2L, 2L), .Label = c("Muito bom", 
? ? "Bom", "Regular", "Ruim", "Muito Ruim", "Nao se aplica", 
? ? "Nao sabe"), class = "factor"), quest_14 = structure(c(3L, 
? ? 2L, 3L, 4L, 2L, 2L, 4L, 3L, 4L, 2L), .Label = c("Muito satisfeito", 
? ? "Satisfeito", "Nem satisfeito/insatisfeito", "Insatisfeito", 
? ? "Muito Insatisfeito", "Nao sabe"), class = "factor"), quest_15 = structure(c(1L, 
? ? 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("Nao", "Sim", 
? ? "Nao se aplica", "Nao sabe"), class = "factor"), exame = c("1",
? ? "1", "1", "1", "1", "1", "1", "1", "1", "1"), cpod = c(3L, 
? ? 0L, 8L, 3L, 3L, 12L, 1L, 6L, 6L, 3L), p_sang = c(0L, 1L, 
? ? 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L), p_calc = c(0L, 1L, 1L, 0L, 
? ? 0L, 1L, 0L, 1L, 0L, 0L), cpi_max = structure(c(1L, 3L, 3L, 
? ? 1L, 1L, 3L, 1L, 3L, 1L, 2L), .Label = c("Higido", "Sangramento",
? ? "Calculo", "Bolsa 4-5 mm", "Bolsa 6 mm ou +", "4", "A", "X"
? ? ), class = "factor"), dai = c(21L, 32L, 23L, 21L, 19L, 18L, 
? ? 17L, 23L, 34L, 25L), trauma = c(NA_integer_, NA_integer_, 
? ? NA_integer_, NA_integer_, NA_integer_, NA_integer_, NA_integer_, 
? ? NA_integer_, NA_integer_, NA_integer_), n_higido = c(26L, 
? ? 28L, 24L, 24L, 25L, 17L, 27L, 25L, 24L, 25L), n_cariado = c(3L,
? ? 0L, 6L, 2L, 0L, 2L, 1L, 2L, 4L, 3L), n_restcar = c(0L, 0L, 
? ? 0L, 0L, 0L, 0L, 0L, 2L, 1L, 0L), n_restaur = c(0L, 0L, 2L, 
? ? 0L, 3L, 6L, 0L, 2L, 0L, 0L), n_perdcar = c(0L, 0L, 0L, 1L, 
? ? 0L, 4L, 0L, 0L, 1L, 0L), n_perdout = c(0L, 0L, 0L, 0L, 0L, 
? ? 0L, 0L, 0L, 0L, 0L), perdidos = c(0, 0, 0, 1, 0, 4, 0, 0, 
? ? 1, 0), usaprots = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), usaproti = structure(c(1L, 
? ? 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("N?o Usa", 
? ? "Uma Ponte Fixa", "Mais de 1 PF", "Pr?tese Parcial Remov?vel",
? ? "Pr?tese Fixa + Remov?vel", "Pr?tese Total", "Sem Informa??o"
? ? ), class = "factor"), necprots = structure(c(1L, 1L, 3L, 
? ? 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c("N?o necessita", 
? ? "Pr?tese 1 elemento", "Mais de 1 elemento", "Combina??o de pr?teses", 
? ? "Pr?tese Total", "Sem Informa??o"), class = "factor"), necproti = structure(c(1L, 
? ? 1L, 4L, 2L, 1L, 3L, 1L, 1L, 2L, 1L), .Label = c("N?o necessita", 
? ? "Pr?tese 1 elemento", "Mais de 1 elemento", "Combina??o de pr?teses", 
? ? "Pr?tese Total", "Sem Informa??o"), class = "factor"), necprot = structure(c(1L, 
? ? 1L, 3L, 2L, 1L, 2L, 1L, 1L, 2L, 1L), .Label = c("N?o necessita", 
? ? "Parcial 1 maxilar", "Parcial 2 maxilar", "Total 1 maxilar", 
? ? "Parcial + Total", "Total 2 maxilar", "Sem Informa??o"), class = "factor"), 
? ? oidp = c(0L, 0L, 0L, 0L, 0L, 2L, 1L, 0L, 0L, 1L), f1 = c(1, 
? ? 1, 1, 1, 1, 1, 1, 1, 1, 1), f2 = c(0.0518900007009506, 0.0518900007009506, 
? ? 0.0518900007009506, 0.0518900007009506, 0.0518900007009506, 
? ? 0.0518900007009506, 0.0518900007009506, 0.0518900007009506, 
? ? 0.106059998273849, 0.106059998273849), f3 = c(0.0662299990653992,
? ? 0.0662299990653992, 0.0662299990653992, 0.0662299990653992, 
? ? 0.0662299990653992, 0.0662299990653992, 0.0662299990653992, 
? ? 0.0662299990653992, 0.138889998197556, 0.138889998197556), 
? ? f = c(0.00343999988399446, 0.00343999988399446, 0.00343999988399446, 
? ? 0.00343999988399446, 0.00343999988399446, 0.00343999988399446, 
? ? 0.00343999988399446, 0.00343999988399446, 0.0147299999371171, 
? ? 0.0147299999371171), bwgr_et = c(291.019989013672, 291.019989013672, 
? ? 291.019989013672, 291.019989013672, 291.019989013672, 291.019989013672, 
? ? 291.019989013672, 291.019989013672, 67.879997253418, 67.879997253418
? ? ), cluster = c(120040105000056, 120040105000056, 120040105000056,
? ? 120040105000056, 120040105000056, 120040105000056, 120040105000056,
? ? 120040105000056, 120040105000062, 120040105000062), cluster2 = c("01120308", 
? ? "01120308", "01120308", "01120308", "01120308", "01120308", 
? ? "01120308", "01120308", "01120309", "01120309"), getni = c(1L, 
? ? 4L, 4L, 4L, 2L, 4L, 4L, 4L, 4L, 4L), q04 = c(2L, 3L, 3L, 
? ? 4L, NA, 2L, 1L, NA, 3L, 3L), q06 = c(2L, 2L, 2L, 2L, 2L, 
? ? 2L, 2L, 2L, 2L, 2L), q07 = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 
? ? 1L, 2L, 1L), q08 = c(0, 0, 0, 0, 0, 0, 0, 0, 3, 0), q10 = c(NA,
? ? NA, 2L, 3L, 3L, 2L, 2L, 3L, 3L, 3L), q11 = c(NA, NA, 1L, 
? ? 1L, 2L, 1L, 1L, 1L, 1L, 1L), q12 = c(NA, NA, 2L, 4L, 1L, 
? ? 3L, 1L, 4L, 2L, 1L), q13 = structure(c(NA, NA, 2L, 3L, 2L, 
? ? 3L, 2L, 2L, 2L, 2L), .Label = c("1", "2", "3", "4", "5"), class = "factor"), 
? ? q14 = c(3L, 2L, 3L, 4L, 2L, 2L, 4L, 3L, 4L, 2L), q15 = c(1L, 
? ? 1L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), edcat = c(3, 1, 4, 2, 
? ? 3, 3, 2, 4, 2, 2), peso = c(291, 291, 291, 291, 291, 291, 
? ? 291, 291, 68, 68), cariado = c(3L, 0L, 6L, 2L, 0L, 2L, 1L, 
? ? 4L, 5L, 3L), getar = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)), .Names = c("regiao", 
"estado", "cod_mun", "setor", "cap_int", "idade", "getario", 
"sexo", "grp_etni", "quest_01", "quest_02", "density", "quest_03", 
"quest_04", "inc_percapita1", "inc_percapita2", "inc_sqrt1", 
"inc_sqrt2", "q05", "quest_06", "quest_07", "quest_08", "quest_09", 
"quest_10", "quest_11", "quest_12", "quest_13", "quest_14", "quest_15", 
"exame", "cpod", "p_sang", "p_calc", "cpi_max", "dai", "trauma", 
"n_higido", "n_cariado", "n_restcar", "n_restaur", "n_perdcar", 
"n_perdout", "perdidos", "usaprots", "usaproti", "necprots", 
"necproti", "necprot", "oidp", "f1", "f2", "f3", "f", "bwgr_et", 
"cluster", "cluster2", "getni", "q04", "q06", "q07", "q08", "q10", 
"q11", "q12", "q13", "q14", "q15", "edcat", "peso", "cariado", 
"getar"), row.names = c("1241", "1242", "1243", "1244", "1245", 
"1246", "1247", "1248", "1256", "1268"), class = "data.frame")
################################################################################

id3$q13<-as.factor(id3$q13)
m1 <- polr(q13 ~ q11 + q10 + q12 + edcat + q08 + q06 + q14, data=id3, Hess=TRUE) 
summary(m1)
ordinal.or.display(m1)
####################################################################################

fm1 <- clm(q13 ~ q11 + q10 + q12 + edcat + q08 + q06 + q14, data=id3) 

summary(fm1)
exp(coef(fm1)) 

[[elided Yahoo spam]]
(ci <- confint(fm1))
exp(cbind(OR = coef(fm1), ci))

############################################################################################
nominal_test(fm1)# test partial proportional odds assumption

##clm2 - partial proprotional odds

fm.nom <- clm2(q13 ~ q11 + q10 + q12 + q08 + q06 + q14, data=id3, nominal=~ edcat)
summary(fm.nom)
exp(coef(fm.nom))
[[elided Yahoo spam]]
exp(cbind(OR = coef(fm.nom), ci))

Thanks,
Luciane
--------------------------------------------
Em qui, 9/7/15, Kevin Wright <kw.stat at gmail.com> escreveu:

 Assunto: Re: [R] clm funtion and CI

 Data: Quinta-feira, 9 de Julho de 2015, 11:44

 You need a reproducible
 example.

 On Wed, Jul 8, 2015 at 7:43
 PM, Luciane Maria Pilotto

 wrote:
 > Hi,
 >
 > I'm working with ordinal logistic
 regression and fitting the model with the "clm"
 funtion of the ordinal package and would like to get the CI.
 According to the Tutorial on fitting Cumulative Link Models
 with the ordinal Package, Rune Haubo B Christensen (21
 January 2015) you can run the OR, but not CI. The same
 happens with the "clm2" for partial proportional
 odds.
 >
 > I appreciate
 any help !!
 >
 >
 > Luciane
 >
 >
 ______________________________________________
 > R-help at r-project.org
 mailing list -- To UNSUBSCRIBE and more, see
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal,
 self-contained, reproducible code.



 -- 
 Kevin Wright


From r.otojanov at qmul.ac.uk  Fri Jul 10 17:15:53 2015
From: r.otojanov at qmul.ac.uk (mrrox)
Date: Fri, 10 Jul 2015 08:15:53 -0700 (PDT)
Subject: [R] cointegration and VECM, urca package and Eviews
Message-ID: <1436541353529-4709708.post@n4.nabble.com>

Hello,

I estimated a VECM in Eviews and R using urca package's ca.jo(), cajorl()
and vec2var() functions. 
Specifications are 'no trend' in Eviews and 'none' in R (no theory, just
testing, feel free to make changes). 

Results are different, ecm and cointegrating vectors are completely
different.

R code is: 
*johcoint=ca.jo(Ydata,type="trace",ecdet=c("none"),K=2,spec="transitory")
summary(johcoint)
vecm.r1=cajorls(johcoint,r=1)
vecm.r1
vecm.l=vec2var(johcoint,r=1)
ll=irf(vecm.l, impulse = "B",response = "A", boot = FALSE)
plot(ll$irf[[1]])*

Data:

	     A	              B	              C              D
1	8.646924	3.925155	2.297737	2.764267
2	8.643810	4.048215	2.140731	2.769231
3	8.634732	4.117114	2.063724	2.747604
4	8.603337	3.976002	1.939290	2.741640
5	8.604344	3.924697	1.928255	2.732419
6	8.628887	3.921517	1.878674	2.718437
7	8.653167	3.906076	1.943236	2.693620
8	8.661854	3.940468	2.107718	2.670370
9	8.609839	3.872782	2.003064	2.689212
10	8.614091	3.905839	1.973719	2.679186
11	8.613692	3.890797	1.939311	2.659350
12	8.651488	4.052423	1.961038	2.640751
13	8.654469	4.137534	2.130873	2.622611
14	8.693121	4.074753	2.108427	2.595760
15	8.699435	3.872412	2.091816	2.622049
16	8.808724	3.851373	2.345740	2.646252
17	8.814437	3.806048	2.057104	2.728953
18	8.836529	3.743046	1.825827	2.748266
19	8.826898	3.693897	1.823880	3.027604
20	8.809117	3.673126	2.020016	2.820051
21	8.654972	3.652903	1.523249	2.538225
22	8.515917	3.659592	1.617734	2.523293
23	8.589919	3.655822	1.827645	2.371598
24	8.595193	3.645937	1.825603	2.251557
25	8.615332	3.629201	1.661946	2.254364
26	8.671222	3.609464	1.733073	2.145093
27	8.611882	3.612110	1.794937	1.819291
28	8.688414	3.579205	1.505888	1.654666
29	8.690125	3.554958	1.426589	1.731257
30	8.725932	3.533288	1.522311	1.788969
31	8.743279	3.527591	1.601261	1.760313
32	8.694805	3.531611	1.634085	1.732271
33	8.687983	3.527327	1.601985	1.836593
34	8.716976	3.514645	1.589035	1.745653
35	8.775464	3.492427	1.471562	1.699377
36	8.808898	3.471036	1.460162	1.686131
37	8.842847	3.451130	1.579547	1.670513
38	8.871786	3.428002	1.597216	1.618989
39	8.907425	3.423887	1.626208	1.652055
40	8.924721	3.403657	1.578576	1.509779
41	8.941122	3.357645	1.521236	1.607082
42	9.009112	3.314089	1.506758	1.544039
43	9.029894	3.267795	1.483968	1.518783
44	9.055359	3.240397	1.517348	1.517085
45	9.040278	3.235410	1.590436	1.509334
46	8.993796	3.252374	1.651106	1.431041
47	8.967464	3.236265	1.672098	1.338936
48	8.952859	3.235916	1.662450	1.287055
49	9.121430	3.217599	1.711292	1.263948
50	9.147871	3.205194	1.676641	1.211038

Will anyone please help why this might happen?
Perhaps I am estimating the models incorrectly?
Thank you



--
View this message in context: http://r.789695.n4.nabble.com/cointegration-and-VECM-urca-package-and-Eviews-tp4709708.html
Sent from the R help mailing list archive at Nabble.com.


From bastien.tran at gmail.com  Fri Jul 10 18:44:12 2015
From: bastien.tran at gmail.com (Bastien Tran)
Date: Fri, 10 Jul 2015 09:44:12 -0700 (PDT)
Subject: [R] How to assign value to a variable dynamically constructed
In-Reply-To: <CAF8bMcZyBF1RgVvC=M_iU+QToBK0_FuDozTw7w0qk70WZ4BLaA@mail.gmail.com>
References: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
	<CAFEqCdyF82JcOQ7p4UmaH7S1wS81M7tBPuPT1sz9Loqck2jJdA@mail.gmail.com>
	<CAF8bMcZyBF1RgVvC=M_iU+QToBK0_FuDozTw7w0qk70WZ4BLaA@mail.gmail.com>
Message-ID: <44a899b0-dc83-4dfd-99d5-f6dc91d17474@googlegroups.com>

Dear all,

Provided I understood correctly, shouldn't assign() do the trick? Most similar threads seem to include this approach (among others, indeed).

Regards,
Bastien


On Wednesday, July 8, 2015 at 7:30:04 PM UTC+2, William Dunlap wrote:
> You can use an environment instead of a list using the same [[ syntax.  It
> is like 'get0(..., inherit=FALSE)' on the left side of the <- and like
> 'assign(...)' on the right side.   E.g.,
>    myData <- new.env()
>    varName <- "v1"
>    myData[[varName]] <- 1:10
>    myData[[varName]][4] <- myData[[varName]][4] * 100
>    myData[[varName]]
>    #  [1]   1   2   3 400   5   6   7   8   9  10
>    names(myData)
>    # [1] "v1"
> (Before R-3.2.0 or so, you had to use objects(myData,all=TRUE) if
> myData was an environment and names(myData) if it was a list.  Now
> names() works for environments.)
> 
> It is better to use a dedicated environment (or list) for each set of
> related
> variables so that name collisions do not cause problems.
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> On Wed, Jul 8, 2015 at 10:06 AM, Greg Snow <538280 at gmail.com> wrote:
> 
> > This is FAQ 7.21.
> >
> > The most important part of the answer in FAQ 7.21 is the last section
> > where it states that it is often easier to use a list rather than
> > messing around with trying to dynamically name global variables.
> >
> > If you tell us what you are trying to accomplish then we may have
> > better advice.  The route you are headed down now usually leads to
> > inefficient code and hard to find bugs.
> >
> > On Tue, Jul 7, 2015 at 2:53 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
> > > Dear list,
> > >
> > > Let's say we have a variable (id), whose name is dynamically constructed.
> > > This variable represents a vector or data frame with many elements. Now I
> > > want to specifically assign a value to one of the elements. I couldn't
> > get
> > > it right.
> > >
> > > test <- 'id' # "id" is dynamically constructed through paste()
> > >
> > > id <- 1:4
> > >
> > > # I can get the element by doing
> > >
> > > get(test)[2]
> > >
> > > # Now I want to assign a value to the second element of this dynamical
> > > variable.
> > >
> > > get(test)[2] <- 5  # doesn't work.
> > >
> > > Thanks a lot.
> > >
> > > Jun Shen
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > 538280 at gmail.com
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ljylia.0101 at gmail.com  Fri Jul 10 11:17:56 2015
From: ljylia.0101 at gmail.com (Lia LEE)
Date: Fri, 10 Jul 2015 18:17:56 +0900
Subject: [R] (no subject)
Message-ID: <CAPHYR5u10WT36nsc2uh7jkR0esVh_AMTYrr8uYt5oXL2evA-fg@mail.gmail.com>

Hello forum members,

I am taking this R course which I have to admit that it is beyond my
capability. I am asked to analyze the following data(file attached) according
to the question provided below:

1. The file [data_13-9.txt] contains a data of length 225, which seems to
have some cycle pattern.
(1) Fit a suitable model for this data, then report the model equation.
(2) Provide the forecast values for 3-steps ahead and 6-steps ahead.

I am desperately seeking for your help, since I really don't know where to
start.. I have done "plot.ts(data.13-6)", but not further. Above question
is just part of the whole task, but I believe solving this would be very
much helpful for me to jump in to the other questions. It would be easier
for me to follow if your answer is in R script..
I appreciate your patience and help.
Thank you so much in advance!

Cheers,
Lia
-------------- next part --------------
fourth_a.ts
1 -0.0242227774450564
2 0.447778425867187
3 0.0679634009051251
4 0.390085614914361
5 0.249535226083153
6 0.518797872921499
7 -0.0969256279112136
8 0.123337983703432
9 -0.414941322119016
10 -0.27757276812968
11 0.0403939285820523
12 0.494211788018315
13 0.124244228153442
14 0.394493565117432
15 0.188705401390586
16 0.532762644104765
17 -0.10747895472636
18 0.144164346660537
19 -0.336048738751554
20 -0.268668787023198
21 0.0632175364558109
22 0.425890066866375
23 0.129576388534439
24 0.434428059119178
25 0.166373082777172
26 0.500324747168639
27 -0.181757008467706
28 0.111908426500126
29 -0.271053619415509
30 -0.167744715393572
31 -0.0338728333227656
32 0.333625139545928
33 0.0140758980245519
34 0.357007127635719
35 0.117528321727847
36 0.436606929603184
37 -0.281483140210634
38 0.0207127868483681
39 -0.347869271054934
40 -0.173998579595848
41 -0.0799206011874377
42 0.277044216890528
43 -0.114919716596858
44 0.284979270819777
45 -0.0204983858654412
46 0.404494915834395
47 -0.293744301550975
48 0.127254025577103
49 -0.260160101188769
50 -0.0875216627615043
51 -0.0462217977187355
52 0.397719199372734
53 -0.0591056840364732
54 0.367738474710558
55 -0.0152206137909984
56 0.353167727849039
57 -0.312136059481424
58 0.143633095475107
59 -0.171180250675521
60 -0.0465141674925802
61 0.0899046896169316
62 0.598910847300957
63 0.0475490561242251
64 0.358240966214597
65 -0.115970806757625
66 0.322290982770865
67 -0.286140052945316
68 0.0618578078197809
69 -0.212161273189703
70 -0.0454341579685268
71 0.0105666268253198
72 0.581041116690388
73 0.0693941384243427
74 0.261971378607031
75 -0.123479577576325
76 0.367590419177466
77 -0.141235430279281
78 0.116237028180501
79 -0.198695126295741
80 -0.0657961678537194
81 -0.0135147919675166
82 0.64984396453257
83 -0.0167548399541178
84 0.227526983440523
85 -0.299388449116629
86 0.416617177307962
87 -0.203407983560365
88 -0.0023010455436364
89 -0.282560660653303
90 -0.176413766464725
91 -0.0983101390414709
92 0.676705573655853
93 0.0203031107706873
94 0.312294913410111
95 -0.165489563226253
96 0.495102364234008
97 -0.305931623403696
98 0.0254406084608136
99 -0.225414817884392
100 -0.15036962703039
101 -0.112423279517229
102 0.745007684199088
103 0.0255308466866797
104 0.315917556432058
105 -0.303111962111323
106 0.4422161899518
107 -0.369563414156803
108 0.0535188548883159
109 -0.164608441001884
110 -0.156343742785993
111 -0.207438206198236
112 0.689341282064895
113 -0.00226758654004458
114 0.288620788540252
115 -0.400804924124587
116 0.359566102139385
117 -0.321517807983831
118 0.00695988179093469
119 -0.231063555270884
120 -0.0754597599841716
121 -0.217474888732617
122 0.683362779387654
123 -0.00314961139535584
124 0.30330965844177
125 -0.345238161601586
126 0.360311898301801
127 -0.301677126208083
128 0.0752078039771654
129 -0.155192129731062
130 -0.061593861998895
131 -0.219884190719033
132 0.628023504848516
133 -0.0382384606055213
134 0.297499086480709
135 -0.346531677222434
136 0.327798639134225
137 -0.391356801473802
138 0.0491703726922144
139 -0.249887313782597
140 -0.114655677012198
141 -0.220433378749168
142 0.560059582446338
143 -0.196920702930255
144 0.219878996139467
145 -0.373914844022043
146 0.292758745532306
147 -0.471257747874178
148 0.101291169049118
149 -0.177115964022913
150 -0.132451723823155
151 -0.21848909838528
152 0.514021082036989
153 -0.211628524400154
154 0.249156930945686
155 -0.277957223804047
156 0.292060580594635
157 -0.411605289128537
158 0.15610957238629
159 -0.167147715462073
160 -0.116581851584324
161 -0.234964260449157
162 0.526997072058525
163 -0.234040498456114
164 0.193292408158503
165 -0.286173628961541
166 0.329912374179291
167 -0.439242525292821
168 0.12873648130915
169 -0.211483057184516
170 -0.160118716712957
171 -0.168318618554501
172 0.553840105722893
173 -0.184533155434883
174 0.134661753730845
175 -0.264620194457709
176 0.378708889591781
177 -0.400340933892769
178 0.105012175958662
179 -0.28484391241728
180 -0.200352172901001
181 -0.217957678983035
182 0.524658753398018
183 -0.287612226565405
184 0.0883221554409499
185 -0.341060900924494
186 0.263243703023783
187 -0.430524950362145
188 0.073447743960423
189 -0.360135307960038
190 -0.245117659025775
191 -0.224060777472356
192 0.558292040382389
193 -0.232121189219085
194 0.198573003982196
195 -0.328853987308007
196 0.236404318927077
197 -0.400985244515162
198 0.0974478031384133
199 -0.382761496314351
200 -0.169842414452298
201 -0.126470090270059
202 0.56566108701561
203 -0.261072841655357
204 0.116756010565968
205 -0.348463293841007
206 0.248673973810354
207 -0.30726639828461
208 0.164793650388641
209 -0.405712873570711
210 -0.14634664930125
211 -0.189300141404592
212 0.497447286812312
213 -0.40434597513608
214 0.0310354180559974
215 -0.417353746807388
216 0.168938560946504
217 -0.316782981470489
218 0.181235173313896
219 -0.386664119093966
220 -0.179256321862778
221 -0.213160895671369
222 0.417867789679067
223 -0.363034274119974
224 0.0658399806102965
225 -0.309143911660818
226 0.263322896383933

From bgunter.4567 at gmail.com  Fri Jul 10 19:30:41 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 10 Jul 2015 10:30:41 -0700
Subject: [R] (no subject)
In-Reply-To: <CAPHYR5u10WT36nsc2uh7jkR0esVh_AMTYrr8uYt5oXL2evA-fg@mail.gmail.com>
References: <CAPHYR5u10WT36nsc2uh7jkR0esVh_AMTYrr8uYt5oXL2evA-fg@mail.gmail.com>
Message-ID: <CAGxFJbTxOigV95-vrWywXPzpWx-K==zgen9LX2zMW1iUCwwvWw@mail.gmail.com>

Sorry Lia. We don't do homework on this list. You can imagine how
clogged it would be if we did.

Maybe you'll get lucky and someone will answer you privately.
Otherwise, seek help from your teachers, fellow students, and/or
course homework forums.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Jul 10, 2015 at 2:17 AM, Lia LEE <ljylia.0101 at gmail.com> wrote:
> Hello forum members,
>
> I am taking this R course which I have to admit that it is beyond my
> capability. I am asked to analyze the following data(file attached) according
> to the question provided below:
>
> 1. The file [data_13-9.txt] contains a data of length 225, which seems to
> have some cycle pattern.
> (1) Fit a suitable model for this data, then report the model equation.
> (2) Provide the forecast values for 3-steps ahead and 6-steps ahead.
>
> I am desperately seeking for your help, since I really don't know where to
> start.. I have done "plot.ts(data.13-6)", but not further. Above question
> is just part of the whole task, but I believe solving this would be very
> much helpful for me to jump in to the other questions. It would be easier
> for me to follow if your answer is in R script..
> I appreciate your patience and help.
> Thank you so much in advance!
>
> Cheers,
> Lia
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Fri Jul 10 19:39:58 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 10 Jul 2015 10:39:58 -0700
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <CAM+rpY=tHRRBAG+tv56q8qhB20A_1rcOGr3dXhA=DFDBJug_-Q@mail.gmail.com>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
	<7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>
	<CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>
	<CAM+rpY=ucmtVQREyhcgoXzc7v5X+kWaej4DiOTbeZNajnB-CSg@mail.gmail.com>
	<003b01d0ba7c$cf953e80$6ebfbb80$@mcmaster.ca>
	<CAM+rpY=tHRRBAG+tv56q8qhB20A_1rcOGr3dXhA=DFDBJug_-Q@mail.gmail.com>
Message-ID: <CAGxFJbT=6xK-MAes=+2mXp7DPq4jzMQwqfmSj-RNN6T7UTbE0A@mail.gmail.com>

Yes. This is one of the fundamental challenges in text searching --
defining exactly what text defines a match and what doesn't. So,
continuing your example, one might imagine that heroin and heroine
might both be matches, but maybe heroines shouldn't be (e.g. if the
text contains movie reviews). So what one might want to do is add
semantic analysis to searches, ? la google, a topic way beyond the
simple capabilities discussed, or likely needed, here.

Incidentally, Jeff Newmiller's (final) regular expression solution is
preferable to mine in all respects, I think.

-- Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Jul 10, 2015 at 10:30 AM, Christopher W Ryan
<cryan at binghamton.edu> wrote:
> Interesting thoughts about the partial-word matches, and speed  On
> another real data set, about 73,000 records and 6 columns to search
> through for matches (one column of which contains very long character
> strings--several paragraphs each), I ran both John's and Bert's
> solutions.  John's was noticeably slower, although still quite
> tolerable.  There were a different number of matches, though:
>
>       oic.2
> oic          FALSE    TRUE     Sum
>   FALSE 74939         0        74939
>   TRUE    274           927     1201
>   Sum     75213        927     76140
>
> where oic is the logical vector generated by John's solution, and
> oic.2 is the logical vector generated by Bert's solution. Bert's
> solution detected about 77% of the cases detected by John's.
>
> I'm still exploring why that might be. One possible explanation, for
> at least part of the difference, is the issue of partial-word matches.
> Substantively, I am searching ambulance run records for words related
> to opioid overdose, and I've noticed that the medics often spell
> heroin as "heroine"  So in this context, I like partial-word
> matches--I want to pick up records that (partially) match "heroin"
> because it is contained in the word "heroine" .
>
> There may be other things going on too.
>
> Thanks.
>
> --Chris
>
> On Thu, Jul 9, 2015 at 3:24 PM, John Fox <jfox at mcmaster.ca> wrote:
>> Dear Christopher,
>>
>> My usual orientation to this kind of one-off problem is that I'm looking for a simple correct solution. Computing time is usually much smaller than programming time.
>>
>> That said, Bert Gunter's solution was about 5 times faster in a simple check that I ran with microbenchmark, and Jeff Newmiller's solution was about 10 times faster. Both Bert's and Jeff's (eventual) solution protect against partial (rather than full-word) matches, while mine doesn't (though it could easily be modified to do that).
>>
>> Best,
>>  John
>>
>>> -----Original Message-----
>>> From: Christopher W Ryan [mailto:cryan at binghamton.edu]
>>> Sent: July-09-15 2:49 PM
>>> To: Bert Gunter
>>> Cc: Jeff Newmiller; R Help; John Fox
>>> Subject: Re: [R] detecting any element in a vector of strings, appearing
>>> anywhere in any of several character variables in a dataframe
>>>
>>> Thanks everyone.  John's original solution worked great.  And with
>>> 27,000 records, 65 alarm.words, and 6 columns to search, it takes only
>>> about 15 seconds.  That is certainly adequate for my needs.  But I
>>> will try out the other strategies too.
>>>
>>> And thanks also for lot's of new R things to learn--grep, grepl,
>>> do.call . . . that's always a bonus!
>>>
>>> --Chris Ryan
>>>
>>> On Thu, Jul 9, 2015 at 1:52 PM, Bert Gunter <bgunter.4567 at gmail.com>
>>> wrote:
>>> > Yup, that does it. Let grep figure out what's a word rather than doing
>>> > it manually. Forgot about "\b"
>>> >
>>> > Cheers,
>>> > Bert
>>> >
>>> >
>>> > Bert Gunter
>>> >
>>> > "Data is not information. Information is not knowledge. And knowledge
>>> > is certainly not wisdom."
>>> >    -- Clifford Stoll
>>> >
>>> >
>>> > On Thu, Jul 9, 2015 at 10:30 AM, Jeff Newmiller
>>> > <jdnewmil at dcn.davis.ca.us> wrote:
>>> >> Just add a word break marker before and after:
>>> >>
>>> >> zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ),
>>> ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
>>> >> ---------------------------------------------------------------------
>>> ------
>>> >> Jeff Newmiller                        The     .....       .....  Go
>>> Live...
>>> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>> Go...
>>> >>                                       Live:   OO#.. Dead: OO#..
>>> Playing
>>> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>> >> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> rocks...1k
>>> >> ---------------------------------------------------------------------
>>> ------
>>> >> Sent from my phone. Please excuse my brevity.
>>> >>
>>> >> On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>>> wrote:
>>> >>>Jeff:
>>> >>>
>>> >>>Well, it would be much better (no loops!) except, I think, for one
>>> >>>issue: "red" would match "barred" and I don't think that this is what
>>> >>>is wanted: the matches should be on whole "words" not just string
>>> >>>patterns.
>>> >>>
>>> >>>So you would need to fix up the matching pattern to make this work,
>>> >>>but it may be a little tricky, as arbitrary whitespace characters,
>>> >>>e.g. " " or "\n" etc. could be in the strings to be matched
>>> separating
>>> >>>the words or ending the "sentence."  I'm sure it can be done, but
>>> I'll
>>> >>>leave it to you or others to figure it out.
>>> >>>
>>> >>>Of course, if my diagnosis is wrong or silly, please point this out.
>>> >>>
>>> >>>Cheers,
>>> >>>Bert
>>> >>>
>>> >>>
>>> >>>Bert Gunter
>>> >>>
>>> >>>"Data is not information. Information is not knowledge. And knowledge
>>> >>>is certainly not wisdom."
>>> >>>   -- Clifford Stoll
>>> >>>
>>> >>>
>>> >>>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
>>> >>><jdnewmil at dcn.davis.ca.us> wrote:
>>> >>>> I think grep is better suited to this:
>>> >>>>
>>> >>>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call(
>>> paste,
>>> >>>zz[ , 2:3 ] ) ) )
>>> >>>>
>>> >>>---------------------------------------------------------------------
>>> ------
>>> >>>> Jeff Newmiller                        The     .....       .....  Go
>>> >>>Live...
>>> >>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
>>> Live
>>> >>>Go...
>>> >>>>                                       Live:   OO#.. Dead: OO#..
>>> >>>Playing
>>> >>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
>>> with
>>> >>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> >>>rocks...1k
>>> >>>>
>>> >>>---------------------------------------------------------------------
>>> ------
>>> >>>> Sent from my phone. Please excuse my brevity.
>>> >>>>
>>> >>>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter
>>> <bgunter.4567 at gmail.com>
>>> >>>wrote:
>>> >>>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only
>>> a
>>> >>>>>single, not a double, loop. It should be more efficient.
>>> >>>>>
>>> >>>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>>> >>>>>+       function(x)any(x %in% alarm.words))
>>> >>>>>
>>> >>>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>>> >>>>>
>>> >>>>>The idea is to paste the strings in each row (do.call allows an
>>> >>>>>arbitrary number of columns) into a single string and then use
>>> >>>>>strsplit to break the string into individual "words" on whitespace.
>>> >>>>>Then the matching is vectorized with the any( %in% ... ) call.
>>> >>>>>
>>> >>>>>Cheers,
>>> >>>>>Bert
>>> >>>>>Bert Gunter
>>> >>>>>
>>> >>>>>"Data is not information. Information is not knowledge. And
>>> knowledge
>>> >>>>>is certainly not wisdom."
>>> >>>>>   -- Clifford Stoll
>>> >>>>>
>>> >>>>>
>>> >>>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>>> >>>>>> Dear Chris,
>>> >>>>>>
>>> >>>>>> If I understand correctly what you want, how about the following?
>>> >>>>>>
>>> >>>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>>> >>>>>grepl, x=x)))
>>> >>>>>>> zz[rows, ]
>>> >>>>>>
>>> >>>>>>           v1                              v2                v3 v4
>>> >>>>>> 3  -1.022329                    green turtle    ronald weasley  2
>>> >>>>>> 6   0.336599              waffle the hamster        red sparks  1
>>> >>>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>>> >>>>>> 10  1.130622                      black bear  gandalf the grey  2
>>> >>>>>>
>>> >>>>>> I hope this helps,
>>> >>>>>>  John
>>> >>>>>>
>>> >>>>>> ------------------------------------------------
>>> >>>>>> John Fox, Professor
>>> >>>>>> McMaster University
>>> >>>>>> Hamilton, Ontario, Canada
>>> >>>>>> http://socserv.mcmaster.ca/jfox/
>>> >>>>>>
>>> >>>>>>
>>> >>>>>> On Wed, 08 Jul 2015 22:23:37 -0400
>>> >>>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>> >>>>>>> Running R 3.1.1 on windows 7
>>> >>>>>>>
>>> >>>>>>> I want to identify as a case any record in a dataframe that
>>> >>>contains
>>> >>>>>any
>>> >>>>>>> of several keywords in any of several variables.
>>> >>>>>>>
>>> >>>>>>> Example:
>>> >>>>>>>
>>> >>>>>>> # create a dataframe with 4 variables and 10 records
>>> >>>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>>> >>>>>fox",
>>> >>>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>>> >>>>>"hello
>>> >>>>>>> world", "yellow giraffe with a long neck", "black bear")
>>> >>>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>>> >>>>>"ginny
>>> >>>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>>> >>>>>dress
>>> >>>>>>> robes", "gandalf the white", "gandalf the grey")
>>> >>>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
>>> >>>lambda=2),
>>> >>>>>>> stringsAsFactors=FALSE)
>>> >>>>>>> str(zz)
>>> >>>>>>> zz
>>> >>>>>>>
>>> >>>>>>> # here are the keywords
>>> >>>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>> >>>>>>>
>>> >>>>>>> # For each row/record, I want to test whether the string in v2
>>> or
>>> >>>>>the
>>> >>>>>>> string in v3 contains any of the strings in alarm.words. And
>>> then
>>> >>>if
>>> >>>>>so,
>>> >>>>>>> set zz$v5=TRUE for that record.
>>> >>>>>>>
>>> >>>>>>> # I'm thinking the str_detect function in the stringr package
>>> >>>ought
>>> >>>>>to
>>> >>>>>>> be able to help, perhaps with some use of apply over the rows,
>>> but
>>> >>>I
>>> >>>>>>> obviously misunderstand something about how str_detect works
>>> >>>>>>>
>>> >>>>>>> library(stringr)
>>> >>>>>>>
>>> >>>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>>> >>>>>search
>>> >>>>>>>                                      # must be a vector, not
>>> >>>>>multiple
>>> >>>>>>>                                      # columns
>>> >>>>>>>
>>> >>>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>> >>>>>>>
>>> >>>>>>> str_detect(zz[,2], alarm.words)      # error, length of
>>> >>>alarm.words
>>> >>>>>>>                                      # is less than the number
>>> of
>>> >>>>>>>                                      # rows I am using for the
>>> >>>>>>>                                      # comparison
>>> >>>>>>>
>>> >>>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>> >>>>>>> length(alarm.words)                  # confining nrows
>>> >>>>>>>                                      # to the length of
>>> >>>alarm.words
>>> >>>>>>>
>>> >>>>>>> str_detect(zz, alarm.words)          # obviously not right
>>> >>>>>>>
>>> >>>>>>> # maybe I need apply() ?
>>> >>>>>>> my.f <- function(x){str_detect(x, alarm.words)}
>>> >>>>>>>
>>> >>>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>> >>>>>>>                            # between alarm.words and that
>>> >>>>>>>                            # in which I am searching for
>>> >>>>>>>                            # matching strings
>>> >>>>>>>
>>> >>>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>> >>>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>> >>>>>>>                            # rows of the dataframe
>>> >>>>>>>
>>> >>>>>>>
>>> >>>>>>> # perhaps %in% could do the job?
>>> >>>>>>>
>>> >>>>>>> Appreciate any advice.
>>> >>>>>>>
>>> >>>>>>> --Chris Ryan
>>> >>>>>>>
>>> >>>>>>> ______________________________________________
>>> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>>>>>> PLEASE do read the posting guide
>>> >>>>>http://www.R-project.org/posting-guide.html
>>> >>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>> >>>>>>
>>> >>>>>> ______________________________________________
>>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>>>>> PLEASE do read the posting guide
>>> >>>>>http://www.R-project.org/posting-guide.html
>>> >>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>> >>>>>
>>> >>>>>______________________________________________
>>> >>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>>>>PLEASE do read the posting guide
>>> >>>>>http://www.R-project.org/posting-guide.html
>>> >>>>>and provide commented, minimal, self-contained, reproducible code.
>>> >>>>
>>> >>
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>> ---
>> This email has been checked for viruses by Avast antivirus software.
>> https://www.avast.com/antivirus
>>


From bgunter.4567 at gmail.com  Fri Jul 10 19:50:06 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 10 Jul 2015 10:50:06 -0700
Subject: [R] How to assign value to a variable dynamically constructed
In-Reply-To: <44a899b0-dc83-4dfd-99d5-f6dc91d17474@googlegroups.com>
References: <CAMCXXmroJCA8WZ0WcZRg-RtOL2yNM1Rbw_s+SrdfsF1VSw0Jyg@mail.gmail.com>
	<CAFEqCdyF82JcOQ7p4UmaH7S1wS81M7tBPuPT1sz9Loqck2jJdA@mail.gmail.com>
	<CAF8bMcZyBF1RgVvC=M_iU+QToBK0_FuDozTw7w0qk70WZ4BLaA@mail.gmail.com>
	<44a899b0-dc83-4dfd-99d5-f6dc91d17474@googlegroups.com>
Message-ID: <CAGxFJbSu5b3LZfS12mE+NgGDqD=rP2pXMHw9StpK1qpSb-AzpQ@mail.gmail.com>

I'll let Bill respond in detail if he cares to(he is both more
knowledgable and fluent at this than I), but as a nearly unbreakable
rule, get() and assign() should not be used in R. Basically, they
represent a macro (script)-oriented strategy for handling R's objects,
whereas R is designed to use an object-oriented ("everything is an
object") and functional ("all procedures are functions") approach.
Using get() and assign() leads to messy, confusing, error-prone,
non-portable code in R, and that's why they should be avoided.

For details, you should consult web tutorials, John Chambers's books,
and other books on R programming.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Jul 10, 2015 at 9:44 AM, Bastien Tran <bastien.tran at gmail.com> wrote:
> Dear all,
>
> Provided I understood correctly, shouldn't assign() do the trick? Most similar threads seem to include this approach (among others, indeed).
>
> Regards,
> Bastien
>
>
> On Wednesday, July 8, 2015 at 7:30:04 PM UTC+2, William Dunlap wrote:
>> You can use an environment instead of a list using the same [[ syntax.  It
>> is like 'get0(..., inherit=FALSE)' on the left side of the <- and like
>> 'assign(...)' on the right side.   E.g.,
>>    myData <- new.env()
>>    varName <- "v1"
>>    myData[[varName]] <- 1:10
>>    myData[[varName]][4] <- myData[[varName]][4] * 100
>>    myData[[varName]]
>>    #  [1]   1   2   3 400   5   6   7   8   9  10
>>    names(myData)
>>    # [1] "v1"
>> (Before R-3.2.0 or so, you had to use objects(myData,all=TRUE) if
>> myData was an environment and names(myData) if it was a list.  Now
>> names() works for environments.)
>>
>> It is better to use a dedicated environment (or list) for each set of
>> related
>> variables so that name collisions do not cause problems.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Wed, Jul 8, 2015 at 10:06 AM, Greg Snow <538280 at gmail.com> wrote:
>>
>> > This is FAQ 7.21.
>> >
>> > The most important part of the answer in FAQ 7.21 is the last section
>> > where it states that it is often easier to use a list rather than
>> > messing around with trying to dynamically name global variables.
>> >
>> > If you tell us what you are trying to accomplish then we may have
>> > better advice.  The route you are headed down now usually leads to
>> > inefficient code and hard to find bugs.
>> >
>> > On Tue, Jul 7, 2015 at 2:53 PM, Jun Shen <jun.shen.ut at gmail.com> wrote:
>> > > Dear list,
>> > >
>> > > Let's say we have a variable (id), whose name is dynamically constructed.
>> > > This variable represents a vector or data frame with many elements. Now I
>> > > want to specifically assign a value to one of the elements. I couldn't
>> > get
>> > > it right.
>> > >
>> > > test <- 'id' # "id" is dynamically constructed through paste()
>> > >
>> > > id <- 1:4
>> > >
>> > > # I can get the element by doing
>> > >
>> > > get(test)[2]
>> > >
>> > > # Now I want to assign a value to the second element of this dynamical
>> > > variable.
>> > >
>> > > get(test)[2] <- 5  # doesn't work.
>> > >
>> > > Thanks a lot.
>> > >
>> > > Jun Shen
>> > >
>> > >         [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>> >
>> > --
>> > Gregory (Greg) L. Snow Ph.D.
>> > 538280 at gmail.com
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> >
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mccormack at molbio.mgh.harvard.edu  Fri Jul 10 19:51:48 2015
From: mccormack at molbio.mgh.harvard.edu (Matthew)
Date: Fri, 10 Jul 2015 13:51:48 -0400
Subject: [R] tcltk2 entry box
In-Reply-To: <CAFEqCdwOnK3gD5WjJRCA6UN3TyiKOWfyiyM3rmWjQUOZ3jM+hw@mail.gmail.com>
References: <559DB9A7.40202@molbio.mgh.harvard.edu>	<004f01d0b9df$751d89a0$5f589ce0$@mcmaster.ca>	<559DD555.2020506@molbio.mgh.harvard.edu>
	<CAFEqCdwOnK3gD5WjJRCA6UN3TyiKOWfyiyM3rmWjQUOZ3jM+hw@mail.gmail.com>
Message-ID: <55A00634.2010501@molbio.mgh.harvard.edu>

Thank you very much, Greg, for the tkwait commands.

  I am just starting to try out examples on the sciviews web page to get 
a feel for tcltk in R and the tkwait.variable and tkwait.window seem 
like they could be very useful to me. I will add these in to my practice 
scripts and see what I can do with them.

Matthew

On 7/9/2015 5:31 PM, Greg Snow wrote:
> If you want you script to wait until you have a value entered then you
> can use the tkwait.variable or tkwait.window commands to make the
> script wait before continuing (or you can bind the code to a button so
> that you enter the value, then click on the button to run the code).
>
> On Wed, Jul 8, 2015 at 7:58 PM, Matthew McCormack
> <mccormack at molbio.mgh.harvard.edu> wrote:
>> Wow !  Very nice.  Thank you very much, John.  This is very helpful and just
>> what I need.
>> Yes, I can see that I should have paid attention to tcltk before going to
>> tcltk2.
>>
>> Matthew
>>
>>
>> On 7/8/2015 8:37 PM, John Fox wrote:
>>> Dear Matthew,
>>>
>>> For file selection, see ?tcltk::tk_choose.files or ?tcltk::tkgetOpenFile .
>>>
>>> You could enter a number in a tk entry widget, but, depending upon the
>>> nature of the number, a slider or other widget might be a better choice.
>>>
>>> For a variety of helpful tcltk examples see
>>> <http://www.sciviews.org/_rgui/tcltk/>, originally by James Wettenhall but
>>> now maintained by Philippe Grosjean (the author of the tcltk2 package).
>>> (You
>>> probably don't need tcltk2 for the simple operations that you mention, but
>>> see ?tk2spinbox for an alternative to a slider.)
>>>
>>> Best,
>>>    John
>>>
>>> -----------------------------------------------
>>> John Fox, Professor
>>> McMaster University
>>> Hamilton, Ontario, Canada
>>> http://socserv.socsci.mcmaster.ca/jfox/
>>>
>>>
>>>
>>>
>>>> -----Original Message-----
>>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matthew
>>>> Sent: July-08-15 8:01 PM
>>>> To: r-help
>>>> Subject: [R] tcltk2 entry box
>>>>
>>>> Is anyone familiar enough with the tcltk2 package to know if it is
>>>> possible to have an entry box where a user can enter information (such
>>>> as a path to a file or a number) and then be able to use the entered
>>>> information downstream in a R script ?
>>>>
>>>> The idea is for someone unfamiliar with R to just start an R script that
>>>> would take care of all the commands for them so all they have to do is
>>>> get the script started. However, there is always a couple of pieces of
>>>> information that will change each time the script is used (for example,
>>>> a different file will be processed by the script). So, I would like a
>>>> way for the user to input that information as the script ran.
>>>>
>>>> Matthew McCormack
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>> guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ---
>>> This email has been checked for viruses by Avast antivirus software.
>>> https://www.avast.com/antivirus
>>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>


From clint at ecy.wa.gov  Fri Jul 10 20:08:47 2015
From: clint at ecy.wa.gov (Clint Bowman)
Date: Fri, 10 Jul 2015 11:08:47 -0700 (PDT)
Subject: [R] Reading the non delimited file with no particular patterns
 in the data to R
In-Reply-To: <1436532925238-4709699.post@n4.nabble.com>
References: <1433839253539-4708379.post@n4.nabble.com>
	<1436532925238-4709699.post@n4.nabble.com>
Message-ID: <alpine.LRH.2.11.1507101108200.4043@aeolus.ecy.wa.gov>

Is this what your are expecting?

> test <- read.fwf("test.dat", width = c(10, 4, 1, 4, 9, 
12,26,1,1,1,1,1,1,3,8))
> test
   V1   V2 V3 V4       V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15
1  1 2015  1  4 0.766696  1  1  0  0   0   0   0   0   0  10
2  2 2015  1  4 1.458186  1  1  0  0   0   1   0   0   0  20
3  3 2015  1  4 0.185492  1  1  0  0   0   0   0   0   0  15
4  4 2015  1  4 0.961584  1  1  0  0   0   0   0   0   0   3
5  5 2015  1  4 0.650091  2  0  0  0   1   0   0   0   0  NA
6  6 2015  1  4 0.430350  1  1  0  0   0   0   0   0   0  20
7  7 2015  1  4 3.192895  2  1  0  1   1   0   0   0   0   0
8  8 2015  1  4 0.617127  1  1  0  1   0   1   0   0   0  15
9  9 2015  1  4 0.399207  1  1  0  0   0   0   0   0   0  10

Clint

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600

         USPS:           PO Box 47600, Olympia, WA 98504-7600
         Parcels:        300 Desmond Drive, Lacey, WA 98503-1274

On Fri, 10 Jul 2015, jagadishpchary wrote:

> I am beginner in R and I want to read a ASCII file to R environment. However,
> the ASCII file is a non delimited and the data is not continuous (have some
> blank spaces between the variables) so in order to read the data i have used
> the below syntax i.e
> test <- read.fwf("D:/R_process/ASCII.txt", width = c(10, 4, 1, 4, 9, 9,
> 1,1,1,1,1,1,1,3,8))
>
> Now i am able to read it but the data read is wrong. Actually my out put
> should have only the applicable variables data but not the blank data.
> Attached is the ASCII data. Please let me know how should i write the syntax
> to read only the applicable data in the file.
>
> Thanks for your help in advance. test.txt
> <http://r.789695.n4.nabble.com/file/n4709699/test.txt>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Cross-tabulation-with-top-one-variable-and-side-as-multiple-variables-tp4708379p4709699.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch.duncan at gmail.com  Fri Jul 10 20:44:55 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 10 Jul 2015 14:44:55 -0400
Subject: [R]
 =?windows-1252?q?how_to_embed_a_3D_plot_created_by_=93rgl=94_?=
 =?windows-1252?q?into_=22gWidgets_ggraphics_device=93=3F?=
In-Reply-To: <tencent_2610C29B02B3E5313D99F015@qq.com>
References: <tencent_2610C29B02B3E5313D99F015@qq.com>
Message-ID: <55A012A7.7090805@gmail.com>

On 10/07/2015 8:43 AM, just_rookie wrote:
> I created some 3D cubes by using "rgl" package. The next step is to embed the 3D plot("rgl" device) into "ggraphics" device from the "gWidgets" package, but I have no idea how to implement it. Is there a way to implement by "gWidgets" or other package? Any help? Thank you in advance!

Is there an existing widget that displays HTML, and can display WebGL?
Then it's probably easiest to use rgl::writeWebGL to create the
Javascript code, and that other widget to display it.

If you want it displayed using native OpenGL code, it's likely a lot
more work.  On Windows it's probably easiest; I don't know how hard it
would be to do on other platforms.  I would not personally spend any
time on it.

Duncan Murdoch

> My code:
> library(rgl)
> library(scatterplot3d)
> library(gWidgets)
> options(guiToolkit="RGtk2")
> 
> 
> df<-data.frame(x=c(1,2,3,4),
>                y=c(2,4,6,8),
>                z=c(3,6,9,12),
>                value=c(33,43,75,21))
> 
> clr <- df$value/max(df$value)
> f <- colorRamp(c("green", "yellow", "purple", "red"))
> 
> ## 3D PLOT
> for(i in 1:length(df$x)){
>   shade3d(translate3d(scale3d(cube3d(col=rgb(f(clr[i])/255),alpha=0.15 ),
>                                      10.0, 10.0, 2.5),df$x[i],df$y[i],df$z[i]))
> } 
> 
> window <- gwindow("TEST", width = 800, height= 600, visible = FALSE)
> group <- ggroup(cont = window, expand = TRUE)
> 
> loadBtn <- gbutton("display", cont = group)
> 
> addHandlerChanged(loadBtn, handler = function(h,...){
>   sudwin<-gwindow("3D",visible = TRUE)
>   dev <- ggraphics(cont=subwin)
>   #### DO NOT KNOW HOW TO DO NEXT...
> })
> visible(window)<-TRUE
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From rshepard at appl-ecosys.com  Fri Jul 10 23:09:45 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 10 Jul 2015 14:09:45 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
Message-ID: <alpine.LNX.2.11.1507101404490.10467@localhost>

   Hadley's ggplot2 book is quite old and a new version is in the works, but
not yet out. I've been using lattice graphics but the knitr package doesn't
support lattice, only basic plots and ggplot2. My Web searches for Trellis
plots in ggplot2 equivalent to those in lattice have not been productive.

   I would appreciate a pointer to a resource that would teach me how to
translate from lattice xyplot() to ggplot2 ggplot().

   This is one such plot needing translation:

xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T)

Rich


From roy.mendelssohn at noaa.gov  Fri Jul 10 23:16:20 2015
From: roy.mendelssohn at noaa.gov (Roy Mendelssohn - NOAA Federal)
Date: Fri, 10 Jul 2015 14:16:20 -0700
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101404490.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
Message-ID: <F98F75BF-E0E7-4918-8B5E-AD62E2ED0BB3@noaa.gov>

Don?t know for certain but might this help:

http://journal.r-project.org/archive/2015-1/murrell.pdf

From the latest issue of R Journal.  

Abstract The gridGraphics package provides a function, grid.echo(), that can be used to convert a plot drawn with the graphics package to a visually identical plot drawn using grid. This conversion provides access to a variety of grid tools for making customisations and additions to the plot that are not possible with the graphics package.

-Roy

> On Jul 10, 2015, at 2:09 PM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> 
>  Hadley's ggplot2 book is quite old and a new version is in the works, but
> not yet out. I've been using lattice graphics but the knitr package doesn't
> support lattice, only basic plots and ggplot2. My Web searches for Trellis
> plots in ggplot2 equivalent to those in lattice have not been productive.
> 
>  I would appreciate a pointer to a resource that would teach me how to
> translate from lattice xyplot() to ggplot2 ggplot().
> 
>  This is one such plot needing translation:
> 
> xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T)
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new address and phone***
110 Shaffer Road
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From rshepard at appl-ecosys.com  Fri Jul 10 23:20:23 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 10 Jul 2015 14:20:23 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <F98F75BF-E0E7-4918-8B5E-AD62E2ED0BB3@noaa.gov>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<F98F75BF-E0E7-4918-8B5E-AD62E2ED0BB3@noaa.gov>
Message-ID: <alpine.LNX.2.11.1507101419120.10467@localhost>

On Fri, 10 Jul 2015, Roy Mendelssohn - NOAA Federal wrote:

> Don?t know for certain but might this help:
> http://journal.r-project.org/archive/2015-1/murrell.pdf
> From the latest issue of R Journal.

Roy,

   Thanks. I'll certainly read that article.

Carpe weekend,

Rich


From rshepard at appl-ecosys.com  Fri Jul 10 23:38:40 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 10 Jul 2015 14:38:40 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <F98F75BF-E0E7-4918-8B5E-AD62E2ED0BB3@noaa.gov>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<F98F75BF-E0E7-4918-8B5E-AD62E2ED0BB3@noaa.gov>
Message-ID: <alpine.LNX.2.11.1507101425040.10467@localhost>

On Fri, 10 Jul 2015, Roy Mendelssohn - NOAA Federal wrote:

> Don?t know for certain but might this help:

   It's very interesting, but does not appear to resolve the immediate need
to write the R code in a knitr chunk for incorporation into the compiled LyX
document.

   The gridGraphics package description notes that both lattice and ggplot2
are built on the grid framework, and the new gridGraphics package will
'echo' either to a grid format for further tweaking. Unless the knitr
package will accept the echoed image ...

   It does not. Here is what the compiled document shows (the R code is
included in the document):

pdf('carlin-1-description.pdf')
xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T)
require(gridGraphics)

## Loading required package:  gridGraphics
## Loading required package:  grid

grid.echo()

## Error in grid.echo.recordedplot(recordPlot(), newpage, prefix): No
## graphics to replay

dev.off()

Rich


From h.wickham at gmail.com  Fri Jul 10 23:50:53 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Fri, 10 Jul 2015 14:50:53 -0700
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101404490.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
Message-ID: <CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>

Have you tried explicitly print()ing the lattice graphics in your knitr doc?

Hadley

On Friday, July 10, 2015, Rich Shepard <rshepard at appl-ecosys.com> wrote:

>   Hadley's ggplot2 book is quite old and a new version is in the works, but
> not yet out. I've been using lattice graphics but the knitr package doesn't
> support lattice, only basic plots and ggplot2. My Web searches for Trellis
> plots in ggplot2 equivalent to those in lattice have not been productive.
>
>   I would appreciate a pointer to a resource that would teach me how to
> translate from lattice xyplot() to ggplot2 ggplot().
>
>   This is one such plot needing translation:
>
> xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T)
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/

	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Fri Jul 10 23:57:44 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 10 Jul 2015 14:57:44 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507101454360.10467@localhost>

On Fri, 10 Jul 2015, Hadley Wickham wrote:

> Have you tried explicitly print()ing the lattice graphics in your knitr
> doc?

Hadley,

   Only now. Had not thought of trying this before.

pdf('carlin-1-descriptive.pdf')
print(xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T))

No error messages, but no graphic, either. Without specifying the pdf
device, TeX complains it cannot find a graphics device and lists bit-mapped,
ps and svg devices.

   Most likely I do not have the correct syntax.

Thanks,

Rich


From jdnewmil at dcn.davis.CA.us  Fri Jul 10 23:58:19 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 10 Jul 2015 14:58:19 -0700
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101404490.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
Message-ID: <768C6D2F-BF80-4E15-AE77-26A3B02781E5@dcn.davis.CA.us>

I don't actually use lattice very much, but I have no difficulty setting up a lattice plot in a knitr/rmarkdown file, and can think of no reason why you might have concluded that knitr does not support lattice.

Sorry, not going to translate your non-reproducible example... please go the extra little effort to dput some example data so we can communicate clearly about your problem. [1]

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 10, 2015 2:09:45 PM PDT, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>Hadley's ggplot2 book is quite old and a new version is in the works,
>but
>not yet out. I've been using lattice graphics but the knitr package
>doesn't
>support lattice, only basic plots and ggplot2. My Web searches for
>Trellis
>plots in ggplot2 equivalent to those in lattice have not been
>productive.
>
>  I would appreciate a pointer to a resource that would teach me how to
>translate from lattice xyplot() to ggplot2 ggplot().
>
>   This is one such plot needing translation:
>
>xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T)
>
>Rich
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Sat Jul 11 00:21:45 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Fri, 10 Jul 2015 15:21:45 -0700
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101454360.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
Message-ID: <CABdHhvHwVAPDLS2ZRYMrwZGeFz5KhFczqLYnZb44D2EX90Yusw@mail.gmail.com>

You shouldn't be explicitly opening a device in a knitr document. I think
maybe you should post a minimal document so we can figure out what's going
wrong.

Hadley

On Friday, July 10, 2015, Rich Shepard <rshepard at appl-ecosys.com> wrote:

> On Fri, 10 Jul 2015, Hadley Wickham wrote:
>
>  Have you tried explicitly print()ing the lattice graphics in your knitr
>> doc?
>>
>
> Hadley,
>
>   Only now. Had not thought of trying this before.
>
> pdf('carlin-1-descriptive.pdf')
> print(xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T))
>
> No error messages, but no graphic, either. Without specifying the pdf
> device, TeX complains it cannot find a graphics device and lists
> bit-mapped,
> ps and svg devices.
>
>   Most likely I do not have the correct syntax.
>
> Thanks,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/

	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Sat Jul 11 00:39:08 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 10 Jul 2015 15:39:08 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <CABdHhvHwVAPDLS2ZRYMrwZGeFz5KhFczqLYnZb44D2EX90Yusw@mail.gmail.com>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<CABdHhvHwVAPDLS2ZRYMrwZGeFz5KhFczqLYnZb44D2EX90Yusw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507101535400.10467@localhost>

On Fri, 10 Jul 2015, Hadley Wickham wrote:

> You shouldn't be explicitly opening a device in a knitr document.

Hadley,

   Didn't think so.

> I think maybe you should post a minimal document so we can figure out
> what's going wrong.

   Agreed. Attached are the raw data (carlin.csv) and a stripped down LyX
document with the knitr chunks.

   This is my first attempt to use knitr; I'm reading the knitr book and
that's where I got the impression that lattice graphics are not supported.

Rich
-------------- next part --------------
siteid,sampdate,Temp.h2o,Temp.air,Disc.cfs,Turb,SC,DO,pH,ANC,HCO3,CO3,Alk,Hard,TDS,TSS,N.tot,N.org,NH4,NO3,NO2,PO4,C,Ca,Mg,Na,K,Cl,SO4,F,Si,As,Ba,Be,Cd,Cr,Co,Cu,Fe,Pb,Mn,Mo,Ni,Ag,Sr,V,Zi,Al,Li,Se,CN,Hg
10321000,1965-10-01,,,98,,490,,8.4,,238,4,,150,,,,,,,,,,40,12,46,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1965-11-01,,,141,,507,,8.3,213,252,4,,170,,,,,,,,,,50,11,42,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1965-12-01,,,128,,564,,8.1,238,290,0,,190,359,,,,,,,,,56,13,47,6.4,16,37,0.5,40,,,,,,,,,,,,,,,,,,,,,
10321000,1966-01-01,,,122,,506,,8.2,212,259,0,,170,321,,,,,,,,,50,11,45,6.1,16,34,0.6,31,,,,,,,,,,,,,,,,,,,,,
10321000,1966-01-17,,,97,,551,,8.5,241,276,9,,200,,,,,,,,,,58,13,48,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-02-01,,,108,,506,,8.2,216,263,0,,180,,,,,,,,,,52,11,42,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-03-01,,,384,,516,,7.7,201,245,0,,160,,,,,,,,,,46,11,46,8,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-03-18,,,701,,628,,8.1,235,286,0,,190,,,,,,,,,,51,16,61,8.5,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-04-01,,,612,,470,,8,189,231,0,,160,301,,,,,,,,,43,12,40,6.3,14,37,0.5,34,,,,,,,,,,,,,,,,,,,,,
10321000,1966-04-11,,,541,,423,,8,172,210,0,,140,,,,,,,,,,40,10,34,5.4,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-04-18,,,355,,466,,8.1,193,235,0,,160,,,,,,,,,,43,12,39,5.9,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-05-01,,,167,,498,,8,208,253,0,,170,,,,,,,,,,46,13,42,6.8,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-05-06,,,260,,420,,8.1,181,221,0,,140,,,,,,,,,,41,10,33,5.4,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-07-01,,,7,,566,,8,226,275,0,,180,450,,,,,,,,,47,16,52,96,21,48,0.6,34,,,,,,,,,,,,,,,,,,,,,
10321000,1966-08-01,,,3.9,,550,,8.1,210,256,0,,170,,,,,,,,,,40,17,51,9.7,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-09-01,,,4.1,,536,,8.4,208,254,0,,170,,,,,,,,,,41,17,48,8.9,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-10-01,,,7.7,,546,,8.4,220,260,4,,190,340,,,,,,,,,48,16,46,8.5,19,43,0.5,27,,,,,,,,,,,,,,,,,,,,,
10321000,1966-11-01,,,16,,536,,8.4,224,263,5,,180,,,,,,,,,,49,15,45,7.6,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1966-12-01,,,31,,533,,8.6,222,248,11,,180,336,,,,,,,,,50,13,47,7.5,14,37,0.5,34,,,,,,,,,,,,,,,,,,,,,
10321000,1966-12-16,,,30,,615,,8.6,264,296,13,,210,,,,,,,,,,60,14,56,8.7,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-01-01,,,43,,517,,8.8,214,228,16,,170,333,,,,,,,,,50,12,48,7.3,19,40,0.5,28,,,,,,,,,,,,,,,,,,,,,
10321000,1967-02-01,,,72,,513,,8.7,217,234,15,,180,,,,,,,,,,51,12,46,6.9,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-02-12,,,113,,474,,8.3,189,230,3,,160,,,,,,,,,,46,11,41,6.8,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-03-01,,,232,,464,,8.5,174,212,7,,150,,,,,,,,,,44,10,41,6.6,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-03-25,,,422,,498,,8.5,179,218,6,,160,,,,,,,,,,48,10,43,6.5,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-04-01,,,333,,448,,8.5,164,200,5,,150,283,,,,,,,,,42,11,39,5.7,15,40,0.5,26,,,,,,,,,,,,,,,,,,,,,
10321000,1967-05-01,,,327,,451,,8.5,165,201,6,,150,,,,,,,,,,43,10,38,5.8,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-05-10,,,406,,371,,8.3,127,155,2,,110,,,,,,,,,,29,9.5,34,5.7,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-05-14,,,396,,435,,8.3,158,193,2,,160,,,,,,,,,,45,11,38,6.7,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-05-20,,,844,,365,,8,147,179,0,,150,,,,,,,,,,45,8.1,25,5.6,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-06-01,,,1560,,443,,8.4,189,230,4,,180,,,,,,,,,,55,10,35,6.7,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-07-01,,,1340,,386,,8.5,175,213,13,,150,255,,,,,,,,,47,8.6,27,4.8,7.4,14,0.5,28,,,,,,,,,,,,,,,,,,,,,
10321000,1967-07-11,,,651,,406,,8.3,187,228,2,,170,,,,,,,,,,55,9.1,27,5,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-07-21,,,254,,453,,8.2,207,252,0,,190,,,,,,,,,,58,10,35,6,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-08-01,,,48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-08-01,,,48,,525,,8.2,223,272,0,,190,,,,,,,,,,53,13,47,7.4,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-09-01,,,18,,508,,8.1,210,256,0,,180,,,,,,,,,,46,15,45,7.9,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-10-01,,,20,,556,,8.4,221,255,7,,180,322,,,,,,,,,47,15,45,8.2,16,32,0.6,24,,,,,,,,,,,,,,,,,,,,,
10321000,1967-10-01,,,7.7,,546,,8.4,213,260,4,,190,340,,,,,0.02,,,,48,16,46,8.5,19,43,0.5,27,,,,,,,,,,,,,,,,,,,,,
10321000,1967-10-09,,,27,,612,,8.4,242,281,7,,190,,,,,,,,,,53,15,57,11,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-10-17,,,23,,586,,8.2,238,290,0,,190,,,,,,,,,,53,15,49,8.3,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-11-01,,,36,,582,,8.4,238,278,6,,190,,,,,,,,,,55,14,50,7.8,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-12-01,,,44,,565,,8.2,239,292,0,,170,,,,,,,,,,50,12,53,8.2,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-12-09,,,41,,626,,8.2,271,330,0,,210,,,,,,,,,,60,14,57,8.7,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1967-12-19,,,46,,553,,8.2,241,294,0,,180,,,,,,,,,,54,12,51,8.3,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-01-01,,,50,,519,,8.6,218,238,14,,170,334,,,,,,,,,45,13,50,7.9,16,41,0.6,28,,,,,,,,,,,,,,,,,,,,,
10321000,1968-01-15,,,48,,590,,8.3,251,298,4,,200,372,,,,,,,,,57,13,52,8.6,16,43,0.6,30,,,,,,,,,,,,,,,,,,,,,
10321000,1968-02-01,,,70,,406,,7.8,142,173,0,,110,,,,,,,,,,27,11,40,6.6,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-02-11,,,221,,480,,8.3,195,232,3,,160,,,,,,,,,,44,11,43,6.3,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-03-01,,,259,,503,,8.4,204,241,4,,160,,,,,,,,,,45,11,46,6.9,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-03-09,,,221,,471,,8.3,191,227,3,,150,,,,,,,,,,44,10,40,6.4,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-03-15,,,229,,479,,8,195,238,0,,160,,,,,,,,,,44,12,41,6.4,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-04-01,,,169,,415,,8.2,157,192,0,,130,271,,,,,,,,,36,10,38,5.8,18,35,0.5,27,,,,,,,,,,,,,,,,,,,,,
10321000,1968-05-01,,,160,,381,,8.4,163,193,3,,130,,,,,,,,,,38,8.6,33,5.5,,26,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-05-15,,,164,,380,,8,154,188,0,,120,235,,,,,,,,,33,8.7,36,5.8,12,26,0.4,19,,,,,,,,,,,,,,,,,,,,,
10321000,1968-06-01,,,498,,311,,7.8,138,168,0,,120,,,,,,,,,,37,5.8,20,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-06-07,,,914,,394,,8.2,174,212,0,,140,,,,,,,,,,42,8.5,31,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-06-15,,,985,,424,,7.7,184,224,0,,150,,,,,,,,,,45,9.2,33,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-07-01,,,140,,436,,8.2,195,238,0,,150,273,,,,,,,,,44,10,37,5.7,11,21,1.1,24,,,,,,,,,,,,,,,,,,,,,
10321000,1968-08-01,,,20,,520,,8.6,213,240,10,,170,,,,,,,,,,45,13,49,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-08-15,,,12,,566,,8.2,231,282,0,,180,,,,,,,,,,50,14,52,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-08-24,,,44,,473,,8.6,205,226,12,,160,,,,,,,,,,45,11,42,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-09-01,,,29,,512,,8.4,220,256,6,,170,,,,,,,,,,45,13,48,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1968-09-15,,,28,,536,,8.1,225,274,0,,170,,,,,,,,,,47,13,49,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-05-20,16,,126,,483,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-06-29,23.5,,149,,445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-07-29,24.5,,14,,517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-08-11,15.5,,14,,464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-10-06,10.5,,17,,506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-11-09,,,26,,496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1977-12-19,0,,26,,551,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-01-24,0,,64,,527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-02-27,6.5,,156,,541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-03-24,10,,557,,407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-04-20,12,,579,,389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-07-07,17.5,,370,,384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-08-22,19.5,,19,,550,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-09-21,16.5,,52,,498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1978-11-21,4,,77,,490,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-01-04,0.5,,54,,619,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-02-05,0.5,,103,,495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-03-02,3.5,,338,,536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-04-26,13,,724,,359,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-04-26,13,,724,54,374,8.6,8.6,210,230,13,,140,273,90,0.6,0.46,0.03,,,,6.1,41,8.8,29,5.5,11,24,0.4,27,,,,,,,,,,,,,,,,,,,,,
10321000,1979-05-29,16.5,,2160,84,366,7.9,8.2,280,340,0,,130,234,53,1.1,1,0.01,,,,,39,8.3,28,7.1,9.7,23,0.4,28,6,100,,,,,,20,,20,,,,,,20,,,1,,0.10
10321000,1979-07-09,22,,204,13,418,8,8.6,215,240,11,,160,283,89,0.43,0.37,0.010,,,,5.1,46,11,33,5.6,9.3,21,0.4,27,,,,,,,,,,,,,,,,,,,,,
10321000,1979-07-09,22,,204,,377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-07-30,26,,88,,448,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-07-30,26,,80,10,458,7.5,8.8,235,280,3,,160,299,83,0.66,0.62,0.010,,,,,44,11,35,7.1,11,24,0.4,25,8,90,,2,,3,,10,,1,,,,,,3,,,1,,0.10
10321000,1979-08-29,21.5,,29,,518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1979-08-29,21.5,,19,13,509,8.5,8.4,185,220,3,,160,201,78,0.69,0.42,0.010,,,,5.4,46,11,48,9.6,21,39,0.6,26,,,,,,,,,,,,,,,,,,,,,
10321000,1979-10-26,12.5,,42,7,610,11,8.9,259,230,42,,190,395,,0.81,0.49,0.09,,,,,54,14,55,11,27,48,0.6,30,10,100,,,0,3,,10,0,,,,0,,,3,,,0,,0.2
10321000,1979-11-27,1,,74,10,521,12.2,8.6,204,220,14,,170,327,,0.59,0.39,0.01,,,,4,50,11,44,7.3,17,33,0.6,31,,,,,,,,,,,,,,,,,,,,,
10321000,1979-12-21,1,,96,6,514,12,8.5,,,,,170,,,1.2,0.92,0,,,,7.8,50,12,43,7.2,17,33,0.5,32,,,,,,,,,,,,,,,,,,,,,
10321000,1980-02-05,3,,227,21,522,11.2,7.9,205,250,0,,180,333,89,0.85,0.49,0.04,,,,,51,13,48,7.4,17,39,0.5,33,6,90,,1,0,3,,30,,,,0,0,,,3,,,0,,0.2
10321000,1980-02-28,8,,612,60,550,9.6,,197,240,0,,170,331,83,1.2,0.88,,,,,9.1,47,12,47,8.2,20,48,0.5,29,,,,,,,,,,,,,,,,,,,,,
10321000,1980-03-27,5,,299,19,505,,7.6,197,240,0,,160,307,,0.56,0.48,0.01,,,,13,46,12,42,6.4,15,37,0.4,30,,,,,,,,,,,,,,,,,,,,,
10321000,1980-04-24,12,,652,130,308,8.3,8.3,,,,,100,,81,1.8,1.6,0.08,,,,13,30,7.1,25,5.3,9.9,18,0.3,28,,,,,,,,,,,,,,,,,,,,,
10321000,1980-05-29,14.5,,3180,88,449,7.6,7.8,190,233,0,,140,281,,1.4,1.2,0.06,,,,,42,9.6,41,7.9,13,29,0.2,29,7,90,,1,0,3,10,20,0,10,,0,0,,,,,,0,,0.1
10321000,1980-06-23,20.5,,2070,23,362,6.7,,,,,,140,,,,,0.04,,,,8.9,42,9.2,24,5.2,7.9,11,0.3,32,,,,,,,,,,,,,,,,,,,,,
10321000,1980-07-29,24,,203,11,427,8.1,8.2,180,230,0,,160,257,,0.63,0.5,0.01,,,,,46,9.9,31,5.9,10,20,0.5,25,7,90,,,0,3,,40,,20,,0,0,,,10,,,0,,0
10321000,1980-08-25,19.5,,49,11,545,9.2,7.7,216,260,0,,170,342,,1.4,1.1,0,,,,10,50,12,47,8.2,20,40,0.8,26,,,,,,,,,,,,,,,,,,,,,
10321000,1980-09-30,17,,47,6,530,11,8.4,223,260,6,,170,336,,0.82,0.74,0,,,,4.4,47,12,50,8.8,23,36,0.6,25,,,,,,,,,,,,,,,,,,,,,
10321000,1980-11-21,5,,108,10,553,12.2,8.5,210,220,8,,180,346,,0.6,0.46,0.02,,,,,52,12,45,8.5,21,52,0.5,28,7,100,,1,0,3,,10,0,10,,0,0,,,3,,,0,,0
10321000,1981-01-28,3.5,,127,14,527,12,8.7,210,270,19,,170,355,,,,0.05,,,,,50,12,46,6.5,19,38,0.5,30,8,100,,1,0,3,0,30,,20,,0,0,,,10,,,0,,0
10321000,1981-03-25,9.5,,110,24,600,11.8,8.8,230,250,17,,200,364,,0.55,0.51,0.05,,,,4.9,56,14,53,6.8,20,47,0.5,28,,,,,,,,,,,,,,,,,,,,,
10321000,1981-05-20,13,,117,16,474,,8.6,200,230,7,,180,,,1,0.94,0.08,,,,,55,10,36,7.7,14,,0.5,27,9,,,1,0,3,,20,,20,,,0,,,,7,,0,,0
10321000,1981-07-21,25.5,,5.7,9,616,9.2,8.6,240,270,12,,200,382,,1,0.71,0.13,,,,,54,15,55,10,25,43,0.5,34,14,120,,1,,3,,10,,10,,,0,,,,,,0,,0.1
10321000,1981-09-17,21.5,,9.5,3,487,10.2,8.1,200,250,0,,170,292,,1.2,0.97,0.13,,,,,44,14,37,8,15,25,0.6,24,9,100,,1,0,3,,10,,20,,,0,,,10,,,1,,0
10321000,1981-12-04,3,,23,4,623,13,8.8,240,260,15,,210,406,,0.57,0.38,0.09,0.100,0.020,,,62,13,58,9.5,33,56,0.6,30,9,100,,1,10,3,1,30,1,20,,,1,,,3,,,1,,0.10
10321000,1982-01-27,1,,61,8,529,12.5,8.2,210,,,,180,342,,0.88,,,,,0.55,,52,11,43,7.7,23,42,0.5,32,,,,,,,,,,,,,,,,,,,,,
10321000,1982-03-12,7,,649,96,395,10.7,8.4,160,,,,130,248,87,0.70,0.48,0.13,,,0.31,,37,9.3,33,7.4,15,22,0.4,28,9,82,,1,10,,,,,,,,1,,,4,,,1,,0.10
10321000,1982-05-26,19,,1530,56,367,7,8.2,158,,,,130,235,70,1.3,1.1,0.12,0.100,0.020,0.31,,37,8.5,26,5.4,9.6,22,0.4,31,7,84,,1,10,1,,80,1,,,1,1,,,20,,,1,,0.10
10321000,1982-07-20,25,,349,26,418,7.5,8,190,,,,150,253,,1.7,1.5,0.14,,,0.31,,45,9.3,25,6,9,17,0.4,27,,,,,,,,,,,,,,,,,,,,,
10321000,1982-09-21,20.5,,56,6,510,,8.6,208,,,,160,318,,1.0,0.84,0.06,,,0.37,,49,10,46,8.5,20,32,0.5,27,9,100,,1,10,1,,,1,,,,1,,,10,,,1,,0.10
10321000,1982-11-24,,,281,25,486,12.7,8.6,213,,,,180,313,,1.1,0.87,0.13,,,0.09,,50,12,38,5.6,14,35,0.4,29,5,81,0.5,1,1,3,,,1,,10,1,1,300,6.0,10,10,30,1,,0.10
10321000,1983-01-25,2,,270,20,476,13.1,8.4,195,,,,160,294,,0.83,0.58,0.12,,,0.25,,47,11,37,5.5,16,30,0.4,30,,,,,,,,,,,,,,,,,,,,,
10321000,1983-03-06,5,,4800,200,308,,7.6,109,,,,79.9,175,79,2.1,1.6,0.33,,,0.521,,23,5.4,26,6.9,10,21,0.3,15,6,49,0.5,1,1,3,,40,,20,10,,1,150,6.0,,40,20,1,,0.1
10321000,1983-05-24,19.5,26,1500,110,359,7.2,8.2,153,,,,127,232,86,1.2,0.97,0.13,,,0.184,,36,9,26,5.4,12,20,0.3,30,6,73,0.5,1,1,3,,30,,,10,,1,220,6.0,40,40,20,1,,0.10
10321000,1983-07-20,19.5,22.5,499,26,369,7.7,8.4,176,,,,143,231,,1.0,0.77,0.13,,,0.245,,43,8.7,23,4.4,7.1,16,0.4,22,,,,,,,,,,,,,,,,,,,,,
10321000,1983-09-20,16,17,75,4,505,11,,212,,,,175,323,,0.9,0.80,0.100,,,0.368,,50,12,47,7.1,22,31,0.5,25,8,98,0.5,1,1,3,,3,,10,10,,1,390,6.0,,10,40,1,,0.10
10321000,1983-11-15,5.5,12.5,335,13,512,12.2,8.4,230,,,,193,333,88,1.1,0.87,0.13,,,0.123,,54,14,43,5.8,17,32,0.4,28,5,87,0.5,1,1,3,,,1,10,10,1,1,350,6.0,,10,40,1,,0.10
10321000,1984-01-24,0,3.5,369,32,532,10.7,7.7,222,,,,190,335,80,0.97,0.68,0.12,,,0.184,,53,14,40,5.6,20,37,0.4,31,,,,,,,,,,,,,,,,,,,,,
10321000,1984-03-14,3.5,8.5,2630,640,361,11,8.1,134,,,,111,215,80,2.6,2.4,0.13,,,0.245,,32,7.4,30,5.9,13,24,0.2,20,5,67,0.5,,1,3,,130,1,20,10,1,1,200,6.0,130,130,20,1,,0.10
10321000,1984-05-18,15,18.5,8130,180,352,8.4,8.1,140,,,,111,224,,2.1,1.9,0.07,,,0.153,,31,8,28,10,13,22,0.4,27,5,59,1,1,1,3,,,,10,10,1,1,190,6,3,10,20,1,,0.10
10321000,1984-07-25,24,34,1200,39,327,,,142,,,,126,196,,0.93,0.71,0.09,,,0.153,,38,7.5,19,3.7,6.9,14,0.3,21,,,,,,,,,,,,,,,,,,,,,
10321000,1984-09-27,13.5,20.5,160,8,519,10.6,8.5,205,,,,179,318,,0.40,0.27,0.03,,,0.245,,50,13,39,7.7,21,37,0.5,26,7,110,1,1,1,3,,,,10,10,,1,370,6.0,,10,30,1,,0.10
10321000,1984-11-30,2.5,9.5,335,15,600,14,8.4,239,,,,208,,,1.1,0.95,0.05,,,0.153,,55,17,48,6.9,,,0.4,30,5,110,0.5,1,,3,,,,20,10,,1,390,6.0,3,10,30,1,,0.10
10321000,1985-01-22,0.5,3.5,218,31,404,12.2,8.2,129,,,,144,294,,0.86,0.49,0.11,,,0.276,,41,10,29,4.3,19,29,0.3,21,,,,,,,,,,,,,,,,,,,,,
10321000,1985-03-27,3.5,4,1040,90,476,12.8,8.3,169,,,,166,305,88,1.5,1.2,0.07,,,0.215,,45,13,38,6.8,20,41,0.3,27,6,83,0.6,1.0,1,3.0,5,91,7,11,10.0,2,1,260,6.0,18,180,26,1,,0.1
10321000,1985-05-21,17.5,22.5,606,27,430,8.7,8.4,166,,,,151,268,90,0.60,0.42,0.08,,,0.031,,44,10,30,6.1,14,28,0.4,28,5,80,0.50,1.0,1,3.0,6,27,4,8,10.0,1,1,260,6.0,28,30,22,1,,0.1
10321000,1985-07-23,23.5,26,54,17,577,8.7,8.6,215,,,,176,358,98,0.70,0.57,0.03,,,0.276,,49,13,50,9.9,26,42,0.5,29,,,,,,,,,,,,,,,,,,,,,
10321000,1985-09-26,13.5,17,33,9,675,10.6,8.8,229,,,,188,394,,0.90,0.72,0.08,,,1.17,,52,14,66,13,40,57,0.6,27,12,140,0.50,1.0,1,3.0,1,4,1,11,10.0,1,1,590,6.0,14,10,67,1,,0.1
10321000,1985-11-19,0,-3.5,76,11,579,15.2,8.7,205,,,,190,361,,0.61,0.37,0.1,0.09,0.02,0.429,,56,13,50,7.1,26,51,0.6,33,8,110,0.50,1.0,1,3.0,3,9,1,56,10.0,2,1,440,6.0,10,10,37,1,,0.2
10321000,1986-01-22,1,5.5,247,50,451,12.6,8.3,165,,,,140,273,86,1.1,0.66,0.2,0.16,0.02,0.521,,39,11,36,7.1,16,38,0.4,26,,,,,,,,,,,,,,,,,,,,,
10321000,1986-03-27,12,21.5,822,47,399,9.8,8.2,146,,,,131,240,89,0.60,0.45,0.05,0.090,0.01,0.153,,36,10,29,5.3,16,29,0.4,28,7,68,0.50,1.0,1,3.0,4,47,1,8,10.0,1,1,230,6.0,7,50,18,1,,0.1
10321000,1986-05-22,12,11,942,40,384,8,8.4,149,,,,131,229,,1.0,0.85,0.01,0.100,0.010,0.184,,38,8.8,25,4.8,11,24,0.3,28,,,,,,,,,,,,,,,,,,,,,
10321000,1986-07-23,21,21,126,10,443,8.9,8.7,190,,,,159,276,,0.50,0.36,0.02,0.090,0.01,0.245,,47,10,33,6.3,14,30,0.4,23,,,,,,,,,,,,,,,,,,,,,
10321000,1986-09-23,15,22.5,29,1,612,12.3,8.8,231,,,,201,400,,,,0.02,0.100,0.010,0.797,,57,14,59,13,34,54,0.6,28,10,120,0.50,1.0,1,3.0,1,9,,12,10.0,1,1,510,6.0,9,10,55,1,,0.1
10321000,1986-11-25,4,6,152,10,475,11.6,8.3,198,,,,177,303,,0.30,0.19,0.010,0.100,0.010,0.061,,51,12,39,6,16,32,0.4,29,7,90,0.50,1.0,1,3.0,1,79,5,13,10.0,1,1,360,6.0,17,10,32,1,,0.1
10321000,1987-01-28,0.5,3.5,74,3,506,11.8,8.3,,235,4,,177,309,,0.60,0.48,0.010,0.100,0.010,0.092,,51,12,39,6.1,16,37,0.5,28,,,,,,,,,,,,,,,,,,,,,
10321000,1987-03-24,7.5,7.5,300,33,603,10.5,8.6,230,,,,209,392,79,0.56,0.44,0.01,0.06,0.010,0.123,,57,16,55,7.4,27,60,0.6,33,8,100,0.50,1.0,1,3.0,1,19,5,11,10.0,2,1,370,6.0,4,10,39,1,,0.5
10321000,1987-06-01,18.5,18,483,31,460,,8.5,193,,,,159,285,,1.3,1.1,0.02,0.100,0.010,0.123,,47,10,34,5.9,16,28,0.4,31,3,82,0.50,1,3,3.0,8,27,5,8,10.0,1,1,280,6.0,12,10,26,1,,0.1
10321000,1987-08-26,21,26.5,10,5,491,9.8,8.6,,,,,171,306,85,0.30,0.17,0.010,0.100,0.010,0.031,,45,14,41,8.6,21,30,0.8,24,9,100,0.50,1.0,2,3.0,2,4,5,12,10.0,1,1,490,6.0,7,10,45,1,,0.1
10321000,1987-11-03,14.5,16,8,3,521,,8.5,205,,,,173,307,98,0.60,0.47,0.02,0.100,0.010,0.031,,46,14,46,7.8,20,38,0.5,20,10,110,0.50,1.0,1,3.0,2,3.0,5,10,10.0,4,1,420,6.0,3.0,10,41,1,,0.1
10321000,1988-01-28,0.5,4,33,1,588,14,8.6,235,,,,201,369,,0.30,0.18,0.010,0.090,0.01,0.031,,59,13,48,8.3,22,46,0.5,31,,,,,,,,,,,,,,,,,,,,,
10321000,1988-03-31,8.5,11.5,234,33,409,,8.6,181,,,,162,322,98,0.60,0.47,0.03,0.100,0.010,0.153,,45,12,38,6.5,17,44,0.4,30,6,74,0.50,1.0,2,3.0,1,10,5,10,10.0,5,1,310,6.0,5,10,24,1,,0.1
10321000,1988-05-26,17.5,29.5,384,41,300,8.3,8.4,114,,,,109,195,98,0.40,0.28,0.02,0.100,0.010,0.061,,34,5.9,20,4.3,8.5,26,0.4,21,4,55,0.50,1.0,1,3.0,1,9,5,3,10.0,1,1,210,6.0,8,10,17,1,,0.1
10321000,1988-08-24,24.5,34.5,11,4,479,8.8,8.5,187,,,,171,298,85,1.0,0.90,0.010,0.100,0.010,0.031,,45,14,40,7.8,15,28,0.5,24,8,100,0.50,1.0,1,3.0,1,5,5,14,10.0,3,1,480,6.0,3.0,10,51,1,,0.1
10321000,1988-11-29,0.5,0.5,64,13,548,12.4,8.6,225,,,223,187,358,,1,0.36,0.04,0.62,0.01,0.276,,53,13,52,7.8,22,44,0.5,28,8,100,0.50,1.0,1,3.0,1,16,5,11,10.0,5,1,490,6.0,3,10,47,1,,0.1
10321000,1988-11-29,0.5,,,,545,12.4,8.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1988-11-29,0.5,,,,550,12.4,8.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1988-11-29,0.5,,,,545,12.4,8.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-01-26,0,-5.0,29,68,615,14.9,8.5,,,,270,217,383,,0.30,0.16,0.03,0.100,0.010,0.031,,64,14,51,8.9,17,36,0.3,30,,,,,,,,,,,,,,,,,,,,,
10321000,1989-04-08,14.5,21.5,905,50,395,8.4,8.3,136,,,136,128,240,80,0.80,0.65,0.04,0.100,0.010,0.031,,36,9.2,29,5.8,15,35,0.3,28,7,67,0.50,1.0,1,3.0,8,47,5,7,10.0,6,1,230,6.0,6,60,17,1,,0.1
10321000,1989-05-31,17,21,524,12,378,7.6,8.2,167,,,166,142,242,90,0.60,0.46,0.010,0.100,0.010,0.123,,42,9,27,5.2,8.9,21,0.3,28,5,69,0.50,1.0,1,3.0,4,13,1,6,10.0,2,1.0,250,6.0,5,20,20,1,,0.1
10321000,1989-05-31,17,,,,380,7.6,8.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-05-31,17,,,,378,7.6,8.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-05-31,17,,,,376,7.6,8.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-07-31,24.5,33.5,19,4,530,9.1,8.7,201,,,,163,320,72,0.60,0.50,0.010,0.100,0.010,0.031,,44,13,47,8.7,20,43,0.7,23,,,,,,,,,,,,,,,,,,,,,
10321000,1989-08-28,23,23.5,15,4,445,9.4,8.5,189,,,188,152,272,87,0.40,0.29,0.01,0.100,0.010,0.031,,41,12,36,8,14,27,0.5,21,8,98,0.50,1.0,1,3.0,1,6,1,8,10.0,1.0,1.0,440,6.0,4,10,43,1,,0.1
10321000,1989-08-28,22,,,,445,9.4,8.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-08-28,22,,,,445,9.4,8.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-08-28,22,,,,442,9.4,8.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,1989-11-29,0.5,2,49,12,502,13.2,8.3,208,,,240,175,350,68,0.50,0.39,0.010,0.100,0.010,0.061,,50,12,45,7.9,21,41,0.6,29,8,99,0.50,1.0,1,3.0,5,16,1,14,10.0,5,1.0,450,6.0,3.0,20,41,1,,0.1
10321000,1990-01-17,1,2.5,71,15,459,12.2,8.7,183,,,190,160,294,64,0.40,0.28,0.01,0.100,0.010,0.061,,46,11,38,5.5,17,36,0.5,27,,,,,,,,,,,,,,,,,,,,,
10321000,1990-03-30,11,17,307,26,420,9.9,8.5,175,,,168,149,272,99,0.90,0.77,0.010,0.100,0.010,0.092,,43,10,34,6.6,17,32,0.3,28,7,72,0.7,1.0,5.0,3.0,10.0,29,10,6,10,10.0,1,280,6.0,3,50,24,1,,0.1
10321000,1990-05-22,20,26,195,9,396,9.2,8.6,164,,,169,133,242,83,0.70,0.60,0.010,0.100,0.010,0.061,,40,7.9,29,5.9,13,24,0.1,21,5,62,0.50,1.0,1,3.0,2,11,1,2,10.0,1.0,1.0,260,6.0,8,10,22,1,,0.1
10321000,1990-06-28,20,28,238,10,378,8,8.7,174,,,173,139,235,93,0.50,0.37,0.03,0.100,0.010,0.031,,42,8.4,26,4.6,11,16,0.4,23,,,,,,,,,,,,,,,,,,,,,
10321000,1990-08-21,19.5,21,11,5,437,9.5,8.4,183,,,186,150,272,93,0.80,0.69,0.02,0.090,0.01,0.031,,40,12,34,7.3,14,29,1.3,22,7,84,0.50,1.0,1,3.0,2,3,1,4,10.0,1,1.0,430,6.0,3.0,20,42,1,,0.1
10321000,1990-11-01,7,4.5,14,3,576,13.6,8.6,209,,,210,179,302,61,0.60,0.49,0.010,0.100,0.010,0.061,,50,13,38,7.3,16,30,0.2,21,7,120,0.50,1.0,1,3.0,1,6,1,12,10.0,1.0,1.0,500,6.0,27,10,44,1,,0.1
10321000,1991-01-03,0,-5,15,3,623,13.2,7.8,264,,,265,219,397,43,0.4,0.25,0.03,0.1,0.010,0.031,,63,15,48,9.7,22,52,0.6,27,,,,,,,,,,,,,,,,,,,,,
10321000,1991-02-22,7,17,152,20,478,11.8,7.9,195,,,170,156,290,86,0.70,0.57,0.01,0.100,0.010,0.092,,46,9.9,39,6.3,14,46,0.4,25,6,79,0.50,1.0,1,3.0,3,25,1,11,10.0,1,1.0,330,6.0,5,40,30,1,,0.4
10321000,1991-04-29,11.5,8.5,119,14,488,10.4,8.7,199,,,193,159,295,99,0.56,0.48,0.03,0.05,0.01,0.031,,47,10,39,6.7,15,34,0.6,26,6,80,0.50,1.0,1,3.0,2,18,1,15,10.0,2,1.0,350,6.0,7,20,32,1,,0.1
10321000,1991-06-28,17,25.5,487,18,365,6.8,8.5,167,,,163,144,222,83,0.65,0.58,0.01,0.050,0.010,0.031,,45,7.6,22,3.8,8,16,0.1,22,,,,,,,,,,,,,,,,,,,,,
10321000,1991-08-28,17.5,21,16,3,455,11.2,8.5,199,,,190,160,279,97,0.45,0.40,0.010,0.050,0.010,0.031,,44,12,34,6.8,17,28,0.6,22,6,95,0.50,1.0,1,3.0,1,6,1,14,10.0,1,1.0,450,6.0,13,30,43,1,,0.1
10321000,1991-11-01,4,8,32,2,458,12.3,8.5,208,,,194,181,326,76,0.26,0.18,0.02,0.063,0.010,0.031,,51,13,40,7.7,21,48,0.6,27,,110,,,,3.0,,7,,12,10.0,1.0,1.0,430,6.0,,10,63,1,,
10321000,1991-12-28,0.5,4,68,5,506,12.2,8.2,230,,,159,189,308,65,0.25,0.19,0.010,0.050,0.010,0.031,,56,12,45,7.1,20,47,0.5,25,,,,,,,,,,,,,,,,,,,,0.01,
10321000,1992-02-26,7,16.5,215,49,412,11.2,8.4,184,,,185,154,274,97,0.55,0.48,0.010,0.050,0.010,0.061,,46,9.4,32,7.2,16,27,0.5,25,,71,,,,3.0,,64,,8,10.0,2,1.0,290,6.0,,120,26,1,,
10321000,1992-04-28,19.5,29,149,13,432,9.4,8.5,180,,,178,142,275,79,0.35,0.28,0.010,0.050,0.010,0.031,,42,9,39,6.6,18,31,0.5,22,,71,,,,3.0,,10,,5,10.0,1,1.0,330,6.0,,20,30,1,,
10321000,1992-08-27,13,14,11,4,451,8.8,8.4,191,,,190,153,292,95,0.45,0.38,0.02,0.050,0.010,0.031,,38,14,41,8.3,20,33,0.6,23,,110,,,,3.0,,8,,7,10.0,1.0,1.0,480,6.0,,10,49,1,0.01,
10321000,1992-10-27,11,11.5,16,2,474,9.4,8.4,208,,,208,170,299,72,0.35,0.29,0.010,0.050,0.010,0.031,,48,12,36,7.7,15,30,0.5,20,,120,,,,3.0,,6,,8,10.0,1.0,1.0,500,6.0,,20,45,1,0.01,
10321000,1993-02-26,1,-7,40,4,543,11.8,8.4,213,,,245,175,365,64,0.44,0.26,0.04,0.13,0.01,0.031,,50,12,50,7.5,22,49,0.4,26,,96,,,,3.0,,8,,12,10.0,1,1.0,500,6.0,,10,51,1,0.01,
10321000,1993-04-29,12,9.5,485,18,452,8.4,8.4,155,,,149,144,274,86,0.65,0.58,0.02,0.050,0.010,0.123,,41,10,35,6.7,20,46,0.4,25,,72,,,,3.0,,16,,6,10.0,1,1.0,270,6.0,,20,23,1,0.01,
10321000,1993-06-16,15,8.5,967,16,392,7.5,8.4,163,,,166,146,247,60,0.55,0.48,0.02,0.050,0.010,0.123,,44,8.8,28,4.9,11,22,0.4,24,5,63,1.00,1.00,1,1.00,1,,1.00,3,3,1,1.00,,,3,22,,,0.01,
10321000,1993-08-25,19.5,21,26,2,486,8.6,8.4,187,,,191,158,289,72,0.55,0.48,0.02,0.050,0.010,0.031,,45,11,38,7,17,37,0.5,18,,92,,,,3.0,,6,,16,10.0,1,1.0,410,6.0,,20,39,1,0.01,
10321000,1993-10-26,8,14,36,1,595,11.4,8.6,210,,,171,188,349,62,0.25,0.18,0.02,0.050,0.010,0.031,,52,14,58,8.7,26,63,0.7,23,,110,,,,3.0,,6,,7,10.0,1.0,1.0,530,6.0,,10,53,1,0.01,
10321000,1994-02-09,0,2,66,3,573,12.3,8.6,232,,,228,199,367,43,0.45,0.38,0.02,0.050,0.010,0.031,,58,13,50,7.9,22,52,0.6,26,,99,,,,3.0,,5,,22,10.0,1.0,1.0,480,6.0,,10,40,1,0.01,
10321000,1994-04-28,9.5,11,231,8,409,9.8,8.4,175,,,168,145,248,61,0.45,0.39,0.01,0.050,0.010,0.031,,43,9,31,5.8,13,26,0.4,19,,67,,,,3.0,,13,,8,10.0,1,1.0,290,6.0,,10,22,1,0.01,
10321000,1994-06-07,15,20.5,690,11,363,,8.4,171,,,160,139,218,62,0.55,0.48,0.02,0.050,0.010,0.092,,43,7.6,22,4.1,7.6,16,0.3,20,4,56,0.50,1.0,1.0,3.0,2,19,1,5,10.0,1.0,1.0,240,6.0,3.0,40,13,1,0.01,
10321000,1994-09-01,18.5,23,6.2,5,446,8.1,8.6,193,,,185,152,276,66,0.45,0.38,0.02,0.050,0.010,0.245,,41,12,36,8.1,16,24,0.5,24,,130,,,,3.0,,65,,160,10.0,1,1.0,410,6.0,,50,42,1,0.01,
10321000,1994-10-27,9,20,15,6,476,10.8,8.5,212,,,203,167,284,,,,0.02,0.050,0.010,0.031,,47,12,36,7.3,15,26,0.5,17,8,130,0.50,1.0,1.0,3.0,1.0,14,1,58,10.0,1,1.0,480,6.0,4,10,39,1,0.01,0.1
10321000,1994-12-21,0,6,18,7,630,12.6,8.5,245,,,240,200,383,54,0.25,0.20,0.02,0.050,0.010,0.061,,57,14,55,9.9,23,56,0.7,27,,,,,,,,,,,,,,,,,,,,0.01,
10321000,1995-03-02,6,8.5,245,18,434,10.2,8.3,170,,,165,151,269,95,0.55,0.50,0.02,0.050,0.010,0.092,,44,10,34,6.1,16,35,0.4,24,,78,,,,3.0,,25,,13,10.0,1.0,1.0,310,6.0,,20,27,1,0.01,
10321000,1995-04-27,11.5,17.5,453,17,411,9.3,8.4,160,,,142,147,243,84,0.25,0.20,0.02,0.050,0.010,0.061,,44,9,32,5,14,30,0.5,23,,72,,,,4,,60,,5,10.0,1,1.0,260,7,,90,24,1,0.01,
10321000,1995-06-28,21,26,2180,32,370,6.5,8.1,161,,,154,135,217,65,0.65,0.60,0.02,0.050,0.010,0.153,,41,8,22,4.3,9.3,15,0.3,25,,,,,,,,,,,,,,,,,,,,0.01,
10321000,1995-08-22,22,24,80,2,390,9,8.6,165,,,156,141,238,73,0.35,0.30,0.02,0.050,0.010,0.031,,42,8.7,30,5.5,14,26,0.4,17,,71,,,,3.0,,11,,10,10.0,1.0,1.0,320,6.0,,20,21,1,0.01,
10321000,1995-10-27,7.5,5.5,74,6,481,10.2,8.3,200,,,174,165,294,,,,0.02,0.050,0.010,0.031,,46,12,38,6.5,16,37,0.5,21,7,88,0.50,1.0,1.0,3.0,1.0,13,1,14,10.0,1.0,1.0,400,6.0,3.0,20,31,1,0.01,0.1
10321000,1996-04-19,8,16.5,1100,35,370,11.2,8.5,147,,,142,117,224,,,,0.02,0.050,0.010,0.123,,34,7.8,27,4.4,13,28,0.4,24,4,60,0.50,1.0,1.0,3.0,12,58,1,4,10.0,2,1.0,210,6.0,8,70,17,1,0.01,0.1
10321000,1996-09-09,16,20.5,24,1,448,9.2,8.5,198,,,186,158,261,,,,0.02,0.06,0.010,0.031,,45,11,30,6.5,14,24,0.5,18,6,88,0.50,1.0,1.0,3.0,4,11,1,8,10.0,1.0,1.0,430,6.0,4,20,33,1,0.01,0.1
10321000,1996-11-22,,,,12,,,,210,,,,175,328,,,,0.02,0.09,0.010,0.061,,50,12,43,7,20,46,0.6,22,7,90,0.50,1.0,1.0,3.0,8,24,1,6,10.0,1.0,1.0,410,6.0,3.0,40,30,1,0.01,0.1
10321000,1997-04-17,13,21,698,34,416,8.9,8.3,159,,,146,135,248,,,,0.02,0.050,0.010,0.123,,38.3,9.6,32.3,5.4,14.9,32.5,0.5,26,6,70.5,0.50,1.0,1.0,3.0,1.5,3.0,1,3.19,13.5,1.0,1.0,252,7.5,3.0,5,19.5,1,0.01,0.1
10321000,1997-08-25,22,32,140,2,313,9.5,8.7,134,,,131,101,182,,,,0.02,0.050,0.010,0.113,,30.7,5.97,20.8,4.3,9,15.5,0.3,16.6,6,47.4,0.50,1.0,1.0,3.0,1.0,4.3,1,8.53,10.0,1.0,1.0,225,6.0,8.1,5,18.1,1,0.01,0.1
10321000,1997-12-17,0.5,0,66,4,585,12.2,8.4,229,,,229,194,360,,,,0.02,0.050,0.010,0.055,,55.8,13.1,46.9,6.9,21,50.1,0.5,28.5,7,94.9,1.00,8.0,1.0,12.0,1.0,10.0,1,11.2,60.0,1.0,1.0,409,10.0,20.0,10,35.4,1,0.01,0.1
10321000,1998-04-07,7,6,864,44,413,,8.8,159,,,162,141,262,,,,0.02,0.067,0.010,0.159,,39,10.4,33,5.4,15.7,34.7,0.3,26.2,6,74.2,1.00,8.0,1.0,12.0,3.8,18.8,1,4.7,60.0,1.0,1.0,254,10.0,20.0,10,21,1,0.01,0.1
10321000,1998-08-27,18,24.5,62,2,532,9.7,8.5,212,,,200,170,314,,,,0.02,0.050,0.010,0.043,,47.8,12.2,44.2,7.7,19.2,40.3,0.6,21.6,10,97.4,1.00,8.0,1.0,12.0,1.9,10.0,1,9,60.0,1,1.0,401,10.0,20.0,10,37.2,1,0.01,0.1
10321000,1998-08-27,,,,2,,,,212,,,,170,321,,,,0.02,0.050,0.010,0.043,,47.8,12.2,44.2,7.7,19.2,40.3,0.6,21.6,10,97.4,1.00,8.0,1.0,12.0,1.9,10.0,1,9,60.0,1,1.0,401,10.0,20.0,10,37.2,1,0.01,0.1
10321000,2015-05-01,16.5,,80,,390,8.1,8.7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10321000,2015-06-26,21.6,,94,,422,3,8.2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
-------------- next part --------------
#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrreprt
\begin_preamble
\date{}
\usepackage{textcomp,url,multicol}
%\setkomafont{sectioning}{\rmfamily}
\end_preamble
\options abstract=on
\use_default_options false
\begin_modules
natbibapa
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style humannat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

load('.RData')
\begin_inset Argument 1
status open

\begin_layout Plain Layout
echo=FALSE
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The first step in analyzing water chemistry data for CWA compliance is reading
 it into the analytical software.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

carlin <- read.csv("./carlin.csv", header = TRUE, sep = ",", stringsAsFactors
 = F)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Next, check that the data are what you expect to see and convert dates from
 factors.
 
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

carlin$sampdate <- as.Date(carlin$sampdate)
\end_layout

\begin_layout Plain Layout

str(carlin)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The above table shows the structure of the data in a R data.frame format.
 
\end_layout

\begin_layout Standard
Another perspective on the data is the summary of each of the data columns.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

summary(carlin)
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Plots of these distributions are the next step because the graphic conveys
 all the written information, plus much more insight into the data characteristi
ecs, more easily for decision-makers.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

carlin.1 <- subset(carlin, select = siteid:CO3)
\end_layout

\begin_layout Plain Layout

carlin.2 <- subset(carlin, select = c(siteid, sampdate, Alk:Ca))
\end_layout

\begin_layout Plain Layout

carlin.3 <- subset(carlin, select = c(siteid, sampdate, Mg:Cr))
\end_layout

\begin_layout Plain Layout

carlin.4 <- subset(carlin, select = c(siteid, sampdate, Co:Hg))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To plot each chemical constituent's concentration as a function of collection
 date the data format needs to be reshaped from wide to long:
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

carlin.1.melt <- melt(carlin.1, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.2.melt <- melt(carlin.2, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.3.melt <- melt(carlin.3, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.4.melt <- melt(carlin.4, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The X-Y plots use only measured data; missing data are not included.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

pdf('carlin-1-descriptive.pdf')
\end_layout

\begin_layout Plain Layout

print(xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T))
\end_layout

\end_inset


\end_layout

\end_body
\end_document

From r.turner at auckland.ac.nz  Sat Jul 11 01:25:21 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 11 Jul 2015 11:25:21 +1200
Subject: [R] Time series exercise (was:  (no subject)).
In-Reply-To: <CAPHYR5u10WT36nsc2uh7jkR0esVh_AMTYrr8uYt5oXL2evA-fg@mail.gmail.com>
References: <CAPHYR5u10WT36nsc2uh7jkR0esVh_AMTYrr8uYt5oXL2evA-fg@mail.gmail.com>
Message-ID: <55A05461.5030501@auckland.ac.nz>

On 10/07/15 21:17, Lia LEE wrote:
> Hello forum members,
>
> I am taking this R course which I have to admit that it is beyond my
> capability. I am asked to analyze the following data(file attached) according
> to the question provided below:
>
> 1. The file [data_13-9.txt] contains a data of length 225, which seems to
> have some cycle pattern.
> (1) Fit a suitable model for this data, then report the model equation.
> (2) Provide the forecast values for 3-steps ahead and 6-steps ahead.
>
> I am desperately seeking for your help, since I really don't know where to
> start.. I have done "plot.ts(data.13-6)", but not further. Above question
> is just part of the whole task, but I believe solving this would be very
> much helpful for me to jump in to the other questions. It would be easier
> for me to follow if your answer is in R script..
> I appreciate your patience and help.
> Thank you so much in advance!

As Bert Gunter said, we don't do homework for people on this list. 
Notwithstanding I couldn't resist having a crack at analysing your time 
series, and I found it, uh, challenging to say the least.  I think of 
myself as having at least *some* skill at fitting time series models, 
and I struggled to find a model that fitted adequately to your data.
(I finally managed to get a decent fit, after trying a number of models 
on a hammer-and-hope basis.)

Perhaps I am even more of a thicko than I think I am.  Nevertheless it 
seems to me that your instructor has dropped you in the deep end by 
setting you this exercise.  (Perhaps I am missing something obvious, as 
is so often the case.  Others may wish to chime in with some comments to 
the effect "You idiot Rolf.  The *obvious* model is ....")

At any rate, your first stop should be your instructor.  Get him or her 
to give you some pointers as to how to get started.  That is what 
instructors are for.

cheers,

Rolf Turner

Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From h.wickham at gmail.com  Sat Jul 11 01:48:04 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Fri, 10 Jul 2015 16:48:04 -0700
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101535400.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<CABdHhvHwVAPDLS2ZRYMrwZGeFz5KhFczqLYnZb44D2EX90Yusw@mail.gmail.com>
	<alpine.LNX.2.11.1507101535400.10467@localhost>
Message-ID: <CABdHhvGequkwVPeF5eFfpopy0X9JCv2SZ6yDtcmsCJdkTQZ64g@mail.gmail.com>

I'd recommend starting with a simpler .Rmd or .Rnw file, rather than
using it with lyx. The basic .Rmd file below works for me without any
further adjustments:

# Lattice test

```{r}
library(lattice)
xyplot(mpg ~ wt, data = mtcars)
```


Hadley

On Fri, Jul 10, 2015 at 3:39 PM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> On Fri, 10 Jul 2015, Hadley Wickham wrote:
>
>> You shouldn't be explicitly opening a device in a knitr document.
>
>
> Hadley,
>
>   Didn't think so.
>
>> I think maybe you should post a minimal document so we can figure out
>> what's going wrong.
>
>
>   Agreed. Attached are the raw data (carlin.csv) and a stripped down LyX
> document with the knitr chunks.
>
>   This is my first attempt to use knitr; I'm reading the knitr book and
> that's where I got the impression that lattice graphics are not supported.
>
> Rich
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://had.co.nz/


From dwinsemius at comcast.net  Sat Jul 11 02:00:15 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 10 Jul 2015 17:00:15 -0700
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101454360.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
Message-ID: <1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>


On Jul 10, 2015, at 2:57 PM, Rich Shepard wrote:

> On Fri, 10 Jul 2015, Hadley Wickham wrote:
> 
>> Have you tried explicitly print()ing the lattice graphics in your knitr
>> doc?
> 
> Hadley,
> 
>  Only now. Had not thought of trying this before.
> 
> pdf('carlin-1-descriptive.pdf')
> print(xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T))

#Generally one needs to complete a pdf() call with:

dev.off()

# And an empty file is the symptom os such a failure.


David.
> 
> No error messages, but no graphic, either. Without specifying the pdf
> device, TeX complains it cannot find a graphics device and lists bit-mapped,
> ps and svg devices.
> 
>  Most likely I do not have the correct syntax.
> 
> Thanks,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.CA.us  Sat Jul 11 02:00:03 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 10 Jul 2015 17:00:03 -0700
Subject: [R] (no subject)
In-Reply-To: <CAGxFJbTxOigV95-vrWywXPzpWx-K==zgen9LX2zMW1iUCwwvWw@mail.gmail.com>
References: <CAPHYR5u10WT36nsc2uh7jkR0esVh_AMTYrr8uYt5oXL2evA-fg@mail.gmail.com>
	<CAGxFJbTxOigV95-vrWywXPzpWx-K==zgen9LX2zMW1iUCwwvWw@mail.gmail.com>
Message-ID: <93040150-EC56-4A9A-B8D9-9CEBD27F3FC7@dcn.davis.CA.us>

I was under the impression that different instructional environments have different standards for ethical conduct, so we list members cannot in general know whether we would be condoning/assisting in cheating. However, if you are taking a class, then the instructor/institution should be providing appropriate resources to assist the student so we should not have to second guess them anyway. Reducing list volume is at best a secondary concern from my perspective.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 10, 2015 10:30:41 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>Sorry Lia. We don't do homework on this list. You can imagine how
>clogged it would be if we did.
>
>Maybe you'll get lucky and someone will answer you privately.
>Otherwise, seek help from your teachers, fellow students, and/or
>course homework forums.
>
>Cheers,
>Bert
>
>
>Bert Gunter
>
>"Data is not information. Information is not knowledge. And knowledge
>is certainly not wisdom."
>   -- Clifford Stoll
>
>
>On Fri, Jul 10, 2015 at 2:17 AM, Lia LEE <ljylia.0101 at gmail.com> wrote:
>> Hello forum members,
>>
>> I am taking this R course which I have to admit that it is beyond my
>> capability. I am asked to analyze the following data(file attached)
>according
>> to the question provided below:
>>
>> 1. The file [data_13-9.txt] contains a data of length 225, which
>seems to
>> have some cycle pattern.
>> (1) Fit a suitable model for this data, then report the model
>equation.
>> (2) Provide the forecast values for 3-steps ahead and 6-steps ahead.
>>
>> I am desperately seeking for your help, since I really don't know
>where to
>> start.. I have done "plot.ts(data.13-6)", but not further. Above
>question
>> is just part of the whole task, but I believe solving this would be
>very
>> much helpful for me to jump in to the other questions. It would be
>easier
>> for me to follow if your answer is in R script..
>> I appreciate your patience and help.
>> Thank you so much in advance!
>>
>> Cheers,
>> Lia
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Sat Jul 11 02:18:51 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 10 Jul 2015 17:18:51 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
Message-ID: <alpine.LNX.2.11.1507101717020.10467@localhost>

On Fri, 10 Jul 2015, David Winsemius wrote:

> #Generally one needs to complete a pdf() call with:
>
> dev.off()
>
> # And an empty file is the symptom os such a failure.


David,

   I did leave that off the example file, but it make no difference. The
attached is the compiled example.pdf

Rich
-------------- next part --------------
A non-text attachment was scrubbed...
Name: example.pdf
Type: application/pdf
Size: 63741 bytes
Desc: 
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150710/0f2c8fa1/attachment.pdf>

From xie at yihui.name  Sat Jul 11 03:46:37 2015
From: xie at yihui.name (Yihui Xie)
Date: Fri, 10 Jul 2015 20:46:37 -0500
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <alpine.LNX.2.11.1507101717020.10467@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
Message-ID: <CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>

Your LyX example has two problems: 1) We don't have your .RData; 2)
You didn't library(reshape) and library(lattice). Hence it is not a
self-contained reproducible example.

After fixing these two issues, I don't see why lattice graphics can be
problematic with knitr. There is no need to print() the plot, and no
need to pdf(), either. See the attached PDF I generated.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Fri, Jul 10, 2015 at 7:18 PM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> On Fri, 10 Jul 2015, David Winsemius wrote:
>
>> #Generally one needs to complete a pdf() call with:
>>
>> dev.off()
>>
>> # And an empty file is the symptom os such a failure.
>
>
>
> David,
>
>   I did leave that off the example file, but it make no difference. The
> attached is the compiled example.pdf
>
> Rich
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: example.pdf
Type: application/pdf
Size: 57487 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150710/ca3f32e2/attachment.pdf>

From dawn1313 at gmail.com  Sat Jul 11 01:35:23 2015
From: dawn1313 at gmail.com (Dawn)
Date: Fri, 10 Jul 2015 16:35:23 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
Message-ID: <CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>

Thank you all and sorry for the data messing. It has worked!

Best,
Dawn

On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Dawn,
> Your data are a bit messed up, but try the following:
>
> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
>
> I'm assuming that you want to discard the NA values.
>
> Jim
>
> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
> > Hello,
> >
> > Please use ?dput to give a data example, like this it's completely
> > unreadable. If your data.frame is named 'dat' use
> >
> > dput(head(dat, 30))  # paste the outut of this in your mail
> >
> >
> > And don't post in html, use plain text only, like the posting guide says.
> >
> > Rui Barradas
> >
> >
> > Em 09-07-2015 18:12, Dawn escreveu:
> >>
> >> Hi,
> >>
> >> I have a big dataframe as follows
> >>
> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC    25ABC
> >> 25XYZ
> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC    36SUR
> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM    42SUR
> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC    66XYZ
> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ    76ABC
> >> 76XYZ    82ABC    85ABC    POV
> >> Cluster_1                                                        17    1
> >> 3    10    14    5    2    2        1    1    1    2
> >>                          2                            TT:61
> >> Cluster_2                    1                                4    20
> >> 6    5    3    6    9    9    6        10        1    3    1
> >>                              4                            TT:88
> >> Cluster_3    3        3                            6        4        17
> >> 17    18    13    17    19    22    11    5    21    8    5    18    4
> >> 7                                        9
> >> TT:227
> >> ........
> >>
> >> I want to get two columns, i.e,  one is to sum columns for all including
> >> ABC for each row and the other is  to sum columns for all including XYZ
> >> for
> >> each row.
> >>
> >> Is there some help? Thank you!
> >> Dawn
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Sat Jul 11 15:06:07 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 11 Jul 2015 06:06:07 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
In-Reply-To: <CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507110602040.25203@localhost>

On Fri, 10 Jul 2015, Yihui Xie wrote:

> Your LyX example has two problems: 1) We don't have your .RData; 2) You
> didn't library(reshape) and library(lattice). Hence it is not a
> self-contained reproducible example.

Yihui,

   I assumed that reading in the *.csv file would create a local .RData file.

   Here, lattice and reshape2 are loaded when R is invoked. However,
explicitly loading them in the embedded code will be done.

> After fixing these two issues, I don't see why lattice graphics can be
> problematic with knitr. There is no need to print() the plot, and no
> need to pdf(), either. See the attached PDF I generated.

   I'll see if requireing those two libraries in the document resolves the
issue. Initially, I did not specify pdf() and added the explicit print()
based on that suggestion in this thread.

   Will test later today.

Thanks very much,

Rich


From fabien.tarrade at gmail.com  Sat Jul 11 09:46:16 2015
From: fabien.tarrade at gmail.com (Fabien Tarrade)
Date: Sat, 11 Jul 2015 09:46:16 +0200
Subject: [R] R 3.1.2 : arima.sim(model=list(ma=0.5), n=250, innov=rnorm(250,
 mean=0, sd=0.1)) versus arima.sim(model=list(ma=0.5), n=250, mean=0,
 sd=0.1) => only the first element is not identical !
Message-ID: <55A0C9C8.4030605@gmail.com>

Dear all,

When doing a DataCamp tutorial with R I find the following observation 
that using 2 different syntax for "arima.sim" give different answer for 
the first element

If I use the the function using the list of argument describe in the 
help manual :
arima.sim(model=list(ma=0.5),n=250,innov=rnorm(250,mean=0,sd=0.1))

or if I use the following syntax use in a DataCamp example :

arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1) it is accepted by 
DataCamp

I don't find exactly the same results. The reason is that even if the 
seed is the same in both cases the first element is not identical while 
it should be (it doesn't mean that the results is wrong, maybe for the 
first element the seed is not propagated correctly)

here the results of the difference using the same seed (only the first 
element is different using the 2 different syntaxes) :

   [1] -0.252214  0.000000  0.000000  0.000000  0.000000  0.000000 
0.000000  0.000000  0.000000  0.000000
  [11]  0.000000  0.000000  0.000000  0.000000  0.000000 0.000000  
0.000000  0.000000  0.000000  0.000000


here the code to reproduce this feature :

set.seed(123);
test1 <- 0.05 + 
arima.sim(model=list(ma=0.5),n=250,innov=rnorm(250,mean=0,sd=0.1))
set.seed(123);
test2 <- 0.05 + arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1)

test1-test2

I am using R 3.1.2 GUI 1.65 Mavericks build (6833) on Mac (I guess arima 
come with stats which is included in R (?))
The DataCamp team ask me to report to you about this observation on this 
mailing list. If you want me to fill a bug report some R bug tracking 
system, let me know

Please tell me if this is the wrong list and which other information do 
you need from R and how to get then (compiler, version of some R 
packages ...)


Hope this help
Thanks
Cheers
Fabien
-- 
Dr Fabien Tarrade

Quantitative Analyst - Data Scientist - Researcher

Senior data analyst specialised in the modelling, processing and 
statistical treatment of data.
PhD in Physics, 10 years of experience as researcher at the forefront of 
international scientific research.
Fascinated by finance and data modelling.

Geneva, Switzerland

Email : contact at fabien-tarrade.eu <mailto:contact at fabien-tarrade.eu>
Phone : www.fabien-tarrade.eu <http://www.fabien-tarrade.eu>
Phone : +33 (0)6 14 78 70 90

LinkedIn <http://ch.linkedin.com/in/fabientarrade/> Twitter 
<https://twitter.com/fabtar> Google 
<https://plus.google.com/+FabienTarradeProfile/posts> Facebook 
<https://www.facebook.com/fabien.tarrade.eu> Google 
<skype:fabtarhiggs?call> Xing <https://www.xing.com/profile/Fabien_Tarrade>

From elham_h763 at yahoo.com  Sat Jul 11 12:21:53 2015
From: elham_h763 at yahoo.com (Patty Haaem)
Date: Sat, 11 Jul 2015 10:21:53 +0000 (UTC)
Subject: [R] A question about nlmer in lme4 package
Message-ID: <115421383.313866.1436610113162.JavaMail.yahoo@mail.yahoo.com>

Dear all,I have studied ?Mixed models in R using the lme4 package Part 6: Nonlinear mixed models? by?Douglas Bates.?In this tutorial, there are some codes to fit nonlinear mixed models for Theoph data. The codes are as fallows:?>Th. start <- c(lKe = -2.5, lKa = 0.5 , lCl = -3)> nm1 <- nlmer ( conc ~ SSfol (Dose , Time ,lKe , lKa , lCl) ~+ ? ?0+ lKe+lKa+lCl +(0+ lKe| Subject )+(0+ lKa| Subject )+ ? ?+(0+ lCl| Subject ), nAGQ =0, Theoph ,+ ? ? start = Th.start , verbose = TRUE )
? I want to add a covariate (like age) to CL parameter. How should I modify above codes? what's more, how ?are selected initial values?(lKe = -2.5, lKa = 0.5 , lCl = -3)??
Thanks in advanceElham
	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Sat Jul 11 16:03:41 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 11 Jul 2015 07:03:41 -0700 (PDT)
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
 [RESOLVED]
In-Reply-To: <CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507110657310.25203@localhost>

On Fri, 10 Jul 2015, Yihui Xie wrote:

> Your LyX example has two problems:

Yihui, et al.:

   All fixed now. After adding the specific libraries the chuck calling
xyplot() to produce the graphic caused an error when I tried to generate a
dvips preview:

LaTeX Error: File 'figure/unnamed-chunk-7-1' not found.

I could not locate the file with any of these extensions:
.eps,.ps,.eps.gz,.ps.gz,.eps.Z

Which is what sent me down the path of specifying pdf() and dev.off().

   However, compiling with pdflatex produces the graphic in the document. All
part of being brand new to using knitr.

Thank you all very much,

Rich


From bgunter.4567 at gmail.com  Sat Jul 11 16:36:16 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 11 Jul 2015 07:36:16 -0700
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <CAM+rpYnR-bmUNuy7q-HiHTCKwqM8b8RWHAvX-M1UE44iACBChw@mail.gmail.com>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
	<7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>
	<CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>
	<CAM+rpY=ucmtVQREyhcgoXzc7v5X+kWaej4DiOTbeZNajnB-CSg@mail.gmail.com>
	<003b01d0ba7c$cf953e80$6ebfbb80$@mcmaster.ca>
	<CAM+rpY=tHRRBAG+tv56q8qhB20A_1rcOGr3dXhA=DFDBJug_-Q@mail.gmail.com>
	<CAGxFJbT=6xK-MAes=+2mXp7DPq4jzMQwqfmSj-RNN6T7UTbE0A@mail.gmail.com>
	<CAM+rpYnR-bmUNuy7q-HiHTCKwqM8b8RWHAvX-M1UE44iACBChw@mail.gmail.com>
Message-ID: <CAGxFJbT0FRa6u6sHC51iztH6MW1rUqen0xwxA6yT3nf2fXvXzw@mail.gmail.com>

Note that John's solution probably includes incorrect partial matches
and that mine fails to match "red" in "this is red." If you change my
proposal to

 sapply(strsplit(do.call(paste,zz[,2:3]),"\\W"), function(x)any(x %in%
alarm.words))

it should agree with Jeff's. Note, however, that you have missed
capital letters:  "Red" would not match "This is red".


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Jul 10, 2015 at 10:54 AM, Christopher W Ryan
<cryan at binghamton.edu> wrote:
> Indeed, the perils of syndromic surveillance with free text.
>
>> with(dd.2, table(fox))
> fox
> FALSE  TRUE
> 74939  1201
>
>> with(dd.2, table(gunter))
> gunter
> FALSE  TRUE
> 75213   927
>
>> with(dd.2, table(newmiller))
> newmiller
> FALSE  TRUE
> 75028  1112
>
>
> Of, course, the simplest thing for me to do would be add "heroine" to
> the alarm.words.  I'm surprised that the US national organization that
> promulgated this list of drug-related terms did not include it. Many
> other common misspellings are included.  I will have to contact them.
>
> --Chris
>
> On Fri, Jul 10, 2015 at 1:39 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> Yes. This is one of the fundamental challenges in text searching --
>> defining exactly what text defines a match and what doesn't. So,
>> continuing your example, one might imagine that heroin and heroine
>> might both be matches, but maybe heroines shouldn't be (e.g. if the
>> text contains movie reviews). So what one might want to do is add
>> semantic analysis to searches, ? la google, a topic way beyond the
>> simple capabilities discussed, or likely needed, here.
>>
>> Incidentally, Jeff Newmiller's (final) regular expression solution is
>> preferable to mine in all respects, I think.
>>
>> -- Bert
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Fri, Jul 10, 2015 at 10:30 AM, Christopher W Ryan
>> <cryan at binghamton.edu> wrote:
>>> Interesting thoughts about the partial-word matches, and speed  On
>>> another real data set, about 73,000 records and 6 columns to search
>>> through for matches (one column of which contains very long character
>>> strings--several paragraphs each), I ran both John's and Bert's
>>> solutions.  John's was noticeably slower, although still quite
>>> tolerable.  There were a different number of matches, though:
>>>
>>>       oic.2
>>> oic          FALSE    TRUE     Sum
>>>   FALSE 74939         0        74939
>>>   TRUE    274           927     1201
>>>   Sum     75213        927     76140
>>>
>>> where oic is the logical vector generated by John's solution, and
>>> oic.2 is the logical vector generated by Bert's solution. Bert's
>>> solution detected about 77% of the cases detected by John's.
>>>
>>> I'm still exploring why that might be. One possible explanation, for
>>> at least part of the difference, is the issue of partial-word matches.
>>> Substantively, I am searching ambulance run records for words related
>>> to opioid overdose, and I've noticed that the medics often spell
>>> heroin as "heroine"  So in this context, I like partial-word
>>> matches--I want to pick up records that (partially) match "heroin"
>>> because it is contained in the word "heroine" .
>>>
>>> There may be other things going on too.
>>>
>>> Thanks.
>>>
>>> --Chris
>>>
>>> On Thu, Jul 9, 2015 at 3:24 PM, John Fox <jfox at mcmaster.ca> wrote:
>>>> Dear Christopher,
>>>>
>>>> My usual orientation to this kind of one-off problem is that I'm looking for a simple correct solution. Computing time is usually much smaller than programming time.
>>>>
>>>> That said, Bert Gunter's solution was about 5 times faster in a simple check that I ran with microbenchmark, and Jeff Newmiller's solution was about 10 times faster. Both Bert's and Jeff's (eventual) solution protect against partial (rather than full-word) matches, while mine doesn't (though it could easily be modified to do that).
>>>>
>>>> Best,
>>>>  John
>>>>
>>>>> -----Original Message-----
>>>>> From: Christopher W Ryan [mailto:cryan at binghamton.edu]
>>>>> Sent: July-09-15 2:49 PM
>>>>> To: Bert Gunter
>>>>> Cc: Jeff Newmiller; R Help; John Fox
>>>>> Subject: Re: [R] detecting any element in a vector of strings, appearing
>>>>> anywhere in any of several character variables in a dataframe
>>>>>
>>>>> Thanks everyone.  John's original solution worked great.  And with
>>>>> 27,000 records, 65 alarm.words, and 6 columns to search, it takes only
>>>>> about 15 seconds.  That is certainly adequate for my needs.  But I
>>>>> will try out the other strategies too.
>>>>>
>>>>> And thanks also for lot's of new R things to learn--grep, grepl,
>>>>> do.call . . . that's always a bonus!
>>>>>
>>>>> --Chris Ryan
>>>>>
>>>>> On Thu, Jul 9, 2015 at 1:52 PM, Bert Gunter <bgunter.4567 at gmail.com>
>>>>> wrote:
>>>>> > Yup, that does it. Let grep figure out what's a word rather than doing
>>>>> > it manually. Forgot about "\b"
>>>>> >
>>>>> > Cheers,
>>>>> > Bert
>>>>> >
>>>>> >
>>>>> > Bert Gunter
>>>>> >
>>>>> > "Data is not information. Information is not knowledge. And knowledge
>>>>> > is certainly not wisdom."
>>>>> >    -- Clifford Stoll
>>>>> >
>>>>> >
>>>>> > On Thu, Jul 9, 2015 at 10:30 AM, Jeff Newmiller
>>>>> > <jdnewmil at dcn.davis.ca.us> wrote:
>>>>> >> Just add a word break marker before and after:
>>>>> >>
>>>>> >> zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ),
>>>>> ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
>>>>> >> ---------------------------------------------------------------------
>>>>> ------
>>>>> >> Jeff Newmiller                        The     .....       .....  Go
>>>>> Live...
>>>>> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>>> Go...
>>>>> >>                                       Live:   OO#.. Dead: OO#..
>>>>> Playing
>>>>> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>>> >> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>>> rocks...1k
>>>>> >> ---------------------------------------------------------------------
>>>>> ------
>>>>> >> Sent from my phone. Please excuse my brevity.
>>>>> >>
>>>>> >> On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>>>>> wrote:
>>>>> >>>Jeff:
>>>>> >>>
>>>>> >>>Well, it would be much better (no loops!) except, I think, for one
>>>>> >>>issue: "red" would match "barred" and I don't think that this is what
>>>>> >>>is wanted: the matches should be on whole "words" not just string
>>>>> >>>patterns.
>>>>> >>>
>>>>> >>>So you would need to fix up the matching pattern to make this work,
>>>>> >>>but it may be a little tricky, as arbitrary whitespace characters,
>>>>> >>>e.g. " " or "\n" etc. could be in the strings to be matched
>>>>> separating
>>>>> >>>the words or ending the "sentence."  I'm sure it can be done, but
>>>>> I'll
>>>>> >>>leave it to you or others to figure it out.
>>>>> >>>
>>>>> >>>Of course, if my diagnosis is wrong or silly, please point this out.
>>>>> >>>
>>>>> >>>Cheers,
>>>>> >>>Bert
>>>>> >>>
>>>>> >>>
>>>>> >>>Bert Gunter
>>>>> >>>
>>>>> >>>"Data is not information. Information is not knowledge. And knowledge
>>>>> >>>is certainly not wisdom."
>>>>> >>>   -- Clifford Stoll
>>>>> >>>
>>>>> >>>
>>>>> >>>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
>>>>> >>><jdnewmil at dcn.davis.ca.us> wrote:
>>>>> >>>> I think grep is better suited to this:
>>>>> >>>>
>>>>> >>>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call(
>>>>> paste,
>>>>> >>>zz[ , 2:3 ] ) ) )
>>>>> >>>>
>>>>> >>>---------------------------------------------------------------------
>>>>> ------
>>>>> >>>> Jeff Newmiller                        The     .....       .....  Go
>>>>> >>>Live...
>>>>> >>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
>>>>> Live
>>>>> >>>Go...
>>>>> >>>>                                       Live:   OO#.. Dead: OO#..
>>>>> >>>Playing
>>>>> >>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
>>>>> with
>>>>> >>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>>> >>>rocks...1k
>>>>> >>>>
>>>>> >>>---------------------------------------------------------------------
>>>>> ------
>>>>> >>>> Sent from my phone. Please excuse my brevity.
>>>>> >>>>
>>>>> >>>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter
>>>>> <bgunter.4567 at gmail.com>
>>>>> >>>wrote:
>>>>> >>>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only
>>>>> a
>>>>> >>>>>single, not a double, loop. It should be more efficient.
>>>>> >>>>>
>>>>> >>>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>>>>> >>>>>+       function(x)any(x %in% alarm.words))
>>>>> >>>>>
>>>>> >>>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>>>>> >>>>>
>>>>> >>>>>The idea is to paste the strings in each row (do.call allows an
>>>>> >>>>>arbitrary number of columns) into a single string and then use
>>>>> >>>>>strsplit to break the string into individual "words" on whitespace.
>>>>> >>>>>Then the matching is vectorized with the any( %in% ... ) call.
>>>>> >>>>>
>>>>> >>>>>Cheers,
>>>>> >>>>>Bert
>>>>> >>>>>Bert Gunter
>>>>> >>>>>
>>>>> >>>>>"Data is not information. Information is not knowledge. And
>>>>> knowledge
>>>>> >>>>>is certainly not wisdom."
>>>>> >>>>>   -- Clifford Stoll
>>>>> >>>>>
>>>>> >>>>>
>>>>> >>>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>>>>> >>>>>> Dear Chris,
>>>>> >>>>>>
>>>>> >>>>>> If I understand correctly what you want, how about the following?
>>>>> >>>>>>
>>>>> >>>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>>>>> >>>>>grepl, x=x)))
>>>>> >>>>>>> zz[rows, ]
>>>>> >>>>>>
>>>>> >>>>>>           v1                              v2                v3 v4
>>>>> >>>>>> 3  -1.022329                    green turtle    ronald weasley  2
>>>>> >>>>>> 6   0.336599              waffle the hamster        red sparks  1
>>>>> >>>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>>>>> >>>>>> 10  1.130622                      black bear  gandalf the grey  2
>>>>> >>>>>>
>>>>> >>>>>> I hope this helps,
>>>>> >>>>>>  John
>>>>> >>>>>>
>>>>> >>>>>> ------------------------------------------------
>>>>> >>>>>> John Fox, Professor
>>>>> >>>>>> McMaster University
>>>>> >>>>>> Hamilton, Ontario, Canada
>>>>> >>>>>> http://socserv.mcmaster.ca/jfox/
>>>>> >>>>>>
>>>>> >>>>>>
>>>>> >>>>>> On Wed, 08 Jul 2015 22:23:37 -0400
>>>>> >>>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>>>>> >>>>>>> Running R 3.1.1 on windows 7
>>>>> >>>>>>>
>>>>> >>>>>>> I want to identify as a case any record in a dataframe that
>>>>> >>>contains
>>>>> >>>>>any
>>>>> >>>>>>> of several keywords in any of several variables.
>>>>> >>>>>>>
>>>>> >>>>>>> Example:
>>>>> >>>>>>>
>>>>> >>>>>>> # create a dataframe with 4 variables and 10 records
>>>>> >>>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>>>>> >>>>>fox",
>>>>> >>>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>>>>> >>>>>"hello
>>>>> >>>>>>> world", "yellow giraffe with a long neck", "black bear")
>>>>> >>>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>>>>> >>>>>"ginny
>>>>> >>>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>>>>> >>>>>dress
>>>>> >>>>>>> robes", "gandalf the white", "gandalf the grey")
>>>>> >>>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
>>>>> >>>lambda=2),
>>>>> >>>>>>> stringsAsFactors=FALSE)
>>>>> >>>>>>> str(zz)
>>>>> >>>>>>> zz
>>>>> >>>>>>>
>>>>> >>>>>>> # here are the keywords
>>>>> >>>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>>>>> >>>>>>>
>>>>> >>>>>>> # For each row/record, I want to test whether the string in v2
>>>>> or
>>>>> >>>>>the
>>>>> >>>>>>> string in v3 contains any of the strings in alarm.words. And
>>>>> then
>>>>> >>>if
>>>>> >>>>>so,
>>>>> >>>>>>> set zz$v5=TRUE for that record.
>>>>> >>>>>>>
>>>>> >>>>>>> # I'm thinking the str_detect function in the stringr package
>>>>> >>>ought
>>>>> >>>>>to
>>>>> >>>>>>> be able to help, perhaps with some use of apply over the rows,
>>>>> but
>>>>> >>>I
>>>>> >>>>>>> obviously misunderstand something about how str_detect works
>>>>> >>>>>>>
>>>>> >>>>>>> library(stringr)
>>>>> >>>>>>>
>>>>> >>>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>>>>> >>>>>search
>>>>> >>>>>>>                                      # must be a vector, not
>>>>> >>>>>multiple
>>>>> >>>>>>>                                      # columns
>>>>> >>>>>>>
>>>>> >>>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>>>>> >>>>>>>
>>>>> >>>>>>> str_detect(zz[,2], alarm.words)      # error, length of
>>>>> >>>alarm.words
>>>>> >>>>>>>                                      # is less than the number
>>>>> of
>>>>> >>>>>>>                                      # rows I am using for the
>>>>> >>>>>>>                                      # comparison
>>>>> >>>>>>>
>>>>> >>>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>>>>> >>>>>>> length(alarm.words)                  # confining nrows
>>>>> >>>>>>>                                      # to the length of
>>>>> >>>alarm.words
>>>>> >>>>>>>
>>>>> >>>>>>> str_detect(zz, alarm.words)          # obviously not right
>>>>> >>>>>>>
>>>>> >>>>>>> # maybe I need apply() ?
>>>>> >>>>>>> my.f <- function(x){str_detect(x, alarm.words)}
>>>>> >>>>>>>
>>>>> >>>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>>>>> >>>>>>>                            # between alarm.words and that
>>>>> >>>>>>>                            # in which I am searching for
>>>>> >>>>>>>                            # matching strings
>>>>> >>>>>>>
>>>>> >>>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>>>>> >>>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>>>>> >>>>>>>                            # rows of the dataframe
>>>>> >>>>>>>
>>>>> >>>>>>>
>>>>> >>>>>>> # perhaps %in% could do the job?
>>>>> >>>>>>>
>>>>> >>>>>>> Appreciate any advice.
>>>>> >>>>>>>
>>>>> >>>>>>> --Chris Ryan
>>>>> >>>>>>>
>>>>> >>>>>>> ______________________________________________
>>>>> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>>>> see
>>>>> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> >>>>>>> PLEASE do read the posting guide
>>>>> >>>>>http://www.R-project.org/posting-guide.html
>>>>> >>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>> >>>>>>
>>>>> >>>>>> ______________________________________________
>>>>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> >>>>>> PLEASE do read the posting guide
>>>>> >>>>>http://www.R-project.org/posting-guide.html
>>>>> >>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>> >>>>>
>>>>> >>>>>______________________________________________
>>>>> >>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> >>>>>PLEASE do read the posting guide
>>>>> >>>>>http://www.R-project.org/posting-guide.html
>>>>> >>>>>and provide commented, minimal, self-contained, reproducible code.
>>>>> >>>>
>>>>> >>
>>>>> >
>>>>> > ______________________________________________
>>>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> > PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>> guide.html
>>>>> > and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>> ---
>>>> This email has been checked for viruses by Avast antivirus software.
>>>> https://www.avast.com/antivirus
>>>>


From bgunter.4567 at gmail.com  Sat Jul 11 16:47:22 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 11 Jul 2015 07:47:22 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
Message-ID: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>

I noticed the following:

> strsplit("red green","\\b")
[[1]]
[1] "r" "e" "d" " " "g" "r" "e" "e" "n"

> strsplit("red green","\\W")
[[1]]
[1] "red"   "green"

I would have thought that "\\b" should give what "\\W" did. Note that:

> grep("\\bred\\b","red green")
[1] 1
## as expected

Does strsplit use a different regex engine than grep()? Or more
likely, what am I misunderstanding?

Thanks.

Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


From markleeds2 at gmail.com  Sat Jul 11 17:38:56 2015
From: markleeds2 at gmail.com (Mark Leeds)
Date: Sat, 11 Jul 2015 11:38:56 -0400
Subject: [R] R 3.1.2 : arima.sim(model=list(ma=0.5), n=250,
 innov=rnorm(250, mean=0, sd=0.1)) versus arima.sim(model=list(ma=0.5), n=250,
 mean=0, sd=0.1) => only the first element is not identical !
In-Reply-To: <55A0C9C8.4030605@gmail.com>
References: <55A0C9C8.4030605@gmail.com>
Message-ID: <CAHz+bWZ7Mty7_YiCXqGTfARVuyjo4_PfyqQXLN4cYdvBHSoCxg@mail.gmail.com>

Hi Fabian: I think one would say that that is not a bug. I looked at the
details of arima.sim ( using debug(arima.sim) )
 and there are two different series that are created inside the function.
one is called innov and the other is start.innov. start.innov is
used to create a burn in period for the constructed series.

in your test1 call, you are not  supplying arguments for what should be
used for the innovations associated with start.innov which is used for the
burn in period. So, arima.sim  uses the defaults of mean = 0 and sd = 1.0.
For your test2 call, you do provide them ( so they are used for both innov
and start.innov )  and you use  sd = 0.1 So, for test2,  the values  for
the burn in period end up being different from the ones in test1.

Below, I made a test3 that can be used to get the same values as test2. In
short, by specifiying the innov call EXACTLY in test1, you're letting
arima.sim use the default arguments for the start.innov call so that's why
they're different.


#====================================================================================================================
 ##undebug(arima.sim)

set.seed(123);
test1 <- arima.sim(model=list(ma=0.5), n =
250,innov=rnorm(250,mean=0,sd=0.1))
print(head(test1))

set.seed(123);
test2 <- arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1)
print(head(test2))

set.seed(123);
test3 <- arima.sim(model=list(ma=0.5), innov = rnorm(250,mean=0, sd=0.1), n
= 250, mean=0, sd=0.1)
print(head(test3))

On Sat, Jul 11, 2015 at 3:46 AM, Fabien Tarrade <fabien.tarrade at gmail.com>
wrote:

> Dear all,
>
> When doing a DataCamp tutorial with R I find the following observation
> that using 2 different syntax for "arima.sim" give different answer for the
> first element
>
> If I use the the function using the list of argument describe in the help
> manual :
> arima.sim(model=list(ma=0.5),n=250,innov=rnorm(250,mean=0,sd=0.1))
>
> or if I use the following syntax use in a DataCamp example :
>
> arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1) it is accepted by
> DataCamp
>
> I don't find exactly the same results. The reason is that even if the seed
> is the same in both cases the first element is not identical while it
> should be (it doesn't mean that the results is wrong, maybe for the first
> element the seed is not propagated correctly)
>
> here the results of the difference using the same seed (only the first
> element is different using the 2 different syntaxes) :
>
>   [1] -0.252214  0.000000  0.000000  0.000000  0.000000  0.000000
> 0.000000  0.000000  0.000000  0.000000
>  [11]  0.000000  0.000000  0.000000  0.000000  0.000000 0.000000
> 0.000000  0.000000  0.000000  0.000000
>
>
> here the code to reproduce this feature :
>
> set.seed(123);
> test1 <- 0.05 +
> arima.sim(model=list(ma=0.5),n=250,innov=rnorm(250,mean=0,sd=0.1))
> set.seed(123);
> test2 <- 0.05 + arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1)
>
> test1-test2
>
> I am using R 3.1.2 GUI 1.65 Mavericks build (6833) on Mac (I guess arima
> come with stats which is included in R (?))
> The DataCamp team ask me to report to you about this observation on this
> mailing list. If you want me to fill a bug report some R bug tracking
> system, let me know
>
> Please tell me if this is the wrong list and which other information do
> you need from R and how to get then (compiler, version of some R packages
> ...)
>
>
> Hope this help
> Thanks
> Cheers
> Fabien
> --
> Dr Fabien Tarrade
>
> Quantitative Analyst - Data Scientist - Researcher
>
> Senior data analyst specialised in the modelling, processing and
> statistical treatment of data.
> PhD in Physics, 10 years of experience as researcher at the forefront of
> international scientific research.
> Fascinated by finance and data modelling.
>
> Geneva, Switzerland
>
> Email : contact at fabien-tarrade.eu <mailto:contact at fabien-tarrade.eu>
> Phone : www.fabien-tarrade.eu <http://www.fabien-tarrade.eu>
> Phone : +33 (0)6 14 78 70 90
>
> LinkedIn <http://ch.linkedin.com/in/fabientarrade/> Twitter <
> https://twitter.com/fabtar> Google <
> https://plus.google.com/+FabienTarradeProfile/posts> Facebook <
> https://www.facebook.com/fabien.tarrade.eu> Google
> <skype:fabtarhiggs?call> Xing <https://www.xing.com/profile/Fabien_Tarrade
> >
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Sat Jul 11 17:52:13 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sat, 11 Jul 2015 08:52:13 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
Message-ID: <DAE2816E-2500-4651-B9FE-D419A9D3461F@dcn.davis.CA.us>

"\\b" is a zero length match. strsplit seems to chop at least one character off the beginning of the string if it sees a match, and then it looks at the shortened string that remains and repeats.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 11, 2015 7:47:22 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>I noticed the following:
>
>> strsplit("red green","\\b")
>[[1]]
>[1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>
>> strsplit("red green","\\W")
>[[1]]
>[1] "red"   "green"
>
>I would have thought that "\\b" should give what "\\W" did. Note that:
>
>> grep("\\bred\\b","red green")
>[1] 1
>## as expected
>
>Does strsplit use a different regex engine than grep()? Or more
>likely, what am I misunderstanding?
>
>Thanks.
>
>Bert
>
>
>Bert Gunter
>
>"Data is not information. Information is not knowledge. And knowledge
>is certainly not wisdom."
>   -- Clifford Stoll
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bbolker at gmail.com  Sat Jul 11 17:40:17 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 11 Jul 2015 15:40:17 +0000
Subject: [R] A question about nlmer in lme4 package
References: <115421383.313866.1436610113162.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <loom.20150711T173454-724@post.gmane.org>

Patty Haaem via R-help <r-help <at> r-project.org> writes:


> Dear all,I have studied ?Mixed models in R using the lme4 package
> Part 6: Nonlinear mixed models? by?Douglas Bates.?In this tutorial,
> there are some codes to fit nonlinear mixed models for Theoph
> data. The codes are as [follows:]

Th. start <- c(lKe = -2.5, lKa = 0.5 , lCl = -3)
nm1 <- nlmer ( conc ~ SSfol(Dose , Time ,lKe , lKa , lCl) ~
    0+ lKe+lKa+lCl +(0+ lKe| Subject )+(0+ lKa| Subject )+ 
    (0+ lCl| Subject ), nAGQ =0, Theoph , start = Th.start , verbose = TRUE )


> ? I want to add a covariate (like age) to CL parameter. How should I
> modify above codes? what's more, how ?are selected initial
> values?(lKe = -2.5, lKa = 0.5 , lCl = -3)??  Thanks in advanceElham

  You'll do better asking this question on r-sig-mixed-models at r-project.org
The short answer (to the first question) is that it's not easy, but it
can be done, see e.g.

http://stackoverflow.com/questions/15141952/nlmer-longitudinal-data
http://rpubs.com/bbolker/3423

  Ben Bolker

From bgunter.4567 at gmail.com  Sat Jul 11 18:19:47 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 11 Jul 2015 09:19:47 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <DAE2816E-2500-4651-B9FE-D419A9D3461F@dcn.davis.CA.us>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<DAE2816E-2500-4651-B9FE-D419A9D3461F@dcn.davis.CA.us>
Message-ID: <CAGxFJbQ37uahTNwYTY5_bot-RTzWeg4farCExfghUc7VL5RQmw@mail.gmail.com>

Thanks Jeff. That doesn't explain it for me. Could you go through the
algorithm a step at a time to show why it splits at the individual
characters rather than the words, perhaps privately. Feel free to
refuse, as I'm sure you have better things to do.

-- Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 11, 2015 at 8:52 AM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> "\\b" is a zero length match. strsplit seems to chop at least one character off the beginning of the string if it sees a match, and then it looks at the shortened string that remains and repeats.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 11, 2015 7:47:22 AM PDT, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>I noticed the following:
>>
>>> strsplit("red green","\\b")
>>[[1]]
>>[1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>>
>>> strsplit("red green","\\W")
>>[[1]]
>>[1] "red"   "green"
>>
>>I would have thought that "\\b" should give what "\\W" did. Note that:
>>
>>> grep("\\bred\\b","red green")
>>[1] 1
>>## as expected
>>
>>Does strsplit use a different regex engine than grep()? Or more
>>likely, what am I misunderstanding?
>>
>>Thanks.
>>
>>Bert
>>
>>
>>Bert Gunter
>>
>>"Data is not information. Information is not knowledge. And knowledge
>>is certainly not wisdom."
>>   -- Clifford Stoll
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Sat Jul 11 20:05:12 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 11 Jul 2015 11:05:12 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
Message-ID: <EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>


On Jul 11, 2015, at 7:47 AM, Bert Gunter wrote:

> I noticed the following:
> 
>> strsplit("red green","\\b")
> [[1]]
> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"

After reading the ?regex help page, I didn't understand why `\b` would split within sequences of "word"-characters, either. I expected this to be the result:

[[1]]
[1] "red"  " "  "green"

There is a warning in that paragraph: "(The interpretation of ?word? depends on the locale and implementation.)"

I got the expected result with only one of "\\>" and "\\<"

> strsplit("red green","\\<")
[[1]]
[1] "r" "e" "d" " " "g" "r" "e" "e" "n"

> strsplit("red green","\\>")
[[1]]
[1] "red"    " green"

The result with "\\<" seems decidedly unexpected.

I'm wondered if the "original" regex documentation uses the same language as the R help page. So I went to the cited website and find:
=======
An assertion-character can be any of the following:

	? < ? Beginning of word
	? > ? End of word
	? b ? Word boundary
	? B ? Non-word boundary
	? d ? Digit character (equivalent to [[:digit:]])
	? D ? Non-digit character (equivalent to [^[:digit:]])
	? s ? Space character (equivalent to [[:space:]])
	? S ? Non-space character (equivalent to [^[:space:]])
	? w ? Word character (equivalent to [[:alnum:]_])
	? W ? Non-word character (equivalent to [^[:alnum:]_])
========

The word-"word" appears nowhere else on that page.


>> strsplit("red green","\\W")
> [[1]]
> [1] "red"   "green"

`\W` matches the byte-width non-word characters. So the " "-character would be discarded.

> 
> I would have thought that "\\b" should give what "\\W" did. Note that:
> 
>> grep("\\bred\\b","red green")
> [1] 1
> ## as expected
> 
> Does strsplit use a different regex engine than grep()? Or more
> likely, what am I misunderstanding?
> 
> Thanks.
> 
> Bert
> 
> 


David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Sat Jul 11 20:14:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 11 Jul 2015 11:14:53 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
Message-ID: <8232A5B2-8ADD-460D-8D46-90530A18A0C5@comcast.net>


On Jul 11, 2015, at 11:05 AM, David Winsemius wrote:

> 
> On Jul 11, 2015, at 7:47 AM, Bert Gunter wrote:
> 
>> I noticed the following:
>> 
>>> strsplit("red green","\\b")
>> [[1]]
>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
> 
> After reading the ?regex help page, I didn't understand why `\b` would split within sequences of "word"-characters, either. I expected this to be the result:
> 
> [[1]]
> [1] "red"  " "  "green"
> 
> There is a warning in that paragraph: "(The interpretation of ?word? depends on the locale and implementation.)"
> 
> I got the expected result with only one of "\\>" and "\\<"
> 
>> strsplit("red green","\\<")
> [[1]]
> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
> 
>> strsplit("red green","\\>")
> [[1]]
> [1] "red"    " green"
> 
> The result with "\\<" seems decidedly unexpected.
> 
> I'm wondered if the "original" regex documentation uses the same language as the R help page. So I went to the cited website and find:
> =======
> An assertion-character can be any of the following:
> 
> 	? < ? Beginning of word
> 	? > ? End of word
> 	? b ? Word boundary
> 	? B ? Non-word boundary
> 	? d ? Digit character (equivalent to [[:digit:]])
> 	? D ? Non-digit character (equivalent to [^[:digit:]])
> 	? s ? Space character (equivalent to [[:space:]])
> 	? S ? Non-space character (equivalent to [^[:space:]])
> 	? w ? Word character (equivalent to [[:alnum:]_])
> 	? W ? Non-word character (equivalent to [^[:alnum:]_])
> ========
> 
> The word-"word" appears nowhere else on that page.
> 

This page:

http://www.regular-expressions.info/wordboundaries.html

 implies that naked boundaries were not expected to be use and that "\B" and "\b" were expected to be "flanking" patterns with the real "meat" either sandwiched between them or perhaps at either end.

   > strsplit( "     red green   blue", split="\\b   \\b")
[[1]]
[1] "     red green" "blue"  


So perhaps there is an implicit "any-word" that follows the "\\b" assertion?

> strsplit( "redgreen", split="\\bgreen")
[[1]]
[1] "redgreen"

> strsplit( "redgreen", split="green\\b")
[[1]]
[1] "red"


-- 
David.
> 
>>> strsplit("red green","\\W")
>> [[1]]
>> [1] "red"   "green"
> 
> `\W` matches the byte-width non-word characters. So the " "-character would be discarded.
> 
>> 
>> I would have thought that "\\b" should give what "\\W" did. Note that:
>> 
>>> grep("\\bred\\b","red green")
>> [1] 1
>> ## as expected
>> 
>> Does strsplit use a different regex engine than grep()? Or more
>> likely, what am I misunderstanding?
>> 
>> Thanks.
>> 
>> Bert
>> 
>> 
> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From xie at yihui.name  Sat Jul 11 21:01:59 2015
From: xie at yihui.name (Yihui Xie)
Date: Sat, 11 Jul 2015 14:01:59 -0500
Subject: [R] Trellis Plots: translating lattice xyplot() to ggplot()
	[RESOLVED]
In-Reply-To: <alpine.LNX.2.11.1507110657310.25203@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
Message-ID: <CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>

I guess you didn't tell us you were compiling the document with dvips
(BTW, I'm surprised that dvips is still alive today...), otherwise the
solution would be simply to use the postscript device instead of the
default pdf device (i.e. use the chunk option dev='postscript').
Hopefully you learned some lessons on what a reproducible example
means.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Sat, Jul 11, 2015 at 9:03 AM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> On Fri, 10 Jul 2015, Yihui Xie wrote:
>
>> Your LyX example has two problems:
>
>
> Yihui, et al.:
>
>   All fixed now. After adding the specific libraries the chuck calling
> xyplot() to produce the graphic caused an error when I tried to generate a
> dvips preview:
>
> LaTeX Error: File 'figure/unnamed-chunk-7-1' not found.
>
> I could not locate the file with any of these extensions:
> .eps,.ps,.eps.gz,.ps.gz,.eps.Z
>
> Which is what sent me down the path of specifying pdf() and dev.off().
>
>   However, compiling with pdflatex produces the graphic in the document. All
> part of being brand new to using knitr.
>
> Thank you all very much,
>
> Rich


From jdnewmil at dcn.davis.ca.us  Sat Jul 11 21:47:29 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 11 Jul 2015 12:47:29 -0700 (PDT)
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <8232A5B2-8ADD-460D-8D46-90530A18A0C5@comcast.net>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
	<8232A5B2-8ADD-460D-8D46-90530A18A0C5@comcast.net>
Message-ID: <alpine.BSF.2.00.1507111222290.58065@pedal.dcn.davis.ca.us>

I was going to recommend the regular-expression.info website, but you got 
to it first.

I tried looking at the source, but it is kind of dense. I think stepping 
through the code below may illustrate why the zero width match returned 
from "\\b" cannot be allowed as-is or the strsplit algorithm will go
into an infinite loop.

slowstrsplit1 <- function( x, split ) {
   result <- list()
   right <- x
   while (  0 < nchar( right )
         && 0 <= ( idx <- regexpr( split, right ) )
         ) {
     # index of beginning of pattern
     patidx <- c( idx )
     left <- substr( right, 1, if ( 1==patidx ) { 1 } else { patidx-1 } )
     # number of matched characters in the string corresponding
     # to the pattern
     patlen <- attr( idx, "match.length" )
     if ( 0 == patlen ) {
       patlen <- 1
     }
     # if patlen is allowed to be zero in the following line then the
     # loop will never end
     rightidx <- patidx + patlen
     # remember the left chunk
     result <- append( result, left )
     right <- substr( right, rightidx, nchar( right ) )
   }
   if ( 0 != nchar( right ) ) {
     result <- append( result, right )
   }
   unlist( result )
}

slowstrsplit <- function( x, split ) {
   lapply( x, function( x1 ) { slowstrsplit1( x1, split ) } )
}

# test
teststr <- "red green"
pat <- "\\b"
slowstrsplit( teststr, pat )
pat <- " "
slowstrsplit( teststr, pat )

On Sat, 11 Jul 2015, David Winsemius wrote:

>
> On Jul 11, 2015, at 11:05 AM, David Winsemius wrote:
>
>>
>> On Jul 11, 2015, at 7:47 AM, Bert Gunter wrote:
>>
>>> I noticed the following:
>>>
>>>> strsplit("red green","\\b")
>>> [[1]]
>>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>>
>> After reading the ?regex help page, I didn't understand why `\b` would split within sequences of "word"-characters, either. I expected this to be the result:
>>
>> [[1]]
>> [1] "red"  " "  "green"
>>
>> There is a warning in that paragraph: "(The interpretation of ?word? depends on the locale and implementation.)"
>>
>> I got the expected result with only one of "\\>" and "\\<"
>>
>>> strsplit("red green","\\<")
>> [[1]]
>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>>
>>> strsplit("red green","\\>")
>> [[1]]
>> [1] "red"    " green"
>>
>> The result with "\\<" seems decidedly unexpected.
>>
>> I'm wondered if the "original" regex documentation uses the same language as the R help page. So I went to the cited website and find:
>> =======
>> An assertion-character can be any of the following:
>>
>> 	? < ? Beginning of word
>> 	? > ? End of word
>> 	? b ? Word boundary
>> 	? B ? Non-word boundary
>> 	? d ? Digit character (equivalent to [[:digit:]])
>> 	? D ? Non-digit character (equivalent to [^[:digit:]])
>> 	? s ? Space character (equivalent to [[:space:]])
>> 	? S ? Non-space character (equivalent to [^[:space:]])
>> 	? w ? Word character (equivalent to [[:alnum:]_])
>> 	? W ? Non-word character (equivalent to [^[:alnum:]_])
>> ========
>>
>> The word-"word" appears nowhere else on that page.
>>
>
> This page:
>
> http://www.regular-expressions.info/wordboundaries.html
>
> implies that naked boundaries were not expected to be use and that "\B" and "\b" were expected to be "flanking" patterns with the real "meat" either sandwiched between them or perhaps at either end.
>
>   > strsplit( "     red green   blue", split="\\b   \\b")
> [[1]]
> [1] "     red green" "blue"
>
>
> So perhaps there is an implicit "any-word" that follows the "\\b" assertion?
>
>> strsplit( "redgreen", split="\\bgreen")
> [[1]]
> [1] "redgreen"
>
>> strsplit( "redgreen", split="green\\b")
> [[1]]
> [1] "red"
>
>
> -- 
> David.
>>
>>>> strsplit("red green","\\W")
>>> [[1]]
>>> [1] "red"   "green"
>>
>> `\W` matches the byte-width non-word characters. So the " "-character would be discarded.
>>
>>>
>>> I would have thought that "\\b" should give what "\\W" did. Note that:
>>>
>>>> grep("\\bred\\b","red green")
>>> [1] 1
>>> ## as expected
>>>
>>> Does strsplit use a different regex engine than grep()? Or more
>>> likely, what am I misunderstanding?
>>>
>>> Thanks.
>>>
>>> Bert
>>>
>>>
>>
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From jawadhussain at vcomsats.edu.pk  Sat Jul 11 20:44:42 2015
From: jawadhussain at vcomsats.edu.pk (Syed Jawad Hussain Shahzad)
Date: Sun, 12 Jul 2015 02:44:42 +0800
Subject: [R] Rolling Window Granger Causality
Message-ID: <CAOb7=cWGV5qvvJTak9z7DX_4PDn8uCqhCw3DRZN=+8aF9pQGyg@mail.gmail.com>

Dear Sir, Hi

I am new to R and want some help on the subject analysis. I need help
to apply causality analysis (available in Package 'VARS') with a
per-define rolling window (like rollapply in Package 'Zoo').

Best Wishes
Jawad


From dagmar.juranka at gmail.com  Sat Jul 11 21:21:19 2015
From: dagmar.juranka at gmail.com (=?UTF-8?Q?Dagmar_Jurankov=C3=A1?=)
Date: Sat, 11 Jul 2015 21:21:19 +0200
Subject: [R] data$variable=factor(....) <NA> <NA> <NA>
Message-ID: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>

Hello everybody, I have a problem with R.


I uploaded a questionnaire saved as csv into R and I tried to test
independence between two variables.



data <- read.csv("C:/Users/Me/Desktop/data.csv")>   View(data)> df =
read.csv("C:/Users/Me/Desktop/data.csv")> ls()
[1] "df"     "data"> attributes(data$gender)
$levels
[1] " F" " M" "F"  "M"

$class
[1] "factor"


I changed my variable "gender" into a factor using:


data$gender=factor(data$gender, levels=c(1:2), labels= c( "F", "M"),
exclude= NA, nmax= NA).


Then I wrote data$gender and the only thing i got was:


[1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
<NA> <NA> <NA> <NA> <NA> <NA>

[21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
<NA> <NA> <NA> <NA> <NA> <NA>

[41] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
<NA> <NA> <NA> <NA> <NA> <NA>

[61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>

Levels: F M


Does anybody know why?


-My csv doc in the column gender is filled out properly. (M=Male, F= Female)

-My imported dataset in R is complete (all values)


! I have done this with a different excel document and it worked out
without any problems. I am really clueless. I cant go further and compare
the variables and do t-tests without this working.


Could someone please help me out?

Thank you.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Jul 11 22:19:35 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 11 Jul 2015 13:19:35 -0700 (PDT)
Subject: [R] data$variable=factor(....) <NA> <NA> <NA>
In-Reply-To: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
References: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1507111302190.58065@pedal.dcn.davis.ca.us>

Well, you can help yourself on this list if you stop letting your email 
client determine the format (HTML in this case) that you use since that 
format gets corrupted on this mailing list leading to frequent 
misunderstandings. Learn how to make your email client send plain text 
format.

If you go back to your first line and look at str(data), you will see that 
read.csv automatically converted the gender column to a factor for you. 
In your later attempt to convert it you thought it would draw on the 
underlying integer values when it "acts" like character data so none of 
the specified levels ("1" or "2") were found in it.

If you want to control the levels used in the factor (as I usually prefer 
to do) then use either the as.is=TRUE or stringsAsFactors=FALSE parameter 
to the read.csv function to make sure no factors are automatically 
created. Then specify character values for your levels instead of 
second-guessing R.

Note that there is a bit of an art to reading the help files, as in:

?read.csv

that you should start to practice. When you do read that help file, you 
will find that there are a lot of parameters to the "read.table" function, 
and rather fewer specified for the read.csv definition. The reason is that 
the read.csv function simply calls the read.table function with certain 
parameters forced to specific values. You can set any of the other 
parameters that read.table expects in your call to read.csv and they will 
be passed on to read.table.

Oh, and one other thing: functions are quite similar to data objects in R, 
and there is a function called "data" that comes with R. While defining 
your own object called "data" works in this case, it is good practice to 
learn to not re-use object names like that since it can make reading your 
code confusing at the very least.

On Sat, 11 Jul 2015, Dagmar Jurankov? wrote:

> Hello everybody, I have a problem with R.
>
>
> I uploaded a questionnaire saved as csv into R and I tried to test
> independence between two variables.
>
>
>
> data <- read.csv("C:/Users/Me/Desktop/data.csv")>   View(data)> df =
> read.csv("C:/Users/Me/Desktop/data.csv")> ls()
> [1] "df"     "data"> attributes(data$gender)
> $levels
> [1] " F" " M" "F"  "M"
>
> $class
> [1] "factor"
>
>
> I changed my variable "gender" into a factor using:
>
>
> data$gender=factor(data$gender, levels=c(1:2), labels= c( "F", "M"),
> exclude= NA, nmax= NA).
>
>
> Then I wrote data$gender and the only thing i got was:
>
>
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [41] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>
> Levels: F M
>
>
> Does anybody know why?
>
>
> -My csv doc in the column gender is filled out properly. (M=Male, F= Female)
>
> -My imported dataset in R is complete (all values)
>
>
> ! I have done this with a different excel document and it worked out
> without any problems. I am really clueless. I cant go further and compare
> the variables and do t-tests without this working.
>
>
> Could someone please help me out?
>
> Thank you.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From ruipbarradas at sapo.pt  Sat Jul 11 22:37:58 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Sat, 11 Jul 2015 21:37:58 +0100
Subject: [R] data$variable=factor(....) <NA> <NA> <NA>
In-Reply-To: <alpine.BSF.2.00.1507111302190.58065@pedal.dcn.davis.ca.us>
References: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
	<alpine.BSF.2.00.1507111302190.58065@pedal.dcn.davis.ca.us>
Message-ID: <55A17EA6.6070304@sapo.pt>

Hello,

Just to add that the op's data$gender has 4 levels, not just 2. So it 
would be better to remove the leading spaces from " F" and " M", by 
using ?sub or ?gsub.

Hope this helps,

Rui Barradas

Em 11-07-2015 21:19, Jeff Newmiller escreveu:
> Well, you can help yourself on this list if you stop letting your email
> client determine the format (HTML in this case) that you use since that
> format gets corrupted on this mailing list leading to frequent
> misunderstandings. Learn how to make your email client send plain text
> format.
>
> If you go back to your first line and look at str(data), you will see
> that read.csv automatically converted the gender column to a factor for
> you. In your later attempt to convert it you thought it would draw on
> the underlying integer values when it "acts" like character data so none
> of the specified levels ("1" or "2") were found in it.
>
> If you want to control the levels used in the factor (as I usually
> prefer to do) then use either the as.is=TRUE or stringsAsFactors=FALSE
> parameter to the read.csv function to make sure no factors are
> automatically created. Then specify character values for your levels
> instead of second-guessing R.
>
> Note that there is a bit of an art to reading the help files, as in:
>
> ?read.csv
>
> that you should start to practice. When you do read that help file, you
> will find that there are a lot of parameters to the "read.table"
> function, and rather fewer specified for the read.csv definition. The
> reason is that the read.csv function simply calls the read.table
> function with certain parameters forced to specific values. You can set
> any of the other parameters that read.table expects in your call to
> read.csv and they will be passed on to read.table.
>
> Oh, and one other thing: functions are quite similar to data objects in
> R, and there is a function called "data" that comes with R. While
> defining your own object called "data" works in this case, it is good
> practice to learn to not re-use object names like that since it can make
> reading your code confusing at the very least.
>
> On Sat, 11 Jul 2015, Dagmar Jurankov? wrote:
>
>> Hello everybody, I have a problem with R.
>>
>>
>> I uploaded a questionnaire saved as csv into R and I tried to test
>> independence between two variables.
>>
>>
>>
>> data <- read.csv("C:/Users/Me/Desktop/data.csv")>   View(data)> df =
>> read.csv("C:/Users/Me/Desktop/data.csv")> ls()
>> [1] "df"     "data"> attributes(data$gender)
>> $levels
>> [1] " F" " M" "F"  "M"
>>
>> $class
>> [1] "factor"
>>
>>
>> I changed my variable "gender" into a factor using:
>>
>>
>> data$gender=factor(data$gender, levels=c(1:2), labels= c( "F", "M"),
>> exclude= NA, nmax= NA).
>>
>>
>> Then I wrote data$gender and the only thing i got was:
>>
>>
>> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>> <NA> <NA> <NA> <NA> <NA> <NA>
>>
>> [21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>> <NA>
>> <NA> <NA> <NA> <NA> <NA> <NA>
>>
>> [41] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>> <NA>
>> <NA> <NA> <NA> <NA> <NA> <NA>
>>
>> [61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>>
>> Levels: F M
>>
>>
>> Does anybody know why?
>>
>>
>> -My csv doc in the column gender is filled out properly. (M=Male, F=
>> Female)
>>
>> -My imported dataset in R is complete (all values)
>>
>>
>> ! I have done this with a different excel document and it worked out
>> without any problems. I am really clueless. I cant go further and compare
>> the variables and do t-tests without this working.
>>
>>
>> Could someone please help me out?
>>
>> Thank you.
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                        Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sat Jul 11 22:41:10 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 11 Jul 2015 13:41:10 -0700 (PDT)
Subject: [R] Rolling Window Granger Causality
In-Reply-To: <CAOb7=cWGV5qvvJTak9z7DX_4PDn8uCqhCw3DRZN=+8aF9pQGyg@mail.gmail.com>
References: <CAOb7=cWGV5qvvJTak9z7DX_4PDn8uCqhCw3DRZN=+8aF9pQGyg@mail.gmail.com>
Message-ID: <alpine.BSF.2.00.1507111321340.58065@pedal.dcn.davis.ca.us>

This is a rather broad request. If you are looking for help understanding 
these topics, you should probably ask in a forum where statistical theory 
is on topic (e.g. stats.stackexchange.com), since in this forum you should 
already have a good idea of what algorithms you want to apply and in what 
sequence and be asking about how to get R to do that.  In practically all 
cases you should have some reproducible (including sample data) code that 
illustrates how far you have come, and specific questions about how to 
link the dots. Sample data is crucial.

I also highly recommend reading the vignettes and documentation associated 
with each the packages you plan to use. For example, functions in the 
"zoo" (note that capitalization matters) package can be used to create 
rolling averages, and then those results can be given to functions in 
another package (such as "vars"... note again that capitalization makes a 
difference) to analyze the original data along with the zoo results. 
While there may be experts in both packages you name lurking in this 
forum, often people may only be familiar with one or none of the packages 
you name but they may be able to interpret why something you are trying to 
do is not working if they can reproduce what you have already done. There 
are thousands of different R packages, but not nearly so many active 
helpers on R-help.

Also, you can use the "findFn" function in the "sos" package to search for 
"granger causality" among all available R packages on CRAN and find that 
there are several packages that match, so the "vars" package is not your 
only option.

Try reading [1] to get a feel for how to communicate your problem 
effectively.

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

On Sun, 12 Jul 2015, Syed Jawad Hussain Shahzad wrote:

> Dear Sir, Hi
>
> I am new to R and want some help on the subject analysis. I need help
> to apply causality analysis (available in Package 'VARS') with a
> per-define rolling window (like rollapply in Package 'Zoo').
>
> Best Wishes
> Jawad
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From peter.langfelder at gmail.com  Sat Jul 11 22:46:09 2015
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Sat, 11 Jul 2015 13:46:09 -0700
Subject: [R] data$variable=factor(....) <NA> <NA> <NA>
In-Reply-To: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
References: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
Message-ID: <CA+hbrhVqYQLTd9qG6jLUzwqjD29G67WLyYSzMRg5HGkN_RbGgQ@mail.gmail.com>

There are two issues here. First, your original factor seems to have 4
levels: " F", " M", "F", "M". Note the extra space in front of the
first two F and M. You may want to fix that first:

gender.fixed = sub(" ", "", as.character(data$gender))

Check that everything is correct by typing

table(gender.fixed)

or

table(data$gender, gender.fixed)

Then you can convert the fixed gender back to a factor, but pay
attention to the levels:

data$gender = factor(gender.fixed, levels = c("F", "M"))

Hopefully this works,

Peter

On Sat, Jul 11, 2015 at 12:21 PM, Dagmar Jurankov?
<dagmar.juranka at gmail.com> wrote:
> Hello everybody, I have a problem with R.
>
>
> I uploaded a questionnaire saved as csv into R and I tried to test
> independence between two variables.
>
>
>
> data <- read.csv("C:/Users/Me/Desktop/data.csv")>   View(data)> df =
> read.csv("C:/Users/Me/Desktop/data.csv")> ls()
> [1] "df"     "data"> attributes(data$gender)
> $levels
> [1] " F" " M" "F"  "M"
>
> $class
> [1] "factor"
>
>
> I changed my variable "gender" into a factor using:
>
>
> data$gender=factor(data$gender, levels=c(1:2), labels= c( "F", "M"),
> exclude= NA, nmax= NA).
>
>
> Then I wrote data$gender and the only thing i got was:
>
>
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [41] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>
> Levels: F M
>
>
> Does anybody know why?
>
>
> -My csv doc in the column gender is filled out properly. (M=Male, F= Female)
>
> -My imported dataset in R is complete (all values)
>
>
> ! I have done this with a different excel document and it worked out
> without any problems. I am really clueless. I cant go further and compare
> the variables and do t-tests without this working.
>
>
> Could someone please help me out?
>
> Thank you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Sun Jul 12 00:07:19 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 11 Jul 2015 15:07:19 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
Message-ID: <CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>

David/Jeff:

Thank you both.

You seem to confirm that my observation of an "infelicity" in
strsplit() is real. That is most helpful.

I found nothing in David's message 2 code that was surprising. That
is, the splits shown conform to what I would expect from "\\b" . But
not to what I originally showed and David enlarged upon in his first
message. I still don't really get why a split should occur at every
letter.

Jeff may very well have found the explanation, but I have not gone
through his code.

If the infelicities noted (are there more?) by David and me are not
really bugs -- and I would be frankly surprised if they were -- I
would suggest that perhaps they deserve mention in the strsplit() man
page. Something to the effect that "\b and \< should not be used as
split characters..." .

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 11, 2015 at 11:05 AM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
> On Jul 11, 2015, at 7:47 AM, Bert Gunter wrote:
>
>> I noticed the following:
>>
>>> strsplit("red green","\\b")
>> [[1]]
>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>
> After reading the ?regex help page, I didn't understand why `\b` would split within sequences of "word"-characters, either. I expected this to be the result:
>
> [[1]]
> [1] "red"  " "  "green"
>
> There is a warning in that paragraph: "(The interpretation of ?word? depends on the locale and implementation.)"
>
> I got the expected result with only one of "\\>" and "\\<"
>
>> strsplit("red green","\\<")
> [[1]]
> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>
>> strsplit("red green","\\>")
> [[1]]
> [1] "red"    " green"
>
> The result with "\\<" seems decidedly unexpected.
>
> I'm wondered if the "original" regex documentation uses the same language as the R help page. So I went to the cited website and find:
> =======
> An assertion-character can be any of the following:
>
>         ? < ? Beginning of word
>         ? > ? End of word
>         ? b ? Word boundary
>         ? B ? Non-word boundary
>         ? d ? Digit character (equivalent to [[:digit:]])
>         ? D ? Non-digit character (equivalent to [^[:digit:]])
>         ? s ? Space character (equivalent to [[:space:]])
>         ? S ? Non-space character (equivalent to [^[:space:]])
>         ? w ? Word character (equivalent to [[:alnum:]_])
>         ? W ? Non-word character (equivalent to [^[:alnum:]_])
> ========
>
> The word-"word" appears nowhere else on that page.
>
>
>>> strsplit("red green","\\W")
>> [[1]]
>> [1] "red"   "green"
>
> `\W` matches the byte-width non-word characters. So the " "-character would be discarded.
>
>>
>> I would have thought that "\\b" should give what "\\W" did. Note that:
>>
>>> grep("\\bred\\b","red green")
>> [1] 1
>> ## as expected
>>
>> Does strsplit use a different regex engine than grep()? Or more
>> likely, what am I misunderstanding?
>>
>> Thanks.
>>
>> Bert
>>
>>
>
>
> David Winsemius
> Alameda, CA, USA
>


From dwinsemius at comcast.net  Sun Jul 12 00:31:27 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 11 Jul 2015 15:31:27 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
	<CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>
Message-ID: <BD27075F-7493-4B2D-8D53-5C9700DE97E4@comcast.net>


On Jul 11, 2015, at 3:07 PM, Bert Gunter wrote:

> David/Jeff:
> 
> Thank you both.
> 
> You seem to confirm that my observation of an "infelicity" in
> strsplit() is real. That is most helpful.
> 
> I found nothing in David's message 2 code that was surprising. That
> is, the splits shown conform to what I would expect from "\\b" . But
> not to what I originally showed and David enlarged upon in his first
> message. I still don't really get why a split should occur at every
> letter.
> 
> Jeff may very well have found the explanation, but I have not gone
> through his code.
> 
> If the infelicities noted (are there more?) by David and me are not
> really bugs -- and I would be frankly surprised if they were -- I
> would suggest that perhaps they deserve mention in the strsplit() man
> page. Something to the effect that "\b and \< should not be used as
> split characters..." .

It's more of a regex infelicity or what appears (to us both at a minimum)  as a violation of a 'least surprise principle':

>  gsub("\\b", " ", "  This is a test case")
[1] "     T h i s   i s   a   t e s t   c a s e "


-- 
David.
 
> 
> Bert Gunter
> 
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>   -- Clifford Stoll
> 
> 
> On Sat, Jul 11, 2015 at 11:05 AM, David Winsemius
> <dwinsemius at comcast.net> wrote:
>> 
>> On Jul 11, 2015, at 7:47 AM, Bert Gunter wrote:
>> 
>>> I noticed the following:
>>> 
>>>> strsplit("red green","\\b")
>>> [[1]]
>>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>> 
>> After reading the ?regex help page, I didn't understand why `\b` would split within sequences of "word"-characters, either. I expected this to be the result:
>> 
>> [[1]]
>> [1] "red"  " "  "green"
>> 
>> There is a warning in that paragraph: "(The interpretation of ?word? depends on the locale and implementation.)"
>> 
>> I got the expected result with only one of "\\>" and "\\<"
>> 
>>> strsplit("red green","\\<")
>> [[1]]
>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>> 
>>> strsplit("red green","\\>")
>> [[1]]
>> [1] "red"    " green"
>> 
>> The result with "\\<" seems decidedly unexpected.
>> 
>> I'm wondered if the "original" regex documentation uses the same language as the R help page. So I went to the cited website and find:
>> =======
>> An assertion-character can be any of the following:
>> 
>>        ? < ? Beginning of word
>>        ? > ? End of word
>>        ? b ? Word boundary
>>        ? B ? Non-word boundary
>>        ? d ? Digit character (equivalent to [[:digit:]])
>>        ? D ? Non-digit character (equivalent to [^[:digit:]])
>>        ? s ? Space character (equivalent to [[:space:]])
>>        ? S ? Non-space character (equivalent to [^[:space:]])
>>        ? w ? Word character (equivalent to [[:alnum:]_])
>>        ? W ? Non-word character (equivalent to [^[:alnum:]_])
>> ========
>> 
>> The word-"word" appears nowhere else on that page.
>> 
>> 
>>>> strsplit("red green","\\W")
>>> [[1]]
>>> [1] "red"   "green"
>> 
>> `\W` matches the byte-width non-word characters. So the " "-character would be discarded.
>> 
>>> 
>>> I would have thought that "\\b" should give what "\\W" did. Note that:
>>> 
>>>> grep("\\bred\\b","red green")
>>> [1] 1
>>> ## as expected
>>> 
>>> Does strsplit use a different regex engine than grep()? Or more
>>> likely, what am I misunderstanding?
>>> 
>>> Thanks.
>>> 
>>> Bert
>>> 
>>> 
>> 
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 

David Winsemius
Alameda, CA, USA


From marongiu.luigi at gmail.com  Sun Jul 12 00:55:54 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Sat, 11 Jul 2015 23:55:54 +0100
Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
 alternative for Lattice)
In-Reply-To: <000301d0ba33$16715950$43540bf0$@bigpond.com>
References: <CAMk+s2QqBw_Xjp+nUtxakFgRHU=EcqxyJ22taeq-t_wAKE4jyw@mail.gmail.com>
	<000001d0b842$43905870$cab10950$@bigpond.com>
	<CAMk+s2TCoN=vFv1p37o42RDG2=3CBPR1cu2J_yQO78Q729EXuA@mail.gmail.com>
	<000401d0b9f2$d1ed7a10$75c86e30$@bigpond.com>
	<CAMk+s2S=bhHPzoRNpKXiva3-BLTWQeJCKW17oEOYzLkpeJzBDw@mail.gmail.com>
	<000301d0ba33$16715950$43540bf0$@bigpond.com>
Message-ID: <CAMk+s2S1_rx5aHh-G+UUsL4vfGs4CPcnF=zqqfa6TE2f15dtvQ@mail.gmail.com>

Dear Duncan,
I tried the solution you indicated in the previous message but I am
still obtaining the error length(dimx) == 2 is not TRUE. Assuming that
the dataframe is working (it does work on my machine), I added the
useOuterStrips bit as you showed:
useOuterStrips(
    strip = strip.custom(
        factor.levels = paste("column",1:6),
        par.strip.text = list(cex = 0.75)),
    strip.left = strip.custom(factor.levels = paste("row", 1:2),
                              horizontal = FALSE,
                              par.strip.text = list(cex = 0.75)
                              ),
   xyplot(Rn ~ Cycle | Well,
       data = my.data,
       groups = Well,
       ylab= "Y axis",
       xlab="X axis",
       main="Title",
       scales = list(
           x = list(draw = FALSE),
           y = list(draw = FALSE),
           relation="same"
       ),
       as.table = TRUE,
       layout = L,
       par.settings = list(
           strip.background=list(col="white"),
           axis.text = list(cex = 0.6),
           par.xlab.text = list(cex = 0.75),
           par.ylab.text = list(cex = 0.75),
           par.main.text = list(cex = 0.8),
           superpose.symbol = list(pch = ".", cex = 1)
       ),
       strip    = FALSE,
       type = "l",
       col = 3,
       panel = panel.superpose
    )
)

WHere am I getting it wrong?
Best regards
Luigi


On Thu, Jul 9, 2015 at 11:36 AM, Duncan Mackay <dulcalma at bigpond.com> wrote:
> Hi Luigi
>
> If you wanted to add some text you could use grid.text from the grid package
>
> I use it regularly for annotating
>
> See also
> https://stat.ethz.ch/pipermail/r-help/2005-February/066264.html
>
> https://stat.ethz.ch/pipermail/r-help/2005-April/069456.html
>
>
>
> Duncan
>
> -----Original Message-----
> From: Luigi Marongiu [mailto:marongiu.luigi at gmail.com]
> Sent: Thursday, 9 July 2015 19:01
> To: Duncan Mackay
> Subject: Re: [R] add outer strip for levels in lattice plot (useOuterStrips alternative for Lattice)
>
> Dear Duncan,
> funny this deviation of the variables outcomes. yesterday when i wrote
> the script it worked fine and even today when i loaded it on rstudio
> at work i obtained:
> Cycle  num[1:540]
> Line    num[1:540]
> Rn      num[1:540]
> Target chr[1:540]
> Well    num[1:540]
> my.data 540 obs. of 4 variables
> and the call for my.data returned a proper dataframe.
> I did not write  lattice_Extra but "latticeextra's useOuterStrips()"
> [forgot to capitalize the second E].
> I'll try your suggestion, but essentially wouldn't be possible to
> write some text on the top and left portions of the xyplot graph using
> some "text" function within lattice itself without having to call
> latticeExtra?
> Many thanks
> regards
> Luigi
>
>
> On Thu, Jul 9, 2015 at 3:56 AM, Duncan Mackay <dulcalma at bigpond.com> wrote:
>> Hi Luigi
>>
>> str(my.data)
>> 'data.frame':   540 obs. of  4 variables:
>>  $ Line  : chr  "1" "2" "3" "4" ...
>>  $ Well  : chr  "1" "1" "1" "1" ...
>>  $ Target: chr  "alpha" "alpha" "alpha" "alpha" ...
>>  $ Rn    : chr  "0.728" "0.735" "0.749" "0.758" ...
>>
>> You didnot create the data.frame properly
>>
>> mydata <-
>> data.frame(Line=Line, Well=Well, Target=Target, Rn=Rn,Cycle= Cycle)
>> str(mydata)
>> 'data.frame':   540 obs. of  4 variables:
>>  $ Line  : num  1 2 3 4 5 6 7 8 9 10 ...
>>  $ Well  : num  1 1 1 1 1 1 1 1 1 1 ...
>>  $ Target: chr  "alpha" "alpha" "alpha" "alpha" ...
>>  $ Rn    : num  0.728 0.735 0.749 0.758 0.77 0.778 0.78 0.784 0.786 0.785 ...
>>
>> I was having problems with your script so I started with the basics (before I found the problem with the df)
>>
>>     xyplot(Rn ~ Cycle | sprintf("%2d",Well), data = mydata,
>>            as.table = TRUE,
>>            layout = c(6,2),
>>            groups = Well)
>>
>> You are not splitting up the data with groups being the same as the conditioning.
>> It can be necessary to use this setup in some cases but not this.
>>
>> latticeExtra is on Cran so you can use install.packages("latticeExtra") or use the menu
>> I have never heard of lattice_Extra
>>
>>
>>     mydata$rown <- ifelse(mydata$Well>6,2,1)
>>     mydata$welln <- rep(1:6, 2)[sapply(mydata$Well, pmatch, 1:12)]
>>
>>
>> useOuterStrips(strip      = strip.custom(factor.levels = paste("column",1:6),
>>                                          par.strip.text = list(cex = 0.75)),
>>                strip.left = strip.custom(factor.levels = paste("row", 1:2),
>>                                          horizontal = FALSE,
>>                                          par.strip.text = list(cex = 0.75)),
>>
>>     xyplot(Rn ~ Cycle | welln*rown, data = mydata,
>>            as.table = TRUE,
>>            layout = c(6,2)
>>           )
>>
>> ) ## useOuterStrips
>>
>>   ps.Colours <-
>>   c("#000000","#FF0000","#00FF00","#0000FF","#FFA54F",
>>     "#00FFFF","#FF00FF","#C00000","#00B414","#FFD18F",
>>     "#00B2EE","#ffe5cc","#6A5ACD","#C0C0C0","#CCCCCC",
>>     "#6495ED","#FFFAFA")
>>
>>
>>
>> useOuterStrips(strip      = strip.custom(factor.levels = paste("column",1:6),
>>                                          par.strip.text = list(cex = 0.75)),
>>                strip.left = strip.custom(factor.levels = paste("row", 1:2),
>>                                          horizontal = FALSE,
>>                                          par.strip.text = list(cex = 0.75)),
>>
>>     xyplot(Rn ~ Cycle | welln*rown, data = mydata,
>>            as.table = TRUE,
>>            groups = Well,
>>            col = ps.Colours,
>>            layout = c(6,2)
>>           )
>>
>> ) ## useOuterStrips
>>
>> Duncan
>>
>> -----Original Message-----
>> From: Luigi Marongiu [mailto:marongiu.luigi at gmail.com]
>> Sent: Thursday, 9 July 2015 09:22
>> To: Duncan Mackay; Dennis Murphy; r-help
>> Subject: Re: [R] add outer strip for levels in lattice plot (useOuterStrips alternative for Lattice)
>>
>> In relation to this question I have prepared a workable example. First
>> I prepare a dataframe with three variables (Cycle, Target, Rn), then I
>> plot the results with lattice's xyplot(). I won't use the scales but
>> only the labels and the panels are NOT indicated by the variable Well.
>> What I would need to use are instead the vectors row.name and col.name
>> that can identify each column and row of the plot.
>> Secondly I create replicates of the row.name and col.name in order to
>> fit the data and create a second dataframe, then I plot using lattice
>> extra's useOuterStrips().
>> However (a) I think the call is wrong anyway, (b) I obtain "Error:
>> length(dimx) == 2 is not TRUE" (c) I need a package on top of lattice
>> (d) I might introduce errors during the creation of the second
>> dataframe.
>> The requirements remains to create a strip on the top and left side of
>> the plot to allocate the elements of row.name and col.name possibly
>> using lattice only.
>> Thank you for your help.
>> Luigi
>>
>>>>>
>> Line<-c(    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
>>   11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
>>  21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
>> 31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
>> 41,    42,    43,    44,    45,    46,    47,    48,    49,    50,
>> 51,    52,    53,    54,    55,    56,    57,    58,    59,    60,
>> 61,    62,    63,    64,    65,    66,    67,    68,    69,    70,
>> 71,    72,    73,    74,    75,    76,    77,    78,    79,    80,
>> 81,    82,    83,    84,    85,    86,    87,    88,    89,    90,
>> 91,    92,    93,    94,    95,    96,    97,    98,    99,    100,
>> 101,    102,    103,    104,    105,    106,    107,    108,    109,
>>  110,    111,    112,    113,    114,    115,    116,    117,    118,
>>   119,    120,    121,    122,    123,    124,    125,    126,    127,
>>    128,    129,    130,    131,    132,    133,    134,    135,
>> 136,    137,    138,    139,    140,    141,    142,    143,    144,
>>  145,    146,    147,    148,    149,    150,    151,    152,    153,
>>   154,    155,    156,    157,    158,    159,    160,    161,    162,
>>    163,    164,    165,    166,    167,    168,    169,    170,
>> 171,    172,    173,    174,    175,    176,    177,    178,    179,
>>  180,    181,    182,    183,    184,    185,    186,    187,    188,
>>   189,    190,    191,    192,    193,    194,    195,    196,    197,
>>    198,    199,    200,    201,    202,    203,    204,    205,
>> 206,    207,    208,    209,    210,    211,    212,    213,    214,
>>  215,    216,    217,    218,    219,    220,    221,    222,    223,
>>   224,    225,    226,    227,    228,    229,    230,    231,    232,
>>    233,    234,    235,    236,    237,    238,    239,    240,
>> 241,    242,    243,    244,    245,    246,    247,    248,    249,
>>  250,    251,    252,    253,    254,    255,    256,    257,    258,
>>   259,    260,    261,    262,    263,    264,    265,    266,    267,
>>    268,    269,    270,    271,    272,    273,    274,    275,
>> 276,    277,    278,    279,    280,    281,    282,    283,    284,
>>  285,    286,    287,    288,    289,    290,    291,    292,    293,
>>   294,    295,    296,    297,    298,    299,    300,    301,    302,
>>    303,    304,    305,    306,    307,    308,    309,    310,
>> 311,    312,    313,    314,    315,    316,    317,    318,    319,
>>  320,    321,    322,    323,    324,    325,    326,    327,    328,
>>   329,    330,    331,    332,    333,    334,    335,    336,    337,
>>    338,    339,    340,    341,    342,    343,    344,    345,
>> 346,    347,    348,    349,    350,    351,    352,    353,    354,
>>  355,    356,    357,    358,    359,    360,    361,    362,    363,
>>   364,    365,    366,    367,    368,    369,    370,    371,    372,
>>    373,    374,    375,    376,    377,    378,    379,    380,
>> 381,    382,    383,    384,    385,    386,    387,    388,    389,
>>  390,    391,    392,    393,    394,    395,    396,    397,    398,
>>   399,    400,    401,    402,    403,    404,    405,    406,    407,
>>    408,    409,    410,    411,    412,    413,    414,    415,
>> 416,    417,    418,    419,    420,    421,    422,    423,    424,
>>  425,    426,    427,    428,    429,    430,    431,    432,    433,
>>   434,    435,    436,    437,    438,    439,    440,    441,    442,
>>    443,    444,    445,    446,    447,    448,    449,    450,
>> 451,    452,    453,    454,    455,    456,    457,    458,    459,
>>  460,    461,    462,    463,    464,    465,    466,    467,    468,
>>   469,    470,    471,    472,    473,    474,    475,    476,    477,
>>    478,    479,    480,    481,    482,    483,    484,    485,
>> 486,    487,    488,    489,    490,    491,    492,    493,    494,
>>  495,    496,    497,    498,    499,    500,    501,    502,    503,
>>   504,    505,    506,    507,    508,    509,    510,    511,    512,
>>    513,    514,    515,    516,    517,    518,    519,    520,
>> 521,    522,    523,    524,    525,    526,    527,    528,    529,
>>  530,    531,    532,    533,    534,    535,    536,    537,    538,
>>   539,    540)
>> Well<-c(    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>>  1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>>   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>>    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>> 2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
>>  2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
>>   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,
>>    2,    2,    2,    2,    2,    2,    2,    2,    2,    3,    3,
>> 3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
>>  3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
>>   3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,
>>    3,    3,    3,    3,    3,    3,    3,    4,    4,    4,    4,
>> 4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
>>  4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
>>   4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,
>>    4,    4,    4,    4,    4,    5,    5,    5,    5,    5,    5,
>> 5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
>>  5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
>>   5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,    5,
>>    5,    5,    5,    6,    6,    6,    6,    6,    6,    6,    6,
>> 6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
>>  6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
>>   6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,    6,
>>    6,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
>> 7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
>>  7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,
>>   7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    7,    8,
>>    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
>> 8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
>>  8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,    8,
>>   8,    8,    8,    8,    8,    8,    8,    8,    8,    9,    9,    9,
>>    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
>> 9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
>>  9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,    9,
>>   9,    9,    9,    9,    9,    9,    9,    10,    10,    10,    10,
>>  10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
>> 10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
>> 10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
>> 10,    10,    10,    10,    10,    10,    10,    10,    10,    10,
>> 10,    11,    11,    11,    11,    11,    11,    11,    11,    11,
>> 11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
>> 11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
>> 11,    11,    11,    11,    11,    11,    11,    11,    11,    11,
>> 11,    11,    11,    11,    11,    11,    12,    12,    12,    12,
>> 12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
>> 12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
>> 12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
>> 12,    12,    12,    12,    12,    12,    12,    12,    12,    12,
>> 12)
>> Cycle<-c(    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
>>    11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
>>   21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
>>  31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
>> 41,    42,    43,    44,    45,    1,    2,    3,    4,    5,    6,
>> 7,    8,    9,    10,    11,    12,    13,    14,    15,    16,    17,
>>    18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
>>   28,    29,    30,    31,    32,    33,    34,    35,    36,    37,
>>  38,    39,    40,    41,    42,    43,    44,    45,    1,    2,
>> 3,    4,    5,    6,    7,    8,    9,    10,    11,    12,    13,
>> 14,    15,    16,    17,    18,    19,    20,    21,    22,    23,
>> 24,    25,    26,    27,    28,    29,    30,    31,    32,    33,
>> 34,    35,    36,    37,    38,    39,    40,    41,    42,    43,
>> 44,    45,    1,    2,    3,    4,    5,    6,    7,    8,    9,
>> 10,    11,    12,    13,    14,    15,    16,    17,    18,    19,
>> 20,    21,    22,    23,    24,    25,    26,    27,    28,    29,
>> 30,    31,    32,    33,    34,    35,    36,    37,    38,    39,
>> 40,    41,    42,    43,    44,    45,    1,    2,    3,    4,    5,
>>  6,    7,    8,    9,    10,    11,    12,    13,    14,    15,    16,
>>    17,    18,    19,    20,    21,    22,    23,    24,    25,    26,
>>   27,    28,    29,    30,    31,    32,    33,    34,    35,    36,
>>  37,    38,    39,    40,    41,    42,    43,    44,    45,    1,
>> 2,    3,    4,    5,    6,    7,    8,    9,    10,    11,    12,
>> 13,    14,    15,    16,    17,    18,    19,    20,    21,    22,
>> 23,    24,    25,    26,    27,    28,    29,    30,    31,    32,
>> 33,    34,    35,    36,    37,    38,    39,    40,    41,    42,
>> 43,    44,    45,    1,    2,    3,    4,    5,    6,    7,    8,
>> 9,    10,    11,    12,    13,    14,    15,    16,    17,    18,
>> 19,    20,    21,    22,    23,    24,    25,    26,    27,    28,
>> 29,    30,    31,    32,    33,    34,    35,    36,    37,    38,
>> 39,    40,    41,    42,    43,    44,    45,    1,    2,    3,    4,
>>   5,    6,    7,    8,    9,    10,    11,    12,    13,    14,    15,
>>    16,    17,    18,    19,    20,    21,    22,    23,    24,    25,
>>   26,    27,    28,    29,    30,    31,    32,    33,    34,    35,
>>  36,    37,    38,    39,    40,    41,    42,    43,    44,    45,
>> 1,    2,    3,    4,    5,    6,    7,    8,    9,    10,    11,
>> 12,    13,    14,    15,    16,    17,    18,    19,    20,    21,
>> 22,    23,    24,    25,    26,    27,    28,    29,    30,    31,
>> 32,    33,    34,    35,    36,    37,    38,    39,    40,    41,
>> 42,    43,    44,    45,    1,    2,    3,    4,    5,    6,    7,
>> 8,    9,    10,    11,    12,    13,    14,    15,    16,    17,
>> 18,    19,    20,    21,    22,    23,    24,    25,    26,    27,
>> 28,    29,    30,    31,    32,    33,    34,    35,    36,    37,
>> 38,    39,    40,    41,    42,    43,    44,    45,    1,    2,    3,
>>    4,    5,    6,    7,    8,    9,    10,    11,    12,    13,    14,
>>    15,    16,    17,    18,    19,    20,    21,    22,    23,    24,
>>   25,    26,    27,    28,    29,    30,    31,    32,    33,    34,
>>  35,    36,    37,    38,    39,    40,    41,    42,    43,    44,
>> 45,    1,    2,    3,    4,    5,    6,    7,    8,    9,    10,
>> 11,    12,    13,    14,    15,    16,    17,    18,    19,    20,
>> 21,    22,    23,    24,    25,    26,    27,    28,    29,    30,
>> 31,    32,    33,    34,    35,    36,    37,    38,    39,    40,
>> 41,    42,    43,    44,    45)
>> Target <-c(
>>     rep("alpha",45),
>>     rep("beta", 45),
>>     rep("gamma", 45),
>>     rep("delta", 45),
>>     rep("epsilon", 45),
>>     rep("zeta", 45),
>>     rep("eta", 45),
>>     rep("theta", 45),
>>     rep("iota", 45),
>>     rep("kappa", 45),
>>     rep("lamba", 45),
>>     rep("mu", 45)
>> )
>> Rn<-c(    0.728,    0.735,    0.749,    0.758,    0.77,    0.778,
>> 0.78,    0.784,    0.786,    0.785,    0.786,    0.786,    0.785,
>> 0.785,    0.784,    0.783,    0.784,    0.786,    0.786,    0.787,
>> 0.789,    0.786,    0.784,    0.786,    0.786,    0.785,    0.784,
>> 0.785,    0.786,    0.784,    0.784,    0.78,    0.781,    0.779,
>> 0.78,    0.78,    0.78,    0.781,    0.781,    0.78,    0.78,
>> 0.781,    0.781,    0.78,    0.781,    0.695,    0.712,    0.751,
>> 0.784,    0.81,    0.831,    0.852,    0.867,    0.877,    0.889,
>> 0.896,    0.902,    0.908,    0.912,    0.912,    0.915,    0.919,
>> 0.916,    0.918,    0.92,    0.917,    0.914,    0.917,    0.914,
>> 0.914,    0.913,    0.913,    0.91,    0.908,    0.906,    0.902,
>> 0.9,    0.901,    0.897,    0.896,    0.895,    0.896,    0.892,
>> 0.89,    0.889,    0.89,    0.888,    0.889,    0.885,    0.886,
>> 1.701,    1.702,    1.69,    1.678,    1.666,    1.65,    1.642,
>> 1.632,    1.623,    1.616,    1.605,    1.598,    1.591,    1.582,
>> 1.575,    1.568,    1.561,    1.556,    1.553,    1.549,    1.546,
>> 1.541,    1.536,    1.531,    1.529,    1.526,    1.524,    1.522,
>> 1.52,    1.517,    1.516,    1.514,    1.512,    1.512,    1.509,
>> 1.509,    1.506,    1.505,    1.505,    1.508,    1.513,    1.508,
>> 1.506,    1.507,    1.503,    0.761,    0.774,    0.797,    0.817,
>> 0.833,    0.844,    0.85,    0.856,    0.864,    0.867,    0.869,
>> 0.873,    0.874,    0.875,    0.873,    0.872,    0.872,    0.87,
>> 0.866,    0.865,    0.864,    0.864,    0.86,    0.857,    0.855,
>> 0.852,    0.851,    0.849,    0.845,    0.843,    0.842,    0.84,
>> 0.835,    0.833,    0.83,    0.827,    0.825,    0.826,    0.824,
>> 0.821,    0.82,    0.818,    0.817,    0.816,    0.813,    0.982,
>> 0.988,    0.998,    1.009,    1.015,    1.018,    1.021,    1.023,
>> 1.023,    1.02,    1.016,    1.015,    1.009,    1.005,    1.003,
>> 1,    0.995,    0.989,    0.985,    0.981,    0.975,    0.969,
>> 0.964,    0.96,    0.956,    0.952,    0.948,    0.944,    0.94,
>> 0.935,    0.932,    0.927,    0.924,    0.921,    0.918,    0.915,
>> 0.91,    0.907,    0.904,    0.901,    0.898,    0.896,    0.892,
>> 0.889,    0.888,    1.14,    1.133,    1.117,    1.105,    1.096,
>> 1.086,    1.074,    1.063,    1.052,    1.042,    1.033,    1.024,
>> 1.015,    1.006,    0.999,    0.993,    0.987,    0.982,    0.975,
>> 0.969,    0.965,    0.96,    0.955,    0.952,    0.947,    0.944,
>> 0.943,    0.939,    0.935,    0.933,    0.93,    0.927,    0.925,
>> 0.921,    0.919,    0.919,    0.917,    0.917,    0.915,    0.912,
>> 0.912,    0.912,    0.909,    0.907,    0.907,    1.304,    1.31,
>> 1.325,    1.338,    1.349,    1.355,    1.359,    1.36,    1.361,
>> 1.362,    1.359,    1.353,    1.344,    1.335,    1.331,    1.323,
>> 1.315,    1.308,    1.3,    1.292,    1.284,    1.276,    1.268,
>> 1.262,    1.256,    1.25,    1.245,    1.241,    1.234,    1.228,
>> 1.222,    1.216,    1.213,    1.208,    1.205,    1.2,    1.197,
>> 1.192,    1.189,    1.186,    1.184,    1.182,    1.181,    1.178,
>> 1.178,    0.802,    0.801,    0.801,    0.8,    0.799,    0.797,
>> 0.794,    0.791,    0.785,    0.781,    0.777,    0.772,    0.766,
>> 0.76,    0.756,    0.753,    0.751,    0.746,    0.742,    0.739,
>> 0.735,    0.732,    0.728,    0.726,    0.725,    0.722,    0.718,
>> 0.717,    0.716,    0.715,    0.71,    0.709,    0.711,    0.71,
>> 0.709,    0.709,    0.709,    0.709,    0.708,    0.709,    0.71,
>> 0.71,    0.711,    0.711,    0.712,    1.209,    1.206,    1.204,
>> 1.202,    1.197,    1.186,    1.175,    1.165,    1.154,    1.143,
>> 1.133,    1.12,    1.11,    1.105,    1.098,    1.091,    1.085,
>> 1.078,    1.072,    1.067,    1.063,    1.054,    1.049,    1.048,
>> 1.04,    1.036,    1.033,    1.029,    1.027,    1.024,    1.021,
>> 1.019,    1.017,    1.013,    1.01,    1.008,    1.006,    1.005,
>> 1.004,    1.002,    1.002,    1.001,    1,    0.998,    0.995,
>> 2.936,    2.942,    2.946,    2.951,    2.956,    2.956,    2.968,
>> 2.964,    2.953,    2.945,    2.939,    2.929,    2.919,    2.909,
>> 2.902,    2.893,    2.882,    2.871,    2.857,    2.847,    2.835,
>> 2.825,    2.819,    2.806,    2.795,    2.787,    2.781,    2.766,
>> 2.761,    2.752,    2.749,    2.74,    2.731,    2.722,    2.718,
>> 2.711,    2.705,    2.7,    2.693,    2.69,    2.686,    2.676,
>> 2.672,    2.668,    2.667,    1.032,    1.033,    1.031,    1.033,
>> 1.031,    1.029,    1.025,    1.02,    1.019,    1.016,    1.012,
>> 1.008,    1.007,    1.011,    1.015,    1.032,    1.068,    1.124,
>> 1.209,    1.327,    1.472,    1.632,    1.8,    1.971,    2.14,
>> 2.302,    2.459,    2.612,    2.754,    2.886,    3.008,    3.122,
>> 3.218,    3.306,    3.39,    3.472,    3.547,    3.613,    3.674,
>> 3.731,    3.772,    3.81,    3.84,    3.86,    3.882,    0.808,
>> 0.808,    0.808,    0.807,    0.805,    0.804,    0.802,    0.801,
>> 0.798,    0.796,    0.794,    0.79,    0.788,    0.785,    0.781,
>> 0.78,    0.777,    0.774,    0.772,    0.771,    0.769,    0.767,
>> 0.767,    0.766,    0.766,    0.764,    0.764,    0.765,    0.762,
>> 0.762,    0.76,    0.759,    0.759,    0.758,    0.758,    0.758,
>> 0.756,    0.754,    0.754,    0.753,    0.754,    0.755,    0.753,
>> 0.754,    0.753)
>>
>> my.data <- as.data.frame(cbind(Line, Well, Target, Rn))
>>
>> L <- c(6,2)
>> row.name <- c("A", "B")
>> col.name <- 1:6
>>
>>
>> library("lattice")
>> xyplot(Rn ~ Cycle | Well,
>>        data = my.data,
>>        groups = Well,
>>        ylab= "Y axis",
>>        xlab="X axis",
>>        main="Title",
>>        scales = list(
>>            x = list(draw = FALSE),
>>            y = list(draw = FALSE),
>>            relation="same"
>>        ),
>>        as.table = TRUE,
>>        layout = L,
>>        par.settings = list(
>>            strip.background=list(col="white"),
>>            axis.text = list(cex = 0.6),
>>            par.xlab.text = list(cex = 0.75),
>>            par.ylab.text = list(cex = 0.75),
>>            par.main.text = list(cex = 0.8),
>>            superpose.symbol = list(pch = ".", cex = 1)
>>        ),
>>        strip    = FALSE,
>>        type = "l",
>>        col = 3,
>>        panel = panel.superpose
>> )
>>
>>
>> ROW <- c(rep(row.name[1], 45*6), rep(row.name[2], 45*6))
>> CO <- c(rep(col.name[1], 45),
>>         rep(col.name[2], 45),
>>         rep(col.name[3], 45),
>>         rep(col.name[4], 45),
>>         rep(col.name[5], 45),
>>         rep(col.name[6], 45)
>> )
>> COL <- rep(CO,2)
>> new.data <- cbind(my.data, ROW, COL)
>> head(new.data, 200)
>>
>> useOuterStrips(
>>     xyplot(Rn ~ Cycle | Well,
>>        data = my.data,
>>        groups = Well,
>>        ylab= "Y axis",
>>        xlab="X axis",
>>        main="Title",
>>        scales = list(
>>            x = list(draw = FALSE),
>>            y = list(draw = FALSE),
>>            relation="same"
>>        ),
>>        as.table = TRUE,
>>        layout = L,
>>        par.settings = list(
>>            strip.background=list(col="white"),
>>            axis.text = list(cex = 0.6),
>>            par.xlab.text = list(cex = 0.75),
>>            par.ylab.text = list(cex = 0.75),
>>            par.main.text = list(cex = 0.8),
>>            superpose.symbol = list(pch = ".", cex = 1)
>>        ),
>>        strip    = FALSE,
>>        type = "l",
>>        col = 3,
>>        panel = panel.superpose
>> ),
>> c(ROW,COL))
>>
>> On Tue, Jul 7, 2015 at 12:19 AM, Duncan Mackay <dulcalma at bigpond.com> wrote:
>>> Hi Luigi
>>>
>>> Not exactly sure what you want
>>>
>>> Have a look at
>>> https://stat.ethz.ch/pipermail/r-help/2007-May/132785.html
>>> and
>>> https://stat.ethz.ch/pipermail/r-help/2007-July/135551.html
>>>
>>> otherwise have a look at ?trellis.focus and
>>> https://stat.ethz.ch/pipermail/r-help/2006-July/109585.html
>>>
>>> failing that ?gridRect and ?gridText from library(grid)
>>>
>>> Regards
>>>
>>> Duncan
>>>
>>> Duncan Mackay
>>> Department of Agronomy and Soil Science
>>> University of New England
>>> Armidale NSW 2351
>>> Email: home: mackay at northnet.com.au
>>>
>>> -----Original Message-----
>>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
>>> Marongiu
>>> Sent: Monday, 6 July 2015 09:56
>>> To: r-help
>>> Subject: [R] add outer strip for levels in lattice plot (useOuterStrips
>>> alternative for Lattice)
>>>
>>> Dear all,
>>> I would like to add an outer strip or something like that on a lattice
>>> plot I am making. Such plot contains 384 cells and, since I am not
>>> interested in the axis values, I set:
>>>            scales = list(
>>>                x = list(draw = FALSE),
>>>                y = list(draw = FALSE),
>>>                relation="same"
>>>                ),
>>> on a xyplot from the LATTICE package.
>>> Nevertheless there are axis labels which run like:
>>>            ylab= "Y axis",
>>>            xlab= "X axis",
>>> I would like to place some more information regarding the individual
>>> cells thus I would like to draw a sort of extra axis labels that are
>>> similar to the outer strip of the LATTICE_EXTRA package, that is
>>> markers placed between the axis labels and the axis values and
>>> centered for each cells, typically placed on the top and left sides of
>>> the plot. This is performed by the useOuterStrips function but:
>>> a) LatticeExtra is not in the CRAN repository thus I have to install
>>> it through a more laborious approach which makes LatticeExtra less
>>> direct than Lattice
>>> b)  useOuterStrips uses information directly from the data whereas I
>>> will have to provide the extra information from ad hoc vectors not
>>> present in the data set.
>>>
>>> The question therefore is: is there a way to write text from a vector
>>> in the top and left corners of a lattice xyplot and place the
>>> individual elements at the centre of the rows and columns that compose
>>> the graph?
>>>
>>> Many thanks,
>>> Luigi
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>


From r.turner at auckland.ac.nz  Sun Jul 12 01:00:26 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sun, 12 Jul 2015 11:00:26 +1200
Subject: [R] [FORGED]  data$variable=factor(....) <NA> <NA> <NA>
In-Reply-To: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
References: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
Message-ID: <55A1A00A.70205@auckland.ac.nz>


Try:

ggg <- c("F","M","F",M")
data$gender <- factor(ggg[data$gender])

This in effect converts the (spurious) " F" and " M" levels into "F" and 
"M" respectively, giving you a factor with the two levels that you 
really want.

cheers,

Rolf Turner

P. S.  *Not* a good idea to use "data" as the name of your data frame.
See fortune("dog").

R. T.

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 12/07/15 07:21, Dagmar Jurankov? wrote:
> Hello everybody, I have a problem with R.
>
>
> I uploaded a questionnaire saved as csv into R and I tried to test
> independence between two variables.
>
>
>
> data <- read.csv("C:/Users/Me/Desktop/data.csv")>   View(data)> df =
> read.csv("C:/Users/Me/Desktop/data.csv")> ls()
> [1] "df"     "data"> attributes(data$gender)
> $levels
> [1] " F" " M" "F"  "M"
>
> $class
> [1] "factor"
>
>
> I changed my variable "gender" into a factor using:
>
>
> data$gender=factor(data$gender, levels=c(1:2), labels= c( "F", "M"),
> exclude= NA, nmax= NA).
>
>
> Then I wrote data$gender and the only thing i got was:
>
>
> [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [41] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA>
>
> [61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>
> Levels: F M
>
>
> Does anybody know why?
>
>
> -My csv doc in the column gender is filled out properly. (M=Male, F= Female)
>
> -My imported dataset in R is complete (all values)
>
>
> ! I have done this with a different excel document and it worked out
> without any problems. I am really clueless. I cant go further and compare
> the variables and do t-tests without this working.
>
>
> Could someone please help me out?
>
> Thank you.


From bgunter.4567 at gmail.com  Sun Jul 12 01:12:41 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 11 Jul 2015 16:12:41 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <BD27075F-7493-4B2D-8D53-5C9700DE97E4@comcast.net>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
	<CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>
	<BD27075F-7493-4B2D-8D53-5C9700DE97E4@comcast.net>
Message-ID: <CAGxFJbQqF2nhS=AHf6kgya7KTAwQux2DOwDM7OHZmWJkL3H7GA@mail.gmail.com>

omigosh -- you're right.

-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 11, 2015 at 3:31 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> On Jul 11, 2015, at 3:07 PM, Bert Gunter wrote:
>
>> David/Jeff:
>>
>> Thank you both.
>>
>> You seem to confirm that my observation of an "infelicity" in
>> strsplit() is real. That is most helpful.
>>
>> I found nothing in David's message 2 code that was surprising. That
>> is, the splits shown conform to what I would expect from "\\b" . But
>> not to what I originally showed and David enlarged upon in his first
>> message. I still don't really get why a split should occur at every
>> letter.
>>
>> Jeff may very well have found the explanation, but I have not gone
>> through his code.
>>
>> If the infelicities noted (are there more?) by David and me are not
>> really bugs -- and I would be frankly surprised if they were -- I
>> would suggest that perhaps they deserve mention in the strsplit() man
>> page. Something to the effect that "\b and \< should not be used as
>> split characters..." .
>
> It's more of a regex infelicity or what appears (to us both at a minimum)  as a violation of a 'least surprise principle':
>
>>  gsub("\\b", " ", "  This is a test case")
> [1] "     T h i s   i s   a   t e s t   c a s e "
>
>
> --
> David.
>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>>
>>
>> On Sat, Jul 11, 2015 at 11:05 AM, David Winsemius
>> <dwinsemius at comcast.net> wrote:
>>>
>>> On Jul 11, 2015, at 7:47 AM, Bert Gunter wrote:
>>>
>>>> I noticed the following:
>>>>
>>>>> strsplit("red green","\\b")
>>>> [[1]]
>>>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>>>
>>> After reading the ?regex help page, I didn't understand why `\b` would split within sequences of "word"-characters, either. I expected this to be the result:
>>>
>>> [[1]]
>>> [1] "red"  " "  "green"
>>>
>>> There is a warning in that paragraph: "(The interpretation of ?word? depends on the locale and implementation.)"
>>>
>>> I got the expected result with only one of "\\>" and "\\<"
>>>
>>>> strsplit("red green","\\<")
>>> [[1]]
>>> [1] "r" "e" "d" " " "g" "r" "e" "e" "n"
>>>
>>>> strsplit("red green","\\>")
>>> [[1]]
>>> [1] "red"    " green"
>>>
>>> The result with "\\<" seems decidedly unexpected.
>>>
>>> I'm wondered if the "original" regex documentation uses the same language as the R help page. So I went to the cited website and find:
>>> =======
>>> An assertion-character can be any of the following:
>>>
>>>        ? < ? Beginning of word
>>>        ? > ? End of word
>>>        ? b ? Word boundary
>>>        ? B ? Non-word boundary
>>>        ? d ? Digit character (equivalent to [[:digit:]])
>>>        ? D ? Non-digit character (equivalent to [^[:digit:]])
>>>        ? s ? Space character (equivalent to [[:space:]])
>>>        ? S ? Non-space character (equivalent to [^[:space:]])
>>>        ? w ? Word character (equivalent to [[:alnum:]_])
>>>        ? W ? Non-word character (equivalent to [^[:alnum:]_])
>>> ========
>>>
>>> The word-"word" appears nowhere else on that page.
>>>
>>>
>>>>> strsplit("red green","\\W")
>>>> [[1]]
>>>> [1] "red"   "green"
>>>
>>> `\W` matches the byte-width non-word characters. So the " "-character would be discarded.
>>>
>>>>
>>>> I would have thought that "\\b" should give what "\\W" did. Note that:
>>>>
>>>>> grep("\\bred\\b","red green")
>>>> [1] 1
>>>> ## as expected
>>>>
>>>> Does strsplit use a different regex engine than grep()? Or more
>>>> likely, what am I misunderstanding?
>>>>
>>>> Thanks.
>>>>
>>>> Bert
>>>>
>>>>
>>>
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>
> David Winsemius
> Alameda, CA, USA
>


From ccberry at ucsd.edu  Sun Jul 12 01:26:21 2015
From: ccberry at ucsd.edu (Charles C. Berry)
Date: Sat, 11 Jul 2015 16:26:21 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
	<CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>
Message-ID: <alpine.OSX.2.11.1507111520360.1005@charles-berrys-macbook.local>

On Sat, 11 Jul 2015, Bert Gunter wrote:

> David/Jeff:
>
> Thank you both.
>
> You seem to confirm that my observation of an "infelicity" in
> strsplit() is real. That is most helpful.
>
> I found nothing in David's message 2 code that was surprising. That
> is, the splits shown conform to what I would expect from "\\b" . But
> not to what I originally showed and David enlarged upon in his first
> message. I still don't really get why a split should occur at every
> letter.
>
> Jeff may very well have found the explanation, but I have not gone
> through his code.
>
> If the infelicities noted (are there more?) by David and me are not
> really bugs -- and I would be frankly surprised if they were -- I
> would suggest that perhaps they deserve mention in the strsplit() man
> page. Something to the effect that "\b and \< should not be used as
> split characters..." .

Bert et al,

?strsplit already says:

"If empty matches occur, in particular if split has length 0, x is split 
into single characters."

And there are various ways that empty matches can happen besides using 
"\\b" as the split arg. But there would be no harm in adding your cases to 
'in particular ...'

The comment in the code (src/main/grep.c: line 493) suggests this was a 
deliberate decision. However, similar functions in other languages do not 
do this.

For example, emacs `(split-string "red green" "\\b")' gives

 	("" "red" " " "green" "")

as the result.

Chuck


From marongiu.luigi at gmail.com  Sun Jul 12 01:46:25 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Sun, 12 Jul 2015 00:46:25 +0100
Subject: [R] change font size within the panel.text function (package
	lattice)
Message-ID: <CAMk+s2SR9CGHy+JsiLZssxJDiU8b4icy0sDC_vW7Y2spXOy5SA@mail.gmail.com>

Dear all,
I am adding some text to the panels of a graph generated through the
lattice function using the argument:
   xyplot( ...,
              panel =
               function(x, y,...)
               {
                   panel.xyplot(x,y,...)
                   panel.text(0,0,labels=V[panel.number()])
               }

How can I regulate the font size of such text?

Best regards
luigi


From rshepard at appl-ecosys.com  Sun Jul 12 02:07:52 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 11 Jul 2015 17:07:52 -0700 (PDT)
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
 xyplot() to ggplot()]
In-Reply-To: <CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507111644170.28768@localhost>

On Sat, 11 Jul 2015, Yihui Xie wrote:

> I guess you didn't tell us you were compiling the document with dvips

   That's how I preview lyx docs for the past decade-and-a-half or so. The
final .pdf is generated with pdflatex.

   There is now an issue when a second figure is added to the document: it
won't compile.

   Attached are the .RData file from the cwd, the test.lyx file (lyx-2.1.3),
and the two test files from /tmp/.../lyx_tmpbuf3/.

   Here are the lyx messages when I tried compiling the document with
pdflatex:

16:51:11.255: Exporting ...
16:51:11.259: (buffer-export pdf2)
16:51:11.278: Rscript --verbose --no-save --no-restore
"/usr/share/lyx/scripts/lyxknitr.R"
"/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.Rnw"
"/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.tex" ISO-8859-15
"/home/rshepard/documents/white-papers/water-chem-anal/"
16:51:11.281: running
16:51:11.286:   '/usr/lib/R/bin/R --slave --no-restore --no-save
--no-restore --file=/usr/share/lyx/scripts/lyxknitr.R --args
-- /tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/test.Rnw
-- / /tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpb
16:51:11.288: uf3/test.tex ISO-8859-15
/home/rshepard/documents/white-papers/water-chem-anal/'
16:51:11.292: 
16:51:11.697: Loading required package: methods
16:51:11.709: Loading required package: survival
16:51:11.712: Loading required package: graphics
16:51:11.721: Loading required package: stats
16:51:11.921: 
16:51:11.923: Attaching package: ?NADA?
16:51:11.925: 
16:51:11.927: The following object is masked from ?package:stats?:
16:51:11.929: 
16:51:11.932:     cor
16:51:11.933: 
16:51:11.969: 
16:51:11.970: Attaching package: ?zoo?
16:51:11.972: 
16:51:11.975: The following objects are masked from ?package:base?:
16:51:11.976: 
16:51:11.979:     as.Date, as.Date.numeric
16:51:11.981: 
16:51:12.091: 
16:51:12.092: 
16:51:12.094: processing file:
/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/test.Rnw
16:51:12.112:
   |
   |                                                                 |   0%
   | 
16:51:12.113:
   |....                                                             |   6%
16:51:12.116:   ordinary text without R code
16:51:12.118: 
16:51:12.120:
   |
   |........                                                         |  12%
16:51:12.123: label: unnamed-chunk-1 (with options) 
16:51:12.126: List of 1
16:51:12.128:  $ echo: logi FALSE
16:51:12.130: 
16:51:12.204:
   |
   |...........                                                      |  18%
16:51:12.205:   ordinary text without R code
16:51:12.207: 
16:51:12.210:
   |
   |...............                                                  |  24%
16:51:12.213: label: unnamed-chunk-2
16:51:12.239:
   |
   |...................                                              |  29%
16:51:12.240:   ordinary text without R code
16:51:12.242: 
16:51:12.244:
   |
   |.......................                                          |  35%
16:51:12.247: label: unnamed-chunk-3
16:51:12.293:
   |
   |...........................                                      |  41%
16:51:12.295:   ordinary text without R code
16:51:12.297: 
16:51:12.299:
   |
   |...............................                                  |  47%
16:51:12.302: label: unnamed-chunk-4
16:51:12.344:
   |
   |..................................                               |  53%
16:51:12.346:   ordinary text without R code
16:51:12.348: 
16:51:12.350:
   |
   |......................................                           |  59%
16:51:12.353: label: unnamed-chunk-5
16:51:12.432:
   |
   |..........................................                       |  65%
16:51:12.434:   ordinary text without R code
16:51:12.436: 
16:51:12.438:
   |
   |..............................................                   |  71%
16:51:12.441: label: unnamed-chunk-6
16:51:12.484:
   |
   |..................................................               |  76%
16:51:12.486:   ordinary text without R code
16:51:12.488: 
16:51:12.490:
   |
   |......................................................           |  82%
16:51:12.493: label: unnamed-chunk-7
16:51:12.837:
   |
   |.........................................................        |  88%
16:51:12.839:   ordinary text without R code
16:51:12.841: 
16:51:12.843:
   |
   |.............................................................    |  94%
16:51:12.846: label: unnamed-chunk-8
16:51:12.849: Quitting from lines 101-102
(/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/test.Rnw) 
16:51:12.851: Error in parse(text = x, srcfile = src) : 
16:51:12.853:   <text>:2:0: unexpected end of input
16:51:12.855: 1: xyplot(value ~ sampdate | variable, data=carlin.2.melt,
rm.na = T, ylab = 'Measured Value', xlab = 'Date'
16:51:12.857:    ^
16:51:12.859: Calls: knit ... <Anonymous> -> parse_all ->
parse_all.character -> parse
16:51:12.861: 
16:51:12.862: Execution halted
Systemcall.cpp (292): Systemcall: 'Rscript --verbose --no-save --no-restore
"/usr/share/lyx/scripts/lyxknitr.R"
"/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.Rnw"
"/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.tex" ISO-8859-15
"/home/rshepard/documents/white-papers/water-chem-anal/"' finished with exit
code 1
Error: Cannot convert file
----------------------------------------
An error occurred while running:
Rscript --verbose --no-save --no-restore "/usr/share/lyx/scripts/lyxknitr.R"
"/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.Rnw"
"/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.tex" ISO-8859-15
"/home/rshepard/documents/white-papers/water-chem-anal/"
16:51:16.136: Error while exporting format: PDF (pdflatex)

   There is no log file in the tempbuf. Note the references to
unnamed-chunks. Each chunk is labeled with a number when opened in lyx; the
two figures each have a distinct label. I don't see the specific error
generated by adding a second figure to the document. If there are more files
needed to reproduce this situation, let me know and I'll provide it.

Rich
-------------- next part --------------
#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrreprt
\begin_preamble
\date{}
\usepackage{textcomp,url,multicol}
%\setkomafont{sectioning}{\rmfamily}
\end_preamble
\options abstract=on
\use_default_options false
\begin_modules
natbibapa
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style humannat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

load('.RData')
\begin_inset Argument 1
status open

\begin_layout Plain Layout
echo=FALSE
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Some text in here; separating knitr chunks.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

carlin <- read.csv("./carlin.csv", header = TRUE, sep = ",", stringsAsFactors
 = F)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Next paragraph of text.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

carlin$sampdate <- as.Date(carlin$sampdate)
\end_layout

\begin_layout Plain Layout

str(carlin)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Another paragraph or two of text.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

summary(carlin)
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Plots of these distributions are the next step ...
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

carlin.1 <- subset(carlin, select = siteid:CO3)
\end_layout

\begin_layout Plain Layout

carlin.2 <- subset(carlin, select = c(siteid, sampdate, Alk:Ca))
\end_layout

\begin_layout Plain Layout

carlin.3 <- subset(carlin, select = c(siteid, sampdate, Mg:Cr))
\end_layout

\begin_layout Plain Layout

carlin.4 <- subset(carlin, select = c(siteid, sampdate, Co:Hg))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To plot each chemical constituent's concentration as a function of collection
 date the data format needs to be reshaped from wide to long:
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

library(reshape2)
\end_layout

\begin_layout Plain Layout

carlin.1.melt <- melt(carlin.1, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.2.melt <- melt(carlin.2, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.3.melt <- melt(carlin.3, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.4.melt <- melt(carlin.4, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The scatter (X-Y) plots use only measured data; missing data are not included.
 These plots show the measured values over time for each measured constituent.
 For example, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig: scatter-plot group 1"

\end_inset

 shows that most chemical constitent concentrations are low and have low
 variability.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

library(lattice)
\end_layout

\begin_layout Plain Layout

xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T, ylab = 'Measured
 Value', xlab = 'Date')
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig: scatter-plot group 1"

\end_inset

Variability of measured values in group 1 constituents over the period of
 record at the Carlin station.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Because river discharge (in cubic feet per second) spiked at about 8,000
 cfs in the mis-1980s the Y-axis for all panels in the figure have the save
 range.
 This compresses low values yet any single constituent can be plotted by
 itself and the details of its variability over time easily seen.
\end_layout

\begin_layout Standard
The second group of constituents (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig: scatter-plot group 2"

\end_inset

)
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

xyplot(value ~ sampdate | variable, data=carlin.2.melt, rm.na = T, ylab = 'Measured
 Value', xlab = 'Date'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig: scatter-plot group 2"

\end_inset

Variability of measured values in group 2 constituents over the period of
 record at the Carlin station.
\end_layout

\end_inset


\end_layout

\end_inset

 reveals that nutrients (the nitrogen compounds, phosphate, and organic
 carbon) are all consistenly low with low variability while total dissolved
 solids (TDS) concentrations are very highly variable both within years
 (as shown by the different concentrations at the same date) and across
 the period of observations.
 Alkalinity and hardness are also quite variable at both time scales.
\end_layout

\end_body
\end_document
-------------- next part --------------
\batchmode
\makeatletter
\def\input at path{{/home/rshepard/documents/white-papers/water-chem-anal//}}
\makeatother
\documentclass[letterpaper,twoside,english,abstract=on]{scrreprt}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\pdfpageheight\paperheight
\pdfpagewidth\paperwidth


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage[natbibapa]{apacite}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\date{}
\usepackage{textcomp,url,multicol}
%\setkomafont{sectioning}{\rmfamily}

\makeatother

\usepackage{babel}
\begin{document}
<<echo=FALSE>>=
load('.RData')
@

Some text in here; separating knitr chunks.

<<>>=
carlin <- read.csv("./carlin.csv", header = TRUE, sep = ",", stringsAsFactors = F)
@

Next paragraph of text.

<<>>=
carlin$sampdate <- as.Date(carlin$sampdate)
str(carlin)
@

Another paragraph or two of text.

<<>>=
summary(carlin)
@ 

Plots of these distributions are the next step ...

<<>>=
carlin.1 <- subset(carlin, select = siteid:CO3)
carlin.2 <- subset(carlin, select = c(siteid, sampdate, Alk:Ca))
carlin.3 <- subset(carlin, select = c(siteid, sampdate, Mg:Cr))
carlin.4 <- subset(carlin, select = c(siteid, sampdate, Co:Hg))
@

To plot each chemical constituent's concentration as a function of
collection date the data format needs to be reshaped from wide to
long:

<<>>=
library(reshape2)
carlin.1.melt <- melt(carlin.1, na.rm = F, id.vars = c('siteid', 'sampdate'))
carlin.2.melt <- melt(carlin.2, na.rm = F, id.vars = c('siteid', 'sampdate'))
carlin.3.melt <- melt(carlin.3, na.rm = F, id.vars = c('siteid', 'sampdate'))
carlin.4.melt <- melt(carlin.4, na.rm = F, id.vars = c('siteid', 'sampdate'))
@

The scatter (X-Y) plots use only measured data; missing data are not
included. These plots show the measured values over time for each
measured constituent. For example, Figure \ref{fig: scatter-plot group 1}
shows that most chemical constitent concentrations are low and have
low variability.
\begin{figure}
\begin{centering}
<<>>=
library(lattice)
xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T, ylab = 'Measured Value', xlab = 'Date')
@
\par\end{centering}

\protect\caption{\label{fig: scatter-plot group 1}Variability of measured values in
group 1 constituents over the period of record at the Carlin station.}


\end{figure}
Because river discharge (in cubic feet per second) spiked at about
8,000 cfs in the mis-1980s the Y-axis for all panels in the figure
have the save range. This compresses low values yet any single constituent
can be plotted by itself and the details of its variability over time
easily seen.

The second group of constituents (Figure \ref{fig: scatter-plot group 2})
\begin{figure}
\begin{centering}
<<>>=
xyplot(value ~ sampdate | variable, data=carlin.2.melt, rm.na = T, ylab = 'Measured Value', xlab = 'Date'
@
\par\end{centering}

\protect\caption{\label{fig: scatter-plot group 2}Variability of measured values in
group 2 constituents over the period of record at the Carlin station.}
\end{figure}
 reveals that nutrients (the nitrogen compounds, phosphate, and organic
carbon) are all consistenly low with low variability while total dissolved
solids (TDS) concentrations are very highly variable both within years
(as shown by the different concentrations at the same date) and across
the period of observations. Alkalinity and hardness are also quite
variable at both time scales.
\end{document}

From dwinsemius at comcast.net  Sun Jul 12 02:09:55 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 11 Jul 2015 17:09:55 -0700
Subject: [R] change font size within the panel.text function (package
	lattice)
In-Reply-To: <CAMk+s2SR9CGHy+JsiLZssxJDiU8b4icy0sDC_vW7Y2spXOy5SA@mail.gmail.com>
References: <CAMk+s2SR9CGHy+JsiLZssxJDiU8b4icy0sDC_vW7Y2spXOy5SA@mail.gmail.com>
Message-ID: <73AA4C54-90BB-48A1-9F2F-023E31ECE1BA@comcast.net>


On Jul 11, 2015, at 4:46 PM, Luigi Marongiu wrote:

> Dear all,
> I am adding some text to the panels of a graph generated through the
> lattice function using the argument:
>   xyplot( ...,
>              panel =
>               function(x, y,...)
>               {
>                   panel.xyplot(x,y,...)
>                   panel.text(0,0,labels=V[panel.number()])
>               }
> 
> How can I regulate the font size of such text?

library(lattice)
  A <- data.frame(x = rnorm(100), y = rnorm(100))
  xyplot(y ~ x, data = A,
  panel= function(x,y,...){panel.points(x,y,...)
                           panel.text(0.5, 0.2, "Hello, world", cex=3)}
        )


> 
> Best regards
> luigi
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From rshepard at appl-ecosys.com  Sun Jul 12 02:21:39 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 11 Jul 2015 17:21:39 -0700 (PDT)
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
 xyplot() to ggplot()]
In-Reply-To: <alpine.LNX.2.11.1507111644170.28768@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
Message-ID: <alpine.LNX.2.11.1507111720310.28768@localhost>

On Sat, 11 Jul 2015, Rich Shepard wrote:

>  Attached are the .RData file from the cwd, the test.lyx file (lyx-2.1.3),
> and the two test files from /tmp/.../lyx_tmpbuf3/.

   The .RData and .lyx files were stripped off. I'll try sending them again.

Rich
-------------- next part --------------
#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrreprt
\begin_preamble
\date{}
\usepackage{textcomp,url,multicol}
%\setkomafont{sectioning}{\rmfamily}
\end_preamble
\options abstract=on
\use_default_options false
\begin_modules
natbibapa
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style humannat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

load('.RData')
\begin_inset Argument 1
status open

\begin_layout Plain Layout
echo=FALSE
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Some text in here; separating knitr chunks.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

carlin <- read.csv("./carlin.csv", header = TRUE, sep = ",", stringsAsFactors
 = F)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Next paragraph of text.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

carlin$sampdate <- as.Date(carlin$sampdate)
\end_layout

\begin_layout Plain Layout

str(carlin)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Another paragraph or two of text.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

summary(carlin)
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
Plots of these distributions are the next step ...
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

carlin.1 <- subset(carlin, select = siteid:CO3)
\end_layout

\begin_layout Plain Layout

carlin.2 <- subset(carlin, select = c(siteid, sampdate, Alk:Ca))
\end_layout

\begin_layout Plain Layout

carlin.3 <- subset(carlin, select = c(siteid, sampdate, Mg:Cr))
\end_layout

\begin_layout Plain Layout

carlin.4 <- subset(carlin, select = c(siteid, sampdate, Co:Hg))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To plot each chemical constituent's concentration as a function of collection
 date the data format needs to be reshaped from wide to long:
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

library(reshape2)
\end_layout

\begin_layout Plain Layout

carlin.1.melt <- melt(carlin.1, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.2.melt <- melt(carlin.2, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.3.melt <- melt(carlin.3, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\begin_layout Plain Layout

carlin.4.melt <- melt(carlin.4, na.rm = F, id.vars = c('siteid', 'sampdate'))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The scatter (X-Y) plots use only measured data; missing data are not included.
 These plots show the measured values over time for each measured constituent.
 For example, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig: scatter-plot group 1"

\end_inset

 shows that most chemical constitent concentrations are low and have low
 variability.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

library(lattice)
\end_layout

\begin_layout Plain Layout

xyplot(value ~ sampdate | variable, data=carlin.1.melt, rm.na = T, ylab = 'Measured
 Value', xlab = 'Date')
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig: scatter-plot group 1"

\end_inset

Variability of measured values in group 1 constituents over the period of
 record at the Carlin station.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Because river discharge (in cubic feet per second) spiked at about 8,000
 cfs in the mis-1980s the Y-axis for all panels in the figure have the save
 range.
 This compresses low values yet any single constituent can be plotted by
 itself and the details of its variability over time easily seen.
\end_layout

\begin_layout Standard
The second group of constituents (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig: scatter-plot group 2"

\end_inset

)
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

xyplot(value ~ sampdate | variable, data=carlin.2.melt, rm.na = T, ylab = 'Measured
 Value', xlab = 'Date'
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig: scatter-plot group 2"

\end_inset

Variability of measured values in group 2 constituents over the period of
 record at the Carlin station.
\end_layout

\end_inset


\end_layout

\end_inset

 reveals that nutrients (the nitrogen compounds, phosphate, and organic
 carbon) are all consistenly low with low variability while total dissolved
 solids (TDS) concentrations are very highly variable both within years
 (as shown by the different concentrations at the same date) and across
 the period of observations.
 Alkalinity and hardness are also quite variable at both time scales.
\end_layout

\end_body
\end_document

From rshepard at appl-ecosys.com  Sun Jul 12 02:30:35 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sat, 11 Jul 2015 17:30:35 -0700 (PDT)
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
 xyplot() to ggplot()]
In-Reply-To: <alpine.LNX.2.11.1507111720310.28768@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
	<alpine.LNX.2.11.1507111720310.28768@localhost>
Message-ID: <alpine.LNX.2.11.1507111730000.28768@localhost>

On Sat, 11 Jul 2015, Rich Shepard wrote:

>  The .RData and .lyx files were stripped off. I'll try sending them again.

   .RData didn't make it. Perhaps now.

Rich

From xie at yihui.name  Sun Jul 12 02:43:44 2015
From: xie at yihui.name (Yihui Xie)
Date: Sat, 11 Jul 2015 19:43:44 -0500
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
 xyplot() to ggplot()]
In-Reply-To: <alpine.LNX.2.11.1507111644170.28768@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
Message-ID: <CANROs4frwdCofQuuN2cMDHeDTAc9UP4f-av75J+0bd+5rUj8+g@mail.gmail.com>

Perhaps it is not obvious to you, but it is fairly obvious to me that
the R code in this code chunk is incomplete:

<<>>=
xyplot(value ~ sampdate | variable, data=carlin.2.melt, rm.na = T,
ylab = 'Measured Value', xlab = 'Date'
@

There is a ')' missing in the end. When you see errors from parse(),
that often means the code is not syntactically correct.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Sat, Jul 11, 2015 at 7:07 PM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> On Sat, 11 Jul 2015, Yihui Xie wrote:
>
>> I guess you didn't tell us you were compiling the document with dvips
>
>
>   That's how I preview lyx docs for the past decade-and-a-half or so. The
> final .pdf is generated with pdflatex.
>
>   There is now an issue when a second figure is added to the document: it
> won't compile.
>
>   Attached are the .RData file from the cwd, the test.lyx file (lyx-2.1.3),
> and the two test files from /tmp/.../lyx_tmpbuf3/.
>
>   Here are the lyx messages when I tried compiling the document with
> pdflatex:
>
> 16:51:11.255: Exporting ...
> 16:51:11.259: (buffer-export pdf2)
> 16:51:11.278: Rscript --verbose --no-save --no-restore
> "/usr/share/lyx/scripts/lyxknitr.R"
> "/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.Rnw"
> "/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.tex" ISO-8859-15
> "/home/rshepard/documents/white-papers/water-chem-anal/"
> 16:51:11.281: running
> 16:51:11.286:   '/usr/lib/R/bin/R --slave --no-restore --no-save
> --no-restore --file=/usr/share/lyx/scripts/lyxknitr.R --args
> -- /tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/test.Rnw
> -- / /tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpb
> 16:51:11.288: uf3/test.tex ISO-8859-15
> /home/rshepard/documents/white-papers/water-chem-anal/'
> 16:51:11.292: 16:51:11.697: Loading required package: methods
> 16:51:11.709: Loading required package: survival
> 16:51:11.712: Loading required package: graphics
> 16:51:11.721: Loading required package: stats
> 16:51:11.921: 16:51:11.923: Attaching package: ?NADA?
> 16:51:11.925: 16:51:11.927: The following object is masked from
> ?package:stats?:
> 16:51:11.929: 16:51:11.932:     cor
> 16:51:11.933: 16:51:11.969: 16:51:11.970: Attaching package: ?zoo?
> 16:51:11.972: 16:51:11.975: The following objects are masked from
> ?package:base?:
> 16:51:11.976: 16:51:11.979:     as.Date, as.Date.numeric
> 16:51:11.981: 16:51:12.091: 16:51:12.092: 16:51:12.094: processing file:
> /tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/test.Rnw
> 16:51:12.112:
>   |
>   |                                                                 |   0%
>   | 16:51:12.113:
>   |....                                                             |   6%
> 16:51:12.116:   ordinary text without R code
> 16:51:12.118: 16:51:12.120:
>   |
>   |........                                                         |  12%
> 16:51:12.123: label: unnamed-chunk-1 (with options) 16:51:12.126: List of 1
> 16:51:12.128:  $ echo: logi FALSE
> 16:51:12.130: 16:51:12.204:
>   |
>   |...........                                                      |  18%
> 16:51:12.205:   ordinary text without R code
> 16:51:12.207: 16:51:12.210:
>   |
>   |...............                                                  |  24%
> 16:51:12.213: label: unnamed-chunk-2
> 16:51:12.239:
>   |
>   |...................                                              |  29%
> 16:51:12.240:   ordinary text without R code
> 16:51:12.242: 16:51:12.244:
>   |
>   |.......................                                          |  35%
> 16:51:12.247: label: unnamed-chunk-3
> 16:51:12.293:
>   |
>   |...........................                                      |  41%
> 16:51:12.295:   ordinary text without R code
> 16:51:12.297: 16:51:12.299:
>   |
>   |...............................                                  |  47%
> 16:51:12.302: label: unnamed-chunk-4
> 16:51:12.344:
>   |
>   |..................................                               |  53%
> 16:51:12.346:   ordinary text without R code
> 16:51:12.348: 16:51:12.350:
>   |
>   |......................................                           |  59%
> 16:51:12.353: label: unnamed-chunk-5
> 16:51:12.432:
>   |
>   |..........................................                       |  65%
> 16:51:12.434:   ordinary text without R code
> 16:51:12.436: 16:51:12.438:
>   |
>   |..............................................                   |  71%
> 16:51:12.441: label: unnamed-chunk-6
> 16:51:12.484:
>   |
>   |..................................................               |  76%
> 16:51:12.486:   ordinary text without R code
> 16:51:12.488: 16:51:12.490:
>   |
>   |......................................................           |  82%
> 16:51:12.493: label: unnamed-chunk-7
> 16:51:12.837:
>   |
>   |.........................................................        |  88%
> 16:51:12.839:   ordinary text without R code
> 16:51:12.841: 16:51:12.843:
>   |
>   |.............................................................    |  94%
> 16:51:12.846: label: unnamed-chunk-8
> 16:51:12.849: Quitting from lines 101-102
> (/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/test.Rnw) 16:51:12.851: Error in
> parse(text = x, srcfile = src) : 16:51:12.853:   <text>:2:0: unexpected end
> of input
> 16:51:12.855: 1: xyplot(value ~ sampdate | variable, data=carlin.2.melt,
> rm.na = T, ylab = 'Measured Value', xlab = 'Date'
> 16:51:12.857:    ^
> 16:51:12.859: Calls: knit ... <Anonymous> -> parse_all ->
> parse_all.character -> parse
> 16:51:12.861: 16:51:12.862: Execution halted
> Systemcall.cpp (292): Systemcall: 'Rscript --verbose --no-save --no-restore
> "/usr/share/lyx/scripts/lyxknitr.R"
> "/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.Rnw"
> "/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.tex" ISO-8859-15
> "/home/rshepard/documents/white-papers/water-chem-anal/"' finished with exit
> code 1
> Error: Cannot convert file
> ----------------------------------------
> An error occurred while running:
> Rscript --verbose --no-save --no-restore "/usr/share/lyx/scripts/lyxknitr.R"
> "/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.Rnw"
> "/tmp/lyx_tmpdir.rGQZdOZ30938/lyx_tmpbuf3/""test.tex" ISO-8859-15
> "/home/rshepard/documents/white-papers/water-chem-anal/"
> 16:51:16.136: Error while exporting format: PDF (pdflatex)
>
>   There is no log file in the tempbuf. Note the references to
> unnamed-chunks. Each chunk is labeled with a number when opened in lyx; the
> two figures each have a distinct label. I don't see the specific error
> generated by adding a second figure to the document. If there are more files
> needed to reproduce this situation, let me know and I'll provide it.
>
> Rich


From dwinsemius at comcast.net  Sun Jul 12 02:47:01 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 11 Jul 2015 17:47:01 -0700
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
	xyplot() to ggplot()]
In-Reply-To: <alpine.LNX.2.11.1507111730000.28768@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
	<alpine.LNX.2.11.1507111720310.28768@localhost>
	<alpine.LNX.2.11.1507111730000.28768@localhost>
Message-ID: <DC884F1A-B8B4-42A6-9234-A75F8E27E8D4@comcast.net>


On Jul 11, 2015, at 5:30 PM, Rich Shepard wrote:

> On Sat, 11 Jul 2015, Rich Shepard wrote:
> 
>> The .RData and .lyx files were stripped off. I'll try sending them again.
> 
>  .RData didn't make it. Perhaps now.

Please stop trying to send data types that are explicitly not allowed. (I was surprised that you even got the mail server to forward a lyx file.)  The types known to pass scrutiny are .txt, .ps, .png, and .pdf.

> 
> Rich
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.CA.us  Sun Jul 12 02:50:13 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sat, 11 Jul 2015 17:50:13 -0700
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
	xyplot() to ggplot()]
In-Reply-To: <alpine.LNX.2.11.1507111730000.28768@localhost>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
	<alpine.LNX.2.11.1507111720310.28768@localhost>
	<alpine.LNX.2.11.1507111730000.28768@localhost>
Message-ID: <71C41858-2979-4E31-9A84-EA4431ACFA4F@dcn.davis.CA.us>

Read the Posting Guide. Most attachments get stripped on the list to discourage viruses and encourage small, reproducible examples in the body of the email.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 11, 2015 5:30:35 PM PDT, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>On Sat, 11 Jul 2015, Rich Shepard wrote:
>
>>  The .RData and .lyx files were stripped off. I'll try sending them
>again.
>
>   .RData didn't make it. Perhaps now.
>
>Rich
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sun Jul 12 02:51:00 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 11 Jul 2015 17:51:00 -0700
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
	xyplot() to ggplot()]
In-Reply-To: <DC884F1A-B8B4-42A6-9234-A75F8E27E8D4@comcast.net>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
	<alpine.LNX.2.11.1507111720310.28768@localhost>
	<alpine.LNX.2.11.1507111730000.28768@localhost>
	<DC884F1A-B8B4-42A6-9234-A75F8E27E8D4@comcast.net>
Message-ID: <07980532-D282-4846-81BB-EEB8E2F5B4A4@comcast.net>


On Jul 11, 2015, at 5:47 PM, David Winsemius wrote:

> 
> On Jul 11, 2015, at 5:30 PM, Rich Shepard wrote:
> 
>> On Sat, 11 Jul 2015, Rich Shepard wrote:
>> 
>>> The .RData and .lyx files were stripped off. I'll try sending them again.
>> 
>> .RData didn't make it. Perhaps now.
> 
> Please stop trying to send data types that are explicitly not allowed. (I was surprised that you even got the mail server to forward a lyx file.)  The types known to pass scrutiny are .txt, .ps, .png, and .pdf.
> 

You could use `dput` or `dump` to create an ASCII version of either a .Rdata or a .R file as long as you made the file extension .txt. Instructions used to be in the Posting Guide, but I'm not sure they were reetained in hte most recent versions. (Nobody read them, anyway.) <Sigh>

-- 
David.


>> 
>> Rich
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Sun Jul 12 04:09:11 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 11 Jul 2015 19:09:11 -0700
Subject: [R] Do grep() and strsplit() use different regex engines?
In-Reply-To: <alpine.OSX.2.11.1507111520360.1005@charles-berrys-macbook.local>
References: <CAGxFJbR9FnDsnOK0zeg63kgmqPtn=kXGKyWTQgq96_qJkb=RpQ@mail.gmail.com>
	<EBD74F5E-77A9-40C5-A2FE-E77D8983194B@comcast.net>
	<CAGxFJbTwgVADYLuxC6fXtGq8G4F641GnUKiN7fi98S3CTKZ5uA@mail.gmail.com>
	<alpine.OSX.2.11.1507111520360.1005@charles-berrys-macbook.local>
Message-ID: <CAGxFJbSnO10mTQvn2tV432t0MUQTqWpft=61BaQ6W=cBdoDLdg@mail.gmail.com>

Thanks, Chuck (he says, red-faced).

Maybe I should read the man page more carefully ...!

And as for grep(), similar issues: (from ?grep)

"POSIX 1003.2 mode of gsub and gregexpr does not work correctly with
repeated word-boundaries (e.g., pattern = "\b"). Use perl = TRUE for
such matches (but that may not work as expected with non-ASCII inputs,
as the meaning of ?word? is system-dependent)."

And no, I don't think anything needs to be added to ?strsplit. The man
page writers spelled it out clearly. They're not responsible for my
dummheit.

My apologies to all for wasted bandwidth...


Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 11, 2015 at 4:26 PM, Charles C. Berry <ccberry at ucsd.edu> wrote:
> On Sat, 11 Jul 2015, Bert Gunter wrote:
>
>> David/Jeff:
>>
>> Thank you both.
>>
>> You seem to confirm that my observation of an "infelicity" in
>> strsplit() is real. That is most helpful.
>>
>> I found nothing in David's message 2 code that was surprising. That
>> is, the splits shown conform to what I would expect from "\\b" . But
>> not to what I originally showed and David enlarged upon in his first
>> message. I still don't really get why a split should occur at every
>> letter.
>>
>> Jeff may very well have found the explanation, but I have not gone
>> through his code.
>>
>> If the infelicities noted (are there more?) by David and me are not
>> really bugs -- and I would be frankly surprised if they were -- I
>> would suggest that perhaps they deserve mention in the strsplit() man
>> page. Something to the effect that "\b and \< should not be used as
>> split characters..." .
>
>
> Bert et al,
>
> ?strsplit already says:
>
> "If empty matches occur, in particular if split has length 0, x is split
> into single characters."
>
> And there are various ways that empty matches can happen besides using "\\b"
> as the split arg. But there would be no harm in adding your cases to 'in
> particular ...'
>
> The comment in the code (src/main/grep.c: line 493) suggests this was a
> deliberate decision. However, similar functions in other languages do not do
> this.
>
> For example, emacs `(split-string "red green" "\\b")' gives
>
>         ("" "red" " " "green" "")
>
> as the result.
>
> Chuck


From arne.henningsen at gmail.com  Sun Jul 12 08:23:57 2015
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Sun, 12 Jul 2015 08:23:57 +0200
Subject: [R] NaN produced from log() with positive input
In-Reply-To: <trinity-f817d98c-77a9-4dad-9eae-ca80864933b8-1436370769520@3capp-mailcom-bs11>
References: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>
	<CAMTWbJhOfaSvmvbKfeg4r8Vnytwy0h86ACJZNe++FBExFinRyA@mail.gmail.com>
	<trinity-f817d98c-77a9-4dad-9eae-ca80864933b8-1436370769520@3capp-mailcom-bs11>
Message-ID: <CAMTWbJgTwf5uxHzKOSaK2ztgf5c344zR7fxWZ0npvnj5r2AsbQ@mail.gmail.com>

Dear Maram

On 8 July 2015 at 17:52, Maram Salem <marammagdysalem at gmx.com> wrote:
> Dear Arne,
>
> On a second thought, as per your mail "the warning messages occur each time,
> when maxLik() tries to calculate
> the logLik value for theta[1] <= 0, theta[1] + theta[2] <= 0, theta[3] <= 0
> or something similar."
>
> The component of the theta vector are all indeed strictly positive, and the
> initial values I used are c(40,50,2). and this means that " theta[1] > 0,
> theta[1] + theta[2] > 0, and theta[3] > 0 ".  These initial values are the
> parameter values that were used for generating the data (the C and T
> vectors). That's why I don't know why the warnings occur in the first place
> and why the estimates are far away from the initial values.a
> Any suggestions?

- don't send your messages twice;

- do what I suggested in my previous e-mail;

- increase the number of observations;

- check your log-likelihood function;

- use an optimisation algorithm that does not use derivatives;

- use numeric derivatives in the optimisation;

- check your function for returning the derivatives of the
log-likelihood function, e.g. with compareDerivatives();

- check the data generating process

Best regards,
Arne

-- 
Arne Henningsen
http://www.arne-henningsen.name


From rshepard at appl-ecosys.com  Sun Jul 12 14:56:44 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Sun, 12 Jul 2015 05:56:44 -0700 (PDT)
Subject: [R] Knitr/Lattice/Lyx [was: Trellis Plots: translating lattice
 xyplot() to ggplot()]
In-Reply-To: <CANROs4frwdCofQuuN2cMDHeDTAc9UP4f-av75J+0bd+5rUj8+g@mail.gmail.com>
References: <alpine.LNX.2.11.1507101404490.10467@localhost>
	<CABdHhvEmxD4SNMEHuHr86CnPX7-19mqt1haq+F-Jr1kL1UKVZQ@mail.gmail.com>
	<alpine.LNX.2.11.1507101454360.10467@localhost>
	<1BC5B9F5-D368-4BFE-8F3A-43B671DBEEE9@comcast.net>
	<alpine.LNX.2.11.1507101717020.10467@localhost>
	<CANROs4cMSWapZzP211cZb5697nGn_FOAJ9xatBaVhgXjqoNzHw@mail.gmail.com>
	<alpine.LNX.2.11.1507110657310.25203@localhost>
	<CANROs4cjvWsmP2phCKW8QYXv+yHNE+WPtYDuU=jQY_DkzJ_E+A@mail.gmail.com>
	<alpine.LNX.2.11.1507111644170.28768@localhost>
	<CANROs4frwdCofQuuN2cMDHeDTAc9UP4f-av75J+0bd+5rUj8+g@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507120554290.3957@localhost>

On Sat, 11 Jul 2015, Yihui Xie wrote:

> There is a ')' missing in the end. When you see errors from parse(),
> that often means the code is not syntactically correct.

   Most sincere appologies for my not catching that each time I looked at the
code. Will be more thorough and put the problem aside for a while before
looking for the problem again. Won't be repeated.

Rich


From rhurlin at gwdg.de  Sun Jul 12 16:39:37 2015
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Sun, 12 Jul 2015 16:39:37 +0200
Subject: [R] Rmarkdown / knitr naming the output file
In-Reply-To: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>
References: <20150706142414.Horde.9leqLcL5GQ2Y92lXS1cOxQ1@webmail.um.es>
Message-ID: <55A27C29.90903@gwdg.de>

Am 06.07.2015 um 14:24 schrieb AURORA GONZALEZ VIDAL:
> Hello.
> I have a question for Rmarkdown users.
> 
> Is there any way to give a name to the output document inside the Rmd?
> 
> For example, my rmd's name is "bb.Rmd" but when I knitr to pdf I want it to
> name the pdf differently than "bb.pdf", for example, "doc1.pdf". Is there
> any way to do this?

Here is another way of changing the output filename. I am using it from
R's commandline:

rmarkdown::render(
  input="filename_orig.R",
  output_format="pdf_document",
  output_file="filename_different.pdf"
)

Greetings,
Rainer Hurling


> 
> Thank you very much
> 
> 
> ------
> Aurora Gonz?lez Vidal
> 
> Secci?n Apoyo Estad?stico.
> Servicio de Apoyo a la Investigaci?n (SAI).
> Vicerrectorado de Investigaci?n.
> Universidad de Murcia
> Edif. SACE . Campus de Espinardo.
> 30100 Murcia
> 
> @. aurora.gonzalez2 at um.es
> T. 868 88 7315
> F. 868 88 7302
> www.um.es/sai
> www.um.es/ae


From marammagdysalem at gmx.com  Sun Jul 12 16:52:33 2015
From: marammagdysalem at gmx.com (Maram Salem)
Date: Sun, 12 Jul 2015 16:52:33 +0200
Subject: [R] NaN produced from log() with positive input
In-Reply-To: <CAMTWbJgTwf5uxHzKOSaK2ztgf5c344zR7fxWZ0npvnj5r2AsbQ@mail.gmail.com>
References: <F91C542C-8A83-48B2-A60E-F11B972382CA@gmx.com>
	<CAMTWbJhOfaSvmvbKfeg4r8Vnytwy0h86ACJZNe++FBExFinRyA@mail.gmail.com>
	<trinity-f817d98c-77a9-4dad-9eae-ca80864933b8-1436370769520@3capp-mailcom-bs11>
	<CAMTWbJgTwf5uxHzKOSaK2ztgf5c344zR7fxWZ0npvnj5r2AsbQ@mail.gmail.com>
Message-ID: <9894EFA4-BBFE-43C7-97BE-56F91B873900@gmx.com>

Dear Arne,
Will do. Thanks for helping.
Maram

Sent from my iPhone

> On Jul 12, 2015, at 8:23 AM, Arne Henningsen <arne.henningsen at gmail.com> wrote:
> 
> Dear Maram
> 
>> On 8 July 2015 at 17:52, Maram Salem <marammagdysalem at gmx.com> wrote:
>> Dear Arne,
>> 
>> On a second thought, as per your mail "the warning messages occur each time,
>> when maxLik() tries to calculate
>> the logLik value for theta[1] <= 0, theta[1] + theta[2] <= 0, theta[3] <= 0
>> or something similar."
>> 
>> The component of the theta vector are all indeed strictly positive, and the
>> initial values I used are c(40,50,2). and this means that " theta[1] > 0,
>> theta[1] + theta[2] > 0, and theta[3] > 0 ".  These initial values are the
>> parameter values that were used for generating the data (the C and T
>> vectors). That's why I don't know why the warnings occur in the first place
>> and why the estimates are far away from the initial values.a
>> Any suggestions?
> 
> - don't send your messages twice;
> 
> - do what I suggested in my previous e-mail;
> 
> - increase the number of observations;
> 
> - check your log-likelihood function;
> 
> - use an optimisation algorithm that does not use derivatives;
> 
> - use numeric derivatives in the optimisation;
> 
> - check your function for returning the derivatives of the
> log-likelihood function, e.g. with compareDerivatives();
> 
> - check the data generating process
> 
> Best regards,
> Arne
> 
> -- 
> Arne Henningsen
> http://www.arne-henningsen.name


From aldi at dsgmail.wustl.edu  Sun Jul 12 16:45:56 2015
From: aldi at dsgmail.wustl.edu (aldi)
Date: Sun, 12 Jul 2015 09:45:56 -0500
Subject: [R] merge: right set overwrite left set
Message-ID: <55A27DA4.4010008@dsgmail.wustl.edu>

Hi,
I have two sets of data x.HHu and y.HHo, rows are IDs and columns are 
individuals. I do not know in advance indv or HHid, both of them will be 
captured from the data. As the y.HHo set updates, y.HHo set has better 
information then x.HHu set. Thus I want a merge where right set 
overwrites left set info based on HHid, i.e. to overwrite x.HHu set with 
y.HHo set but keep any extra info from the x.HHu set that is not present 
in y.HHo set.
HHids will be complete based on z.map, with the corresponding positions.
I am having trouble with the part after this line: ### 
============================================+++++++++++++++++++++++++++
I am thinking that I am creating new columns "position" "indv1" and 
"indv2", but R is interpreting them as row information.
See the expected final table at the end. HHid is common, indv3 is from 
x.HHu, and the rest position and indv1 and indv2 are from y.HHo
Any suggestions are appreciated.
Thank you in advance,
Aldi

x.HHu<- data.frame(
            HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10')
          , indv1 = c( 2, 0, 2 , 0, 2, 0)
          , indv2 = c( 0, NA, 2, 2, 2, 2)
          , ind3 = c( 0, 0, 0, 0, 0, 0)
          )
### the HHo data will be the top set to overwrite any HHu data, when 
they exist, thinking that HHo are better than HHu results
### when they are available

y.HHo<-data.frame(HHid=c('HH1', 'HH2','HH5', 'HH3', 'HH10')
          , indv1 = c(2, 0, 2, 0, NA)
          , indv2 = c(0, 2, 2, 1, 2)
          )

z.map<-data.frame(HHid = c('HH1', 'HH2', 'HH3', 'HH4', 'HH5', 
'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11')
                 , position= c(10,20,30,42,55,66,81,75,92,101,111)
                 )
### see objects
x.HHu
y.HHo
z.map
### now sort the map by position, this sorted map will be used to sort 
finally all data
z.map<-z.map[with(z.map, order(position)), ]
z.map

### First I introduce position to both sets so I can sort them in 
advance by position.
x.HHu.map <-merge( z.map, x.HHu, by='HHid', all=T)
x.HHu.map<-x.HHu.map[with(x.HHu.map, order(position)), ]
x.HHu.map

y.HHo.map <-merge( z.map, y.HHo, by='HHid', all= T)
y.HHo.map<-y.HHo.map[with(y.HHo.map, order(position)), ]
y.HHo.map

### now merge HHu  and HHo  with the hope to overwrite the HHu set with 
HHo wherever they overlap by column names.
zzz <- merge(x.HHu.map, y.HHo.map, by='HHid', all=T)
zzz
### find common variable names in two sets

commonNames <- names(x.impu.map)[which(colnames(x.impu.map) %in% 
colnames(y.geno.map))]

## remove HHid wich is common for x and y, but work with the rest of columns
commonNames<-commonNames[-c(1)]

### ============================================+++++++++++++++++++++++++++
for(i in 1:length(commonNames)){

print(commonNames[i])
zzz$commonNames[i] <- NA

print(paste("zzz","$",commonNames[i],".y",sep=""))

zzz$commonNames[i] <- zzz[,paste(commonNames[i],".y",sep="")]

### paste(zzz$commonNames[i],".x",sep='') <- NULL;
### paste(zzz$commonNames[i],".y",sep='') <- NULL;

}
zzz

The final expected set has to be: HHid is common, indv3 is from x.HHu, 
and the rest position and indv1 and indv2 are from y.HHo
    HHid     position     ind3  indv1 indv2
1   HH1         10          0     2       0
2  HH10        101          0    NA       2
3  HH11        111         NA    NA      NA
4   HH2         20          0     0       2
5   HH3         30          0     0       1
6   HH4         42          0    NA      NA
7   HH5         55          0     2       2
8   HH6         66         NA    NA      NA
9   HH7         75         NA    NA      NA
10  HH8         81         NA    NA      NA
11  HH9         92         NA    NA      NA

-- 


	[[alternative HTML version deleted]]


From istazahn at gmail.com  Sun Jul 12 19:06:08 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Sun, 12 Jul 2015 13:06:08 -0400
Subject: [R] merge: right set overwrite left set
In-Reply-To: <55A27DA4.4010008@dsgmail.wustl.edu>
References: <55A27DA4.4010008@dsgmail.wustl.edu>
Message-ID: <CA+vqiLE3OYAzDa4es6ugua0Ee5YQAE9ZmNb+zak1FGy7QTLmxg@mail.gmail.com>

I think this does what you want:

## find idiv coloumns in x.HHu.map that don't exist in y.HHo.map
x.HHu.map <- x.HHu.map[
    c("HHid",
      "position",
      names(x.HHu.map)[
               !names(x.HHu.map)
               %in% names(y.HHo.map)]
      )]
## merge, adding extra column from x.HHu.map
zzz <- merge(y.HHo.map, x.HHu.map, by=c('HHid', 'position'), all=T)
## order by HHid
zzz <- zzz[order(zzz$HHid),]

Best,
Ista

On Sun, Jul 12, 2015 at 10:45 AM, aldi <aldi at dsgmail.wustl.edu> wrote:
> Hi,
> I have two sets of data x.HHu and y.HHo, rows are IDs and columns are
> individuals. I do not know in advance indv or HHid, both of them will be
> captured from the data. As the y.HHo set updates, y.HHo set has better
> information then x.HHu set. Thus I want a merge where right set
> overwrites left set info based on HHid, i.e. to overwrite x.HHu set with
> y.HHo set but keep any extra info from the x.HHu set that is not present
> in y.HHo set.
> HHids will be complete based on z.map, with the corresponding positions.
> I am having trouble with the part after this line: ###
> ============================================+++++++++++++++++++++++++++
> I am thinking that I am creating new columns "position" "indv1" and
> "indv2", but R is interpreting them as row information.
> See the expected final table at the end. HHid is common, indv3 is from
> x.HHu, and the rest position and indv1 and indv2 are from y.HHo
> Any suggestions are appreciated.
> Thank you in advance,
> Aldi
>
> x.HHu<- data.frame(
>             HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10')
>           , indv1 = c( 2, 0, 2 , 0, 2, 0)
>           , indv2 = c( 0, NA, 2, 2, 2, 2)
>           , ind3 = c( 0, 0, 0, 0, 0, 0)
>           )
> ### the HHo data will be the top set to overwrite any HHu data, when
> they exist, thinking that HHo are better than HHu results
> ### when they are available
>
> y.HHo<-data.frame(HHid=c('HH1', 'HH2','HH5', 'HH3', 'HH10')
>           , indv1 = c(2, 0, 2, 0, NA)
>           , indv2 = c(0, 2, 2, 1, 2)
>           )
>
> z.map<-data.frame(HHid = c('HH1', 'HH2', 'HH3', 'HH4', 'HH5',
> 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11')
>                  , position= c(10,20,30,42,55,66,81,75,92,101,111)
>                  )
> ### see objects
> x.HHu
> y.HHo
> z.map
> ### now sort the map by position, this sorted map will be used to sort
> finally all data
> z.map<-z.map[with(z.map, order(position)), ]
> z.map
>
> ### First I introduce position to both sets so I can sort them in
> advance by position.
> x.HHu.map <-merge( z.map, x.HHu, by='HHid', all=T)
> x.HHu.map<-x.HHu.map[with(x.HHu.map, order(position)), ]
> x.HHu.map
>
> y.HHo.map <-merge( z.map, y.HHo, by='HHid', all= T)
> y.HHo.map<-y.HHo.map[with(y.HHo.map, order(position)), ]
> y.HHo.map
>
> ### now merge HHu  and HHo  with the hope to overwrite the HHu set with
> HHo wherever they overlap by column names.
> zzz <- merge(x.HHu.map, y.HHo.map, by='HHid', all=T)
> zzz
> ### find common variable names in two sets
>
> commonNames <- names(x.impu.map)[which(colnames(x.impu.map) %in%
> colnames(y.geno.map))]
>
> ## remove HHid wich is common for x and y, but work with the rest of columns
> commonNames<-commonNames[-c(1)]
>
> ### ============================================+++++++++++++++++++++++++++
> for(i in 1:length(commonNames)){
>
> print(commonNames[i])
> zzz$commonNames[i] <- NA
>
> print(paste("zzz","$",commonNames[i],".y",sep=""))
>
> zzz$commonNames[i] <- zzz[,paste(commonNames[i],".y",sep="")]
>
> ### paste(zzz$commonNames[i],".x",sep='') <- NULL;
> ### paste(zzz$commonNames[i],".y",sep='') <- NULL;
>
> }
> zzz
>
> The final expected set has to be: HHid is common, indv3 is from x.HHu,
> and the rest position and indv1 and indv2 are from y.HHo
>     HHid     position     ind3  indv1 indv2
> 1   HH1         10          0     2       0
> 2  HH10        101          0    NA       2
> 3  HH11        111         NA    NA      NA
> 4   HH2         20          0     0       2
> 5   HH3         30          0     0       1
> 6   HH4         42          0    NA      NA
> 7   HH5         55          0     2       2
> 8   HH6         66         NA    NA      NA
> 9   HH7         75         NA    NA      NA
> 10  HH8         81         NA    NA      NA
> 11  HH9         92         NA    NA      NA
>
> --
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Sun Jul 12 20:35:48 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sun, 12 Jul 2015 11:35:48 -0700 (PDT)
Subject: [R] merge: right set overwrite left set
In-Reply-To: <55A27DA4.4010008@dsgmail.wustl.edu>
References: <55A27DA4.4010008@dsgmail.wustl.edu>
Message-ID: <alpine.BSF.2.00.1507121116050.12268@pedal.dcn.davis.ca.us>

I get confused by your use of the position map table. If I follow your 
description toward your desired result, I take a different route that 
makes sense to me. Perhaps it will make sense to you as well. The key idea 
is to make individual comparisons of the values for each combination of 
HHid and indv, regardless of where they are in the original data frames.

Below are two different syntactic representations of my understanding of 
your problem. They differ only in the approach taken to strip away syntax 
clutter. I start by making the HHid identifiers character values in the 
original data frames, because their respective factor levels in the two 
data frames would not necessarily correspond.

x.HHu <- data.frame(
     HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10' )
   , indv1 = c( 2, 0, 2 , 0, 2, 0 )
   , indv2 = c( 0, NA, 2, 2, 2, 2 )
   , ind3 = c( 0, 0, 0, 0, 0, 0 )
   , stringsAsFactors = FALSE # avoid creating HHid as a factor
)
y.HHo <- data.frame(
     HHid=c( 'HH1', 'HH2','HH5', 'HH3', 'HH10' )
   , indv1 = c( 2, 0, 2, 0, NA )
   , indv2 = c( 0, 2, 2, 1, 2 )
   , stringsAsFactors = FALSE
)
z.map <- data.frame(
     HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5'
             , 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11' )
   , position= c( 10, 20, 30, 42, 55, 66, 81, 75, 92, 101, 111 )
   , stringsAsFactors = FALSE
)


# reshape2 solution
library(reshape2)

x.HHu.long <- melt( x.HHu, "HHid", variable.name = "indv" )
x.HHu.long$indv <- as.character( x.HHu.long$indv )
y.HHo.long <- melt( y.HHo, "HHid", variable.name = "indv" )
y.HHo.long$indv <- as.character( y.HHo.long$indv )
xy.HH.long <- merge( x.HHu.long
                    , y.HHo.long
                    , by = c( "HHid", "indv" )
                    , all = TRUE )
xy.HH.long$value <- with( xy.HH.long
                         , ifelse( is.na( value.y )
                                 , value.x
                                 , value.y ) )
xy.HH0 <- dcast( xy.HH.long, HHid ~ indv )
xy.HH <- merge( xy.HH0, z.map, all=TRUE )
xy.HH <- xy.HH[ order( xy.HH$HHid ), ]
# compare xy.HH with zzz ... I think there is an error in zzz for 
# HH10/indv1, because NA should not be considered more informative
# than 0...

# tidyr/dplyr solution
library(tidyr)
library(dplyr)

# define a common processing sequence
lengthen <- (   . # period is placeholder for data frame
             %>% gather( indv, value, -HHid )
             %>% mutate( indv = as.character( indv ) )
             )
x.HHu.dlong <- x.HHu %>% lengthen
y.HHo.dlong <- y.HHo %>% lengthen
xy.HH.d <- (   x.HHu.dlong
            %>% full_join( y.HHo.dlong, by= c( "HHid", "indv" ) )
            %>% transmute( HHid = HHid
                         , indv = indv
                         , value = ifelse( is.na( value.y )
                                         , value.x
                                         , value.y )
                         )
            %>% spread( indv, value )
            %>% full_join( z.map, by="HHid" )
            %>% arrange( HHid )
            %>% as.data.frame
            )

On Sun, 12 Jul 2015, aldi wrote:

> Hi,
> I have two sets of data x.HHu and y.HHo, rows are IDs and columns are
> individuals. I do not know in advance indv or HHid, both of them will be
> captured from the data. As the y.HHo set updates, y.HHo set has better
> information then x.HHu set. Thus I want a merge where right set
> overwrites left set info based on HHid, i.e. to overwrite x.HHu set with
> y.HHo set but keep any extra info from the x.HHu set that is not present
> in y.HHo set.
> HHids will be complete based on z.map, with the corresponding positions.
> I am having trouble with the part after this line: ###
> ============================================+++++++++++++++++++++++++++
> I am thinking that I am creating new columns "position" "indv1" and
> "indv2", but R is interpreting them as row information.
> See the expected final table at the end. HHid is common, indv3 is from
> x.HHu, and the rest position and indv1 and indv2 are from y.HHo
> Any suggestions are appreciated.
> Thank you in advance,
> Aldi
>
> x.HHu<- data.frame(
>            HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10')
>          , indv1 = c( 2, 0, 2 , 0, 2, 0)
>          , indv2 = c( 0, NA, 2, 2, 2, 2)
>          , ind3 = c( 0, 0, 0, 0, 0, 0)
>          )
> ### the HHo data will be the top set to overwrite any HHu data, when
> they exist, thinking that HHo are better than HHu results
> ### when they are available
>
> y.HHo<-data.frame(HHid=c('HH1', 'HH2','HH5', 'HH3', 'HH10')
>          , indv1 = c(2, 0, 2, 0, NA)
>          , indv2 = c(0, 2, 2, 1, 2)
>          )
>
> z.map<-data.frame(HHid = c('HH1', 'HH2', 'HH3', 'HH4', 'HH5',
> 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11')
>                 , position= c(10,20,30,42,55,66,81,75,92,101,111)
>                 )
> ### see objects
> x.HHu
> y.HHo
> z.map
> ### now sort the map by position, this sorted map will be used to sort
> finally all data
> z.map<-z.map[with(z.map, order(position)), ]
> z.map
>
> ### First I introduce position to both sets so I can sort them in
> advance by position.
> x.HHu.map <-merge( z.map, x.HHu, by='HHid', all=T)
> x.HHu.map<-x.HHu.map[with(x.HHu.map, order(position)), ]
> x.HHu.map
>
> y.HHo.map <-merge( z.map, y.HHo, by='HHid', all= T)
> y.HHo.map<-y.HHo.map[with(y.HHo.map, order(position)), ]
> y.HHo.map
>
> ### now merge HHu  and HHo  with the hope to overwrite the HHu set with
> HHo wherever they overlap by column names.
> zzz <- merge(x.HHu.map, y.HHo.map, by='HHid', all=T)
> zzz
> ### find common variable names in two sets
>
> commonNames <- names(x.impu.map)[which(colnames(x.impu.map) %in%
> colnames(y.geno.map))]
>
> ## remove HHid wich is common for x and y, but work with the rest of columns
> commonNames<-commonNames[-c(1)]
>
> ### ============================================+++++++++++++++++++++++++++
> for(i in 1:length(commonNames)){
>
> print(commonNames[i])
> zzz$commonNames[i] <- NA
>
> print(paste("zzz","$",commonNames[i],".y",sep=""))
>
> zzz$commonNames[i] <- zzz[,paste(commonNames[i],".y",sep="")]
>
> ### paste(zzz$commonNames[i],".x",sep='') <- NULL;
> ### paste(zzz$commonNames[i],".y",sep='') <- NULL;
>
> }
> zzz
>
> The final expected set has to be: HHid is common, indv3 is from x.HHu,
> and the rest position and indv1 and indv2 are from y.HHo
>    HHid     position     ind3  indv1 indv2
> 1   HH1         10          0     2       0
> 2  HH10        101          0    NA       2
> 3  HH11        111         NA    NA      NA
> 4   HH2         20          0     0       2
> 5   HH3         30          0     0       1
> 6   HH4         42          0    NA      NA
> 7   HH5         55          0     2       2
> 8   HH6         66         NA    NA      NA
> 9   HH7         75         NA    NA      NA
> 10  HH8         81         NA    NA      NA
> 11  HH9         92         NA    NA      NA
>
> -- 
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From r.turner at auckland.ac.nz  Sun Jul 12 23:12:03 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 13 Jul 2015 09:12:03 +1200
Subject: [R] [FORGED]  data$variable=factor(....) <NA> <NA> <NA>
In-Reply-To: <CAA4Hcip2tp1gAdYErQ+OFjmwvzL=a7EVRJjcWQbyVJkBe5f_4Q@mail.gmail.com>
References: <CAA4Hcirhhuv4=qCCmp3z+PCWH_pqBdnO6fba1JdyQYdqz1bVmA@mail.gmail.com>
	<55A1A00A.70205@auckland.ac.nz>
	<CAA4Hcip2tp1gAdYErQ+OFjmwvzL=a7EVRJjcWQbyVJkBe5f_4Q@mail.gmail.com>
Message-ID: <55A2D823.5080603@auckland.ac.nz>


(1) Please keep the discourse on list.

(2) Moral of your story:  Don't use Excel --- for *anything*!!!

(3) Why didn't you follow my suggestion?

(4) Naturally you get NAs!  There are no levels of "1" or "2" in your 
data.  The levels are "F" and "M", for crying out loud!!!  Why *on 
earth* did you say "levels=c(1:2)"?  This could never possibly make any 
sense at all.

(5) And by the way, why on earth do you write "c(1:2)" rather than just 
"1:2"?  What do you think the "c()" is doing for you?  Understand what 
things *mean*; don't just slap code down and hope.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 13/07/15 00:25, Dagmar Jurankov? wrote:
> Hello.
>
> There was a gap in front of F and M in my Excel table, thats why there
> were more levels of F and M. I corrected it (and saved as csv) and it
> still shows NA NA NA.
>
>
> selbst <- read.csv("C:/Users/Dadka/Desktop/Rcsv/doc.ex.csv/selbst.csv")
>  > View(selbst) > df=
> read.csv("C:/Users/Dadka/Desktop/Rcsv/doc.ex.csv/selbst.csv") > selbst$q_2   [1] F F M F F M F M F M M F F M M F M F F F F F F M M F M F F F M F F F F F F F F F F M F F M F M F F F F
> [52] F F F M M M M F M F F F F F F M F
> Levels: F M
>>attributes(selbst$q_2) $levels
> [1] "F" "M"
>
> $class
> [1] "factor"
>
>>selbst$q_2= factor(selbst$q_2, levels=c(1:2),
> labels=c("F","M"),exclude=NA, nmax=NA) > selbst$q_2 [1] <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> [21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> [41] <NA> <NA> <NA>
> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
> <NA> <NA> <NA> [61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> Levels: F M
>
>
> I think the problem might be in my csv document.
> Screenshot in png format is attached below.
> Could you maybe have a look at it please?
> Thank you.
>
> 2015-07-12 1:00 GMT+02:00 Rolf Turner <r.turner at auckland.ac.nz
> <mailto:r.turner at auckland.ac.nz>>:
>
>
>     Try:
>
>     ggg <- c("F","M","F",M")
>     data$gender <- factor(ggg[data$gender])
>
>     This in effect converts the (spurious) " F" and " M" levels into "F"
>     and "M" respectively, giving you a factor with the two levels that
>     you really want.
>
>     cheers,
>
>     Rolf Turner
>
>     P. S.  *Not* a good idea to use "data" as the name of your data frame.
>     See fortune("dog").
>
>     R. T.
>
>     --
>     Technical Editor ANZJS
>     Department of Statistics
>     University of Auckland
>     Phone: +64-9-373-7599 ext. 88276 <tel:%2B64-9-373-7599%20ext.%2088276>
>
>
>     On 12/07/15 07:21, Dagmar Jurankov? wrote:
>
>         Hello everybody, I have a problem with R.
>
>
>         I uploaded a questionnaire saved as csv into R and I tried to test
>         independence between two variables.
>
>
>
>         data <- read.csv("C:/Users/Me/Desktop/data.csv")>   View(data)> df =
>         read.csv("C:/Users/Me/Desktop/data.csv")> ls()
>         [1] "df"     "data"> attributes(data$gender)
>         $levels
>         [1] " F" " M" "F"  "M"
>
>         $class
>         [1] "factor"
>
>
>         I changed my variable "gender" into a factor using:
>
>
>         data$gender=factor(data$gender, levels=c(1:2), labels= c( "F", "M"),
>         exclude= NA, nmax= NA).
>
>
>         Then I wrote data$gender and the only thing i got was:
>
>
>         [1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>         <NA> <NA>
>         <NA> <NA> <NA> <NA> <NA> <NA>
>
>         [21] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>         <NA> <NA>
>         <NA> <NA> <NA> <NA> <NA> <NA>
>
>         [41] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>         <NA> <NA>
>         <NA> <NA> <NA> <NA> <NA> <NA>
>
>         [61] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
>
>         Levels: F M
>
>
>         Does anybody know why?
>
>
>         -My csv doc in the column gender is filled out properly.
>         (M=Male, F= Female)
>
>         -My imported dataset in R is complete (all values)
>
>
>         ! I have done this with a different excel document and it worked out
>         without any problems. I am really clueless. I cant go further
>         and compare
>         the variables and do t-tests without this working.
>
>
>         Could someone please help me out?
>
>         Thank you.


From sreenath.rajur at macfast.ac.in  Mon Jul 13 06:27:55 2015
From: sreenath.rajur at macfast.ac.in (sreenath)
Date: Sun, 12 Jul 2015 21:27:55 -0700 (PDT)
Subject: [R] Putting Zero in table using R
Message-ID: <1436761675781-4709807.post@n4.nabble.com>

I have exel table having 39 coloumns ,and some cells are balnk.How can i put
zero in these empty cells using R



--
View this message in context: http://r.789695.n4.nabble.com/Putting-Zero-in-table-using-R-tp4709807.html
Sent from the R help mailing list archive at Nabble.com.


From dwinsemius at comcast.net  Mon Jul 13 07:51:58 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 12 Jul 2015 22:51:58 -0700
Subject: [R] Putting Zero in table using R
In-Reply-To: <1436761675781-4709807.post@n4.nabble.com>
References: <1436761675781-4709807.post@n4.nabble.com>
Message-ID: <C9155859-B74C-4C81-91C5-6CA53A6E4FBF@comcast.net>


On Jul 12, 2015, at 9:27 PM, sreenath wrote:

> I have exel table having 39 coloumns ,and some cells are balnk.How can i put
> zero in these empty cells using R
> 

If you export with tab or comma as the separator, it will get imported as an NA and you can change the NA's to 0.

> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Putting-Zero-in-table-using-R-tp4709807.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From julleeyaw at yahoo.ca  Mon Jul 13 13:34:23 2015
From: julleeyaw at yahoo.ca (Julie Lee-Yaw)
Date: Mon, 13 Jul 2015 11:34:23 +0000 (UTC)
Subject: [R]  Party package: varimp(...,
 conditional=TRUE) error: term 1 would require 9e+12 columns (fwd)
Message-ID: <2080569471.1206396.1436787263448.JavaMail.yahoo@mail.yahoo.com>

Hello,
I'm following previous threads on the use of varimp in the R "party" package. I'm running up against similar problems to that described here (and the thread referenced therein):
https://stat.ethz.ch/pipermail/r-help/2011-October/292897.html

Specifically, I'm running cforest and then varimp on a dataset with one binary (yes/no) variable to classified and 32 predictors (This is in the context of using environmental data to predict site occupancy and all my predictors are numeric or integer). As described in previous posts, "varimp" command is giving me the following error:
"Error in model.matrix.default(as.formula(f), data = blocks) : term 1 would require 1e+25 columns"
(I've tried changing the threshold parameter with no luck).?

It seems like one of the package author's responses (and possible solutions) to a previous inquiry may have been made outside of the listserv and I'm wondering if it could be posted for reference? Is there something wrong with applying this method given the nature of my data (e.g. many, strongly correlated predictors)?
Thanks in advance!



	[[alternative HTML version deleted]]


From cryan at binghamton.edu  Fri Jul 10 19:30:20 2015
From: cryan at binghamton.edu (Christopher W Ryan)
Date: Fri, 10 Jul 2015 13:30:20 -0400
Subject: [R] detecting any element in a vector of strings,
 appearing anywhere in any of several character variables in a
 dataframe
In-Reply-To: <003b01d0ba7c$cf953e80$6ebfbb80$@mcmaster.ca>
References: <559DDB29.3040803@binghamton.edu>
	<web-565125331@cgpsrv2.cis.mcmaster.ca>
	<CAGxFJbRs7wROn0+OssgfBH4NKD8kSnsgXXTVXvArv07eFV8DTw@mail.gmail.com>
	<60536AF0-73B9-4133-8B53-B1EAA20E8346@dcn.davis.CA.us>
	<CAGxFJbQLF-XVofsCEXE=RDY=iirbhqGSh2dFN92z4D-9au-kJg@mail.gmail.com>
	<7474A305-56D6-4B53-A0E4-6C4C8ABE2F52@dcn.davis.CA.us>
	<CAGxFJbTp6UDtOgjRW7Hr1c+zs0b6gn00QPtzGiaYWAHb0noLLg@mail.gmail.com>
	<CAM+rpY=ucmtVQREyhcgoXzc7v5X+kWaej4DiOTbeZNajnB-CSg@mail.gmail.com>
	<003b01d0ba7c$cf953e80$6ebfbb80$@mcmaster.ca>
Message-ID: <CAM+rpY=tHRRBAG+tv56q8qhB20A_1rcOGr3dXhA=DFDBJug_-Q@mail.gmail.com>

Interesting thoughts about the partial-word matches, and speed  On
another real data set, about 73,000 records and 6 columns to search
through for matches (one column of which contains very long character
strings--several paragraphs each), I ran both John's and Bert's
solutions.  John's was noticeably slower, although still quite
tolerable.  There were a different number of matches, though:

      oic.2
oic          FALSE    TRUE     Sum
  FALSE 74939         0        74939
  TRUE    274           927     1201
  Sum     75213        927     76140

where oic is the logical vector generated by John's solution, and
oic.2 is the logical vector generated by Bert's solution. Bert's
solution detected about 77% of the cases detected by John's.

I'm still exploring why that might be. One possible explanation, for
at least part of the difference, is the issue of partial-word matches.
Substantively, I am searching ambulance run records for words related
to opioid overdose, and I've noticed that the medics often spell
heroin as "heroine"  So in this context, I like partial-word
matches--I want to pick up records that (partially) match "heroin"
because it is contained in the word "heroine" .

There may be other things going on too.

Thanks.

--Chris

On Thu, Jul 9, 2015 at 3:24 PM, John Fox <jfox at mcmaster.ca> wrote:
> Dear Christopher,
>
> My usual orientation to this kind of one-off problem is that I'm looking for a simple correct solution. Computing time is usually much smaller than programming time.
>
> That said, Bert Gunter's solution was about 5 times faster in a simple check that I ran with microbenchmark, and Jeff Newmiller's solution was about 10 times faster. Both Bert's and Jeff's (eventual) solution protect against partial (rather than full-word) matches, while mine doesn't (though it could easily be modified to do that).
>
> Best,
>  John
>
>> -----Original Message-----
>> From: Christopher W Ryan [mailto:cryan at binghamton.edu]
>> Sent: July-09-15 2:49 PM
>> To: Bert Gunter
>> Cc: Jeff Newmiller; R Help; John Fox
>> Subject: Re: [R] detecting any element in a vector of strings, appearing
>> anywhere in any of several character variables in a dataframe
>>
>> Thanks everyone.  John's original solution worked great.  And with
>> 27,000 records, 65 alarm.words, and 6 columns to search, it takes only
>> about 15 seconds.  That is certainly adequate for my needs.  But I
>> will try out the other strategies too.
>>
>> And thanks also for lot's of new R things to learn--grep, grepl,
>> do.call . . . that's always a bonus!
>>
>> --Chris Ryan
>>
>> On Thu, Jul 9, 2015 at 1:52 PM, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>> > Yup, that does it. Let grep figure out what's a word rather than doing
>> > it manually. Forgot about "\b"
>> >
>> > Cheers,
>> > Bert
>> >
>> >
>> > Bert Gunter
>> >
>> > "Data is not information. Information is not knowledge. And knowledge
>> > is certainly not wisdom."
>> >    -- Clifford Stoll
>> >
>> >
>> > On Thu, Jul 9, 2015 at 10:30 AM, Jeff Newmiller
>> > <jdnewmil at dcn.davis.ca.us> wrote:
>> >> Just add a word break marker before and after:
>> >>
>> >> zz$v5 <- grepl( paste0( "\\b(", paste0( alarm.words, collapse="|" ),
>> ")\\b" ), do.call( paste, zz[ , 2:3 ] ) ) )
>> >> ---------------------------------------------------------------------
>> ------
>> >> Jeff Newmiller                        The     .....       .....  Go
>> Live...
>> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>> >>                                       Live:   OO#.. Dead: OO#..
>> Playing
>> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> >> /Software/Embedded Controllers)               .OO#.       .OO#.
>> rocks...1k
>> >> ---------------------------------------------------------------------
>> ------
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>> >> On July 9, 2015 10:12:23 AM PDT, Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>> >>>Jeff:
>> >>>
>> >>>Well, it would be much better (no loops!) except, I think, for one
>> >>>issue: "red" would match "barred" and I don't think that this is what
>> >>>is wanted: the matches should be on whole "words" not just string
>> >>>patterns.
>> >>>
>> >>>So you would need to fix up the matching pattern to make this work,
>> >>>but it may be a little tricky, as arbitrary whitespace characters,
>> >>>e.g. " " or "\n" etc. could be in the strings to be matched
>> separating
>> >>>the words or ending the "sentence."  I'm sure it can be done, but
>> I'll
>> >>>leave it to you or others to figure it out.
>> >>>
>> >>>Of course, if my diagnosis is wrong or silly, please point this out.
>> >>>
>> >>>Cheers,
>> >>>Bert
>> >>>
>> >>>
>> >>>Bert Gunter
>> >>>
>> >>>"Data is not information. Information is not knowledge. And knowledge
>> >>>is certainly not wisdom."
>> >>>   -- Clifford Stoll
>> >>>
>> >>>
>> >>>On Thu, Jul 9, 2015 at 9:34 AM, Jeff Newmiller
>> >>><jdnewmil at dcn.davis.ca.us> wrote:
>> >>>> I think grep is better suited to this:
>> >>>>
>> >>>> zz$v5 <- grepl( paste0( alarm.words, collapse="|" ), do.call(
>> paste,
>> >>>zz[ , 2:3 ] ) ) )
>> >>>>
>> >>>---------------------------------------------------------------------
>> ------
>> >>>> Jeff Newmiller                        The     .....       .....  Go
>> >>>Live...
>> >>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
>> Live
>> >>>Go...
>> >>>>                                       Live:   OO#.. Dead: OO#..
>> >>>Playing
>> >>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.
>> with
>> >>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>> >>>rocks...1k
>> >>>>
>> >>>---------------------------------------------------------------------
>> ------
>> >>>> Sent from my phone. Please excuse my brevity.
>> >>>>
>> >>>> On July 9, 2015 8:51:10 AM PDT, Bert Gunter
>> <bgunter.4567 at gmail.com>
>> >>>wrote:
>> >>>>>Here's a way to do it that uses %in% (i.e. match() ) and uses only
>> a
>> >>>>>single, not a double, loop. It should be more efficient.
>> >>>>>
>> >>>>>> sapply(strsplit(do.call(paste,zz[,2:3]),"[[:space:]]+"),
>> >>>>>+       function(x)any(x %in% alarm.words))
>> >>>>>
>> >>>>> [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE  TRUE
>> >>>>>
>> >>>>>The idea is to paste the strings in each row (do.call allows an
>> >>>>>arbitrary number of columns) into a single string and then use
>> >>>>>strsplit to break the string into individual "words" on whitespace.
>> >>>>>Then the matching is vectorized with the any( %in% ... ) call.
>> >>>>>
>> >>>>>Cheers,
>> >>>>>Bert
>> >>>>>Bert Gunter
>> >>>>>
>> >>>>>"Data is not information. Information is not knowledge. And
>> knowledge
>> >>>>>is certainly not wisdom."
>> >>>>>   -- Clifford Stoll
>> >>>>>
>> >>>>>
>> >>>>>On Thu, Jul 9, 2015 at 6:05 AM, John Fox <jfox at mcmaster.ca> wrote:
>> >>>>>> Dear Chris,
>> >>>>>>
>> >>>>>> If I understand correctly what you want, how about the following?
>> >>>>>>
>> >>>>>>> rows <- apply(zz[, 2:3], 1, function(x) any(sapply(alarm.words,
>> >>>>>grepl, x=x)))
>> >>>>>>> zz[rows, ]
>> >>>>>>
>> >>>>>>           v1                              v2                v3 v4
>> >>>>>> 3  -1.022329                    green turtle    ronald weasley  2
>> >>>>>> 6   0.336599              waffle the hamster        red sparks  1
>> >>>>>> 9  -1.631874 yellow giraffe with a long neck gandalf the white  1
>> >>>>>> 10  1.130622                      black bear  gandalf the grey  2
>> >>>>>>
>> >>>>>> I hope this helps,
>> >>>>>>  John
>> >>>>>>
>> >>>>>> ------------------------------------------------
>> >>>>>> John Fox, Professor
>> >>>>>> McMaster University
>> >>>>>> Hamilton, Ontario, Canada
>> >>>>>> http://socserv.mcmaster.ca/jfox/
>> >>>>>>
>> >>>>>>
>> >>>>>> On Wed, 08 Jul 2015 22:23:37 -0400
>> >>>>>>  "Christopher W. Ryan" <cryan at binghamton.edu> wrote:
>> >>>>>>> Running R 3.1.1 on windows 7
>> >>>>>>>
>> >>>>>>> I want to identify as a case any record in a dataframe that
>> >>>contains
>> >>>>>any
>> >>>>>>> of several keywords in any of several variables.
>> >>>>>>>
>> >>>>>>> Example:
>> >>>>>>>
>> >>>>>>> # create a dataframe with 4 variables and 10 records
>> >>>>>>> v2 <- c("white bird", "blue bird", "green turtle", "quick brown
>> >>>>>fox",
>> >>>>>>> "big black dog", "waffle the hamster", "benny likes food a lot",
>> >>>>>"hello
>> >>>>>>> world", "yellow giraffe with a long neck", "black bear")
>> >>>>>>> v3 <- c("harry potter", "hermione grainger", "ronald weasley",
>> >>>>>"ginny
>> >>>>>>> weasley", "dudley dursley", "red sparks", "blue sparks", "white
>> >>>>>dress
>> >>>>>>> robes", "gandalf the white", "gandalf the grey")
>> >>>>>>> zz <- data.frame(v1=rnorm(10), v2=v2, v3=v3, v4=rpois(10,
>> >>>lambda=2),
>> >>>>>>> stringsAsFactors=FALSE)
>> >>>>>>> str(zz)
>> >>>>>>> zz
>> >>>>>>>
>> >>>>>>> # here are the keywords
>> >>>>>>> alarm.words <- c("red", "green", "turtle", "gandalf")
>> >>>>>>>
>> >>>>>>> # For each row/record, I want to test whether the string in v2
>> or
>> >>>>>the
>> >>>>>>> string in v3 contains any of the strings in alarm.words. And
>> then
>> >>>if
>> >>>>>so,
>> >>>>>>> set zz$v5=TRUE for that record.
>> >>>>>>>
>> >>>>>>> # I'm thinking the str_detect function in the stringr package
>> >>>ought
>> >>>>>to
>> >>>>>>> be able to help, perhaps with some use of apply over the rows,
>> but
>> >>>I
>> >>>>>>> obviously misunderstand something about how str_detect works
>> >>>>>>>
>> >>>>>>> library(stringr)
>> >>>>>>>
>> >>>>>>> str_detect(zz[,2:3], alarm.words)    # error: the target of the
>> >>>>>search
>> >>>>>>>                                      # must be a vector, not
>> >>>>>multiple
>> >>>>>>>                                      # columns
>> >>>>>>>
>> >>>>>>> str_detect(zz[1:4,2:3], alarm.words) # same error
>> >>>>>>>
>> >>>>>>> str_detect(zz[,2], alarm.words)      # error, length of
>> >>>alarm.words
>> >>>>>>>                                      # is less than the number
>> of
>> >>>>>>>                                      # rows I am using for the
>> >>>>>>>                                      # comparison
>> >>>>>>>
>> >>>>>>> str_detect(zz[1:4,2], alarm.words)   # works as hoped when
>> >>>>>>> length(alarm.words)                  # confining nrows
>> >>>>>>>                                      # to the length of
>> >>>alarm.words
>> >>>>>>>
>> >>>>>>> str_detect(zz, alarm.words)          # obviously not right
>> >>>>>>>
>> >>>>>>> # maybe I need apply() ?
>> >>>>>>> my.f <- function(x){str_detect(x, alarm.words)}
>> >>>>>>>
>> >>>>>>> apply(zz[,2], 1, my.f)     # again, a mismatch in lengths
>> >>>>>>>                            # between alarm.words and that
>> >>>>>>>                            # in which I am searching for
>> >>>>>>>                            # matching strings
>> >>>>>>>
>> >>>>>>> apply(zz, 2, my.f)         # now I'm getting somewhere
>> >>>>>>> apply(zz[1:4,], 2, my.f)   # but still only works with 4
>> >>>>>>>                            # rows of the dataframe
>> >>>>>>>
>> >>>>>>>
>> >>>>>>> # perhaps %in% could do the job?
>> >>>>>>>
>> >>>>>>> Appreciate any advice.
>> >>>>>>>
>> >>>>>>> --Chris Ryan
>> >>>>>>>
>> >>>>>>> ______________________________________________
>> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>>> PLEASE do read the posting guide
>> >>>>>http://www.R-project.org/posting-guide.html
>> >>>>>>> and provide commented, minimal, self-contained, reproducible
>> code.
>> >>>>>>
>> >>>>>> ______________________________________________
>> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>> PLEASE do read the posting guide
>> >>>>>http://www.R-project.org/posting-guide.html
>> >>>>>> and provide commented, minimal, self-contained, reproducible
>> code.
>> >>>>>
>> >>>>>______________________________________________
>> >>>>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>> >>>>>PLEASE do read the posting guide
>> >>>>>http://www.R-project.org/posting-guide.html
>> >>>>>and provide commented, minimal, self-contained, reproducible code.
>> >>>>
>> >>
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>
> ---
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/antivirus
>


From elham_h763 at yahoo.com  Mon Jul 13 09:04:02 2015
From: elham_h763 at yahoo.com (Patty Haaem)
Date: Mon, 13 Jul 2015 07:04:02 +0000 (UTC)
Subject: [R] an error in nlme package
Message-ID: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>

Dear All,I am trying to fit?one compartment IV bolus model on pharmacokinetic data using phenoModel function in nlme?package, based on a tutorial entitled ?"Development of population PK model using R- Case study I". The codes are as fallowing:
library(nlme)mydata.grp <- groupedData(CONC~TIME|CID,data=mydata)mydata.fit < -nlme(CONC~phenoModel(CID,TIME,AMT,lCl,lV),fixed=lCl+lV~1,random=pdDiag(lCl+lV~1), data=mydata.grp,start=c(-5,0),weight=varConstPower(const=1,fixed=list(power=1)),na.action=na.include,naPattern=~!is.na(CONC))
when I run above codes, I get the following error:Error in nlme.formula(CONC ~ phenoModel(CID, TIME, AMT, lCl, lV), fixed = lCl + ?:?? object 'na.include' not found
could you please tell me, how should I correct the codes?Thanks in advanceElham Haem



	[[alternative HTML version deleted]]


From fabien.tarrade at gmail.com  Mon Jul 13 12:28:06 2015
From: fabien.tarrade at gmail.com (Fabien Tarrade)
Date: Mon, 13 Jul 2015 12:28:06 +0200
Subject: [R] R 3.1.2 : arima.sim(model=list(ma=0.5), n=250,
 innov=rnorm(250, mean=0, sd=0.1)) versus arima.sim(model=list(ma=0.5), n=250,
 mean=0, sd=0.1) => only the first element is not identical !
In-Reply-To: <CAHz+bWZ7Mty7_YiCXqGTfARVuyjo4_PfyqQXLN4cYdvBHSoCxg@mail.gmail.com>
References: <55A0C9C8.4030605@gmail.com>
	<CAHz+bWZ7Mty7_YiCXqGTfARVuyjo4_PfyqQXLN4cYdvBHSoCxg@mail.gmail.com>
Message-ID: <55A392B6.20705@gmail.com>

Hi Mark,

Thanks for your message and sorry for the delay but it took me some time 
to understand your message and to try few things.
> I think one would say that that is not a bug. I looked at the details 
> of arima.sim ( using debug(arima.sim) )
> and there are two different series that are created inside the 
> function. one is called innov and the other is start.innov. start.innov is
> used to create a burn in period for the constructed series.
I am not sure I fully understand how the 2 time series are created :one 
is called innov and the other is start.innov but what I am not sure to 
understand is the following :

nobs = 5000000

set.seed(183);
test1 <- arima.sim(model=list(ma=0.5), n = 
nobs,innov=rnorm(nobs,mean=0,sd=0.1))

set.seed(183);
test2 <- arima.sim(model=list(ma=0.5), n=nobs, mean=0, sd=0.1)

sum(abs(test1-test2)>0)
test1[1]
test2[1]
test1[2]
test2[2]

Browse[4]> sum(abs(test1-test2)>0)
[1] 1
Browse[4]> test1[1]
[1] 0.09417
Browse[4]> test2[1]
[1] -0.02648
Browse[4]> test1[2]
[1] -0.1053
Browse[4]> test2[2]
[1] -0.1053

which indicate that on 5 millions of generated event with the 2 methods 
only the first element is different ! (I try to change the seed and I 
saw the same). I could understand that most of the element are different 
but the fact that only the first one is different is something that I 
don't understand, sorry.


I made a copy of the function to play a bit and print some value :

=> first line :
    test1 <- test(model=list(ma=0.5), n = 
nobs,innov=rnorm(nobs,mean=0,sd=0.1))

=> second line :
    test2 <- test(model=list(ma=0.5), n=nobs, mean=0, sd=0.1)


after :
         x <- ts(c(start.innov[seq_len(n.start)], innov[1L:n]), start = 1 -
                         n.start)
I see :
[1]  0.2681255 -0.0398882 -0.0853729 -0.0707863 -0.0262917 -0.0001408
[1]  0.0268126 -0.0398882 -0.0853729 -0.0707863 -0.0262917 -0.0001408

after :
         if (length(model$ma)) {
                 x <- filter(x, c(1, model$ma), sides = 1L)
                 x[seq_along(model$ma)] <- 0
         }
I see :
[1]  0.00000  0.09417 -0.10532 -0.11347 -0.06168 -0.01329
[1]  0.00000 -0.02648 -0.10532 -0.11347 -0.06168 -0.01329

after :
         if (length(model$ar))
                 x <- filter(x, model$ar, method = "recursive")
I see :
[1]  0.00000  0.09417 -0.10532 -0.11347 -0.06168 -0.01329
[1]  0.00000 -0.02648 -0.10532 -0.11347 -0.06168 -0.01329

after :
         if (n.start > 0)
                 x <- x[-(seq_len(n.start))]
I see :
[1]  0.09417 -0.10532 -0.11347 -0.06168 -0.01329
[1] -0.02648 -0.10532 -0.11347 -0.06168 -0.01329

after :
         if (d > 0)
                 x <- diffinv(x, differences = d)
I see :
[1]  0.09417 -0.10532 -0.11347 -0.06168 -0.01329
[1] -0.02648 -0.10532 -0.11347 -0.06168 -0.01329

so the difference appear since the beginning with the with element and 
which seems to be divided by 10!

after :
         x <- ts(c(start.innov[seq_len(n.start)], innov[1L:n]), start = 1 -
                         n.start)
I see :
[1]  0.2681255 ...
[1]  0.0268126 ...

> in your test1 call, you are not  supplying arguments for what should 
> be used for the innovations associated with start.innov which is used 
> for the burn in period. So, arima.sim  uses the defaults of mean = 0 
> and sd = 1.0.
 > set.seed(123);
 > test1 <- arima.sim(model=list(ma=0.5), n = 
250,innov=rnorm(250,mean=0,sd=0.1))
 > print(head(test1))
[1] -0.30326  0.14436  0.08499  0.01645  0.17797  0.13184

 > set.seed(123);
 > test3 <- arima.sim(model=list(ma=0.5), innov = rnorm(250,mean=0, 
sd=1.0), n = 250, mean=0, sd=0.1)
 > print(head(test3))
[1] -0.2582  1.4436  0.8499  0.1645  1.7797  1.3184

this doesn't seems to be true or I misinterpret you explanation
> For your test2 call, you do provide them ( so they are used for both 
> innov and start.innov )  and you use  sd = 0.1 So, for test2,  the 
> values  for the burn in period end up being different from the ones in 
> test1.
I am not sure I understand the meaning of "burn in period". Do we expect 
that to change only the first element of 2 time series of 5M of events >

> Below, I made a test3 that can be used to get the same values as 
> test2. In short, by specifiying the innov call EXACTLY in test1, 
> you're letting
> arima.sim use the default arguments for the start.innov call so that's 
> why they're different.
I did the test and I agree

Thanks a lot
Cheers
Fabien
>
>
> #====================================================================================================================
>  ##undebug(arima.sim)
>
> set.seed(123);
> test1 <- arima.sim(model=list(ma=0.5), n = 
> 250,innov=rnorm(250,mean=0,sd=0.1))
> print(head(test1))
>
> set.seed(123);
> test2 <- arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1)
> print(head(test2))
>
> set.seed(123);
> test3 <- arima.sim(model=list(ma=0.5), innov = rnorm(250,mean=0, 
> sd=0.1), n = 250, mean=0, sd=0.1)
> print(head(test3))
>
> On Sat, Jul 11, 2015 at 3:46 AM, Fabien Tarrade 
> <fabien.tarrade at gmail.com <mailto:fabien.tarrade at gmail.com>> wrote:
>
>     Dear all,
>
>     When doing a DataCamp tutorial with R I find the following
>     observation that using 2 different syntax for "arima.sim" give
>     different answer for the first element
>
>     If I use the the function using the list of argument describe in
>     the help manual :
>     arima.sim(model=list(ma=0.5),n=250,innov=rnorm(250,mean=0,sd=0.1))
>
>     or if I use the following syntax use in a DataCamp example :
>
>     arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1) it is
>     accepted by DataCamp
>
>     I don't find exactly the same results. The reason is that even if
>     the seed is the same in both cases the first element is not
>     identical while it should be (it doesn't mean that the results is
>     wrong, maybe for the first element the seed is not propagated
>     correctly)
>
>     here the results of the difference using the same seed (only the
>     first element is different using the 2 different syntaxes) :
>
>       [1] -0.252214  0.000000  0.000000  0.000000  0.000000 0.000000
>     0.000000  0.000000  0.000000  0.000000
>      [11]  0.000000  0.000000  0.000000  0.000000  0.000000 0.000000 
>     0.000000  0.000000  0.000000  0.000000
>
>
>     here the code to reproduce this feature :
>
>     set.seed(123);
>     test1 <- 0.05 +
>     arima.sim(model=list(ma=0.5),n=250,innov=rnorm(250,mean=0,sd=0.1))
>     set.seed(123);
>     test2 <- 0.05 + arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1)
>
>     test1-test2
>
>     I am using R 3.1.2 GUI 1.65 Mavericks build (6833) on Mac (I guess
>     arima come with stats which is included in R (?))
>     The DataCamp team ask me to report to you about this observation
>     on this mailing list. If you want me to fill a bug report some R
>     bug tracking system, let me know
>
>     Please tell me if this is the wrong list and which other
>     information do you need from R and how to get then (compiler,
>     version of some R packages ...)
>
>
>     Hope this help
>     Thanks
>     Cheers
>     Fabien
>     -- 
>     Dr Fabien Tarrade
>
>     Quantitative Analyst - Data Scientist - Researcher
>
>     Senior data analyst specialised in the modelling, processing and
>     statistical treatment of data.
>     PhD in Physics, 10 years of experience as researcher at the
>     forefront of international scientific research.
>     Fascinated by finance and data modelling.
>
>     Geneva, Switzerland
>
>     Email : contact at fabien-tarrade.eu
>     <mailto:contact at fabien-tarrade.eu>
>     <mailto:contact at fabien-tarrade.eu <mailto:contact at fabien-tarrade.eu>>
>     Phone : www.fabien-tarrade.eu <http://www.fabien-tarrade.eu>
>     <http://www.fabien-tarrade.eu>
>     Phone : +33 (0)6 14 78 70 90
>     <tel:%2B33%20%280%296%2014%2078%2070%2090>
>
>     LinkedIn <http://ch.linkedin.com/in/fabientarrade/> Twitter
>     <https://twitter.com/fabtar> Google
>     <https://plus.google.com/+FabienTarradeProfile/posts> Facebook
>     <https://www.facebook.com/fabien.tarrade.eu> Google
>     <skype:fabtarhiggs?call> Xing
>     <https://www.xing.com/profile/Fabien_Tarrade>
>     ______________________________________________
>     R-help at r-project.org <mailto:R-help at r-project.org> mailing list --
>     To UNSUBSCRIBE and more, see
>     https://stat.ethz.ch/mailman/listinfo/r-help
>     PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>     and provide commented, minimal, self-contained, reproducible code.
>
>

-- 
Dr Fabien Tarrade

Quantitative Analyst - Data Scientist - Researcher

Senior data analyst specialised in the modelling, processing and 
statistical treatment of data.
PhD in Physics, 10 years of experience as researcher at the forefront of 
international scientific research.
Fascinated by finance and data modelling.

Geneva, Switzerland

Email : contact at fabien-tarrade.eu <mailto:contact at fabien-tarrade.eu>
Phone : www.fabien-tarrade.eu <http://www.fabien-tarrade.eu>
Phone : +33 (0)6 14 78 70 90

LinkedIn <http://ch.linkedin.com/in/fabientarrade/> Twitter 
<https://twitter.com/fabtar> Google 
<https://plus.google.com/+FabienTarradeProfile/posts> Facebook 
<https://www.facebook.com/fabien.tarrade.eu> Google 
<skype:fabtarhiggs?call> Xing <https://www.xing.com/profile/Fabien_Tarrade>

From fabien.tarrade at gmail.com  Mon Jul 13 12:31:58 2015
From: fabien.tarrade at gmail.com (Fabien Tarrade)
Date: Mon, 13 Jul 2015 12:31:58 +0200
Subject: [R] R 3.1.2 : arima.sim(model=list(ma=0.5), n=250,
 innov=rnorm(250, mean=0, sd=0.1)) versus arima.sim(model=list(ma=0.5), n=250,
 mean=0, sd=0.1) => only the first element is not identical !
In-Reply-To: <55A392B6.20705@gmail.com>
References: <55A0C9C8.4030605@gmail.com>
	<CAHz+bWZ7Mty7_YiCXqGTfARVuyjo4_PfyqQXLN4cYdvBHSoCxg@mail.gmail.com>
	<55A392B6.20705@gmail.com>
Message-ID: <55A3939E.3000205@gmail.com>

just to correct something :

>> in your test1 call, you are not  supplying arguments for what should 
>> be used for the innovations associated with start.innov which is used 
>> for the burn in period. So, arima.sim  uses the defaults of mean = 0 
>> and sd = 1.0.
> > set.seed(123);
> > test1 <- arima.sim(model=list(ma=0.5), n = 
> 250,innov=rnorm(250,mean=0,sd=0.1))
> > print(head(test1))
> [1] -0.30326  0.14436  0.08499  0.01645  0.17797  0.13184
>
> > set.seed(123);
> > test3 <- arima.sim(model=list(ma=0.5), innov = rnorm(250,mean=0, 
> sd=1.0), n = 250, mean=0, sd=0.1)
> > print(head(test3))
> [1] -0.2582  1.4436  0.8499  0.1645  1.7797  1.3184
>
> this doesn't seems to be true or I misinterpret you explanation
as pointed out by Mark in a previous message :
set.seed(123);
test4 <- arima.sim(model=list(ma=0.5), innov = rnorm(250,mean=0, 
sd=0.1), n = 250, mean=0, sd=1.0)
print(head(test4))

do give the same result than test 1

Thanks
Cheers
Fabien
-- 
Dr Fabien Tarrade

Quantitative Analyst - Data Scientist - Researcher

Senior data analyst specialised in the modelling, processing and 
statistical treatment of data.
PhD in Physics, 10 years of experience as researcher at the forefront of 
international scientific research.
Fascinated by finance and data modelling.

Geneva, Switzerland

Email : contact at fabien-tarrade.eu <mailto:contact at fabien-tarrade.eu>
Phone : www.fabien-tarrade.eu <http://www.fabien-tarrade.eu>
Phone : +33 (0)6 14 78 70 90

LinkedIn <http://ch.linkedin.com/in/fabientarrade/> Twitter 
<https://twitter.com/fabtar> Google 
<https://plus.google.com/+FabienTarradeProfile/posts> Facebook 
<https://www.facebook.com/fabien.tarrade.eu> Google 
<skype:fabtarhiggs?call> Xing <https://www.xing.com/profile/Fabien_Tarrade>

From aldi at dsgmail.wustl.edu  Mon Jul 13 13:23:00 2015
From: aldi at dsgmail.wustl.edu (aldi)
Date: Mon, 13 Jul 2015 06:23:00 -0500
Subject: [R] merge: right set overwrite left set
In-Reply-To: <CA+vqiLE3OYAzDa4es6ugua0Ee5YQAE9ZmNb+zak1FGy7QTLmxg@mail.gmail.com>
References: <55A27DA4.4010008@dsgmail.wustl.edu>
	<CA+vqiLE3OYAzDa4es6ugua0Ee5YQAE9ZmNb+zak1FGy7QTLmxg@mail.gmail.com>
Message-ID: <55A39F94.6010309@dsgmail.wustl.edu>

Thank you Ista,
Your solution is smart, by sub-setting from x.HHu.map data only "HHid", 
"position" as indices (because they are unique) for the merge, and any 
extra columns in x.HHu.map that are not present in y.HHo,map, thus when 
the merge is done with option all=T, will work among the two sets of 
data, without creating .x and .y when the variables are in common in two 
sets.

With best wishes,
Aldi

 > ## find indv columns in x.HHu.map that don't exist in y.HHo.map
 > x.HHu.map <- x.HHu.map[
+     c("HHid",
+       "position",
+       names(x.HHu.map)[
+                !names(x.HHu.map)
+                %in% names(y.HHo.map)]
+       )]
 > ## merge, adding extra column from x.HHu.map
 > zzz <- merge(y.HHo.map, x.HHu.map, by=c('HHid', 'position'), all=T)
 > ## order by HHid
 > zzz <- zzz[order(zzz$HHid),]
 > zzz
    HHid position indv1 indv2 ind3
1   HH1       10     2     0    0
2  HH10      101    NA     2    0
3  HH11      111    NA    NA   NA
4   HH2       20     0     2    0
5   HH3       30     0     1    0
6   HH4       42    NA    NA    0
7   HH5       55     2     2    0
8   HH6       66    NA    NA   NA
9   HH7       75    NA    NA   NA
10  HH8       81    NA    NA   NA
11  HH9       92    NA    NA   NA


On 7/12/2015 12:06 PM, Ista Zahn wrote:
> I think this does what you want:
>
> ## find idiv coloumns in x.HHu.map that don't exist in y.HHo.map
> x.HHu.map <- x.HHu.map[
>      c("HHid",
>        "position",
>        names(x.HHu.map)[
>                 !names(x.HHu.map)
>                 %in% names(y.HHo.map)]
>        )]
> ## merge, adding extra column from x.HHu.map
> zzz <- merge(y.HHo.map, x.HHu.map, by=c('HHid', 'position'), all=T)
> ## order by HHid
> zzz <- zzz[order(zzz$HHid),]
>
> Best,
> Ista
>
> On Sun, Jul 12, 2015 at 10:45 AM, aldi <aldi at dsgmail.wustl.edu> wrote:
>> Hi,
>> I have two sets of data x.HHu and y.HHo, rows are IDs and columns are
>> individuals. I do not know in advance indv or HHid, both of them will be
>> captured from the data. As the y.HHo set updates, y.HHo set has better
>> information then x.HHu set. Thus I want a merge where right set
>> overwrites left set info based on HHid, i.e. to overwrite x.HHu set with
>> y.HHo set but keep any extra info from the x.HHu set that is not present
>> in y.HHo set.
>> HHids will be complete based on z.map, with the corresponding positions.
>> I am having trouble with the part after this line: ###
>> ============================================+++++++++++++++++++++++++++
>> I am thinking that I am creating new columns "position" "indv1" and
>> "indv2", but R is interpreting them as row information.
>> See the expected final table at the end. HHid is common, indv3 is from
>> x.HHu, and the rest position and indv1 and indv2 are from y.HHo
>> Any suggestions are appreciated.
>> Thank you in advance,
>> Aldi
>>
>> x.HHu<- data.frame(
>>              HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10')
>>            , indv1 = c( 2, 0, 2 , 0, 2, 0)
>>            , indv2 = c( 0, NA, 2, 2, 2, 2)
>>            , ind3 = c( 0, 0, 0, 0, 0, 0)
>>            )
>> ### the HHo data will be the top set to overwrite any HHu data, when
>> they exist, thinking that HHo are better than HHu results
>> ### when they are available
>>
>> y.HHo<-data.frame(HHid=c('HH1', 'HH2','HH5', 'HH3', 'HH10')
>>            , indv1 = c(2, 0, 2, 0, NA)
>>            , indv2 = c(0, 2, 2, 1, 2)
>>            )
>>
>> z.map<-data.frame(HHid = c('HH1', 'HH2', 'HH3', 'HH4', 'HH5',
>> 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11')
>>                   , position= c(10,20,30,42,55,66,81,75,92,101,111)
>>                   )
>> ### see objects
>> x.HHu
>> y.HHo
>> z.map
>> ### now sort the map by position, this sorted map will be used to sort
>> finally all data
>> z.map<-z.map[with(z.map, order(position)), ]
>> z.map
>>
>> ### First I introduce position to both sets so I can sort them in
>> advance by position.
>> x.HHu.map <-merge( z.map, x.HHu, by='HHid', all=T)
>> x.HHu.map<-x.HHu.map[with(x.HHu.map, order(position)), ]
>> x.HHu.map
>>
>> y.HHo.map <-merge( z.map, y.HHo, by='HHid', all= T)
>> y.HHo.map<-y.HHo.map[with(y.HHo.map, order(position)), ]
>> y.HHo.map
>>
>> ### now merge HHu  and HHo  with the hope to overwrite the HHu set with
>> HHo wherever they overlap by column names.
>> zzz <- merge(x.HHu.map, y.HHo.map, by='HHid', all=T)
>> zzz
>> ### find common variable names in two sets
>>
>> commonNames <- names(x.impu.map)[which(colnames(x.impu.map) %in%
>> colnames(y.geno.map))]
>>
>> ## remove HHid wich is common for x and y, but work with the rest of columns
>> commonNames<-commonNames[-c(1)]
>>
>> ### ============================================+++++++++++++++++++++++++++
>> for(i in 1:length(commonNames)){
>>
>> print(commonNames[i])
>> zzz$commonNames[i] <- NA
>>
>> print(paste("zzz","$",commonNames[i],".y",sep=""))
>>
>> zzz$commonNames[i] <- zzz[,paste(commonNames[i],".y",sep="")]
>>
>> ### paste(zzz$commonNames[i],".x",sep='') <- NULL;
>> ### paste(zzz$commonNames[i],".y",sep='') <- NULL;
>>
>> }
>> zzz
>>
>> The final expected set has to be: HHid is common, indv3 is from x.HHu,
>> and the rest position and indv1 and indv2 are from y.HHo
>>      HHid     position     ind3  indv1 indv2
>> 1   HH1         10          0     2       0
>> 2  HH10        101          0    NA       2
>> 3  HH11        111         NA    NA      NA
>> 4   HH2         20          0     0       2
>> 5   HH3         30          0     0       1
>> 6   HH4         42          0    NA      NA
>> 7   HH5         55          0     2       2
>> 8   HH6         66         NA    NA      NA
>> 9   HH7         75         NA    NA      NA
>> 10  HH8         81         NA    NA      NA
>> 11  HH9         92         NA    NA      NA
>>
>> --
>>
>>


	[[alternative HTML version deleted]]


From aldi at dsgmail.wustl.edu  Mon Jul 13 14:14:49 2015
From: aldi at dsgmail.wustl.edu (aldi)
Date: Mon, 13 Jul 2015 07:14:49 -0500
Subject: [R] merge: right set overwrite left set
In-Reply-To: <alpine.BSF.2.00.1507121116050.12268@pedal.dcn.davis.ca.us>
References: <55A27DA4.4010008@dsgmail.wustl.edu>
	<alpine.BSF.2.00.1507121116050.12268@pedal.dcn.davis.ca.us>
Message-ID: <55A3ABB9.3070406@dsgmail.wustl.edu>

Thank you Jeff,
Your solutions have two great aspects: a) you provide a different 
approach by using reshape2 syntax / tidyr, and b) the concern that it is 
better to update x.HHu.map with y.HHo.map, without overwriting x.HHu.map 
with NA from y.HHo.map, thus keeping intact the old value(s). That is 
the ideal situation, but I do not know exactly if the old value is 
correct, which if it is not correct then it may create strata in the 
data. Therefore I prefer better to overwrite any old column with the new 
column when it exits, even with NA. In addition, you introduce the 
safety stringAsFactors=F. The map is only for getting "position" based 
on HHid, and at the end I plan sorting final set based on position.

While the reshape2 worked with no problems from the start, the other 
solution with
# tidyr/dplyr solution
library(tidyr)
library(dplyr)
did not work correct in the start (it stopped R working and OS says R 
have to close) in windows 8.1 with R 3.1.2, quite possible because 
called libraries were compiled under R 3.1.3.
When installed the new version of R: 3.2.1, the second solution worked 
also with no problem.
For whoever has written the libraries tidyr and dplyr, they produced 
warnings for function name conflicts with base and stats:
Attaching package: ?dplyr?
The following objects are masked from ?package:stats?:
     filter, lag
The following objects are masked from ?package:base?:
     intersect, setdiff, setequal, union

Great solutions!
Thank you,
Aldi

 > x.HHu <- data.frame(
+     HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10' )
+   , indv1 = c( 2, 0, 2 , 0, 2, 0 )
+   , indv2 = c( 0, NA, 2, 2, 2, 2 )
+   , ind3 = c( 0, 0, 0, 0, 0, 0 )
+   , stringsAsFactors = FALSE # avoid creating HHid as a factor
+ )
 > y.HHo <- data.frame(
+     HHid=c( 'HH1', 'HH2','HH5', 'HH3', 'HH10' )
+   , indv1 = c( 2, 0, 2, 0, NA )
+   , indv2 = c( 0, 2, 2, 1, 2 )
+   , stringsAsFactors = FALSE
+ )
 > z.map <- data.frame(
+     HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5'
+             , 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11' )
+   , position= c( 10, 20, 30, 42, 55, 66, 81, 75, 92, 101, 111 )
+   , stringsAsFactors = FALSE
+ )
 > # reshape2 solution
 > library(reshape2)
 >
 > x.HHu.long <- melt( x.HHu, "HHid", variable.name = "indv" )
 > x.HHu.long$indv <- as.character( x.HHu.long$indv )
 > y.HHo.long <- melt( y.HHo, "HHid", variable.name = "indv" )
 > y.HHo.long$indv <- as.character( y.HHo.long$indv )
 > xy.HH.long <- merge( x.HHu.long
+                    , y.HHo.long
+                    , by = c( "HHid", "indv" )
+                    , all = TRUE )
 > xy.HH.long$value <- with( xy.HH.long
+                         , ifelse( is.na( value.y )
+                                 , value.x
+                                 , value.y ) )
 > xy.HH0 <- dcast( xy.HH.long, HHid ~ indv )
 > xy.HH <- merge( xy.HH0, z.map, all=TRUE )
 > xy.HH <- xy.HH[ order( xy.HH$HHid ), ]
 > # compare xy.HH with zzz ... I think there is an error in zzz for # 
HH10/indv1, because NA should not be considered more informative
 > # than 0...
### solution with reshape2
 > xy.HH
    HHid ind3 indv1 indv2 position
1   HH1    0     2     0       10
2  HH10    0     0     2      101
3  HH11   NA    NA    NA      111
4   HH2    0     0     2       20
5   HH3    0     0     1       30
6   HH4    0     0     2       42
7   HH5    0     2     2       55
8   HH6   NA    NA    NA       66
9   HH7   NA    NA    NA       75
10  HH8   NA    NA    NA       81
11  HH9   NA    NA    NA       92

### Solution with tidyr:
 > xy.HH.d
    HHid ind3 indv1 indv2 position
1   HH1    0     2     0       10
2  HH10    0     0     2      101
3  HH11   NA    NA    NA      111
4   HH2    0     0     2       20
5   HH3    0     0     1       30
6   HH4    0     0     2       42
7   HH5    0     2     2       55
8   HH6   NA    NA    NA       66
9   HH7   NA    NA    NA       75
10  HH8   NA    NA    NA       81
11  HH9   NA    NA    NA       92

On 7/12/2015 1:35 PM, Jeff Newmiller wrote:
> I get confused by your use of the position map table. If I follow your 
> description toward your desired result, I take a different route that 
> makes sense to me. Perhaps it will make sense to you as well. The key 
> idea is to make individual comparisons of the values for each 
> combination of HHid and indv, regardless of where they are in the 
> original data frames.
>
> Below are two different syntactic representations of my understanding 
> of your problem. They differ only in the approach taken to strip away 
> syntax clutter. I start by making the HHid identifiers character 
> values in the original data frames, because their respective factor 
> levels in the two data frames would not necessarily correspond.
>
> x.HHu <- data.frame(
>     HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10' )
>   , indv1 = c( 2, 0, 2 , 0, 2, 0 )
>   , indv2 = c( 0, NA, 2, 2, 2, 2 )
>   , ind3 = c( 0, 0, 0, 0, 0, 0 )
>   , stringsAsFactors = FALSE # avoid creating HHid as a factor
> )
> y.HHo <- data.frame(
>     HHid=c( 'HH1', 'HH2','HH5', 'HH3', 'HH10' )
>   , indv1 = c( 2, 0, 2, 0, NA )
>   , indv2 = c( 0, 2, 2, 1, 2 )
>   , stringsAsFactors = FALSE
> )
> z.map <- data.frame(
>     HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5'
>             , 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11' )
>   , position= c( 10, 20, 30, 42, 55, 66, 81, 75, 92, 101, 111 )
>   , stringsAsFactors = FALSE
> )
>
>
> # reshape2 solution
> library(reshape2)
>
> x.HHu.long <- melt( x.HHu, "HHid", variable.name = "indv" )
> x.HHu.long$indv <- as.character( x.HHu.long$indv )
> y.HHo.long <- melt( y.HHo, "HHid", variable.name = "indv" )
> y.HHo.long$indv <- as.character( y.HHo.long$indv )
> xy.HH.long <- merge( x.HHu.long
>                    , y.HHo.long
>                    , by = c( "HHid", "indv" )
>                    , all = TRUE )
> xy.HH.long$value <- with( xy.HH.long
>                         , ifelse( is.na( value.y )
>                                 , value.x
>                                 , value.y ) )
> xy.HH0 <- dcast( xy.HH.long, HHid ~ indv )
> xy.HH <- merge( xy.HH0, z.map, all=TRUE )
> xy.HH <- xy.HH[ order( xy.HH$HHid ), ]
> # compare xy.HH with zzz ... I think there is an error in zzz for # 
> HH10/indv1, because NA should not be considered more informative
> # than 0...
>
> # tidyr/dplyr solution
> library(tidyr)
> library(dplyr)
>
> # define a common processing sequence
> lengthen <- (   . # period is placeholder for data frame
>             %>% gather( indv, value, -HHid )
>             %>% mutate( indv = as.character( indv ) )
>             )
> x.HHu.dlong <- x.HHu %>% lengthen
> y.HHo.dlong <- y.HHo %>% lengthen
> xy.HH.d <- (   x.HHu.dlong
>            %>% full_join( y.HHo.dlong, by= c( "HHid", "indv" ) )
>            %>% transmute( HHid = HHid
>                         , indv = indv
>                         , value = ifelse( is.na( value.y )
>                                         , value.x
>                                         , value.y )
>                         )
>            %>% spread( indv, value )
>            %>% full_join( z.map, by="HHid" )
>            %>% arrange( HHid )
>            %>% as.data.frame
>            )
>
> On Sun, 12 Jul 2015, aldi wrote:
>
>> Hi,
>> I have two sets of data x.HHu and y.HHo, rows are IDs and columns are
>> individuals. I do not know in advance indv or HHid, both of them will be
>> captured from the data. As the y.HHo set updates, y.HHo set has better
>> information then x.HHu set. Thus I want a merge where right set
>> overwrites left set info based on HHid, i.e. to overwrite x.HHu set with
>> y.HHo set but keep any extra info from the x.HHu set that is not present
>> in y.HHo set.
>> HHids will be complete based on z.map, with the corresponding positions.
>> I am having trouble with the part after this line: ###
>> ============================================+++++++++++++++++++++++++++
>> I am thinking that I am creating new columns "position" "indv1" and
>> "indv2", but R is interpreting them as row information.
>> See the expected final table at the end. HHid is common, indv3 is from
>> x.HHu, and the rest position and indv1 and indv2 are from y.HHo
>> Any suggestions are appreciated.
>> Thank you in advance,
>> Aldi
>>
>> x.HHu<- data.frame(
>>            HHid = c( 'HH1', 'HH2', 'HH3', 'HH4', 'HH5', 'HH10')
>>          , indv1 = c( 2, 0, 2 , 0, 2, 0)
>>          , indv2 = c( 0, NA, 2, 2, 2, 2)
>>          , ind3 = c( 0, 0, 0, 0, 0, 0)
>>          )
>> ### the HHo data will be the top set to overwrite any HHu data, when
>> they exist, thinking that HHo are better than HHu results
>> ### when they are available
>>
>> y.HHo<-data.frame(HHid=c('HH1', 'HH2','HH5', 'HH3', 'HH10')
>>          , indv1 = c(2, 0, 2, 0, NA)
>>          , indv2 = c(0, 2, 2, 1, 2)
>>          )
>>
>> z.map<-data.frame(HHid = c('HH1', 'HH2', 'HH3', 'HH4', 'HH5',
>> 'HH6','HH8', 'HH7', 'HH9', 'HH10', 'HH11')
>>                 , position= c(10,20,30,42,55,66,81,75,92,101,111)
>>                 )
>> ### see objects
>> x.HHu
>> y.HHo
>> z.map
>> ### now sort the map by position, this sorted map will be used to sort
>> finally all data
>> z.map<-z.map[with(z.map, order(position)), ]
>> z.map
>>
>> ### First I introduce position to both sets so I can sort them in
>> advance by position.
>> x.HHu.map <-merge( z.map, x.HHu, by='HHid', all=T)
>> x.HHu.map<-x.HHu.map[with(x.HHu.map, order(position)), ]
>> x.HHu.map
>>
>> y.HHo.map <-merge( z.map, y.HHo, by='HHid', all= T)
>> y.HHo.map<-y.HHo.map[with(y.HHo.map, order(position)), ]
>> y.HHo.map
>>
>> ### now merge HHu  and HHo  with the hope to overwrite the HHu set with
>> HHo wherever they overlap by column names.
>> zzz <- merge(x.HHu.map, y.HHo.map, by='HHid', all=T)
>> zzz
>> ### find common variable names in two sets
>>
>> commonNames <- names(x.impu.map)[which(colnames(x.impu.map) %in%
>> colnames(y.geno.map))]
>>
>> ## remove HHid wich is common for x and y, but work with the rest of 
>> columns
>> commonNames<-commonNames[-c(1)]
>>
>> ### 
>> ============================================+++++++++++++++++++++++++++
>> for(i in 1:length(commonNames)){
>>
>> print(commonNames[i])
>> zzz$commonNames[i] <- NA
>>
>> print(paste("zzz","$",commonNames[i],".y",sep=""))
>>
>> zzz$commonNames[i] <- zzz[,paste(commonNames[i],".y",sep="")]
>>
>> ### paste(zzz$commonNames[i],".x",sep='') <- NULL;
>> ### paste(zzz$commonNames[i],".y",sep='') <- NULL;
>>
>> }
>> zzz
>>
>> The final expected set has to be: HHid is common, indv3 is from x.HHu,
>> and the rest position and indv1 and indv2 are from y.HHo
>>    HHid     position     ind3  indv1 indv2
>> 1   HH1         10          0     2       0
>> 2  HH10        101          0    NA       2
>> 3  HH11        111         NA    NA      NA
>> 4   HH2         20          0     0       2
>> 5   HH3         30          0     0       1
>> 6   HH4         42          0    NA      NA
>> 7   HH5         55          0     2       2
>> 8   HH6         66         NA    NA      NA
>> 9   HH7         75         NA    NA      NA
>> 10  HH8         81         NA    NA      NA
>> 11  HH9         92         NA    NA      NA
>>
>> -- 
>>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Mon Jul 13 16:35:22 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 13 Jul 2015 10:35:22 -0400
Subject: [R] an error in nlme package
In-Reply-To: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
References: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAM_vju=-2qHMTPMduLQ=EFWwWyK15a+r2gHHOFDE0EpdPt1doQ@mail.gmail.com>

On Mon, Jul 13, 2015 at 3:04 AM, Patty Haaem via R-help
<r-help at r-project.org> wrote:
> Dear All,I am trying to fit one compartment IV bolus model on pharmacokinetic data using phenoModel function in nlme package, based on a tutorial entitled  "Development of population PK model using R- Case study I". The codes are as fallowing:
> library(nlme)mydata.grp <- groupedData(CONC~TIME|CID,data=mydata)mydata.fit < -nlme(CONC~phenoModel(CID,TIME,AMT,lCl,lV),fixed=lCl+lV~1,random=pdDiag(lCl+lV~1), data=mydata.grp,start=c(-5,0),weight=varConstPower(const=1,fixed=list(power=1)),na.action=na.include,naPattern=~!is.na(CONC))
> when I run above codes, I get the following error:Error in nlme.formula(CONC ~ phenoModel(CID, TIME, AMT, lCl, lV), fixed = lCl +  :   object 'na.include' not found
> could you please tell me, how should I correct the codes?Thanks in advanceElham Haem


First, you should not post in HTML.

Second, you should put the argument in quotes:

na.action="na.include"


Sarah


>
>
>         [[alternative HTML version deleted]]
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From bgunter.4567 at gmail.com  Mon Jul 13 17:19:49 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 13 Jul 2015 08:19:49 -0700
Subject: [R] an error in nlme package
In-Reply-To: <CAM_vju=-2qHMTPMduLQ=EFWwWyK15a+r2gHHOFDE0EpdPt1doQ@mail.gmail.com>
References: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
	<CAM_vju=-2qHMTPMduLQ=EFWwWyK15a+r2gHHOFDE0EpdPt1doQ@mail.gmail.com>
Message-ID: <CAGxFJbTHE4u+HybL=0cQUu2DK5q_T+qhv6kQbVny+prGzMUncg@mail.gmail.com>

No, Sarah.

na.action must be a function, not a character string. But you're
close: there is no na.include function, as the message says.  It
should be na.exclude .

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 13, 2015 at 7:35 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> On Mon, Jul 13, 2015 at 3:04 AM, Patty Haaem via R-help
> <r-help at r-project.org> wrote:
>> Dear All,I am trying to fit one compartment IV bolus model on pharmacokinetic data using phenoModel function in nlme package, based on a tutorial entitled  "Development of population PK model using R- Case study I". The codes are as fallowing:
>> library(nlme)mydata.grp <- groupedData(CONC~TIME|CID,data=mydata)mydata.fit < -nlme(CONC~phenoModel(CID,TIME,AMT,lCl,lV),fixed=lCl+lV~1,random=pdDiag(lCl+lV~1), data=mydata.grp,start=c(-5,0),weight=varConstPower(const=1,fixed=list(power=1)),na.action=na.include,naPattern=~!is.na(CONC))
>> when I run above codes, I get the following error:Error in nlme.formula(CONC ~ phenoModel(CID, TIME, AMT, lCl, lV), fixed = lCl +  :   object 'na.include' not found
>> could you please tell me, how should I correct the codes?Thanks in advanceElham Haem
>
>
> First, you should not post in HTML.
>
> Second, you should put the argument in quotes:
>
> na.action="na.include"
>
>
> Sarah
>
>
>>
>>
>>         [[alternative HTML version deleted]]
>>
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Mon Jul 13 17:28:14 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 13 Jul 2015 08:28:14 -0700
Subject: [R] an error in nlme package
In-Reply-To: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
References: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <6A547C08-8F92-46F2-B217-846B0473230B@comcast.net>


On Jul 13, 2015, at 12:04 AM, Patty Haaem via R-help wrote:

> Dear All,I am trying to fit one compartment IV bolus model on pharmacokinetic data using phenoModel function in nlme package, based on a tutorial entitled  "Development of population PK model using R- Case study I". The codes are as fallowing:
> library(nlme)mydata.grp <- groupedData(CONC~TIME|CID,data=mydata)mydata.fit < -nlme(CONC~phenoModel(CID,TIME,AMT,lCl,lV),fixed=lCl+lV~1,random=pdDiag(lCl+lV~1), data=mydata.grp,start=c(-5,0),weight=varConstPower(const=1,fixed=list(power=1)),na.action=na.include,naPattern=~!is.na(CONC))
> when I run above codes, I get the following error:Error in nlme.formula(CONC ~ phenoModel(CID, TIME, AMT, lCl, lV), fixed = lCl +  :   object 'na.include' not found
> could you please tell me, how should I correct the codes?Thanks in advanceElham Haem

I would ask the question: where id you get the idea that there was an `na.include` function or option to `nmle`?

I don't see it mentioned in the help page (and in fact see a specific warning about what will happen with dataset that have NA's. Most of the na.* options are actually functions that process the data and can be found with ?na.omit.

I could certainly be uninformed about some new wonderful workaround to the problem, but at the moment I'm guessing you were hoping for a magical effect from na.include that doesn't yet have an underlying R incantation.

-- 

David Winsemius
Alameda, CA, USA


From sarah.goslee at gmail.com  Mon Jul 13 17:36:55 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 13 Jul 2015 11:36:55 -0400
Subject: [R] an error in nlme package
In-Reply-To: <CAGxFJbTHE4u+HybL=0cQUu2DK5q_T+qhv6kQbVny+prGzMUncg@mail.gmail.com>
References: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
	<CAM_vju=-2qHMTPMduLQ=EFWwWyK15a+r2gHHOFDE0EpdPt1doQ@mail.gmail.com>
	<CAGxFJbTHE4u+HybL=0cQUu2DK5q_T+qhv6kQbVny+prGzMUncg@mail.gmail.com>
Message-ID: <CAM_vjumzZb0H+HidKQbGF=e8Tf1FJaE9idCYHW-nmPQxR=jXgA@mail.gmail.com>

On Mon, Jul 13, 2015 at 11:19 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> No, Sarah.
>
> na.action must be a function, not a character string. But you're
> close: there is no na.include function, as the message says.  It
> should be na.exclude .

My apologies: I was assuming it worked like options():

> class(options()$na.action)
[1] "character"
> options()$na.action
[1] "na.omit"

Sarah

>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Mon, Jul 13, 2015 at 7:35 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>> On Mon, Jul 13, 2015 at 3:04 AM, Patty Haaem via R-help
>> <r-help at r-project.org> wrote:
>>> Dear All,I am trying to fit one compartment IV bolus model on pharmacokinetic data using phenoModel function in nlme package, based on a tutorial entitled  "Development of population PK model using R- Case study I". The codes are as fallowing:
>>> library(nlme)mydata.grp <- groupedData(CONC~TIME|CID,data=mydata)mydata.fit < -nlme(CONC~phenoModel(CID,TIME,AMT,lCl,lV),fixed=lCl+lV~1,random=pdDiag(lCl+lV~1), data=mydata.grp,start=c(-5,0),weight=varConstPower(const=1,fixed=list(power=1)),na.action=na.include,naPattern=~!is.na(CONC))
>>> when I run above codes, I get the following error:Error in nlme.formula(CONC ~ phenoModel(CID, TIME, AMT, lCl, lV), fixed = lCl +  :   object 'na.include' not found
>>> could you please tell me, how should I correct the codes?Thanks in advanceElham Haem
>>
>>
>> First, you should not post in HTML.
>>
>> Second, you should put the argument in quotes:
>>
>> na.action="na.include"
>>
>>
>> Sarah


From dwinsemius at comcast.net  Mon Jul 13 17:59:41 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 13 Jul 2015 08:59:41 -0700
Subject: [R] an error in nlme package
In-Reply-To: <CAM_vjumzZb0H+HidKQbGF=e8Tf1FJaE9idCYHW-nmPQxR=jXgA@mail.gmail.com>
References: <577516768.1124517.1436771042247.JavaMail.yahoo@mail.yahoo.com>
	<CAM_vju=-2qHMTPMduLQ=EFWwWyK15a+r2gHHOFDE0EpdPt1doQ@mail.gmail.com>
	<CAGxFJbTHE4u+HybL=0cQUu2DK5q_T+qhv6kQbVny+prGzMUncg@mail.gmail.com>
	<CAM_vjumzZb0H+HidKQbGF=e8Tf1FJaE9idCYHW-nmPQxR=jXgA@mail.gmail.com>
Message-ID: <6D709B87-6E4B-49E9-9EB4-5BFCAED02A44@comcast.net>


On Jul 13, 2015, at 8:36 AM, Sarah Goslee wrote:

> On Mon, Jul 13, 2015 at 11:19 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> No, Sarah.
>> 
>> na.action must be a function, not a character string. But you're
>> close: there is no na.include function, as the message says.  It
>> should be na.exclude .
> 
> My apologies: I was assuming it worked like options():
> 
>> class(options()$na.action)
> [1] "character"
>> options()$na.action
> [1] "na.omit"

If such an option existed, then my understanding was that it would have an underlying function. But that's an inference from the various na.* functions that I find documented and vague memories of having seen code where there was a `get` or similar method of having a character value become a function name. I haven't actually tracked down the code for the substitution of function name for function value, since in the current instance it appears, looking first at `nlme.formula` and `model.frame.default`, that the character value gets sent to:

   External2(C_modelframe, ...)


Best;
 David.
> 
> Sarah
> 
>> 
>> Cheers,
>> Bert
>> 
>> 
>> Bert Gunter
>> 
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>   -- Clifford Stoll
>> 
>> 
>> On Mon, Jul 13, 2015 at 7:35 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
>>> On Mon, Jul 13, 2015 at 3:04 AM, Patty Haaem via R-help
>>> <r-help at r-project.org> wrote:
>>>> Dear All,I am trying to fit one compartment IV bolus model on pharmacokinetic data using phenoModel function in nlme package, based on a tutorial entitled  "Development of population PK model using R- Case study I". The codes are as fallowing:
>>>> library(nlme)mydata.grp <- groupedData(CONC~TIME|CID,data=mydata)mydata.fit < -nlme(CONC~phenoModel(CID,TIME,AMT,lCl,lV),fixed=lCl+lV~1,random=pdDiag(lCl+lV~1), data=mydata.grp,start=c(-5,0),weight=varConstPower(const=1,fixed=list(power=1)),na.action=na.include,naPattern=~!is.na(CONC))
>>>> when I run above codes, I get the following error:Error in nlme.formula(CONC ~ phenoModel(CID, TIME, AMT, lCl, lV), fixed = lCl +  :   object 'na.include' not found
>>>> could you please tell me, how should I correct the codes?Thanks in advanceElham Haem
>>> 
>>> 
>>> First, you should not post in HTML.
>>> 
>>> Second, you should put the argument in quotes:
>>> 
>>> na.action="na.include"
>>> 
>>> 
>>> Sarah
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From devazresearch at gmail.com  Mon Jul 13 18:38:49 2015
From: devazresearch at gmail.com (deva d)
Date: Mon, 13 Jul 2015 22:08:49 +0530
Subject: [R] Simulation of panel data
In-Reply-To: <CAKuYVCUn4X=S1Rnhkon_NEUKfGk8VW8EBuvOyMTb6rKWxr2Bwg@mail.gmail.com>
References: <CAKuYVCUn4X=S1Rnhkon_NEUKfGk8VW8EBuvOyMTb6rKWxr2Bwg@mail.gmail.com>
Message-ID: <CAKuYVCUfDUhQv-PT8wVOFhS=R9-pDGhs7i8YmsAxCPsgwth45w@mail.gmail.com>

Dear all,

I have a strongly balanced panel dataset of 46 entities x11 years.
Observed vars are not normally distributed

How should I simulate the ov ?

I do not know the distribution

Can somebody pl help

--------------------------T&R ... Deva,

	[[alternative HTML version deleted]]


From marcel.83.meyer at gmail.com  Mon Jul 13 20:22:07 2015
From: marcel.83.meyer at gmail.com (Marcel Meyer)
Date: Mon, 13 Jul 2015 20:22:07 +0200
Subject: [R] Output of the probemod package
Message-ID: <85ACB652-743B-403C-8E60-17AC28F70945@gmail.com>

Hello,

I am trying to follow up a significant moderation effect in my data, using the pick-a-point (pap) and Johnson-Neyman (J-N) techniques. I have found the probemod package for this, which is very useful. Working with it over the last few weeks, 3 minor queries about the output have come up and I would be grateful for any advice in this respect. Specifically:

 (1) I found that the J-N output always provides me with the exact range of my moderator. I know that this is the default setting, but what I am a bit puzzled about is that it still gives me the output for the entire range, even when I specify a smaller range manually using the mrange argument (for an example, see my script below). Have I made any mistake here? Do others have the same problem? 

(2) In my case the data ranges from 8 to 15, but the package displays numbers 1-8 in the J-N output instead. I notice that 1-8 is exactly the difference between 8-15, so perhaps 1 = 8, 2 = 9, 3 = 10 and so on, in which case I can live with that ... That said, such a convention will get a bit more confusing when the moderator has a large number of values, say of range 200-1000. Is there any way such that the actual values of the moderator can be displayed in the output instead (note: the mrange argument works fine for the plot functions of this package)?

(3) What I am also a bit confused about is that, for my data, the J-N technique yields p-values of around .05 at scores 6 and 7, which presumably represent values 13 and 14 of the moderator. Using the pap approach, I obtain a p-value < .02 at the moderator value of 13.48. How can this be that these two approaches reveal such different outcomes for very similar values (i.e. 13.48 versus 13 or 14)? Perhaps there is something I am not doing right here or have misunderstood, so I would appreciate any pointers. 

For ease of replicability, I provide my R code along with the data (see end of message) in the following:



# create data frame

mod1<-data.frame(fp, m, dm) 



#Range of moderator

range(mod1$m)



# full interaction model

m1<-lm(dm~fp*m, data=mod1) 





# call probemod package

library(probemod)

#Probemod:J-N
jnresults <- jn(m1, dv='dm', iv='fp', mod='m', mrange=10:15)

plot(jnresults)

jnresults



#Probemod: pick-a-point

ppres<-pickapoint(m1, dv='dm', iv='fp', mod='m')

plot(ppres)


ppres



#Full data:

m<-c(11, 14, 11, 12, 11, 12,  9, 12, 12,  9, 12,  8, 11, 12, 10,  9, 13, 13, 13, 12,  8, 11, 13, 

      10, 12, 12, 10, 11, 11, 15, 11, 11, 13, 10, 10, 15, 14, 12, 14, 13, 14, 15, 14, 10, 13,  9, 15,

      13, 15, 12, 12, 14, 10, 12, 14, 10, 12, 10, 14,  9,  9, 11, 11, 13)

fp<-c(5,  6,  9,  7,  8,  8, 9,  3,  3,  7,  3,  6,  9,  3,  8,  5,  4, 6,  2,  6,  6,  5,  3,  5,  7,

      8,  3,  3,  4,  3,  7,  7, 5,  4, 10,  9,  2,  9,  2,  2,  4,  3,  3,  3,  8,  5,  4,  6,  9,  4,

      4,  4,  5,  5,  6,  4,  4,  3,  3,  8,  6,  6,  8,  6)

dm<-c(798.5027, 773.7591, 816.7397, 867.3680, 827.8940, 824.8648, 810.3585, 832.5348, 773.7681, 792.2763, 

      NA, 884.4126, 866.2052, 862.0126, 851.3000, 812.8300, 778.7394, 781.0571, 798.9329, 806.9844, 

      831.4983, 814.9005, 836.8078, 823.0125, 763.5780, 780.9182, NA, 842.2906, 788.2910, 835.8092, 

      768.4258, 734.9783, 855.5227, 833.1630, 817.5763, NA, 802.0592, 758.7745, 846.8749, 791.8602, NA,

      NA, 869.1863, 766.5122, 834.5878, 882.9315, 917.4202, 804.2642, 748.3454, 800.6837, 790.6344, 

      758.0473, NA, NA, 814.9149, 785.4883, NA, 778.5333, 865.5467, 820.8561, 779.8348, 813.4988,


      784.0798, 781.3917)



Any comments would be much appreciated.

Best,

Marcel



	[[alternative HTML version deleted]]


From marcel.83.meyer at gmail.com  Mon Jul 13 20:22:14 2015
From: marcel.83.meyer at gmail.com (Marcel Meyer)
Date: Mon, 13 Jul 2015 20:22:14 +0200
Subject: [R] R squared change value for a moderation effect
Message-ID: <25194B83-574D-4EC9-8BB4-D98D889040FE@gmail.com>

Hello,

I want to test a regression model with neuroticism as focal predictor, agreeableness as moderator and RT variability as dependent measure (covariates: attentional control and mean RT). Previously, I have used the modprobe macro in SPSS by Andrew Hayes for this (for full reference see end of message). I am in the process of transitioning to R, however, and would like to learn how to run a similar routine there. I have set up my regression model as follows:

    m3<-lm(data=stp2_sub2, all_SD~Neuroticism*Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # full interaction model
    m33<-lm(data=stp2_sub2, all_SD~Neuroticism+Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # reduced model

I know that I can obtain F-change and p-change, using:

    anova(m3, m33) # provides F-change and p-change

What I still don?t know yet is how to obtain the R squared change value, which gives me the effect size of the interaction effect. Any advice on this would be much appreciated.

Best,
Marcel

Reference:
Hayes, A. F., & Matthes, J. (2009). Computational procedures for probing interactions in OLS and logistic regression: SPSS and SAS implementations. Behavior Research Methods, 41(3), 924?36. doi:10.3758/BRM.41.3.924
	[[alternative HTML version deleted]]


From lid.zigh at gmail.com  Tue Jul 14 00:01:42 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Mon, 13 Jul 2015 17:01:42 -0500
Subject: [R] remove 0 and NA values
Message-ID: <CAMqbV1AsmSaCMiLzm6EU6Sc2+YmEMdUHdD-qCe7uRh5o7h384g@mail.gmail.com>

Hi there,

I have a matrix which its elements are 0, 1,2,NA
I want to remove the columns which the colsums are equal to 0 or NA and
drop these columns from the original matrix and create the new matrix for
the nonzero and NA value?
 (I think I have consider na.rm=True and remove the colums with colsum=0,
 because if I consider na.rm=False all the values of my colsums get NA)

this is my matrix format:

mat[1:5,1:5]

           1:110590170    1:110888172     1:110906406   1:110993854
 1:110996710   1:111144756
A05363           0                        0                     0
        0                         NA                     0
A05370           0                        0                     0
        0                         0                     NA
A05380           1
         NA                   2                  0
     NA                     0
A05397           0                        0
0                  1                         0                       2
A05400           2                        0                     0
          0                         0                        0
A05426           0
0                     NA               0
0                        0



summat <-  colSums(mat,na.rm = TRUE)

> head(summat)
                        [,1]
1:110590170     3
1:110888172     0
1:110906406      2
1:110993854     1
1:110996710     0
1:111144756     2

 The 2nd and 5th columns have colsum=0 so I Ishould remove them from the
met and keep the rest of columns in  another matrix.

my out put should be like below:

metnonzero

       1:110590170         1:110906406          1:110993854
  1:111144756
A05363           0                          0
0                                   0
A05370           0                          0
0                                  NA
A05380           1                          2
0                                   0
A05397           0
0                  1                                   2
A05400           2                          0
0                                   0
A05426           0                          NA
0                                   0

would you please let me know how can I do that?

Many thanks,
Lid

	[[alternative HTML version deleted]]


From csardi.gabor at gmail.com  Tue Jul 14 00:06:40 2015
From: csardi.gabor at gmail.com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Mon, 13 Jul 2015 18:06:40 -0400
Subject: [R] Understanding mod.matrix for directed networks
In-Reply-To: <8BB99D4AA51D4A4397817F165A7BD8DD6F0A7988@CHIMBX5.ad.uillinois.edu>
References: <8BB99D4AA51D4A4397817F165A7BD8DD6F0A7988@CHIMBX5.ad.uillinois.edu>
Message-ID: <CABtg=Kk=5E6FkESHS3pehPeZNCbXSLtJRDjaUzoh=Bvzr=xnQg@mail.gmail.com>

I am sorry for the slow response. Please note that these are better
channels for igraph help: http://igraph.org/r/#help

As for your question, can you please send some R code that
demonstrates your problem?

Gabor

On Tue, May 19, 2015 at 1:50 AM, Aziz, Muhammad Fayez
<aziz4 at illinois.edu> wrote:
> Dear Gabor,
>
>
>
> Please find attached an Excel file with detailed information on a four node
> directed network to calculate its pairwise modularity matrix using igraph:
> mod.matrix(). I have tried multiple combinations to figure out how the
> formula works or is applied but failed to find consistency, specially for
> the pairs of arcs in opposing directions where the pairwise modularity is
> different (highlighted and color-coded). I gave membership(wtc) as input to
> mod.matrix() but not even sure if its used in the formula provided in the
> following R manual page:
>
>
>
> http://igraph.org/r/doc/modularity.html
>
>
>
> Advice with a solved example in the Excel sheet would be deeply appreciated.
>
>
>
> Thanks and regards,
>
> Fayez
>
> University of Illinois at Urbana Champaign, USA


From johannes at huesing.name  Sat Jul 11 06:36:38 2015
From: johannes at huesing.name (Johannes Huesing)
Date: Sat, 11 Jul 2015 06:36:38 +0200
Subject: [R] problem understanding grid coordinate systems
Message-ID: <20150711043638.GA17505@huesing.name>


According to "R Graphics" by Paul Murrell, the coordinates that were used for the
most recent lattics plot can be retrieved with "native" units.

I have difficulties to access these coordinates. The following code
renders the following results:

> library(grid)
> library(lattice)
> pl <- xyplot(1:10 ~ 10:1)
> print(pl)
> convertX(unit(.9, "npc"), "native")
[1] 605.7native

  R version 3.0.2 (2013-09-25)

I had expected something around 9.5 in native coordinates. The result
looks more like raw coordinates, which might be what is meant by
native. Which coordinate system do I have to address instead?



-- 
Johannes H?sing


From dcarov at gmail.com  Mon Jul 13 23:06:43 2015
From: dcarov at gmail.com (Daniel Caro)
Date: Mon, 13 Jul 2015 22:06:43 +0100
Subject: [R] Function returning multiple objects but printing only one
Message-ID: <CAMeQTh2hWLKDCbMC64dAioQYHUtnGJk1=H5PXEPQaub8kGRbHQ@mail.gmail.com>

Hello,

Sorry if this has already been addressed before but I could not find any
helpful references.

I would like to create a function that outputs a single element of a list
but stores all elements, similar to  'lm' and many other functions. There
are several answers on how to return multiple objects with lists, for
example:

http://r.789695.n4.nabble.com/How-to-return-multiple-values-in-a-function-td858528.html

http://stackoverflow.com/questions/8936099/returning-multiple-objects-in-an-r-function

But the examples show how to print multiple outputs, such as

functionReturningTwoValues <- function() {return(list(first=1, second=2))}
functionReturningTwoValues()

And I only want the function to print a single element from the list but
still store the other elements such that they can be retrieved with
functionReturningTwoValues$first, for example. My function produces
bootstrap coefficients so clearly I don't want to print the bootstrap
output but I do want users to be able to access it.

Many thanks,
Daniel

	[[alternative HTML version deleted]]


From karlashikev at gmail.com  Mon Jul 13 23:44:23 2015
From: karlashikev at gmail.com (Karla Shikev)
Date: Mon, 13 Jul 2015 18:44:23 -0300
Subject: [R] overlap between line segments
Message-ID: <CAPU4_4r44A=+=smbQNjSPE6a6dPtLv5biVN+ySTCsVQnmNn5og@mail.gmail.com>

Hi there,

This is a newbie question, and I'm sure there are simple ways to do this,
but I've spent my entire afternoon and I couldn't get it to work.

Imagine that I got my samples distributed along a transect and my data
refer to the first and last occurrences of each sample. For instance:

> dat<-matrix(c(1,3,2.5,4), ncol=2, byrow=TRUE)
> dat
     [,1] [,2]
[1,]  1.0    3
[2,]  2.5    4


The first line indicates that the first and last occurrences of this
subject were 1 and 3, respectively, whereas the second subject was found
between 2.5 and 4.

I need a simple way to calculate the overlap of their extents (0.5 in this
case). This way should provide 0 if there is no overlap, and it should also
work in the case where one subject is found only within the extent of the
second subject.

Any help will be greatly appreciated.

Karla

	[[alternative HTML version deleted]]


From jawadhussain at vcomsats.edu.pk  Tue Jul 14 04:41:07 2015
From: jawadhussain at vcomsats.edu.pk (Syed Jawad Hussain Shahzad)
Date: Tue, 14 Jul 2015 10:41:07 +0800
Subject: [R] Rolling Granger Causality
Message-ID: <CAOb7=cVhfOgA0MU-rYh8CJCKmPrL4cVJPXVvAqCsSaxQWo0tgQ@mail.gmail.com>

I am using MSBVAR package in R, and have used the following codes:

library("MSBVAR")
data(IsraelPalestineConflict)
granger.test.c <- function(x) c(granger.test(x, p = 6))
aa<-rollapplyr(IsraelPalestineConflict, 1275, granger.test.c,
by.column = FALSE )

In rollapply, the width is 1275 (data contains total 1278
observations) and hence i have four rolls. In granger.test.c the lag
order "p" is fixed at 6. I want to use a changing "p" for each roll
using R object "p<-c(3,4,5,6)".

Help is requested, please.

Best
Jawad


From djnordlund at frontier.com  Tue Jul 14 07:52:28 2015
From: djnordlund at frontier.com (Daniel Nordlund)
Date: Mon, 13 Jul 2015 22:52:28 -0700
Subject: [R] remove 0 and NA values
In-Reply-To: <CAMqbV1BZygEJPoYVJahPT+McCATJgQ0Av8Ln9Kb9ixuSEkUi=g@mail.gmail.com>
References: <CAMqbV1AsmSaCMiLzm6EU6Sc2+YmEMdUHdD-qCe7uRh5o7h384g@mail.gmail.com>
	<55A43C26.5040101@frontier.com>
	<CAMqbV1BZygEJPoYVJahPT+McCATJgQ0Av8Ln9Kb9ixuSEkUi=g@mail.gmail.com>
Message-ID: <55A4A39C.9010603@frontier.com>

On 7/13/2015 6:02 PM, Lida Zeighami wrote:
> Hi Dan,
>
> Thanks for reply,
> Sorry the format of matrix is ruiend!
> Yes, this matrix is 6?6 but my orginal matrix is so biger than this!!
>
> No, I don't think so your code do that for me!
> I want to remove the columns which the sum of their values are equal to
> zero!
>
> On Jul 13, 2015 5:31 PM, "Daniel Nordlund" <djnordlund at frontier.com
> <mailto:djnordlund at frontier.com>> wrote:
>
>     On 7/13/2015 3:01 PM, Lida Zeighami wrote:
>
>         Hi there,
>
>         I have a matrix which its elements are 0, 1,2,NA
>         I want to remove the columns which the colsums are equal to 0 or
>         NA and
>         drop these columns from the original matrix and create the new
>         matrix for
>         the nonzero and NA value?
>            (I think I have consider na.rm=True and remove the colums
>         with colsum=0,
>            because if I consider na.rm=False all the values of my
>         colsums get NA)
>
>         this is my matrix format:
>
>         mat[1:5,1:5]
>
>                      1:110590170    1:110888172     1:110906406
>           1:110993854
>            1:110996710   1:111144756
>         A05363           0                        0                     0
>                   0                         NA                     0
>         A05370           0                        0                     0
>                   0                         0                     NA
>         A05380           1
>                    NA                   2                  0
>                NA                     0
>         A05397           0                        0
>         0                  1                         0
>               2
>         A05400           2                        0                     0
>                     0                         0                        0
>         A05426           0
>         0                     NA               0
>         0                        0
>
>
>
>         summat <-  colSums(mat,na.rm = TRUE)
>
>             head(summat)
>
>                                   [,1]
>         1:110590170     3
>         1:110888172     0
>         1:110906406      2
>         1:110993854     1
>         1:110996710     0
>         1:111144756     2
>
>            The 2nd and 5th columns have colsum=0 so I Ishould remove
>         them from the
>         met and keep the rest of columns in  another matrix.
>
>         my out put should be like below:
>
>         metnonzero
>
>                  1:110590170         1:110906406          1:110993854
>             1:111144756
>         A05363           0                          0
>         0                                   0
>         A05370           0                          0
>         0                                  NA
>         A05380           1                          2
>         0                                   0
>         A05397           0
>         0                  1                                   2
>         A05400           2                          0
>         0                                   0
>         A05426           0                          NA
>         0                                   0
>
>         would you please let me know how can I do that?
>
>         Many thanks,
>         Lid
>
>
>     First, you matrix appears to be 6x6.  That being said, does this get
>     you what you want?
>
>     mat[, -which(summat[,1] ]
>
>
>     Dan
>
>     --
>     Daniel Nordlund
>     Bothell, WA USA
>

Lida,

I seem to have cut-and-pasted something very badly, and for that I 
apologize.  Here is a revised version:

mat <- structure(c(0L, 0L, 1L, 0L, 2L, 0L, 0L, 0L, NA, 0L, 0L, 0L, 0L,
0L, 2L, 0L, 0L, NA, 0L, 0L, 0L, 1L, 0L, 0L, NA, 0L, NA, 0L, 0L,
0L, 0L, NA, 0L, 2L, 0L, 0L), .Dim = c(6L, 6L), .Dimnames = list(
c("A05363", "A05370", "A05380", "A05397", "A05400", "A05426"), 
c("X1.110590170", "X1.110888172", "X1.110906406", "X1.110993854", 
"X1.110996710", "X1.111144756")))
summat <- colSums(mat,na.rm = TRUE)
mat[,-which(summat==0)]
        X1.110590170 X1.110906406 X1.110993854 X1.111144756
A05363            0            0            0            0
A05370            0            0            0           NA
A05380            1            2            0            0
A05397            0            0            1            2
A05400            2            0            0            0
A05426            0           NA            0            0
 >

Hope this is more helpful,

Dan

-- 
Daniel Nordlund
Bothell, WA USA


From devazresearch at gmail.com  Tue Jul 14 08:20:36 2015
From: devazresearch at gmail.com (deva d)
Date: Tue, 14 Jul 2015 11:50:36 +0530
Subject: [R] Simulation of strongly balanced panel data
Message-ID: <CAKuYVCUQhGE6F0zJ6g2xeNg8t+bSjztNrCkwmmrHUSQ7rEjczQ@mail.gmail.com>

Dear all,

I have a strongly balanced panel dataset of 46 entities x11 years.
Observed vars are not normally distributed

How should I simulate the ov ?

I do not know the distribution

Can somebody pl help

--------------------------


*....*

*Deva*

	[[alternative HTML version deleted]]


From alexkim205 at yahoo.com  Tue Jul 14 07:33:17 2015
From: alexkim205 at yahoo.com (Alex Kim)
Date: Tue, 14 Jul 2015 01:33:17 -0400
Subject: [R] A simple question
Message-ID: <CAMy2ZoOrKWhw6ZOBuG-JxSyigkrznA9a_iYAFgL8sHb4gdnxkA@mail.gmail.com>

Hello,

I am trying to create a matrix that looks like this, using the
stri_locate_all function.

    > x <- "ABCDJAKSLABCDAKJSABCD"
    > m <- stri_locate_all_regex(x, 'ABCD')
    > m
    [[1]]
         start end
    [1,]     1   4
    [2,]    10  13
    [3,]    18  21

I tried converting m into a matrix, however it always seems to wrap around
the wrong way:

    > output <- matrix(unlist(m), ncol = 2, byrow = TRUE)
    > output
         [,1] [,2]
    [1,]    1   10
    [2,]   18    4
    [3,]   13   21

I want to output the start locations in the first column and the end
locations in the second column into a matrix to look like this.

         [,1] [,2]
    [1,]     1   4
    [2,]    10  13
    [3,]    18  21

Thank you for your help,
Alex

	[[alternative HTML version deleted]]


From dumboisverydumb at gmail.com  Tue Jul 14 07:40:29 2015
From: dumboisverydumb at gmail.com (Alex Kim)
Date: Tue, 14 Jul 2015 01:40:29 -0400
Subject: [R] A Simple Question
Message-ID: <CAMy2ZoMCm_wnDK8kN6iHur1ZmGkkpJDf8kC524ZYn1LiiCqj-Q@mail.gmail.com>

Hello,

I am trying to create a matrix that looks like this, using the
stri_locate_all function.

    > x <- "ABCDJAKSLABCDAKJSABCD"
    > m <- stri_locate_all_regex(x, 'ABCD')
    > m
    [[1]]
         start end
    [1,]     1   4
    [2,]    10  13
    [3,]    18  21

I tried converting m into a matrix, however it always seems to wrap around
the wrong way:

    > output <- matrix(unlist(m), ncol = 2, byrow = TRUE)
    > output
         [,1] [,2]
    [1,]    1   10
    [2,]   18    4
    [3,]   13   21

I want to output the start locations in the first column and the end
locations in the second column into a matrix to look like this.

         [,1] [,2]
    [1,]     1   4
    [2,]    10  13
    [3,]    18  21

	[[alternative HTML version deleted]]


From erich.subs at neuwirth.priv.at  Tue Jul 14 08:42:13 2015
From: erich.subs at neuwirth.priv.at (Erich Subscriptions)
Date: Tue, 14 Jul 2015 08:42:13 +0200
Subject: [R] A Simple Question
In-Reply-To: <CAMy2ZoMCm_wnDK8kN6iHur1ZmGkkpJDf8kC524ZYn1LiiCqj-Q@mail.gmail.com>
References: <CAMy2ZoMCm_wnDK8kN6iHur1ZmGkkpJDf8kC524ZYn1LiiCqj-Q@mail.gmail.com>
Message-ID: <AFED3B05-0DCC-4743-B2B4-2901681B46B6@neuwirth.priv.at>

Is m[[1]]
what you need?

> On 14 Jul 2015, at 07:40, Alex Kim <dumboisverydumb at gmail.com> wrote:
> 
> Hello,
> 
> I am trying to create a matrix that looks like this, using the
> stri_locate_all function.
> 
>> x <- "ABCDJAKSLABCDAKJSABCD"
>> m <- stri_locate_all_regex(x, 'ABCD')
>> m
>    [[1]]
>         start end
>    [1,]     1   4
>    [2,]    10  13
>    [3,]    18  21
> 
> I tried converting m into a matrix, however it always seems to wrap around
> the wrong way:
> 
>> output <- matrix(unlist(m), ncol = 2, byrow = TRUE)
>> output
>         [,1] [,2]
>    [1,]    1   10
>    [2,]   18    4
>    [3,]   13   21
> 
> I want to output the start locations in the first column and the end
> locations in the second column into a matrix to look like this.
> 
>         [,1] [,2]
>    [1,]     1   4
>    [2,]    10  13
>    [3,]    18  21
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From maechler at stat.math.ethz.ch  Tue Jul 14 09:34:33 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 14 Jul 2015 09:34:33 +0200
Subject: [R] : Ramanujan and the accuracy of floating point computations
 - using Rmpfr in R
In-Reply-To: <55988B83.1050805@gmail.com>
References: <559883DA.8090209@gmail.com>
	<55988B83.1050805@gmail.com>
Message-ID: <21924.48009.540218.216746@stat.math.ethz.ch>

>>>>> "P" == ProfJCNash  <profjcnash at gmail.com>
>>>>>     on Sat, 4 Jul 2015 21:42:27 -0400 writes:

    P> n163 <- mpfr(163, 500)

    P> is how I set up the number.

Yes, and you have needed to specify the desired precision.

As author and maintainer of Rmpfr, let me give my summary of this overly
long thread (with many wrong statements before finally RK give
the "obvious" answer):

1) Rmpfr is about high (or low, for didactical reasons, see Rich
   Heiberger's talk at 'useR! 2015') precision __numerical__ computations.
   This is what the GNU MPFR library is about, and Rmpfr wants
   to be a smart and R-like {e.g., automatic coercions wherever they
   make sense} R interface to MPFR.

2) If you use Rmpfr, you as user should decide about the desired accuracy
   of your "inputs".
   I think it would be a very bad idea to redefine 'pi' (in
   Rmpfr) to be Const("pi", 120).

   R-like also means that in a computation   a * b   the
   properties of a and b determine the result, and hence if  a <- pi
   then we know that pi is a double precision number with 53-bit
   mantissa.


    p> On 15-07-04 05:10 PM, Ravi Varadhan wrote:
    >>> What about numeric constants, like `163'?

well, if you _combine_ them with an mpfr() number, they are used
in their precision.  An integer like 163 is exact of course; but
pi (another numeric constant) is 53-bit as mentioned above, 
*and*     sqrt(163)  is ("R-like") a double precision number
computed by R's internal double precision arithmetic.

My summary:

--------------------------------------------------------------------
    1. Rmpfr behaves entirely correctly and as documented 
       (in all the examples given here).

    2. The idea of substituting expressions containing pi with
       something like Const("pi", 120) is not a good one.  (*)
--------------------------------------------------------------------


*) One could think of adding a new class, say "symbolicNumber"
   or rather "numericExpression" which in arithmetic would
   try to behave correctly, namely find out what precision all
   the other components in "the arithmetic" have and make sure
   all parts of the 'numericExpression' are coerced to
   'mpfr' with the correct precision, and only then start
   evaluating "the arithmetic".

   But that *does* look like implementation of
   Maple/Mathematica/... in R  and that seems silly; rather R
   should interface to Free (as in "speech") aka "open source"
   symbolic math packages such as Pari, macysma, ..

Martin Maechler
ETH Zurich


From drjimlemon at gmail.com  Tue Jul 14 12:03:12 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 14 Jul 2015 20:03:12 +1000
Subject: [R] overlap between line segments
In-Reply-To: <CAPU4_4r44A=+=smbQNjSPE6a6dPtLv5biVN+ySTCsVQnmNn5og@mail.gmail.com>
References: <CAPU4_4r44A=+=smbQNjSPE6a6dPtLv5biVN+ySTCsVQnmNn5og@mail.gmail.com>
Message-ID: <CA+8X3fXug3FFzcR25jZx5_jZbvUWK234rne1Chw_oPi-o5p9SA@mail.gmail.com>

Hi Karla,
This might help. I haven't tested it exhaustively.

transect_overlap<-function(x) {
 if(!is.matrix(x)) stop("x must be a 2x2 matrix")
 if(x[1,1] <= x[2,1]) {
  if(x[2,2] > x[1,2]) overlap<-x[1,2]-x[2,1]
  else overlap<-x[2,2]-x[2,1]
 }
 else {
  if(x[1,2] > x[2,2]) overlap<-x[2,2]-x[1,1]
  else overlap<-x[1,2]- x[1,1]
 }
 if(overlap < 0) overlap<-0
 return(overlap)
}

Jim


On Tue, Jul 14, 2015 at 7:44 AM, Karla Shikev <karlashikev at gmail.com> wrote:
> Hi there,
>
> This is a newbie question, and I'm sure there are simple ways to do this,
> but I've spent my entire afternoon and I couldn't get it to work.
>
> Imagine that I got my samples distributed along a transect and my data
> refer to the first and last occurrences of each sample. For instance:
>
>> dat<-matrix(c(1,3,2.5,4), ncol=2, byrow=TRUE)
>> dat
>      [,1] [,2]
> [1,]  1.0    3
> [2,]  2.5    4
>
>
> The first line indicates that the first and last occurrences of this
> subject were 1 and 3, respectively, whereas the second subject was found
> between 2.5 and 4.
>
> I need a simple way to calculate the overlap of their extents (0.5 in this
> case). This way should provide 0 if there is no overlap, and it should also
> work in the case where one subject is found only within the extent of the
> second subject.
>
> Any help will be greatly appreciated.
>
> Karla
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Tue Jul 14 13:17:53 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 14 Jul 2015 07:17:53 -0400
Subject: [R] Function returning multiple objects but printing only one
In-Reply-To: <CAMeQTh2hWLKDCbMC64dAioQYHUtnGJk1=H5PXEPQaub8kGRbHQ@mail.gmail.com>
References: <CAMeQTh2hWLKDCbMC64dAioQYHUtnGJk1=H5PXEPQaub8kGRbHQ@mail.gmail.com>
Message-ID: <55A4EFE1.8020203@gmail.com>

On 13/07/2015 5:06 PM, Daniel Caro wrote:
> Hello,
> 
> Sorry if this has already been addressed before but I could not find any
> helpful references.
> 
> I would like to create a function that outputs a single element of a list
> but stores all elements, similar to  'lm' and many other functions. There
> are several answers on how to return multiple objects with lists, for
> example:
> 
> http://r.789695.n4.nabble.com/How-to-return-multiple-values-in-a-function-td858528.html
> 
> http://stackoverflow.com/questions/8936099/returning-multiple-objects-in-an-r-function
> 
> But the examples show how to print multiple outputs, such as
> 
> functionReturningTwoValues <- function() {return(list(first=1, second=2))}
> functionReturningTwoValues()
> 
> And I only want the function to print a single element from the list but
> still store the other elements such that they can be retrieved with
> functionReturningTwoValues$first, for example. My function produces
> bootstrap coefficients so clearly I don't want to print the bootstrap
> output but I do want users to be able to access it.

You need to give your object a class, and define a print method for that
class.  It's pretty simple:


functionReturningTwoValues <- function() {return(structure(list(first=1,
second=2), class="MyClass"))}

print.MyClass <- function(x, ...) {
  print(x$first, ...)
}

This is using "S3 classes".  There are other systems (S4, etc.) that let
you do this, but none are simpler.

Duncan Murdoch


From jvadams at usgs.gov  Tue Jul 14 14:47:30 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Tue, 14 Jul 2015 07:47:30 -0500
Subject: [R] A simple question
In-Reply-To: <CAMy2ZoOrKWhw6ZOBuG-JxSyigkrznA9a_iYAFgL8sHb4gdnxkA@mail.gmail.com>
References: <CAMy2ZoOrKWhw6ZOBuG-JxSyigkrznA9a_iYAFgL8sHb4gdnxkA@mail.gmail.com>
Message-ID: <CAN5YmCG+4pykCBZ0LTJuAcQKg-_nGH46FiCL8JFi+N=nz6z2nQ@mail.gmail.com>

R-help readers,

For your information ...

The package stringi is required to run Alex's code.

Alex's message was cross posted to StackOverflow, and seems to have been
answered there,
http://stackoverflow.com/questions/31398466/r-stri-locate-all-creating-a-start-and-end-matrix

Jean

On Tue, Jul 14, 2015 at 12:33 AM, Alex Kim via R-help <r-help at r-project.org>
wrote:

> Hello,
>
> I am trying to create a matrix that looks like this, using the
> stri_locate_all function.
>
>     > x <- "ABCDJAKSLABCDAKJSABCD"
>     > m <- stri_locate_all_regex(x, 'ABCD')
>     > m
>     [[1]]
>          start end
>     [1,]     1   4
>     [2,]    10  13
>     [3,]    18  21
>
> I tried converting m into a matrix, however it always seems to wrap around
> the wrong way:
>
>     > output <- matrix(unlist(m), ncol = 2, byrow = TRUE)
>     > output
>          [,1] [,2]
>     [1,]    1   10
>     [2,]   18    4
>     [3,]   13   21
>
> I want to output the start locations in the first column and the end
> locations in the second column into a matrix to look like this.
>
>          [,1] [,2]
>     [1,]     1   4
>     [2,]    10  13
>     [3,]    18  21
>
> Thank you for your help,
> Alex
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jvadams at usgs.gov  Tue Jul 14 14:54:37 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Tue, 14 Jul 2015 07:54:37 -0500
Subject: [R] Function returning multiple objects but printing only one
In-Reply-To: <CAMeQTh2hWLKDCbMC64dAioQYHUtnGJk1=H5PXEPQaub8kGRbHQ@mail.gmail.com>
References: <CAMeQTh2hWLKDCbMC64dAioQYHUtnGJk1=H5PXEPQaub8kGRbHQ@mail.gmail.com>
Message-ID: <CAN5YmCE5ZHx2=UNFPDCCAn3eAzPCtj_V_X9RTUO-TNx0mN9+Wg@mail.gmail.com>

Daniel,

I'm not sure if this is what you're after, but you could include a print()
call in your function.  For example:

myfun <- function(x) {
  m1 <- min(x)
  m2 <- mean(x)
  m3 <- max(x)
  out <- list(m1, m2, m3)
  print(out[[2]])
  return(out)
}

result <- myfun(1:10)

?Jean
?

On Mon, Jul 13, 2015 at 4:06 PM, Daniel Caro <dcarov at gmail.com> wrote:

> Hello,
>
> Sorry if this has already been addressed before but I could not find any
> helpful references.
>
> I would like to create a function that outputs a single element of a list
> but stores all elements, similar to  'lm' and many other functions. There
> are several answers on how to return multiple objects with lists, for
> example:
>
>
> http://r.789695.n4.nabble.com/How-to-return-multiple-values-in-a-function-td858528.html
>
>
> http://stackoverflow.com/questions/8936099/returning-multiple-objects-in-an-r-function
>
> But the examples show how to print multiple outputs, such as
>
> functionReturningTwoValues <- function() {return(list(first=1, second=2))}
> functionReturningTwoValues()
>
> And I only want the function to print a single element from the list but
> still store the other elements such that they can be retrieved with
> functionReturningTwoValues$first, for example. My function produces
> bootstrap coefficients so clearly I don't want to print the bootstrap
> output but I do want users to be able to access it.
>
> Many thanks,
> Daniel
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From knoxt at bp.com  Tue Jul 14 12:09:33 2015
From: knoxt at bp.com (Knox, Tom)
Date: Tue, 14 Jul 2015 10:09:33 +0000
Subject: [R] Problem accessing xslx on CRAN mirrir
Message-ID: <F7258AFE5A68984E91FB0FF929AC92012CF733AD@DE35S00FHST08.DSC.BP.COM>

I am very new to the use of R and trying to install it in order to use a package called mh1823 for determining Probability of Detection stats for Non-Destructive Evaluation.  I have hit an immediate problem installing the xlsx package and get the following messages:

Warning: unable to access index for repository http://star-www.st-andrews.ac.uk/cran/src/contrib
Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/src/contrib
Error in install.packages(NULL, .libPaths()[1L], dependencies = NA, type = type) :
  no packages were specified
In addition: Warning message:
In open.connection(con, "r") : cannot open: HTTP status was '0 (nil)'

I do not seem to be able to get past this issue, though am able to load the mh1823 POD package successfully from local zip file

Regards,

Tom
Tom Knox
NDE Subject Matter Expert
Upstream Engineering Centre
BP
Sunbury-on-Thames
Mobile: +44 (0)7796 182926
Fax:     +44 (0)1932 763439
E-mail: tom.knox at bp.com
Postal Address:  Building H, BP Exploration, Chertsey Road, Sunbury-on-Thames, UK, TW16 7LN





	[[alternative HTML version deleted]]


From karlashikev at gmail.com  Tue Jul 14 15:18:54 2015
From: karlashikev at gmail.com (Karla Shikev)
Date: Tue, 14 Jul 2015 10:18:54 -0300
Subject: [R] overlap between line segments
In-Reply-To: <CA+8X3fXug3FFzcR25jZx5_jZbvUWK234rne1Chw_oPi-o5p9SA@mail.gmail.com>
References: <CAPU4_4r44A=+=smbQNjSPE6a6dPtLv5biVN+ySTCsVQnmNn5og@mail.gmail.com>
	<CA+8X3fXug3FFzcR25jZx5_jZbvUWK234rne1Chw_oPi-o5p9SA@mail.gmail.com>
Message-ID: <CAPU4_4o0mdmkArO744e+ZNS=beTPKYjGrJaBsU6BWH-D+vOVUw@mail.gmail.com>

Fantastic. Thanks!

On Tue, Jul 14, 2015 at 7:03 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Karla,
> This might help. I haven't tested it exhaustively.
>
> transect_overlap<-function(x) {
>  if(!is.matrix(x)) stop("x must be a 2x2 matrix")
>  if(x[1,1] <= x[2,1]) {
>   if(x[2,2] > x[1,2]) overlap<-x[1,2]-x[2,1]
>   else overlap<-x[2,2]-x[2,1]
>  }
>  else {
>   if(x[1,2] > x[2,2]) overlap<-x[2,2]-x[1,1]
>   else overlap<-x[1,2]- x[1,1]
>  }
>  if(overlap < 0) overlap<-0
>  return(overlap)
> }
>
> Jim
>
>
> On Tue, Jul 14, 2015 at 7:44 AM, Karla Shikev <karlashikev at gmail.com>
> wrote:
> > Hi there,
> >
> > This is a newbie question, and I'm sure there are simple ways to do this,
> > but I've spent my entire afternoon and I couldn't get it to work.
> >
> > Imagine that I got my samples distributed along a transect and my data
> > refer to the first and last occurrences of each sample. For instance:
> >
> >> dat<-matrix(c(1,3,2.5,4), ncol=2, byrow=TRUE)
> >> dat
> >      [,1] [,2]
> > [1,]  1.0    3
> > [2,]  2.5    4
> >
> >
> > The first line indicates that the first and last occurrences of this
> > subject were 1 and 3, respectively, whereas the second subject was found
> > between 2.5 and 4.
> >
> > I need a simple way to calculate the overlap of their extents (0.5 in
> this
> > case). This way should provide 0 if there is no overlap, and it should
> also
> > work in the case where one subject is found only within the extent of the
> > second subject.
> >
> > Any help will be greatly appreciated.
> >
> > Karla
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue Jul 14 16:50:53 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 14 Jul 2015 10:50:53 -0400
Subject: [R] Problem accessing xslx on CRAN mirrir
In-Reply-To: <F7258AFE5A68984E91FB0FF929AC92012CF733AD@DE35S00FHST08.DSC.BP.COM>
References: <F7258AFE5A68984E91FB0FF929AC92012CF733AD@DE35S00FHST08.DSC.BP.COM>
Message-ID: <55A521CD.7070601@gmail.com>

On 14/07/2015 6:09 AM, Knox, Tom wrote:
> I am very new to the use of R and trying to install it in order to use a package called mh1823 for determining Probability of Detection stats for Non-Destructive Evaluation.  I have hit an immediate problem installing the xlsx package and get the following messages:
> 
> Warning: unable to access index for repository http://star-www.st-andrews.ac.uk/cran/src/contrib
> Warning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/src/contrib
> Error in install.packages(NULL, .libPaths()[1L], dependencies = NA, type = type) :
>   no packages were specified
> In addition: Warning message:
> In open.connection(con, "r") : cannot open: HTTP status was '0 (nil)'
> 
> I do not seem to be able to get past this issue, though am able to load the mh1823 POD package successfully from local zip file

It looks as though R couldn't make any connection.  Perhaps you are
using a proxy?  It looks as though you are on Windows; if so, you could
try running

setInternet2(TRUE)

before the install; that will use the proxy settings for Internet Explorer.

Duncan Murdoch

> 
> Regards,
> 
> Tom
> Tom Knox
> NDE Subject Matter Expert
> Upstream Engineering Centre
> BP
> Sunbury-on-Thames
> Mobile: +44 (0)7796 182926
> Fax:     +44 (0)1932 763439
> E-mail: tom.knox at bp.com
> Postal Address:  Building H, BP Exploration, Chertsey Road, Sunbury-on-Thames, UK, TW16 7LN
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From roger.bos at rothschild.com  Tue Jul 14 17:35:42 2015
From: roger.bos at rothschild.com (Bos, Roger)
Date: Tue, 14 Jul 2015 15:35:42 +0000
Subject: [R] advice on making HTML tables
Message-ID: <0765308CD028654885F30322557308D81EEF3BF2@NYCSM0208.rth.ad.rothschild.com>

This might be a little off topic, but I am starting to produce some HTML reports that contain mostly tables and they look great in Chrome but really bad in IE, so I wanted to see if anyone knows of a better way or an easy fix.  One option I have used is to convert to PDF, but sometimes it is nice to have the report in HTML format.

For my reproducible example, copy the text below into R Studio and hit the knit button. If you look at the HTML output in Chrome the columns are nicely spread out and in IE the columns are jammed right next to each other with minimal/no spacing.  Maybe there is a CSS fix?

---
title: "Untitled"
output: html_document
---

```{r}
knitr::kable(cars)
```

Here is my session info:

R version 3.2.1 Patched (2015-07-11 r68646)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] datasets  tools     utils     stats     graphics  grDevices methods   base

other attached packages:
 [1] xtable_1.7-4        sqldf_0.4-10        RSQLite_1.0.0       DBI_0.3.1           gsubfn_0.6-6
 [6] proto_0.3-10        Rcpp_0.11.6         Quandl_2.6.0        testthat_0.10.0     lubridate_1.3.3
[11] sendmailR_1.2-1     rmarkdown_0.7       devtools_1.8.0      data.table_1.9.4    Rpad_1.3.0
[16] formatR_1.2         dplyr_0.4.2.9002    plyr_1.8.3          reshape2_1.4.1      ggplot2_1.0.1
[21] xts_0.9-7           zoo_1.7-12          XLConnect_0.2-11    XLConnectJars_0.2-9 timeDate_3012.100
[26] R2HTML_2.3.1        RODBC_1.3-12        quadprog_1.5-5      prettyR_2.1-1       MASS_7.3-42
[31] fortunes_1.5-2      corpcor_1.6.8       manipulate_1.0.1

loaded via a namespace (and not attached):
 [1] rJava_0.9-6      lattice_0.20-31  tcltk_3.2.1      colorspace_1.2-6 htmltools_0.2.6
 [6] yaml_2.1.13      base64enc_0.1-2  chron_2.3-47     stringr_1.0.0    munsell_0.4.2
[11] gtable_0.1.2     memoise_0.2.1    evaluate_0.7     knitr_1.10.5     parallel_3.2.1
[16] curl_0.9.1       highr_0.5        scales_0.2.5     rversions_1.0.2  digest_0.6.8
[21] stringi_0.5-5    grid_3.2.1       magrittr_1.5     crayon_1.3.1     xml2_0.1.1
[26] assertthat_0.1   R6_2.1.0         git2r_0.10.1







***************************************************************
This message and any attachments are for the intended recipient's use only.
This message may contain confidential, proprietary or legally privileged
information. No right to confidential or privileged treatment
of this message is waived or lost by an error in transmission.
If you have received this message in error, please immediately
notify the sender by e-mail, delete the message, any attachments and all
copies from your system and destroy any hard copies.  You must
not, directly or indirectly, use, disclose, distribute,
print or copy any part of this message or any attachments if you are not
the intended recipient.


From joanne.ingram at ed.ac.uk  Tue Jul 14 16:52:49 2015
From: joanne.ingram at ed.ac.uk (INGRAM Joanne)
Date: Tue, 14 Jul 2015 14:52:49 +0000
Subject: [R] Plot in Rcmdr
Message-ID: <AMSPR05MB4068A1FBC16F6CCDD9FDDFFD69B0@AMSPR05MB406.eurprd05.prod.outlook.com>

Hello,

I wondered if anyone could help me with a small issue in Rcmdr.  

I have used the 'Graphs' function in the drop-down menu to create a scatterplot for groups (gender).  But when I do this the legend (telling me the symbols which represent male etc.) keeps obscuring the title of the plot.  Does anyone know how to fix this problem - within Rcmdr?

Please note I am not looking for help with creating the graph in another way (for example in R).  I am specifically trying to figure out if this can be fixed in Rcmdr.  If the answer is "No - this cannot currently be changed within Rcmdr" I would still like to hear from you.

Many thanks for any help.

Joanne Ingram
Research Associate (Medical Statistics)
Centre for Population Health Science
University of Edinburgh

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From dusa.adrian at unibuc.ro  Tue Jul 14 18:45:48 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Tue, 14 Jul 2015 19:45:48 +0300
Subject: [R] open connection to system
Message-ID: <CAJ=0CtDSPKbQs9xJwWevaAkfAtr73bWqsbMS7SDu0TteQOau8A@mail.gmail.com>

Dear list,

Probably not the best subject line, but hopefully I can explain.
I would like to use R and open a connection to a (system) command line base
chess engine
(for example, there is an open source one at stockfishchess.org)

In the Terminal window (using MacOS), I can type two commands:

$ ./stockfish-6-64     <-- this is the first command
Stockfish 6 64 by Tord Romstad, Marco Costalba and Joona Kiiski
go movetime 3000     <-- this is the second command

(then lots of lines calculated by the engine, with a final answer after 3
seconds)

First command opens a connection to the chess engine, the seconds one tells
it to search for a move.
The question is, can I do this via R?

I tried the system() command, which works with the first command:
> system("./stockfish-6-64", intern=TRUE)
[1] "Stockfish 6 64 by Tord Romstad, Marco Costalba and Joona Kiiski"

but it closes the connection and returns an error if I attempt the second
command:

> system("./stockfish-6-64\ngo movetime 3000", intern=TRUE)
Error in system("./stockfish-6-64\ngo movetime 3000", intern = TRUE) :
  error in running command
sh: line 1: go: command not found


Any hint would be really appreciated, thanks in advance,
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From roger.bos at rothschild.com  Tue Jul 14 19:09:36 2015
From: roger.bos at rothschild.com (Bos, Roger)
Date: Tue, 14 Jul 2015 17:09:36 +0000
Subject: [R] advice on making HTML tables
In-Reply-To: <0765308CD028654885F30322557308D81EEF3BF2@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D81EEF3BF2@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <0765308CD028654885F30322557308D81EEF3F1A@NYCSM0208.rth.ad.rothschild.com>

Answering my own question, I was able to make the tables look better in IE using some simple CSS:

td { 
    padding: 6px;
}


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Bos, Roger
Sent: Tuesday, July 14, 2015 11:36 AM
To: r-help at r-project.org
Subject: [R] advice on making HTML tables

This might be a little off topic, but I am starting to produce some HTML reports that contain mostly tables and they look great in Chrome but really bad in IE, so I wanted to see if anyone knows of a better way or an easy fix.  One option I have used is to convert to PDF, but sometimes it is nice to have the report in HTML format.

For my reproducible example, copy the text below into R Studio and hit the knit button. If you look at the HTML output in Chrome the columns are nicely spread out and in IE the columns are jammed right next to each other with minimal/no spacing.  Maybe there is a CSS fix?

---
title: "Untitled"
output: html_document
---

```{r}
knitr::kable(cars)
```

Here is my session info:

R version 3.2.1 Patched (2015-07-11 r68646)
Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252 [3] LC_MONETARY=English_United States.1252 LC_NUMERIC=C [5] LC_TIME=English_United States.1252

attached base packages:
[1] datasets  tools     utils     stats     graphics  grDevices methods   base

other attached packages:
 [1] xtable_1.7-4        sqldf_0.4-10        RSQLite_1.0.0       DBI_0.3.1           gsubfn_0.6-6
 [6] proto_0.3-10        Rcpp_0.11.6         Quandl_2.6.0        testthat_0.10.0     lubridate_1.3.3
[11] sendmailR_1.2-1     rmarkdown_0.7       devtools_1.8.0      data.table_1.9.4    Rpad_1.3.0
[16] formatR_1.2         dplyr_0.4.2.9002    plyr_1.8.3          reshape2_1.4.1      ggplot2_1.0.1
[21] xts_0.9-7           zoo_1.7-12          XLConnect_0.2-11    XLConnectJars_0.2-9 timeDate_3012.100
[26] R2HTML_2.3.1        RODBC_1.3-12        quadprog_1.5-5      prettyR_2.1-1       MASS_7.3-42
[31] fortunes_1.5-2      corpcor_1.6.8       manipulate_1.0.1

loaded via a namespace (and not attached):
 [1] rJava_0.9-6      lattice_0.20-31  tcltk_3.2.1      colorspace_1.2-6 htmltools_0.2.6
 [6] yaml_2.1.13      base64enc_0.1-2  chron_2.3-47     stringr_1.0.0    munsell_0.4.2
[11] gtable_0.1.2     memoise_0.2.1    evaluate_0.7     knitr_1.10.5     parallel_3.2.1
[16] curl_0.9.1       highr_0.5        scales_0.2.5     rversions_1.0.2  digest_0.6.8
[21] stringi_0.5-5    grid_3.2.1       magrittr_1.5     crayon_1.3.1     xml2_0.1.1
[26] assertthat_0.1   R6_2.1.0         git2r_0.10.1







***************************************************************
This message and any attachments are for the intended recipient's use only.
This message may contain confidential, proprietary or legally privileged information. No right to confidential or privileged treatment of this message is waived or lost by an error in transmission.
If you have received this message in error, please immediately notify the sender by e-mail, delete the message, any attachments and all copies from your system and destroy any hard copies.  You must not, directly or indirectly, use, disclose, distribute, print or copy any part of this message or any attachments if you are not the intended recipient.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Tue Jul 14 19:17:06 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 14 Jul 2015 17:17:06 +0000
Subject: [R] Plot in Rcmdr
In-Reply-To: <AMSPR05MB4068A1FBC16F6CCDD9FDDFFD69B0@AMSPR05MB406.eurprd05.prod.outlook.com>
References: <AMSPR05MB4068A1FBC16F6CCDD9FDDFFD69B0@AMSPR05MB406.eurprd05.prod.outlook.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6A304F@mb02.ads.tamu.edu>

It can be changed by slightly modifying the scatterplot() command in the R Script window and re-submitting it.

>From the top menu select Data | Data in packages | Read data set from an attached package. Then type Pottery in the space next to "Enter name of data set" (notice that Pottery is capitalized). 

>From the top menu select Graphs | Scatterplot and then select Al as the x-variable and Ca as the y-variable. Click on Plot by groups... and select Site (and unselect Plot lines by group). Click OK and OK again to produce the plot. The legend is outside the plot region and the top margin has been expanded to make room for it.

In the R Script window you will see the command:

scatterplot(Ca~Al | Site, reg.line=lm, smooth=TRUE, spread=TRUE, 
  id.method='mahal', id.n = 2, boxplots='xy', span=0.5, by.groups=FALSE, 
  data=Pottery)

add a single argument to the end of the command so that it looks like this:

scatterplot(Ca~Al | Site, reg.line=lm, smooth=TRUE, spread=TRUE, 
  id.method='mahal', id.n = 2, boxplots='xy', span=0.5, by.groups=FALSE, 
  data=Pottery, legend.coords="topright")

Then select all three lines and click Submit:

The new plot puts the legend in the upper right corner of the plot region. R Commander uses the scatterplot() function from package ca to create the plot. It has several options that are not included on the options dialog window in R Commander, but can be accessed simply by editing the command that R Commander creates.

To see these options type

?scatterplot

On an empty line in the R Script window, put the cursor on the line and click Submit. This will open your web browser with the manual page for scatterplot.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of INGRAM Joanne
Sent: Tuesday, July 14, 2015 9:53 AM
To: r-help at R-project.org
Subject: [R] Plot in Rcmdr

Hello,

I wondered if anyone could help me with a small issue in Rcmdr.  

I have used the 'Graphs' function in the drop-down menu to create a scatterplot for groups (gender).  But when I do this the legend (telling me the symbols which represent male etc.) keeps obscuring the title of the plot.  Does anyone know how to fix this problem - within Rcmdr?

Please note I am not looking for help with creating the graph in another way (for example in R).  I am specifically trying to figure out if this can be fixed in Rcmdr.  If the answer is "No - this cannot currently be changed within Rcmdr" I would still like to hear from you.

Many thanks for any help.

Joanne Ingram
Research Associate (Medical Statistics)
Centre for Population Health Science
University of Edinburgh

-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From devazresearch at gmail.com  Tue Jul 14 19:18:28 2015
From: devazresearch at gmail.com (DzR)
Date: Tue, 14 Jul 2015 17:18:28 -0000
Subject: [R] Spline curve
Message-ID: <55a5445e.c511460a.0cfe.4900@mx.google.com>

I wish to fit spline curves to longitudinal data

Which package should I use and how should data be structured to facilitate the analysis

-----
Deva

From john.archie.mckown at gmail.com  Tue Jul 14 19:48:03 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Tue, 14 Jul 2015 12:48:03 -0500
Subject: [R] open connection to system
In-Reply-To: <CAJ=0CtDSPKbQs9xJwWevaAkfAtr73bWqsbMS7SDu0TteQOau8A@mail.gmail.com>
References: <CAJ=0CtDSPKbQs9xJwWevaAkfAtr73bWqsbMS7SDu0TteQOau8A@mail.gmail.com>
Message-ID: <CAAJSdjikO2YPtpZHMYex-qYKemvA9=Mz522P+1VbO067=mrgSw@mail.gmail.com>

On Tue, Jul 14, 2015 at 11:45 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:

> Dear list,
>
> Probably not the best subject line, but hopefully I can explain.
> I would like to use R and open a connection to a (system) command line base
> chess engine
> (for example, there is an open source one at stockfishchess.org)
>
> In the Terminal window (using MacOS), I can type two commands:
>
> $ ./stockfish-6-64     <-- this is the first command
> Stockfish 6 64 by Tord Romstad, Marco Costalba and Joona Kiiski
> go movetime 3000     <-- this is the second command
>
> (then lots of lines calculated by the engine, with a final answer after 3
> seconds)
>
> First command opens a connection to the chess engine, the seconds one tells
> it to search for a move.
> The question is, can I do this via R?
>
> I tried the system() command, which works with the first command:
> > system("./stockfish-6-64", intern=TRUE)
> [1] "Stockfish 6 64 by Tord Romstad, Marco Costalba and Joona Kiiski"
>
> but it closes the connection and returns an error if I attempt the second
> command:
>
> > system("./stockfish-6-64\ngo movetime 3000", intern=TRUE)
> Error in system("./stockfish-6-64\ngo movetime 3000", intern = TRUE) :
>   error in running command
> sh: line 1: go: command not found
>
>
> Any hint would be really appreciated, thanks in advance,
> Adrian
>
> --
> Adrian Dusa
> University of Bucharest
>

What system() does is run a command & wait for it to end. ?I take it you
are running on a Mac. Do you want to send multiple command to "stockfish",
or only one command? If the latter, you can do something like:

commands=c("go movetime 3000");
system("./stockfish-6-64",intern=TRUE,input=commands);

?If you want to send a number of commands, and not "interact" with the
"stockfish" command, you can:

commands=c("first command","second command"); # and so on
system("./stockfish-6-64",intern=TRUE,input=commands);

But if you want to "interact" with "stockfish", that's much more difficult
and I don't have any example available. I _think_ you'd need to look at
using mcparallel() and the "parallel" package. Or maybe the
socketConnection() function in some way.                                 ?



-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Tue Jul 14 19:51:30 2015
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 14 Jul 2015 13:51:30 -0400
Subject: [R] Plot in Rcmdr
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6A304F@mb02.ads.tamu.edu>
References: <AMSPR05MB4068A1FBC16F6CCDD9FDDFFD69B0@AMSPR05MB406.eurprd05.prod.outlook.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6A304F@mb02.ads.tamu.edu>
Message-ID: <001201d0be5d$b712c140$253843c0$@mcmaster.ca>

Dear David and Joanne,

David, thank you for answering Joanne's question before I saw it.

The help page for car::scatterplot() is also accessible via the Help button
in the Rcmdr scatterplot dialog.

I'll think about whether to add a control for legend position to the
scatterplot dialog. There are already some enhancements to the dialog in the
forthcoming version 2.2-0 of the Rcmdr package, due late this summer, but I
try not to make the dialogs too complicated.

Best,
 John

-----------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.socsci.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of David L
> Carlson
> Sent: July-14-15 1:17 PM
> To: INGRAM Joanne; r-help at R-project.org
> Subject: Re: [R] Plot in Rcmdr
> 
> It can be changed by slightly modifying the scatterplot() command in the
> R Script window and re-submitting it.
> 
> >From the top menu select Data | Data in packages | Read data set from
> an attached package. Then type Pottery in the space next to "Enter name
> of data set" (notice that Pottery is capitalized).
> 
> >From the top menu select Graphs | Scatterplot and then select Al as the
> x-variable and Ca as the y-variable. Click on Plot by groups... and
> select Site (and unselect Plot lines by group). Click OK and OK again to
> produce the plot. The legend is outside the plot region and the top
> margin has been expanded to make room for it.
> 
> In the R Script window you will see the command:
> 
> scatterplot(Ca~Al | Site, reg.line=lm, smooth=TRUE, spread=TRUE,
>   id.method='mahal', id.n = 2, boxplots='xy', span=0.5, by.groups=FALSE,
>   data=Pottery)
> 
> add a single argument to the end of the command so that it looks like
> this:
> 
> scatterplot(Ca~Al | Site, reg.line=lm, smooth=TRUE, spread=TRUE,
>   id.method='mahal', id.n = 2, boxplots='xy', span=0.5, by.groups=FALSE,
>   data=Pottery, legend.coords="topright")
> 
> Then select all three lines and click Submit:
> 
> The new plot puts the legend in the upper right corner of the plot
> region. R Commander uses the scatterplot() function from package ca to
> create the plot. It has several options that are not included on the
> options dialog window in R Commander, but can be accessed simply by
> editing the command that R Commander creates.
> 
> To see these options type
> 
> ?scatterplot
> 
> On an empty line in the R Script window, put the cursor on the line and
> click Submit. This will open your web browser with the manual page for
> scatterplot.
> 
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
> 
> 
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of INGRAM
> Joanne
> Sent: Tuesday, July 14, 2015 9:53 AM
> To: r-help at R-project.org
> Subject: [R] Plot in Rcmdr
> 
> Hello,
> 
> I wondered if anyone could help me with a small issue in Rcmdr.
> 
> I have used the 'Graphs' function in the drop-down menu to create a
> scatterplot for groups (gender).  But when I do this the legend (telling
> me the symbols which represent male etc.) keeps obscuring the title of
> the plot.  Does anyone know how to fix this problem - within Rcmdr?
> 
> Please note I am not looking for help with creating the graph in another
> way (for example in R).  I am specifically trying to figure out if this
> can be fixed in Rcmdr.  If the answer is "No - this cannot currently be
> changed within Rcmdr" I would still like to hear from you.
> 
> Many thanks for any help.
> 
> Joanne Ingram
> Research Associate (Medical Statistics)
> Centre for Population Health Science
> University of Edinburgh
> 
> --
> The University of Edinburgh is a charitable body, registered in
> Scotland, with registration number SC005336.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From bgunter.4567 at gmail.com  Tue Jul 14 19:52:52 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 14 Jul 2015 10:52:52 -0700
Subject: [R] Spline curve
In-Reply-To: <55a5445e.c511460a.0cfe.4900@mx.google.com>
References: <55a5445e.c511460a.0cfe.4900@mx.google.com>
Message-ID: <CAGxFJbQi4vrB2UKKec=Z_vOqKeZje7DejErAnbNT9WqTcPA7HA@mail.gmail.com>

Yikes! That almost requires a book. Why don't you start by doing some
of your own homework instead. Search on:

fit spline curves to longitudinal data in R

and then go through the tutorials/presentations that pop up. There
will also be books that you will find, I'm sure if you're serious
about this.


Cheers,

Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sun, Jun 28, 2015 at 9:56 PM, DzR <devazresearch at gmail.com> wrote:
> I wish to fit spline curves to longitudinal data
>
> Which package should I use and how should data be structured to facilitate the analysis
>
> -----
> Deva
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From fosulli at tcd.ie  Tue Jul 14 18:42:48 2015
From: fosulli at tcd.ie (Fosulli)
Date: Tue, 14 Jul 2015 09:42:48 -0700 (PDT)
Subject: [R] Altering Forest plot in Metafor package
Message-ID: <1436892168176-4709857.post@n4.nabble.com>

Dear All, 

I'm having trouble tweaking a forest plot made using the R meta-analysis
package metafor. My main problem is that I have two studies which have very
large Confidence intervals and as such my forest plot is very wide, and not
neat. As I would like to add more descriptive columns into the plot too, I
was wondering if there was a way to cut the confidence interval in the graph
and add arrows to suggest that it continues on, while keeping the OR values
correct so that the reader can view the CI clearly.  
<http://r.789695.n4.nabble.com/file/n4709857/SNIP.png>  I hope I am clear in
what I am asking, but here is an example of what I am hoping is possible in
Metafor  <http://r.789695.n4.nabble.com/file/n4709857/arrows.png> 





--
View this message in context: http://r.789695.n4.nabble.com/Altering-Forest-plot-in-Metafor-package-tp4709857.html
Sent from the R help mailing list archive at Nabble.com.


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Jul 14 21:52:40 2015
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 14 Jul 2015 21:52:40 +0200
Subject: [R] Altering Forest plot in Metafor package
In-Reply-To: <1436892168176-4709857.post@n4.nabble.com>
References: <1436892168176-4709857.post@n4.nabble.com>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730F16F7D7BC@UM-MAIL4112.unimaas.nl>

Use the 'alim' argument (or the 'at' argument) to restrict the axis limits, so that CI bounds below/above are indicated with an arrow. And play around with the 'xlim' argument to make better use of the space in the plotting region. And the 'ilab' argument allows you to add columns with additional information to the plot. Please read help(forest.rma) carefully and especially try out all of the examples. They illustrate the use of these arguments.

Best,
Wolfgang
________________________________________
From: R-help [r-help-bounces at r-project.org] On Behalf Of Fosulli [fosulli at tcd.ie]
Sent: Tuesday, July 14, 2015 6:42 PM
To: r-help at r-project.org
Subject: [R] Altering Forest plot in Metafor package

Dear All,

I'm having trouble tweaking a forest plot made using the R meta-analysis
package metafor. My main problem is that I have two studies which have very
large Confidence intervals and as such my forest plot is very wide, and not
neat. As I would like to add more descriptive columns into the plot too, I
was wondering if there was a way to cut the confidence interval in the graph
and add arrows to suggest that it continues on, while keeping the OR values
correct so that the reader can view the CI clearly.
<http://r.789695.n4.nabble.com/file/n4709857/SNIP.png>  I hope I am clear in
what I am asking, but here is an example of what I am hoping is possible in
Metafor  http://r.789695.n4.nabble.com/file/n4709857/arrows.png

--
View this message in context: http://r.789695.n4.nabble.com/Altering-Forest-plot-in-Metafor-package-tp4709857.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From jdnewmil at dcn.davis.CA.us  Wed Jul 15 00:48:08 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 14 Jul 2015 15:48:08 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
	<CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
Message-ID: <5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>

I suspect your data frame "dat" has non-numeric data in some of the columns that have ABC in their names. Any column of a data frame can be numeric or not, but the data frame as a unit cannot be numeric. If your data file has odd characters in done of the otherwise-numeric columns, the whole column will be read in as a factor or character strings. Look at the output of str(dat) for columns that don't show "num'. If you can find the column, and then one of the bad rows, you can use a text editor to fix them manually, or show us examples of the bad data and we can suggest ways to fix it in R.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 14, 2015 2:35:38 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>Hi,
>
>I used a small set of data (several columns and rows) and it works fine
>using the following command:
>abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)
>
>But when I used the real big data table, "Error in rowSums(dat[,
>grep("ABC", names(dat), fixed = T)], na.rm = T) :
>  'x' must be numeric"
>Then it didn't work either using as.numeric():
>> as.numeric(dat)
>Error: (list) object cannot be coerced to type 'double'
>
>Thanks!
>Dawn
>
>
>
>
>On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:
>
>> Thank you all and sorry for the data messing. It has worked!
>>
>> Best,
>> Dawn
>>
>> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com>
>wrote:
>>
>>> Hi Dawn,
>>> Your data are a bit messed up, but try the following:
>>>
>>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
>>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
>>>
>>> I'm assuming that you want to discard the NA values.
>>>
>>> Jim
>>>
>>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas <ruipbarradas at sapo.pt>
>>> wrote:
>>> > Hello,
>>> >
>>> > Please use ?dput to give a data example, like this it's completely
>>> > unreadable. If your data.frame is named 'dat' use
>>> >
>>> > dput(head(dat, 30))  # paste the outut of this in your mail
>>> >
>>> >
>>> > And don't post in html, use plain text only, like the posting
>guide
>>> says.
>>> >
>>> > Rui Barradas
>>> >
>>> >
>>> > Em 09-07-2015 18:12, Dawn escreveu:
>>> >>
>>> >> Hi,
>>> >>
>>> >> I have a big dataframe as follows
>>> >>
>>> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC   
>25ABC
>>> >> 25XYZ
>>> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC
>>> 36SUR
>>> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM   
>42SUR
>>> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC   
>66XYZ
>>> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ   
>76ABC
>>> >> 76XYZ    82ABC    85ABC    POV
>>> >> Cluster_1                                                       
>17
>>> 1
>>> >> 3    10    14    5    2    2        1    1    1    2
>>> >>                          2                            TT:61
>>> >> Cluster_2                    1                                4  
> 20
>>> >> 6    5    3    6    9    9    6        10        1    3    1
>>> >>                              4                            TT:88
>>> >> Cluster_3    3        3                            6        4    
>   17
>>> >> 17    18    13    17    19    22    11    5    21    8    5    18
>   4
>>> >> 7                                        9
>>> >> TT:227
>>> >> ........
>>> >>
>>> >> I want to get two columns, i.e,  one is to sum columns for all
>>> including
>>> >> ABC for each row and the other is  to sum columns for all
>including XYZ
>>> >> for
>>> >> each row.
>>> >>
>>> >> Is there some help? Thank you!
>>> >> Dawn
>>> >>
>>> >>         [[alternative HTML version deleted]]
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> >> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible
>code.
>>> >>
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>


From jdnewmil at dcn.davis.CA.us  Wed Jul 15 01:36:57 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 14 Jul 2015 16:36:57 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <CABtBq8FCpLMoGO3ZRRcOL+PZqMx04=nnshpNf5SEUcozFW89KA@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
	<CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
	<5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>
	<CABtBq8FCpLMoGO3ZRRcOL+PZqMx04=nnshpNf5SEUcozFW89KA@mail.gmail.com>
Message-ID: <ACDF5AA3-79B3-4AFC-95A1-25D669CDBB15@dcn.davis.CA.us>

Well it is pretty obvious that all of your columns have non-numeric data in them, but you are the only one who can tell which ones should have been numeric, and you are also the one who can peruse your data file in a text editor.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 14, 2015 4:05:37 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>I used two rows to test the data frame, as follows.
>
>> dat <- read.table("TOV_43_Protein_Clusters_abundance1.tab",
>header=TRUE,sep = "\t")
>> dat1 <- dat[1:2,]
>> str(dat1)
>'data.frame':    2 obs. of  44 variables:
>$ X      : Factor w/ 1075762 levels "","POV_Cluster_1000001",..: 305266
>625028
> $ X109DCM: Factor w/ 46 levels "","1","10","109DCM",..: 1 1
> $ X109SUR: Factor w/ 41 levels "","1","10","109SUR",..: 1 1
> $ X18DCM : Factor w/ 31 levels "","1","10","11",..: 1 1
> $ X18SUR : Factor w/ 25 levels "","1","10","11",..: 1 1
> $ X22SUR : Factor w/ 50 levels "","1","10","11",..: 1 2
> $ X23DCM : Factor w/ 46 levels "","1","10","11",..: 1 1
> $ X25DCM : Factor w/ 42 levels "","1","10","11",..: 1 1
> $ X25SUR : Factor w/ 47 levels "","1","10","11",..: 1 1
> $ X30DCM : Factor w/ 34 levels "","1","10","11",..: 1 1
> $ X31SUR : Factor w/ 43 levels "","1","10","11",..: 1 1
> $ X32DCM : Factor w/ 15 levels "","1","10","11",..: 1 1
> $ X32SUR : Factor w/ 58 levels "","1","10","11",..: 1 1
> $ X34DCM : Factor w/ 53 levels "","1","10","11",..: 1 35
> $ X34SUR : Factor w/ 47 levels "","1","10","11",..: 10 14
> $ X36DCM : Factor w/ 48 levels "","1","10","11",..: 2 43
> $ X36SUR : Factor w/ 45 levels "","1","10","11",..: 23 38
> $ X38DCM : Factor w/ 40 levels "","1","10","11",..: 3 23
> $ X38SUR : Factor w/ 44 levels "","1","10","11",..: 7 41
> $ X39DCM : Factor w/ 38 levels "","1","10","11",..: 34 38
> $ X39SUR : Factor w/ 40 levels "","1","10","11",..: 13 40
> $ X41DCM : Factor w/ 47 levels "","1","10","11",..: 13 40
> $ X41SUR : Factor w/ 40 levels "","1","10","11",..: 1 1
> $ X42DCM : Factor w/ 48 levels "","1","10","11",..: 2 3
> $ X42SUR : Factor w/ 41 levels "","1","10","11",..: 2 1
> $ X46SUR : Factor w/ 31 levels "","1","10","11",..: 2 2
> $ X52DCM : Factor w/ 49 levels "","1","10","11",..: 13 23
> $ X64DCM : Factor w/ 35 levels "","1","10","11",..: 1 2
> $ X64SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
> $ X65DCM : Factor w/ 38 levels "","1","10","11",..: 1 1
> $ X65SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
> $ X66DCM : Factor w/ 27 levels "","1","10","11",..: 1 1
> $ X66SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
> $ X67SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
> $ X68DCM : Factor w/ 33 levels "","1","10","11",..: 1 1
> $ X68SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
> $ X70MES : Factor w/ 23 levels "","1","10","11",..: 1 1
> $ X70SUR : Factor w/ 37 levels "","1","10","11",..: 1 1
> $ X72DCM : Factor w/ 40 levels "","1","10","11",..: 13 27
> $ X72SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
> $ X76DCM : Factor w/ 44 levels "","1","10","11",..: 1 1
> $ X76SUR : Factor w/ 34 levels "","1","10","11",..: 1 1
> $ X82DCM : Factor w/ 29 levels "","1","10","11",..: 1 1
> $ X85DCM : Factor w/ 30 levels "","1","10","11",..: 1 1
>
>
>Thank you!!
>Dawn
>
>On Tue, Jul 14, 2015 at 3:48 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> I suspect your data frame "dat" has non-numeric data in some of the
>> columns that have ABC in their names. Any column of a data frame can
>be
>> numeric or not, but the data frame as a unit cannot be numeric. If
>your
>> data file has odd characters in done of the otherwise-numeric
>columns, the
>> whole column will be read in as a factor or character strings. Look
>at the
>> output of str(dat) for columns that don't show "num'. If you can find
>the
>> column, and then one of the bad rows, you can use a text editor to
>fix them
>> manually, or show us examples of the bad data and we can suggest ways
>to
>> fix it in R.
>>
>---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>>
>---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 14, 2015 2:35:38 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>> >Hi,
>> >
>> >I used a small set of data (several columns and rows) and it works
>fine
>> >using the following command:
>> >abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)
>> >
>> >But when I used the real big data table, "Error in rowSums(dat[,
>> >grep("ABC", names(dat), fixed = T)], na.rm = T) :
>> >  'x' must be numeric"
>> >Then it didn't work either using as.numeric():
>> >> as.numeric(dat)
>> >Error: (list) object cannot be coerced to type 'double'
>> >
>> >Thanks!
>> >Dawn
>> >
>> >
>> >
>> >
>> >On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:
>> >
>> >> Thank you all and sorry for the data messing. It has worked!
>> >>
>> >> Best,
>> >> Dawn
>> >>
>> >> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com>
>> >wrote:
>> >>
>> >>> Hi Dawn,
>> >>> Your data are a bit messed up, but try the following:
>> >>>
>> >>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
>> >>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
>> >>>
>> >>> I'm assuming that you want to discard the NA values.
>> >>>
>> >>> Jim
>> >>>
>> >>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas
><ruipbarradas at sapo.pt>
>> >>> wrote:
>> >>> > Hello,
>> >>> >
>> >>> > Please use ?dput to give a data example, like this it's
>completely
>> >>> > unreadable. If your data.frame is named 'dat' use
>> >>> >
>> >>> > dput(head(dat, 30))  # paste the outut of this in your mail
>> >>> >
>> >>> >
>> >>> > And don't post in html, use plain text only, like the posting
>> >guide
>> >>> says.
>> >>> >
>> >>> > Rui Barradas
>> >>> >
>> >>> >
>> >>> > Em 09-07-2015 18:12, Dawn escreveu:
>> >>> >>
>> >>> >> Hi,
>> >>> >>
>> >>> >> I have a big dataframe as follows
>> >>> >>
>> >>> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC
>> >25ABC
>> >>> >> 25XYZ
>> >>> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ   
>36ABC
>> >>> 36SUR
>> >>> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM
>> >42SUR
>> >>> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC
>> >66XYZ
>> >>> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ
>> >76ABC
>> >>> >> 76XYZ    82ABC    85ABC    POV
>> >>> >> Cluster_1
>> >17
>> >>> 1
>> >>> >> 3    10    14    5    2    2        1    1    1    2
>> >>> >>                          2                            TT:61
>> >>> >> Cluster_2                    1                               
>4
>> > 20
>> >>> >> 6    5    3    6    9    9    6        10        1    3    1
>> >>> >>                              4                           
>TT:88
>> >>> >> Cluster_3    3        3                            6        4
>> >   17
>> >>> >> 17    18    13    17    19    22    11    5    21    8    5   
>18
>> >   4
>> >>> >> 7                                        9
>> >>> >> TT:227
>> >>> >> ........
>> >>> >>
>> >>> >> I want to get two columns, i.e,  one is to sum columns for all
>> >>> including
>> >>> >> ABC for each row and the other is  to sum columns for all
>> >including XYZ
>> >>> >> for
>> >>> >> each row.
>> >>> >>
>> >>> >> Is there some help? Thank you!
>> >>> >> Dawn
>> >>> >>
>> >>> >>         [[alternative HTML version deleted]]
>> >>> >>
>> >>> >> ______________________________________________
>> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> >> PLEASE do read the posting guide
>> >>> >> http://www.R-project.org/posting-guide.html
>> >>> >> and provide commented, minimal, self-contained, reproducible
>> >code.
>> >>> >>
>> >>> >
>> >>> > ______________________________________________
>> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>see
>> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> > PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> > and provide commented, minimal, self-contained, reproducible
>code.
>> >>>
>> >>
>> >>
>>
>>


From bgunter.4567 at gmail.com  Wed Jul 15 01:44:43 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 14 Jul 2015 16:44:43 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <ACDF5AA3-79B3-4AFC-95A1-25D669CDBB15@dcn.davis.CA.us>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
	<CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
	<5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>
	<CABtBq8FCpLMoGO3ZRRcOL+PZqMx04=nnshpNf5SEUcozFW89KA@mail.gmail.com>
	<ACDF5AA3-79B3-4AFC-95A1-25D669CDBB15@dcn.davis.CA.us>
Message-ID: <CAGxFJbRaRWwAsfVfO64sPQd35CrcrNhsBe-9DupCUEQAhshJXw@mail.gmail.com>

It seems that Dawn could really benefit from spending some time with
an online R tutorial or two, as she appears not to have much of a clue
about R's basic data structures.

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Jul 14, 2015 at 4:36 PM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> Well it is pretty obvious that all of your columns have non-numeric data in them, but you are the only one who can tell which ones should have been numeric, and you are also the one who can peruse your data file in a text editor.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 14, 2015 4:05:37 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>>I used two rows to test the data frame, as follows.
>>
>>> dat <- read.table("TOV_43_Protein_Clusters_abundance1.tab",
>>header=TRUE,sep = "\t")
>>> dat1 <- dat[1:2,]
>>> str(dat1)
>>'data.frame':    2 obs. of  44 variables:
>>$ X      : Factor w/ 1075762 levels "","POV_Cluster_1000001",..: 305266
>>625028
>> $ X109DCM: Factor w/ 46 levels "","1","10","109DCM",..: 1 1
>> $ X109SUR: Factor w/ 41 levels "","1","10","109SUR",..: 1 1
>> $ X18DCM : Factor w/ 31 levels "","1","10","11",..: 1 1
>> $ X18SUR : Factor w/ 25 levels "","1","10","11",..: 1 1
>> $ X22SUR : Factor w/ 50 levels "","1","10","11",..: 1 2
>> $ X23DCM : Factor w/ 46 levels "","1","10","11",..: 1 1
>> $ X25DCM : Factor w/ 42 levels "","1","10","11",..: 1 1
>> $ X25SUR : Factor w/ 47 levels "","1","10","11",..: 1 1
>> $ X30DCM : Factor w/ 34 levels "","1","10","11",..: 1 1
>> $ X31SUR : Factor w/ 43 levels "","1","10","11",..: 1 1
>> $ X32DCM : Factor w/ 15 levels "","1","10","11",..: 1 1
>> $ X32SUR : Factor w/ 58 levels "","1","10","11",..: 1 1
>> $ X34DCM : Factor w/ 53 levels "","1","10","11",..: 1 35
>> $ X34SUR : Factor w/ 47 levels "","1","10","11",..: 10 14
>> $ X36DCM : Factor w/ 48 levels "","1","10","11",..: 2 43
>> $ X36SUR : Factor w/ 45 levels "","1","10","11",..: 23 38
>> $ X38DCM : Factor w/ 40 levels "","1","10","11",..: 3 23
>> $ X38SUR : Factor w/ 44 levels "","1","10","11",..: 7 41
>> $ X39DCM : Factor w/ 38 levels "","1","10","11",..: 34 38
>> $ X39SUR : Factor w/ 40 levels "","1","10","11",..: 13 40
>> $ X41DCM : Factor w/ 47 levels "","1","10","11",..: 13 40
>> $ X41SUR : Factor w/ 40 levels "","1","10","11",..: 1 1
>> $ X42DCM : Factor w/ 48 levels "","1","10","11",..: 2 3
>> $ X42SUR : Factor w/ 41 levels "","1","10","11",..: 2 1
>> $ X46SUR : Factor w/ 31 levels "","1","10","11",..: 2 2
>> $ X52DCM : Factor w/ 49 levels "","1","10","11",..: 13 23
>> $ X64DCM : Factor w/ 35 levels "","1","10","11",..: 1 2
>> $ X64SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
>> $ X65DCM : Factor w/ 38 levels "","1","10","11",..: 1 1
>> $ X65SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
>> $ X66DCM : Factor w/ 27 levels "","1","10","11",..: 1 1
>> $ X66SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
>> $ X67SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
>> $ X68DCM : Factor w/ 33 levels "","1","10","11",..: 1 1
>> $ X68SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
>> $ X70MES : Factor w/ 23 levels "","1","10","11",..: 1 1
>> $ X70SUR : Factor w/ 37 levels "","1","10","11",..: 1 1
>> $ X72DCM : Factor w/ 40 levels "","1","10","11",..: 13 27
>> $ X72SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
>> $ X76DCM : Factor w/ 44 levels "","1","10","11",..: 1 1
>> $ X76SUR : Factor w/ 34 levels "","1","10","11",..: 1 1
>> $ X82DCM : Factor w/ 29 levels "","1","10","11",..: 1 1
>> $ X85DCM : Factor w/ 30 levels "","1","10","11",..: 1 1
>>
>>
>>Thank you!!
>>Dawn
>>
>>On Tue, Jul 14, 2015 at 3:48 PM, Jeff Newmiller
>><jdnewmil at dcn.davis.ca.us>
>>wrote:
>>
>>> I suspect your data frame "dat" has non-numeric data in some of the
>>> columns that have ABC in their names. Any column of a data frame can
>>be
>>> numeric or not, but the data frame as a unit cannot be numeric. If
>>your
>>> data file has odd characters in done of the otherwise-numeric
>>columns, the
>>> whole column will be read in as a factor or character strings. Look
>>at the
>>> output of str(dat) for columns that don't show "num'. If you can find
>>the
>>> column, and then one of the bad rows, you can use a text editor to
>>fix them
>>> manually, or show us examples of the bad data and we can suggest ways
>>to
>>> fix it in R.
>>>
>>---------------------------------------------------------------------------
>>> Jeff Newmiller                        The     .....       .....  Go
>>Live...
>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>> Go...
>>>                                       Live:   OO#.. Dead: OO#..
>>Playing
>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>rocks...1k
>>>
>>---------------------------------------------------------------------------
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On July 14, 2015 2:35:38 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>>> >Hi,
>>> >
>>> >I used a small set of data (several columns and rows) and it works
>>fine
>>> >using the following command:
>>> >abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)
>>> >
>>> >But when I used the real big data table, "Error in rowSums(dat[,
>>> >grep("ABC", names(dat), fixed = T)], na.rm = T) :
>>> >  'x' must be numeric"
>>> >Then it didn't work either using as.numeric():
>>> >> as.numeric(dat)
>>> >Error: (list) object cannot be coerced to type 'double'
>>> >
>>> >Thanks!
>>> >Dawn
>>> >
>>> >
>>> >
>>> >
>>> >On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:
>>> >
>>> >> Thank you all and sorry for the data messing. It has worked!
>>> >>
>>> >> Best,
>>> >> Dawn
>>> >>
>>> >> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com>
>>> >wrote:
>>> >>
>>> >>> Hi Dawn,
>>> >>> Your data are a bit messed up, but try the following:
>>> >>>
>>> >>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
>>> >>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
>>> >>>
>>> >>> I'm assuming that you want to discard the NA values.
>>> >>>
>>> >>> Jim
>>> >>>
>>> >>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas
>><ruipbarradas at sapo.pt>
>>> >>> wrote:
>>> >>> > Hello,
>>> >>> >
>>> >>> > Please use ?dput to give a data example, like this it's
>>completely
>>> >>> > unreadable. If your data.frame is named 'dat' use
>>> >>> >
>>> >>> > dput(head(dat, 30))  # paste the outut of this in your mail
>>> >>> >
>>> >>> >
>>> >>> > And don't post in html, use plain text only, like the posting
>>> >guide
>>> >>> says.
>>> >>> >
>>> >>> > Rui Barradas
>>> >>> >
>>> >>> >
>>> >>> > Em 09-07-2015 18:12, Dawn escreveu:
>>> >>> >>
>>> >>> >> Hi,
>>> >>> >>
>>> >>> >> I have a big dataframe as follows
>>> >>> >>
>>> >>> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC
>>> >25ABC
>>> >>> >> 25XYZ
>>> >>> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ
>>36ABC
>>> >>> 36SUR
>>> >>> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM
>>> >42SUR
>>> >>> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC
>>> >66XYZ
>>> >>> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ
>>> >76ABC
>>> >>> >> 76XYZ    82ABC    85ABC    POV
>>> >>> >> Cluster_1
>>> >17
>>> >>> 1
>>> >>> >> 3    10    14    5    2    2        1    1    1    2
>>> >>> >>                          2                            TT:61
>>> >>> >> Cluster_2                    1
>>4
>>> > 20
>>> >>> >> 6    5    3    6    9    9    6        10        1    3    1
>>> >>> >>                              4
>>TT:88
>>> >>> >> Cluster_3    3        3                            6        4
>>> >   17
>>> >>> >> 17    18    13    17    19    22    11    5    21    8    5
>>18
>>> >   4
>>> >>> >> 7                                        9
>>> >>> >> TT:227
>>> >>> >> ........
>>> >>> >>
>>> >>> >> I want to get two columns, i.e,  one is to sum columns for all
>>> >>> including
>>> >>> >> ABC for each row and the other is  to sum columns for all
>>> >including XYZ
>>> >>> >> for
>>> >>> >> each row.
>>> >>> >>
>>> >>> >> Is there some help? Thank you!
>>> >>> >> Dawn
>>> >>> >>
>>> >>> >>         [[alternative HTML version deleted]]
>>> >>> >>
>>> >>> >> ______________________________________________
>>> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>see
>>> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> >> PLEASE do read the posting guide
>>> >>> >> http://www.R-project.org/posting-guide.html
>>> >>> >> and provide commented, minimal, self-contained, reproducible
>>> >code.
>>> >>> >>
>>> >>> >
>>> >>> > ______________________________________________
>>> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>see
>>> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> >>> > PLEASE do read the posting guide
>>> >>> http://www.R-project.org/posting-guide.html
>>> >>> > and provide commented, minimal, self-contained, reproducible
>>code.
>>> >>>
>>> >>
>>> >>
>>>
>>>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dawn1313 at gmail.com  Tue Jul 14 23:35:38 2015
From: dawn1313 at gmail.com (Dawn)
Date: Tue, 14 Jul 2015 14:35:38 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
Message-ID: <CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>

Hi,

I used a small set of data (several columns and rows) and it works fine
using the following command:
abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)

But when I used the real big data table, "Error in rowSums(dat[,
grep("ABC", names(dat), fixed = T)], na.rm = T) :
  'x' must be numeric"
Then it didn't work either using as.numeric():
> as.numeric(dat)
Error: (list) object cannot be coerced to type 'double'

Thanks!
Dawn




On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:

> Thank you all and sorry for the data messing. It has worked!
>
> Best,
> Dawn
>
> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Dawn,
>> Your data are a bit messed up, but try the following:
>>
>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
>>
>> I'm assuming that you want to discard the NA values.
>>
>> Jim
>>
>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas <ruipbarradas at sapo.pt>
>> wrote:
>> > Hello,
>> >
>> > Please use ?dput to give a data example, like this it's completely
>> > unreadable. If your data.frame is named 'dat' use
>> >
>> > dput(head(dat, 30))  # paste the outut of this in your mail
>> >
>> >
>> > And don't post in html, use plain text only, like the posting guide
>> says.
>> >
>> > Rui Barradas
>> >
>> >
>> > Em 09-07-2015 18:12, Dawn escreveu:
>> >>
>> >> Hi,
>> >>
>> >> I have a big dataframe as follows
>> >>
>> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC    25ABC
>> >> 25XYZ
>> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC
>> 36SUR
>> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM    42SUR
>> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC    66XYZ
>> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ    76ABC
>> >> 76XYZ    82ABC    85ABC    POV
>> >> Cluster_1                                                        17
>> 1
>> >> 3    10    14    5    2    2        1    1    1    2
>> >>                          2                            TT:61
>> >> Cluster_2                    1                                4    20
>> >> 6    5    3    6    9    9    6        10        1    3    1
>> >>                              4                            TT:88
>> >> Cluster_3    3        3                            6        4        17
>> >> 17    18    13    17    19    22    11    5    21    8    5    18    4
>> >> 7                                        9
>> >> TT:227
>> >> ........
>> >>
>> >> I want to get two columns, i.e,  one is to sum columns for all
>> including
>> >> ABC for each row and the other is  to sum columns for all including XYZ
>> >> for
>> >> each row.
>> >>
>> >> Is there some help? Thank you!
>> >> Dawn
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From dawn1313 at gmail.com  Wed Jul 15 01:05:37 2015
From: dawn1313 at gmail.com (Dawn)
Date: Tue, 14 Jul 2015 16:05:37 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
	<CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
	<5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>
Message-ID: <CABtBq8FCpLMoGO3ZRRcOL+PZqMx04=nnshpNf5SEUcozFW89KA@mail.gmail.com>

I used two rows to test the data frame, as follows.

> dat <- read.table("TOV_43_Protein_Clusters_abundance1.tab",
header=TRUE,sep = "\t")
> dat1 <- dat[1:2,]
> str(dat1)
'data.frame':    2 obs. of  44 variables:
 $ X      : Factor w/ 1075762 levels "","POV_Cluster_1000001",..: 305266
625028
 $ X109DCM: Factor w/ 46 levels "","1","10","109DCM",..: 1 1
 $ X109SUR: Factor w/ 41 levels "","1","10","109SUR",..: 1 1
 $ X18DCM : Factor w/ 31 levels "","1","10","11",..: 1 1
 $ X18SUR : Factor w/ 25 levels "","1","10","11",..: 1 1
 $ X22SUR : Factor w/ 50 levels "","1","10","11",..: 1 2
 $ X23DCM : Factor w/ 46 levels "","1","10","11",..: 1 1
 $ X25DCM : Factor w/ 42 levels "","1","10","11",..: 1 1
 $ X25SUR : Factor w/ 47 levels "","1","10","11",..: 1 1
 $ X30DCM : Factor w/ 34 levels "","1","10","11",..: 1 1
 $ X31SUR : Factor w/ 43 levels "","1","10","11",..: 1 1
 $ X32DCM : Factor w/ 15 levels "","1","10","11",..: 1 1
 $ X32SUR : Factor w/ 58 levels "","1","10","11",..: 1 1
 $ X34DCM : Factor w/ 53 levels "","1","10","11",..: 1 35
 $ X34SUR : Factor w/ 47 levels "","1","10","11",..: 10 14
 $ X36DCM : Factor w/ 48 levels "","1","10","11",..: 2 43
 $ X36SUR : Factor w/ 45 levels "","1","10","11",..: 23 38
 $ X38DCM : Factor w/ 40 levels "","1","10","11",..: 3 23
 $ X38SUR : Factor w/ 44 levels "","1","10","11",..: 7 41
 $ X39DCM : Factor w/ 38 levels "","1","10","11",..: 34 38
 $ X39SUR : Factor w/ 40 levels "","1","10","11",..: 13 40
 $ X41DCM : Factor w/ 47 levels "","1","10","11",..: 13 40
 $ X41SUR : Factor w/ 40 levels "","1","10","11",..: 1 1
 $ X42DCM : Factor w/ 48 levels "","1","10","11",..: 2 3
 $ X42SUR : Factor w/ 41 levels "","1","10","11",..: 2 1
 $ X46SUR : Factor w/ 31 levels "","1","10","11",..: 2 2
 $ X52DCM : Factor w/ 49 levels "","1","10","11",..: 13 23
 $ X64DCM : Factor w/ 35 levels "","1","10","11",..: 1 2
 $ X64SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
 $ X65DCM : Factor w/ 38 levels "","1","10","11",..: 1 1
 $ X65SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
 $ X66DCM : Factor w/ 27 levels "","1","10","11",..: 1 1
 $ X66SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
 $ X67SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
 $ X68DCM : Factor w/ 33 levels "","1","10","11",..: 1 1
 $ X68SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
 $ X70MES : Factor w/ 23 levels "","1","10","11",..: 1 1
 $ X70SUR : Factor w/ 37 levels "","1","10","11",..: 1 1
 $ X72DCM : Factor w/ 40 levels "","1","10","11",..: 13 27
 $ X72SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
 $ X76DCM : Factor w/ 44 levels "","1","10","11",..: 1 1
 $ X76SUR : Factor w/ 34 levels "","1","10","11",..: 1 1
 $ X82DCM : Factor w/ 29 levels "","1","10","11",..: 1 1
 $ X85DCM : Factor w/ 30 levels "","1","10","11",..: 1 1


Thank you!!
Dawn

On Tue, Jul 14, 2015 at 3:48 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> I suspect your data frame "dat" has non-numeric data in some of the
> columns that have ABC in their names. Any column of a data frame can be
> numeric or not, but the data frame as a unit cannot be numeric. If your
> data file has odd characters in done of the otherwise-numeric columns, the
> whole column will be read in as a factor or character strings. Look at the
> output of str(dat) for columns that don't show "num'. If you can find the
> column, and then one of the bad rows, you can use a text editor to fix them
> manually, or show us examples of the bad data and we can suggest ways to
> fix it in R.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 14, 2015 2:35:38 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
> >Hi,
> >
> >I used a small set of data (several columns and rows) and it works fine
> >using the following command:
> >abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)
> >
> >But when I used the real big data table, "Error in rowSums(dat[,
> >grep("ABC", names(dat), fixed = T)], na.rm = T) :
> >  'x' must be numeric"
> >Then it didn't work either using as.numeric():
> >> as.numeric(dat)
> >Error: (list) object cannot be coerced to type 'double'
> >
> >Thanks!
> >Dawn
> >
> >
> >
> >
> >On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:
> >
> >> Thank you all and sorry for the data messing. It has worked!
> >>
> >> Best,
> >> Dawn
> >>
> >> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com>
> >wrote:
> >>
> >>> Hi Dawn,
> >>> Your data are a bit messed up, but try the following:
> >>>
> >>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
> >>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
> >>>
> >>> I'm assuming that you want to discard the NA values.
> >>>
> >>> Jim
> >>>
> >>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas <ruipbarradas at sapo.pt>
> >>> wrote:
> >>> > Hello,
> >>> >
> >>> > Please use ?dput to give a data example, like this it's completely
> >>> > unreadable. If your data.frame is named 'dat' use
> >>> >
> >>> > dput(head(dat, 30))  # paste the outut of this in your mail
> >>> >
> >>> >
> >>> > And don't post in html, use plain text only, like the posting
> >guide
> >>> says.
> >>> >
> >>> > Rui Barradas
> >>> >
> >>> >
> >>> > Em 09-07-2015 18:12, Dawn escreveu:
> >>> >>
> >>> >> Hi,
> >>> >>
> >>> >> I have a big dataframe as follows
> >>> >>
> >>> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC
> >25ABC
> >>> >> 25XYZ
> >>> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ    36ABC
> >>> 36SUR
> >>> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM
> >42SUR
> >>> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC
> >66XYZ
> >>> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ
> >76ABC
> >>> >> 76XYZ    82ABC    85ABC    POV
> >>> >> Cluster_1
> >17
> >>> 1
> >>> >> 3    10    14    5    2    2        1    1    1    2
> >>> >>                          2                            TT:61
> >>> >> Cluster_2                    1                                4
> > 20
> >>> >> 6    5    3    6    9    9    6        10        1    3    1
> >>> >>                              4                            TT:88
> >>> >> Cluster_3    3        3                            6        4
> >   17
> >>> >> 17    18    13    17    19    22    11    5    21    8    5    18
> >   4
> >>> >> 7                                        9
> >>> >> TT:227
> >>> >> ........
> >>> >>
> >>> >> I want to get two columns, i.e,  one is to sum columns for all
> >>> including
> >>> >> ABC for each row and the other is  to sum columns for all
> >including XYZ
> >>> >> for
> >>> >> each row.
> >>> >>
> >>> >> Is there some help? Thank you!
> >>> >> Dawn
> >>> >>
> >>> >>         [[alternative HTML version deleted]]
> >>> >>
> >>> >> ______________________________________________
> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> >> PLEASE do read the posting guide
> >>> >> http://www.R-project.org/posting-guide.html
> >>> >> and provide commented, minimal, self-contained, reproducible
> >code.
> >>> >>
> >>> >
> >>> > ______________________________________________
> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>> > PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> > and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>
>
>

	[[alternative HTML version deleted]]


From dawn1313 at gmail.com  Wed Jul 15 01:49:23 2015
From: dawn1313 at gmail.com (Dawn)
Date: Tue, 14 Jul 2015 16:49:23 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <ACDF5AA3-79B3-4AFC-95A1-25D669CDBB15@dcn.davis.CA.us>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
	<CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
	<5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>
	<CABtBq8FCpLMoGO3ZRRcOL+PZqMx04=nnshpNf5SEUcozFW89KA@mail.gmail.com>
	<ACDF5AA3-79B3-4AFC-95A1-25D669CDBB15@dcn.davis.CA.us>
Message-ID: <CABtBq8H8JAzEUZdVt-cdTy4aSMSGEO0rff6bnr9kT_3-qhKrow@mail.gmail.com>

I attached the file including the first two rows and please help to make it
the numeric data frame. Hopefully the following command works:

dcm <- rowSums(dat1[,grep("DCM",names(dat1),fixed=T)],na.rm=T)

Thank you very much!
Dawn

On Tue, Jul 14, 2015 at 4:36 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Well it is pretty obvious that all of your columns have non-numeric data
> in them, but you are the only one who can tell which ones should have been
> numeric, and you are also the one who can peruse your data file in a text
> editor.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 14, 2015 4:05:37 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
> >I used two rows to test the data frame, as follows.
> >
> >> dat <- read.table("TOV_43_Protein_Clusters_abundance1.tab",
> >header=TRUE,sep = "\t")
> >> dat1 <- dat[1:2,]
> >> str(dat1)
> >'data.frame':    2 obs. of  44 variables:
> >$ X      : Factor w/ 1075762 levels "","POV_Cluster_1000001",..: 305266
> >625028
> > $ X109DCM: Factor w/ 46 levels "","1","10","109DCM",..: 1 1
> > $ X109SUR: Factor w/ 41 levels "","1","10","109SUR",..: 1 1
> > $ X18DCM : Factor w/ 31 levels "","1","10","11",..: 1 1
> > $ X18SUR : Factor w/ 25 levels "","1","10","11",..: 1 1
> > $ X22SUR : Factor w/ 50 levels "","1","10","11",..: 1 2
> > $ X23DCM : Factor w/ 46 levels "","1","10","11",..: 1 1
> > $ X25DCM : Factor w/ 42 levels "","1","10","11",..: 1 1
> > $ X25SUR : Factor w/ 47 levels "","1","10","11",..: 1 1
> > $ X30DCM : Factor w/ 34 levels "","1","10","11",..: 1 1
> > $ X31SUR : Factor w/ 43 levels "","1","10","11",..: 1 1
> > $ X32DCM : Factor w/ 15 levels "","1","10","11",..: 1 1
> > $ X32SUR : Factor w/ 58 levels "","1","10","11",..: 1 1
> > $ X34DCM : Factor w/ 53 levels "","1","10","11",..: 1 35
> > $ X34SUR : Factor w/ 47 levels "","1","10","11",..: 10 14
> > $ X36DCM : Factor w/ 48 levels "","1","10","11",..: 2 43
> > $ X36SUR : Factor w/ 45 levels "","1","10","11",..: 23 38
> > $ X38DCM : Factor w/ 40 levels "","1","10","11",..: 3 23
> > $ X38SUR : Factor w/ 44 levels "","1","10","11",..: 7 41
> > $ X39DCM : Factor w/ 38 levels "","1","10","11",..: 34 38
> > $ X39SUR : Factor w/ 40 levels "","1","10","11",..: 13 40
> > $ X41DCM : Factor w/ 47 levels "","1","10","11",..: 13 40
> > $ X41SUR : Factor w/ 40 levels "","1","10","11",..: 1 1
> > $ X42DCM : Factor w/ 48 levels "","1","10","11",..: 2 3
> > $ X42SUR : Factor w/ 41 levels "","1","10","11",..: 2 1
> > $ X46SUR : Factor w/ 31 levels "","1","10","11",..: 2 2
> > $ X52DCM : Factor w/ 49 levels "","1","10","11",..: 13 23
> > $ X64DCM : Factor w/ 35 levels "","1","10","11",..: 1 2
> > $ X64SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
> > $ X65DCM : Factor w/ 38 levels "","1","10","11",..: 1 1
> > $ X65SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
> > $ X66DCM : Factor w/ 27 levels "","1","10","11",..: 1 1
> > $ X66SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
> > $ X67SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
> > $ X68DCM : Factor w/ 33 levels "","1","10","11",..: 1 1
> > $ X68SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
> > $ X70MES : Factor w/ 23 levels "","1","10","11",..: 1 1
> > $ X70SUR : Factor w/ 37 levels "","1","10","11",..: 1 1
> > $ X72DCM : Factor w/ 40 levels "","1","10","11",..: 13 27
> > $ X72SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
> > $ X76DCM : Factor w/ 44 levels "","1","10","11",..: 1 1
> > $ X76SUR : Factor w/ 34 levels "","1","10","11",..: 1 1
> > $ X82DCM : Factor w/ 29 levels "","1","10","11",..: 1 1
> > $ X85DCM : Factor w/ 30 levels "","1","10","11",..: 1 1
> >
> >
> >Thank you!!
> >Dawn
> >
> >On Tue, Jul 14, 2015 at 3:48 PM, Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> I suspect your data frame "dat" has non-numeric data in some of the
> >> columns that have ABC in their names. Any column of a data frame can
> >be
> >> numeric or not, but the data frame as a unit cannot be numeric. If
> >your
> >> data file has odd characters in done of the otherwise-numeric
> >columns, the
> >> whole column will be read in as a factor or character strings. Look
> >at the
> >> output of str(dat) for columns that don't show "num'. If you can find
> >the
> >> column, and then one of the bad rows, you can use a text editor to
> >fix them
> >> manually, or show us examples of the bad data and we can suggest ways
> >to
> >> fix it in R.
> >>
>
> >---------------------------------------------------------------------------
> >> Jeff Newmiller                        The     .....       .....  Go
> >Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >> Go...
> >>                                       Live:   OO#.. Dead: OO#..
> >Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> >rocks...1k
> >>
>
> >---------------------------------------------------------------------------
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 14, 2015 2:35:38 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
> >> >Hi,
> >> >
> >> >I used a small set of data (several columns and rows) and it works
> >fine
> >> >using the following command:
> >> >abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)
> >> >
> >> >But when I used the real big data table, "Error in rowSums(dat[,
> >> >grep("ABC", names(dat), fixed = T)], na.rm = T) :
> >> >  'x' must be numeric"
> >> >Then it didn't work either using as.numeric():
> >> >> as.numeric(dat)
> >> >Error: (list) object cannot be coerced to type 'double'
> >> >
> >> >Thanks!
> >> >Dawn
> >> >
> >> >
> >> >
> >> >
> >> >On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:
> >> >
> >> >> Thank you all and sorry for the data messing. It has worked!
> >> >>
> >> >> Best,
> >> >> Dawn
> >> >>
> >> >> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com>
> >> >wrote:
> >> >>
> >> >>> Hi Dawn,
> >> >>> Your data are a bit messed up, but try the following:
> >> >>>
> >> >>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
> >> >>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
> >> >>>
> >> >>> I'm assuming that you want to discard the NA values.
> >> >>>
> >> >>> Jim
> >> >>>
> >> >>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas
> ><ruipbarradas at sapo.pt>
> >> >>> wrote:
> >> >>> > Hello,
> >> >>> >
> >> >>> > Please use ?dput to give a data example, like this it's
> >completely
> >> >>> > unreadable. If your data.frame is named 'dat' use
> >> >>> >
> >> >>> > dput(head(dat, 30))  # paste the outut of this in your mail
> >> >>> >
> >> >>> >
> >> >>> > And don't post in html, use plain text only, like the posting
> >> >guide
> >> >>> says.
> >> >>> >
> >> >>> > Rui Barradas
> >> >>> >
> >> >>> >
> >> >>> > Em 09-07-2015 18:12, Dawn escreveu:
> >> >>> >>
> >> >>> >> Hi,
> >> >>> >>
> >> >>> >> I have a big dataframe as follows
> >> >>> >>
> >> >>> >>      109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC
> >> >25ABC
> >> >>> >> 25XYZ
> >> >>> >>     30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ
> >36ABC
> >> >>> 36SUR
> >> >>> >> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM
> >> >42SUR
> >> >>> >> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC
> >> >66XYZ
> >> >>> >> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ
> >> >76ABC
> >> >>> >> 76XYZ    82ABC    85ABC    POV
> >> >>> >> Cluster_1
> >> >17
> >> >>> 1
> >> >>> >> 3    10    14    5    2    2        1    1    1    2
> >> >>> >>                          2                            TT:61
> >> >>> >> Cluster_2                    1
> >4
> >> > 20
> >> >>> >> 6    5    3    6    9    9    6        10        1    3    1
> >> >>> >>                              4
> >TT:88
> >> >>> >> Cluster_3    3        3                            6        4
> >> >   17
> >> >>> >> 17    18    13    17    19    22    11    5    21    8    5
> >18
> >> >   4
> >> >>> >> 7                                        9
> >> >>> >> TT:227
> >> >>> >> ........
> >> >>> >>
> >> >>> >> I want to get two columns, i.e,  one is to sum columns for all
> >> >>> including
> >> >>> >> ABC for each row and the other is  to sum columns for all
> >> >including XYZ
> >> >>> >> for
> >> >>> >> each row.
> >> >>> >>
> >> >>> >> Is there some help? Thank you!
> >> >>> >> Dawn
> >> >>> >>
> >> >>> >>         [[alternative HTML version deleted]]
> >> >>> >>
> >> >>> >> ______________________________________________
> >> >>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> >>> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>> >> PLEASE do read the posting guide
> >> >>> >> http://www.R-project.org/posting-guide.html
> >> >>> >> and provide commented, minimal, self-contained, reproducible
> >> >code.
> >> >>> >>
> >> >>> >
> >> >>> > ______________________________________________
> >> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> >see
> >> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> >>> > PLEASE do read the posting guide
> >> >>> http://www.R-project.org/posting-guide.html
> >> >>> > and provide commented, minimal, self-contained, reproducible
> >code.
> >> >>>
> >> >>
> >> >>
> >>
> >>
>
>

From dupuis at beaconpower.com  Wed Jul 15 00:21:42 2015
From: dupuis at beaconpower.com (Dupuis, Robert)
Date: Tue, 14 Jul 2015 22:21:42 +0000
Subject: [R]  Parsing large amounts of csv data with limited RAM
Message-ID: <9BC639F46970F943BADB7C8F36F631F45B435B1D@faceoff.bconcorp.com>

I'm relatively new to using R, and I am trying to find a decent solution for my current dilemma.

Right now, I am currently trying to parse second data from a 7 months of CSV data. This is over 10GB of data, and I've run into some "memory issues" loading them all into a single dataset to be plotted. If possible, I'd really like to keep both the one second resolution, and all 100 or so columns intact to make things easier on myself.

The problem I have is that the machine that is running this script only has 8GB of RAM. I've had issues parsing files with lapply, and some sort of csv reader. So far I've tried read.csv, readr.read_table, and data.table.fread with only fread having any sort of memory management (fread seems to crash on me however). The basic approach I am using is as follows:

# Get the data
files = list.files(pattern="*.csv")
set <- lapply(files, function(x) fread(x, header = T, sep = ',')) #replace fread with something that can parse csv data

# Handle the data (Do my plotting down here)
...

These processes work with smaller data sets, but I would like to in a worse case scenario be able to parse through 1 year data which would be around 20GB.

Thank you for your time,
Robert Dupuis


From rcpaufc at gmail.com  Wed Jul 15 03:02:25 2015
From: rcpaufc at gmail.com (=?UTF-8?B?Um9nw6lyaW8gQXJhw7pqbw==?=)
Date: Tue, 14 Jul 2015 22:02:25 -0300
Subject: [R] Troubleshooting DCchoice for dichotomous choice CV data
Message-ID: <CAB1NK4+WFu9AW2uueOw0P361X5GQ_Qd=tDHFvvN5mH8+nssqLA@mail.gmail.com>

I'm trying to run dichotomous choice CV data using DCchoice package. But I
didn't succeed. I keep receiving those messages for 'sbchoice' and
'dbchoice' functions:

Warning in install.packages :
  package ?DCchoice? is not available (for R version 3.2.1)

Error: could not find function "sbchoice"

Anyone could give me a hint how to solve this problem?

-- 
Prof. Rog?rio C?sar Pereira de Ara?jo, Ph.D.
Departamento de Economia Agr?cola
Universidade Federal do Cear?
Campus do Pici, CP 6017
Fortaleza-CE, Brasil, CEP 60455-970
E-mail: rcpa at ufc.br
Tel.: +55 85 3366-9716

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 15 03:17:09 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 14 Jul 2015 18:17:09 -0700
Subject: [R] Troubleshooting DCchoice for dichotomous choice CV data
In-Reply-To: <CAB1NK4+WFu9AW2uueOw0P361X5GQ_Qd=tDHFvvN5mH8+nssqLA@mail.gmail.com>
References: <CAB1NK4+WFu9AW2uueOw0P361X5GQ_Qd=tDHFvvN5mH8+nssqLA@mail.gmail.com>
Message-ID: <F60BBBE0-C90F-4CCF-A8E6-66E8A589F1CB@comcast.net>


On Jul 14, 2015, at 6:02 PM, Rog?rio Ara?jo wrote:

> I'm trying to run dichotomous choice CV data using DCchoice package. But I
> didn't succeed. I keep receiving those messages for 'sbchoice' and
> 'dbchoice' functions:
> 
> Warning in install.packages :
>  package ?DCchoice? is not available (for R version 3.2.1)

The webpage for this package on R-Forge says is fails to build. You really should be corresponding with its author.
> 
> Error: could not find function "sbchoice"
> 
> Anyone could give me a hint how to solve this problem?
> 
> -- 
> Prof. Rog?rio C?sar Pereira de Ara?jo, Ph.D.
> Departamento de Economia Agr?cola
> Universidade Federal do Cear?
> Campus do Pici, CP 6017
> Fortaleza-CE, Brasil, CEP 60455-970
> E-mail: rcpa at ufc.br
> Tel.: +55 85 3366-9716
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Wed Jul 15 03:28:26 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 14 Jul 2015 18:28:26 -0700
Subject: [R] sum some columns for each row
In-Reply-To: <CABtBq8H8JAzEUZdVt-cdTy4aSMSGEO0rff6bnr9kT_3-qhKrow@mail.gmail.com>
References: <CABtBq8Hhg9mDbaGL9WPXXrScbArzV5-jApYsxepYpnbH9XL3Bg@mail.gmail.com>
	<559EDF2B.6030705@sapo.pt>
	<CA+8X3fWNPw5qBXqQx8fcNB3Lc-nj=inFaP44HgX5j2bHyW4X8A@mail.gmail.com>
	<CABtBq8Gp-9e8K7o6e_qq+dZ3PTOF+gdMhEUe=3kfdpT=BH1csg@mail.gmail.com>
	<CABtBq8Fu5Ec=CV0GgeXAnH7cjvJh0fidFJu5Ob9zL_B9UqTYog@mail.gmail.com>
	<5B040424-743E-4F8F-8497-86C35380F011@dcn.davis.CA.us>
	<CABtBq8FCpLMoGO3ZRRcOL+PZqMx04=nnshpNf5SEUcozFW89KA@mail.gmail.com>
	<ACDF5AA3-79B3-4AFC-95A1-25D669CDBB15@dcn.davis.CA.us>
	<CABtBq8H8JAzEUZdVt-cdTy4aSMSGEO0rff6bnr9kT_3-qhKrow@mail.gmail.com>
Message-ID: <5D9C9527-525B-4025-AE64-90557EED0F68@comcast.net>


On Jul 14, 2015, at 4:49 PM, Dawn wrote:

> I attached the file

Well, you may have attached it, but you evidently did not read the posting guide about which filetypes are accepted by the mailserver.

> .... including the first two rows and please help to make it
> the numeric data frame. Hopefully the following command works:
> 
> dcm <- rowSums(dat1[,grep("DCM",names(dat1),fixed=T)],na.rm=T)

How do you expect that to deliver anything meaningful if all of your columns are factor class?

That was the reason for this error in an earlier posting of yours:

But when I used the real big data table, "Error in rowSums(dat[,
grep("ABC", names(dat), fixed = T)], na.rm = T) :
 'x' must be numeric"

You are not paying attention to the responses you have received so far.

I think Bert Gunter's suggestion that you need to work through more introductory tutorials is on point. 

-- 
David.
> 
> Thank you very much!
> Dawn
> 
> On Tue, Jul 14, 2015 at 4:36 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Well it is pretty obvious that all of your columns have non-numeric data
>> in them, but you are the only one who can tell which ones should have been
>> numeric, and you are also the one who can peruse your data file in a text
>> editor.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                      Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 14, 2015 4:05:37 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>>> I used two rows to test the data frame, as follows.
>>> 
>>>> dat <- read.table("TOV_43_Protein_Clusters_abundance1.tab",
>>> header=TRUE,sep = "\t")
>>>> dat1 <- dat[1:2,]
>>>> str(dat1)
>>> 'data.frame':    2 obs. of  44 variables:
>>> $ X      : Factor w/ 1075762 levels "","POV_Cluster_1000001",..: 305266
>>> 625028
>>> $ X109DCM: Factor w/ 46 levels "","1","10","109DCM",..: 1 1
>>> $ X109SUR: Factor w/ 41 levels "","1","10","109SUR",..: 1 1
>>> $ X18DCM : Factor w/ 31 levels "","1","10","11",..: 1 1
>>> $ X18SUR : Factor w/ 25 levels "","1","10","11",..: 1 1
>>> $ X22SUR : Factor w/ 50 levels "","1","10","11",..: 1 2
>>> $ X23DCM : Factor w/ 46 levels "","1","10","11",..: 1 1
>>> $ X25DCM : Factor w/ 42 levels "","1","10","11",..: 1 1
>>> $ X25SUR : Factor w/ 47 levels "","1","10","11",..: 1 1
>>> $ X30DCM : Factor w/ 34 levels "","1","10","11",..: 1 1
>>> $ X31SUR : Factor w/ 43 levels "","1","10","11",..: 1 1
>>> $ X32DCM : Factor w/ 15 levels "","1","10","11",..: 1 1
>>> $ X32SUR : Factor w/ 58 levels "","1","10","11",..: 1 1
>>> $ X34DCM : Factor w/ 53 levels "","1","10","11",..: 1 35
>>> $ X34SUR : Factor w/ 47 levels "","1","10","11",..: 10 14
>>> $ X36DCM : Factor w/ 48 levels "","1","10","11",..: 2 43
>>> $ X36SUR : Factor w/ 45 levels "","1","10","11",..: 23 38
>>> $ X38DCM : Factor w/ 40 levels "","1","10","11",..: 3 23
>>> $ X38SUR : Factor w/ 44 levels "","1","10","11",..: 7 41
>>> $ X39DCM : Factor w/ 38 levels "","1","10","11",..: 34 38
>>> $ X39SUR : Factor w/ 40 levels "","1","10","11",..: 13 40
>>> $ X41DCM : Factor w/ 47 levels "","1","10","11",..: 13 40
>>> $ X41SUR : Factor w/ 40 levels "","1","10","11",..: 1 1
>>> $ X42DCM : Factor w/ 48 levels "","1","10","11",..: 2 3
>>> $ X42SUR : Factor w/ 41 levels "","1","10","11",..: 2 1
>>> $ X46SUR : Factor w/ 31 levels "","1","10","11",..: 2 2
>>> $ X52DCM : Factor w/ 49 levels "","1","10","11",..: 13 23
>>> $ X64DCM : Factor w/ 35 levels "","1","10","11",..: 1 2
>>> $ X64SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
>>> $ X65DCM : Factor w/ 38 levels "","1","10","11",..: 1 1
>>> $ X65SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
>>> $ X66DCM : Factor w/ 27 levels "","1","10","11",..: 1 1
>>> $ X66SUR : Factor w/ 35 levels "","1","10","11",..: 1 1
>>> $ X67SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
>>> $ X68DCM : Factor w/ 33 levels "","1","10","11",..: 1 1
>>> $ X68SUR : Factor w/ 36 levels "","1","10","11",..: 1 1
>>> $ X70MES : Factor w/ 23 levels "","1","10","11",..: 1 1
>>> $ X70SUR : Factor w/ 37 levels "","1","10","11",..: 1 1
>>> $ X72DCM : Factor w/ 40 levels "","1","10","11",..: 13 27
>>> $ X72SUR : Factor w/ 38 levels "","1","10","11",..: 1 1
>>> $ X76DCM : Factor w/ 44 levels "","1","10","11",..: 1 1
>>> $ X76SUR : Factor w/ 34 levels "","1","10","11",..: 1 1
>>> $ X82DCM : Factor w/ 29 levels "","1","10","11",..: 1 1
>>> $ X85DCM : Factor w/ 30 levels "","1","10","11",..: 1 1
>>> 
>>> 
>>> Thank you!!
>>> Dawn
>>> 
>>> On Tue, Jul 14, 2015 at 3:48 PM, Jeff Newmiller
>>> <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>> 
>>>> I suspect your data frame "dat" has non-numeric data in some of the
>>>> columns that have ABC in their names. Any column of a data frame can
>>> be
>>>> numeric or not, but the data frame as a unit cannot be numeric. If
>>> your
>>>> data file has odd characters in done of the otherwise-numeric
>>> columns, the
>>>> whole column will be read in as a factor or character strings. Look
>>> at the
>>>> output of str(dat) for columns that don't show "num'. If you can find
>>> the
>>>> column, and then one of the bad rows, you can use a text editor to
>>> fix them
>>>> manually, or show us examples of the bad data and we can suggest ways
>>> to
>>>> fix it in R.
>>>> 
>> 
>>> ---------------------------------------------------------------------------
>>>> Jeff Newmiller                        The     .....       .....  Go
>>> Live...
>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>> Go...
>>>>                                      Live:   OO#.. Dead: OO#..
>>> Playing
>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> rocks...1k
>>>> 
>> 
>>> ---------------------------------------------------------------------------
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 14, 2015 2:35:38 PM PDT, Dawn <dawn1313 at gmail.com> wrote:
>>>>> Hi,
>>>>> 
>>>>> I used a small set of data (several columns and rows) and it works
>>> fine
>>>>> using the following command:
>>>>> abc <- rowSums(test[,grep("ABC",names(test),fixed=T)],na.rm=T)
>>>>> 
>>>>> But when I used the real big data table, "Error in rowSums(dat[,
>>>>> grep("ABC", names(dat), fixed = T)], na.rm = T) :
>>>>> 'x' must be numeric"
>>>>> Then it didn't work either using as.numeric():
>>>>>> as.numeric(dat)
>>>>> Error: (list) object cannot be coerced to type 'double'
>>>>> 
>>>>> Thanks!
>>>>> Dawn
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> On Fri, Jul 10, 2015 at 4:35 PM, Dawn <dawn1313 at gmail.com> wrote:
>>>>> 
>>>>>> Thank you all and sorry for the data messing. It has worked!
>>>>>> 
>>>>>> Best,
>>>>>> Dawn
>>>>>> 
>>>>>> On Fri, Jul 10, 2015 at 4:15 AM, Jim Lemon <drjimlemon at gmail.com>
>>>>> wrote:
>>>>>> 
>>>>>>> Hi Dawn,
>>>>>>> Your data are a bit messed up, but try the following:
>>>>>>> 
>>>>>>> colSums(dat[,grep("ABC",names(dat),fixed=TRUE)],na.rm=TRUE)
>>>>>>> colSums(dat[,grep("XYZ",names(dat),fixed=TRUE)],na.rm=TRUE)
>>>>>>> 
>>>>>>> I'm assuming that you want to discard the NA values.
>>>>>>> 
>>>>>>> Jim
>>>>>>> 
>>>>>>> On Fri, Jul 10, 2015 at 6:52 AM, Rui Barradas
>>> <ruipbarradas at sapo.pt>
>>>>>>> wrote:
>>>>>>>> Hello,
>>>>>>>> 
>>>>>>>> Please use ?dput to give a data example, like this it's
>>> completely
>>>>>>>> unreadable. If your data.frame is named 'dat' use
>>>>>>>> 
>>>>>>>> dput(head(dat, 30))  # paste the outut of this in your mail
>>>>>>>> 
>>>>>>>> 
>>>>>>>> And don't post in html, use plain text only, like the posting
>>>>> guide
>>>>>>> says.
>>>>>>>> 
>>>>>>>> Rui Barradas
>>>>>>>> 
>>>>>>>> 
>>>>>>>> Em 09-07-2015 18:12, Dawn escreveu:
>>>>>>>>> 
>>>>>>>>> Hi,
>>>>>>>>> 
>>>>>>>>> I have a big dataframe as follows
>>>>>>>>> 
>>>>>>>>>     109ABC    109XYZ    18ABC    18XYZ    22XYZ    23ABC
>>>>> 25ABC
>>>>>>>>> 25XYZ
>>>>>>>>>    30ABC    31XYZ    32ABC    32XYZ    34DCM    34XYZ
>>> 36ABC
>>>>>>> 36SUR
>>>>>>>>> 38DCM    38XYZ    39DCM    39SUR    41DCM    41SUR    42DCM
>>>>> 42SUR
>>>>>>>>> 46SUR    52DCM    64ABC    64XYZ    65ABC    65XYZ    66ABC
>>>>> 66XYZ
>>>>>>>>> 67XYZ    68ABC    68SUR    70MES    70SUR    72ABC    72XYZ
>>>>> 76ABC
>>>>>>>>> 76XYZ    82ABC    85ABC    POV
>>>>>>>>> Cluster_1
>>>>> 17
>>>>>>> 1
>>>>>>>>> 3    10    14    5    2    2        1    1    1    2
>>>>>>>>>                         2                            TT:61
>>>>>>>>> Cluster_2                    1
>>> 4
>>>>> 20
>>>>>>>>> 6    5    3    6    9    9    6        10        1    3    1
>>>>>>>>>                             4
>>> TT:88
>>>>>>>>> Cluster_3    3        3                            6        4
>>>>>  17
>>>>>>>>> 17    18    13    17    19    22    11    5    21    8    5
>>> 18
>>>>>  4
>>>>>>>>> 7                                        9
>>>>>>>>> TT:227
>>>>>>>>> ........
>>>>>>>>> 
>>>>>>>>> I want to get two columns, i.e,  one is to sum columns for all
>>>>>>> including
>>>>>>>>> ABC for each row and the other is  to sum columns for all
>>>>> including XYZ
>>>>>>>>> for
>>>>>>>>> each row.
>>>>>>>>> 
>>>>>>>>> Is there some help? Thank you!
>>>>>>>>> Dawn
>>>>>>>>> 
>>>>>>>>>        [[alternative HTML version deleted]]
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>>>> code.
>>>>>>>>> 
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>>> see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible
>>> code.
>>>>>>> 
>>>>>> 
>>>>>> 
>>>> 
>>>> 
>> 
>> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.CA.us  Wed Jul 15 04:27:01 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 14 Jul 2015 19:27:01 -0700
Subject: [R] Parsing large amounts of csv data with limited RAM
In-Reply-To: <9BC639F46970F943BADB7C8F36F631F45B435B1D@faceoff.bconcorp.com>
References: <9BC639F46970F943BADB7C8F36F631F45B435B1D@faceoff.bconcorp.com>
Message-ID: <D298F05B-554A-45E7-B190-CEFD284ED445@dcn.davis.CA.us>

You seem to want your cake and eat it too. Not unexpected, but you may have your work cut out to learn about the price of having it all.

Plotting: pretty silly to stick with gigabytes of data in your plots. Some kind of aggregation seems required here, with the raw data being a stepping stone to that goal.

Loading: if you don't have RAM, buy more or use one of the disk-based solutions. There are proprietary solutions for a fee, and there are packages like ff. When I have dealt with large data sets I have used sqldf or RODBC (which I think works best for read-only access), so I cannot advise you on ff.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 14, 2015 3:21:42 PM PDT, "Dupuis, Robert" <dupuis at beaconpower.com> wrote:
>I'm relatively new to using R, and I am trying to find a decent
>solution for my current dilemma.
>
>Right now, I am currently trying to parse second data from a 7 months
>of CSV data. This is over 10GB of data, and I've run into some "memory
>issues" loading them all into a single dataset to be plotted. If
>possible, I'd really like to keep both the one second resolution, and
>all 100 or so columns intact to make things easier on myself.
>
>The problem I have is that the machine that is running this script only
>has 8GB of RAM. I've had issues parsing files with lapply, and some
>sort of csv reader. So far I've tried read.csv, readr.read_table, and
>data.table.fread with only fread having any sort of memory management
>(fread seems to crash on me however). The basic approach I am using is
>as follows:
>
># Get the data
>files = list.files(pattern="*.csv")
>set <- lapply(files, function(x) fread(x, header = T, sep = ','))
>#replace fread with something that can parse csv data
>
># Handle the data (Do my plotting down here)
>...
>
>These processes work with smaller data sets, but I would like to in a
>worse case scenario be able to parse through 1 year data which would be
>around 20GB.
>
>Thank you for your time,
>Robert Dupuis
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jholtman at gmail.com  Wed Jul 15 04:49:45 2015
From: jholtman at gmail.com (jim holtman)
Date: Tue, 14 Jul 2015 22:49:45 -0400
Subject: [R] Parsing large amounts of csv data with limited RAM
In-Reply-To: <D298F05B-554A-45E7-B190-CEFD284ED445@dcn.davis.CA.us>
References: <9BC639F46970F943BADB7C8F36F631F45B435B1D@faceoff.bconcorp.com>
	<D298F05B-554A-45E7-B190-CEFD284ED445@dcn.davis.CA.us>
Message-ID: <CAAxdm-6nkf6-DYo_juTf9XFNWDDmLd+rYV8ip=zjJ7BqgoY11Q@mail.gmail.com>

take a look at the sqldf package because it has the ability to load a csv
file to a database from which you can then process the data in pieces


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Tue, Jul 14, 2015 at 10:27 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> You seem to want your cake and eat it too. Not unexpected, but you may
> have your work cut out to learn about the price of having it all.
>
> Plotting: pretty silly to stick with gigabytes of data in your plots. Some
> kind of aggregation seems required here, with the raw data being a stepping
> stone to that goal.
>
> Loading: if you don't have RAM, buy more or use one of the disk-based
> solutions. There are proprietary solutions for a fee, and there are
> packages like ff. When I have dealt with large data sets I have used sqldf
> or RODBC (which I think works best for read-only access), so I cannot
> advise you on ff.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 14, 2015 3:21:42 PM PDT, "Dupuis, Robert" <dupuis at beaconpower.com>
> wrote:
> >I'm relatively new to using R, and I am trying to find a decent
> >solution for my current dilemma.
> >
> >Right now, I am currently trying to parse second data from a 7 months
> >of CSV data. This is over 10GB of data, and I've run into some "memory
> >issues" loading them all into a single dataset to be plotted. If
> >possible, I'd really like to keep both the one second resolution, and
> >all 100 or so columns intact to make things easier on myself.
> >
> >The problem I have is that the machine that is running this script only
> >has 8GB of RAM. I've had issues parsing files with lapply, and some
> >sort of csv reader. So far I've tried read.csv, readr.read_table, and
> >data.table.fread with only fread having any sort of memory management
> >(fread seems to crash on me however). The basic approach I am using is
> >as follows:
> >
> ># Get the data
> >files = list.files(pattern="*.csv")
> >set <- lapply(files, function(x) fread(x, header = T, sep = ','))
> >#replace fread with something that can parse csv data
> >
> ># Handle the data (Do my plotting down here)
> >...
> >
> >These processes work with smaller data sets, but I would like to in a
> >worse case scenario be able to parse through 1 year data which would be
> >around 20GB.
> >
> >Thank you for your time,
> >Robert Dupuis
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sreenath.rajur at macfast.ac.in  Wed Jul 15 08:22:08 2015
From: sreenath.rajur at macfast.ac.in (sreenath)
Date: Tue, 14 Jul 2015 23:22:08 -0700 (PDT)
Subject: [R] Problem in installing simba package
Message-ID: <1436941328793-4709882.post@n4.nabble.com>

 When installing simba package following error showing

install.packages("simba")
Installing package into ?/home/cbl/R/x86_64-redhat-linux-gnu-library/3.1?
(as ?lib? is unspecified)
--- Please select a CRAN mirror for use in this session ---
also installing the dependency ?vegan?

trying URL 'http://ftp.iitm.ac.in/cran/src/contrib/vegan_2.3-0.tar.gz'
Error in download.file(url, destfile, method, mode = "wb", ...) : 
  cannot open URL
'http://ftp.iitm.ac.in/cran/src/contrib/vegan_2.3-0.tar.gz'
In addition: Warning message:
In download.file(url, destfile, method, mode = "wb", ...) :
  cannot open: HTTP status was '0 (null)'
Warning in download.packages(pkgs, destdir = tmpd, available = available,  :
  download of package ?vegan? failed
trying URL 'http://ftp.iitm.ac.in/cran/src/contrib/simba_0.3-5.tar.gz'
Error in download.file(url, destfile, method, mode = "wb", ...) : 
  cannot open URL
'http://ftp.iitm.ac.in/cran/src/contrib/simba_0.3-5.tar.gz'
In addition: Warning message:
In download.file(url, destfile, method, mode = "wb", ...) :
  cannot open: HTTP status was '0 (null)'
Warning in download.packages(pkgs, destdir = tmpd, available = available,  :
  download of package ?simba? failed




--
View this message in context: http://r.789695.n4.nabble.com/Problem-in-installing-simba-package-tp4709882.html
Sent from the R help mailing list archive at Nabble.com.


From jdnewmil at dcn.davis.CA.us  Wed Jul 15 10:10:04 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 15 Jul 2015 01:10:04 -0700
Subject: [R] Problem in installing simba package
In-Reply-To: <1436941328793-4709882.post@n4.nabble.com>
References: <1436941328793-4709882.post@n4.nabble.com>
Message-ID: <D00F4A8F-F033-4FF9-BA65-16E8E234DA4A@dcn.davis.CA.us>

Wait?
Try a different server?
Verify your internet settings?
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 14, 2015 11:22:08 PM PDT, sreenath <sreenath.rajur at macfast.ac.in> wrote:
> When installing simba package following error showing
>
>install.packages("simba")
>Installing package into
>?/home/cbl/R/x86_64-redhat-linux-gnu-library/3.1?
>(as ?lib? is unspecified)
>--- Please select a CRAN mirror for use in this session ---
>also installing the dependency ?vegan?
>
>trying URL 'http://ftp.iitm.ac.in/cran/src/contrib/vegan_2.3-0.tar.gz'
>Error in download.file(url, destfile, method, mode = "wb", ...) : 
>  cannot open URL
>'http://ftp.iitm.ac.in/cran/src/contrib/vegan_2.3-0.tar.gz'
>In addition: Warning message:
>In download.file(url, destfile, method, mode = "wb", ...) :
>  cannot open: HTTP status was '0 (null)'
>Warning in download.packages(pkgs, destdir = tmpd, available =
>available,  :
>  download of package ?vegan? failed
>trying URL 'http://ftp.iitm.ac.in/cran/src/contrib/simba_0.3-5.tar.gz'
>Error in download.file(url, destfile, method, mode = "wb", ...) : 
>  cannot open URL
>'http://ftp.iitm.ac.in/cran/src/contrib/simba_0.3-5.tar.gz'
>In addition: Warning message:
>In download.file(url, destfile, method, mode = "wb", ...) :
>  cannot open: HTTP status was '0 (null)'
>Warning in download.packages(pkgs, destdir = tmpd, available =
>available,  :
>  download of package ?simba? failed
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Problem-in-installing-simba-package-tp4709882.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From marongiu.luigi at gmail.com  Wed Jul 15 12:26:12 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Wed, 15 Jul 2015 11:26:12 +0100
Subject: [R] convert character vector to decimal for numeric testing
Message-ID: <CAMk+s2QTaqwY0j=ovMjnxJiM8yxCdj4ef0xQmEdgiV_Gs3xrjw@mail.gmail.com>

Dear all,
I have a vector that comes from some calculations I am making and this
vectors turns out to be in character format rather than numerical. I
can convert it in numerical format but then the calculations are not
correct, as you can see in the following example. I was also expecting
that rounding a number such as 5.43 to a three digits one would return
5.430 but that did not happen. Any tips on how to apply the
calculation correctly?
Thank you
best regards
luigi

>>>
vec.ori <- c("5.43", "6.63", "-1.18593063116494e+36", "6.2", "5.61",
"4.96842801255869e+30", "5.59", "-Inf", "Inf", "5.49", "18.35",
"-3.11", "6.07", NA)

vec.num <- as.numeric(vec.ori)

vec.num <0

[1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
TRUE FALSE    NA

vec.num >0

 [1]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
FALSE  TRUE    NA

for(i in 1:length(vec.num)) {
  cat("value at beginning: ", vec.num[i], "\n", sep="")
  if(vec.num[i] < 0) {
    vec.num[i] <- "LO"
  } else if(vec.num[i] > 45) {
    vec.num[i] <- "HI"
  } else if (is.na(vec.num[i])== TRUE) {
    vec.num[i] <- "na"
  } else if (is.infinite(vec.num[i]) == TRUE) {
    vec.num[i] <- "INF"
  } else {
    vec.num[i] <- round(vec.num[i], 3)
  }
  cat("value at end: ", vec.num[i], "\n", sep="")
}

value at beginning: 5.43
value at end: 5.43
value at beginning: 6.63
value at end: 6.63
value at beginning: -1185930631164940020264024442864400022
value at end: LO   # REM: error!
value at beginning: 6.2
value at end: HI   # REM: error!
value at beginning: 5.61
value at end: HI   # REM: error!
value at beginning: 4968428012558689723622822000404
value at end: HI
value at beginning: 5.59
value at end: HI   # REM: error!
value at beginning: -Inf
value at end: LO   # REM: error!
value at beginning: Inf
value at end: HI   # REM: error!
value at beginning: 5.49
value at end: HI   # REM: error!
value at beginning: 18.35
Error in round(vec.num[i], 3) :
  non-numeric argument to mathematical function
# REM: cycle crashed


From aurora.gonzalez2 at um.es  Wed Jul 15 13:16:48 2015
From: aurora.gonzalez2 at um.es (AURORA GONZALEZ VIDAL)
Date: Wed, 15 Jul 2015 13:16:48 +0200
Subject: [R] rgl 3d surface
Message-ID: <20150715131648.Horde.w9xAUfTb15kLQwU5MA7kYA1@webmail.um.es>

Hello.

I am trying to plot a 3d surface given its equation. The R code is written
in blue.
So, let's say that I have the points x,y,z and I plot them. Also, I compute
its regression surface doing polynomical regression (fit)

library('rgl')
x <- c(-32.09652, -28.79491, -25.48977, -23.18746,-20.88934, -18.58220,
-17.27919)
y <- c(-32.096, -28.794, -25.489, -23.187,-20.889, -18.582, -17.279)
z <- c(12.16344, 28.84962, 22.36605, 20.13733, 79.50248, 65.46150,44.52274)
plot3d(x,y,z, type="s", col="red", size=1)

fit <- lm(z ~ poly(x,2) + poly(y,2))

In this way, I obtain the coefficients of the surface

coef(fit)

? (Intercept)?? poly(x, 2)1?? poly(x, 2)2
?3.900045e+01? 1.763363e+06? 6.683531e+05
? poly(y, 2)1?? poly(y, 2)2
-1.763303e+06 -6.683944e+05

So I want to repressent the surface
3.900045e+01 +1.763363e+06*x + 6.683531e+05*x*x
-1.763303e+06*y-6.683944e+05*y*y

How could I do it? Any idea??

Thank you very much!


------
Aurora Gonz?lez Vidal

Secci?n Apoyo Estad?stico.
Servicio de Apoyo a la Investigaci?n (SAI).
Vicerrectorado de Investigaci?n.
Universidad de Murcia
Edif. SACE . Campus de Espinardo.
30100 Murcia

@. aurora.gonzalez2 at um.es
T. 868 88 7315
F. 868 88 7302
www.um.es/sai
www.um.es/ae

	[[alternative HTML version deleted]]


From giovanni.franzo1 at gmail.com  Wed Jul 15 12:09:43 2015
From: giovanni.franzo1 at gmail.com (giovanni_gmail)
Date: Wed, 15 Jul 2015 12:09:43 +0200
Subject: [R] =?utf-8?q?=E2=80=98ips=E2=80=99_had_non-zero_exit_status?=
Message-ID: <55A63167.4030801@gmail.com>

dear all,
sorry for the naive question,
I'm trying to install (in R version 3.2.1 (2015-06-18))  the "ips" 
package for phylogenetic analysis.
However I get the following message:


 > install.packages("ips",dependencies = T)
Installing package into ?/home/giovanni/R/x86_64-pc-linux-gnu-library/3.2?
(as ?lib? is unspecified)
also installing the dependency ?XML?

provo con l'URL 'https://cran.rstudio.com/src/contrib/XML_3.98-1.3.tar.gz'
Content type 'application/x-gzip' length 1607725 bytes (1.5 MB)
==================================================
downloaded 1.5 MB

provo con l'URL 'https://cran.rstudio.com/src/contrib/ips_0.0-7.tar.gz'
Content type 'application/x-gzip' length 62904 bytes (61 KB)
==================================================
downloaded 61 KB

* installing *source* package ?XML? ...
** package ?XML? successfully unpacked and MD5 sums checked
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking how to run the C preprocessor... gcc -E
checking for sed... /bin/sed
checking for pkg-config... /usr/bin/pkg-config
checking for xml2-config... no
Cannot find xml2-config
ERROR: configuration failed for package ?XML?
* removing ?/home/giovanni/R/x86_64-pc-linux-gnu-library/3.2/XML?
Warning in install.packages :
   installation of package ?XML? had non-zero exit status
ERROR: dependency ?XML? is not available for package ?ips?
* removing ?/home/giovanni/R/x86_64-pc-linux-gnu-library/3.2/ips?
Warning in install.packages :
   installation of package ?ips? had non-zero exit status

The downloaded source packages are in
     ?/tmp/RtmpbTmMtc/downloaded_packages?

can anybody help me?
best regards
Giovanni


From penv254 at uni-hamburg.de  Wed Jul 15 10:50:46 2015
From: penv254 at uni-hamburg.de (penv254)
Date: Wed, 15 Jul 2015 01:50:46 -0700 (PDT)
Subject: [R] cronbachs alpha and missing values
Message-ID: <1436950246489-4709885.post@n4.nabble.com>

i want to calculate cronbachs alpha for my df. my df has some missing values
so that there are only 23 out of 56 complete cases. if i run alpha on only
the complete cases, i get a value of .79 and if i run it on the whole df, I
get .82. My question is: what does alpha do with those missing values, if i
include the incomplete cases? are they imputed in some way?



--
View this message in context: http://r.789695.n4.nabble.com/cronbachs-alpha-and-missing-values-tp4709885.html
Sent from the R help mailing list archive at Nabble.com.


From sreenath.rajur at macfast.ac.in  Wed Jul 15 09:35:05 2015
From: sreenath.rajur at macfast.ac.in (sreenath)
Date: Wed, 15 Jul 2015 00:35:05 -0700 (PDT)
Subject: [R] Jaccard index
Message-ID: <1436945705449-4709883.post@n4.nabble.com>

How can i find jaccard index of two groups,which package is to be used?
please help



--
View this message in context: http://r.789695.n4.nabble.com/Jaccard-index-tp4709883.html
Sent from the R help mailing list archive at Nabble.com.


From murdoch.duncan at gmail.com  Wed Jul 15 13:53:05 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 15 Jul 2015 07:53:05 -0400
Subject: [R] rgl 3d surface
In-Reply-To: <20150715131648.Horde.w9xAUfTb15kLQwU5MA7kYA1@webmail.um.es>
References: <20150715131648.Horde.w9xAUfTb15kLQwU5MA7kYA1@webmail.um.es>
Message-ID: <55A649A1.6070204@gmail.com>

On 15/07/2015 7:16 AM, AURORA GONZALEZ VIDAL wrote:
> Hello.
> 
> I am trying to plot a 3d surface given its equation. The R code is written
> in blue.
> So, let's say that I have the points x,y,z and I plot them. Also, I compute
> its regression surface doing polynomical regression (fit)
> 
> library('rgl')
> x <- c(-32.09652, -28.79491, -25.48977, -23.18746,-20.88934, -18.58220,
> -17.27919)
> y <- c(-32.096, -28.794, -25.489, -23.187,-20.889, -18.582, -17.279)
> z <- c(12.16344, 28.84962, 22.36605, 20.13733, 79.50248, 65.46150,44.52274)
> plot3d(x,y,z, type="s", col="red", size=1)
> 
> fit <- lm(z ~ poly(x,2) + poly(y,2))
> 
> In this way, I obtain the coefficients of the surface
> 
> coef(fit)
> 
>   (Intercept)   poly(x, 2)1   poly(x, 2)2
>  3.900045e+01  1.763363e+06  6.683531e+05
>   poly(y, 2)1   poly(y, 2)2
> -1.763303e+06 -6.683944e+05
> 
> So I want to repressent the surface
> 3.900045e+01 +1.763363e+06*x + 6.683531e+05*x*x
> -1.763303e+06*y-6.683944e+05*y*y
> 
> How could I do it? Any idea??
> 
> Thank you very much!

You need to write a function f of x and y that produces the fitted
values.  I haven't checked, but I'd assume it needs to take vector
inputs and produce a vector of responses.  Then


persp3d(f)

will draw the surface.  See ?persp3d.function for details on setting the
x and y ranges, etc.

Duncan Murdoch


From meyners.m at pg.com  Wed Jul 15 14:14:06 2015
From: meyners.m at pg.com (Meyners, Michael)
Date: Wed, 15 Jul 2015 12:14:06 +0000
Subject: [R] Jaccard index
In-Reply-To: <1436945705449-4709883.post@n4.nabble.com>
References: <1436945705449-4709883.post@n4.nabble.com>
Message-ID: <AE85F2AAC98B094483D12EEA72230EBD72EEA469@GSDC-EMB019.na.pg.com>

Did you search the internet? At first attempt, 

vegdist {vegan}    (worked well for me in the past) and 

dist.binary {ade4}

seem to offer what you need. 

HTH, Michael

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of sreenath
> Sent: Mittwoch, 15. Juli 2015 09:35
> To: r-help at r-project.org
> Subject: [R] Jaccard index
> 
> How can i find jaccard index of two groups,which package is to be used?
> please help
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Jaccard-index-
> tp4709883.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bhh at xs4all.nl  Wed Jul 15 14:23:25 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 15 Jul 2015 14:23:25 +0200
Subject: [R] Jaccard index
In-Reply-To: <1436945705449-4709883.post@n4.nabble.com>
References: <1436945705449-4709883.post@n4.nabble.com>
Message-ID: <962AE1C5-B1ED-46B8-B92A-8E2BB4DA0B01@xs4all.nl>


> On 15-07-2015, at 09:35, sreenath <sreenath.rajur at macfast.ac.in> wrote:
> 
> How can i find jaccard index of two groups,which package is to be used?
> please help

library(sos)
findFn(?jaccard?)

gives many links.

Berend


From jholtman at gmail.com  Wed Jul 15 14:43:00 2015
From: jholtman at gmail.com (jim holtman)
Date: Wed, 15 Jul 2015 08:43:00 -0400
Subject: [R] convert character vector to decimal for numeric testing
In-Reply-To: <CAMk+s2QTaqwY0j=ovMjnxJiM8yxCdj4ef0xQmEdgiV_Gs3xrjw@mail.gmail.com>
References: <CAMk+s2QTaqwY0j=ovMjnxJiM8yxCdj4ef0xQmEdgiV_Gs3xrjw@mail.gmail.com>
Message-ID: <CAAxdm-4kNHqV=52xCwu1UkUqUqwR60WviN5C81mQJfC6ZBwUFA@mail.gmail.com>

It does round to 3 digits, but since the last one is a zero, is only prints
5.43 and not 5.430.  Why do you want the last zero?  Is this going into a
report or something?  You can always use sprintf:

> x <-round(5.43, 3)
> x
[1] 5.43
> sprintf("%.3f", x)
[1] "5.430"
>

BTW you are converting your numeric vector back to character with
statements like:

 if(vec.num[i] < 0) {
    vec.num[i] <- "LO"

What is the problem you are trying to solve?


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Wed, Jul 15, 2015 at 6:26 AM, Luigi Marongiu <marongiu.luigi at gmail.com>
wrote:

> Dear all,
> I have a vector that comes from some calculations I am making and this
> vectors turns out to be in character format rather than numerical. I
> can convert it in numerical format but then the calculations are not
> correct, as you can see in the following example. I was also expecting
> that rounding a number such as 5.43 to a three digits one would return
> 5.430 but that did not happen. Any tips on how to apply the
> calculation correctly?
> Thank you
> best regards
> luigi
>
> >>>
> vec.ori <- c("5.43", "6.63", "-1.18593063116494e+36", "6.2", "5.61",
> "4.96842801255869e+30", "5.59", "-Inf", "Inf", "5.49", "18.35",
> "-3.11", "6.07", NA)
>
> vec.num <- as.numeric(vec.ori)
>
> vec.num <0
>
> [1] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
> TRUE FALSE    NA
>
> vec.num >0
>
>  [1]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE
> FALSE  TRUE    NA
>
> for(i in 1:length(vec.num)) {
>   cat("value at beginning: ", vec.num[i], "\n", sep="")
>   if(vec.num[i] < 0) {
>     vec.num[i] <- "LO"
>   } else if(vec.num[i] > 45) {
>     vec.num[i] <- "HI"
>   } else if (is.na(vec.num[i])== TRUE) {
>     vec.num[i] <- "na"
>   } else if (is.infinite(vec.num[i]) == TRUE) {
>     vec.num[i] <- "INF"
>   } else {
>     vec.num[i] <- round(vec.num[i], 3)
>   }
>   cat("value at end: ", vec.num[i], "\n", sep="")
> }
>
> value at beginning: 5.43
> value at end: 5.43
> value at beginning: 6.63
> value at end: 6.63
> value at beginning: -1185930631164940020264024442864400022
> value at end: LO   # REM: error!
> value at beginning: 6.2
> value at end: HI   # REM: error!
> value at beginning: 5.61
> value at end: HI   # REM: error!
> value at beginning: 4968428012558689723622822000404
> value at end: HI
> value at beginning: 5.59
> value at end: HI   # REM: error!
> value at beginning: -Inf
> value at end: LO   # REM: error!
> value at beginning: Inf
> value at end: HI   # REM: error!
> value at beginning: 5.49
> value at end: HI   # REM: error!
> value at beginning: 18.35
> Error in round(vec.num[i], 3) :
>   non-numeric argument to mathematical function
> # REM: cycle crashed
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From laura.riggi at slu.se  Wed Jul 15 15:47:22 2015
From: laura.riggi at slu.se (lauraRIG)
Date: Wed, 15 Jul 2015 06:47:22 -0700 (PDT)
Subject: [R] error message in package "FD", function dbFD ()
Message-ID: <1436968042747-4709896.post@n4.nabble.com>

Dear R community,

I  have some trouble with the dbFD function in the FD package. 
> str(a)
'data.frame':    150 obs. of  48 variables:
> str(x)
'data.frame':    48 obs. of  9 variables:

ex1 <- dbFD(x,a)
Error in dbFD(x, a) : 
  Species labels in 'x' and 'a' need to be identical and ordered
alphabetically (or simply in the same order).

I have checked multiple time the data set but this message keeps on
appearing. The names of the species are identical in both data.frames. 

I was wondering if you could help me by giving me an example of an excel
trait and species matrix table to upload for this package. Below is a
snapshop of my data. I have also checked that there are no NA?s problems and
that there is no species abundance = 0 or no community with 0 species.

Thank you for any advice!
Best
Laura 

Species table (a)
Agonum_assimile	Agonum_dorsale	Agonum_gracile	Agonum_gracilipes
2	3	0	0
0	6	0	0
1	10	0	0
1	5	0	0
0	8	0	0
2	7	0	0
1	6	0	0


Trait table(x)

Species	         SizeCategoy Ecology	Reproduction	Diet	Wing
Agonum_assimile	C	nocturnal	spring	Carnivorous	Brachypterous
Agonum_dorsale       	B	nocturnal	spring	Carnivorous	Macropterous
Agonum_gracile	       B	diurnal	spring	Carnivorous	Macropterous
Agonum_gracilipes	B	both	        spring	Carnivorous	Macropterous





--
View this message in context: http://r.789695.n4.nabble.com/error-message-in-package-FD-function-dbFD-tp4709896.html
Sent from the R help mailing list archive at Nabble.com.


From ltcorredorb at gmail.com  Wed Jul 15 16:26:59 2015
From: ltcorredorb at gmail.com (=?UTF-8?Q?Laura_Teresa_Corredor_Boh=C3=B3rquez?=)
Date: Wed, 15 Jul 2015 16:26:59 +0200
Subject: [R] nls singular gradient matrix with an integrand
Message-ID: <CAPTHsVsc-Mb-dSXvDTVkdVUfxO45x1GfKu6deBoo32kpdzJXTQ@mail.gmail.com>

Hi. I am trying to make a nls fit for a little bit complicated expression that
includes two integrals (please find enclosed the equations).

I got the error "Error in nlsModel(formula, mf, start, wts) :
singular gradient
matrix at initial parameter estimates". First of all, I have searched
already in the previous answers, but didn?t help. The parameters initialization
seems to be ok, I have tried to change the parameters but no one works. If
my function has just one integral everything works very nicely, but when adding
a second integral term just got the error. I don?t believe the function is
over-parametrized, as I have performed other fits with much more parameters
and they worked. I am enclosing the data too (file.csv).

And the minimal example is the following:

# read the data from a csv file
dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
x = 0*(1:97)
y = 0*(1:97)
for(i in 1:97){
  x[i] = dados[i,1]
  y[i] = dados[i,2]
}
integrand <- function(X) {
  return(X^4/(2*sinh(X/2))^2)
}
fitting = function(T1, T2, N, D, x){
  int1 = integrate(integrand, lower=0, upper = T1)$value
  int2 = integrate(integrand, lower=0, upper = T2)$value
  return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2
)+(448.956*(x/T1)^3*int1)+(299.304*(x/T2)^3*int2))
}
fit = nls(y ~ fitting(T1, T2, N, D, x),
start=list(T1=400,T2=200,N=0.01,D=2))

------>For reference, the fit that worked is the following:

# read the data from a csv file
dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
x = 0*(1:97)
y = 0*(1:97)
for(i in 1:97){
  x[i] = dados[i,1]
  y[i] = dados[i,2]
}
integrand <- function(X) {
  return(X^4/(2*sinh(X/2))^2)
}
fitting = function(T1, N, D, x){
  int = integrate(integrand, lower=0, upper = T1)$value
  return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2 )+(748.26)*(x/T1)^3*int)
}
fit = nls(y ~ fitting(T1 , N, D, x), start=list(T1=400,N=0.01,D=2))


I cannot figure out what happen. I need to perform this fit for three
integral components, but even for two I have this problem. I appreciate so
much your help. Thank you.

From jdnewmil at dcn.davis.CA.us  Wed Jul 15 17:04:09 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 15 Jul 2015 08:04:09 -0700
Subject: [R] cronbachs alpha and missing values
In-Reply-To: <1436950246489-4709885.post@n4.nabble.com>
References: <1436950246489-4709885.post@n4.nabble.com>
Message-ID: <EEA424AA-E304-410D-88AD-33D8A0E6CDEA@dcn.davis.CA.us>

Sorry, ESP not functioning. Reproducible example missing. There is always more than one way to do things in R, so you need to be specific.

Protip: learn to use the help system. For example, of the function you are using is called alpha, then type

?alpha

at the R prompt and read. If the function is in a contributed package then you should load that package first. Documentation quality may vary due to volunteer dedication, but it often answers questions like this.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 15, 2015 1:50:46 AM PDT, penv254 <penv254 at uni-hamburg.de> wrote:
>i want to calculate cronbachs alpha for my df. my df has some missing
>values
>so that there are only 23 out of 56 complete cases. if i run alpha on
>only
>the complete cases, i get a value of .79 and if i run it on the whole
>df, I
>get .82. My question is: what does alpha do with those missing values,
>if i
>include the incomplete cases? are they imputed in some way?
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/cronbachs-alpha-and-missing-values-tp4709885.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Wed Jul 15 17:20:33 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 15 Jul 2015 08:20:33 -0700
Subject: [R] error message in package "FD", function dbFD ()
In-Reply-To: <1436968042747-4709896.post@n4.nabble.com>
References: <1436968042747-4709896.post@n4.nabble.com>
Message-ID: <580AA9D4-CA6F-4740-8076-B4EDE9460567@dcn.davis.CA.us>

Thank you for making an effort, but your example is still not reproducible. Study something like [1] for more clarity on how to communicate online. Problems that you encounter in getting your data into R are different than problems with the functions in base R or contributed packages, and a reproducible example using dput to give us the data allows us to distinguish the two.

Regarding your request for an Excel example, that is unlikely to occur given that this is an R help mailing list. There are many example data sets in base R and contributed packages. Have you read the examples on the help page for the FD package? Try

?FD-package

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 15, 2015 6:47:22 AM PDT, lauraRIG <laura.riggi at slu.se> wrote:
>Dear R community,
>
>I  have some trouble with the dbFD function in the FD package. 
>> str(a)
>'data.frame':    150 obs. of  48 variables:
>> str(x)
>'data.frame':    48 obs. of  9 variables:
>
>ex1 <- dbFD(x,a)
>Error in dbFD(x, a) : 
>  Species labels in 'x' and 'a' need to be identical and ordered
>alphabetically (or simply in the same order).
>
>I have checked multiple time the data set but this message keeps on
>appearing. The names of the species are identical in both data.frames. 
>
>I was wondering if you could help me by giving me an example of an
>excel
>trait and species matrix table to upload for this package. Below is a
>snapshop of my data. I have also checked that there are no NA?s
>problems and
>that there is no species abundance = 0 or no community with 0 species.
>
>Thank you for any advice!
>Best
>Laura 
>
>Species table (a)
>Agonum_assimile	Agonum_dorsale	Agonum_gracile	Agonum_gracilipes
>2	3	0	0
>0	6	0	0
>1	10	0	0
>1	5	0	0
>0	8	0	0
>2	7	0	0
>1	6	0	0
>
>
>Trait table(x)
>
>Species	         SizeCategoy Ecology	Reproduction	Diet	Wing
>Agonum_assimile	C	nocturnal	spring	Carnivorous	Brachypterous
>Agonum_dorsale       	B	nocturnal	spring	Carnivorous	Macropterous
>Agonum_gracile	       B	diurnal	spring	Carnivorous	Macropterous
>Agonum_gracilipes	B	both	        spring	Carnivorous	Macropterous
>
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/error-message-in-package-FD-function-dbFD-tp4709896.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Wed Jul 15 17:57:06 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 15 Jul 2015 08:57:06 -0700
Subject: [R] error message in package "FD", function dbFD ()
In-Reply-To: <1436968042747-4709896.post@n4.nabble.com>
References: <1436968042747-4709896.post@n4.nabble.com>
Message-ID: <CAF8bMcaCa2=aWC6h6p7y_i_RWnC2Ri_=bP9A62vf-bLeaqvuYg@mail.gmail.com>

Try doing
    rownames(x) <- x["Species"]
before running dbFD.  The function is probably using the row and column
names of its inputs as species names, not a certain row or column of the
data.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Jul 15, 2015 at 6:47 AM, lauraRIG <laura.riggi at slu.se> wrote:

> Dear R community,
>
> I  have some trouble with the dbFD function in the FD package.
> > str(a)
> 'data.frame':    150 obs. of  48 variables:
> > str(x)
> 'data.frame':    48 obs. of  9 variables:
>
> ex1 <- dbFD(x,a)
> Error in dbFD(x, a) :
>   Species labels in 'x' and 'a' need to be identical and ordered
> alphabetically (or simply in the same order).
>
> I have checked multiple time the data set but this message keeps on
> appearing. The names of the species are identical in both data.frames.
>
> I was wondering if you could help me by giving me an example of an excel
> trait and species matrix table to upload for this package. Below is a
> snapshop of my data. I have also checked that there are no NA?s problems
> and
> that there is no species abundance = 0 or no community with 0 species.
>
> Thank you for any advice!
> Best
> Laura
>
> Species table (a)
> Agonum_assimile Agonum_dorsale  Agonum_gracile  Agonum_gracilipes
> 2       3       0       0
> 0       6       0       0
> 1       10      0       0
> 1       5       0       0
> 0       8       0       0
> 2       7       0       0
> 1       6       0       0
>
>
> Trait table(x)
>
> Species          SizeCategoy Ecology    Reproduction    Diet    Wing
> Agonum_assimile C       nocturnal       spring  Carnivorous
>  Brachypterous
> Agonum_dorsale          B       nocturnal       spring  Carnivorous
>  Macropterous
> Agonum_gracile         B        diurnal spring  Carnivorous
>  Macropterous
> Agonum_gracilipes       B       both            spring  Carnivorous
>  Macropterous
>
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/error-message-in-package-FD-function-dbFD-tp4709896.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Wed Jul 15 18:08:27 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 15 Jul 2015 09:08:27 -0700
Subject: [R] =?utf-8?q?=E2=80=98ips=E2=80=99_had_non-zero_exit_status?=
In-Reply-To: <55A63167.4030801@gmail.com>
References: <55A63167.4030801@gmail.com>
Message-ID: <AB5E7F7D-701F-4A79-B305-037106AC61D1@dcn.davis.CA.us>

Your error says the XML package was not installed because you are missing "xml2-config"... if you search the web for that term you will find that your operating system needs to have libxml2 installed, including the development tools. There is a SystemRequirements entry on the CRAN web page for the XML package mentions libxml2.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 15, 2015 3:09:43 AM PDT, giovanni_gmail <giovanni.franzo1 at gmail.com> wrote:
>dear all,
>sorry for the naive question,
>I'm trying to install (in R version 3.2.1 (2015-06-18))  the "ips" 
>package for phylogenetic analysis.
>However I get the following message:
>
>
> > install.packages("ips",dependencies = T)
>Installing package into
>?/home/giovanni/R/x86_64-pc-linux-gnu-library/3.2?
>(as ?lib? is unspecified)
>also installing the dependency ?XML?
>
>provo con l'URL
>'https://cran.rstudio.com/src/contrib/XML_3.98-1.3.tar.gz'
>Content type 'application/x-gzip' length 1607725 bytes (1.5 MB)
>==================================================
>downloaded 1.5 MB
>
>provo con l'URL 'https://cran.rstudio.com/src/contrib/ips_0.0-7.tar.gz'
>Content type 'application/x-gzip' length 62904 bytes (61 KB)
>==================================================
>downloaded 61 KB
>
>* installing *source* package ?XML? ...
>** package ?XML? successfully unpacked and MD5 sums checked
>checking for gcc... gcc
>checking for C compiler default output file name... a.out
>checking whether the C compiler works... yes
>checking whether we are cross compiling... no
>checking for suffix of executables...
>checking for suffix of object files... o
>checking whether we are using the GNU C compiler... yes
>checking whether gcc accepts -g... yes
>checking for gcc option to accept ISO C89... none needed
>checking how to run the C preprocessor... gcc -E
>checking for sed... /bin/sed
>checking for pkg-config... /usr/bin/pkg-config
>checking for xml2-config... no
>Cannot find xml2-config
>ERROR: configuration failed for package ?XML?
>* removing ?/home/giovanni/R/x86_64-pc-linux-gnu-library/3.2/XML?
>Warning in install.packages :
>   installation of package ?XML? had non-zero exit status
>ERROR: dependency ?XML? is not available for package ?ips?
>* removing ?/home/giovanni/R/x86_64-pc-linux-gnu-library/3.2/ips?
>Warning in install.packages :
>   installation of package ?ips? had non-zero exit status
>
>The downloaded source packages are in
>     ?/tmp/RtmpbTmMtc/downloaded_packages?
>
>can anybody help me?
>best regards
>Giovanni
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.CA.us  Wed Jul 15 18:17:08 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 15 Jul 2015 09:17:08 -0700
Subject: [R] nls singular gradient matrix with an integrand
In-Reply-To: <CAPTHsVsc-Mb-dSXvDTVkdVUfxO45x1GfKu6deBoo32kpdzJXTQ@mail.gmail.com>
References: <CAPTHsVsc-Mb-dSXvDTVkdVUfxO45x1GfKu6deBoo32kpdzJXTQ@mail.gmail.com>
Message-ID: <9BA2B4E8-E28B-4913-9881-EA46E06211A8@dcn.davis.CA.us>

Very few attachment types are permitted on the R mailing lists... apparently whatever file you attached did not qualify.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 15, 2015 7:26:59 AM PDT, "Laura Teresa Corredor Boh?rquez" <ltcorredorb at gmail.com> wrote:
>Hi. I am trying to make a nls fit for a little bit complicated
>expression that
>includes two integrals (please find enclosed the equations).
>
>I got the error "Error in nlsModel(formula, mf, start, wts) :
>singular gradient
>matrix at initial parameter estimates". First of all, I have searched
>already in the previous answers, but didn?t help. The parameters
>initialization
>seems to be ok, I have tried to change the parameters but no one works.
>If
>my function has just one integral everything works very nicely, but
>when adding
>a second integral term just got the error. I don?t believe the function
>is
>over-parametrized, as I have performed other fits with much more
>parameters
>and they worked. I am enclosing the data too (file.csv).
>
>And the minimal example is the following:
>
># read the data from a csv file
>dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
>x = 0*(1:97)
>y = 0*(1:97)
>for(i in 1:97){
>  x[i] = dados[i,1]
>  y[i] = dados[i,2]
>}
>integrand <- function(X) {
>  return(X^4/(2*sinh(X/2))^2)
>}
>fitting = function(T1, T2, N, D, x){
>  int1 = integrate(integrand, lower=0, upper = T1)$value
>  int2 = integrate(integrand, lower=0, upper = T2)$value
>  return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2
>)+(448.956*(x/T1)^3*int1)+(299.304*(x/T2)^3*int2))
>}
>fit = nls(y ~ fitting(T1, T2, N, D, x),
>start=list(T1=400,T2=200,N=0.01,D=2))
>
>------>For reference, the fit that worked is the following:
>
># read the data from a csv file
>dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
>x = 0*(1:97)
>y = 0*(1:97)
>for(i in 1:97){
>  x[i] = dados[i,1]
>  y[i] = dados[i,2]
>}
>integrand <- function(X) {
>  return(X^4/(2*sinh(X/2))^2)
>}
>fitting = function(T1, N, D, x){
>  int = integrate(integrand, lower=0, upper = T1)$value
>  return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2 )+(748.26)*(x/T1)^3*int)
>}
>fit = nls(y ~ fitting(T1 , N, D, x), start=list(T1=400,N=0.01,D=2))
>
>
>I cannot figure out what happen. I need to perform this fit for three
>integral components, but even for two I have this problem. I appreciate
>so
>much your help. Thank you.
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From vyshnnaviammu at gmail.com  Wed Jul 15 20:33:53 2015
From: vyshnnaviammu at gmail.com (Vyshnnavi Parthasarathy)
Date: Wed, 15 Jul 2015 11:33:53 -0700
Subject: [R] multi core procesing
Message-ID: <CADeBCTkRPNmLtTeHtJ1iv2TANXsrCN8CsNbDQaVemn5S6mHnBA@mail.gmail.com>

Hello,
I am using a 8 core processor system. Is there a way to run different R
scripts on different cores instead of all scripts running on the same core?
If I open a new RStudio session for each script, can I somehow assign each
Rstudio session to a particular core processor so as to make the process
efficient? I have looked at the doParallel package and from my
understanding including the package enables one script to be run using
multiple cores but what I am looking for is being able to run different
scripts on different cores.

Thanks,
Vyshnnavi

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Jul 15 20:50:46 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 15 Jul 2015 14:50:46 -0400
Subject: [R] multi core procesing
In-Reply-To: <CADeBCTkRPNmLtTeHtJ1iv2TANXsrCN8CsNbDQaVemn5S6mHnBA@mail.gmail.com>
References: <CADeBCTkRPNmLtTeHtJ1iv2TANXsrCN8CsNbDQaVemn5S6mHnBA@mail.gmail.com>
Message-ID: <55A6AB86.9070208@gmail.com>

On 15/07/2015 2:33 PM, Vyshnnavi Parthasarathy wrote:
> Hello,
> I am using a 8 core processor system. Is there a way to run different R
> scripts on different cores instead of all scripts running on the same core?
> If I open a new RStudio session for each script, can I somehow assign each
> Rstudio session to a particular core processor so as to make the process
> efficient? I have looked at the doParallel package and from my
> understanding including the package enables one script to be run using
> multiple cores but what I am looking for is being able to run different
> scripts on different cores.

This is a question about your OS, isn't it?  Does your OS allow you to
run multiple processes on different cores?  Or maybe this is a question
about RStudio.  Does it allow multiple instances to run on the same
computer, so your OS can allocate them to different cores?

If you run your scripts in R (not under RStudio) it's certainly possible
to have multiple R processes running at the same time.  Just do it.

Duncan Murdoch


From daniel_wright89 at hotmail.com  Wed Jul 15 17:43:58 2015
From: daniel_wright89 at hotmail.com (dmw)
Date: Wed, 15 Jul 2015 08:43:58 -0700 (PDT)
Subject: [R] Help - Group Subsector Weighted Average
Message-ID: <1436975038501-4709901.post@n4.nabble.com>

Hi,

I am new to this forum and R in general. I have a database that I would like
to analyze. My ultimate goal is to show the weighted average of each
security in its sector to its predecessor Sector level. I want it to be as
granular as possible.   

For example:

Weighted average of Industrials (Sector Level 2) in Corporate (Sector Level
1). 
Weighted average of Automotive (Sector Level 3) in Industrials (Sector Level
2).
Weighted average of each security in Auto Parts & Equipment (Sector Level 4)
in Automotive (Sector Level 3) and so on.

I'd like to weight the securities and sector levels by the calculated ytw,
ytm, dtw, ed, oas, and avglife. Preferably in a table format, sorted by
sector level and composite ratings. I know it is possible, I just don't know
how to tackle the code, what packages, if any, to add etc. Like I mentioned
before, I am just now getting my feet wet with R and am enjoying it so far.
It's a powerful tool and I'd like to know how to put its power to use. Any
help is greatly appreciated, I am very open to suggestions and ideas.

Here is the dropbox link to my database:
Dropbox Database Link
<https://www.dropbox.com/s/yr8olrj0v5mp4nu/dmw_dataset.csv?dl=0>  


My code so far:

-----------------------------------------
#Read file into R
mkt <- read.csv

#Add librarys to work with data
library(plyr)
library(lubridate)

#Split data frames by variables
divisor <-
ddply(mkt,.(Sector.Level.2,Sector.Level.3,Sector.Level.4,year,Composite.Rating),summarize,amt=sum(Face.Value))
divisor$id <- paste(divisor$Sector.Level.4,divisor$year,sep="")
subind.year <- divisor[,c("id","amt")]

#Format output format of maturity and year
mkt$Maturity <- mdy(as.character(mkt$Maturity))
mkt$year <- year(mkt$Maturity)

#Format composite rating
mkt$Composite.Rating <- gsub("([A-Z]+)([0-9]+)","\\1",mkt$Composite.Rating)

#Test composite rating formating
test <- c("A1", "AA2", "BBB3")
gsub("([A-Z]+)([0-9]+)","\\1",test)

#Show output format
#head(mkt$Composite.Rating,50)


#Split data frames by variables
table <-
ddply(mkt,.(Sector.Level.2,Sector.Level.3,Sector.Level.4,year,Composite.Rating),summarize,
               m.ytw = round(mean(Yield.to.Worst),digits=2),
               m.ytm = round(mean(Effective.Yield),digits=2),
               m.dtw = round(mean(Duration.To.Worst),digits=2),
               m.ed = round(mean(Effective.Duration),digits=2),
               m.oas = round(mean(OAS.vs.Govt),digits=2),
               m.avglife = round(mean(year),digits=2),
               num = length(Sector.Level.3),
               amt=sum(Face.Value))


#Show output format
head(table,100)
-----------------------------------------

This post is also cross-listed in "Talk Stats". Link to that post:
Talk Stats Post
<http://www.talkstats.com/showthread.php/61584-Weighted-Average>  



--
View this message in context: http://r.789695.n4.nabble.com/Help-Group-Subsector-Weighted-Average-tp4709901.html
Sent from the R help mailing list archive at Nabble.com.


From navraj42 at gmail.com  Wed Jul 15 19:25:31 2015
From: navraj42 at gmail.com (Navraj Singh)
Date: Wed, 15 Jul 2015 11:25:31 -0600
Subject: [R] Seeing Build Commands when running "R CMD INSTALL <pkg>"
Message-ID: <CAMOObHJJGZeEuGuWir6-mgz+qqutzGB_JR+pdAjGsYyY3OFS=Q@mail.gmail.com>

Hello,

I was trying to build a package from source (specifically the 'rzmq'
package) on the following platform:

Mac OS X 10.8.5 (Mountain Lion)
R: version 3.2.1
rzmq: version 0.7.7 (obtained from the github repo for this package)

The reason I want to build this from source is that there is no binary
available from CRAN for this version of rzmq for Mountain Lion (it is only
available for Maverick).

I'm using "R CMD INSTALL ..." to build the package. However, I get some
errors. For now, I would just like to be able to see the exact raw commands
being sent to the compiler (gcc in my case), because I think the commands
aren't being set up correctly. Is there a way to do this?

Thank you!

	[[alternative HTML version deleted]]


From robingropp at lclark.edu  Wed Jul 15 21:12:03 2015
From: robingropp at lclark.edu (Robin Gropp)
Date: Wed, 15 Jul 2015 12:12:03 -0700
Subject: [R] Assigning fate percent to plots
Message-ID: <CADdHyNNSLJfKXpDB3ZBh7S_arfmrZjkC-mioyU6JuP2Pj6GBhA@mail.gmail.com>

Hi.

I have a big data set which could be represented something like this:

*plot     fate*
1         S
2         M
3         S
3         S
3         M
4         S
4         S
5         S
5         M
5         S
5         S
6         M
7         M

where plot is a location, and fate is either "survivorship" or "mortality"
( a plant lives or dies.) Thus in plot 5 there are 4 plants. 3 of them
survive, 1 dies.
I want to figure out a way to make R calculate the fraction of individuals
that survive in each plot for all of these. It is proving very challenging.

Example: Plot 5 would return a survivorship value of 3/4 or 75%/
                Plot 3 would return a survivorship value of 2/3 or 66%

Any help would be much appreciated.
Thank you

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed Jul 15 22:30:45 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 15 Jul 2015 16:30:45 -0400
Subject: [R] Seeing Build Commands when running "R CMD INSTALL <pkg>"
In-Reply-To: <CAMOObHJJGZeEuGuWir6-mgz+qqutzGB_JR+pdAjGsYyY3OFS=Q@mail.gmail.com>
References: <CAMOObHJJGZeEuGuWir6-mgz+qqutzGB_JR+pdAjGsYyY3OFS=Q@mail.gmail.com>
Message-ID: <55A6C2F5.5090402@gmail.com>

On 15/07/2015 1:25 PM, Navraj Singh wrote:
> Hello,
> 
> I was trying to build a package from source (specifically the 'rzmq'
> package) on the following platform:
> 
> Mac OS X 10.8.5 (Mountain Lion)
> R: version 3.2.1
> rzmq: version 0.7.7 (obtained from the github repo for this package)
> 
> The reason I want to build this from source is that there is no binary
> available from CRAN for this version of rzmq for Mountain Lion (it is only
> available for Maverick).
> 
> I'm using "R CMD INSTALL ..." to build the package. However, I get some
> errors. For now, I would just like to be able to see the exact raw commands
> being sent to the compiler (gcc in my case), because I think the commands
> aren't being set up correctly. Is there a way to do this?

Normally you will see those commands.  For example, if I try R CMD
INSTALL rgl_0.95.1252.tar.gz on a Mac I see


* installing to library
?/Library/Frameworks/R.framework/Versions/3.2/Resources/library?
* installing *source* package ?rgl? ...
checking for gcc... clang
checking whether the C compiler works... yes
 ... many lines of configure code deleted ...
config.status: creating src/Makevars
** libs
clang++ -I/Library/Frameworks/R.framework/Resources/include -DNDEBUG
-I/System/Library/Frameworks/OpenGL.framework/Headers  -DHAVE_PNG_H
-I/opt/X11/include/libpng15 -I/usr/X11/include -DDarwin -DNO_GL_PREFIX
-I/usr/X11R6/include -DHAVE_FREETYPE -Iext/ftgl
-I/opt/X11/include/freetype2 -Iext -I/usr/local/include
-I/usr/local/include/freetype2 -I/opt/X11/include   -Wall -mtune=core2
-g -O2  -fPIC  -Wall -mtune=core2 -g -O2  -c ABCLineSet.cpp -o ABCLineSet.o

and so on.

Duncan Murdoch


From dwinsemius at comcast.net  Wed Jul 15 22:34:46 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 15 Jul 2015 13:34:46 -0700
Subject: [R] Assigning fate percent to plots
In-Reply-To: <CADdHyNNSLJfKXpDB3ZBh7S_arfmrZjkC-mioyU6JuP2Pj6GBhA@mail.gmail.com>
References: <CADdHyNNSLJfKXpDB3ZBh7S_arfmrZjkC-mioyU6JuP2Pj6GBhA@mail.gmail.com>
Message-ID: <B20FA14E-A42A-4ACF-96D1-670F34F09906@comcast.net>


On Jul 15, 2015, at 12:12 PM, Robin Gropp wrote:

> Hi.
> 
> I have a big data set which could be represented something like this:
> 
> *plot     fate*
> 1         S
> 2         M
> 3         S
> 3         S
> 3         M
> 4         S
> 4         S
> 5         S
> 5         M
> 5         S
> 5         S
> 6         M
> 7         M
> 
> where plot is a location, and fate is either "survivorship" or "mortality"
> ( a plant lives or dies.) Thus in plot 5 there are 4 plants. 3 of them
> survive, 1 dies.
> I want to figure out a way to make R calculate the fraction of individuals
> that survive in each plot for all of these. It is proving very challenging.
> 
> Example: Plot 5 would return a survivorship value of 3/4 or 75%/
>                Plot 3 would return a survivorship value of 2/3 or 66%

Divide the survivors by the number at risk.

tdat <- with( dat, table(plot, fate) )  # a matrix-like object
tdat[,'S']/rowSums(tdat)

        1         2         3         4         5         6 
1.0000000 0.0000000 0.6666667 1.0000000 0.7500000 0.0000000 
        7 
0.0000000 

> 
> Any help would be much appreciated.
> Thank you
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ignacio82 at gmail.com  Thu Jul 16 05:15:52 2015
From: ignacio82 at gmail.com (Ignacio Martinez)
Date: Thu, 16 Jul 2015 03:15:52 +0000
Subject: [R] Speeding up code?
Message-ID: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>

Hi R-Help!

I'm hoping that some of you may give me some tips that could make my code
more efficient. More precisely, I would like to make the answer to my
stakoverflow
<http://stackoverflow.com/questions/31137940/randomly-assign-teachers-to-classrooms-imposing-restrictions>
question more efficient.

This is the code:

library(dplyr)
library(randomNames)
library(geosphere)
set.seed(7142015)# Define Parameters
n.Schools <- 20
first.grade<-3
last.grade<-5
n.Grades <-last.grade-first.grade+1
n.Classrooms <- 20 # THIS IS WHAT I WANTED TO BE ABLE TO CHANGE
n.Teachers <- (n.Schools*n.Grades*n.Classrooms)/2 #Two classrooms per teacher
# Define Random names function:
gen.names <- function(n, which.names = "both", name.order = "last.first"){
  names <- unique(randomNames(n=n, which.names = which.names,
name.order = name.order))
  need <- n - length(names)
  while(need>0){
    names <- unique(c(randomNames(n=need, which.names = which.names,
name.order = name.order), names))
    need <- n - length(names)
  }
  return(names)}
# Generate n.Schools names
gen.schools <- function(n.schools) {
  School.ID <-
    paste0(gen.names(n = n.schools, which.names = "last"), ' School')
  School.long <- rnorm(n = n.schools, mean = 21.7672, sd = 0.025)
  School.lat <- rnorm(n = n.schools, mean = 58.8471, sd = 0.025)
  School.RE <- rnorm(n = n.schools, mean = 0, sd = 1)
  Schools <-
    data.frame(School.ID, School.lat, School.long, School.RE) %>%
    mutate(School.ID = as.character(School.ID)) %>%
    rowwise() %>%  mutate (School.distance = distHaversine(
      p1 = c(School.long, School.lat),
      p2 = c(21.7672, 58.8471), r = 3961
    ))
  return(Schools)}

Schools <- gen.schools(n.schools = n.Schools)
# Generate Grades
Grades <- c(first.grade:last.grade)
# Generate n.Classrooms

Classrooms <- LETTERS[1:n.Classrooms]
# Group schools and grades

SchGr <- outer(paste0(Schools$School.ID, '-'), paste0(Grades, '-'),
FUN="paste")#head(SchGr)
# Group SchGr and Classrooms

SchGrClss <- outer(SchGr, paste0(Classrooms, '-'), FUN="paste")#head(SchGrClss)
# These are the combination of  School-Grades-Classroom
SchGrClssTmp <- as.matrix(SchGrClss, ncol=1, nrow=length(SchGrClss) )
SchGrClssEnd <- as.data.frame(SchGrClssTmp)
# Assign n.Teachers (2 classroom in a given school-grade)
Allpairs <- as.data.frame(t(combn(SchGrClssTmp, 2)))
AllpairsTmp <- paste(Allpairs$V1, Allpairs$V2, sep=" ")

library(stringr)
separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern = "-"))
separoPairs <- as.data.frame(t(separoPairs))
row.names(separoPairs) <- NULL
separoPairs <- separoPairs %>% select(-V7)  %>%  #Drops empty column
  mutate(V1=as.character(V1), V4=as.character(V4), V2=as.numeric(V2),
V5=as.numeric(V5)) %>% mutate(V4 = trimws(V4, which = "both"))

separoPairs[120,]$V4#Only the rows with V1=V4 and V2=V5 are valid
validPairs <- separoPairs %>% filter(V1==V4 & V2==V5) %>% select(V1, V2, V3, V6)
# Generate n.Teachers

gen.teachers <- function(n.teachers){
  Teacher.ID <- gen.names(n = n.teachers, name.order = "last.first")
  Teacher.exp <- runif(n = n.teachers, min = 1, max = 30)
  Teacher.Other <- sample(c(0,1), replace = T, prob = c(0.5, 0.5),
size = n.teachers)
  Teacher.RE <- rnorm(n = n.teachers, mean = 0, sd = 1)
  Teachers <- data.frame(Teacher.ID, Teacher.exp, Teacher.Other, Teacher.RE)
  return(Teachers)}
Teachers <- gen.teachers(n.teachers = n.Teachers) %>%
  mutate(Teacher.ID = as.character(Teacher.ID))
# Randomly assign n.Teachers teachers to the "ValidPairs"
TmpAssignments <- validPairs[sample(1:nrow(validPairs), n.Teachers), ]
Assignments <- cbind.data.frame(Teachers$Teacher.ID, TmpAssignments)
names(Assignments) <- c("Teacher.ID", "School.ID", "Grade", "Class_1",
"Class_2")
# Tidy Data
library(tidyr)
TeacherClassroom <- Assignments %>%
  gather(x, Classroom, Class_1,Class_2) %>%
  select(-x) %>%
  mutate(Teacher.ID = as.character(Teacher.ID))
# Merge
DF_Classrooms <- TeacherClassroom %>% full_join(Teachers,
by="Teacher.ID") %>% full_join(Schools, by="School.ID")
rm(list=setdiff(ls(), "DF_Classrooms")) # Clean the work space!

*I want to end up with the same*  'DF_Classrooms *data frame* but getting
there in a more efficient way. In particular, when is use n.Classrooms <-4 the
code run fast, but *if I increase it to something like 20 it is painfully
slow.*

Thanks!!!

	[[alternative HTML version deleted]]


From johannes at huesing.name  Thu Jul 16 05:32:34 2015
From: johannes at huesing.name (Johannes Huesing)
Date: Thu, 16 Jul 2015 05:32:34 +0200
Subject: [R] problem understanding grid coordinate systems
In-Reply-To: <55A6CCC1.6000206@stat.auckland.ac.nz>
References: <20150711043638.GA17505@huesing.name>
	<55A6CCC1.6000206@stat.auckland.ac.nz>
Message-ID: <20150716033205.GA16408@huesing.name>

Paul Murrell <paul at stat.auckland.ac.nz> [Wed, Jul 15, 2015 at 11:12:33PM CEST]:
...
>downViewport("plot_01.panel.1.1.vp")
...

This works like a charm. Thank you!

Upon reading more of the documentation, I found that current.vpTree() shows all names of active viewports.

-- 
Johannes H?sing               There is something fascinating about science. 
                               One gets such wholesale returns of conjecture 
mailto:johannes at huesing.name  from such a trifling investment of fact.                
http://derwisch.wikidot.com         (Mark Twain, "Life on the Mississippi")


From cflynch at ncsu.edu  Thu Jul 16 05:34:15 2015
From: cflynch at ncsu.edu (Collin Lynch)
Date: Wed, 15 Jul 2015 23:34:15 -0400
Subject: [R] Speeding up code?
In-Reply-To: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>
References: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>
Message-ID: <CAE=6FXahjzNEtXm4w3Ye625xieHpPmRfhrVLwc-gqBVgLznq6g@mail.gmail.com>

Hi Ignacio, If I am reading your code correctly then the top while loop is
essentially seeking to select a random set of names from the original set,
then using unique to reduce it down, you then iterate until you have built
your quota.  Ultimately this results in a very inefficient attempt at
sampling without replacement.  Why not just sample without replacement
rather than loop iteratively and use unique?  Or if the set of possible
names are short enough why not just randomize it and then pull the first n
items off?

    Best,
    Collin.

On Wed, Jul 15, 2015 at 11:15 PM, Ignacio Martinez <ignacio82 at gmail.com>
wrote:

> Hi R-Help!
>
> I'm hoping that some of you may give me some tips that could make my code
> more efficient. More precisely, I would like to make the answer to my
> stakoverflow
> <
> http://stackoverflow.com/questions/31137940/randomly-assign-teachers-to-classrooms-imposing-restrictions
> >
> question more efficient.
>
> This is the code:
>
> library(dplyr)
> library(randomNames)
> library(geosphere)
> set.seed(7142015)# Define Parameters
> n.Schools <- 20
> first.grade<-3
> last.grade<-5
> n.Grades <-last.grade-first.grade+1
> n.Classrooms <- 20 # THIS IS WHAT I WANTED TO BE ABLE TO CHANGE
> n.Teachers <- (n.Schools*n.Grades*n.Classrooms)/2 #Two classrooms per
> teacher
> # Define Random names function:
> gen.names <- function(n, which.names = "both", name.order = "last.first"){
>   names <- unique(randomNames(n=n, which.names = which.names,
> name.order = name.order))
>   need <- n - length(names)
>   while(need>0){
>     names <- unique(c(randomNames(n=need, which.names = which.names,
> name.order = name.order), names))
>     need <- n - length(names)
>   }
>   return(names)}
> # Generate n.Schools names
> gen.schools <- function(n.schools) {
>   School.ID <-
>     paste0(gen.names(n = n.schools, which.names = "last"), ' School')
>   School.long <- rnorm(n = n.schools, mean = 21.7672, sd = 0.025)
>   School.lat <- rnorm(n = n.schools, mean = 58.8471, sd = 0.025)
>   School.RE <- rnorm(n = n.schools, mean = 0, sd = 1)
>   Schools <-
>     data.frame(School.ID, School.lat, School.long, School.RE) %>%
>     mutate(School.ID = as.character(School.ID)) %>%
>     rowwise() %>%  mutate (School.distance = distHaversine(
>       p1 = c(School.long, School.lat),
>       p2 = c(21.7672, 58.8471), r = 3961
>     ))
>   return(Schools)}
>
> Schools <- gen.schools(n.schools = n.Schools)
> # Generate Grades
> Grades <- c(first.grade:last.grade)
> # Generate n.Classrooms
>
> Classrooms <- LETTERS[1:n.Classrooms]
> # Group schools and grades
>
> SchGr <- outer(paste0(Schools$School.ID, '-'), paste0(Grades, '-'),
> FUN="paste")#head(SchGr)
> # Group SchGr and Classrooms
>
> SchGrClss <- outer(SchGr, paste0(Classrooms, '-'),
> FUN="paste")#head(SchGrClss)
> # These are the combination of  School-Grades-Classroom
> SchGrClssTmp <- as.matrix(SchGrClss, ncol=1, nrow=length(SchGrClss) )
> SchGrClssEnd <- as.data.frame(SchGrClssTmp)
> # Assign n.Teachers (2 classroom in a given school-grade)
> Allpairs <- as.data.frame(t(combn(SchGrClssTmp, 2)))
> AllpairsTmp <- paste(Allpairs$V1, Allpairs$V2, sep=" ")
>
> library(stringr)
> separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
> "-"))
> separoPairs <- as.data.frame(t(separoPairs))
> row.names(separoPairs) <- NULL
> separoPairs <- separoPairs %>% select(-V7)  %>%  #Drops empty column
>   mutate(V1=as.character(V1), V4=as.character(V4), V2=as.numeric(V2),
> V5=as.numeric(V5)) %>% mutate(V4 = trimws(V4, which = "both"))
>
> separoPairs[120,]$V4#Only the rows with V1=V4 and V2=V5 are valid
> validPairs <- separoPairs %>% filter(V1==V4 & V2==V5) %>% select(V1, V2,
> V3, V6)
> # Generate n.Teachers
>
> gen.teachers <- function(n.teachers){
>   Teacher.ID <- gen.names(n = n.teachers, name.order = "last.first")
>   Teacher.exp <- runif(n = n.teachers, min = 1, max = 30)
>   Teacher.Other <- sample(c(0,1), replace = T, prob = c(0.5, 0.5),
> size = n.teachers)
>   Teacher.RE <- rnorm(n = n.teachers, mean = 0, sd = 1)
>   Teachers <- data.frame(Teacher.ID, Teacher.exp, Teacher.Other,
> Teacher.RE)
>   return(Teachers)}
> Teachers <- gen.teachers(n.teachers = n.Teachers) %>%
>   mutate(Teacher.ID = as.character(Teacher.ID))
> # Randomly assign n.Teachers teachers to the "ValidPairs"
> TmpAssignments <- validPairs[sample(1:nrow(validPairs), n.Teachers), ]
> Assignments <- cbind.data.frame(Teachers$Teacher.ID, TmpAssignments)
> names(Assignments) <- c("Teacher.ID", "School.ID", "Grade", "Class_1",
> "Class_2")
> # Tidy Data
> library(tidyr)
> TeacherClassroom <- Assignments %>%
>   gather(x, Classroom, Class_1,Class_2) %>%
>   select(-x) %>%
>   mutate(Teacher.ID = as.character(Teacher.ID))
> # Merge
> DF_Classrooms <- TeacherClassroom %>% full_join(Teachers,
> by="Teacher.ID") %>% full_join(Schools, by="School.ID")
> rm(list=setdiff(ls(), "DF_Classrooms")) # Clean the work space!
>
> *I want to end up with the same*  'DF_Classrooms *data frame* but getting
> there in a more efficient way. In particular, when is use n.Classrooms <-4
> the
> code run fast, but *if I increase it to something like 20 it is painfully
> slow.*
>
> Thanks!!!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From l.shulman at hotmail.com  Thu Jul 16 01:04:22 2015
From: l.shulman at hotmail.com (lstat)
Date: Wed, 15 Jul 2015 16:04:22 -0700 (PDT)
Subject: [R] R-package TDA- RipsDiag issues
Message-ID: <1437001462258-4709925.post@n4.nabble.com>

Hi there, 
I was hoping you could help me out with the following:
- I am trying to compute the diagram of a Rips filtration built on top of a
point cloud. 
I've gone through several tutorials, and I always get the same answer as the
tutorial. Let's say I am inputting an n by d matrix of coordinates, where n
is the number of points in d dimensional space, I am able to get a
persistence diagram with dimensions 0, 1, and 2-- when I specify
maxdimension=2. 
HOWEVER-- I was wondering; if you input an N BY N matrix of distances of n
points, say a 15 by 15 matrix, which is what I am working with, I can't ever
see any 2 dimensional features, or 1 dimensional-- the diagram always shows
dim0, no matter what matrix I input. 
The code I have been using is as follows: 
Diag<-ripsDiag(XX, maxscale, dist="arbitrary", printProgress=TRUE), where XX
is the name of my distance matrix. 
Is it not possible to ever extract dim 1 or dim 2 features from this? Even
though I specified maxdimension=2. 
Could someone please help me out, as this is causing great grief. 
In the tutorial I found online by cran project, they input a distance matrix
for a triangle with lengths 1,2, and 4; they specify maxdimension=1,
maxscale=5, and input the same as I did above; they only get dim0 features
on the diagram... wouldn't we expect the loop of the triangle to be a key
feature?
If not, either way, would we not expect to see dim1 and dim2 noise features
at all?

Thanks so much!



--
View this message in context: http://r.789695.n4.nabble.com/R-package-TDA-RipsDiag-issues-tp4709925.html
Sent from the R help mailing list archive at Nabble.com.


From muhammadsohailraza at live.com  Thu Jul 16 05:23:21 2015
From: muhammadsohailraza at live.com (Muhammad Sohail Raza)
Date: Thu, 16 Jul 2015 03:23:21 +0000
Subject: [R] Problem Regarding R Packages installation
Message-ID: <BLU175-W33A30E6F9B2EE7CFAA9CFAAF990@phx.gbl>

Hi Everyone!
I am trying to install R Package "VariantAnnotation". I entered commands:

source("http://bioconductor.org/biocLite.R")
 biocLite("VariantAnnotation")


But i am getting the following errors, 

ERROR: configuration failed for package ?XML?
* removing ?/usr/lib64/R/library/XML?
* installing *source* package ?RCurl? ...
** package ?RCurl? successfully unpacked and MD5 sums checked
configure: loading site script /usr/share/site/x86_64-unknown-linux-gnu
checking for curl-config... no
Cannot find curl-config
ERROR: configuration failed for package ?RCurl?
* removing ?/usr/lib64/R/library/RCurl?
* installing *source* package ?Rsamtools? ...
** libs
gcc -std=gnu99 -I/usr/lib64/R/include -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/S4Vectors/include" -I"/usr/lib64/R/library/IRanges/include" -I"/usr/lib64/R/library/XVector/include" -I"/usr/lib64/R/library/Biostrings/include"  -fopenmp -D_USE_KNETFILE -D_FILE_OFFSET_BITS=64 -U_FORTIFY_SOURCE -DBGZF_CACHE -Dfprintf=_samtools_fprintf -Dexit=_samtools_exit -Dabort=_samtools_abort -I./samtools -I./samtools/bcftools -I./tabix -fpic  -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables  -c Biostrings_stubs.c -o Biostrings_stubs.o
gcc -std=gnu99 -I/usr/lib64/R/include -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/S4Vectors/include" -I"/usr/lib64/R/library/IRanges/include" -I"/usr/lib64/R/library/XVector/include" -I"/usr/lib64/R/library/Biostrings/include"  -fopenmp -D_USE_KNETFILE -D_FILE_OFFSET_BITS=64 -U_FORTIFY_SOURCE -DBGZF_CACHE -Dfprintf=_samtools_fprintf -Dexit=_samtools_exit -Dabort=_samtools_abort -I./samtools -I./samtools/bcftools -I./tabix -fpic  -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables  -c IRanges_stubs.c -o IRanges_stubs.o
g++ -I/usr/lib64/R/include -DNDEBUG  -I/usr/local/include -I"/usr/lib64/R/library/S4Vectors/include" -I"/usr/lib64/R/library/IRanges/include" -I"/usr/lib64/R/library/XVector/include" -I"/usr/lib64/R/library/Biostrings/include"   -fpic  -fmessage-length=0 -grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector -funwind-tables -fasynchronous-unwind-tables  -c PileupBuffer.cpp -o PileupBuffer.o
/bin/sh: g++: command not found
/usr/lib64/R/etc/Makeconf:143: recipe for target 'PileupBuffer.o' failed
make: *** [PileupBuffer.o] Error 127
ERROR: compilation failed for package ?Rsamtools?
* removing ?/usr/lib64/R/library/Rsamtools?
* installing *source* package ?futile.logger? ...
** package ?futile.logger? successfully unpacked and MD5 sums checked
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (futile.logger)
ERROR: dependencies ?XML?, ?RCurl? are not available for package ?biomaRt?
* removing ?/usr/lib64/R/library/biomaRt?
* installing *source* package ?BiocParallel? ...
** R
** inst
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded
* DONE (BiocParallel)
ERROR: dependency ?Rsamtools? is not available for package ?GenomicAlignments?
* removing ?/usr/lib64/R/library/GenomicAlignments?
ERROR: dependencies ?XML?, ?RCurl?, ?Rsamtools?, ?GenomicAlignments? are not available for package ?rtracklayer?
* removing ?/usr/lib64/R/library/rtracklayer?
ERROR: dependencies ?rtracklayer?, ?Rsamtools? are not available for package ?BSgenome?
* removing ?/usr/lib64/R/library/BSgenome?
ERROR: dependencies ?rtracklayer?, ?biomaRt?, ?RCurl? are not available for package ?GenomicFeatures?
* removing ?/usr/lib64/R/library/GenomicFeatures?
ERROR: dependencies ?Rsamtools?, ?BSgenome?, ?rtracklayer?, ?GenomicFeatures? are not available for package ?VariantAnnotation?
* removing ?/usr/lib64/R/library/VariantAnnotation?

The downloaded source packages are in
        ?/tmp/RtmpvpoYjO/downloaded_packages?
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Warning messages:
1: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?XML? had non-zero exit status
2: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?RCurl? had non-zero exit status
3: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?Rsamtools? had non-zero exit status
4: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?biomaRt? had non-zero exit status
5: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?GenomicAlignments? had non-zero exit status
6: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?rtracklayer? had non-zero exit status
7: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?BSgenome? had non-zero exit status
8: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?GenomicFeatures? had non-zero exit status
9: In install.packages(pkgs = doing, lib = lib, ...) :
  installation of package ?VariantAnnotation? had non-zero exit status


I don't know how to deal with it, can anyone helpo me how to deal with it?

Thanks!


************************************************************************************
Muhammad Sohail RazaBeijing Institute of Genomics, CASBeijing, China.Phone: +8613552957083
email:   sohail at big.ac.cn              muhammadsohailraza at live.com 		 	   		  
	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Thu Jul 16 15:01:28 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Thu, 16 Jul 2015 14:01:28 +0100
Subject: [R] rgl 3d surface
In-Reply-To: <55A649A1.6070204@gmail.com>
References: <20150715131648.Horde.w9xAUfTb15kLQwU5MA7kYA1@webmail.um.es>
	<55A649A1.6070204@gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E26724028D7122@GBTEDVPEXCMB04.corp.lgc-group.com>



> -----Original Message-----
> > I compute its regression surface doing polynomical regression (fit)
> > ...
> > fit <- lm(z ~ poly(x,2) + poly(y,2))
> >
.....
> > So I want to repressent the surface
.... 
> > How could I do it? Any idea??
> 
> You need to write a function f of x and y that produces the fitted values.  I
> haven't checked, but I'd assume it needs to take vector inputs and produce a
> vector of responses.  

Perhaps worth noting that since the fit was produced by lm(), predict() will generate the surface values without a separate function.

For example, if we said something like [sorry, not yet checked]

xvals <- seq(-35, -15, 0.5)
yvals <- seq(-35, -15, 0.5)
new.data <- data.frame(x=rep(xvals, each=length(yvals)), y=rep(yvals, length(xvals) )
new.data$z <- predict(fit, newdata=new.data)
#This should generate the predicted surface

#Then:
with(new.data, persp3d(x, y, z)) 

#ought to do the job.



> Then
> 
> 
> persp3d(f)
> 
> will draw the surface.  See ?persp3d.function for details on setting the x and y
> ranges, etc.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From ivan.calandra at univ-reims.fr  Thu Jul 16 15:13:43 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 16 Jul 2015 15:13:43 +0200
Subject: [R] problem with wilcox.test()
Message-ID: <55A7AE07.5000304@univ-reims.fr>

Dear useRs,

I am running a wilcox.test() on two subsets of a dataset and get exactly 
the same results although the raw data are different in the subsets.

mydata <- structure(list(cat1 = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L), .Label = c("high", "low"), 
class = "factor"), cat2 = structure(c(1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 
1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L), .Label = c("large", "small"), 
class = "factor"), var1 = c(2.012743, 1.51272, 1.328453, 1.2609935, 
1.617757, 1.8175455, 1.890035, 2.3652205, 1.295888, 1.5985145, 1.081813, 
1.856733, 2.366358, 2.27421, 1.727023, 2.230433, 5.272843, 3.7626355), 
var2 = c(0.00196, 0.0066545, 0.006188, 0.0058985, 0.004453, 0.005468, 
0.003773, 0.004742, 0.007525, 0.0081235, 0.004611, 0.0050475, 0.006643, 
0.0097335, 0.009213, 0.0049525, 0.006243, 0.006021)), .Names = c("cat1", 
"cat2", "var1", "var2"), row.names = c(NA, 18L), class = "data.frame")

#p-values are identical but W different for the first variable
wilcox.test(var1~cat1, data=mydata[mydata$cat2=="large",])
wilcox.test(var1~cat1, data=mydata[mydata$cat2=="small",])

#both p-values and W are identical for the second variable
wilcox.test(var2~cat1, data=mydata[mydata$cat2=="large",])
wilcox.test(var2~cat1, data=mydata[mydata$cat2=="small",])

Did I do something wrong or does it just have something to do with my 
dataset? Or is it just a coincidence?

Thank you in advance for your help,
Ivan

-- 
Ivan Calandra, ATER
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra


From jrkrideau at inbox.com  Thu Jul 16 15:20:11 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 16 Jul 2015 05:20:11 -0800
Subject: [R] cronbachs alpha and missing values
In-Reply-To: <1436950246489-4709885.post@n4.nabble.com>
Message-ID: <D9E390D7ED7.00000128jrkrideau@inbox.com>

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html

Currently we don't even know what version of Cronbach's alpha you are using.

Also use google as well as help()

Try R statistics Cronbach's alpha  

John Kane
Kingston ON Canada


> -----Original Message-----
> From: penv254 at uni-hamburg.de
> Sent: Wed, 15 Jul 2015 01:50:46 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] cronbachs alpha and missing values
> 
> i want to calculate cronbachs alpha for my df. my df has some missing
> values
> so that there are only 23 out of 56 complete cases. if i run alpha on
> only
> the complete cases, i get a value of .79 and if i run it on the whole df,
> I
> get .82. My question is: what does alpha do with those missing values, if
> i
> include the incomplete cases? are they imputed in some way?
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/cronbachs-alpha-and-missing-values-tp4709885.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From pdalgd at gmail.com  Thu Jul 16 15:47:00 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 16 Jul 2015 15:47:00 +0200
Subject: [R] problem with wilcox.test()
In-Reply-To: <55A7AE07.5000304@univ-reims.fr>
References: <55A7AE07.5000304@univ-reims.fr>
Message-ID: <38820EA2-DCA2-4C92-9131-BE40B45839EF@gmail.com>


> On 16 Jul 2015, at 15:13 , Ivan Calandra <ivan.calandra at univ-reims.fr> wrote:
> 
> Dear useRs,
> 
> I am running a wilcox.test() on two subsets of a dataset and get exactly the same results although the raw data are different in the subsets.
> 
> mydata <- structure(list(cat1 = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L), .Label = c("high", "low"), class = "factor"), cat2 = structure(c(1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L), .Label = c("large", "small"), class = "factor"), var1 = c(2.012743, 1.51272, 1.328453, 1.2609935, 1.617757, 1.8175455, 1.890035, 2.3652205, 1.295888, 1.5985145, 1.081813, 1.856733, 2.366358, 2.27421, 1.727023, 2.230433, 5.272843, 3.7626355), var2 = c(0.00196, 0.0066545, 0.006188, 0.0058985, 0.004453, 0.005468, 0.003773, 0.004742, 0.007525, 0.0081235, 0.004611, 0.0050475, 0.006643, 0.0097335, 0.009213, 0.0049525, 0.006243, 0.006021)), .Names = c("cat1", "cat2", "var1", "var2"), row.names = c(NA, 18L), class = "data.frame")
> 
> #p-values are identical but W different for the first variable
> wilcox.test(var1~cat1, data=mydata[mydata$cat2=="large",])
> wilcox.test(var1~cat1, data=mydata[mydata$cat2=="small",])
> 
> #both p-values and W are identical for the second variable
> wilcox.test(var2~cat1, data=mydata[mydata$cat2=="large",])
> wilcox.test(var2~cat1, data=mydata[mydata$cat2=="small",])
> 
> Did I do something wrong or does it just have something to do with my dataset? Or is it just a coincidence?

Coincidence, mostly, I think:

You have

> table(mydata[mydata$cat2=="small","cat1"])

high  low 
   4    5 

> table(mydata[mydata$cat2=="large","cat1"])

high  low 
   4    5 

and all of your response variables' values are distinct.

In both cases, the null distribution of the rank sum W is that of (sum(sample(1:9,4))-sum(1:4)) which is a distribution on 0:20, symmetric around 10. Hence there are only 11 different p-values possible, so it is not particularly odd that you may get the same one twice.


> 
> Thank you in advance for your help,
> Ivan
> 
> -- 
> Ivan Calandra, ATER
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mvheese at zeelandnet.nl  Thu Jul 16 09:08:49 2015
From: mvheese at zeelandnet.nl (MH)
Date: Thu, 16 Jul 2015 09:08:49 +0200
Subject: [R] R 3.2.0 - Windows 8,
	1 - SSL certificate problem installing course in swirl
Message-ID: <000a01d0bf96$438eb550$caac1ff0$@zeelandnet.nl>

Hello,

I try to install a course for swirl and got a SSL problem:

> install_from_swirl("R Programming")
Error in function (type, msg, asError = TRUE)  : 
  SSL certificate problem: self signed certificate in certificate chain

Found this answer with Google but does not work either:

> set_config( config( ssl.verifypeer = 0L ) )
Error: could not find function "set_config"

What do I do to avoid or accept the SSL certificate?

 

Kind regards,

 

Martin


	[[alternative HTML version deleted]]


From ignacio82 at gmail.com  Thu Jul 16 13:56:54 2015
From: ignacio82 at gmail.com (Ignacio Martinez)
Date: Thu, 16 Jul 2015 11:56:54 +0000
Subject: [R] Speeding up code?
In-Reply-To: <CAE=6FXahjzNEtXm4w3Ye625xieHpPmRfhrVLwc-gqBVgLznq6g@mail.gmail.com>
References: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>
	<CAE=6FXahjzNEtXm4w3Ye625xieHpPmRfhrVLwc-gqBVgLznq6g@mail.gmail.com>
Message-ID: <CAJA1VFxU6p7PvPpbJ0Zmy-NCKBXzh3gwGvov2dRf97-B10-GPQ@mail.gmail.com>

Hi Collin,

The objective of the gen.names function is to generate N *unique *random
names, where N is a *large *number. In my computer `gen.names(n = 50000)`
takes under a second, so is probably not the root problem in my code. That
said, I would love to improve it. I'm not exactly sure how you propose to
change it using sample. What is the object that I would be sampling? I
would love to run a little benchmark to compare my version with yours.

What really takes a long time to run is:

    separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
"-"))

So that and the chunk of code before that is probably where I would get big
gains in speed. Sadly, I have no clue how to do it differently

Thanks a lot for the help!!

Ignacio


On Wed, Jul 15, 2015 at 11:34 PM Collin Lynch <cflynch at ncsu.edu> wrote:

> Hi Ignacio, If I am reading your code correctly then the top while loop is
> essentially seeking to select a random set of names from the original set,
> then using unique to reduce it down, you then iterate until you have built
> your quota.  Ultimately this results in a very inefficient attempt at
> sampling without replacement.  Why not just sample without replacement
> rather than loop iteratively and use unique?  Or if the set of possible
> names are short enough why not just randomize it and then pull the first n
> items off?
>
>     Best,
>     Collin.
>
> On Wed, Jul 15, 2015 at 11:15 PM, Ignacio Martinez <ignacio82 at gmail.com>
> wrote:
>
>> Hi R-Help!
>>
>> I'm hoping that some of you may give me some tips that could make my code
>>
> more efficient. More precisely, I would like to make the answer to my
>> stakoverflow
>> <
>> http://stackoverflow.com/questions/31137940/randomly-assign-teachers-to-classrooms-imposing-restrictions
>> >
>
>
>> question more efficient.
>>
>> This is the code:
>>
>> library(dplyr)
>> library(randomNames)
>> library(geosphere)
>>
> set.seed(7142015)# Define Parameters
>
>
>> n.Schools <- 20
>> first.grade<-3
>> last.grade<-5
>> n.Grades <-last.grade-first.grade+1
>> n.Classrooms <- 20 # THIS IS WHAT I WANTED TO BE ABLE TO CHANGE
>> n.Teachers <- (n.Schools*n.Grades*n.Classrooms)/2 #Two classrooms per
>> teacher
>> # Define Random names function:
>> gen.names <- function(n, which.names = "both", name.order = "last.first"){
>>   names <- unique(randomNames(n=n, which.names = which.names,
>> name.order = name.order))
>>   need <- n - length(names)
>>   while(need>0){
>>     names <- unique(c(randomNames(n=need, which.names = which.names,
>> name.order = name.order), names))
>>     need <- n - length(names)
>>   }
>>   return(names)}
>> # Generate n.Schools names
>> gen.schools <- function(n.schools) {
>>   School.ID <-
>>     paste0(gen.names(n = n.schools, which.names = "last"), ' School')
>>   School.long <- rnorm(n = n.schools, mean = 21.7672, sd = 0.025)
>>   School.lat <- rnorm(n = n.schools, mean = 58.8471, sd = 0.025)
>>   School.RE <- rnorm(n = n.schools, mean = 0, sd = 1)
>>   Schools <-
>>     data.frame(School.ID, School.lat, School.long, School.RE) %>%
>>     mutate(School.ID = as.character(School.ID)) %>%
>>     rowwise() %>%  mutate (School.distance = distHaversine(
>>       p1 = c(School.long, School.lat),
>>       p2 = c(21.7672, 58.8471), r = 3961
>>     ))
>>   return(Schools)}
>>
>> Schools <- gen.schools(n.schools = n.Schools)
>> # Generate Grades
>> Grades <- c(first.grade:last.grade)
>> # Generate n.Classrooms
>>
>> Classrooms <- LETTERS[1:n.Classrooms]
>> # Group schools and grades
>>
>> SchGr <- outer(paste0(Schools$School.ID, '-'), paste0(Grades, '-'),
>> FUN="paste")#head(SchGr)
>> # Group SchGr and Classrooms
>>
>> SchGrClss <- outer(SchGr, paste0(Classrooms, '-'),
>> FUN="paste")#head(SchGrClss)
>> # These are the combination of  School-Grades-Classroom
>> SchGrClssTmp <- as.matrix(SchGrClss, ncol=1, nrow=length(SchGrClss) )
>> SchGrClssEnd <- as.data.frame(SchGrClssTmp)
>> # Assign n.Teachers (2 classroom in a given school-grade)
>> Allpairs <- as.data.frame(t(combn(SchGrClssTmp, 2)))
>> AllpairsTmp <- paste(Allpairs$V1, Allpairs$V2, sep=" ")
>>
>> library(stringr)
>> separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
>> "-"))
>> separoPairs <- as.data.frame(t(separoPairs))
>> row.names(separoPairs) <- NULL
>> separoPairs <- separoPairs %>% select(-V7)  %>%  #Drops empty column
>>   mutate(V1=as.character(V1), V4=as.character(V4), V2=as.numeric(V2),
>> V5=as.numeric(V5)) %>% mutate(V4 = trimws(V4, which = "both"))
>>
>> separoPairs[120,]$V4#Only the rows with V1=V4 and V2=V5 are valid
>
>
>> validPairs <- separoPairs %>% filter(V1==V4 & V2==V5) %>% select(V1, V2,
>> V3, V6)
>> # Generate n.Teachers
>>
>> gen.teachers <- function(n.teachers){
>>   Teacher.ID <- gen.names(n = n.teachers, name.order = "last.first")
>>   Teacher.exp <- runif(n = n.teachers, min = 1, max = 30)
>>   Teacher.Other <- sample(c(0,1), replace = T, prob = c(0.5, 0.5),
>> size = n.teachers)
>>   Teacher.RE <- rnorm(n = n.teachers, mean = 0, sd = 1)
>>   Teachers <- data.frame(Teacher.ID, Teacher.exp, Teacher.Other,
>> Teacher.RE)
>>   return(Teachers)}
>> Teachers <- gen.teachers(n.teachers = n.Teachers) %>%
>>   mutate(Teacher.ID = as.character(Teacher.ID))
>> # Randomly assign n.Teachers teachers to the "ValidPairs"
>> TmpAssignments <- validPairs[sample(1:nrow(validPairs), n.Teachers), ]
>> Assignments <- cbind.data.frame(Teachers$Teacher.ID, TmpAssignments)
>> names(Assignments) <- c("Teacher.ID", "School.ID", "Grade", "Class_1",
>> "Class_2")
>> # Tidy Data
>> library(tidyr)
>> TeacherClassroom <- Assignments %>%
>>   gather(x, Classroom, Class_1,Class_2) %>%
>>   select(-x) %>%
>>   mutate(Teacher.ID = as.character(Teacher.ID))
>> # Merge
>> DF_Classrooms <- TeacherClassroom %>% full_join(Teachers,
>> by="Teacher.ID") %>% full_join(Schools, by="School.ID")
>> rm(list=setdiff(ls(), "DF_Classrooms")) # Clean the work space!
>>
>> *I want to end up with the same*  'DF_Classrooms *data frame* but getting
>
>
>> there in a more efficient way. In particular, when is use n.Classrooms
>> <-4 the
>>
> code run fast, but *if I increase it to something like 20 it is painfully
>> slow.*
>>
>> Thanks!!!
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From ltcorredorb at gmail.com  Thu Jul 16 14:21:18 2015
From: ltcorredorb at gmail.com (=?UTF-8?Q?Laura_Teresa_Corredor_Boh=C3=B3rquez?=)
Date: Thu, 16 Jul 2015 14:21:18 +0200
Subject: [R] nls singular gradient matrix - fit parameters in integral's
	upper limits
Message-ID: <CAPTHsVvg-dDEsWd0+qa_rtvCyrncDr5Po7b+QbxeDBNRkduzmQ@mail.gmail.com>

-------The las post rejected two files I had attached, so I modified
it.---------------


Hi. I am trying to make a nls fit for a little bit complicated expression that
includes two integrals with two of the fit parameters in their upper limits.

I got the error "Error in nlsModel(formula, mf, start, wts) :
singular gradient
matrix at initial parameter estimates". First of all, I have searched
already in the previous answers, but didn?t help. The parameters initialization
seems to be ok, I have tried to change the parameters but no one works. If
my function has just one integral everything works very nicely, but when adding
a second integral term just got the error. I don?t believe the function is
over-parametrized, as I have performed other fits with much more parameters
and they worked. I did try to enclose the data but the attachment was
rejected.

The minimal example is the following:

# read the data from a csv file
dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
x = 0*(1:97)
y = 0*(1:97)
for(i in 1:97){
  x[i] = dados[i,1]
  y[i] = dados[i,2]
}
integrand <- function(X) {
  return(X^4/(2*sinh(X/2))^2)
}
fitting = function(T1, T2, N, D, x){
  int1 = integrate(integrand, lower=0, upper = T1)$value
  int2 = integrate(integrand, lower=0, upper = T2)$value
  return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2
)+(448.956*(x/T1)^3*int1)+(299.304*(x/T2)^3*int2))
}
fit = nls(y ~ fitting(T1, T2, N, D, x),
start=list(T1=400,T2=200,N=0.01,D=2))

------>For reference, the fit that worked is the following:

# read the data from a csv file
dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
x = 0*(1:97)
y = 0*(1:97)
for(i in 1:97){
  x[i] = dados[i,1]
  y[i] = dados[i,2]
}
integrand <- function(X) {
  return(X^4/(2*sinh(X/2))^2)
}
fitting = function(T1, N, D, x){
  int = integrate(integrand, lower=0, upper = T1)$value
  return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2 )+(748.26)*(x/T1)^3*int)
}
fit = nls(y ~ fitting(T1 , N, D, x), start=list(T1=400,N=0.01,D=2))


I cannot figure out what happen. I need to perform this fit for three
integral components, but even for two I have this problem. I appreciate so
much your help. Thank you.

	[[alternative HTML version deleted]]


From prabir111 at gmail.com  Thu Jul 16 08:45:21 2015
From: prabir111 at gmail.com (prabir das)
Date: Thu, 16 Jul 2015 12:15:21 +0530
Subject: [R] (no subject)
Message-ID: <CAN_n9ha32ObnMtKirywTAAOWxznYc-zAp6RpdvqEJzbtcK-AQA@mail.gmail.com>

I am trying to analyse time-series .netcdf (3D lat,long and time domain)
climate data. I want to apply the SPEI package (calculation of standardized
precipitation evapotranspiration index) on it. But unable to arrange my data
in the required data frame. As I am a beginner in R, it will be very much
helpful if someone provide me the details of the code to be written before
executing the package.

The details of SPEI proggrame is as follows:

spei(data, scale, kernel = list(type = 'rectangular', shift = 0),
distribution = 'log-Logistic', fit = 'ub-pwm', na.rm = FALSE,
ref.start=NULL, ref.end=NULL, x=FALSE, params=NULL, ...)


Thanks in advance.

-- 
Prabir Kumar Das
Scientist/Engineer - SD
RRSC - East
NRSC, ISRO
Dept. of Space, Govt. of India
Kolkata

	[[alternative HTML version deleted]]


From profjcnash at gmail.com  Thu Jul 16 16:36:58 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Thu, 16 Jul 2015 10:36:58 -0400
Subject: [R] nls singular gradient matrix - fit parameters in integral's
 upper limits
In-Reply-To: <CAPTHsVvg-dDEsWd0+qa_rtvCyrncDr5Po7b+QbxeDBNRkduzmQ@mail.gmail.com>
References: <CAPTHsVvg-dDEsWd0+qa_rtvCyrncDr5Po7b+QbxeDBNRkduzmQ@mail.gmail.com>
Message-ID: <55A7C18A.6020901@gmail.com>

The list rejects almost all attachments.

You could dput the data and put it in your posting.

You may also want to try a Marquardt solver. In R from my nlmrt or
compiled in Kate Mullen's minpack.lm. They are slightly different in
flavour and the call is a bit different from nls.

JN

On 15-07-16 08:21 AM, Laura Teresa Corredor Boh?rquez wrote:
> -------The las post rejected two files I had attached, so I modified
> it.---------------
> 
> 
> Hi. I am trying to make a nls fit for a little bit complicated expression that
> includes two integrals with two of the fit parameters in their upper limits.
> 
> I got the error "Error in nlsModel(formula, mf, start, wts) :
> singular gradient
> matrix at initial parameter estimates". First of all, I have searched
> already in the previous answers, but didn?t help. The parameters initialization
> seems to be ok, I have tried to change the parameters but no one works. If
> my function has just one integral everything works very nicely, but when adding
> a second integral term just got the error. I don?t believe the function is
> over-parametrized, as I have performed other fits with much more parameters
> and they worked. I did try to enclose the data but the attachment was
> rejected.
> 
> The minimal example is the following:
> 
> # read the data from a csv file
> dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
> x = 0*(1:97)
> y = 0*(1:97)
> for(i in 1:97){
>   x[i] = dados[i,1]
>   y[i] = dados[i,2]
> }
> integrand <- function(X) {
>   return(X^4/(2*sinh(X/2))^2)
> }
> fitting = function(T1, T2, N, D, x){
>   int1 = integrate(integrand, lower=0, upper = T1)$value
>   int2 = integrate(integrand, lower=0, upper = T2)$value
>   return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2
> )+(448.956*(x/T1)^3*int1)+(299.304*(x/T2)^3*int2))
> }
> fit = nls(y ~ fitting(T1, T2, N, D, x),
> start=list(T1=400,T2=200,N=0.01,D=2))
> 
> ------>For reference, the fit that worked is the following:
> 
> # read the data from a csv file
> dados = read.csv("file.csv", header=FALSE, stringsAsFactors=FALSE)
> x = 0*(1:97)
> y = 0*(1:97)
> for(i in 1:97){
>   x[i] = dados[i,1]
>   y[i] = dados[i,2]
> }
> integrand <- function(X) {
>   return(X^4/(2*sinh(X/2))^2)
> }
> fitting = function(T1, N, D, x){
>   int = integrate(integrand, lower=0, upper = T1)$value
>   return(N*(D/x)^2*(exp(D/x)/(1+exp(D/x))^2 )+(748.26)*(x/T1)^3*int)
> }
> fit = nls(y ~ fitting(T1 , N, D, x), start=list(T1=400,N=0.01,D=2))
> 
> 
> I cannot figure out what happen. I need to perform this fit for three
> integral components, but even for two I have this problem. I appreciate so
> much your help. Thank you.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From macqueen1 at llnl.gov  Thu Jul 16 16:49:54 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 16 Jul 2015 14:49:54 +0000
Subject: [R] Problem Regarding R Packages installation
In-Reply-To: <BLU175-W33A30E6F9B2EE7CFAA9CFAAF990@phx.gbl>
References: <BLU175-W33A30E6F9B2EE7CFAA9CFAAF990@phx.gbl>
Message-ID: <D1CD120B.13190F%macqueen1@llnl.gov>

the first error message is
  "configuration failed for package RCurl"
immediately before that it said
  "Cannot find curl-config".

It would appear that you don't have curl installed on your computer.
Unfortunately, I cannot help you with that. But you might look at the
documentation for RCurl (available from CRAN) to start trying to find
where to get curl.

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/15/15, 8:23 PM, "R-help on behalf of Muhammad Sohail Raza"
<r-help-bounces at r-project.org on behalf of muhammadsohailraza at live.com>
wrote:

>Hi Everyone!
>I am trying to install R Package "VariantAnnotation". I entered commands:
>
>source("http://bioconductor.org/biocLite.R")
> biocLite("VariantAnnotation")
>
>
>But i am getting the following errors,
>
>ERROR: configuration failed for package ?XML?
>* removing ?/usr/lib64/R/library/XML?
>* installing *source* package ?RCurl? ...
>** package ?RCurl? successfully unpacked and MD5 sums checked
>configure: loading site script /usr/share/site/x86_64-unknown-linux-gnu
>checking for curl-config... no
>Cannot find curl-config
>ERROR: configuration failed for package ?RCurl?
>* removing ?/usr/lib64/R/library/RCurl?
>* installing *source* package ?Rsamtools? ...
>** libs
>gcc -std=gnu99 -I/usr/lib64/R/include -DNDEBUG  -I/usr/local/include
>-I"/usr/lib64/R/library/S4Vectors/include"
>-I"/usr/lib64/R/library/IRanges/include"
>-I"/usr/lib64/R/library/XVector/include"
>-I"/usr/lib64/R/library/Biostrings/include"  -fopenmp -D_USE_KNETFILE
>-D_FILE_OFFSET_BITS=64 -U_FORTIFY_SOURCE -DBGZF_CACHE
>-Dfprintf=_samtools_fprintf -Dexit=_samtools_exit -Dabort=_samtools_abort
>-I./samtools -I./samtools/bcftools -I./tabix -fpic  -fmessage-length=0
>-grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector
>-funwind-tables -fasynchronous-unwind-tables  -c Biostrings_stubs.c -o
>Biostrings_stubs.o
>gcc -std=gnu99 -I/usr/lib64/R/include -DNDEBUG  -I/usr/local/include
>-I"/usr/lib64/R/library/S4Vectors/include"
>-I"/usr/lib64/R/library/IRanges/include"
>-I"/usr/lib64/R/library/XVector/include"
>-I"/usr/lib64/R/library/Biostrings/include"  -fopenmp -D_USE_KNETFILE
>-D_FILE_OFFSET_BITS=64 -U_FORTIFY_SOURCE -DBGZF_CACHE
>-Dfprintf=_samtools_fprintf -Dexit=_samtools_exit -Dabort=_samtools_abort
>-I./samtools -I./samtools/bcftools -I./tabix -fpic  -fmessage-length=0
>-grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector
>-funwind-tables -fasynchronous-unwind-tables  -c IRanges_stubs.c -o
>IRanges_stubs.o
>g++ -I/usr/lib64/R/include -DNDEBUG  -I/usr/local/include
>-I"/usr/lib64/R/library/S4Vectors/include"
>-I"/usr/lib64/R/library/IRanges/include"
>-I"/usr/lib64/R/library/XVector/include"
>-I"/usr/lib64/R/library/Biostrings/include"   -fpic  -fmessage-length=0
>-grecord-gcc-switches -O2 -Wall -D_FORTIFY_SOURCE=2 -fstack-protector
>-funwind-tables -fasynchronous-unwind-tables  -c PileupBuffer.cpp -o
>PileupBuffer.o
>/bin/sh: g++: command not found
>/usr/lib64/R/etc/Makeconf:143: recipe for target 'PileupBuffer.o' failed
>make: *** [PileupBuffer.o] Error 127
>ERROR: compilation failed for package ?Rsamtools?
>* removing ?/usr/lib64/R/library/Rsamtools?
>* installing *source* package ?futile.logger? ...
>** package ?futile.logger? successfully unpacked and MD5 sums checked
>** R
>** preparing package for lazy loading
>** help
>*** installing help indices
>** building package indices
>** testing if installed package can be loaded
>* DONE (futile.logger)
>ERROR: dependencies ?XML?, ?RCurl? are not available for package ?biomaRt?
>* removing ?/usr/lib64/R/library/biomaRt?
>* installing *source* package ?BiocParallel? ...
>** R
>** inst
>** preparing package for lazy loading
>** help
>*** installing help indices
>** building package indices
>** installing vignettes
>** testing if installed package can be loaded
>* DONE (BiocParallel)
>ERROR: dependency ?Rsamtools? is not available for package
>?GenomicAlignments?
>* removing ?/usr/lib64/R/library/GenomicAlignments?
>ERROR: dependencies ?XML?, ?RCurl?, ?Rsamtools?, ?GenomicAlignments? are
>not available for package ?rtracklayer?
>* removing ?/usr/lib64/R/library/rtracklayer?
>ERROR: dependencies ?rtracklayer?, ?Rsamtools? are not available for
>package ?BSgenome?
>* removing ?/usr/lib64/R/library/BSgenome?
>ERROR: dependencies ?rtracklayer?, ?biomaRt?, ?RCurl? are not available
>for package ?GenomicFeatures?
>* removing ?/usr/lib64/R/library/GenomicFeatures?
>ERROR: dependencies ?Rsamtools?, ?BSgenome?, ?rtracklayer?,
>?GenomicFeatures? are not available for package ?VariantAnnotation?
>* removing ?/usr/lib64/R/library/VariantAnnotation?
>
>The downloaded source packages are in
>        ?/tmp/RtmpvpoYjO/downloaded_packages?
>Updating HTML index of packages in '.Library'
>Making 'packages.html' ... done
>Warning messages:
>1: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?XML? had non-zero exit status
>2: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?RCurl? had non-zero exit status
>3: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?Rsamtools? had non-zero exit status
>4: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?biomaRt? had non-zero exit status
>5: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?GenomicAlignments? had non-zero exit status
>6: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?rtracklayer? had non-zero exit status
>7: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?BSgenome? had non-zero exit status
>8: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?GenomicFeatures? had non-zero exit status
>9: In install.packages(pkgs = doing, lib = lib, ...) :
>  installation of package ?VariantAnnotation? had non-zero exit status
>
>
>I don't know how to deal with it, can anyone helpo me how to deal with it?
>
>Thanks!
>
>
>**************************************************************************
>**********
>Muhammad Sohail RazaBeijing Institute of Genomics, CASBeijing,
>China.Phone: +8613552957083
>email:   sohail at big.ac.cn              muhammadsohailraza at live.com 		 	
>		  
>	[[alternative HTML version deleted]]
>


From lid.zigh at gmail.com  Thu Jul 16 16:58:25 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Thu, 16 Jul 2015 09:58:25 -0500
Subject: [R] removing the columns with 0 or NA or 1or NA or 2 or NA
Message-ID: <CAMqbV1DB-6mgUup_F5rT_NRi=LNXo=OT6Rggv+48sjh08d-2Sw@mail.gmail.com>

I have ma matrix which its elements are NA,0,1,2 ! I got my answer bout
removing the columns with 0 or NA or both values but now I want to add
additional condition for deleting the columns! I have to delete the columns
which contain the same value. delete the columns with NA or 0 or both and
the columns with NA or 1 or both and the column with NA or 2 or both (I
should keep the columns which have variation in their values)! I use this
code but didn't work properly:

mat_nonNA<- mat[, !apply((is.na(mat) | mat == 0) & (is.na(mat) | mat==1) &(
is.na(mat) | mat==2), 2, all)]

mat
         1:110590170    1:110888172     1:110906406   1:110993854
 1:110996710   1:111144756
A05363           1                       1                     1
          2                         NA
0
A05370           0                       1
0                NA                         0                     NA
A05380           1
         NA                   2                NA
  NA
0
A05397           0                        1
0                NA                         0                       2
A05400           2                        1
0                 2                           0                       0
A05426
0                       NA                     NA             NA
0                       1

my out put should be like below:

       1:110590170         1:110906406          1:111144756
A05363           1                         1
0
A05370           0                          0
NA
A05380           1                          2
0
A05397           0
0                                 2
A05400           2                          0
          0
A05426           0                         NA
1

Thanks for your help

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu Jul 16 17:04:10 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 16 Jul 2015 11:04:10 -0400
Subject: [R] R 3.2.0 - Windows 8,
 1 - SSL certificate problem installing course in swirl
In-Reply-To: <000a01d0bf96$438eb550$caac1ff0$@zeelandnet.nl>
References: <000a01d0bf96$438eb550$caac1ff0$@zeelandnet.nl>
Message-ID: <55A7C7EA.7050602@gmail.com>

On 16/07/2015 3:08 AM, MH wrote:
> Hello,
> 
> I try to install a course for swirl and got a SSL problem:
> 
>> install_from_swirl("R Programming")
> Error in function (type, msg, asError = TRUE)  : 
>   SSL certificate problem: self signed certificate in certificate chain
> 
> Found this answer with Google but does not work either:
> 
>> set_config( config( ssl.verifypeer = 0L ) )
> Error: could not find function "set_config"
> 
> What do I do to avoid or accept the SSL certificate?

We don't know what "install_from_swirl" is doing, so it's hard to say.
You should ask its author.

If it is using the "wininet" method to download from an https site, then
it is Windows that is issuing the error:  you may be able to configure
Internet Explorer to avoid it.  Ask Microsoft how.

Duncan Murdoch


From dimitri.liakhovitski at gmail.com  Thu Jul 16 17:10:36 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 16 Jul 2015 11:10:36 -0400
Subject: [R] Weighted skewness and curtosis
Message-ID: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>

Is there an R package that allows one to calculate skewness and
curtosis - but weighted with individual level weights (one weight per
observation)?

Thank you!

-- 
Dimitri Liakhovitski


From dwinsemius at comcast.net  Thu Jul 16 17:27:21 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 16 Jul 2015 08:27:21 -0700
Subject: [R] Weighted skewness and curtosis
In-Reply-To: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
Message-ID: <2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>


On Jul 16, 2015, at 8:10 AM, Dimitri Liakhovitski wrote:

> Is there an R package that allows one to calculate skewness and
> curtosis - but weighted with individual level weights (one weight per
> observation)?
> 

Integer weights?

-- 

David Winsemius
Alameda, CA, USA


From dimitri.liakhovitski at gmail.com  Thu Jul 16 17:37:25 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 16 Jul 2015 11:37:25 -0400
Subject: [R] Weighted skewness and curtosis
In-Reply-To: <2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
Message-ID: <CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>

Unfortunately not - more like 0.7654, 1.2345.
I understand that I could multiply each number by 100, round it to no
decimal point and then unroll my data in proportion.
I was just hoping someone has done it in C and put it into a package...

On Thu, Jul 16, 2015 at 11:27 AM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
> On Jul 16, 2015, at 8:10 AM, Dimitri Liakhovitski wrote:
>
>> Is there an R package that allows one to calculate skewness and
>> curtosis - but weighted with individual level weights (one weight per
>> observation)?
>>
>
> Integer weights?
>
> --
>
> David Winsemius
> Alameda, CA, USA
>



-- 
Dimitri Liakhovitski


From bgunter.4567 at gmail.com  Thu Jul 16 17:45:12 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 16 Jul 2015 08:45:12 -0700
Subject: [R] Weighted skewness and curtosis
In-Reply-To: <CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
	<CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
Message-ID: <CAGxFJbQHSx+7j3ssErbrkmANyg4Kb33OnHW1OH=U+aWeWqpTiQ@mail.gmail.com>

rep() **does** do it essentially in C !!

See also the "moments" package, which I found instantly by googling
"sample moments in R", though I don't know whether it does what you
want (but probably shouldn't do).

Of course, sample skewness and kurtosis are basically useless, but
that's another, off topic, issue.


Cheers,
Bert


Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 16, 2015 at 8:37 AM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Unfortunately not - more like 0.7654, 1.2345.
> I understand that I could multiply each number by 100, round it to no
> decimal point and then unroll my data in proportion.
> I was just hoping someone has done it in C and put it into a package...
>
> On Thu, Jul 16, 2015 at 11:27 AM, David Winsemius
> <dwinsemius at comcast.net> wrote:
>>
>> On Jul 16, 2015, at 8:10 AM, Dimitri Liakhovitski wrote:
>>
>>> Is there an R package that allows one to calculate skewness and
>>> curtosis - but weighted with individual level weights (one weight per
>>> observation)?
>>>
>>
>> Integer weights?
>>
>> --
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>
>
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Thu Jul 16 18:43:41 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Thu, 16 Jul 2015 12:43:41 -0400
Subject: [R] Weighted skewness and curtosis
In-Reply-To: <CAGxFJbQHSx+7j3ssErbrkmANyg4Kb33OnHW1OH=U+aWeWqpTiQ@mail.gmail.com>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
	<CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
	<CAGxFJbQHSx+7j3ssErbrkmANyg4Kb33OnHW1OH=U+aWeWqpTiQ@mail.gmail.com>
Message-ID: <CAN2xGJbXgZ4gDV0fG22-MD=NFBKCbx4Cp=6nGcbEzaAqJy1EhA@mail.gmail.com>

Thank you!

On Thu, Jul 16, 2015 at 11:45 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> rep() **does** do it essentially in C !!
>
> See also the "moments" package, which I found instantly by googling
> "sample moments in R", though I don't know whether it does what you
> want (but probably shouldn't do).
>
> Of course, sample skewness and kurtosis are basically useless, but
> that's another, off topic, issue.
>
>
> Cheers,
> Bert
>
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Jul 16, 2015 at 8:37 AM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Unfortunately not - more like 0.7654, 1.2345.
>> I understand that I could multiply each number by 100, round it to no
>> decimal point and then unroll my data in proportion.
>> I was just hoping someone has done it in C and put it into a package...
>>
>> On Thu, Jul 16, 2015 at 11:27 AM, David Winsemius
>> <dwinsemius at comcast.net> wrote:
>>>
>>> On Jul 16, 2015, at 8:10 AM, Dimitri Liakhovitski wrote:
>>>
>>>> Is there an R package that allows one to calculate skewness and
>>>> curtosis - but weighted with individual level weights (one weight per
>>>> observation)?
>>>>
>>>
>>> Integer weights?
>>>
>>> --
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>
>>
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From dwinsemius at comcast.net  Thu Jul 16 18:47:00 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 16 Jul 2015 09:47:00 -0700
Subject: [R] Weighted skewness and curtosis
In-Reply-To: <CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
	<CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
Message-ID: <49726BCD-816E-413E-87B6-65A9795F60E6@comcast.net>


On Jul 16, 2015, at 8:37 AM, Dimitri Liakhovitski wrote:

> Unfortunately not - more like 0.7654, 1.2345.
> I understand that I could multiply each number by 100, round it to no
> decimal point and then unroll my data in proportion.
> I was just hoping someone has done it in C and put it into a package...

I saw Bert's comments but I don't think that `rep` handles fractional weights. Take a look at Hmisc::wtd.var (since I have that package loaded and see that code is easy to grasp).

 Also appears after after using sos::findFn("weighted moments") that the package:lmomco deserves your attention.

> 
> On Thu, Jul 16, 2015 at 11:27 AM, David Winsemius
> <dwinsemius at comcast.net> wrote:
>> 
>> On Jul 16, 2015, at 8:10 AM, Dimitri Liakhovitski wrote:
>> 
>>> Is there an R package that allows one to calculate skewness and
>>> curtosis - but weighted with individual level weights (one weight per
>>> observation)?
>>> 
>> 
>> Integer weights?
>> 
>> --
>> 
>> David Winsemius


David Winsemius
Alameda, CA, USA


From oluola2011 at yahoo.com  Thu Jul 16 19:11:05 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Thu, 16 Jul 2015 10:11:05 -0700
Subject: [R] NLOPTR
Message-ID: <1437066665.2651.YahooMailBasic@web161603.mail.bf1.yahoo.com>

Hello,
I have been running a nonlinear GMM using the nloptr wrapper since the last 7 days. The maximum time I have to run the code on the server that I am using to run this code is 7 days which expires in about an hour when the server automatically terminates it. I will like to know if there is a way that I can obtain the estimates at the point of termination so that I can use these estimates as the new starting values for another round of estimation.

Thank you.


From bgunter.4567 at gmail.com  Thu Jul 16 19:12:10 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 16 Jul 2015 10:12:10 -0700
Subject: [R] Weighted skewness and curtosis
In-Reply-To: <49726BCD-816E-413E-87B6-65A9795F60E6@comcast.net>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
	<CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
	<49726BCD-816E-413E-87B6-65A9795F60E6@comcast.net>
Message-ID: <CAGxFJbTQXta3E92i_Vcv2TAsRYZXQ9bzaDsX+WJaxwyanqsJMQ@mail.gmail.com>

David:

I think you missed my point. As I understand him, Dmitri mentioned
rounding off to e.g. 2 decimal digits and multiplying by 100 to
produce integer weights, which would then lead to "unrolling" the
vector via rep().  Your "weighted moments" reference is probably
closer to what he sought, however.

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 16, 2015 at 9:47 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>
> On Jul 16, 2015, at 8:37 AM, Dimitri Liakhovitski wrote:
>
>> Unfortunately not - more like 0.7654, 1.2345.
>> I understand that I could multiply each number by 100, round it to no
>> decimal point and then unroll my data in proportion.
>> I was just hoping someone has done it in C and put it into a package...
>
> I saw Bert's comments but I don't think that `rep` handles fractional weights. Take a look at Hmisc::wtd.var (since I have that package loaded and see that code is easy to grasp).
>
>  Also appears after after using sos::findFn("weighted moments") that the package:lmomco deserves your attention.
>
>>
>> On Thu, Jul 16, 2015 at 11:27 AM, David Winsemius
>> <dwinsemius at comcast.net> wrote:
>>>
>>> On Jul 16, 2015, at 8:10 AM, Dimitri Liakhovitski wrote:
>>>
>>>> Is there an R package that allows one to calculate skewness and
>>>> curtosis - but weighted with individual level weights (one weight per
>>>> observation)?
>>>>
>>>
>>> Integer weights?
>>>
>>> --
>>>
>>> David Winsemius
>
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Thu Jul 16 19:18:29 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 16 Jul 2015 10:18:29 -0700
Subject: [R] NLOPTR
In-Reply-To: <1437066665.2651.YahooMailBasic@web161603.mail.bf1.yahoo.com>
References: <1437066665.2651.YahooMailBasic@web161603.mail.bf1.yahoo.com>
Message-ID: <CAGxFJbR+e=5H-+THhcvu=k87QsW6OcktEydTKU-gEdGV0vqQ-A@mail.gmail.com>

1. I have no idea.

2. However, I doubt that your strategy would work anyway. If there is
not an outright error, you are probably stuck in some endless loop or
are wandering around at random on an essentially "flat" hypersurface.
You would need to change convergence criteria or change the
parameterization of and/or simplify your model before proceeding if
that is the case. Although I would have thought you would have run up
against some sort of maximum iterations limit...

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 16, 2015 at 10:11 AM, Olu Ola via R-help
<r-help at r-project.org> wrote:
> Hello,
> I have been running a nonlinear GMM using the nloptr wrapper since the last 7 days. The maximum time I have to run the code on the server that I am using to run this code is 7 days which expires in about an hour when the server automatically terminates it. I will like to know if there is a way that I can obtain the estimates at the point of termination so that I can use these estimates as the new starting values for another round of estimation.
>
> Thank you.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From therneau at mayo.edu  Thu Jul 16 19:28:28 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 16 Jul 2015 12:28:28 -0500
Subject: [R] matrix manipulation
Message-ID: <2f3a88$11iup2@ironport10.mayo.edu>

This is as much a mathematics as an R question, in the "this should be easy but I don't 
see it" category.

Assume I have a full rank p by p matrix V  (aside: V = (X'X)^{-1} for a particular setup), 
a p by k matrix B, and I want to complete an orthagonal basis for the space with distance 
function V.  That is, find A such that t(B) %*% V %*% A =0, where A has p rows and p-k 
columns.

With V=identity this is easy. I can do it in 1-2 lines using qr(), lm(), or several other 
tools.  A part of me is quite certain that the general problem isn't more than 3 lines of 
R, but after a day of beating my head on the issue I still don't see it.  Math wise it 
looks like a simple homework problem in a mid level class, but I'm not currently sure that 
I'd pass said class.

If someone could show the way I would be grateful.  Either that or assurance that the 
problem actually IS hard and I'm not as dense as I think.

Terry T.


From wolfgang.raffelsberger at gmail.com  Thu Jul 16 19:41:04 2015
From: wolfgang.raffelsberger at gmail.com (Wolfgang Raffelsberger)
Date: Thu, 16 Jul 2015 19:41:04 +0200
Subject: [R] (ordinal) logistic regression
Message-ID: <CALDESV_Y7N7Zj7U3bQOj+c6BwQuJSGXM7z-Q9zFkN6N261B+Jg@mail.gmail.com>

Dear list,
I've been looking at previous posts on the list, but I haven't found any
close enough to my question/problem.
My data can be seen as a matrix of mutiple individuals (columns) with
(rather independent) measures (lines). Now based on supplemental
information, the individuals are organized in (multiple) ordered classes.
Now I would like to test for which type of measure (ie which line form my
matrix of data) the groups are distinct (eg different by group-mean). In
other words, I would like to see in which line of my input matrix the
measures for the groups of individuals associate to distinct group-values.

So I tried glm (or glm2) on each line of the matrix, but in my toy-example
(shown below) I'm surprised to get warnings about not finding convergence
in the "nice" toy-cases (ie distinct groups as I am looking for),e even
with glm2 ! I see in such "nice" cases with glm() the "Pr(>|z|)" is close
to 1, which in first sight is OK (since: H0 : coefficient =0), but I
suppose the test is not really set up right this way.  When trying lrm (rms
package) I even get an error message (Unable to fit model using ?lrm.fit?)
with the "nice" cases.
In my real data with >4000 lines of data (ie >4000 glm tests) multiple
testing correction would transform everything from 1-p to end up at p=1, so
that?s another problem with this approach.
I suppose somehow I should transform my data (I don't see where I would
change the H0 ?) to obtain low and NOT high p-values (example below) in the
case I'm looking for, ie when group-means are distinct.

Any suggestions ?

Thank?s in advance,
Wolfgang

Here my toy-example :
datB1 <- c(12,14:16,18:21,20:22,20,22:24,19.5)   # fit
partially/overlapping to 3grp model
datB2 <- c(11:15,21:25,31:36)                    # too beautiful to be real
...
datB3 <- c(15:12,11:15,12:14,15:12)              # no fit to 3grp model
datB4 <- c(11:15,15:11,21:26)                    # grpA == grpB but grpA !=
grpC

datB <- rbind(datB1,datB2,datB3,datB4)
set.seed(2015)
datB <- datB + round(runif(length(datB),-0.3,0.3),1)  # add some noise
datB <- datB - rowMeans(datB)                         # centering
## here the definition of the groups
grpB <- gl(3,6,labels=LETTERS[1:3])[c(-6,-7)]
   table(grpB)

## display
layout(1:4)
for(i in 1:4) plot(datB[i,],as.numeric(grpB))

## now the 'test'
glmLi <- function(dat,grp) {
  ## run glm : predict grp based on dat
  dat <- data.frame(dat=dat,grp=grp)
  glm(grp ~ dat, data=dat, family="binomial")}

logitB <- apply(datB,1,glmLi,grpB)
lapply(logitB,summary)
sapply(logitB,function(x) summary(x)$coefficients[,4])  # lines 1 & 2 are
designed to be 'positive' but give high p-values with convergence problem

## for completness
sessionInfo()
R version 3.2.0 (2015-04-16)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=French_France.1252  LC_CTYPE=French_France.1252
LC_MONETARY=French_France.1252
[4] LC_NUMERIC=C                   LC_TIME=French_France.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] glm2_1.1.2      MASS_7.3-40     TinnRcom_1.0.18 formatR_1.2
svSocket_0.9-57

loaded via a namespace (and not attached):
[1] tools_3.2.0   svMisc_0.9-70 tcltk_3.2.0

	[[alternative HTML version deleted]]


From jholtman at gmail.com  Thu Jul 16 19:37:05 2015
From: jholtman at gmail.com (jim holtman)
Date: Thu, 16 Jul 2015 13:37:05 -0400
Subject: [R] Speeding up code?
In-Reply-To: <CAJA1VFxU6p7PvPpbJ0Zmy-NCKBXzh3gwGvov2dRf97-B10-GPQ@mail.gmail.com>
References: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>
	<CAE=6FXahjzNEtXm4w3Ye625xieHpPmRfhrVLwc-gqBVgLznq6g@mail.gmail.com>
	<CAJA1VFxU6p7PvPpbJ0Zmy-NCKBXzh3gwGvov2dRf97-B10-GPQ@mail.gmail.com>
Message-ID: <CAAxdm-5tMQ+vD8joQ0CK=DGbFgmkjWcdH4+a7GZh1+j_1v8jsw@mail.gmail.com>

Here is one improvement.  Avoid dataframes in some of these cases.  This
create a character matrix and then converts to a dataframe after doing the
transpose of the matrix.  This just takes less than 10 seconds on my system:


>  library(stringr)
>  # create character matrix; avoid dataframes in this case
>  print(proc.time())
   user  system elapsed
  15.52    5.24  587.70
>  xm <- do.call(rbind, str_split(string = AllpairsTmp, pattern = "-"))
>  # convert to dataframe and do transpose on matrix and not dataframe
>  separoPairs <- as.data.frame(t(xm), stringsAsFactors = FALSE)
>  print(proc.time()
+
+ )
   user  system elapsed
  20.90    5.36  596.57
>


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 16, 2015 at 7:56 AM, Ignacio Martinez <ignacio82 at gmail.com>
wrote:

> Hi Collin,
>
> The objective of the gen.names function is to generate N *unique *random
> names, where N is a *large *number. In my computer `gen.names(n = 50000)`
> takes under a second, so is probably not the root problem in my code. That
> said, I would love to improve it. I'm not exactly sure how you propose to
> change it using sample. What is the object that I would be sampling? I
> would love to run a little benchmark to compare my version with yours.
>
> What really takes a long time to run is:
>
>     separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
> "-"))
>
> So that and the chunk of code before that is probably where I would get big
> gains in speed. Sadly, I have no clue how to do it differently
>
> Thanks a lot for the help!!
>
> Ignacio
>
>
> On Wed, Jul 15, 2015 at 11:34 PM Collin Lynch <cflynch at ncsu.edu> wrote:
>
> > Hi Ignacio, If I am reading your code correctly then the top while loop
> is
> > essentially seeking to select a random set of names from the original
> set,
> > then using unique to reduce it down, you then iterate until you have
> built
> > your quota.  Ultimately this results in a very inefficient attempt at
> > sampling without replacement.  Why not just sample without replacement
> > rather than loop iteratively and use unique?  Or if the set of possible
> > names are short enough why not just randomize it and then pull the first
> n
> > items off?
> >
> >     Best,
> >     Collin.
> >
> > On Wed, Jul 15, 2015 at 11:15 PM, Ignacio Martinez <ignacio82 at gmail.com>
> > wrote:
> >
> >> Hi R-Help!
> >>
> >> I'm hoping that some of you may give me some tips that could make my
> code
> >>
> > more efficient. More precisely, I would like to make the answer to my
> >> stakoverflow
> >> <
> >>
> http://stackoverflow.com/questions/31137940/randomly-assign-teachers-to-classrooms-imposing-restrictions
> >> >
> >
> >
> >> question more efficient.
> >>
> >> This is the code:
> >>
> >> library(dplyr)
> >> library(randomNames)
> >> library(geosphere)
> >>
> > set.seed(7142015)# Define Parameters
> >
> >
> >> n.Schools <- 20
> >> first.grade<-3
> >> last.grade<-5
> >> n.Grades <-last.grade-first.grade+1
> >> n.Classrooms <- 20 # THIS IS WHAT I WANTED TO BE ABLE TO CHANGE
> >> n.Teachers <- (n.Schools*n.Grades*n.Classrooms)/2 #Two classrooms per
> >> teacher
> >> # Define Random names function:
> >> gen.names <- function(n, which.names = "both", name.order =
> "last.first"){
> >>   names <- unique(randomNames(n=n, which.names = which.names,
> >> name.order = name.order))
> >>   need <- n - length(names)
> >>   while(need>0){
> >>     names <- unique(c(randomNames(n=need, which.names = which.names,
> >> name.order = name.order), names))
> >>     need <- n - length(names)
> >>   }
> >>   return(names)}
> >> # Generate n.Schools names
> >> gen.schools <- function(n.schools) {
> >>   School.ID <-
> >>     paste0(gen.names(n = n.schools, which.names = "last"), ' School')
> >>   School.long <- rnorm(n = n.schools, mean = 21.7672, sd = 0.025)
> >>   School.lat <- rnorm(n = n.schools, mean = 58.8471, sd = 0.025)
> >>   School.RE <- rnorm(n = n.schools, mean = 0, sd = 1)
> >>   Schools <-
> >>     data.frame(School.ID, School.lat, School.long, School.RE) %>%
> >>     mutate(School.ID = as.character(School.ID)) %>%
> >>     rowwise() %>%  mutate (School.distance = distHaversine(
> >>       p1 = c(School.long, School.lat),
> >>       p2 = c(21.7672, 58.8471), r = 3961
> >>     ))
> >>   return(Schools)}
> >>
> >> Schools <- gen.schools(n.schools = n.Schools)
> >> # Generate Grades
> >> Grades <- c(first.grade:last.grade)
> >> # Generate n.Classrooms
> >>
> >> Classrooms <- LETTERS[1:n.Classrooms]
> >> # Group schools and grades
> >>
> >> SchGr <- outer(paste0(Schools$School.ID, '-'), paste0(Grades, '-'),
> >> FUN="paste")#head(SchGr)
> >> # Group SchGr and Classrooms
> >>
> >> SchGrClss <- outer(SchGr, paste0(Classrooms, '-'),
> >> FUN="paste")#head(SchGrClss)
> >> # These are the combination of  School-Grades-Classroom
> >> SchGrClssTmp <- as.matrix(SchGrClss, ncol=1, nrow=length(SchGrClss) )
> >> SchGrClssEnd <- as.data.frame(SchGrClssTmp)
> >> # Assign n.Teachers (2 classroom in a given school-grade)
> >> Allpairs <- as.data.frame(t(combn(SchGrClssTmp, 2)))
> >> AllpairsTmp <- paste(Allpairs$V1, Allpairs$V2, sep=" ")
> >>
> >> library(stringr)
> >> separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
> >> "-"))
> >> separoPairs <- as.data.frame(t(separoPairs))
> >> row.names(separoPairs) <- NULL
> >> separoPairs <- separoPairs %>% select(-V7)  %>%  #Drops empty column
> >>   mutate(V1=as.character(V1), V4=as.character(V4), V2=as.numeric(V2),
> >> V5=as.numeric(V5)) %>% mutate(V4 = trimws(V4, which = "both"))
> >>
> >> separoPairs[120,]$V4#Only the rows with V1=V4 and V2=V5 are valid
> >
> >
> >> validPairs <- separoPairs %>% filter(V1==V4 & V2==V5) %>% select(V1, V2,
> >> V3, V6)
> >> # Generate n.Teachers
> >>
> >> gen.teachers <- function(n.teachers){
> >>   Teacher.ID <- gen.names(n = n.teachers, name.order = "last.first")
> >>   Teacher.exp <- runif(n = n.teachers, min = 1, max = 30)
> >>   Teacher.Other <- sample(c(0,1), replace = T, prob = c(0.5, 0.5),
> >> size = n.teachers)
> >>   Teacher.RE <- rnorm(n = n.teachers, mean = 0, sd = 1)
> >>   Teachers <- data.frame(Teacher.ID, Teacher.exp, Teacher.Other,
> >> Teacher.RE)
> >>   return(Teachers)}
> >> Teachers <- gen.teachers(n.teachers = n.Teachers) %>%
> >>   mutate(Teacher.ID = as.character(Teacher.ID))
> >> # Randomly assign n.Teachers teachers to the "ValidPairs"
> >> TmpAssignments <- validPairs[sample(1:nrow(validPairs), n.Teachers), ]
> >> Assignments <- cbind.data.frame(Teachers$Teacher.ID, TmpAssignments)
> >> names(Assignments) <- c("Teacher.ID", "School.ID", "Grade", "Class_1",
> >> "Class_2")
> >> # Tidy Data
> >> library(tidyr)
> >> TeacherClassroom <- Assignments %>%
> >>   gather(x, Classroom, Class_1,Class_2) %>%
> >>   select(-x) %>%
> >>   mutate(Teacher.ID = as.character(Teacher.ID))
> >> # Merge
> >> DF_Classrooms <- TeacherClassroom %>% full_join(Teachers,
> >> by="Teacher.ID") %>% full_join(Schools, by="School.ID")
> >> rm(list=setdiff(ls(), "DF_Classrooms")) # Clean the work space!
> >>
> >> *I want to end up with the same*  'DF_Classrooms *data frame* but
> getting
> >
> >
> >> there in a more efficient way. In particular, when is use n.Classrooms
> >> <-4 the
> >>
> > code run fast, but *if I increase it to something like 20 it is painfully
> >> slow.*
> >>
> >> Thanks!!!
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From inshique at ymail.com  Thu Jul 16 19:40:53 2015
From: inshique at ymail.com (tryingtolearn)
Date: Thu, 16 Jul 2015 10:40:53 -0700 (PDT)
Subject: [R] matching strings in a list
Message-ID: <1437068453415-4709967.post@n4.nabble.com>

Say I have a list: 
[[1]] "I like google"
[[2]] "Hi Google google" 
[[3]] "what's up" 

and they are tweets. And I want to find out how many tweets mention google
(the answer should be 2). 
If I string split and unlist them, then I would get the answer of 3. How do
I make sure I get just 2? 



--
View this message in context: http://r.789695.n4.nabble.com/matching-strings-in-a-list-tp4709967.html
Sent from the R help mailing list archive at Nabble.com.


From jholtman at gmail.com  Thu Jul 16 19:42:18 2015
From: jholtman at gmail.com (jim holtman)
Date: Thu, 16 Jul 2015 13:42:18 -0400
Subject: [R] Speeding up code?
In-Reply-To: <CAAxdm-5tMQ+vD8joQ0CK=DGbFgmkjWcdH4+a7GZh1+j_1v8jsw@mail.gmail.com>
References: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>
	<CAE=6FXahjzNEtXm4w3Ye625xieHpPmRfhrVLwc-gqBVgLznq6g@mail.gmail.com>
	<CAJA1VFxU6p7PvPpbJ0Zmy-NCKBXzh3gwGvov2dRf97-B10-GPQ@mail.gmail.com>
	<CAAxdm-5tMQ+vD8joQ0CK=DGbFgmkjWcdH4+a7GZh1+j_1v8jsw@mail.gmail.com>
Message-ID: <CAAxdm-7bPB1HS46EXOgAKLE-oY4GuNmdKFdauWtx+XCtAQFbSw@mail.gmail.com>

Actually looking at the result, you don't need the transpose; that was an
artifact of how you were doing it before.

 xm <- do.call(rbind, str_split(string = AllpairsTmp, pattern = "-"))
 # convert to dataframe and do transpose on matrix and not dataframe
 separoPairs <- as.data.frame((xm), stringsAsFactors = FALSE)




Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 16, 2015 at 1:37 PM, jim holtman <jholtman at gmail.com> wrote:

> Here is one improvement.  Avoid dataframes in some of these cases.  This
> create a character matrix and then converts to a dataframe after doing the
> transpose of the matrix.  This just takes less than 10 seconds on my system:
>
>
> >  library(stringr)
> >  # create character matrix; avoid dataframes in this case
> >  print(proc.time())
>    user  system elapsed
>   15.52    5.24  587.70
> >  xm <- do.call(rbind, str_split(string = AllpairsTmp, pattern = "-"))
> >  # convert to dataframe and do transpose on matrix and not dataframe
> >  separoPairs <- as.data.frame(t(xm), stringsAsFactors = FALSE)
> >  print(proc.time()
> +
> + )
>    user  system elapsed
>   20.90    5.36  596.57
> >
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Jul 16, 2015 at 7:56 AM, Ignacio Martinez <ignacio82 at gmail.com>
> wrote:
>
>> Hi Collin,
>>
>> The objective of the gen.names function is to generate N *unique *random
>> names, where N is a *large *number. In my computer `gen.names(n = 50000)`
>> takes under a second, so is probably not the root problem in my code. That
>> said, I would love to improve it. I'm not exactly sure how you propose to
>> change it using sample. What is the object that I would be sampling? I
>> would love to run a little benchmark to compare my version with yours.
>>
>> What really takes a long time to run is:
>>
>>     separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
>> "-"))
>>
>> So that and the chunk of code before that is probably where I would get
>> big
>> gains in speed. Sadly, I have no clue how to do it differently
>>
>> Thanks a lot for the help!!
>>
>> Ignacio
>>
>>
>> On Wed, Jul 15, 2015 at 11:34 PM Collin Lynch <cflynch at ncsu.edu> wrote:
>>
>> > Hi Ignacio, If I am reading your code correctly then the top while loop
>> is
>> > essentially seeking to select a random set of names from the original
>> set,
>> > then using unique to reduce it down, you then iterate until you have
>> built
>> > your quota.  Ultimately this results in a very inefficient attempt at
>> > sampling without replacement.  Why not just sample without replacement
>> > rather than loop iteratively and use unique?  Or if the set of possible
>> > names are short enough why not just randomize it and then pull the
>> first n
>> > items off?
>> >
>> >     Best,
>> >     Collin.
>> >
>> > On Wed, Jul 15, 2015 at 11:15 PM, Ignacio Martinez <ignacio82 at gmail.com
>> >
>> > wrote:
>> >
>> >> Hi R-Help!
>> >>
>> >> I'm hoping that some of you may give me some tips that could make my
>> code
>> >>
>> > more efficient. More precisely, I would like to make the answer to my
>> >> stakoverflow
>> >> <
>> >>
>> http://stackoverflow.com/questions/31137940/randomly-assign-teachers-to-classrooms-imposing-restrictions
>>
>> >> >
>> >
>> >
>> >> question more efficient.
>> >>
>> >> This is the code:
>> >>
>> >> library(dplyr)
>> >> library(randomNames)
>> >> library(geosphere)
>> >>
>> > set.seed(7142015)# Define Parameters
>> >
>> >
>> >> n.Schools <- 20
>> >> first.grade<-3
>> >> last.grade<-5
>> >> n.Grades <-last.grade-first.grade+1
>> >> n.Classrooms <- 20 # THIS IS WHAT I WANTED TO BE ABLE TO CHANGE
>> >> n.Teachers <- (n.Schools*n.Grades*n.Classrooms)/2 #Two classrooms per
>> >> teacher
>> >> # Define Random names function:
>> >> gen.names <- function(n, which.names = "both", name.order =
>> "last.first"){
>> >>   names <- unique(randomNames(n=n, which.names = which.names,
>> >> name.order = name.order))
>> >>   need <- n - length(names)
>> >>   while(need>0){
>> >>     names <- unique(c(randomNames(n=need, which.names = which.names,
>> >> name.order = name.order), names))
>> >>     need <- n - length(names)
>> >>   }
>> >>   return(names)}
>> >> # Generate n.Schools names
>> >> gen.schools <- function(n.schools) {
>> >>   School.ID <-
>> >>     paste0(gen.names(n = n.schools, which.names = "last"), ' School')
>> >>   School.long <- rnorm(n = n.schools, mean = 21.7672, sd = 0.025)
>> >>   School.lat <- rnorm(n = n.schools, mean = 58.8471, sd = 0.025)
>> >>   School.RE <- rnorm(n = n.schools, mean = 0, sd = 1)
>> >>   Schools <-
>> >>     data.frame(School.ID, School.lat, School.long, School.RE) %>%
>> >>     mutate(School.ID = as.character(School.ID)) %>%
>> >>     rowwise() %>%  mutate (School.distance = distHaversine(
>> >>       p1 = c(School.long, School.lat),
>> >>       p2 = c(21.7672, 58.8471), r = 3961
>> >>     ))
>> >>   return(Schools)}
>> >>
>> >> Schools <- gen.schools(n.schools = n.Schools)
>> >> # Generate Grades
>> >> Grades <- c(first.grade:last.grade)
>> >> # Generate n.Classrooms
>> >>
>> >> Classrooms <- LETTERS[1:n.Classrooms]
>> >> # Group schools and grades
>> >>
>> >> SchGr <- outer(paste0(Schools$School.ID, '-'), paste0(Grades, '-'),
>> >> FUN="paste")#head(SchGr)
>> >> # Group SchGr and Classrooms
>> >>
>> >> SchGrClss <- outer(SchGr, paste0(Classrooms, '-'),
>> >> FUN="paste")#head(SchGrClss)
>> >> # These are the combination of  School-Grades-Classroom
>> >> SchGrClssTmp <- as.matrix(SchGrClss, ncol=1, nrow=length(SchGrClss) )
>> >> SchGrClssEnd <- as.data.frame(SchGrClssTmp)
>> >> # Assign n.Teachers (2 classroom in a given school-grade)
>> >> Allpairs <- as.data.frame(t(combn(SchGrClssTmp, 2)))
>> >> AllpairsTmp <- paste(Allpairs$V1, Allpairs$V2, sep=" ")
>> >>
>> >> library(stringr)
>> >> separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
>> >> "-"))
>> >> separoPairs <- as.data.frame(t(separoPairs))
>> >> row.names(separoPairs) <- NULL
>> >> separoPairs <- separoPairs %>% select(-V7)  %>%  #Drops empty column
>> >>   mutate(V1=as.character(V1), V4=as.character(V4), V2=as.numeric(V2),
>> >> V5=as.numeric(V5)) %>% mutate(V4 = trimws(V4, which = "both"))
>> >>
>> >> separoPairs[120,]$V4#Only the rows with V1=V4 and V2=V5 are valid
>> >
>> >
>> >> validPairs <- separoPairs %>% filter(V1==V4 & V2==V5) %>% select(V1,
>> V2,
>> >> V3, V6)
>> >> # Generate n.Teachers
>> >>
>> >> gen.teachers <- function(n.teachers){
>> >>   Teacher.ID <- gen.names(n = n.teachers, name.order = "last.first")
>> >>   Teacher.exp <- runif(n = n.teachers, min = 1, max = 30)
>> >>   Teacher.Other <- sample(c(0,1), replace = T, prob = c(0.5, 0.5),
>> >> size = n.teachers)
>> >>   Teacher.RE <- rnorm(n = n.teachers, mean = 0, sd = 1)
>> >>   Teachers <- data.frame(Teacher.ID, Teacher.exp, Teacher.Other,
>> >> Teacher.RE)
>> >>   return(Teachers)}
>> >> Teachers <- gen.teachers(n.teachers = n.Teachers) %>%
>> >>   mutate(Teacher.ID = as.character(Teacher.ID))
>> >> # Randomly assign n.Teachers teachers to the "ValidPairs"
>> >> TmpAssignments <- validPairs[sample(1:nrow(validPairs), n.Teachers), ]
>> >> Assignments <- cbind.data.frame(Teachers$Teacher.ID, TmpAssignments)
>> >> names(Assignments) <- c("Teacher.ID", "School.ID", "Grade", "Class_1",
>> >> "Class_2")
>> >> # Tidy Data
>> >> library(tidyr)
>> >> TeacherClassroom <- Assignments %>%
>> >>   gather(x, Classroom, Class_1,Class_2) %>%
>> >>   select(-x) %>%
>> >>   mutate(Teacher.ID = as.character(Teacher.ID))
>> >> # Merge
>> >> DF_Classrooms <- TeacherClassroom %>% full_join(Teachers,
>> >> by="Teacher.ID") %>% full_join(Schools, by="School.ID")
>> >> rm(list=setdiff(ls(), "DF_Classrooms")) # Clean the work space!
>> >>
>> >> *I want to end up with the same*  'DF_Classrooms *data frame* but
>> getting
>> >
>> >
>> >> there in a more efficient way. In particular, when is use n.Classrooms
>> >> <-4 the
>> >>
>> > code run fast, but *if I increase it to something like 20 it is
>> painfully
>> >> slow.*
>> >>
>> >> Thanks!!!
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>> >> PLEASE do read the posting guide
>> >> http://www.R-project.org/posting-guide.html
>> >> and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Thu Jul 16 19:52:00 2015
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Thu, 16 Jul 2015 10:52:00 -0700
Subject: [R] matrix manipulation
In-Reply-To: <2f3a88$11iup2@ironport10.mayo.edu>
References: <2f3a88$11iup2@ironport10.mayo.edu>
Message-ID: <CA+hbrhWOas-iH0rXeWNe9qLs8yiTYrsU6pDjsfOoQh8nTZ7U4g@mail.gmail.com>

Hi Terry,

maybe I'm missing something, but why not define a matrix BB = V'B;
then t(B) %*% V = t(BB), then your problem reduces to finding A such
that t(BB) %*% A = 0?

Peter

On Thu, Jul 16, 2015 at 10:28 AM, Therneau, Terry M., Ph.D.
<therneau at mayo.edu> wrote:
> This is as much a mathematics as an R question, in the "this should be easy
> but I don't see it" category.
>
> Assume I have a full rank p by p matrix V  (aside: V = (X'X)^{-1} for a
> particular setup), a p by k matrix B, and I want to complete an orthagonal
> basis for the space with distance function V.  That is, find A such that
> t(B) %*% V %*% A =0, where A has p rows and p-k columns.
>
> With V=identity this is easy. I can do it in 1-2 lines using qr(), lm(), or
> several other tools.  A part of me is quite certain that the general problem
> isn't more than 3 lines of R, but after a day of beating my head on the
> issue I still don't see it.  Math wise it looks like a simple homework
> problem in a mid level class, but I'm not currently sure that I'd pass said
> class.
>
> If someone could show the way I would be grateful.  Either that or assurance
> that the problem actually IS hard and I'm not as dense as I think.
>
> Terry T.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Thu Jul 16 19:57:03 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 16 Jul 2015 12:57:03 -0500
Subject: [R] matching strings in a list
In-Reply-To: <1437068453415-4709967.post@n4.nabble.com>
References: <1437068453415-4709967.post@n4.nabble.com>
Message-ID: <502A4F63-B44C-40D4-B16D-7B6BECA35B4A@me.com>


> On Jul 16, 2015, at 12:40 PM, tryingtolearn <inshique at ymail.com> wrote:
> 
> Say I have a list: 
> [[1]] "I like google"
> [[2]] "Hi Google google" 
> [[3]] "what's up" 
> 
> and they are tweets. And I want to find out how many tweets mention google
> (the answer should be 2). 
> If I string split and unlist them, then I would get the answer of 3. How do
> I make sure I get just 2? 


See ?grepl presuming that you just want a count of the matches.

If you want it to also be case insensitive, set 'ignore.case = TRUE?.

For example:

Tweets <- list("I like google", "Hi Google google", "what's up?)

> Tweets
[[1]]
[1] "I like google"

[[2]]
[1] "Hi Google google"

[[3]]
[1] "what's up?

> sapply(Tweets, function(x) grepl("google", x, ignore.case = TRUE))
[1]  TRUE  TRUE FALSE

> sum(sapply(Tweets, function(x) grepl("google", x, ignore.case = TRUE)))
[1] 2


Regards,

Marc Schwartz


From istazahn at gmail.com  Thu Jul 16 19:57:48 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 16 Jul 2015 13:57:48 -0400
Subject: [R] matching strings in a list
In-Reply-To: <1437068453415-4709967.post@n4.nabble.com>
References: <1437068453415-4709967.post@n4.nabble.com>
Message-ID: <CA+vqiLEvwb3vhhUe3_ofmh6_A=my+M4sUBtwp52RF9BZzPv0fw@mail.gmail.com>

Why would you strsplit them? I would think

length(grep("google", unlist(x), ignore.case = TRUE))

should do it.

Best,
Ista

On Thu, Jul 16, 2015 at 1:40 PM, tryingtolearn <inshique at ymail.com> wrote:
> Say I have a list:
> [[1]] "I like google"
> [[2]] "Hi Google google"
> [[3]] "what's up"
>
> and they are tweets. And I want to find out how many tweets mention google
> (the answer should be 2).
> If I string split and unlist them, then I would get the answer of 3. How do
> I make sure I get just 2?
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/matching-strings-in-a-list-tp4709967.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From john.archie.mckown at gmail.com  Thu Jul 16 20:00:00 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Thu, 16 Jul 2015 13:00:00 -0500
Subject: [R] matching strings in a list
In-Reply-To: <1437068453415-4709967.post@n4.nabble.com>
References: <1437068453415-4709967.post@n4.nabble.com>
Message-ID: <CAAJSdjgPUNxoR==yjB455yt_LDRwzA5s38h4U-6Jc1=XsrDo1w@mail.gmail.com>

On Thu, Jul 16, 2015 at 12:40 PM, tryingtolearn <inshique at ymail.com> wrote:

> Say I have a list:
> [[1]] "I like google"
> [[2]] "Hi Google google"
> [[3]] "what's up"
>
> and they are tweets. And I want to find out how many tweets mention google
> (the answer should be 2).
> If I string split and unlist them, then I would get the answer of 3. How do
> I make sure I get just 2?
>

?NROW(grep("google",list,ignore.case=TRUE))?


-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Thu Jul 16 20:01:54 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Thu, 16 Jul 2015 13:01:54 -0500
Subject: [R] matching strings in a list
In-Reply-To: <CAAJSdjgPUNxoR==yjB455yt_LDRwzA5s38h4U-6Jc1=XsrDo1w@mail.gmail.com>
References: <1437068453415-4709967.post@n4.nabble.com>
	<CAAJSdjgPUNxoR==yjB455yt_LDRwzA5s38h4U-6Jc1=XsrDo1w@mail.gmail.com>
Message-ID: <CAAJSdjhxp868V2skEupV5=W4LFmUEPe0504DRUPtJ4F8FoO-Yw@mail.gmail.com>

On Thu, Jul 16, 2015 at 1:00 PM, John McKown <john.archie.mckown at gmail.com>
wrote:

> On Thu, Jul 16, 2015 at 12:40 PM, tryingtolearn <inshique at ymail.com>
> wrote:
>
>> Say I have a list:
>> [[1]] "I like google"
>> [[2]] "Hi Google google"
>> [[3]] "what's up"
>>
>> and they are tweets. And I want to find out how many tweets mention google
>> (the answer should be 2).
>> If I string split and unlist them, then I would get the answer of 3. How
>> do
>> I make sure I get just 2?
>>
>
> ?NROW(grep("google",list,ignore.case=TRUE))?
>

?Or

sum(grepl("google",x,ignore.case=TRUE))

-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Thu Jul 16 20:50:06 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 16 Jul 2015 13:50:06 -0500
Subject: [R] matrix manipulation -solved
In-Reply-To: <CA+hbrhWOas-iH0rXeWNe9qLs8yiTYrsU6pDjsfOoQh8nTZ7U4g@mail.gmail.com>
References: <2f3a88$11iup2@ironport10.mayo.edu>
	<CA+hbrhWOas-iH0rXeWNe9qLs8yiTYrsU6pDjsfOoQh8nTZ7U4g@mail.gmail.com>
Message-ID: <2f3a88$11jsld@ironport10.mayo.edu>

Yes it is obvious --- once someone else pointed it out.
Thanks for the hint.

Terry T.


On 07/16/2015 12:52 PM, Peter Langfelder wrote:
> Hi Terry,
>
> maybe I'm missing something, but why not define a matrix BB = V'B;
> then t(B) %*% V = t(BB), then your problem reduces to finding A such
> that t(BB) %*% A = 0?
>
> Peter
>
> On Thu, Jul 16, 2015 at 10:28 AM, Therneau, Terry M., Ph.D.
> <therneau at mayo.edu> wrote:
>> This is as much a mathematics as an R question, in the "this should be easy
>> but I don't see it" category.
>>
>> Assume I have a full rank p by p matrix V  (aside: V = (X'X)^{-1} for a
>> particular setup), a p by k matrix B, and I want to complete an orthagonal
>> basis for the space with distance function V.  That is, find A such that
>> t(B) %*% V %*% A =0, where A has p rows and p-k columns.
>>
>> With V=identity this is easy. I can do it in 1-2 lines using qr(), lm(), or
>> several other tools.  A part of me is quite certain that the general problem
>> isn't more than 3 lines of R, but after a day of beating my head on the
>> issue I still don't see it.  Math wise it looks like a simple homework
>> problem in a mid level class, but I'm not currently sure that I'd pass said
>> class.
>>
>> If someone could show the way I would be grateful.  Either that or assurance
>> that the problem actually IS hard and I'm not as dense as I think.
>>
>> Terry T.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From ignacio82 at gmail.com  Thu Jul 16 19:49:45 2015
From: ignacio82 at gmail.com (Ignacio Martinez)
Date: Thu, 16 Jul 2015 17:49:45 +0000
Subject: [R] Speeding up code?
In-Reply-To: <CAAxdm-7bPB1HS46EXOgAKLE-oY4GuNmdKFdauWtx+XCtAQFbSw@mail.gmail.com>
References: <CAJA1VFyputkJncT-+A-0+hAsLw_jCtZ3ta5EZDPEjeTjsSpgzQ@mail.gmail.com>
	<CAE=6FXahjzNEtXm4w3Ye625xieHpPmRfhrVLwc-gqBVgLznq6g@mail.gmail.com>
	<CAJA1VFxU6p7PvPpbJ0Zmy-NCKBXzh3gwGvov2dRf97-B10-GPQ@mail.gmail.com>
	<CAAxdm-5tMQ+vD8joQ0CK=DGbFgmkjWcdH4+a7GZh1+j_1v8jsw@mail.gmail.com>
	<CAAxdm-7bPB1HS46EXOgAKLE-oY4GuNmdKFdauWtx+XCtAQFbSw@mail.gmail.com>
Message-ID: <CAJA1VFx4_6=sHcm2uX0eY6QtBkk-NGhaGeiFwt1FVkvmq+n-_w@mail.gmail.com>

Thank Jim!

This makes a huge difference. Can you explain why are data frame slower
than a matrix? Any other suggestions on how to improve the code would be
greatly appreciated.

Thanks again!

Ignacio

On Thu, Jul 16, 2015 at 1:42 PM jim holtman <jholtman at gmail.com> wrote:

> Actually looking at the result, you don't need the transpose; that was an
> artifact of how you were doing it before.
>
>  xm <- do.call(rbind, str_split(string = AllpairsTmp, pattern = "-"))
>  # convert to dataframe and do transpose on matrix and not dataframe
>  separoPairs <- as.data.frame((xm), stringsAsFactors = FALSE)
>
>
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Thu, Jul 16, 2015 at 1:37 PM, jim holtman <jholtman at gmail.com> wrote:
>
>> Here is one improvement.  Avoid dataframes in some of these cases.  This
>> create a character matrix and then converts to a dataframe after doing the
>> transpose of the matrix.  This just takes less than 10 seconds on my system:
>>
>>
>> >  library(stringr)
>> >  # create character matrix; avoid dataframes in this case
>> >  print(proc.time())
>>    user  system elapsed
>>   15.52    5.24  587.70
>> >  xm <- do.call(rbind, str_split(string = AllpairsTmp, pattern = "-"))
>> >  # convert to dataframe and do transpose on matrix and not dataframe
>> >  separoPairs <- as.data.frame(t(xm), stringsAsFactors = FALSE)
>> >  print(proc.time()
>> +
>> + )
>>    user  system elapsed
>>   20.90    5.36  596.57
>> >
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Thu, Jul 16, 2015 at 7:56 AM, Ignacio Martinez <ignacio82 at gmail.com>
>> wrote:
>>
>>> Hi Collin,
>>>
>>> The objective of the gen.names function is to generate N *unique *random
>>> names, where N is a *large *number. In my computer `gen.names(n = 50000)`
>>> takes under a second, so is probably not the root problem in my code.
>>> That
>>> said, I would love to improve it. I'm not exactly sure how you propose to
>>> change it using sample. What is the object that I would be sampling? I
>>> would love to run a little benchmark to compare my version with yours.
>>>
>>> What really takes a long time to run is:
>>>
>>>     separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern
>>> =
>>> "-"))
>>>
>>> So that and the chunk of code before that is probably where I would get
>>> big
>>> gains in speed. Sadly, I have no clue how to do it differently
>>>
>>> Thanks a lot for the help!!
>>>
>>> Ignacio
>>>
>>>
>>> On Wed, Jul 15, 2015 at 11:34 PM Collin Lynch <cflynch at ncsu.edu> wrote:
>>>
>>> > Hi Ignacio, If I am reading your code correctly then the top while
>>> loop is
>>> > essentially seeking to select a random set of names from the original
>>> set,
>>> > then using unique to reduce it down, you then iterate until you have
>>> built
>>> > your quota.  Ultimately this results in a very inefficient attempt at
>>> > sampling without replacement.  Why not just sample without replacement
>>> > rather than loop iteratively and use unique?  Or if the set of possible
>>> > names are short enough why not just randomize it and then pull the
>>> first n
>>> > items off?
>>> >
>>> >     Best,
>>> >     Collin.
>>> >
>>> > On Wed, Jul 15, 2015 at 11:15 PM, Ignacio Martinez <
>>> ignacio82 at gmail.com>
>>> > wrote:
>>> >
>>> >> Hi R-Help!
>>> >>
>>> >> I'm hoping that some of you may give me some tips that could make my
>>> code
>>> >>
>>> > more efficient. More precisely, I would like to make the answer to my
>>> >> stakoverflow
>>> >> <
>>> >>
>>> http://stackoverflow.com/questions/31137940/randomly-assign-teachers-to-classrooms-imposing-restrictions
>>>
>>> >> >
>>> >
>>> >
>>> >> question more efficient.
>>> >>
>>> >> This is the code:
>>> >>
>>> >> library(dplyr)
>>> >> library(randomNames)
>>> >> library(geosphere)
>>> >>
>>> > set.seed(7142015)# Define Parameters
>>> >
>>> >
>>> >> n.Schools <- 20
>>> >> first.grade<-3
>>> >> last.grade<-5
>>> >> n.Grades <-last.grade-first.grade+1
>>> >> n.Classrooms <- 20 # THIS IS WHAT I WANTED TO BE ABLE TO CHANGE
>>> >> n.Teachers <- (n.Schools*n.Grades*n.Classrooms)/2 #Two classrooms per
>>> >> teacher
>>> >> # Define Random names function:
>>> >> gen.names <- function(n, which.names = "both", name.order =
>>> "last.first"){
>>> >>   names <- unique(randomNames(n=n, which.names = which.names,
>>> >> name.order = name.order))
>>> >>   need <- n - length(names)
>>> >>   while(need>0){
>>> >>     names <- unique(c(randomNames(n=need, which.names = which.names,
>>> >> name.order = name.order), names))
>>> >>     need <- n - length(names)
>>> >>   }
>>> >>   return(names)}
>>> >> # Generate n.Schools names
>>> >> gen.schools <- function(n.schools) {
>>> >>   School.ID <-
>>> >>     paste0(gen.names(n = n.schools, which.names = "last"), ' School')
>>> >>   School.long <- rnorm(n = n.schools, mean = 21.7672, sd = 0.025)
>>> >>   School.lat <- rnorm(n = n.schools, mean = 58.8471, sd = 0.025)
>>> >>   School.RE <- rnorm(n = n.schools, mean = 0, sd = 1)
>>> >>   Schools <-
>>> >>     data.frame(School.ID, School.lat, School.long, School.RE) %>%
>>> >>     mutate(School.ID = as.character(School.ID)) %>%
>>> >>     rowwise() %>%  mutate (School.distance = distHaversine(
>>> >>       p1 = c(School.long, School.lat),
>>> >>       p2 = c(21.7672, 58.8471), r = 3961
>>> >>     ))
>>> >>   return(Schools)}
>>> >>
>>> >> Schools <- gen.schools(n.schools = n.Schools)
>>> >> # Generate Grades
>>> >> Grades <- c(first.grade:last.grade)
>>> >> # Generate n.Classrooms
>>> >>
>>> >> Classrooms <- LETTERS[1:n.Classrooms]
>>> >> # Group schools and grades
>>> >>
>>> >> SchGr <- outer(paste0(Schools$School.ID, '-'), paste0(Grades, '-'),
>>> >> FUN="paste")#head(SchGr)
>>> >> # Group SchGr and Classrooms
>>> >>
>>> >> SchGrClss <- outer(SchGr, paste0(Classrooms, '-'),
>>> >> FUN="paste")#head(SchGrClss)
>>> >> # These are the combination of  School-Grades-Classroom
>>> >> SchGrClssTmp <- as.matrix(SchGrClss, ncol=1, nrow=length(SchGrClss) )
>>> >> SchGrClssEnd <- as.data.frame(SchGrClssTmp)
>>> >> # Assign n.Teachers (2 classroom in a given school-grade)
>>> >> Allpairs <- as.data.frame(t(combn(SchGrClssTmp, 2)))
>>> >> AllpairsTmp <- paste(Allpairs$V1, Allpairs$V2, sep=" ")
>>> >>
>>> >> library(stringr)
>>> >> separoPairs <- as.data.frame(str_split(string = AllpairsTmp, pattern =
>>> >> "-"))
>>> >> separoPairs <- as.data.frame(t(separoPairs))
>>> >> row.names(separoPairs) <- NULL
>>> >> separoPairs <- separoPairs %>% select(-V7)  %>%  #Drops empty column
>>> >>   mutate(V1=as.character(V1), V4=as.character(V4), V2=as.numeric(V2),
>>> >> V5=as.numeric(V5)) %>% mutate(V4 = trimws(V4, which = "both"))
>>> >>
>>> >> separoPairs[120,]$V4#Only the rows with V1=V4 and V2=V5 are valid
>>> >
>>> >
>>> >> validPairs <- separoPairs %>% filter(V1==V4 & V2==V5) %>% select(V1,
>>> V2,
>>> >> V3, V6)
>>> >> # Generate n.Teachers
>>> >>
>>> >> gen.teachers <- function(n.teachers){
>>> >>   Teacher.ID <- gen.names(n = n.teachers, name.order = "last.first")
>>> >>   Teacher.exp <- runif(n = n.teachers, min = 1, max = 30)
>>> >>   Teacher.Other <- sample(c(0,1), replace = T, prob = c(0.5, 0.5),
>>> >> size = n.teachers)
>>> >>   Teacher.RE <- rnorm(n = n.teachers, mean = 0, sd = 1)
>>> >>   Teachers <- data.frame(Teacher.ID, Teacher.exp, Teacher.Other,
>>> >> Teacher.RE)
>>> >>   return(Teachers)}
>>> >> Teachers <- gen.teachers(n.teachers = n.Teachers) %>%
>>> >>   mutate(Teacher.ID = as.character(Teacher.ID))
>>> >> # Randomly assign n.Teachers teachers to the "ValidPairs"
>>> >> TmpAssignments <- validPairs[sample(1:nrow(validPairs), n.Teachers), ]
>>> >> Assignments <- cbind.data.frame(Teachers$Teacher.ID, TmpAssignments)
>>> >> names(Assignments) <- c("Teacher.ID", "School.ID", "Grade", "Class_1",
>>> >> "Class_2")
>>> >> # Tidy Data
>>> >> library(tidyr)
>>> >> TeacherClassroom <- Assignments %>%
>>> >>   gather(x, Classroom, Class_1,Class_2) %>%
>>> >>   select(-x) %>%
>>> >>   mutate(Teacher.ID = as.character(Teacher.ID))
>>> >> # Merge
>>> >> DF_Classrooms <- TeacherClassroom %>% full_join(Teachers,
>>> >> by="Teacher.ID") %>% full_join(Schools, by="School.ID")
>>> >> rm(list=setdiff(ls(), "DF_Classrooms")) # Clean the work space!
>>> >>
>>> >> *I want to end up with the same*  'DF_Classrooms *data frame* but
>>> getting
>>> >
>>> >
>>> >> there in a more efficient way. In particular, when is use n.Classrooms
>>> >> <-4 the
>>> >>
>>> > code run fast, but *if I increase it to something like 20 it is
>>> painfully
>>> >> slow.*
>>> >>
>>> >> Thanks!!!
>>> >>
>>> >>         [[alternative HTML version deleted]]
>>> >>
>>> >> ______________________________________________
>>> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> >> PLEASE do read the posting guide
>>> >> http://www.R-project.org/posting-guide.html
>>> >> and provide commented, minimal, self-contained, reproducible code.
>>> >>
>>> >
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From yinkunshan at gmail.com  Thu Jul 16 21:35:45 2015
From: yinkunshan at gmail.com (Kunshan Yin)
Date: Thu, 16 Jul 2015 12:35:45 -0700
Subject: [R] User defined function within a formula
Message-ID: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>

Hello, I have a question about the formula and the user defined function:

I can do following:
###Case 1:
> clotting <- data.frame(
+     u = c(5,10,15,20,30,40,60,80,100),
+     lot1 = c(118,58,42,35,27,25,21,19,18),
+     lot2 = c(69,35,26,21,18,16,13,12,12))
> g1=glm(lot1 ~ log(u) + poly(u,1), data = clotting, family = Gamma)
> dc=clotting
> dc$u=1
> predict(g1,dc)
          1           2           3           4           5
6           7           8           9
-0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929
-0.01398929 -0.01398929 -0.01398929

However, if I just simply wrap the poly as a user defined function ( in
reality I would have my own more complex function)  then I will get error:
###Case 2:
> xpoly<-function(x,degree=1){poly(x,degree)}
> g2=glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
> predict(g2,dc)
Error in poly(x, degree) :
  'degree' must be less than number of unique points

It seems that the predict always treat the user defined function in the
formula with I().  My question is how can I get the  results for Case2 same
as case1?

Anyone can have any idea about this?

Thank you very much.

Alex

	[[alternative HTML version deleted]]


From riya_mehra88 at yahoo.com  Thu Jul 16 21:53:01 2015
From: riya_mehra88 at yahoo.com (Riya Mehra)
Date: Thu, 16 Jul 2015 19:53:01 +0000 (UTC)
Subject: [R] Adjusting Probability for Oversampling
Message-ID: <1033266797.275140.1437076381535.JavaMail.yahoo@mail.yahoo.com>

I am developing a marketing (Churn) model that has an event rate of 0.5%. So i thought to perform oversampling. I mean making the number of events equal to number of non-events by reducing non-events (50-50 after sampling). After oversampling, we need to adjust predicted probabilities as it inflates intercept. I always do it in logistic regression. Does the same adjustment require for decision tree, random forest or other ensemble techniques? I am following "Applied Predictive Modeling with R (Caret Package)". The author has used the same technique (down sampling) but he did not adjust predicted probability when he score validation and test data. Any help would be highly appreciated!

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Jul 16 23:09:25 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 16 Jul 2015 14:09:25 -0700
Subject: [R] User defined function within a formula
In-Reply-To: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>
References: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>
Message-ID: <CAF8bMcbb5+hJ+V3n5+aoAO=TncC2869-iSo5RLfU5-+zn6A3mA@mail.gmail.com>

Read about the 'makepredictcall' generic function.  There is a method,
makepredictcall.poly(), for poly() that attaches the polynomial coefficients
used during the fitting procedure to the call to poly() that predict()
makes.
You ought to supply a similar method for your xpoly(), and xpoly() needs to
return an object of a a new class that will cause that method to be used.

E.g.,

xpoly <- function(x,degree=1,...){ ret <- poly(x,degree=degree,...);
class(ret) <- "xpoly" ; ret }
makepredictcall.xpoly <- function (var, call)
{
    if (is.call(call)) {
        if (identical(call[[1]], quote(xpoly))) {
            if (!is.null(tmp <- attr(var, "coefs"))) {
                call$coefs <- tmp
            }
        }
    }
    call
}

g2 <- glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
predict(g2,dc)
#             1              2              3              4              5
#-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
#-0.01398928608
#             6              7              8              9
#-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608

You can see the effects of makepredictcall() in the 'terms' component
of glm's output.  The 'variables' attribute of it gives the original
function
calls and the 'predvars' attribute gives the calls to be used for
prediction:
   > attr(g2$terms, "variables")
   list(lot1, log(u), xpoly(u, 1))
  > attr(g2$terms, "predvars")
  list(lot1, log(u), xpoly(u, 1, coefs = list(alpha = 40, norm2 = c(1,
  9, 8850))))



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 16, 2015 at 12:35 PM, Kunshan Yin <yinkunshan at gmail.com> wrote:

> Hello, I have a question about the formula and the user defined function:
>
> I can do following:
> ###Case 1:
> > clotting <- data.frame(
> +     u = c(5,10,15,20,30,40,60,80,100),
> +     lot1 = c(118,58,42,35,27,25,21,19,18),
> +     lot2 = c(69,35,26,21,18,16,13,12,12))
> > g1=glm(lot1 ~ log(u) + poly(u,1), data = clotting, family = Gamma)
> > dc=clotting
> > dc$u=1
> > predict(g1,dc)
>           1           2           3           4           5
> 6           7           8           9
> -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929
> -0.01398929 -0.01398929 -0.01398929
>
> However, if I just simply wrap the poly as a user defined function ( in
> reality I would have my own more complex function)  then I will get error:
> ###Case 2:
> > xpoly<-function(x,degree=1){poly(x,degree)}
> > g2=glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
> > predict(g2,dc)
> Error in poly(x, degree) :
>   'degree' must be less than number of unique points
>
> It seems that the predict always treat the user defined function in the
> formula with I().  My question is how can I get the  results for Case2 same
> as case1?
>
> Anyone can have any idea about this?
>
> Thank you very much.
>
> Alex
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Fri Jul 17 01:05:40 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 17 Jul 2015 11:05:40 +1200
Subject: [R] [FORGED] Re:  Weighted skewness and curtosis
In-Reply-To: <CAGxFJbQHSx+7j3ssErbrkmANyg4Kb33OnHW1OH=U+aWeWqpTiQ@mail.gmail.com>
References: <CAN2xGJZ2YPz=HddQNC4DVXruW9ENBEhLrSF2VROZkjHHQsAj_g@mail.gmail.com>
	<2D949D0A-3D96-4074-9BD1-FED346A82F3C@comcast.net>
	<CAN2xGJaytJUCHt8G=L-ZwEEQrmfGWU3omUXZ+dDUSpEavssADQ@mail.gmail.com>
	<CAGxFJbQHSx+7j3ssErbrkmANyg4Kb33OnHW1OH=U+aWeWqpTiQ@mail.gmail.com>
Message-ID: <55A838C4.5010807@auckland.ac.nz>

On 17/07/15 03:45, Bert Gunter wrote:

<SNIP>

> Of course, sample skewness and kurtosis are basically useless, but
> that's another, off topic, issue.

Fortune nomination!

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From demmitba at gmail.com  Fri Jul 17 01:35:38 2015
From: demmitba at gmail.com (Brittany Demmitt)
Date: Thu, 16 Jul 2015 17:35:38 -0600
Subject: [R] powerTransform warning message?
Message-ID: <B183F37E-BE5C-4536-BFD4-562E9E263EB9@gmail.com>

Hello,

I have a series of 40 variables that I am trying to transform via the boxcox method using the powerTransfrom function in R.  I have no zero values in any of my variables.  When I run the powerTransform function on the full data set I get the following warning. 

Warning message:
In sqrt(diag(solve(res$hessian))) : NaNs produced

However, when I analyze the variables in groups, rather than all 40 at a time I do not get this warning message.  Why would this be? And does this mean this warning is safe to ignore?

I would like to add that all of my lambda values are in the -5 to 5 range.  I also get different lambda values when I analyze the variables together versus in groups.  Is this to be expected?

Thank you so much!

Brittany
	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Jul 17 01:39:52 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 16 Jul 2015 16:39:52 -0700
Subject: [R] User defined function within a formula
In-Reply-To: <CAGaeMEXRR7hsjf-nPB1k54UHbrXP8Ss895+f5D1FmuJQzdGBoA@mail.gmail.com>
References: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>
	<CAF8bMcbb5+hJ+V3n5+aoAO=TncC2869-iSo5RLfU5-+zn6A3mA@mail.gmail.com>
	<CAGaeMEXRR7hsjf-nPB1k54UHbrXP8Ss895+f5D1FmuJQzdGBoA@mail.gmail.com>
Message-ID: <CAF8bMcb_JSPQQUkM3NXguPxdiv_Dr3QR6p-r7p+XTnnosz_Jug@mail.gmail.com>

OPoly<-function(x,degree=1,weight=1){
  weight=round(weight,0)# weight need to be integer
  if(length(weight)!=length(x))weight=rep(1,length(x))
  p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)
  Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-
seq(2)]))[,degree])
  class(Z)<-"OPoly";Z
}

You need to make OPoly to have optional argument(s) that give
the original-regressor-dependent information to OPoly and then
have it return, as attributes, the value of those arguments.
 makepredictcall
will take the attributes and attach them to the call in predvars so
predict uses values derived from the original regressors, not value derived
from the data to be predicted from.

Take a look at a pair like makepredictcall.scale() and scale() for an
example:
scale has optional arguments 'center' and 'scale' that it returns as
attributes
and makepredictcall.scale adds those to the call to scale that it is given.
Thus when you predict, the scale and center arguments come from the
original data, not from the data you are predicting from.






Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 16, 2015 at 3:43 PM, Kunshan Yin <yinkunshan at gmail.com> wrote:

> Thanks Bill for your quick reply.
>
> I tried your solution and it did work for the simple user defined function
> xploly. But when I try with other function, it gave me error again:
>
> OPoly<-function(x,degree=1,weight=1){
>   weight=round(weight,0)# weight need to be integer
>   if(length(weight)!=length(x))weight=rep(1,length(x))
>   p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)
>
> Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-seq(2)]))[,degree])
>   class(Z)<-"OPoly";Z
> }
>
> ##this OPoly is an FORTRAN orthogonal polynomial routine, it first maps
> the x to range[-2,2] then do QR, then return the results with sqrt(norm2).
> Comparing with poly, this transformation will make the model coefficients
> within a similar range as other variables, the R poly routine will usually
> give you a very large coefficients. I did not find such routine in R, so I
> have to define this as user defined function.
> #######
>
> I  also have following function as you suggested:
>
> makepredictcall.OPoly<-function(var,call)
> {
>   if (is.call(call)) {
>     if (identical(call[[1]], quote(OPoly))) {
>       if (!is.null(tmp <- attr(var, "coefs"))) {
>         call$coefs <- tmp
>       }
>     }
>   }
>   call
> }
>
>
> But I still got error for following:
>
> > g3=glm(lot1 ~ log(u) + OPoly(u,1), data = clotting, family = Gamma)
>
> > predict(g3,dc)Error in poly(4 * (rep(x, weight) - mean(range(x)))/diff(range(x)), degree) :
>   missing values are not allowed in 'poly'
>
> I thought it might be due to the /diff(range(x) in the function.  But even
> I remove that part, it will still give me error. Any idea?
>
> Many thanks in advance.
>
> Alex
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Thu, Jul 16, 2015 at 2:09 PM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> Read about the 'makepredictcall' generic function.  There is a method,
>> makepredictcall.poly(), for poly() that attaches the polynomial
>> coefficients
>> used during the fitting procedure to the call to poly() that predict()
>> makes.
>> You ought to supply a similar method for your xpoly(), and xpoly() needs
>> to return an object of a a new class that will cause that method to be used.
>>
>> E.g.,
>>
>> xpoly <- function(x,degree=1,...){ ret <- poly(x,degree=degree,...);
>> class(ret) <- "xpoly" ; ret }
>> makepredictcall.xpoly <- function (var, call)
>> {
>>     if (is.call(call)) {
>>         if (identical(call[[1]], quote(xpoly))) {
>>             if (!is.null(tmp <- attr(var, "coefs"))) {
>>                 call$coefs <- tmp
>>             }
>>         }
>>     }
>>     call
>> }
>>
>> g2 <- glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>> predict(g2,dc)
>> #             1              2              3              4
>>  5
>> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>> #-0.01398928608
>> #             6              7              8              9
>> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>>
>> You can see the effects of makepredictcall() in the 'terms' component
>> of glm's output.  The 'variables' attribute of it gives the original
>> function
>> calls and the 'predvars' attribute gives the calls to be used for
>> prediction:
>>    > attr(g2$terms, "variables")
>>    list(lot1, log(u), xpoly(u, 1))
>>   > attr(g2$terms, "predvars")
>>   list(lot1, log(u), xpoly(u, 1, coefs = list(alpha = 40, norm2 = c(1,
>>   9, 8850))))
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Thu, Jul 16, 2015 at 12:35 PM, Kunshan Yin <yinkunshan at gmail.com>
>> wrote:
>>
>>> Hello, I have a question about the formula and the user defined function:
>>>
>>> I can do following:
>>> ###Case 1:
>>> > clotting <- data.frame(
>>> +     u = c(5,10,15,20,30,40,60,80,100),
>>> +     lot1 = c(118,58,42,35,27,25,21,19,18),
>>> +     lot2 = c(69,35,26,21,18,16,13,12,12))
>>> > g1=glm(lot1 ~ log(u) + poly(u,1), data = clotting, family = Gamma)
>>> > dc=clotting
>>> > dc$u=1
>>> > predict(g1,dc)
>>>           1           2           3           4           5
>>> 6           7           8           9
>>> -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929
>>> -0.01398929 -0.01398929 -0.01398929
>>>
>>> However, if I just simply wrap the poly as a user defined function ( in
>>> reality I would have my own more complex function)  then I will get
>>> error:
>>> ###Case 2:
>>> > xpoly<-function(x,degree=1){poly(x,degree)}
>>> > g2=glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>>> > predict(g2,dc)
>>> Error in poly(x, degree) :
>>>   'degree' must be less than number of unique points
>>>
>>> It seems that the predict always treat the user defined function in the
>>> formula with I().  My question is how can I get the  results for Case2
>>> same
>>> as case1?
>>>
>>> Anyone can have any idea about this?
>>>
>>> Thank you very much.
>>>
>>> Alex
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Fri Jul 17 01:50:50 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 16 Jul 2015 16:50:50 -0700
Subject: [R] powerTransform warning message?
In-Reply-To: <B183F37E-BE5C-4536-BFD4-562E9E263EB9@gmail.com>
References: <B183F37E-BE5C-4536-BFD4-562E9E263EB9@gmail.com>
Message-ID: <CAGxFJbQ+PuuKtE4TkBL9c7EqwuW92xaModRR5khygmzZsSmySQ@mail.gmail.com>

I suggest you consult a local statistician. You are (way) over your
head statistically here, and  statistical matters are off topic on
this list. The brief answer to your question is: you are almost
certainly producing nonsense.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 16, 2015 at 4:35 PM, Brittany Demmitt <demmitba at gmail.com> wrote:
> Hello,
>
> I have a series of 40 variables that I am trying to transform via the boxcox method using the powerTransfrom function in R.  I have no zero values in any of my variables.  When I run the powerTransform function on the full data set I get the following warning.
>
> Warning message:
> In sqrt(diag(solve(res$hessian))) : NaNs produced
>
> However, when I analyze the variables in groups, rather than all 40 at a time I do not get this warning message.  Why would this be? And does this mean this warning is safe to ignore?
>
> I would like to add that all of my lambda values are in the -5 to 5 range.  I also get different lambda values when I analyze the variables together versus in groups.  Is this to be expected?
>
> Thank you so much!
>
> Brittany
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Fri Jul 17 01:51:00 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 16 Jul 2015 16:51:00 -0700
Subject: [R] User defined function within a formula
In-Reply-To: <CAF8bMcb_JSPQQUkM3NXguPxdiv_Dr3QR6p-r7p+XTnnosz_Jug@mail.gmail.com>
References: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>
	<CAF8bMcbb5+hJ+V3n5+aoAO=TncC2869-iSo5RLfU5-+zn6A3mA@mail.gmail.com>
	<CAGaeMEXRR7hsjf-nPB1k54UHbrXP8Ss895+f5D1FmuJQzdGBoA@mail.gmail.com>
	<CAF8bMcb_JSPQQUkM3NXguPxdiv_Dr3QR6p-r7p+XTnnosz_Jug@mail.gmail.com>
Message-ID: <CAF8bMcbSXEpb02tomJC0gsTh+0ORwEPxY1BAbRk4RTewHLA+xQ@mail.gmail.com>

This might do what you want:

OPoly <- function(x, degree=1, weight=1, coefs=NULL, rangeX=NULL){
  weight <- round(weight,0)# weight need to be integer
  if(length(weight)!=length(x)) {
    weight <- rep(1,length(x))
  }
  if (is.null(rangeX)) {
      rangeX <- range(x)
  }
  p <- poly(4*(rep(x,weight)-mean(rangeX))/diff(rangeX), degree=degree,
coefs=coefs)
  # why t(t(...))?  That strips the attributes.
  Z <- t( t(p[cumsum(weight),]) * sqrt(attr(p,"coefs")$norm2[-seq(2)]) )[,
degree, drop=FALSE]
  class(Z) <- "OPoly"
  attr(Z, "coefs") <- attr(p, "coefs")
  attr(Z, "rangeX") <- rangeX
  Z
}

makepredictcall.OPoly<-function(var,call)
{
  if (is.call(call)) {
    if (identical(call[[1]], quote(OPoly))) {
      if (!is.null(tmp <- attr(var, "coefs"))) {
        call$coefs <- tmp
      }
      if (!is.null(tmp <- attr(var, "rangeX"))) {
        call$rangeX <- tmp
      }
      call$weight <- 1 # weight not relevant in predictions
    }
  }
  call
}

d <- data.frame(Y=1:8, X=log(1:8), Weight=1:8)
fit <- lm(data=d, Y ~ OPoly(X, degree=2, weight=Weight))
predict(fit)[c(3,8)]
predict(fit, newdata=data.frame(X=d$X[c(3,8)])) # same result


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 16, 2015 at 4:39 PM, William Dunlap <wdunlap at tibco.com> wrote:

> OPoly<-function(x,degree=1,weight=1){
>   weight=round(weight,0)# weight need to be integer
>   if(length(weight)!=length(x))weight=rep(1,length(x))
>   p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)
>   Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-
> seq(2)]))[,degree])
>   class(Z)<-"OPoly";Z
> }
>
> You need to make OPoly to have optional argument(s) that give
> the original-regressor-dependent information to OPoly and then
> have it return, as attributes, the value of those arguments.
>  makepredictcall
> will take the attributes and attach them to the call in predvars so
> predict uses values derived from the original regressors, not value derived
> from the data to be predicted from.
>
> Take a look at a pair like makepredictcall.scale() and scale() for an
> example:
> scale has optional arguments 'center' and 'scale' that it returns as
> attributes
> and makepredictcall.scale adds those to the call to scale that it is given.
> Thus when you predict, the scale and center arguments come from the
> original data, not from the data you are predicting from.
>
>
>
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 16, 2015 at 3:43 PM, Kunshan Yin <yinkunshan at gmail.com> wrote:
>
>> Thanks Bill for your quick reply.
>>
>> I tried your solution and it did work for the simple user defined
>> function xploly. But when I try with other function, it gave me error again:
>>
>> OPoly<-function(x,degree=1,weight=1){
>>   weight=round(weight,0)# weight need to be integer
>>   if(length(weight)!=length(x))weight=rep(1,length(x))
>>   p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)
>>
>> Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-seq(2)]))[,degree])
>>   class(Z)<-"OPoly";Z
>> }
>>
>> ##this OPoly is an FORTRAN orthogonal polynomial routine, it first maps
>> the x to range[-2,2] then do QR, then return the results with sqrt(norm2).
>> Comparing with poly, this transformation will make the model coefficients
>> within a similar range as other variables, the R poly routine will usually
>> give you a very large coefficients. I did not find such routine in R, so I
>> have to define this as user defined function.
>> #######
>>
>> I  also have following function as you suggested:
>>
>> makepredictcall.OPoly<-function(var,call)
>> {
>>   if (is.call(call)) {
>>     if (identical(call[[1]], quote(OPoly))) {
>>       if (!is.null(tmp <- attr(var, "coefs"))) {
>>         call$coefs <- tmp
>>       }
>>     }
>>   }
>>   call
>> }
>>
>>
>> But I still got error for following:
>>
>> > g3=glm(lot1 ~ log(u) + OPoly(u,1), data = clotting, family = Gamma)
>>
>> > predict(g3,dc)Error in poly(4 * (rep(x, weight) - mean(range(x)))/diff(range(x)), degree) :
>>   missing values are not allowed in 'poly'
>>
>> I thought it might be due to the /diff(range(x) in the function.  But
>> even I remove that part, it will still give me error. Any idea?
>>
>> Many thanks in advance.
>>
>> Alex
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> On Thu, Jul 16, 2015 at 2:09 PM, William Dunlap <wdunlap at tibco.com>
>> wrote:
>>
>>> Read about the 'makepredictcall' generic function.  There is a method,
>>> makepredictcall.poly(), for poly() that attaches the polynomial
>>> coefficients
>>> used during the fitting procedure to the call to poly() that predict()
>>> makes.
>>> You ought to supply a similar method for your xpoly(), and xpoly() needs
>>> to return an object of a a new class that will cause that method to be used.
>>>
>>> E.g.,
>>>
>>> xpoly <- function(x,degree=1,...){ ret <- poly(x,degree=degree,...);
>>> class(ret) <- "xpoly" ; ret }
>>> makepredictcall.xpoly <- function (var, call)
>>> {
>>>     if (is.call(call)) {
>>>         if (identical(call[[1]], quote(xpoly))) {
>>>             if (!is.null(tmp <- attr(var, "coefs"))) {
>>>                 call$coefs <- tmp
>>>             }
>>>         }
>>>     }
>>>     call
>>> }
>>>
>>> g2 <- glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>>> predict(g2,dc)
>>> #             1              2              3              4
>>>  5
>>> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>>> #-0.01398928608
>>> #             6              7              8              9
>>> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>>>
>>> You can see the effects of makepredictcall() in the 'terms' component
>>> of glm's output.  The 'variables' attribute of it gives the original
>>> function
>>> calls and the 'predvars' attribute gives the calls to be used for
>>> prediction:
>>>    > attr(g2$terms, "variables")
>>>    list(lot1, log(u), xpoly(u, 1))
>>>   > attr(g2$terms, "predvars")
>>>   list(lot1, log(u), xpoly(u, 1, coefs = list(alpha = 40, norm2 = c(1,
>>>   9, 8850))))
>>>
>>>
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>> On Thu, Jul 16, 2015 at 12:35 PM, Kunshan Yin <yinkunshan at gmail.com>
>>> wrote:
>>>
>>>> Hello, I have a question about the formula and the user defined
>>>> function:
>>>>
>>>> I can do following:
>>>> ###Case 1:
>>>> > clotting <- data.frame(
>>>> +     u = c(5,10,15,20,30,40,60,80,100),
>>>> +     lot1 = c(118,58,42,35,27,25,21,19,18),
>>>> +     lot2 = c(69,35,26,21,18,16,13,12,12))
>>>> > g1=glm(lot1 ~ log(u) + poly(u,1), data = clotting, family = Gamma)
>>>> > dc=clotting
>>>> > dc$u=1
>>>> > predict(g1,dc)
>>>>           1           2           3           4           5
>>>> 6           7           8           9
>>>> -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929
>>>> -0.01398929 -0.01398929 -0.01398929
>>>>
>>>> However, if I just simply wrap the poly as a user defined function ( in
>>>> reality I would have my own more complex function)  then I will get
>>>> error:
>>>> ###Case 2:
>>>> > xpoly<-function(x,degree=1){poly(x,degree)}
>>>> > g2=glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>>>> > predict(g2,dc)
>>>> Error in poly(x, degree) :
>>>>   'degree' must be less than number of unique points
>>>>
>>>> It seems that the predict always treat the user defined function in the
>>>> formula with I().  My question is how can I get the  results for Case2
>>>> same
>>>> as case1?
>>>>
>>>> Anyone can have any idea about this?
>>>>
>>>> Thank you very much.
>>>>
>>>> Alex
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Fri Jul 17 02:16:11 2015
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 16 Jul 2015 20:16:11 -0400
Subject: [R] powerTransform warning message?
In-Reply-To: <B183F37E-BE5C-4536-BFD4-562E9E263EB9@gmail.com>
References: <B183F37E-BE5C-4536-BFD4-562E9E263EB9@gmail.com>
Message-ID: <web-565718302@cgpsrv2.cis.mcmaster.ca>

Dear Brittany,

On Thu, 16 Jul 2015 17:35:38 -0600
 Brittany Demmitt <demmitba at gmail.com> wrote:
> Hello,
> 
> I have a series of 40 variables that I am trying to transform via the boxcox method using the powerTransfrom function in R.  I have no zero values in any of my variables.  When I run the powerTransform function on the full data set I get the following warning. 
> 
> Warning message:
> In sqrt(diag(solve(res$hessian))) : NaNs produced
> 
> However, when I analyze the variables in groups, rather than all 40 at a time I do not get this warning message.  Why would this be? And does this mean this warning is safe to ignore?
> 

No, it is not safe to ignore the warning, and the problem has nothing to do with non-positive values in the data -- when you say that there are no 0s in the data, I assume that you mean that the data values are all positive. The square-roots of the diagonal entries of the Hessian at the (pseudo-) ML estimates are the SEs of the estimated transformation parameters. If the Hessian can't be inverted, that usually implies that the maximum of the (pseudo-) likelihood isn't well defined. 

This isn't surprising when you're trying to transform as many as 40 variables at a time to multivariate normality. It's my general experience that people often throw their data into the Box-Cox black box and hope for the best without first examining the data, and, e.g., insuring a reasonable ratio of maximum/minimum values for each variable, checking for extreme outliers, etc. Of course, I don't know that you did that, and it's perfectly possible that you were careful.

> I would like to add that all of my lambda values are in the -5 to 5 range.  I also get different lambda values when I analyze the variables together versus in groups.  Is this to be expected?
> 

Yes. It's very unlikely that both are right. If, e.g., the variables are multivariate normal within groups then their marginal distribution is a mixture of multivariate normals, which almost surely isn't itself normal.

I hope this helps,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/
	
	
> Thank you so much!
> 
> Brittany
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Fri Jul 17 07:21:02 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Fri, 17 Jul 2015 15:21:02 +1000
Subject: [R] (ordinal) logistic regression
In-Reply-To: <CALDESV_Y7N7Zj7U3bQOj+c6BwQuJSGXM7z-Q9zFkN6N261B+Jg@mail.gmail.com>
References: <CALDESV_Y7N7Zj7U3bQOj+c6BwQuJSGXM7z-Q9zFkN6N261B+Jg@mail.gmail.com>
Message-ID: <000601d0c050$60cda170$2268e450$@bigpond.com>

Hi 
your example is not reproducible. With ordinal regression the type of the y values is important
sometimes an ordered factor is required.

Ordinal regression depends on your hypothesis see Ananth and Kleinbaum 1997

functions/packages to look at apart from ordinal
VGAM
polr::MASS
bayespolr::arm
lrm::rms

if you want to do GEE that is another matter.

Regards

Duncan 

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au





-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Wolfgang Raffelsberger
Sent: Friday, 17 July 2015 03:41
To: r-help at r-project.org
Subject: [R] (ordinal) logistic regression

Dear list,
I've been looking at previous posts on the list, but I haven't found any
close enough to my question/problem.
My data can be seen as a matrix of mutiple individuals (columns) with
(rather independent) measures (lines). Now based on supplemental
information, the individuals are organized in (multiple) ordered classes.
Now I would like to test for which type of measure (ie which line form my
matrix of data) the groups are distinct (eg different by group-mean). In
other words, I would like to see in which line of my input matrix the
measures for the groups of individuals associate to distinct group-values.

So I tried glm (or glm2) on each line of the matrix, but in my toy-example
(shown below) I'm surprised to get warnings about not finding convergence
in the "nice" toy-cases (ie distinct groups as I am looking for),e even
with glm2 ! I see in such "nice" cases with glm() the "Pr(>|z|)" is close
to 1, which in first sight is OK (since: H0 : coefficient =0), but I
suppose the test is not really set up right this way.  When trying lrm (rms
package) I even get an error message (Unable to fit model using ?lrm.fit?)
with the "nice" cases.
In my real data with >4000 lines of data (ie >4000 glm tests) multiple
testing correction would transform everything from 1-p to end up at p=1, so
that?s another problem with this approach.
I suppose somehow I should transform my data (I don't see where I would
change the H0 ?) to obtain low and NOT high p-values (example below) in the
case I'm looking for, ie when group-means are distinct.

Any suggestions ?

Thank?s in advance,
Wolfgang

Here my toy-example :
datB1 <- c(12,14:16,18:21,20:22,20,22:24,19.5)   # fit
partially/overlapping to 3grp model
datB2 <- c(11:15,21:25,31:36)                    # too beautiful to be real
...
datB3 <- c(15:12,11:15,12:14,15:12)              # no fit to 3grp model
datB4 <- c(11:15,15:11,21:26)                    # grpA == grpB but grpA !=
grpC

datB <- rbind(datB1,datB2,datB3,datB4)
set.seed(2015)
datB <- datB + round(runif(length(datB),-0.3,0.3),1)  # add some noise
datB <- datB - rowMeans(datB)                         # centering
## here the definition of the groups
grpB <- gl(3,6,labels=LETTERS[1:3])[c(-6,-7)]
   table(grpB)

## display
layout(1:4)
for(i in 1:4) plot(datB[i,],as.numeric(grpB))

## now the 'test'
glmLi <- function(dat,grp) {
  ## run glm : predict grp based on dat
  dat <- data.frame(dat=dat,grp=grp)
  glm(grp ~ dat, data=dat, family="binomial")}

logitB <- apply(datB,1,glmLi,grpB)
lapply(logitB,summary)
sapply(logitB,function(x) summary(x)$coefficients[,4])  # lines 1 & 2 are
designed to be 'positive' but give high p-values with convergence problem

## for completness
sessionInfo()
R version 3.2.0 (2015-04-16)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=French_France.1252  LC_CTYPE=French_France.1252
LC_MONETARY=French_France.1252
[4] LC_NUMERIC=C                   LC_TIME=French_France.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] glm2_1.1.2      MASS_7.3-40     TinnRcom_1.0.18 formatR_1.2
svSocket_0.9-57

loaded via a namespace (and not attached):
[1] tools_3.2.0   svMisc_0.9-70 tcltk_3.2.0

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From yinkunshan at gmail.com  Fri Jul 17 00:43:05 2015
From: yinkunshan at gmail.com (Kunshan Yin)
Date: Thu, 16 Jul 2015 15:43:05 -0700
Subject: [R] User defined function within a formula
In-Reply-To: <CAF8bMcbb5+hJ+V3n5+aoAO=TncC2869-iSo5RLfU5-+zn6A3mA@mail.gmail.com>
References: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>
	<CAF8bMcbb5+hJ+V3n5+aoAO=TncC2869-iSo5RLfU5-+zn6A3mA@mail.gmail.com>
Message-ID: <CAGaeMEXRR7hsjf-nPB1k54UHbrXP8Ss895+f5D1FmuJQzdGBoA@mail.gmail.com>

Thanks Bill for your quick reply.

I tried your solution and it did work for the simple user defined function
xploly. But when I try with other function, it gave me error again:

OPoly<-function(x,degree=1,weight=1){
  weight=round(weight,0)# weight need to be integer
  if(length(weight)!=length(x))weight=rep(1,length(x))
  p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)

Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-seq(2)]))[,degree])
  class(Z)<-"OPoly";Z
}

##this OPoly is an FORTRAN orthogonal polynomial routine, it first maps the
x to range[-2,2] then do QR, then return the results with sqrt(norm2).
Comparing with poly, this transformation will make the model coefficients
within a similar range as other variables, the R poly routine will usually
give you a very large coefficients. I did not find such routine in R, so I
have to define this as user defined function.
#######

I  also have following function as you suggested:

makepredictcall.OPoly<-function(var,call)
{
  if (is.call(call)) {
    if (identical(call[[1]], quote(OPoly))) {
      if (!is.null(tmp <- attr(var, "coefs"))) {
        call$coefs <- tmp
      }
    }
  }
  call
}


But I still got error for following:

> g3=glm(lot1 ~ log(u) + OPoly(u,1), data = clotting, family = Gamma)

> predict(g3,dc)Error in poly(4 * (rep(x, weight) - mean(range(x)))/diff(range(x)), degree) :
  missing values are not allowed in 'poly'

I thought it might be due to the /diff(range(x) in the function.  But even
I remove that part, it will still give me error. Any idea?

Many thanks in advance.

Alex























On Thu, Jul 16, 2015 at 2:09 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Read about the 'makepredictcall' generic function.  There is a method,
> makepredictcall.poly(), for poly() that attaches the polynomial
> coefficients
> used during the fitting procedure to the call to poly() that predict()
> makes.
> You ought to supply a similar method for your xpoly(), and xpoly() needs
> to return an object of a a new class that will cause that method to be used.
>
> E.g.,
>
> xpoly <- function(x,degree=1,...){ ret <- poly(x,degree=degree,...);
> class(ret) <- "xpoly" ; ret }
> makepredictcall.xpoly <- function (var, call)
> {
>     if (is.call(call)) {
>         if (identical(call[[1]], quote(xpoly))) {
>             if (!is.null(tmp <- attr(var, "coefs"))) {
>                 call$coefs <- tmp
>             }
>         }
>     }
>     call
> }
>
> g2 <- glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
> predict(g2,dc)
> #             1              2              3              4
>  5
> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
> #-0.01398928608
> #             6              7              8              9
> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>
> You can see the effects of makepredictcall() in the 'terms' component
> of glm's output.  The 'variables' attribute of it gives the original
> function
> calls and the 'predvars' attribute gives the calls to be used for
> prediction:
>    > attr(g2$terms, "variables")
>    list(lot1, log(u), xpoly(u, 1))
>   > attr(g2$terms, "predvars")
>   list(lot1, log(u), xpoly(u, 1, coefs = list(alpha = 40, norm2 = c(1,
>   9, 8850))))
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 16, 2015 at 12:35 PM, Kunshan Yin <yinkunshan at gmail.com>
> wrote:
>
>> Hello, I have a question about the formula and the user defined function:
>>
>> I can do following:
>> ###Case 1:
>> > clotting <- data.frame(
>> +     u = c(5,10,15,20,30,40,60,80,100),
>> +     lot1 = c(118,58,42,35,27,25,21,19,18),
>> +     lot2 = c(69,35,26,21,18,16,13,12,12))
>> > g1=glm(lot1 ~ log(u) + poly(u,1), data = clotting, family = Gamma)
>> > dc=clotting
>> > dc$u=1
>> > predict(g1,dc)
>>           1           2           3           4           5
>> 6           7           8           9
>> -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929
>> -0.01398929 -0.01398929 -0.01398929
>>
>> However, if I just simply wrap the poly as a user defined function ( in
>> reality I would have my own more complex function)  then I will get error:
>> ###Case 2:
>> > xpoly<-function(x,degree=1){poly(x,degree)}
>> > g2=glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>> > predict(g2,dc)
>> Error in poly(x, degree) :
>>   'degree' must be less than number of unique points
>>
>> It seems that the predict always treat the user defined function in the
>> formula with I().  My question is how can I get the  results for Case2
>> same
>> as case1?
>>
>> Anyone can have any idea about this?
>>
>> Thank you very much.
>>
>> Alex
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From flameboy111 at hotmail.com  Fri Jul 17 09:17:45 2015
From: flameboy111 at hotmail.com (TDix)
Date: Fri, 17 Jul 2015 00:17:45 -0700 (PDT)
Subject: [R] Hausman Test trouble - plm
Message-ID: <1437117465859-4709990.post@n4.nabble.com>

Hi there.

I am a student / very fresh R user who is currently having some issues
running the procedure for a Hausman test in R.

The head for my data sheet named "data" looks like this: 

  Bird    Season Gully Grouping   Food   Habitat.Type
1   83      1        1        1         0.15            2
2   47      1        1        1         0.09            1
3   47      1        1        1         1.34            3
4   47      1        1        1         0.09            1
5   51      1        1        3         0.15            2
6   51      1        1        3         0.15            2

The code to run the test looks like this:

library(plm)

result=read.csv("H:/data",header=T,sep=",",stringsAsFactors=F)

wi=plm(Habitat.Type~Season+Gully+Grouping+Food, data = result, index
=c("Bird"),model="within")

re=plm(Habitat.Type~Season+Gully+Grouping+Food, data = result, index
=c("Bird"),model="random")

phtest(wi, re)


The reasoning behind this format is that Habitat.Type is the dependant
variable.
Season, Gully, Grouping and Food are the independant variables.
Bird (ID) is the index as it is the random effect.

The error message I am getting is this:

duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]])

>From what I have been able to figure out this may be because I have multiple
identical observations. The problem is that I do not want to remove these
multiple identical observations, as that is a large part of my data.

My question to you - am I doing anything wrong? Is there a work around for
the duplicate error that I am getting without removing identical
observations?

Thanks so much for your help.




--
View this message in context: http://r.789695.n4.nabble.com/Hausman-Test-trouble-plm-tp4709990.html
Sent from the R help mailing list archive at Nabble.com.


From drjimlemon at gmail.com  Fri Jul 17 12:55:36 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 17 Jul 2015 20:55:36 +1000
Subject: [R] removing the columns with 0 or NA or 1or NA or 2 or NA
In-Reply-To: <CAMqbV1DB-6mgUup_F5rT_NRi=LNXo=OT6Rggv+48sjh08d-2Sw@mail.gmail.com>
References: <CAMqbV1DB-6mgUup_F5rT_NRi=LNXo=OT6Rggv+48sjh08d-2Sw@mail.gmail.com>
Message-ID: <CA+8X3fUvhnt2dfhec=H-h8-d81K=NwG+D9=FDhS6fUpGHZZMhw@mail.gmail.com>

Hi Lida,
I think that your "matrix" is actually a data frame, so try this:

mat[,sapply(mat,function(x) var(x,na.rm=TRUE)>0)]

Jim


On Fri, Jul 17, 2015 at 12:58 AM, Lida Zeighami <lid.zigh at gmail.com> wrote:
> I have ma matrix which its elements are NA,0,1,2 ! I got my answer bout
> removing the columns with 0 or NA or both values but now I want to add
> additional condition for deleting the columns! I have to delete the columns
> which contain the same value. delete the columns with NA or 0 or both and
> the columns with NA or 1 or both and the column with NA or 2 or both (I
> should keep the columns which have variation in their values)! I use this
> code but didn't work properly:
>
> mat_nonNA<- mat[, !apply((is.na(mat) | mat == 0) & (is.na(mat) | mat==1) &(
> is.na(mat) | mat==2), 2, all)]
>
> mat
>          1:110590170    1:110888172     1:110906406   1:110993854
>  1:110996710   1:111144756
> A05363           1                       1                     1
>           2                         NA
> 0
> A05370           0                       1
> 0                NA                         0                     NA
> A05380           1
>          NA                   2                NA
>   NA
> 0
> A05397           0                        1
> 0                NA                         0                       2
> A05400           2                        1
> 0                 2                           0                       0
> A05426
> 0                       NA                     NA             NA
> 0                       1
>
> my out put should be like below:
>
>        1:110590170         1:110906406          1:111144756
> A05363           1                         1
> 0
> A05370           0                          0
> NA
> A05380           1                          2
> 0
> A05397           0
> 0                                 2
> A05400           2                          0
>           0
> A05426           0                         NA
> 1
>
> Thanks for your help
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Fri Jul 17 15:16:44 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 17 Jul 2015 06:16:44 -0700 (PDT)
Subject: [R] Displaying Compositional Data With 46 Parts
Message-ID: <alpine.LNX.2.11.1507170605450.32475@localhost>

   The compositional data have been divided into two data frames: 46 response
variables (the compositional components) and 5 explanatory variables. There
are 209 observations of each. With no experience analyzing large
compositions with so many parts your advice on how to plot and report
results of analyzing these is needed. Matrix plots (scatter, ternary, etc.) of
so many parts would be too small when printed on a page to be readable.

   Haven't found a compositional data water chemistry publication with so
many component parts that could be used as a basis for learning how to work
with such data sets.

   Advice and suggestions needed.

TIA,

Rich


From jrkrideau at inbox.com  Fri Jul 17 15:36:27 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 17 Jul 2015 05:36:27 -0800
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <alpine.LNX.2.11.1507170605450.32475@localhost>
Message-ID: <E69A8FBCD13.00000E3Ejrkrideau@inbox.com>

Hi Rich,
I think this is more a technical question for the subject matter experts than for R-help if I am understanding the question correctly.  

That said, there seems to be an R package called compositional and a corresponding book http://www.springer.com/us/book/9783642368080 that may help.  If nothing else the names of the author(s) of the package or book  or the references may give you some leads on some decent papers.

It, probably, would help anyone with some expertise in the area to be able to look at some of your data.  Have a look at ?dput and perhaps a glance at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html for some suggestions on how to do this. 

John Kane
Kingston ON Canada


> -----Original Message-----
> From: rshepard at appl-ecosys.com
> Sent: Fri, 17 Jul 2015 06:16:44 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] Displaying Compositional Data With 46 Parts
> 
>    The compositional data have been divided into two data frames: 46
> response
> variables (the compositional components) and 5 explanatory variables.
> There
> are 209 observations of each. With no experience analyzing large
> compositions with so many parts your advice on how to plot and report
> results of analyzing these is needed. Matrix plots (scatter, ternary,
> etc.) of
> so many parts would be too small when printed on a page to be readable.
> 
>    Haven't found a compositional data water chemistry publication with so
> many component parts that could be used as a basis for learning how to
> work
> with such data sets.
> 
>    Advice and suggestions needed.
> 
> TIA,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From rshepard at appl-ecosys.com  Fri Jul 17 16:27:55 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 17 Jul 2015 07:27:55 -0700 (PDT)
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <CAErFSojvcpOC7nPpCO6RBE3EfXP5u4OJAO40Gs9LHtoBzmmpyA@mail.gmail.com>
References: <alpine.LNX.2.11.1507170605450.32475@localhost>
	<CAErFSojvcpOC7nPpCO6RBE3EfXP5u4OJAO40Gs9LHtoBzmmpyA@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507170722340.32475@localhost>

On Fri, 17 Jul 2015, Aaron Mackey wrote:

> One immediate question is how independent you believe the 46 components to
> be, and whether certain components could be reduced or otherwise
> coordinately-modeled; a heatmap of your 46x46 pairwise correlations should
> be informative. Also consider log-scaling, especially if some component
> fractions can be very small/minor components compared to large/major
> components.

Aaron,

   Most components are elements spread across the periodic table; the rest
are composits such as total dissolved/suspended solids, acid neutralizing
capacity, specific conductance, bicarbonate.

   A heat map will be drawn. Log scaling is the key to working with
compositional data. The log-ratios can be calculated using three equations
with two being most frequently applied, each depending on the analysis to
follow.

Thanks,

Rich


From rshepard at appl-ecosys.com  Fri Jul 17 16:33:43 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 17 Jul 2015 07:33:43 -0700 (PDT)
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <E69A8FBCD13.00000E3Ejrkrideau@inbox.com>
References: <E69A8FBCD13.00000E3Ejrkrideau@inbox.com>
Message-ID: <alpine.LNX.2.11.1507170728170.32475@localhost>

On Fri, 17 Jul 2015, John Kane wrote:

> I think this is more a technical question for the subject matter experts
> than for R-help if I am understanding the question correctly.

John,

   I agree completely. Unfortunately, there is no R SIG devoted to CoDA, nor
any other mail list or Web forum that I've been able to find. There is a
CoDA Web site but no active forum.

> That said, there seems to be an R package called compositional and a
> corresponding book http://www.springer.com/us/book/9783642368080 that may
> help. If nothing else the names of the author(s) of the package or book or
> the references may give you some leads on some decent papers.

   In addition to compositions, there's robCompositions and zCompositions.
The book was where I learned how to analyze compositional data. I've
communicated with several of the dozen-or-so statisticians focused on CoDa,
and have a couple of dozen papers, book chapters, and proceedings of the
triennial conferences on compositional data analyses. I've also written a
monograph that demonstrates how CoDA models applied to benthic
macroinvertebrate functional feeding groups can assess water quality and be
used to set standards.

Thanks,

Rich


From flameboy111 at hotmail.com  Fri Jul 17 10:27:49 2015
From: flameboy111 at hotmail.com (TDix)
Date: Fri, 17 Jul 2015 01:27:49 -0700 (PDT)
Subject: [R] Hausman Test trouble - plm
In-Reply-To: <1437117465859-4709990.post@n4.nabble.com>
References: <1437117465859-4709990.post@n4.nabble.com>
Message-ID: <1437121669582-4709992.post@n4.nabble.com>

Might have just solved my own problem team!

I assumed that the issue here was the replicated samples, and so added a
column and gave a number to each replicate.

R seemed to like this and was happy to run the test!

A significant result tells me that the fixed effects model is the most
preferable model to explain the variation seen in my data.

Unless I am doing/assuming something wrong here that you can see then I
might well have solved my own problem.

Let me know if you have any thoughts :)

Cheers



--
View this message in context: http://r.789695.n4.nabble.com/Hausman-Test-trouble-plm-tp4709990p4709992.html
Sent from the R help mailing list archive at Nabble.com.


From aph41 at alumnos.unican.es  Fri Jul 17 13:04:14 2015
From: aph41 at alumnos.unican.es (aidaph)
Date: Fri, 17 Jul 2015 04:04:14 -0700 (PDT)
Subject: [R]
 =?utf-8?q?installation_of_package_=E2=80=98devtools=E2=80=99_?=
 =?utf-8?q?had_non-zero_exit_status_in_a_POWERPC?=
In-Reply-To: <1436511558524-4709687.post@n4.nabble.com>
References: <1436511558524-4709687.post@n4.nabble.com>
Message-ID: <1437131054286-4709994.post@n4.nabble.com>

I'm stuck on this and i don't know how to install Curl without any problem.
Anyone?



--
View this message in context: http://r.789695.n4.nabble.com/installation-of-package-devtools-had-non-zero-exit-status-in-a-POWERPC-tp4709687p4709994.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From issoufou.ouedraogo at student.uclouvain.be  Fri Jul 17 16:21:17 2015
From: issoufou.ouedraogo at student.uclouvain.be (Issoufou Ouedraogo)
Date: Fri, 17 Jul 2015 16:21:17 +0200
Subject: [R] "grid"Package not available in r
Message-ID: <CAK+v5NtkRSbzyifC9_XBu+NG7+p53zHbxo4MFENE9XtQR4hg4g@mail.gmail.com>

Dear  Responsible,

Hello!

I am a PhD student at Universit? Catholique de Louvain, I contact you
because I have some difficulties to install "neuralnet" package in R for my
research. However, to install this package, we need two packages "MASS" and
"grid". The grid package is not available in my  R version.  I use R 3.1.1
version for my research.
Please help me. I need to develop neural netwok in my research.
For example,  I have write very well my script but, if I run, the problem
is grid package not availbable.

  install.packages("grid")
Installing package into ?C:/Users/issoufou/Documents/R/win-library/3.2?
(as ?lib? is unspecified)
Warning in install.packages :
   package ?grid? is not available (for R version 3.2.0)

Best regards

*Issoufou Ouedraogo*
*PhD Student*
*Earth and Life Insitute/ Environmental Sciences*
*Universit? Catholique de Louvain**/Belgique*
*Croix du sud 2, bte 1, B-1348, Louvain la Neuve,Belgique*
*Tel: +32 (0) 10/47.37.19 <%2B32%20%280%29%2010%2F47.37.19>*
*Fax: +32 (0) 10/47.47.45 <%2B32%20%280%29%2010%2F47.47.45>*

	[[alternative HTML version deleted]]


From tracy_diver at fws.gov  Fri Jul 17 16:45:06 2015
From: tracy_diver at fws.gov (tdiver)
Date: Fri, 17 Jul 2015 07:45:06 -0700 (PDT)
Subject: [R] TukeyHSD troubles
In-Reply-To: <1267168200738-1570228.post@n4.nabble.com>
References: <c55bda7a1002252239o1d6a1bbagce154437bde72dbe@mail.gmail.com>
	<1267168200738-1570228.post@n4.nabble.com>
Message-ID: <1437144306235-4709999.post@n4.nabble.com>

Bart,

I want to thank you for your code.  I was having similar problems as Amy,
even after setting my numeric variable as a factor using as.factor().  I
used is.factor() to confirm and received the answer as TRUE from R; however
after running the TukeyHSD() my set factor in my aov() was not read
properly.  I  used your code and it worked perfectly!

Thank you!

Tracy



--
View this message in context: http://r.789695.n4.nabble.com/TukeyHSD-troubles-tp1570205p4709999.html
Sent from the R help mailing list archive at Nabble.com.


From yinkunshan at gmail.com  Fri Jul 17 17:29:06 2015
From: yinkunshan at gmail.com (Kunshan Yin)
Date: Fri, 17 Jul 2015 08:29:06 -0700
Subject: [R] User defined function within a formula
In-Reply-To: <CAF8bMcbSXEpb02tomJC0gsTh+0ORwEPxY1BAbRk4RTewHLA+xQ@mail.gmail.com>
References: <CAGaeMEVJoxCLZG-AUFVNFoabPf0S2O29Ve6R73Ru8Ob6VCXCJQ@mail.gmail.com>
	<CAF8bMcbb5+hJ+V3n5+aoAO=TncC2869-iSo5RLfU5-+zn6A3mA@mail.gmail.com>
	<CAGaeMEXRR7hsjf-nPB1k54UHbrXP8Ss895+f5D1FmuJQzdGBoA@mail.gmail.com>
	<CAF8bMcb_JSPQQUkM3NXguPxdiv_Dr3QR6p-r7p+XTnnosz_Jug@mail.gmail.com>
	<CAF8bMcbSXEpb02tomJC0gsTh+0ORwEPxY1BAbRk4RTewHLA+xQ@mail.gmail.com>
Message-ID: <CAGaeMEXNACCtN0RCnRF=GV6RoSD5Qjh8bU9s58LJ0mW+C=7WVA@mail.gmail.com>

Thank you very much. It worked. I think I need to digest this further
later. Thanks again for the help.

On Thu, Jul 16, 2015 at 4:51 PM, William Dunlap <wdunlap at tibco.com> wrote:

> This might do what you want:
>
> OPoly <- function(x, degree=1, weight=1, coefs=NULL, rangeX=NULL){
>   weight <- round(weight,0)# weight need to be integer
>   if(length(weight)!=length(x)) {
>     weight <- rep(1,length(x))
>   }
>   if (is.null(rangeX)) {
>       rangeX <- range(x)
>   }
>   p <- poly(4*(rep(x,weight)-mean(rangeX))/diff(rangeX), degree=degree,
> coefs=coefs)
>   # why t(t(...))?  That strips the attributes.
>   Z <- t( t(p[cumsum(weight),]) * sqrt(attr(p,"coefs")$norm2[-seq(2)]) )[,
> degree, drop=FALSE]
>   class(Z) <- "OPoly"
>   attr(Z, "coefs") <- attr(p, "coefs")
>   attr(Z, "rangeX") <- rangeX
>   Z
> }
>
> makepredictcall.OPoly<-function(var,call)
> {
>   if (is.call(call)) {
>     if (identical(call[[1]], quote(OPoly))) {
>       if (!is.null(tmp <- attr(var, "coefs"))) {
>         call$coefs <- tmp
>       }
>       if (!is.null(tmp <- attr(var, "rangeX"))) {
>         call$rangeX <- tmp
>       }
>       call$weight <- 1 # weight not relevant in predictions
>     }
>   }
>   call
> }
>
> d <- data.frame(Y=1:8, X=log(1:8), Weight=1:8)
> fit <- lm(data=d, Y ~ OPoly(X, degree=2, weight=Weight))
> predict(fit)[c(3,8)]
> predict(fit, newdata=data.frame(X=d$X[c(3,8)])) # same result
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 16, 2015 at 4:39 PM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> OPoly<-function(x,degree=1,weight=1){
>>   weight=round(weight,0)# weight need to be integer
>>   if(length(weight)!=length(x))weight=rep(1,length(x))
>>   p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)
>>   Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-
>> seq(2)]))[,degree])
>>   class(Z)<-"OPoly";Z
>> }
>>
>> You need to make OPoly to have optional argument(s) that give
>> the original-regressor-dependent information to OPoly and then
>> have it return, as attributes, the value of those arguments.
>>  makepredictcall
>> will take the attributes and attach them to the call in predvars so
>> predict uses values derived from the original regressors, not value
>> derived
>> from the data to be predicted from.
>>
>> Take a look at a pair like makepredictcall.scale() and scale() for an
>> example:
>> scale has optional arguments 'center' and 'scale' that it returns as
>> attributes
>> and makepredictcall.scale adds those to the call to scale that it is
>> given.
>> Thus when you predict, the scale and center arguments come from the
>> original data, not from the data you are predicting from.
>>
>>
>>
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Thu, Jul 16, 2015 at 3:43 PM, Kunshan Yin <yinkunshan at gmail.com>
>> wrote:
>>
>>> Thanks Bill for your quick reply.
>>>
>>> I tried your solution and it did work for the simple user defined
>>> function xploly. But when I try with other function, it gave me error again:
>>>
>>> OPoly<-function(x,degree=1,weight=1){
>>>   weight=round(weight,0)# weight need to be integer
>>>   if(length(weight)!=length(x))weight=rep(1,length(x))
>>>   p=poly(4*(rep(x,weight)-mean(range(x)))/diff(range(x)),degree)
>>>
>>> Z<-(t(t(p[cumsum(weight),])*sqrt(attr(p,"coefs")$norm2[-seq(2)]))[,degree])
>>>   class(Z)<-"OPoly";Z
>>> }
>>>
>>> ##this OPoly is an FORTRAN orthogonal polynomial routine, it first maps
>>> the x to range[-2,2] then do QR, then return the results with sqrt(norm2).
>>> Comparing with poly, this transformation will make the model coefficients
>>> within a similar range as other variables, the R poly routine will usually
>>> give you a very large coefficients. I did not find such routine in R, so I
>>> have to define this as user defined function.
>>> #######
>>>
>>> I  also have following function as you suggested:
>>>
>>> makepredictcall.OPoly<-function(var,call)
>>> {
>>>   if (is.call(call)) {
>>>     if (identical(call[[1]], quote(OPoly))) {
>>>       if (!is.null(tmp <- attr(var, "coefs"))) {
>>>         call$coefs <- tmp
>>>       }
>>>     }
>>>   }
>>>   call
>>> }
>>>
>>>
>>> But I still got error for following:
>>>
>>> > g3=glm(lot1 ~ log(u) + OPoly(u,1), data = clotting, family = Gamma)
>>>
>>> > predict(g3,dc)Error in poly(4 * (rep(x, weight) - mean(range(x)))/diff(range(x)), degree) :
>>>   missing values are not allowed in 'poly'
>>>
>>> I thought it might be due to the /diff(range(x) in the function.  But
>>> even I remove that part, it will still give me error. Any idea?
>>>
>>> Many thanks in advance.
>>>
>>> Alex
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Thu, Jul 16, 2015 at 2:09 PM, William Dunlap <wdunlap at tibco.com>
>>> wrote:
>>>
>>>> Read about the 'makepredictcall' generic function.  There is a method,
>>>> makepredictcall.poly(), for poly() that attaches the polynomial
>>>> coefficients
>>>> used during the fitting procedure to the call to poly() that predict()
>>>> makes.
>>>> You ought to supply a similar method for your xpoly(), and xpoly()
>>>> needs to return an object of a a new class that will cause that method to
>>>> be used.
>>>>
>>>> E.g.,
>>>>
>>>> xpoly <- function(x,degree=1,...){ ret <- poly(x,degree=degree,...);
>>>> class(ret) <- "xpoly" ; ret }
>>>> makepredictcall.xpoly <- function (var, call)
>>>> {
>>>>     if (is.call(call)) {
>>>>         if (identical(call[[1]], quote(xpoly))) {
>>>>             if (!is.null(tmp <- attr(var, "coefs"))) {
>>>>                 call$coefs <- tmp
>>>>             }
>>>>         }
>>>>     }
>>>>     call
>>>> }
>>>>
>>>> g2 <- glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>>>> predict(g2,dc)
>>>> #             1              2              3              4
>>>>    5
>>>> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>>>> #-0.01398928608
>>>> #             6              7              8              9
>>>> #-0.01398928608 -0.01398928608 -0.01398928608 -0.01398928608
>>>>
>>>> You can see the effects of makepredictcall() in the 'terms' component
>>>> of glm's output.  The 'variables' attribute of it gives the original
>>>> function
>>>> calls and the 'predvars' attribute gives the calls to be used for
>>>> prediction:
>>>>    > attr(g2$terms, "variables")
>>>>    list(lot1, log(u), xpoly(u, 1))
>>>>   > attr(g2$terms, "predvars")
>>>>   list(lot1, log(u), xpoly(u, 1, coefs = list(alpha = 40, norm2 = c(1,
>>>>   9, 8850))))
>>>>
>>>>
>>>>
>>>> Bill Dunlap
>>>> TIBCO Software
>>>> wdunlap tibco.com
>>>>
>>>> On Thu, Jul 16, 2015 at 12:35 PM, Kunshan Yin <yinkunshan at gmail.com>
>>>> wrote:
>>>>
>>>>> Hello, I have a question about the formula and the user defined
>>>>> function:
>>>>>
>>>>> I can do following:
>>>>> ###Case 1:
>>>>> > clotting <- data.frame(
>>>>> +     u = c(5,10,15,20,30,40,60,80,100),
>>>>> +     lot1 = c(118,58,42,35,27,25,21,19,18),
>>>>> +     lot2 = c(69,35,26,21,18,16,13,12,12))
>>>>> > g1=glm(lot1 ~ log(u) + poly(u,1), data = clotting, family = Gamma)
>>>>> > dc=clotting
>>>>> > dc$u=1
>>>>> > predict(g1,dc)
>>>>>           1           2           3           4           5
>>>>> 6           7           8           9
>>>>> -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929 -0.01398929
>>>>> -0.01398929 -0.01398929 -0.01398929
>>>>>
>>>>> However, if I just simply wrap the poly as a user defined function ( in
>>>>> reality I would have my own more complex function)  then I will get
>>>>> error:
>>>>> ###Case 2:
>>>>> > xpoly<-function(x,degree=1){poly(x,degree)}
>>>>> > g2=glm(lot1 ~ log(u) + xpoly(u,1), data = clotting, family = Gamma)
>>>>> > predict(g2,dc)
>>>>> Error in poly(x, degree) :
>>>>>   'degree' must be less than number of unique points
>>>>>
>>>>> It seems that the predict always treat the user defined function in the
>>>>> formula with I().  My question is how can I get the  results for Case2
>>>>> same
>>>>> as case1?
>>>>>
>>>>> Anyone can have any idea about this?
>>>>>
>>>>> Thank you very much.
>>>>>
>>>>> Alex
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Jul 17 17:45:11 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 17 Jul 2015 08:45:11 -0700
Subject: [R] "grid"Package not available in r
In-Reply-To: <CAK+v5NtkRSbzyifC9_XBu+NG7+p53zHbxo4MFENE9XtQR4hg4g@mail.gmail.com>
References: <CAK+v5NtkRSbzyifC9_XBu+NG7+p53zHbxo4MFENE9XtQR4hg4g@mail.gmail.com>
Message-ID: <AEAED107-90F4-4211-A862-F30C2A99B66A@comcast.net>


On Jul 17, 2015, at 7:21 AM, Issoufou Ouedraogo wrote:

> Dear  Responsible,
> 
> Hello!
> 
> I am a PhD student at Universit? Catholique de Louvain, I contact you
> because I have some difficulties to install "neuralnet" package in R for my
> research. However, to install this package, we need two packages "MASS" and
> "grid". The grid package is not available in my  R version.  I use R 3.1.1
> version for my research.
> Please help me. I need to develop neural netwok in my research.
> For example,  I have write very well my script but, if I run, the problem
> is grid package not availbable.
> 
>  install.packages("grid")


The grid package is part of the standard R installation. It is not available on CRAN as a separate package. Just do this:

library(grid)  # you should get a message
help(pac=grid)

-- 
David.
> Installing package into ?C:/Users/issoufou/Documents/R/win-library/3.2?
> (as ?lib? is unspecified)
> Warning in install.packages :
>   package ?grid? is not available (for R version 3.2.0)
> 
> Best regards
> 
> *Issoufou Ouedraogo*
> *PhD Student*
> *Earth and Life Insitute/ Environmental Sciences*
> *Universit? Catholique de Louvain**/Belgique*
> *Croix du sud 2, bte 1, B-1348, Louvain la Neuve,Belgique*
> *Tel: +32 (0) 10/47.37.19 <%2B32%20%280%29%2010%2F47.37.19>*
> *Fax: +32 (0) 10/47.47.45 <%2B32%20%280%29%2010%2F47.47.45>*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From demmitba at gmail.com  Fri Jul 17 18:08:45 2015
From: demmitba at gmail.com (Brittany Demmitt)
Date: Fri, 17 Jul 2015 10:08:45 -0600
Subject: [R] powerTransform warning message?
In-Reply-To: <web-565718302@cgpsrv2.cis.mcmaster.ca>
References: <B183F37E-BE5C-4536-BFD4-562E9E263EB9@gmail.com>
	<web-565718302@cgpsrv2.cis.mcmaster.ca>
Message-ID: <D44DA57D-57B1-4CC8-AF32-EE92658EE845@gmail.com>

Thank you so much for the explanation.  That was very helpful! :-)  

Thanks!

Brittany


> On Jul 16, 2015, at 6:16 PM, John Fox <jfox at mcmaster.ca> wrote:
> 
> Dear Brittany,
> 
> On Thu, 16 Jul 2015 17:35:38 -0600
> Brittany Demmitt <demmitba at gmail.com> wrote:
>> Hello,
>> 
>> I have a series of 40 variables that I am trying to transform via the boxcox method using the powerTransfrom function in R.  I have no zero values in any of my variables.  When I run the powerTransform function on the full data set I get the following warning. 
>> 
>> Warning message:
>> In sqrt(diag(solve(res$hessian))) : NaNs produced
>> 
>> However, when I analyze the variables in groups, rather than all 40 at a time I do not get this warning message.  Why would this be? And does this mean this warning is safe to ignore?
>> 
> 
> No, it is not safe to ignore the warning, and the problem has nothing to do with non-positive values in the data -- when you say that there are no 0s in the data, I assume that you mean that the data values are all positive. The square-roots of the diagonal entries of the Hessian at the (pseudo-) ML estimates are the SEs of the estimated transformation parameters. If the Hessian can't be inverted, that usually implies that the maximum of the (pseudo-) likelihood isn't well defined. 
> 
> This isn't surprising when you're trying to transform as many as 40 variables at a time to multivariate normality. It's my general experience that people often throw their data into the Box-Cox black box and hope for the best without first examining the data, and, e.g., insuring a reasonable ratio of maximum/minimum values for each variable, checking for extreme outliers, etc. Of course, I don't know that you did that, and it's perfectly possible that you were careful.
> 
>> I would like to add that all of my lambda values are in the -5 to 5 range.  I also get different lambda values when I analyze the variables together versus in groups.  Is this to be expected?
>> 
> 
> Yes. It's very unlikely that both are right. If, e.g., the variables are multivariate normal within groups then their marginal distribution is a mixture of multivariate normals, which almost surely isn't itself normal.
> 
> I hope this helps,
> John
> 
> ------------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.mcmaster.ca/jfox/
> 	
> 	
>> Thank you so much!
>> 
>> Brittany
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 	


From jdnewmil at dcn.davis.CA.us  Fri Jul 17 18:10:16 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 17 Jul 2015 09:10:16 -0700
Subject: [R]
	=?utf-8?q?installation_of_package_=E2=80=98devtools=E2=80=99_?=
	=?utf-8?q?had_non-zero_exit_status_in_a_POWERPC?=
In-Reply-To: <1437131054286-4709994.post@n4.nabble.com>
References: <1436511558524-4709687.post@n4.nabble.com>
	<1437131054286-4709994.post@n4.nabble.com>
Message-ID: <709984CE-484E-4DB6-BD5F-D4336936AFE1@dcn.davis.CA.us>

That is an operating-system configuration question, not a question about R. There are many OSs out there... please find a forum with users of your OS in which to pursue this question.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 17, 2015 4:04:14 AM PDT, aidaph <aph41 at alumnos.unican.es> wrote:
>I'm stuck on this and i don't know how to install Curl without any
>problem.
>Anyone?
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/installation-of-package-devtools-had-non-zero-exit-status-in-a-POWERPC-tp4709687p4709994.html
>Sent from the R help mailing list archive at Nabble.com.
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Fri Jul 17 18:19:07 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 17 Jul 2015 08:19:07 -0800
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <alpine.LNX.2.11.1507170728170.32475@localhost>
References: <e69a8fbcd13.00000e3ejrkrideau@inbox.com>
Message-ID: <E80628BF495.0000001Bjrkrideau@inbox.com>


See in-line

John Kane
Kingston ON Canada


> -----Original Message-----
> From: rshepard at appl-ecosys.com
> Sent: Fri, 17 Jul 2015 07:33:43 -0700 (PDT)
> To: r-help at r-project.org
> Subject: Re: [R] Displaying Compositional Data With 46 Parts
> 
> On Fri, 17 Jul 2015, John Kane wrote:
> 
>> I think this is more a technical question for the subject matter experts
>> than for R-help if I am understanding the question correctly.
> 
> John,
> 
>    I agree completely. Unfortunately, there is no R SIG devoted to CoDA,
> nor
> any other mail list or Web forum that I've been able to find. There is a
> CoDA Web site but no active forum.
> 
>> That said, there seems to be an R package called compositional and a
>> corresponding book http://www.springer.com/us/book/9783642368080 that
>> may
>> help. If nothing else the names of the author(s) of the package or book
>> or
>> the references may give you some leads on some decent papers.
> 
>    In addition to compositions, there's robCompositions and
> zCompositions.
> The book was where I learned how to analyze compositional data. I've
> communicated with several of the dozen-or-so statisticians focused on
> CoDa,
> and have a couple of dozen papers, book chapters, and proceedings of the
> triennial conferences on compositional data analyses. I've also written a
> monograph that demonstrates how CoDA models applied to benthic
> macroinvertebrate functional feeding groups can assess water quality and
> be
> used to set standards.

Then it sounds like you are one of the experts. Do whatever you think appropriate and either set the standard for future research or get enough feedback to do even better next time.  :)

> 
> Thanks,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Send your photos by email in seconds...
TRY FREE IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if3
Works in all emails, instant messengers, blogs, forums and social networks.


From pd.mes at cbs.dk  Fri Jul 17 18:23:56 2015
From: pd.mes at cbs.dk (Peter Dalgaard)
Date: Fri, 17 Jul 2015 18:23:56 +0200
Subject: [R] Release of R 3.2.2 scheduled for August 14
Message-ID: <1D0E985F-12DC-41DD-8BDF-F00DC3AA4C06@cbs.dk>

We intend to have a patch release on August 14, nickname will be "Fire Safety". The detailed schedule will be made available via developer.r-project.org as usual.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From rshepard at appl-ecosys.com  Fri Jul 17 18:41:10 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 17 Jul 2015 09:41:10 -0700 (PDT)
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <E80628BF495.0000001Bjrkrideau@inbox.com>
References: <e69a8fbcd13.00000e3ejrkrideau@inbox.com>
	<E80628BF495.0000001Bjrkrideau@inbox.com>
Message-ID: <alpine.LNX.2.11.1507170934070.32475@localhost>

On Fri, 17 Jul 2015, John Kane wrote:

> Then it sounds like you are one of the experts. Do whatever you think
> appropriate and either set the standard for future research or get enough
> feedback to do even better next time. :)

John,

   Far from an expert, but becoming more capable with each project.

   Someone, somewhere, has dealt successfully with this issue in the past. So
I need to keep inquiring and hoping that e-mail addresses are still valid
and that I get responses. :-)

   Compositional data are the rule for public (e.g., economic, political,
societal) and environmental chemical data, including some biological data.
It seems to be unknown outside of those in academia focused on developing
the mathematical theory and applying the models to their research.

Regards,

Rich


From rshepard at appl-ecosys.com  Fri Jul 17 19:02:24 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 17 Jul 2015 10:02:24 -0700 (PDT)
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <CAGxFJbRjLuk7HdE5wsT4gm-4NUfjH-jZq3nXvbGt=fT9Fg+V9Q@mail.gmail.com>
References: <e69a8fbcd13.00000e3ejrkrideau@inbox.com>
	<alpine.LNX.2.11.1507170728170.32475@localhost>
	<E80628BF495.0000001Bjrkrideau@inbox.com>
	<CAGxFJbRjLuk7HdE5wsT4gm-4NUfjH-jZq3nXvbGt=fT9Fg+V9Q@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507170955270.32475@localhost>

On Fri, 17 Jul 2015, Bert Gunter wrote:

> I believe John Aitchison's book and papers are the authoritative basic
> resources. Have you read them?

Bert,

   Yes, I have.

> The problem is that the support of the distributions are (hyper)simplexes,
> not Euclidean space, due to the requirement that the proportions must sum
> to 1. This means that complicated animals like Dirichelet distributions
> must be used to model populations, and the sampling theory is therefore
> specialized. It's difficult for most folks to get their heads around this.

   That's true, When I read the math I move my lips and follow along with a
finger. :-)

   My question is focused on presentation of graphic presentation of the
data, such as a matrix of ternary diagrams that show the distribution of the
response variables to the explanatory variables. The analysis of benthic
macroinvertebrate functional feeding groups has 5 response variables which
resulted in a 5-by-5 ternary diagram matrix. Anything larger than that would
require the eyesight of a teenager to see any details.

   I'll keep searching the literature for a suitable example. Perhaps a CoDA
SIG will develop on within the R mail list ecosystem in the not-too-distant
future.

Thanks,

Rich


From oluola2011 at yahoo.com  Fri Jul 17 19:55:08 2015
From: oluola2011 at yahoo.com (Olu Ola)
Date: Fri, 17 Jul 2015 17:55:08 +0000 (UTC)
Subject: [R] OPTIMX: non-finite finite-difference value [26] and scaling
	problem
Message-ID: <1254082374.2956615.1437155708813.JavaMail.yahoo@mail.yahoo.com>

Hello,I am running a nonlinear GMM using the optimx wrapper. I am trying to estimate 37 variables however and my code for the optimx is:
nlgmm = optimx(par=b0, fn=obj,method = "BFGS", itnmax=10000, control=list(follow.on = TRUE,kkt=FALSE,starttests=TRUE,save.failures=TRUE, trace=0))

My staring values are from Ordinary least squares estimates (OLS) and they are:
b0 <- c(-2.00658,-0.04373,0.19079,0.34652,-0.36814,0.21284,-0.24369,0.64622,0.22927,0.29431,0.19547,0.80614,18.8398,0.5928,3.1375,0.4301,-0.4937,2.2016,31.5203,0.6171,1.0206,1.7830,-0.4421,11.0076,-0.03305,0.17087,0.44794,0.17488,0.10781,-0.50747,-0.04563,0.17030,0.41792,0.17526,0.99734,-17.2996,-41.9359)

I got the following error:
Error in optim(par = par, fn = ufn, gr = ugr, lower = lower, upper = upper, ?:?? non-finite finite-difference value [26]
I also got an error about scaling which is as follows:
Parameters or bounds appear to have differentscalings.This can cause poor performance in optimization.  Itis important for derivative free methods like BOBYQA, UOBYQA, NEWUOA.


any help will be greatly appreciated.
Best Regards
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Jul 17 23:54:20 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 18 Jul 2015 07:54:20 +1000
Subject: [R] Displaying Compositional Data With 46 Parts
In-Reply-To: <alpine.LNX.2.11.1507170955270.32475@localhost>
References: <e69a8fbcd13.00000e3ejrkrideau@inbox.com>
	<alpine.LNX.2.11.1507170728170.32475@localhost>
	<E80628BF495.0000001Bjrkrideau@inbox.com>
	<CAGxFJbRjLuk7HdE5wsT4gm-4NUfjH-jZq3nXvbGt=fT9Fg+V9Q@mail.gmail.com>
	<alpine.LNX.2.11.1507170955270.32475@localhost>
Message-ID: <CA+8X3fUuET4=z6qcjpbD0kOaS3v8wd46KpPidn6KR5EBpxR5Aw@mail.gmail.com>

Hi Rich,
Being in a position of relative ignorance on this topic, I'll offer
some suggestions that may well be useless.

You mention ternary diagrams, which use position to represent
compositional proportions. These will not scale up to 46 values in any
way that I can imagine. If you want to display relative concentration
or the like, differentiate the components and display numeric
information about each component, you may be looking for a variant of
the Hinton diagram. This is a bit like a heatmap where the squares are
of different sizes, representing relative numeric values. In the
Hinton diagram, the colors represent sign (+-), but you would probably
want more complex coloring. Finally, identifying labels and/or values
could be displayed on each cell of the matrix. It would not be too
difficult to program something like this, so if this idea is not
completely useless, let me know.

It is of course possible to go the interactive route and produce a
"play with me" display if necessary.

Jim


From varinsacha at yahoo.fr  Sat Jul 18 01:33:07 2015
From: varinsacha at yahoo.fr (varin sacha)
Date: Fri, 17 Jul 2015 23:33:07 +0000 (UTC)
Subject: [R] Nagelkerke Pseudo R-squared
Message-ID: <253097945.6053468.1437175987347.JavaMail.yahoo@mail.yahoo.com>

Dear R-Experts,

I have fitted an ordinal logistic regression with just 1 explanatory variable for the reproducible example here below.

Everything is working, now I try to calculate the Nagelkerke Pseudo R-squared. 
I have found a package BaylorEdPsych providing many Pseudo R-squared, but the example shown in the package is for GLM (binary logistic regression) not for ordinal logistic regression.
How can I calculate the Nagelkerke Pseudo R-squared for ordinal logistic regression ?

Many thanks as usual for your precious help.

Reproducible example :

install.packages("MASS") 
library(MASS) 
a=factor(c("tres grand", "grand", "petit","petit","tres grand","grand","petit","petit","tres grand","grand")) 
b=c("homme", "homme", "femme", "femme", "femme", "homme", "homme", "homme", "femme", "femme") 
m <- polr(a ~ b, Hess=TRUE) 
summary(m)


From dwinsemius at comcast.net  Sat Jul 18 03:33:11 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 17 Jul 2015 18:33:11 -0700
Subject: [R] Nagelkerke Pseudo R-squared
In-Reply-To: <253097945.6053468.1437175987347.JavaMail.yahoo@mail.yahoo.com>
References: <253097945.6053468.1437175987347.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <0AB4C941-2D41-43DB-8373-FF6E8206770C@comcast.net>


On Jul 17, 2015, at 4:33 PM, varin sacha wrote:

> Dear R-Experts,
> 
> I have fitted an ordinal logistic regression with just 1 explanatory variable for the reproducible example here below.
> 
> Everything is working, now I try to calculate the Nagelkerke Pseudo R-squared. 
> I have found a package BaylorEdPsych providing many Pseudo R-squared, but the example shown in the package is for GLM (binary logistic regression) not for ordinal logistic regression.
> How can I calculate the Nagelkerke Pseudo R-squared for ordinal logistic regression ?

polr-objects have a deviance node. If this has statistical value (which I have some doubts regarding) then just apply the usual formula to compare nested models.

-- 
David.

> 
> Many thanks as usual for your precious help.
> 
> Reproducible example :
> 
> install.packages("MASS") 
> library(MASS) 
> a=factor(c("tres grand", "grand", "petit","petit","tres grand","grand","petit","petit","tres grand","grand")) 
> b=c("homme", "homme", "femme", "femme", "femme", "homme", "homme", "homme", "femme", "femme") 
> m <- polr(a ~ b, Hess=TRUE) 
> summary(m)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From marammagdysalem at gmail.com  Sat Jul 18 02:46:29 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Sat, 18 Jul 2015 02:46:29 +0200
Subject: [R] Warning message with maxLik()
Message-ID: <CAPLSCn0v=xZiDGzZZcfQNFe277fvEKO7OowTYgHzJ0mnb5VGPw@mail.gmail.com>

Dear All,
I'm trying to get the MLe for a certain distribution using maxLik ()
function. I wrote the log-likelihood function as follows:
theta <-vector(mode = "numeric", length = 3)
r<- 17
n <-30
 T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
C<-
c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
# The  loglik. func.
loglik <- function(param) {
 theta[1]<- param[1]
 theta[2]<- param[2]
 theta[3]<- param[3]
 l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+
(-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+
(-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
return(l)
 }

then, I evaluated it at theta<- c(40,50,2)

v<-loglik(param=theta)
v
[1] -56.66653

I used this same log-likelihood function, once with analytic gradient and
another time with numerical one, with the maxLik function, and in both
cases I got the same 50 warning messages and an MLE which is completely
unrealistic as per my applied example.

a <- maxLik(loglik, gradlik, hesslik, start=c(40,50,2))

where gradlik and hesslik are the analytic gradient and Hessian matrix,
respectively, given by:

U <- vector(mode="numeric",length=3)
gradlik<-function(param = theta,n, T,C)
 {
U <- vector(mode="numeric",length=3)
theta[1] <- param[1]
theta[2] <- param[2]
theta[3] <- param[3]
r<- 17
n <-30
T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
C<-
c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
 U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+(
-1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
(-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+
(-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
(-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+
(-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
return(U)
}
hesslik<-function(param=theta,n,T,C)
{
theta[1] <- param[1]
theta[2] <- param[2]
theta[3] <- param[3]
r<- 17
n <-30
T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
C<-
c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
G<- matrix(nrow=3,ncol=3)
G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+
(theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+
(theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+
(theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
G[2,1]<-G[1,2]
G[1,3]<-(n/theta[1])+(-1)*sum(
(T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
G[3,1]<-G[1,3]
G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+
(theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
G[3,2]<-G[2,3]
G[3,3]<-((-1*r)/(theta[3])^2)
return(G)
}

and using numeric gradient and hessian matrix:

a <- maxLik(loglik, start=c(40,50,2))
Warning messages:
1: In log(theta[3]) : NaNs produced
2: In log(theta[1] + theta[2]) : NaNs produced
3: In log(theta[1]) : NaNs produced
4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
produced
5: In log((C * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
produced
6: In log(theta[3]) : NaNs produced
7: In log(theta[1] + theta[2]) : NaNs produced
and so on?..

I don't know why I get these 50 warnings although:
1- The inputs of the log() function are strictly positive.
2- When I evaluated the log-likelihood fuction at the very begining it gave
me a number(which is -56.66) and not (NAN).

I've also tried to:
1- Reparamtrize my model using lamda(i)= log(theta(i)), for i=1,2,3, so
that it may solve the problem, but it didn't.
2- I've used the comparederivitive() function, and the analytic and numeric
gradients were so close.

Any help please?
Maram Salem

	[[alternative HTML version deleted]]


From inshique at ymail.com  Sat Jul 18 01:49:07 2015
From: inshique at ymail.com (tryingtolearn)
Date: Fri, 17 Jul 2015 16:49:07 -0700 (PDT)
Subject: [R] matching strings in a list
In-Reply-To: <1437068453415-4709967.post@n4.nabble.com>
References: <1437068453415-4709967.post@n4.nabble.com>
Message-ID: <1437176947245-4710015.post@n4.nabble.com>

Thank you all very much! 



--
View this message in context: http://r.789695.n4.nabble.com/matching-strings-in-a-list-tp4709967p4710015.html
Sent from the R help mailing list archive at Nabble.com.


From searl at vt.edu  Sat Jul 18 02:12:10 2015
From: searl at vt.edu (Steve E.)
Date: Fri, 17 Jul 2015 17:12:10 -0700 (PDT)
Subject: [R] modifying a package installed via GitHub
Message-ID: <1437178330378-4710016.post@n4.nabble.com>

Hi Folks,

I am working with a package installed via GitHub that I would like to
modify. However, I am not sure how I would go about loading a 'local'
version of the package after I have modified it, and whether that process
would including uninstalling the original unmodified package (and,
conversely, how to uninstall my local, modified version if I wanted to go
back to the unmodified version available on GitHub).

Any advice would be appreciated.


Thanks,
Steve



--
View this message in context: http://r.789695.n4.nabble.com/modifying-a-package-installed-via-GitHub-tp4710016.html
Sent from the R help mailing list archive at Nabble.com.


From arne.henningsen at gmail.com  Sat Jul 18 08:01:27 2015
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Sat, 18 Jul 2015 08:01:27 +0200
Subject: [R] Warning message with maxLik()
In-Reply-To: <CAPLSCn0v=xZiDGzZZcfQNFe277fvEKO7OowTYgHzJ0mnb5VGPw@mail.gmail.com>
References: <CAPLSCn0v=xZiDGzZZcfQNFe277fvEKO7OowTYgHzJ0mnb5VGPw@mail.gmail.com>
Message-ID: <CAMTWbJh0LJ6aQLg8z-gD-DSWQ8A4PcgyAOp941LLmrf+jL5jCA@mail.gmail.com>

Dear Maram

- Please do not start a new thread for the same issue but reply to
previous messages in this thread [1].

- Please read my previous responses [1] more carefully, e.g. to use
"theta <- exp( param )" which guarantees that all elements of "theta"
are always positive.

[1] http://r.789695.n4.nabble.com/NaN-produced-from-log-with-positive-input-td4709463.html

Best regards,
Arne



2015-07-18 2:46 GMT+02:00 Maram SAlem <marammagdysalem at gmail.com>:
> Dear All,
> I'm trying to get the MLe for a certain distribution using maxLik ()
> function. I wrote the log-likelihood function as follows:
> theta <-vector(mode = "numeric", length = 3)
> r<- 17
> n <-30
>  T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<-
> c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> # The  loglik. func.
> loglik <- function(param) {
>  theta[1]<- param[1]
>  theta[2]<- param[2]
>  theta[3]<- param[3]
>  l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+
> (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+
> (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
> return(l)
>  }
>
> then, I evaluated it at theta<- c(40,50,2)
>
> v<-loglik(param=theta)
> v
> [1] -56.66653
>
> I used this same log-likelihood function, once with analytic gradient and
> another time with numerical one, with the maxLik function, and in both
> cases I got the same 50 warning messages and an MLE which is completely
> unrealistic as per my applied example.
>
> a <- maxLik(loglik, gradlik, hesslik, start=c(40,50,2))
>
> where gradlik and hesslik are the analytic gradient and Hessian matrix,
> respectively, given by:
>
> U <- vector(mode="numeric",length=3)
> gradlik<-function(param = theta,n, T,C)
>  {
> U <- vector(mode="numeric",length=3)
> theta[1] <- param[1]
> theta[2] <- param[2]
> theta[3] <- param[3]
> r<- 17
> n <-30
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<-
> c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>  U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+(
> -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
> (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+
> (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
> (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+
> (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
> return(U)
> }
> hesslik<-function(param=theta,n,T,C)
> {
> theta[1] <- param[1]
> theta[2] <- param[2]
> theta[3] <- param[3]
> r<- 17
> n <-30
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> C<-
> c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> G<- matrix(nrow=3,ncol=3)
> G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+
> (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
> theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+
> (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+
> (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[2,1]<-G[1,2]
> G[1,3]<-(n/theta[1])+(-1)*sum(
> (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> G[3,1]<-G[1,3]
> G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+
> (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
> theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> G[3,2]<-G[2,3]
> G[3,3]<-((-1*r)/(theta[3])^2)
> return(G)
> }
>
> and using numeric gradient and hessian matrix:
>
> a <- maxLik(loglik, start=c(40,50,2))
> Warning messages:
> 1: In log(theta[3]) : NaNs produced
> 2: In log(theta[1] + theta[2]) : NaNs produced
> 3: In log(theta[1]) : NaNs produced
> 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
> produced
> 5: In log((C * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
> produced
> 6: In log(theta[3]) : NaNs produced
> 7: In log(theta[1] + theta[2]) : NaNs produced
> and so on?..
>
> I don't know why I get these 50 warnings although:
> 1- The inputs of the log() function are strictly positive.
> 2- When I evaluated the log-likelihood fuction at the very begining it gave
> me a number(which is -56.66) and not (NAN).
>
> I've also tried to:
> 1- Reparamtrize my model using lamda(i)= log(theta(i)), for i=1,2,3, so
> that it may solve the problem, but it didn't.
> 2- I've used the comparederivitive() function, and the analytic and numeric
> gradients were so close.
>
> Any help please?
> Maram Salem
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Arne Henningsen
http://www.arne-henningsen.name


From jdnewmil at dcn.davis.ca.us  Sat Jul 18 09:07:24 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 18 Jul 2015 00:07:24 -0700
Subject: [R] OPTIMX: non-finite finite-difference value [26] and scaling
 problem
In-Reply-To: <1254082374.2956615.1437155708813.JavaMail.yahoo@mail.yahoo.com>
References: <1254082374.2956615.1437155708813.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55A9FB2C.20908@dcn.davis.ca.us>

It is quite unlikely that anyone can help you without a reproducible 
example. [1]

You should be aware that sending emails to this list in HTML format is 
likely to yield scrambled mails received. Please make your email client send 
email in plain text.

[1] 
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

On 07/17/2015 10:55 AM, Olu Ola via R-help wrote:
> Hello,I am running a nonlinear GMM using the optimx wrapper. I am trying to estimate 37 variables however and my code for the optimx is:
> nlgmm = optimx(par=b0, fn=obj,method = "BFGS", itnmax=10000, control=list(follow.on = TRUE,kkt=FALSE,starttests=TRUE,save.failures=TRUE, trace=0))
>
> My staring values are from Ordinary least squares estimates (OLS) and they are:
> b0 <- c(-2.00658,-0.04373,0.19079,0.34652,-0.36814,0.21284,-0.24369,0.64622,0.22927,0.29431,0.19547,0.80614,18.8398,0.5928,3.1375,0.4301,-0.4937,2.2016,31.5203,0.6171,1.0206,1.7830,-0.4421,11.0076,-0.03305,0.17087,0.44794,0.17488,0.10781,-0.50747,-0.04563,0.17030,0.41792,0.17526,0.99734,-17.2996,-41.9359)
>
> I got the following error:
> Error in optim(par = par, fn = ufn, gr = ugr, lower = lower, upper = upper,  :   non-finite finite-difference value [26]
> I also got an error about scaling which is as follows:
> Parameters or bounds appear to have differentscalings.This can cause poor performance in optimization.  Itis important for derivative free methods like BOBYQA, UOBYQA, NEWUOA.
>
>
> any help will be greatly appreciated.
> Best Regards
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From itsme at CorinaLogan.com  Sat Jul 18 07:30:12 2015
From: itsme at CorinaLogan.com (Corina)
Date: Fri, 17 Jul 2015 22:30:12 -0700 (PDT)
Subject: [R] Why does dredge() rank models differently for lmer() and
 MCMCglmm()?
Message-ID: <1437197412639-4710020.post@n4.nabble.com>

Hello,
I am running my full model (fm) through lmer() and MCMCglmm() using the
default settings:

model.lmer <- lmer(fm)
model.MCMCglmm <- MCMCglmm(fm)

And the summary outputs are almost exactly the same:

summary(model.lmer)
summary(model.MCMCglmm)

However, when I run the models through dredge():

dredge(model.lmer)
dredge(model.MCMCglmm)

It ranks the models very differently for lmer() and MCMCglmm() in the model
selection tables, even though I used the default settings for dredge() as
well (ranked by AICc). I would assume the difference is because lmer() uses
REML and MCMCglmm() uses Bayes Rule, however if this is the case then why
weren?t the summary outputs different as well?

I have checked through the documentation for all of the associated R
packages and searched the R forums and Google for the answer, but I haven?t
seen this question addressed. I would be grateful if someone could let me
know why this happens.

Thanks in advance!

Corina



--
View this message in context: http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020.html
Sent from the R help mailing list archive at Nabble.com.


From bob at rudis.net  Sat Jul 18 15:49:39 2015
From: bob at rudis.net (boB Rudis)
Date: Sat, 18 Jul 2015 09:49:39 -0400
Subject: [R] modifying a package installed via GitHub
In-Reply-To: <1437178330378-4710016.post@n4.nabble.com>
References: <1437178330378-4710016.post@n4.nabble.com>
Message-ID: <CAJ4QxaOOqCrC=BTSmE5pRVtX1Pgj=gj6GxtuA9i2c8M7UXQZxQ@mail.gmail.com>

You can go to the package directory:

    cd /some/path/to/package

and do

    R CMD install .

from a command-line there.

Many github-based packages are also made using RStudio and you can
just open the .Rproj file (i.e. load it into R studio) and build the
package there which will install it.

The same-named package will overwrite what you have previously installed.

Just:

   devtools::install_github("owner/package")

to go back to the original.

On Fri, Jul 17, 2015 at 8:12 PM, Steve E. <searl at vt.edu> wrote:
> Hi Folks,
>
> I am working with a package installed via GitHub that I would like to
> modify. However, I am not sure how I would go about loading a 'local'
> version of the package after I have modified it, and whether that process
> would including uninstalling the original unmodified package (and,
> conversely, how to uninstall my local, modified version if I wanted to go
> back to the unmodified version available on GitHub).
>
> Any advice would be appreciated.
>
>
> Thanks,
> Steve
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/modifying-a-package-installed-via-GitHub-tp4710016.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Sat Jul 18 17:57:13 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 18 Jul 2015 08:57:13 -0700
Subject: [R] OPTIMX: non-finite finite-difference value [26] and scaling
	problem
In-Reply-To: <55A9FB2C.20908@dcn.davis.ca.us>
References: <1254082374.2956615.1437155708813.JavaMail.yahoo@mail.yahoo.com>
	<55A9FB2C.20908@dcn.davis.ca.us>
Message-ID: <CAGxFJbSLQ9=4B082emWTOAHtYvrtKAi4StvCiK=UipEdeK9r4A@mail.gmail.com>

... but unless there is an outright error in the code, the problem is
due to the specific data and starting values, which means they cannot
be easily reproduced.

More than likely, the optimizer has run into numerical problems. After
all, it is wandering around in 37 dimensional space and the max/min
parameter ratio (scaling issues) is  about 1000. So there could well
be severe overparameterization/inadequate data and poor parameter
scaling. Many applied practitioners seem unaware of the realities and
challenges of *non*linear optimization -- it is, after all, a highly
technical specialty -- and somehow expect that they can throw
multi-parameter nonlinear models at any old data they have. This is
the sort of rude awakening that might be expected under such
circumstances, for which the only solution may be a careful rethink of
the goals of the modeling effort, the complexity of the models, and
the limitations of the data available.

Note that these comments are of necessity largely speculative and
would benefit greatly by amplification and criticism by real experts,
of which I ain't one. So caveat emptor! (as usual on the internet).

Best,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 18, 2015 at 12:07 AM, Jeff Newmiller
<jdnewmil at dcn.davis.ca.us> wrote:
> It is quite unlikely that anyone can help you without a reproducible
> example. [1]
>
> You should be aware that sending emails to this list in HTML format is
> likely to yield scrambled mails received. Please make your email client send
> email in plain text.
>
> [1]
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>
> On 07/17/2015 10:55 AM, Olu Ola via R-help wrote:
>>
>> Hello,I am running a nonlinear GMM using the optimx wrapper. I am trying
>> to estimate 37 variables however and my code for the optimx is:
>> nlgmm = optimx(par=b0, fn=obj,method = "BFGS", itnmax=10000,
>> control=list(follow.on = TRUE,kkt=FALSE,starttests=TRUE,save.failures=TRUE,
>> trace=0))
>>
>> My staring values are from Ordinary least squares estimates (OLS) and they
>> are:
>> b0 <-
>> c(-2.00658,-0.04373,0.19079,0.34652,-0.36814,0.21284,-0.24369,0.64622,0.22927,0.29431,0.19547,0.80614,18.8398,0.5928,3.1375,0.4301,-0.4937,2.2016,31.5203,0.6171,1.0206,1.7830,-0.4421,11.0076,-0.03305,0.17087,0.44794,0.17488,0.10781,-0.50747,-0.04563,0.17030,0.41792,0.17526,0.99734,-17.2996,-41.9359)
>>
>> I got the following error:
>> Error in optim(par = par, fn = ufn, gr = ugr, lower = lower, upper =
>> upper,  :   non-finite finite-difference value [26]
>> I also got an error about scaling which is as follows:
>> Parameters or bounds appear to have differentscalings.This can cause poor
>> performance in optimization.  Itis important for derivative free methods
>> like BOBYQA, UOBYQA, NEWUOA.
>>
>>
>> any help will be greatly appreciated.
>> Best Regards
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>
> and provide commented, minimal, self-contained, reproducible code.


From bbolker at gmail.com  Sat Jul 18 19:03:54 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 18 Jul 2015 17:03:54 +0000
Subject: [R] Why does dredge() rank models differently for lmer() and
	MCMCglmm()?
References: <1437197412639-4710020.post@n4.nabble.com>
Message-ID: <loom.20150718T165928-763@post.gmane.org>

Corina <itsme <at> CorinaLogan.com> writes:

> 
> Hello,
> I am running my full model (fm) through lmer() and MCMCglmm() using the
> default settings:
> 
> model.lmer <- lmer(fm)
> model.MCMCglmm <- MCMCglmm(fm)
> 

  [snip]

> However, when I run the models through dredge():
> 
> dredge(model.lmer)
> dredge(model.MCMCglmm)
 
> It ranks the models very differently for lmer() and MCMCglmm() in
> the model selection tables, even though I used the default settings
> for dredge() as well (ranked by AICc). I would assume the difference
> is because lmer() uses REML and MCMCglmm() uses Bayes Rule, however
> if this is the case then why weren?t the summary outputs different
> as well?
 
  [snip]

   This question *might* be better on r-sig-mixed-models at r-project.org ...
You shouldn't be comparing models with fixed effects that are fitted
with REML -- try  refitting with REML=FALSE.  The model comparison
functions in the lme4 package (e.g. anova()) try to stop you from 
making this mistake, but I'm not sure the MuMIn::dredge does.


From branko.vermote at ugent.be  Sat Jul 18 15:44:12 2015
From: branko.vermote at ugent.be (Branko)
Date: Sat, 18 Jul 2015 06:44:12 -0700 (PDT)
Subject: [R] Error: invalid 'labels'; length 5 should be 1 or 4
Message-ID: <1437227052943-4710026.post@n4.nabble.com>

I have read already the other post 'invalid 'labels'; length 2 should be...'
but I don't understand the answer.

I work with the poLCA package and I get this error: Error in factor(ret$y[,
j], labels = lev) : 
  invalid 'labels'; length 5 should be 1 or 4

What does this mean and what can I do about it?

Kind regards
Branko



--
View this message in context: http://r.789695.n4.nabble.com/Error-invalid-labels-length-5-should-be-1-or-4-tp4710026.html
Sent from the R help mailing list archive at Nabble.com.


From sarah.goslee at gmail.com  Sat Jul 18 19:35:21 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Sat, 18 Jul 2015 13:35:21 -0400
Subject: [R] Error: invalid 'labels'; length 5 should be 1 or 4
In-Reply-To: <1437227052943-4710026.post@n4.nabble.com>
References: <1437227052943-4710026.post@n4.nabble.com>
Message-ID: <CAM_vjuk+7brF2Q38b4DgiX4cXtQc10tZbqZL4r8dqvruND_rkQ@mail.gmail.com>

Hi,

On Sat, Jul 18, 2015 at 9:44 AM, Branko <branko.vermote at ugent.be> wrote:
> I have read already the other post 'invalid 'labels'; length 2 should be...'
> but I don't understand the answer.

This is the R-help email list; I don't know what other post you mean
without a link to the mailing  list archive (the real one at
http://tolstoy.newcastle.edu.au/R/ or another link from the official
page, not Nabble).

>
> I work with the poLCA package and I get this error: Error in factor(ret$y[,
> j], labels = lev) :
>   invalid 'labels'; length 5 should be 1 or 4
>
> What does this mean and what can I do about it?

Well, you can provide a reproducible example, and the information
requested in the posting guide.

Without a reproducible example that includes some sample data (fake is
fine), the code you used, and some clear idea of what output you
expect, it's impossible to figure out how to help you. Here are some
suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

The R-help posting guide may also be useful:
http://www.r-project.org/posting-guide.html

Sarah


> Kind regards
> Branko
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Error-invalid-labels-length-5-should-be-1-or-4-tp4710026.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee
http://www.functionaldiversity.org


From f.harrell at vanderbilt.edu  Sat Jul 18 19:51:26 2015
From: f.harrell at vanderbilt.edu (Frank Harrell)
Date: Sat, 18 Jul 2015 10:51:26 -0700 (PDT)
Subject: [R] Nagelkerke Pseudo R-squared
In-Reply-To: <0AB4C941-2D41-43DB-8373-FF6E8206770C@comcast.net>
References: <253097945.6053468.1437175987347.JavaMail.yahoo@mail.yahoo.com>
	<0AB4C941-2D41-43DB-8373-FF6E8206770C@comcast.net>
Message-ID: <1437241886050-4710031.post@n4.nabble.com>

It's implemented in the R rms package's lrm and orm functions.



-----
Frank Harrell
Department of Biostatistics, Vanderbilt University
--
View this message in context: http://r.789695.n4.nabble.com/Nagelkerke-Pseudo-R-squared-tp4710014p4710031.html
Sent from the R help mailing list archive at Nabble.com.


From varinsacha at yahoo.fr  Sat Jul 18 20:31:15 2015
From: varinsacha at yahoo.fr (varin sacha)
Date: Sat, 18 Jul 2015 18:31:15 +0000 (UTC)
Subject: [R] Nagelkerke Pseudo R-squared
In-Reply-To: <0AB4C941-2D41-43DB-8373-FF6E8206770C@comcast.net>
References: <0AB4C941-2D41-43DB-8373-FF6E8206770C@comcast.net>
Message-ID: <1671285396.260939.1437244275575.JavaMail.yahoo@mail.yahoo.com>

Thanks David for your response.

Best regards,

SV


----- Mail original -----
De : David Winsemius <dwinsemius at comcast.net>
? : varin sacha <varinsacha at yahoo.fr>
Cc : R-help Mailing List <r-help at r-project.org>
Envoy? le : Samedi 18 juillet 2015 3h33
Objet : Re: [R] Nagelkerke Pseudo R-squared


On Jul 17, 2015, at 4:33 PM, varin sacha wrote:

> Dear R-Experts,
> 
> I have fitted an ordinal logistic regression with just 1 explanatory variable for the reproducible example here below.
> 
> Everything is working, now I try to calculate the Nagelkerke Pseudo R-squared. 
> I have found a package BaylorEdPsych providing many Pseudo R-squared, but the example shown in the package is for GLM (binary logistic regression) not for ordinal logistic regression.
> How can I calculate the Nagelkerke Pseudo R-squared for ordinal logistic regression ?

polr-objects have a deviance node. If this has statistical value (which I have some doubts regarding) then just apply the usual formula to compare nested models.

-- 
David.


> 
> Many thanks as usual for your precious help.
> 
> Reproducible example :
> 
> install.packages("MASS") 
> library(MASS) 
> a=factor(c("tres grand", "grand", "petit","petit","tres grand","grand","petit","petit","tres grand","grand")) 
> b=c("homme", "homme", "femme", "femme", "femme", "homme", "homme", "homme", "femme", "femme") 
> m <- polr(a ~ b, Hess=TRUE) 
> summary(m)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Sat Jul 18 22:23:54 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 18 Jul 2015 13:23:54 -0700
Subject: [R] Why does dredge() rank models differently for lmer() and
	MCMCglmm()?
In-Reply-To: <loom.20150718T165928-763@post.gmane.org>
References: <1437197412639-4710020.post@n4.nabble.com>
	<loom.20150718T165928-763@post.gmane.org>
Message-ID: <CAGxFJbTL+WCFe75afOHNtnejskNCawuE60YLx3KXKDPV-QVzvA@mail.gmail.com>

(slightly off topic)

One might also add that different model fitting criteria might produce
rankings that are quite different, but with model predictions that are
very similar. This reflects the inconvenient reality that empirical
models are merely data interpolators and not representations of
"truth." Ergo quite different looking models might fit essentially
equally well, the differences in fits and therefore rankings being
nothing more than noise.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Sat, Jul 18, 2015 at 10:03 AM, Ben Bolker <bbolker at gmail.com> wrote:
> Corina <itsme <at> CorinaLogan.com> writes:
>
>>
>> Hello,
>> I am running my full model (fm) through lmer() and MCMCglmm() using the
>> default settings:
>>
>> model.lmer <- lmer(fm)
>> model.MCMCglmm <- MCMCglmm(fm)
>>
>
>   [snip]
>
>> However, when I run the models through dredge():
>>
>> dredge(model.lmer)
>> dredge(model.MCMCglmm)
>
>> It ranks the models very differently for lmer() and MCMCglmm() in
>> the model selection tables, even though I used the default settings
>> for dredge() as well (ranked by AICc). I would assume the difference
>> is because lmer() uses REML and MCMCglmm() uses Bayes Rule, however
>> if this is the case then why weren?t the summary outputs different
>> as well?
>
>   [snip]
>
>    This question *might* be better on r-sig-mixed-models at r-project.org ...
> You shouldn't be comparing models with fixed effects that are fitted
> with REML -- try  refitting with REML=FALSE.  The model comparison
> functions in the lme4 package (e.g. anova()) try to stop you from
> making this mistake, but I'm not sure the MuMIn::dredge does.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From varinsacha at yahoo.fr  Sat Jul 18 20:39:36 2015
From: varinsacha at yahoo.fr (varin sacha)
Date: Sat, 18 Jul 2015 18:39:36 +0000 (UTC)
Subject: [R] Nagelkerke Pseudo R-squared
In-Reply-To: <1437241886050-4710031.post@n4.nabble.com>
References: <1437241886050-4710031.post@n4.nabble.com>
Message-ID: <1999718089.273948.1437244776090.JavaMail.yahoo@mail.yahoo.com>

Thanks Frank for your response.

I have used the rms package. 
Reproducible example here below :

install.packages("rms") 
library(rms) 
a=c("tres grand", "grand", "petit","petit","tres grand","grand","petit","petit","tres grand","grand") 
b=c("homme", "homme", "femme", "femme", "femme", "homme", "homme", "homme", "femme", "femme") 
h <- orm(a ~ b) 
h

The Nagelkerke R2 = 0.074. Is it right ?

Best Regards,

SV



----- Mail original -----
De : Frank Harrell <f.harrell at vanderbilt.edu>
? : r-help at r-project.org
Cc : 
Envoy? le : Samedi 18 juillet 2015 19h51
Objet : Re: [R] Nagelkerke Pseudo R-squared

It's implemented in the R rms package's lrm and orm functions.




-----
Frank Harrell
Department of Biostatistics, Vanderbilt University
--
View this message in context: http://r.789695.n4.nabble.com/Nagelkerke-Pseudo-R-squared-tp4710014p4710031.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From alessandro_niglio at yahoo.it  Sun Jul 19 02:19:32 2015
From: alessandro_niglio at yahoo.it (00alex00)
Date: Sat, 18 Jul 2015 17:19:32 -0700 (PDT)
Subject: [R] Change SetClass in R (library fPortoflio)?
Message-ID: <1437265172544-4710034.post@n4.nabble.com>

Hi,

I am using the library fPortfolio e and I would like to change an SetClass.
In particular this function portolioSpec() that as default is:

Model List: 
Type: MV
Optimize: minRisk
Estimator: covEstimator
Params: alpha = 0.05 a = 1
Portfolio List: 
Target Weights: NULL
Target Return: NULL
Target Risk: NULL
Risk-Free Rate: 0
Number of Frontier Points: 50
Status: NA
Optim List: 
Solver: solveRquadprog
Objective: portfolioObjective portfolioReturn portfolioRisk
Options: meq = 2
Trace: FALSE


I want to change the Optimize in "maxReturn". In the .pdf I read:

To optimize a portfolio of assets we first have to specify it. All settings
which specify a portfolio of assets are respresented by a S4 class named
fPFOLIOSPEC.
setClass("fPFOLIOSPEC", representation( 
model = "list", portfolio = "list", optim = "list") ) 


The default settings are as follows:
model = list( type = "MV", 
optimize = "minRisk", estimator = "covEstimator", tailRisk = list(), params
= list(alpha = 0.05, a = 2)), 
portfolio = list( weights = NULL, 
targetReturn = NULL, targetRisk = NULL, riskFreeRate = 0, nFrontierPoints =
50, status = NA), 
optim = list( solver = "solveRquadprog", objective = NULL, parames = list(),
control = list(meq = 2), trace = FALSE) 





What to optimize?
The list entry optimize from the @model slot describes what should be
optimized. Two choices are possible. Either
\code{"minRisk"} 


which minimizes the risk if the target returns is given, or
\code{"maxReturn"} 

How I can change in maxReturn, I tried different things but nothing.


Thank you



--
View this message in context: http://r.789695.n4.nabble.com/Change-SetClass-in-R-library-fPortoflio-tp4710034.html
Sent from the R help mailing list archive at Nabble.com.


From varinsacha at yahoo.fr  Sun Jul 19 07:19:20 2015
From: varinsacha at yahoo.fr (varinsacha at yahoo.fr)
Date: Sun, 19 Jul 2015 07:19:20 +0200
Subject: [R] Nagelkerke Pseudo R-squared
In-Reply-To: <1999718089.273948.1437244776090.JavaMail.yahoo@mail.yahoo.com>
References: <1437241886050-4710031.post@n4.nabble.com>
	<1999718089.273948.1437244776090.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <0C8FFC49-8CD4-40BA-A3BD-C833355F0892@yahoo.fr>

Already answered many thanks Frank.
Best Regards
SV

Envoy? de mon iPhone

Le 18 juil. 2015 ? 20:39, varin sacha <varinsacha at yahoo.fr> a ?crit :

> Thanks Frank for your response.
> 
> I have used the rms package. 
> Reproducible example here below :
> 
> install.packages("rms") 
> library(rms) 
> a=c("tres grand", "grand", "petit","petit","tres grand","grand","petit","petit","tres grand","grand") 
> b=c("homme", "homme", "femme", "femme", "femme", "homme", "homme", "homme", "femme", "femme") 
> h <- orm(a ~ b) 
> h
> 
> The Nagelkerke R2 = 0.074. Is it right ?
> 
> Best Regards,
> 
> SV
> 
> 
> 
> ----- Mail original -----
> De : Frank Harrell <f.harrell at vanderbilt.edu>
> ? : r-help at r-project.org
> Cc : 
> Envoy? le : Samedi 18 juillet 2015 19h51
> Objet : Re: [R] Nagelkerke Pseudo R-squared
> 
> It's implemented in the R rms package's lrm and orm functions.
> 
> 
> 
> 
> -----
> Frank Harrell
> Department of Biostatistics, Vanderbilt University
> --
> View this message in context: http://r.789695.n4.nabble.com/Nagelkerke-Pseudo-R-squared-tp4710014p4710031.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From warwickm1002 at hotmail.com  Sun Jul 19 10:07:36 2015
From: warwickm1002 at hotmail.com (warwick maddock)
Date: Sun, 19 Jul 2015 18:37:36 +1030
Subject: [R] searching for key phrases in collection of text files using tm
Message-ID: <SNT147-W4E7C77413183A674ACDC3D3860@phx.gbl>

Hi R-Help!
I am a newbie in R and computer science in general. I have done the basic readings of introduction to R and TM packages. I am using R Foundation on a windows 7 system. 
I have been given a project which requires me to search annual reports of 76 companies for multiple key phrases such as "finance program" or "improving working capital". The goal is to see how many times each key phrase appears in each annual report. 
The following script is what I have accomplished thus far:
#load tm package library(tm)
#set working directory of text files of annual reportssetwd('C:/Users/a446578/Desktop/Annual Reports Text Files')
dest<-("C:/Users/a446578/Desktop/Annual Reports Text Files")
#create corpus of 76 annual reports text files a<-Corpus(DirSource("C:/Users/a446578/Desktop/Annual Reports Text Files"), readerControl = list(language="lat")
#cleaning corpus a<-tm_map(a, removeNumbers)a<-tm_map(a, removePunctuation)a<-tm_map(a, content_transformer(tolower))a<-tm_map(a, removeWords, stopwords("english"))
#create the term document matrix dtm<-DocumentTermMatrix(a)
#searching for key phrases tm_term_score(dtm, c("finance program", "improving working capital", "reduce days", "increase trade receivables"))
Everything runs smoothly apart from the last step (#searching for key phrases). I understand that the tm_term_score function is only used for single key words and not phrases. How can I achieve the same result the tm_term_score function gives me, but with phrases instead of words?
I have posted an almost identical question on another forum but was not able to comprehend the response. I trust you guys at R-help can give me a good solution able to be understood by someone as weak as I am at R. 
Thanks a lot guys!Warwivck
 



 		 	   		  
	[[alternative HTML version deleted]]


From karraspito at yahoo.es  Sun Jul 19 16:34:19 2015
From: karraspito at yahoo.es (Iker Vaquero Alba)
Date: Sun, 19 Jul 2015 14:34:19 +0000 (UTC)
Subject: [R] p-values in "rpt.adj"
Message-ID: <1763622606.511333.1437316459509.JavaMail.yahoo@mail.yahoo.com>


?? Hello everyone,?? My question is about the "rptR" package. I am using the rpt.adj function (for adjusted repeatability). 
?? My model is as follows:

?? rpt.adj(brifield ~ 1 + (1|ring)+(1|year),"ring",data=bellyfield,datatype="Gaussian")
?? where "brifield" is the measurement for which I want to obtain repeatability and "ring" is the individual ID (there are 3 different measurements per ID ? per ring number). 

?? What I want is an adjusted repeatability estimate that includes the year-to-year variance. Reading how to use the function, it says: 
grnameA character string or vector of character strings giving the name(s) of the grouping factor(s), for which the repeatability should be estimated. Spelling needs to match the random effect names as given in formula.?
?? So my grouping factor is "ring". Is that right? That would be my first question (just reassuring).
?? And the second one is about the results:?? 
?? R? = 0.591
?? SE = 0.148
?? CI = [0.27, 0.808]
?? P? = 1.06e-22 []
??? ? ? ? 0.001 []
?? It's basically about the p-values (as it gives me 2). Which one is which? The upper one is the p-value due to grouping factor "ring" (or ID) and the bottom one is the one of grouping factor "year"? Which one should I report, according to what I am interested in?
?? Thank you very much in advance.
?? Iker
?__________________________________________________________________

?? Dr. Iker Vaquero-Alba
 ?? Daphne du Maurier
 ?? Centre for Ecology and Conservation
 ?? College of Life and Environmental Sciences
 ?? University of Exeter, Cornwall Campus
 ?? TR10 9FE 
 ?? Penryn
 ?? U.K.

 ?? http://biosciences.exeter.ac.uk/cec/staff/postgradresearch/ikervaquero-alba/

 ?? https://eric.exeter.ac.uk/repository/handle/10036/3381


	[[alternative HTML version deleted]]


From marammagdysalem at gmail.com  Sun Jul 19 14:03:31 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Sun, 19 Jul 2015 14:03:31 +0200
Subject: [R] Warning message with maxLik()
In-Reply-To: <CAMTWbJh0LJ6aQLg8z-gD-DSWQ8A4PcgyAOp941LLmrf+jL5jCA@mail.gmail.com>
References: <CAPLSCn0v=xZiDGzZZcfQNFe277fvEKO7OowTYgHzJ0mnb5VGPw@mail.gmail.com>
	<CAMTWbJh0LJ6aQLg8z-gD-DSWQ8A4PcgyAOp941LLmrf+jL5jCA@mail.gmail.com>
Message-ID: <CAPLSCn2U6nKDLS0Fkb_YA4kdzBRGxv_77hyuXzuX+diuM_VZgA@mail.gmail.com>

Dear Arne,

The elements of the theta vector are indeed strictly positive. I've just
tried to use instead : lamda = log (theta), which means that theta = exp
(lamda),  so as to get rid of the log() function that appears in the
log-likelihood and is causing the 50 warnings, but still the estimates I
got for lamda and then those I got for theta (using theta=exp(lamda)) are
irrelvant and their standard errors are infinite, which means that therer
is still a problem that I can't yet figure out.

Thanks,
Maram

On 18 July 2015 at 08:01, Arne Henningsen <arne.henningsen at gmail.com> wrote:

> Dear Maram
>
> - Please do not start a new thread for the same issue but reply to
> previous messages in this thread [1].
>
> - Please read my previous responses [1] more carefully, e.g. to use
> "theta <- exp( param )" which guarantees that all elements of "theta"
> are always positive.
>
> [1]
> http://r.789695.n4.nabble.com/NaN-produced-from-log-with-positive-input-td4709463.html
>
> Best regards,
> Arne
>
>
>
> 2015-07-18 2:46 GMT+02:00 Maram SAlem <marammagdysalem at gmail.com>:
> > Dear All,
> > I'm trying to get the MLe for a certain distribution using maxLik ()
> > function. I wrote the log-likelihood function as follows:
> > theta <-vector(mode = "numeric", length = 3)
> > r<- 17
> > n <-30
> >
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> > C<-
> >
> c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> > # The  loglik. func.
> > loglik <- function(param) {
> >  theta[1]<- param[1]
> >  theta[2]<- param[2]
> >  theta[3]<- param[3]
> >
> l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+
> > (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+
> > (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
> > return(l)
> >  }
> >
> > then, I evaluated it at theta<- c(40,50,2)
> >
> > v<-loglik(param=theta)
> > v
> > [1] -56.66653
> >
> > I used this same log-likelihood function, once with analytic gradient and
> > another time with numerical one, with the maxLik function, and in both
> > cases I got the same 50 warning messages and an MLE which is completely
> > unrealistic as per my applied example.
> >
> > a <- maxLik(loglik, gradlik, hesslik, start=c(40,50,2))
> >
> > where gradlik and hesslik are the analytic gradient and Hessian matrix,
> > respectively, given by:
> >
> > U <- vector(mode="numeric",length=3)
> > gradlik<-function(param = theta,n, T,C)
> >  {
> > U <- vector(mode="numeric",length=3)
> > theta[1] <- param[1]
> > theta[2] <- param[2]
> > theta[3] <- param[3]
> > r<- 17
> > n <-30
> >
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> > C<-
> >
> c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> >  U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+(
> >
> -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
> >
> (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+
> >
> (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
> >
> (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+
> >
> (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
> > return(U)
> > }
> > hesslik<-function(param=theta,n,T,C)
> > {
> > theta[1] <- param[1]
> > theta[2] <- param[2]
> > theta[3] <- param[3]
> > r<- 17
> > n <-30
> >
> T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> > C<-
> >
> c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> > G<- matrix(nrow=3,ncol=3)
> > G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+
> >
> (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
> >
> theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> > G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+
> > (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+
> > (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> > G[2,1]<-G[1,2]
> > G[1,3]<-(n/theta[1])+(-1)*sum(
> >
> (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > G[3,1]<-G[1,3]
> > G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+
> >
> (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
> >
> theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> >
> G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > G[3,2]<-G[2,3]
> > G[3,3]<-((-1*r)/(theta[3])^2)
> > return(G)
> > }
> >
> > and using numeric gradient and hessian matrix:
> >
> > a <- maxLik(loglik, start=c(40,50,2))
> > Warning messages:
> > 1: In log(theta[3]) : NaNs produced
> > 2: In log(theta[1] + theta[2]) : NaNs produced
> > 3: In log(theta[1]) : NaNs produced
> > 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
> > produced
> > 5: In log((C * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
> > produced
> > 6: In log(theta[3]) : NaNs produced
> > 7: In log(theta[1] + theta[2]) : NaNs produced
> > and so on?..
> >
> > I don't know why I get these 50 warnings although:
> > 1- The inputs of the log() function are strictly positive.
> > 2- When I evaluated the log-likelihood fuction at the very begining it
> gave
> > me a number(which is -56.66) and not (NAN).
> >
> > I've also tried to:
> > 1- Reparamtrize my model using lamda(i)= log(theta(i)), for i=1,2,3, so
> > that it may solve the problem, but it didn't.
> > 2- I've used the comparederivitive() function, and the analytic and
> numeric
> > gradients were so close.
> >
> > Any help please?
> > Maram Salem
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Arne Henningsen
> http://www.arne-henningsen.name
>

	[[alternative HTML version deleted]]


From lordpreetam at gmail.com  Sun Jul 19 22:58:44 2015
From: lordpreetam at gmail.com (Preetam Pal)
Date: Mon, 20 Jul 2015 02:28:44 +0530
Subject: [R] Quadratic programming
Message-ID: <55ac0f74.4185460a.4e74.155f@mx.google.com>

Hi,
 If i have a quadratic objective function with a system of linear constraints for multiple variables, is there any inbuilt function that i can use?
Regards, 
Preetam
	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sun Jul 19 23:08:07 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 19 Jul 2015 17:08:07 -0400
Subject: [R] Quadratic programming
In-Reply-To: <55ac0f74.4185460a.4e74.155f@mx.google.com>
References: <55ac0f74.4185460a.4e74.155f@mx.google.com>
Message-ID: <55AC11B7.9020007@gmail.com>

On 19/07/2015 4:58 PM, Preetam Pal wrote:
> Hi,
>  If i have a quadratic objective function with a system of linear constraints for multiple variables, is there any inbuilt function that i can use?

Google says the quadprog package should help.

Duncan Murdoch


From christian at echoffmann.ch  Mon Jul 20 00:03:44 2015
From: christian at echoffmann.ch (Christian Hoffmann)
Date: Mon, 20 Jul 2015 00:03:44 +0200
Subject: [R] ERROR: unable to collate and parse R files
Message-ID: <55AC1EC0.7040202@echoffmann.ch>

I tried in vain to execute

R CMD  install /Users/hoffmann/R/cwhmisc  ('cwhmisc' is the package name)

* installing to library 
?/Library/Frameworks/R.framework/Versions/3.2/Resources/library?
* installing *source* package ?cwhmisc? ...
** R
Error : '\&' ist eine unbekannte Escape-Sequenz in der Zeichenkette 
beginnend mit ""\&"
ERROR: unable to collate and parse R files for package ?cwhmisc?
* removing 
?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/cwhmisc?
* restoring previous 
?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/cwhmisc?

under

 > sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-apple-darwin10.8.0 (64-bit)
Running under: OS X 10.7.5 (Lion)

locale:
[1] C

attached base packages:
  [1] tools     stats4    splines   parallel  compiler  grid stats
  [8] graphics  grDevices utils     datasets  methods   base

other attached packages:
  [1] survival_2.38-1    spatial_7.3-9      rpart_4.1-9 nnet_7.3-9
  [5] mgcv_1.8-6         nlme_3.1-120       foreign_0.8-63 codetools_0.2-11
  [9] cluster_2.0.1      class_7.3-12       boot_1.3-16 Matrix_1.2-1
[13] MASS_7.3-40        KernSmooth_2.23-14 cwhmisc_6.0 lattice_0.20-31

Strategy of mine:
- search the net :
- grep for '\&'  : to no avail
- remove ~/.Rprofile : no success (it contained only 
'options(repos=c(CRAN="http://stat.ethz.ch/CRAN/"))' anyway)
- change CRAN mirror in .Rprofile : no success
- check vignette : nothing

- R CMD  check --as-cran /Users/hoffmann/R/cwhmisc : Installation failed.
   00install.out reads (also) Error : '\&' is an unrecognized escape in 
character string starting ""\&"
ERROR: unable to collate and parse R files for package 'cwhmisc'

What could be a way out of this maze?

-- 
Christian W. Hoffmann
CH - 8915 Hausen am Albis, Schweiz
Rigiblickstrasse 15 b, Tel.+41-44-7640853
mailto: christian at echoffmann.ch
home: www.echoffmann.ch


From murdoch.duncan at gmail.com  Mon Jul 20 00:49:48 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 19 Jul 2015 18:49:48 -0400
Subject: [R] ERROR: unable to collate and parse R files
In-Reply-To: <55AC1EC0.7040202@echoffmann.ch>
References: <55AC1EC0.7040202@echoffmann.ch>
Message-ID: <55AC298C.9040808@gmail.com>

On 19/07/2015 6:03 PM, Christian Hoffmann wrote:
> I tried in vain to execute
> 
> R CMD  install /Users/hoffmann/R/cwhmisc  ('cwhmisc' is the package name)
> 
> * installing to library 
> ?/Library/Frameworks/R.framework/Versions/3.2/Resources/library?
> * installing *source* package ?cwhmisc? ...
> ** R
> Error : '\&' ist eine unbekannte Escape-Sequenz in der Zeichenkette 
> beginnend mit ""\&"
> ERROR: unable to collate and parse R files for package ?cwhmisc?
> * removing 
> ?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/cwhmisc?
> * restoring previous 
> ?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/cwhmisc?
> 
> under
> 
>  > sessionInfo()
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-apple-darwin10.8.0 (64-bit)
> Running under: OS X 10.7.5 (Lion)
> 
> locale:
> [1] C
> 
> attached base packages:
>   [1] tools     stats4    splines   parallel  compiler  grid stats
>   [8] graphics  grDevices utils     datasets  methods   base
> 
> other attached packages:
>   [1] survival_2.38-1    spatial_7.3-9      rpart_4.1-9 nnet_7.3-9
>   [5] mgcv_1.8-6         nlme_3.1-120       foreign_0.8-63 codetools_0.2-11
>   [9] cluster_2.0.1      class_7.3-12       boot_1.3-16 Matrix_1.2-1
> [13] MASS_7.3-40        KernSmooth_2.23-14 cwhmisc_6.0 lattice_0.20-31
> 
> Strategy of mine:
> - search the net :
> - grep for '\&'  : to no avail
> - remove ~/.Rprofile : no success (it contained only 
> 'options(repos=c(CRAN="http://stat.ethz.ch/CRAN/"))' anyway)
> - change CRAN mirror in .Rprofile : no success
> - check vignette : nothing
> 
> - R CMD  check --as-cran /Users/hoffmann/R/cwhmisc : Installation failed.
>    00install.out reads (also) Error : '\&' is an unrecognized escape in 
> character string starting ""\&"
> ERROR: unable to collate and parse R files for package 'cwhmisc'
> 
> What could be a way out of this maze?
> 

You should build (to a *.tar.gz tarball) first.  That will probably succeed.

Then try to install the tarball.  That will probably fail with the same
error as above.  If so, make it available to someone else to try.

Duncan Murdoch


From anguillaphile at outlook.com  Mon Jul 20 02:11:07 2015
From: anguillaphile at outlook.com (B Jessop)
Date: Sun, 19 Jul 2015 21:11:07 -0300
Subject: [R] Fitted values problem
Message-ID: <BLU184-W91DD569D30279BF492AE7EB1850@phx.gbl>

Dear list,
 
I have run two ancova models with the same data, one where the covariate Length was centered by Site (group mean centered), the other with Length not centered. The package "car" was used.  The models are:
ancova1a <- lm(log10(Weight) ~ LenCtd2 + Site, data = Data2); Length centered model, Length log10 transformed.
 
ancova2a <- lm(log10(Weight) ~ log10(Length) + Site, data = Data2); Length not centered model.
 
For each model, adjusted mean Weights for each Site were estimated with the "effects" package.  I then extracted the fitted values for each model using "fitted()" and saved them to a file.  For the model ancova1a, the mean of the fitted Weight values for each Site equalled the adjusted mean Weight for that Site.  For model ancova2a, the fitted values were exactly the same as for model ancova1a and the Site means thus did not match the comparable adjusted mean Weights, which differed between models.  This puzzles me, and has for some time as I searched for enlightenment, and I hope that someone can provide an answer to me. My original intent was to correlate the fitted values from each model with values of another variable.  
 
Some might recognize the above models as essentially estimating weight at length or condition in animals, in this case fish.  
 
Thanks for any assistance.
 
Regards,
B. Jessop
 		 	   		  
	[[alternative HTML version deleted]]


From pnsinha68 at gmail.com  Mon Jul 20 08:48:23 2015
From: pnsinha68 at gmail.com (Partha Sinha)
Date: Mon, 20 Jul 2015 12:18:23 +0530
Subject: [R] Value passing in print option
Message-ID: <CADcgpJdw+iYF0-Dccp+bjTtGSOaQ7EJ0xYm7mHV5UVN8=MFKsA@mail.gmail.com>

i want to pass a value with print option
x<-10
y<2*x
print("Current value of y is " ) # confused dont know how to pass value

i want output as Current value of y is 10

	[[alternative HTML version deleted]]


From ligges at statistik.tu-dortmund.de  Mon Jul 20 09:22:09 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Mon, 20 Jul 2015 09:22:09 +0200
Subject: [R] Value passing in print option
In-Reply-To: <CADcgpJdw+iYF0-Dccp+bjTtGSOaQ7EJ0xYm7mHV5UVN8=MFKsA@mail.gmail.com>
References: <CADcgpJdw+iYF0-Dccp+bjTtGSOaQ7EJ0xYm7mHV5UVN8=MFKsA@mail.gmail.com>
Message-ID: <55ACA1A1.2090306@statistik.tu-dortmund.de>

?paste

Best,
Uwe Ligges


On 20.07.2015 08:48, Partha Sinha wrote:
> i want to pass a value with print option
> x<-10
> y<2*x
> print("Current value of y is " ) # confused dont know how to pass value
>
> i want output as Current value of y is 10
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cdetermanjr at gmail.com  Mon Jul 20 14:52:55 2015
From: cdetermanjr at gmail.com (Charles Determan)
Date: Mon, 20 Jul 2015 07:52:55 -0500
Subject: [R] modifying a package installed via GitHub
In-Reply-To: <CAJ4QxaOOqCrC=BTSmE5pRVtX1Pgj=gj6GxtuA9i2c8M7UXQZxQ@mail.gmail.com>
References: <1437178330378-4710016.post@n4.nabble.com>
	<CAJ4QxaOOqCrC=BTSmE5pRVtX1Pgj=gj6GxtuA9i2c8M7UXQZxQ@mail.gmail.com>
Message-ID: <CAKxd1KPv_Q4jZpvkGuNrWv1MK=rTnPj7Ro1t5sxs=uk6RDaEsg@mail.gmail.com>

Steve,

You are able to work with a github package the same as any github repo.  If
you clone the repo:

git clone https://github.com/user/repo.git

If using RStudio it is simple enough to create a new project in that new
directory (if the .Rproj file does not exist, otherwise open that).  Once
you have the project open for that directory you can modify source files
and rebuild and install as you like.  If at the CMD line, you do as Bob
instructed with R CMD install .

I recommend, however, either creating a new branch for you changes (if you
familiar with git management) or at least make sure to change the
subversion of the package so it doesn't conflict with the 'original'.  That
way you 'know' which version of the package is installed at a given time.

Naturally, if you feel your modifications are valuable you may want to
actually fork the package on github and create a pull request of your
changes for the maintainer to incorporate in to the next release.

Hope this helps clarify things,

Charles



On Sat, Jul 18, 2015 at 8:49 AM, boB Rudis <bob at rudis.net> wrote:

> You can go to the package directory:
>
>     cd /some/path/to/package
>
> and do
>
>     R CMD install .
>
> from a command-line there.
>
> Many github-based packages are also made using RStudio and you can
> just open the .Rproj file (i.e. load it into R studio) and build the
> package there which will install it.
>
> The same-named package will overwrite what you have previously installed.
>
> Just:
>
>    devtools::install_github("owner/package")
>
> to go back to the original.
>
> On Fri, Jul 17, 2015 at 8:12 PM, Steve E. <searl at vt.edu> wrote:
> > Hi Folks,
> >
> > I am working with a package installed via GitHub that I would like to
> > modify. However, I am not sure how I would go about loading a 'local'
> > version of the package after I have modified it, and whether that process
> > would including uninstalling the original unmodified package (and,
> > conversely, how to uninstall my local, modified version if I wanted to go
> > back to the unmodified version available on GitHub).
> >
> > Any advice would be appreciated.
> >
> >
> > Thanks,
> > Steve
> >
> >
> >
> > --
> > View this message in context:
> http://r.789695.n4.nabble.com/modifying-a-package-installed-via-GitHub-tp4710016.html
> > Sent from the R help mailing list archive at Nabble.com.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From karl at huftis.org  Mon Jul 20 14:59:34 2015
From: karl at huftis.org (Karl Ove Hufthammer)
Date: Mon, 20 Jul 2015 14:59:34 +0200
Subject: [R] Difference between drop1() vs. anova() for Gaussian glm models
Message-ID: <1706340.GuUD8n3QuO@ID-99504.news.uni-berlin.de>

Dear list members,

I?m having some problems understanding why drop1() and anova() gives 
different results for *Gaussian* glm models. Here?s a simple example:

  d = data.frame(x=1:6,
                 group=factor(c(rep("A",2), rep("B", 4))))
  l = glm(x~group, data=d)

Running the following code gives *three* different p-values. (I would expect 
it to give two different p-values.)

  anova(l, test="F")     # p = 0.04179
  anova(l, test="Chisq") # p = 0.00313
  drop1(l, test="Chisq") # p = 0.00841

I?m used to anova() and drop1() giving identical results for the same ?test? 
argument. However, it looks like the first two tests above use the F-
statistic as a test statistic, while the last one uses a ?scaled deviance? 
statistic:

  1-pf(8.7273, 1, 4)  # F-statistic
  1-pchisq(8.7273, 1) # F-statistic
  1-pchisq(6.9447, 1) # Scaled deviance

I couldn?t find any documentation on this difference. The help page for 
drop1() does say:

  The F tests for the "glm" methods are based on analysis of
  deviance tests, so if the dispersion is estimated it is based
  on the residual deviance, unlike the F tests of anova.glm.

But here it?s talking about *F* tests. And drop1() with test="F" actually 
gives the *same* p-value as anova() with test="F":

  drop1(l, test="F") # p = 0.04179

Any ideas why anova() and drop(1) uses different test statistics for the 
same ?test? arguments? And why the help page implies (?) that the results 
should differ for F-tests (while not mentioning chi-squared test), but here 
they do not (and the chi-squared tests do)?

$ sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-suse-linux-gnu (64-bit)
Running under: openSUSE 20150714 (Tumbleweed) (x86_64)

locale:
 [1] LC_CTYPE=nn_NO.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=nn_NO.UTF-8        LC_COLLATE=nn_NO.UTF-8    
 [5] LC_MONETARY=nn_NO.UTF-8    LC_MESSAGES=nn_NO.UTF-8   
 [7] LC_PAPER=nn_NO.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=nn_NO.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets 
[6] methods   base     

loaded via a namespace (and not attached):
[1] tools_3.2.1

-- 
Karl Ove Hufthammer
E-mail: karl at huftis.org
Jabber: huftis at jabber.no


From dimitri.liakhovitski at gmail.com  Mon Jul 20 15:56:15 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Mon, 20 Jul 2015 09:56:15 -0400
Subject: [R] For Hadley Wickham: Need for a small fix in haven::read_spss
Message-ID: <CAN2xGJa4+yKV_ygz4_VL8KZ+C1+Dce+GMiWAGHUDGNuc4iwzcA@mail.gmail.com>

Hadley,

you've added function labelled to haven, which is great. However, when
it so happens that in SPSS a variable has no long label, your code
considers it to be NULL rather than an NA. NULL is correct, but NA
would probably be better.

For example, I've read in an SPSS file:

library(haven)
spss1 <- read_spss("SPSS_Example.sav")

varnames <- names(spss1)
mylabels <- unlist(lapply(spss1, attr, "label"))

length(varnames)
[1] 64

length(mylabels)
[1] 62


Because in this particular dataset there were 2 variables without
either variable labels or data labels.
When I run lapply(spss1, attr, "label") I see under those 2 variables
"NULL" - which is true and valid.
However,  would it be possible to have instead of NULL an NA? This way
the length of varnames and mylables would the same and one could put
them side by side (e.g., in one data frame)?


Thanks a lot!

-- 
Dimitri Liakhovitski


From h.wickham at gmail.com  Mon Jul 20 16:01:39 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Mon, 20 Jul 2015 09:01:39 -0500
Subject: [R] For Hadley Wickham: Need for a small fix in haven::read_spss
In-Reply-To: <CAN2xGJa4+yKV_ygz4_VL8KZ+C1+Dce+GMiWAGHUDGNuc4iwzcA@mail.gmail.com>
References: <CAN2xGJa4+yKV_ygz4_VL8KZ+C1+Dce+GMiWAGHUDGNuc4iwzcA@mail.gmail.com>
Message-ID: <CABdHhvFsV+fHY8RBP=qcKnCOu2MYn0AePy72Kg+gz1JCTS4wwQ@mail.gmail.com>

(FWIW this would've been better send to me directly or filed on
github, rather than sent to R-help)

I think this is more of a problem with the way that you're accessing
the info, than the design of the underlying structure. I'd do
something like this:

attr_default <- function(x, which, default) {
  val <- attr(x, which)
  if (is.null(val)) default else val
}

sapply(spss1, attr_default, "label", NA_character_)

(code untested, but you get the idea)

Hadley

On Mon, Jul 20, 2015 at 8:56 AM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hadley,
>
> you've added function labelled to haven, which is great. However, when
> it so happens that in SPSS a variable has no long label, your code
> considers it to be NULL rather than an NA. NULL is correct, but NA
> would probably be better.
>
> For example, I've read in an SPSS file:
>
> library(haven)
> spss1 <- read_spss("SPSS_Example.sav")
>
> varnames <- names(spss1)
> mylabels <- unlist(lapply(spss1, attr, "label"))
>
> length(varnames)
> [1] 64
>
> length(mylabels)
> [1] 62
>
>
> Because in this particular dataset there were 2 variables without
> either variable labels or data labels.
> When I run lapply(spss1, attr, "label") I see under those 2 variables
> "NULL" - which is true and valid.
> However,  would it be possible to have instead of NULL an NA? This way
> the length of varnames and mylables would the same and one could put
> them side by side (e.g., in one data frame)?
>
>
> Thanks a lot!
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://had.co.nz/


From dimitri.liakhovitski at gmail.com  Mon Jul 20 16:06:50 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Mon, 20 Jul 2015 10:06:50 -0400
Subject: [R] For Hadley Wickham: Need for a small fix in haven::read_spss
In-Reply-To: <CABdHhvFsV+fHY8RBP=qcKnCOu2MYn0AePy72Kg+gz1JCTS4wwQ@mail.gmail.com>
References: <CAN2xGJa4+yKV_ygz4_VL8KZ+C1+Dce+GMiWAGHUDGNuc4iwzcA@mail.gmail.com>
	<CABdHhvFsV+fHY8RBP=qcKnCOu2MYn0AePy72Kg+gz1JCTS4wwQ@mail.gmail.com>
Message-ID: <CAN2xGJZTnd_Wdk1spo_+CdjdppdVxDcqmX_+N-hP4dDfYf1Sng@mail.gmail.com>

Thank you, Hadley. Yes, you are right - next time I'll email you directly.

On Mon, Jul 20, 2015 at 10:01 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> (FWIW this would've been better send to me directly or filed on
> github, rather than sent to R-help)
>
> I think this is more of a problem with the way that you're accessing
> the info, than the design of the underlying structure. I'd do
> something like this:
>
> attr_default <- function(x, which, default) {
>   val <- attr(x, which)
>   if (is.null(val)) default else val
> }
>
> sapply(spss1, attr_default, "label", NA_character_)
>
> (code untested, but you get the idea)
>
> Hadley
>
> On Mon, Jul 20, 2015 at 8:56 AM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Hadley,
>>
>> you've added function labelled to haven, which is great. However, when
>> it so happens that in SPSS a variable has no long label, your code
>> considers it to be NULL rather than an NA. NULL is correct, but NA
>> would probably be better.
>>
>> For example, I've read in an SPSS file:
>>
>> library(haven)
>> spss1 <- read_spss("SPSS_Example.sav")
>>
>> varnames <- names(spss1)
>> mylabels <- unlist(lapply(spss1, attr, "label"))
>>
>> length(varnames)
>> [1] 64
>>
>> length(mylabels)
>> [1] 62
>>
>>
>> Because in this particular dataset there were 2 variables without
>> either variable labels or data labels.
>> When I run lapply(spss1, attr, "label") I see under those 2 variables
>> "NULL" - which is true and valid.
>> However,  would it be possible to have instead of NULL an NA? This way
>> the length of varnames and mylables would the same and one could put
>> them side by side (e.g., in one data frame)?
>>
>>
>> Thanks a lot!
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> http://had.co.nz/



-- 
Dimitri Liakhovitski


From angelo.arcadi at virgilio.it  Mon Jul 20 16:10:25 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Mon, 20 Jul 2015 16:10:25 +0200 (CEST)
Subject: [R] Differences in output of lme() when introducing interactions
Message-ID: <14eabcd2515.angelo.arcadi@virgilio.it>

Dear List Members, 



I am searching for correlations between a dependent variable and a 
factor or a combination of factors in a repeated measure design. So I 
use lme() function in R. However, I am getting very different results 
depending on whether I add on the lme formula various factors compared 
to when only one is present. If a factor is found to be significant, 
shouldn't remain significant also when more factors are introduced in 
the model?


I give an example of the outputs I get using the two models. In the first model I use one single factor:

library(nlme)
summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: Gravel_ds 
      AIC      BIC   logLik
  2119.28 2130.154 -1055.64

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1952.495 2496.424

Fixed effects: Mode ~ Weight 
                Value Std.Error DF   t-value p-value
(Intercept) 10308.966 2319.0711 95  4.445299   0.000
Weight        -99.036   32.3094 17 -3.065233   0.007
 Correlation: 
       (Intr)
Weight -0.976

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649 

Number of Observations: 114
Number of Groups: 19 


As you can see the p-value for factor Weight is significant. 
This is the second model, in which I add various factors for searching their correlations:

library(nlme)
summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data = Gravel_ds, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: Gravel_ds 
       AIC      BIC    logLik
  1975.165 2021.694 -969.5825

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1.127993 2494.826

Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI 
                                Value Std.Error DF    t-value p-value
(Intercept)                   5115955  10546313 95  0.4850941  0.6287
Weight                      -13651237   6939242  3 -1.9672518  0.1438
Height                         -18678     53202  3 -0.3510740  0.7487
Shoe_Size                       93427    213737  3  0.4371115  0.6916
BMI                         -13011088   7148969  3 -1.8199949  0.1663
Weight:Height                   28128     14191  3  1.9820883  0.1418
Weight:Shoe_Size               351453    186304  3  1.8864467  0.1557
Height:Shoe_Size                 -783      1073  3 -0.7298797  0.5183
Weight:BMI                      19475     11425  3  1.7045450  0.1868
Height:BMI                     226512    118364  3  1.9136867  0.1516
Shoe_Size:BMI                  329377    190294  3  1.7308827  0.1819
Weight:Height:Shoe_Size          -706       371  3 -1.9014817  0.1534
Weight:Height:BMI                -109        63  3 -1.7258742  0.1828
Weight:Shoe_Size:BMI             -273       201  3 -1.3596421  0.2671
Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771  0.1646
Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782  0.2589
 Correlation: 
                            (Intr) Weight Height Sho_Sz BMI    Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
Weight                      -0.895                                                                                                    
Height                      -0.996  0.869                                                                                             
Shoe_Size                   -0.930  0.694  0.933                                                                                      
BMI                         -0.911  0.998  0.887  0.720                                                                               
Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997                                                                        
Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999  0.995                                                                 
Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641  0.609  0.619                                                          
Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972  0.980  0.965  0.637                                                   
Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999  0.999  0.999  0.622  0.973                                            
Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997  0.988  0.998  0.649  0.958  0.995                                     
Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000 -0.997 -1.000 -0.623 -0.971 -1.000 -0.997                              
Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974 -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973                       
Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940 -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974              
Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998 -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923       
Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947  0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158 

Number of Observations: 114
Number of Groups: 19 


This time the p-value associated to Weight is not significant anymore. Why? Which analysis should I trust?


In addition, while in the first output the field "value" (which 
should give me the slope) is -99.036 in the second output it is 
-13651237. Why they are so different? The one in the first output is the
 one that seems definitively more reasonable to me.
I would very grateful if someone could give me an answer


Thanks in advance


Angelo












       
	[[alternative HTML version deleted]]


From batholdy at googlemail.com  Mon Jul 20 17:27:06 2015
From: batholdy at googlemail.com (Martin Batholdy)
Date: Mon, 20 Jul 2015 17:27:06 +0200
Subject: [R] best way to globally set parameters for base graphics
Message-ID: <CB7C866A-FAC9-49BE-8BB0-83562DD08FCB@googlemail.com>

Hi,

I am looking for a way to modify the basic setup for any kind of plot.
(everything that is set with the par function ? like margins, cex, las etc.)

I want to do this once ? preferably across R sessions and not individually before every plot.


My first attempt was to add a par() with all my own defaults to the .Rprofile file.
This obviously does not work because par opens a new drawing device, applying its effect only to this device.

My next attempt was to write my own version of all basic plot functions (like plot, barplot etc.) adding a par() call within these functions.
This works but only if I draw a single plot. As soon as I want to use mfrow or layout to draw multiple plots side by side into one device this version also does not work, since each of these functions will open a new drawing device by themselves.


So, my question is;
Is there any way to globally define parameters given to par() so that they apply to all plots in (at least) an entire R-session?


Thank You!

From murdoch.duncan at gmail.com  Mon Jul 20 17:48:27 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 20 Jul 2015 11:48:27 -0400
Subject: [R] best way to globally set parameters for base graphics
In-Reply-To: <CB7C866A-FAC9-49BE-8BB0-83562DD08FCB@googlemail.com>
References: <CB7C866A-FAC9-49BE-8BB0-83562DD08FCB@googlemail.com>
Message-ID: <55AD184B.1050005@gmail.com>

On 20/07/2015 11:27 AM, Martin Batholdy via R-help wrote:
> Hi,
> 
> I am looking for a way to modify the basic setup for any kind of plot.
> (everything that is set with the par function ? like margins, cex, las etc.)
> 
> I want to do this once ? preferably across R sessions and not individually before every plot.
> 
> 
> My first attempt was to add a par() with all my own defaults to the .Rprofile file.
> This obviously does not work because par opens a new drawing device, applying its effect only to this device.
> 
> My next attempt was to write my own version of all basic plot functions (like plot, barplot etc.) adding a par() call within these functions.
> This works but only if I draw a single plot. As soon as I want to use mfrow or layout to draw multiple plots side by side into one device this version also does not work, since each of these functions will open a new drawing device by themselves.
> 
> 
> So, my question is;
> Is there any way to globally define parameters given to par() so that they apply to all plots in (at least) an entire R-session?

I haven't played with it, but setting a "plot.new" hook (or
"before.plot.new") might do it for you.  See ?plot.new.

It might be tricky, because some par() parameters (e.g. "mfrow")
shouldn't be called before every plot.  You'd have to look at
par("mfrow") to decide whether to call it or not.

Duncan Murdoch


From lists at dewey.myzen.co.uk  Mon Jul 20 17:56:39 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Mon, 20 Jul 2015 16:56:39 +0100
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <14eabcd2515.angelo.arcadi@virgilio.it>
References: <14eabcd2515.angelo.arcadi@virgilio.it>
Message-ID: <55AD1A37.8010902@dewey.myzen.co.uk>

In-line

On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
> Dear List Members,
>
>
>
> I am searching for correlations between a dependent variable and a
> factor or a combination of factors in a repeated measure design. So I
> use lme() function in R. However, I am getting very different results
> depending on whether I add on the lme formula various factors compared
> to when only one is present. If a factor is found to be significant,
> shouldn't remain significant also when more factors are introduced in
> the model?
>

The short answer is 'No'.

The long answer is contained in any good book on statistics which you 
really need to have by your side as the long answer is too long to 
include in an email.

>
> I give an example of the outputs I get using the two models. In the first model I use one single factor:
>
> library(nlme)
> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
> Linear mixed-effects model fit by REML
>   Data: Gravel_ds
>        AIC      BIC   logLik
>    2119.28 2130.154 -1055.64
>
> Random effects:
>   Formula: ~1 | Subject
>          (Intercept) Residual
> StdDev:    1952.495 2496.424
>
> Fixed effects: Mode ~ Weight
>                  Value Std.Error DF   t-value p-value
> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
> Weight        -99.036   32.3094 17 -3.065233   0.007
>   Correlation:
>         (Intr)
> Weight -0.976
>
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>
> Number of Observations: 114
> Number of Groups: 19
>
>
> As you can see the p-value for factor Weight is significant.
> This is the second model, in which I add various factors for searching their correlations:
>
> library(nlme)
> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data = Gravel_ds, random = ~1 | Subject))
> Linear mixed-effects model fit by REML
>   Data: Gravel_ds
>         AIC      BIC    logLik
>    1975.165 2021.694 -969.5825
>
> Random effects:
>   Formula: ~1 | Subject
>          (Intercept) Residual
> StdDev:    1.127993 2494.826
>
> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>                                  Value Std.Error DF    t-value p-value
> (Intercept)                   5115955  10546313 95  0.4850941  0.6287
> Weight                      -13651237   6939242  3 -1.9672518  0.1438
> Height                         -18678     53202  3 -0.3510740  0.7487
> Shoe_Size                       93427    213737  3  0.4371115  0.6916
> BMI                         -13011088   7148969  3 -1.8199949  0.1663
> Weight:Height                   28128     14191  3  1.9820883  0.1418
> Weight:Shoe_Size               351453    186304  3  1.8864467  0.1557
> Height:Shoe_Size                 -783      1073  3 -0.7298797  0.5183
> Weight:BMI                      19475     11425  3  1.7045450  0.1868
> Height:BMI                     226512    118364  3  1.9136867  0.1516
> Shoe_Size:BMI                  329377    190294  3  1.7308827  0.1819
> Weight:Height:Shoe_Size          -706       371  3 -1.9014817  0.1534
> Weight:Height:BMI                -109        63  3 -1.7258742  0.1828
> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421  0.2671
> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771  0.1646
> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782  0.2589
>   Correlation:
>                              (Intr) Weight Height Sho_Sz BMI    Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
> Weight                      -0.895
> Height                      -0.996  0.869
> Shoe_Size                   -0.930  0.694  0.933
> BMI                         -0.911  0.998  0.887  0.720
> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999  0.995
> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641  0.609  0.619
> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972  0.980  0.965  0.637
> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999  0.999  0.999  0.622  0.973
> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997  0.988  0.998  0.649  0.958  0.995
> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000 -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974 -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940 -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998 -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947  0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931
>
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>
> Number of Observations: 114
> Number of Groups: 19
>
>
> This time the p-value associated to Weight is not significant anymore. Why? Which analysis should I trust?
>
>
> In addition, while in the first output the field "value" (which
> should give me the slope) is -99.036 in the second output it is
> -13651237. Why they are so different? The one in the first output is the
>   one that seems definitively more reasonable to me.
> I would very grateful if someone could give me an answer
>
>
> Thanks in advance
>
>
> Angelo
>
>
>
>
>
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From batholdy at googlemail.com  Mon Jul 20 18:11:18 2015
From: batholdy at googlemail.com (Martin Batholdy)
Date: Mon, 20 Jul 2015 18:11:18 +0200
Subject: [R] best way to globally set parameters for base graphics
In-Reply-To: <55AD184B.1050005@gmail.com>
References: <CB7C866A-FAC9-49BE-8BB0-83562DD08FCB@googlemail.com>
	<55AD184B.1050005@gmail.com>
Message-ID: <C63964C5-6FD7-42F9-8043-46571D9FF7E7@googlemail.com>

Thanks for the reply.

It works fine for a single plot-call.
But as soon as I call layout() before plotting I again run into the problem that plots are not drawn into one graphic device but another one is opened for the second plot-call.


see here;


setHook("plot.new", function() 
	par(bty='n', 
		cex=0.8,
		las=1, 
		ann=F, 
		lwd=2, 
		mar=c(5, 4, 2, 1), 
		oma=c(0,0,0,0), 
		pch=19, 
		xpd=T, 
		cex.axis=0.85, 
		mgp=c(2.5, 0.72, 0),
		tcl=-0.4
		)
)


layout(matrix(1:2, 1, 2, byrow=T))

plot(c(1,2,3))
plot(c(3,2,1))






On 20 Jul 2015, at 17:48 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:

> On 20/07/2015 11:27 AM, Martin Batholdy via R-help wrote:
>> Hi,
>> 
>> I am looking for a way to modify the basic setup for any kind of plot.
>> (everything that is set with the par function ? like margins, cex, las etc.)
>> 
>> I want to do this once ? preferably across R sessions and not individually before every plot.
>> 
>> 
>> My first attempt was to add a par() with all my own defaults to the .Rprofile file.
>> This obviously does not work because par opens a new drawing device, applying its effect only to this device.
>> 
>> My next attempt was to write my own version of all basic plot functions (like plot, barplot etc.) adding a par() call within these functions.
>> This works but only if I draw a single plot. As soon as I want to use mfrow or layout to draw multiple plots side by side into one device this version also does not work, since each of these functions will open a new drawing device by themselves.
>> 
>> 
>> So, my question is;
>> Is there any way to globally define parameters given to par() so that they apply to all plots in (at least) an entire R-session?
> 
> I haven't played with it, but setting a "plot.new" hook (or
> "before.plot.new") might do it for you.  See ?plot.new.
> 
> It might be tricky, because some par() parameters (e.g. "mfrow")
> shouldn't be called before every plot.  You'd have to look at
> par("mfrow") to decide whether to call it or not.
> 
> Duncan Murdoch


From pdalgd at gmail.com  Mon Jul 20 19:03:20 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 20 Jul 2015 19:03:20 +0200
Subject: [R] Difference between drop1() vs. anova() for Gaussian glm
	models
In-Reply-To: <1706340.GuUD8n3QuO@ID-99504.news.uni-berlin.de>
References: <1706340.GuUD8n3QuO@ID-99504.news.uni-berlin.de>
Message-ID: <F6C8D894-F191-4C46-BDE0-7E91922B2B35@gmail.com>

I am somewhat surprised that _anything_ sensible comes out of anova.glm(l, test="Chisq"). I think it is mostly expected that you use F tests for that case.

What does seem to come out is the same as for drop1(l, test="Rao"), which gives the scaled score test, which would seem to be equivalent to scaled deviance in this case. 

drop1.glm(l, test="Chisq") appears to be calculating the "real" likelihood ratio test, evaluated in its asymptotic chi-square distribution:

> 2*(logLik(l) - logLik(update(l,.~1)))
'log Lik.' 6.944717 (df=3)

(Apologies for the daft output there... Why does "-" not either subtract the df or unclass the whole thing?)

Notice that the scaled tests basically assume that the scale is known, even if it is estimated, so in that sense, the real LRT should be superior. However, in that case it is well known that the asymptotic approximation can be improved  by transforming the LRT to the F statistic, whose exact distribution is known.

The remaining part of the riddle is why anova.glm doesn't do likelihood differences in the same fashion as drop1.glm. My best guess is that it tries to be consistent with anova.lm and anova.lm tries not to have to refit the sequence of submodels.

 

> On 20 Jul 2015, at 14:59 , Karl Ove Hufthammer <karl at huftis.org> wrote:
> 
> Dear list members,
> 
> I?m having some problems understanding why drop1() and anova() gives 
> different results for *Gaussian* glm models. Here?s a simple example:
> 
>  d = data.frame(x=1:6,
>                 group=factor(c(rep("A",2), rep("B", 4))))
>  l = glm(x~group, data=d)
> 
> Running the following code gives *three* different p-values. (I would expect 
> it to give two different p-values.)
> 
>  anova(l, test="F")     # p = 0.04179
>  anova(l, test="Chisq") # p = 0.00313
>  drop1(l, test="Chisq") # p = 0.00841
> 
> I?m used to anova() and drop1() giving identical results for the same ?test? 
> argument. However, it looks like the first two tests above use the F-
> statistic as a test statistic, while the last one uses a ?scaled deviance? 
> statistic:
> 
>  1-pf(8.7273, 1, 4)  # F-statistic
>  1-pchisq(8.7273, 1) # F-statistic
>  1-pchisq(6.9447, 1) # Scaled deviance
> 
> I couldn?t find any documentation on this difference. The help page for 
> drop1() does say:
> 
>  The F tests for the "glm" methods are based on analysis of
>  deviance tests, so if the dispersion is estimated it is based
>  on the residual deviance, unlike the F tests of anova.glm.
> 
> But here it?s talking about *F* tests. And drop1() with test="F" actually 
> gives the *same* p-value as anova() with test="F":
> 
>  drop1(l, test="F") # p = 0.04179
> 
> Any ideas why anova() and drop(1) uses different test statistics for the 
> same ?test? arguments? And why the help page implies (?) that the results 
> should differ for F-tests (while not mentioning chi-squared test), but here 
> they do not (and the chi-squared tests do)?
> 
> $ sessionInfo()
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-suse-linux-gnu (64-bit)
> Running under: openSUSE 20150714 (Tumbleweed) (x86_64)
> 
> locale:
> [1] LC_CTYPE=nn_NO.UTF-8       LC_NUMERIC=C              
> [3] LC_TIME=nn_NO.UTF-8        LC_COLLATE=nn_NO.UTF-8    
> [5] LC_MONETARY=nn_NO.UTF-8    LC_MESSAGES=nn_NO.UTF-8   
> [7] LC_PAPER=nn_NO.UTF-8       LC_NAME=C                 
> [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=nn_NO.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets 
> [6] methods   base     
> 
> loaded via a namespace (and not attached):
> [1] tools_3.2.1
> 
> -- 
> Karl Ove Hufthammer
> E-mail: karl at huftis.org
> Jabber: huftis at jabber.no
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From g.southon at sheffield.ac.uk  Mon Jul 20 11:02:24 2015
From: g.southon at sheffield.ac.uk (Georgina Southon)
Date: Mon, 20 Jul 2015 10:02:24 +0100
Subject: [R] To simplify or not simplify?
Message-ID: <03F6A5C4-FFEC-4C7E-B89C-1CCEBAA1A7D8@sheffield.ac.uk>

Dear R help,

This is rather a basic question, but I can't seem to find an answer anywhere else.

When I run a model such as lm/aov(height~var1) where var 1 is a categorical variable with 6 levels, I get output that shows some significant parameters and other non significant. Normally I would then proceed to simplify the model by removing the insignificant terms, however, I have recently begun to wonder if that should be standard practice or whether the full model output (not reduced by simplification) has more integrity and should be retained?

Any thoughts would be most welcome!

Thanks,

Lizzie


	[[alternative HTML version deleted]]


From j.para.fernandez at hotmail.com  Mon Jul 20 12:29:07 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Mon, 20 Jul 2015 03:29:07 -0700 (PDT)
Subject: [R] R GUI tklistbox get value
Message-ID: <1437388147621-4710064.post@n4.nabble.com>

Hi, i have a dataframe, dat, with 2 variables, one and two.

I want to print in R the mean of the selected variable of the dataframe. You
can select it with a tklistbox, but when you click OK button, the mean is
not displayed, just NA


########


one<-c(5,5,6,9,5,8)
two<-c(12,13,14,12,14,12)
dat<-data.frame(uno,dos)

require(tcltk)
tt<-tktoplevel()
tl<-tklistbox(tt,height=4,selectmode="single")
tkgrid(tklabel(tt,text="Selecciona la variable para calcular media"))
tkgrid(tl)
for (i in (1:4))
{
    tkinsert(tl,"end",colnames(dat[i]))
}
 
OnOK <- function()
{

  selecvar <- dat[as.numeric(tkcurselection(tl))+1]
   
  print(mean(selecvar))

}
OK.but <-tkbutton(tt,text="   OK   ",command=OnOK)
tkgrid(OK.but)
tkfocus(tt)

#################

Can someone please help me?? Thanks!!! 



--
View this message in context: http://r.789695.n4.nabble.com/R-GUI-tklistbox-get-value-tp4710064.html
Sent from the R help mailing list archive at Nabble.com.


From kamil.bartocha at gmail.com  Mon Jul 20 09:27:03 2015
From: kamil.bartocha at gmail.com (Kamil Bartocha)
Date: Mon, 20 Jul 2015 07:27:03 +0000
Subject: [R] Value passing in print option
In-Reply-To: <55ACA1A1.2090306@statistik.tu-dortmund.de>
References: <CADcgpJdw+iYF0-Dccp+bjTtGSOaQ7EJ0xYm7mHV5UVN8=MFKsA@mail.gmail.com>
	<55ACA1A1.2090306@statistik.tu-dortmund.de>
Message-ID: <CAFcqjU1LJC_Bo22jLeODGzLj+YR=u0EeUHusad_HkqzEnemBag@mail.gmail.com>

Also:

cat("Current value of y is",y,"\n")

Cheers,
K

On Mon, Jul 20, 2015 at 8:24 AM Uwe Ligges <ligges at statistik.tu-dortmund.de>
wrote:

> ?paste
>
> Best,
> Uwe Ligges
>
>
> On 20.07.2015 08:48, Partha Sinha wrote:
> > i want to pass a value with print option
> > x<-10
> > y<2*x
> > print("Current value of y is " ) # confused dont know how to pass value
> >
> > i want output as Current value of y is 10
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sreenath.rajur at macfast.ac.in  Mon Jul 20 07:23:03 2015
From: sreenath.rajur at macfast.ac.in (sreenath)
Date: Sun, 19 Jul 2015 22:23:03 -0700 (PDT)
Subject: [R] jaccards index
Message-ID: <1437369783364-4710057.post@n4.nabble.com>

hi..
I have a csv file containing 35 coloumns and 193 rows.i want to generate
jaccards index to normalise these data.how can i do this also from these
data i want to draw boxplot.plz help



--
View this message in context: http://r.789695.n4.nabble.com/jaccards-index-tp4710057.html
Sent from the R help mailing list archive at Nabble.com.


From rshepard at appl-ecosys.com  Mon Jul 20 19:58:49 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Mon, 20 Jul 2015 10:58:49 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally
Message-ID: <alpine.LNX.2.11.1507201047240.1876@localhost>

   Near the beginning of a LyX document I have a knitr chunk with options
that begin with 'global_options', and includes echo=F. This presents the R
code in that chunk from displaying in the compiled PDF file. However, all
following knitr chunks are included in the PDF file.

   Reading the docs (including the Knitr book) does not show me what I am
doing incorrectly. A pointer to the solution is needed.

Rich


From bgunter.4567 at gmail.com  Mon Jul 20 20:01:57 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 20 Jul 2015 11:01:57 -0700
Subject: [R] To simplify or not simplify?
In-Reply-To: <03F6A5C4-FFEC-4C7E-B89C-1CCEBAA1A7D8@sheffield.ac.uk>
References: <03F6A5C4-FFEC-4C7E-B89C-1CCEBAA1A7D8@sheffield.ac.uk>
Message-ID: <CAGxFJbQ4W-qre7ckfrx_wqR_Cxet0ZqW95-th2g+_BS8qEQV3w@mail.gmail.com>

Off topic . This list is about R programming. Post on a statistics
list like stats.stackexchange.com instead.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 20, 2015 at 2:02 AM, Georgina Southon
<g.southon at sheffield.ac.uk> wrote:
> Dear R help,
>
> This is rather a basic question, but I can't seem to find an answer anywhere else.
>
> When I run a model such as lm/aov(height~var1) where var 1 is a categorical variable with 6 levels, I get output that shows some significant parameters and other non significant. Normally I would then proceed to simplify the model by removing the insignificant terms, however, I have recently begun to wonder if that should be standard practice or whether the full model output (not reduced by simplification) has more integrity and should be retained?
>
> Any thoughts would be most welcome!
>
> Thanks,
>
> Lizzie
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From aalonso at rcumariacristina.com  Mon Jul 20 20:01:59 2015
From: aalonso at rcumariacristina.com (AGUSTIN ALONSO RODRIGUEZ)
Date: Mon, 20 Jul 2015 20:01:59 +0200
Subject: [R] help with contributed package MTS
Message-ID: <000601d0c316$2ca804d0$85f80e70$@rcumariacristina.com>

Dear Sirs:

 

I am using R-3.2.1, and when I type  install.packages(?MTS?), I get the
message:

package ?MTS? is not available (for R version 3.2.1).

 

I have tried also to install from local zip files, and I got it, but when I
type: library(MTS), I got the message that Rcpp was absent.

 

Would you please, tell me how to install MTS in R-3.2.1?

 

Thanks a lot

 

Agust?n Alonso

 


	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Mon Jul 20 20:16:37 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 20 Jul 2015 14:16:37 -0400
Subject: [R] help with contributed package MTS
In-Reply-To: <000601d0c316$2ca804d0$85f80e70$@rcumariacristina.com>
References: <000601d0c316$2ca804d0$85f80e70$@rcumariacristina.com>
Message-ID: <CAM_vju=_zxvXOt2RQNSBWxcC9-GsbW7nqB40xAVbYrtez0iC0A@mail.gmail.com>

Hi,

On Mon, Jul 20, 2015 at 2:01 PM, AGUSTIN ALONSO RODRIGUEZ
<aalonso at rcumariacristina.com> wrote:

> I am using R-3.2.1, and when I type  install.packages(?MTS?), I get the
> message:
>
> package ?MTS? is not available (for R version 3.2.1).

It's on CRAN, and passes check for R-release:
https://cran.r-project.org/web/checks/check_results_MTS.html
I didn't have any problem installing it with install.packages().

I'd suggest trying again after installing the requirements (see
below), and if you still have no success posting your sessionInfo().

> I have tried also to install from local zip files, and I got it, but when I
> type: library(MTS), I got the message that Rcpp was absent.

That means you need to install the Rcpp package before you can install
MTS. You can see these requirements listed:
 https://cran.r-project.org/web/packages/MTS/index.html

> Would you please, tell me how to install MTS in R-3.2.1?

By reading and following the error messages you received.

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


From thierry.onkelinx at inbo.be  Mon Jul 20 20:09:41 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 20 Jul 2015 20:09:41 +0200
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <alpine.LNX.2.11.1507201047240.1876@localhost>
References: <alpine.LNX.2.11.1507201047240.1876@localhost>
Message-ID: <CAJuCY5zjgVK9NkcdXaGd5Y=7gOBWYSQwBsdU1iPQvNSBYc28Dw@mail.gmail.com>

Have you tried echo = FALSE instead of echo = F.

If that doesn't solve your problem, please provide a minimal reproducible
example.
Op 20-jul.-2015 20:02 schreef "Rich Shepard" <rshepard at appl-ecosys.com>:

>   Near the beginning of a LyX document I have a knitr chunk with options
> that begin with 'global_options', and includes echo=F. This presents the R
> code in that chunk from displaying in the compiled PDF file. However, all
> following knitr chunks are included in the PDF file.
>
>   Reading the docs (including the Knitr book) does not show me what I am
> doing incorrectly. A pointer to the solution is needed.
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Mon Jul 20 20:22:41 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 20 Jul 2015 11:22:41 -0700
Subject: [R] To simplify or not simplify?
In-Reply-To: <03F6A5C4-FFEC-4C7E-B89C-1CCEBAA1A7D8@sheffield.ac.uk>
References: <03F6A5C4-FFEC-4C7E-B89C-1CCEBAA1A7D8@sheffield.ac.uk>
Message-ID: <F8855B20-6333-4AE6-9825-A0BB7914BAC2@dcn.davis.CA.us>

This forum is for questions about R. There are forums that focus on the theory of statistics (e.g. stats.stackexchage.com), but this particular issue is addressed in many statistics classes as well... and there is not necessarily a simple answer that always applies in all cases so be prepared to validate your model against a data set set aside for that purpose.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 20, 2015 2:02:24 AM PDT, Georgina Southon <g.southon at sheffield.ac.uk> wrote:
>Dear R help,
>
>This is rather a basic question, but I can't seem to find an answer
>anywhere else.
>
>When I run a model such as lm/aov(height~var1) where var 1 is a
>categorical variable with 6 levels, I get output that shows some
>significant parameters and other non significant. Normally I would then
>proceed to simplify the model by removing the insignificant terms,
>however, I have recently begun to wonder if that should be standard
>practice or whether the full model output (not reduced by
>simplification) has more integrity and should be retained?
>
>Any thoughts would be most welcome!
>
>Thanks,
>
>Lizzie
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jacobwegelin at fastmail.fm  Mon Jul 20 20:26:48 2015
From: jacobwegelin at fastmail.fm (Jacob Wegelin)
Date: Mon, 20 Jul 2015 14:26:48 -0400
Subject: [R] ggplot2 geom_boxplot limits
Message-ID: <alpine.OSX.2.20.1507191849540.9451@sombscqb055040a.local>

With base graphics, one can use the "ylim" argument to zoom in on a boxplot.

With ggplot2, using "limits" to try to zoom in on a boxplot *changes the box*.  Since the box usually indicates the 25th and 75th percentiles of a quantitative variable, this is puzzling.

The toy code below demonstrates this. In ggplot2, "zooming in" causes the two boxes to overlap, when they did not overlap in the full plot.  Also, the center lines --- which usually indicate the median of the variable --- change when one zooms in.

In base graphics, "zooming in" does not cause the boxes to overlap or, as far as I can see, the median line to move relative to the scale.

What is going on here?

pdf(file="toy-example.pdf")
set.seed(1)
toy1<-data.frame(Y=rnorm(500, mean=3), A="one")
toy2<-data.frame(Y=rnorm(500, mean=1.6), A="two")
toy<-rbind(toy1,toy2)
toy$A<-factor(toy$A)
library(ggplot2)
mybreaks<-signif(seq(from=min(toy$Y),to=max(toy$Y),by=0.5),digits=2)
mylimits<-c(0.61,3.7)
print(myplot<-ggplot(toy, aes(x=A,y=Y)) + geom_boxplot()+scale_y_continuous(breaks=mybreaks)+theme_bw())
print(myplot+scale_y_continuous(breaks=mybreaks,limits=mylimits))
boxplot(toy1$Y,toy2$Y)
boxplot(toy1$Y,toy2$Y, ylim=mylimits)
graphics.off()

> sessionInfo()
R version 3.2.1 (2015-06-18)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.4 (Yosemite)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] ggplot2_1.0.1

loaded via a namespace (and not attached):
  [1] MASS_7.3-40      colorspace_1.2-6 scales_0.2.5     magrittr_1.5     plyr_1.8.3       tools_3.2.1      gtable_0.1.2     reshape2_1.4.1
  [9] Rcpp_0.11.6      stringi_0.5-5    grid_3.2.1       stringr_1.0.0    digest_0.6.8     proto_0.3-10     munsell_0.4.2


Jacob A. Wegelin


From roger.bos at rothschild.com  Mon Jul 20 20:52:02 2015
From: roger.bos at rothschild.com (Bos, Roger)
Date: Mon, 20 Jul 2015 18:52:02 +0000
Subject: [R] using dcast with a function of multiple arguments
Message-ID: <0765308CD028654885F30322557308D81EEFB7F3@NYCSM0208.rth.ad.rothschild.com>

I am trying to figure out how to use dcast.data.table with a function with multiple arguments.  Here is my reproducible example for a simple function of one argument:

require(data.table)
dt <- as.data.table(mtcars)
dcast.data.table(dt, carb ~ cyl, value.var='mpg', fun=mean)

If I instead want to use, say, weighted.mean(x, w), how do I do so?

The docs say
...
Any other arguments that maybe passed to the aggregating function.

So I tried:

> dcast.data.table(dt, carb ~ cyl, value.var='mpg', fun=weighted.mean, w="wt")
Error in weighted.mean.default(data[[value.var]][0], ...) :
  'x' and 'w' must have the same length

The docs also say that value.var can be a list, so I tried that:

 In cases where value.var is a list, the function should be able to handle a list input and provide a single value or list of length one as output.

> dcast.data.table(dt, carb ~ cyl, value.var=list('mpg','wt'), fun=weighted.mean)
Error in dcast.data.table(dt, carb ~ cyl, value.var = list("mpg", "wt"),  :
  'value.var' must be a character vector of length 1.

I didn't actually expect that to work, but without an example I don't know what else to try.  Any hints would be greatly appreciated.

Thanks,

Roger



***************************************************************
This message and any attachments are for the intended recipient's use only.
This message may contain confidential, proprietary or legally privileged
information. No right to confidential or privileged treatment
of this message is waived or lost by an error in transmission.
If you have received this message in error, please immediately
notify the sender by e-mail, delete the message, any attachments and all
copies from your system and destroy any hard copies.  You must
not, directly or indirectly, use, disclose, distribute,
print or copy any part of this message or any attachments if you are not
the intended recipient.


From dmck at u.washington.edu  Mon Jul 20 20:02:49 2015
From: dmck at u.washington.edu (Don McKenzie)
Date: Mon, 20 Jul 2015 11:02:49 -0700
Subject: [R] jaccards index
In-Reply-To: <1437369783364-4710057.post@n4.nabble.com>
References: <1437369783364-4710057.post@n4.nabble.com>
Message-ID: <8FDC9F4E-09D6-4ADA-97BE-98A0D10C71E1@u.washington.edu>

Sarah Goslee?s package ?ecodist? will compute a Jaccard index, I believe.  

You are unlikely to get much help, however, unless you provide more details as to what you are trying to accomplish.  See

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

for how to create a reproducible example, as requested in the PostingGuide.

> On Jul 19, 2015, at 10:23 PM, sreenath <sreenath.rajur at macfast.ac.in> wrote:
> 
> hi..
> I have a csv file containing 35 coloumns and 193 rows.i want to generate
> jaccards index to normalise these data.how can i do this also from these
> data i want to draw boxplot.plz help
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/jaccards-index-tp4710057.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




From WMORGAN at wooster.edu  Mon Jul 20 19:42:25 2015
From: WMORGAN at wooster.edu (William Morgan)
Date: Mon, 20 Jul 2015 17:42:25 +0000
Subject: [R] error reading file DESCRIPTION
Message-ID: <6BF38763-6545-4D6F-85ED-CEE2529BDE0B@wooster.edu>

Hello,

After downloading imview from SourceForge.net <http://sourceforge.net/projects/imview/?source=directory>, 
I get the following error and warning when trying to install this package:

> install.packages("~/Downloads/imview-src-1.0.1.tar.gz", repos = NULL, type = "source")
Error: error reading file '/var/folders/57/76qlq1g9607g8nm3pv7txydw0000gp/T//Rtmp9Xr791/R.INSTALL4b47344defc/imview-1.0.1/DESCRIPTION'
Warning in install.packages :
  installation of package ?/Users/wmorgan/Downloads/imview-src-1.0.1.tar.gz? had non-zero exit status

Any suggestions on how to solve this problem?

Thanks,
Bill

P.S. I?m using R Studio, on a Mac Airbook with OS 10.10.4.


William R. Morgan, Ph.D.
Theron L. Peterson and Dorothy R. Peterson Professor of Biology
The College of Wooster
Department of Biology
931 College Mall
Wooster, OH 44691
330-263-2379
wmorgan at wooster.edu




From kandoigaurav at gmail.com  Mon Jul 20 19:44:49 2015
From: kandoigaurav at gmail.com (gaurav kandoi)
Date: Mon, 20 Jul 2015 12:44:49 -0500
Subject: [R] Printing row and column names of cells with specific value in a
	big matrix
Message-ID: <CANGdRbHKxER3bOtYQEitL=XVpHfP7De3J5ymvvCaeF04P-iZFw@mail.gmail.com>

Hi All

I've two big matrices (5k*4k) with the same structure, i.e. :

  mRNA1 mRNA2 mRNA3  lncRNA1 0.395646 0.94995 0.76177  lncRNA2 0.03791
0.661258 0.558658  lncRNA3 0.67846 0.652364 0.359054  lncRNA4 0.57769 0.003
0.459127
Now, I would like to extract the names of the row,col pairs whose value is
less than 0.05. In this case, I should get the output as (lncRNA2,mRNA1)
and (lncRNA4,mRNA2) alongwith their values (0.03791 and 0.003). Since the
structure of both the matrix is same, I would also like to retrieve the
corresponding values and row,col names from the second matrix.
(lncRNA2,mRNA1 and lncRNA4,mRNA2 alongwith their values in the second
matrix.)

I'm using the following code:

Pmatrix = read.table("pmatrix.csv", header=T, sep="," , row.names=1)
> sig_values <- which(Pmatrix<0.05, arr.ind=TRUE)
> sig_values
> Corr_Matrix = read.csv("corr_matrix.csv", header = T, row.names=1)
> Corr_Matrix[sig_values]


However, it only prints the row,col number (sig_values command) or only the
values (Corr_Matrix[sig_values]) command. How can I get the row and column
names alongwith their values?

Regards

-- 
*Gaurav Kandoi*

	[[alternative HTML version deleted]]


From xie at yihui.name  Mon Jul 20 21:02:19 2015
From: xie at yihui.name (Yihui Xie)
Date: Mon, 20 Jul 2015 14:02:19 -0500
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <alpine.LNX.2.11.1507201047240.1876@localhost>
References: <alpine.LNX.2.11.1507201047240.1876@localhost>
Message-ID: <CANROs4fLO_7rM9A0XhKE+ozMRv9hKyDkUgGVwAhYuctSxv6RVw@mail.gmail.com>

Section 5.1.3 of the book "Dynamic Documents with R and knitr" is
titled "Global Options". I don't know how to make it more clear for
readers to find information on global options in the book.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Web: http://yihui.name


On Mon, Jul 20, 2015 at 12:58 PM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
>   Near the beginning of a LyX document I have a knitr chunk with options
> that begin with 'global_options', and includes echo=F. This presents the R
> code in that chunk from displaying in the compiled PDF file. However, all
> following knitr chunks are included in the PDF file.
>
>   Reading the docs (including the Knitr book) does not show me what I am
> doing incorrectly. A pointer to the solution is needed.
>
> Rich


From rshepard at appl-ecosys.com  Mon Jul 20 20:55:13 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Mon, 20 Jul 2015 11:55:13 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <CAJuCY5zjgVK9NkcdXaGd5Y=7gOBWYSQwBsdU1iPQvNSBYc28Dw@mail.gmail.com>
References: <alpine.LNX.2.11.1507201047240.1876@localhost>
	<CAJuCY5zjgVK9NkcdXaGd5Y=7gOBWYSQwBsdU1iPQvNSBYc28Dw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507201145510.1876@localhost>

On Mon, 20 Jul 2015, Thierry Onkelinx wrote:

> Have you tried echo = FALSE instead of echo = F. If that doesn't solve
> your problem, please provide a minimal reproducible example.

   Yes, I have.

   Attached is a TeX file renamed to sample.txt (rather than .tex to ensure
it is not stripped), a PDF of the compiled page, and the source data that's
being read in the sample doc.

Thanks,

Rich
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: sample.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150720/d22694e7/attachment.txt>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample.pdf
Type: application/pdf
Size: 69474 bytes
Desc: 
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150720/d22694e7/attachment.pdf>
-------------- next part --------------
sampdate,Temp,Dischg,Turb,SC,pH,ANC,Alk,Hard,TDS,TSS,DO,HCO3,CO3,N.tot,N.org,NH4,NO3,NO2,PO4,SO4,C,Ca,Mg,Na,K,Cl,F,Si,As,Ba,Be,Cd,Cr,Co,Cu,Fe,Pb,Mn,Mo,Ni,Ag,Sr,V,Zi,Al,Li,Se,CN,Hg
1965-10-01,,98,,490,8.4,,,150,,,,238,4,,,,,,,,,40,12,46,,,,,,,,,,,,,,,,,,,,,,,,,
1965-11-01,,141,,507,8.3,213,,170,,,,252,4,,,,,,,,,50,11,42,,,,,,,,,,,,,,,,,,,,,,,,,
1965-12-01,,128,,564,8.1,238,,190,359,,,290,,,,,,,,37,,56,13,47,6.4,16,0.5,40,,,,,,,,,,,,,,,,,,,,,
1966-01-01,,122,,506,8.2,212,,170,321,,,259,,,,,,,,34,,50,11,45,6.1,16,0.6,31,,,,,,,,,,,,,,,,,,,,,
1966-01-17,,97,,551,8.5,241,,200,,,,276,9,,,,,,,,,58,13,48,,,,,,,,,,,,,,,,,,,,,,,,,
1966-02-01,,108,,506,8.2,216,,180,,,,263,,,,,,,,,,52,11,42,,,,,,,,,,,,,,,,,,,,,,,,,
1966-03-01,,384,,516,7.7,201,,160,,,,245,,,,,,,,,,46,11,46,8,,,,,,,,,,,,,,,,,,,,,,,,
1966-03-18,,701,,628,8.1,235,,190,,,,286,,,,,,,,,,51,16,61,8.5,,,,,,,,,,,,,,,,,,,,,,,,
1966-04-01,,612,,470,8,189,,160,301,,,231,,,,,,,,37,,43,12,40,6.3,14,0.5,34,,,,,,,,,,,,,,,,,,,,,
1966-04-11,,541,,423,8,172,,140,,,,210,,,,,,,,,,40,10,34,5.4,,,,,,,,,,,,,,,,,,,,,,,,
1966-04-18,,355,,466,8.1,193,,160,,,,235,,,,,,,,,,43,12,39,5.9,,,,,,,,,,,,,,,,,,,,,,,,
1966-05-01,,167,,498,8,208,,170,,,,253,,,,,,,,,,46,13,42,6.8,,,,,,,,,,,,,,,,,,,,,,,,
1966-05-06,,260,,420,8.1,181,,140,,,,221,,,,,,,,,,41,10,33,5.4,,,,,,,,,,,,,,,,,,,,,,,,
1966-07-01,,7,,566,8,226,,180,450,,,275,,,,,,,,48,,47,16,52,96,21,0.6,34,,,,,,,,,,,,,,,,,,,,,
1966-08-01,,3.9,,550,8.1,210,,170,,,,256,,,,,,,,,,40,17,51,9.7,,,,,,,,,,,,,,,,,,,,,,,,
1966-09-01,,4.1,,536,8.4,208,,170,,,,254,,,,,,,,,,41,17,48,8.9,,,,,,,,,,,,,,,,,,,,,,,,
1966-10-01,,7.7,,546,8.4,220,,190,340,,,260,4,,,,,,,43,,48,16,46,8.5,19,0.5,27,,,,,,,,,,,,,,,,,,,,,
1966-11-01,,16,,536,8.4,224,,180,,,,263,5,,,,,,,,,49,15,45,7.6,,,,,,,,,,,,,,,,,,,,,,,,
1966-12-01,,31,,533,8.6,222,,180,336,,,248,11,,,,,,,37,,50,13,47,7.5,14,0.5,34,,,,,,,,,,,,,,,,,,,,,
1966-12-16,,30,,615,8.6,264,,210,,,,296,13,,,,,,,,,60,14,56,8.7,,,,,,,,,,,,,,,,,,,,,,,,
1967-01-01,,43,,517,8.8,214,,170,333,,,228,16,,,,,,,40,,50,12,48,7.3,19,0.5,28,,,,,,,,,,,,,,,,,,,,,
1967-02-01,,72,,513,8.7,217,,180,,,,234,15,,,,,,,,,51,12,46,6.9,,,,,,,,,,,,,,,,,,,,,,,,
1967-02-12,,113,,474,8.3,189,,160,,,,230,3,,,,,,,,,46,11,41,6.8,,,,,,,,,,,,,,,,,,,,,,,,
1967-03-01,,232,,464,8.5,174,,150,,,,212,7,,,,,,,,,44,10,41,6.6,,,,,,,,,,,,,,,,,,,,,,,,
1967-03-25,,422,,498,8.5,179,,160,,,,218,6,,,,,,,,,48,10,43,6.5,,,,,,,,,,,,,,,,,,,,,,,,
1967-04-01,,333,,448,8.5,164,,150,283,,,200,5,,,,,,,40,,42,11,39,5.7,15,0.5,26,,,,,,,,,,,,,,,,,,,,,
1967-05-01,,327,,451,8.5,165,,150,,,,201,6,,,,,,,,,43,10,38,5.8,,,,,,,,,,,,,,,,,,,,,,,,
1967-05-10,,406,,371,8.3,127,,110,,,,155,2,,,,,,,,,29,9.5,34,5.7,,,,,,,,,,,,,,,,,,,,,,,,
1967-05-14,,396,,435,8.3,158,,160,,,,193,2,,,,,,,,,45,11,38,6.7,,,,,,,,,,,,,,,,,,,,,,,,
1967-05-20,,844,,365,8,147,,150,,,,179,,,,,,,,,,45,8.1,25,5.6,,,,,,,,,,,,,,,,,,,,,,,,
1967-06-01,,1560,,443,8.4,189,,180,,,,230,4,,,,,,,,,55,10,35,6.7,,,,,,,,,,,,,,,,,,,,,,,,
1967-07-01,,1340,,386,8.5,175,,150,255,,,213,13,,,,,,,14,,47,8.6,27,4.8,7.4,0.5,28,,,,,,,,,,,,,,,,,,,,,
1967-07-11,,651,,406,8.3,187,,170,,,,228,2,,,,,,,,,55,9.1,27,5,,,,,,,,,,,,,,,,,,,,,,,,
1967-07-21,,254,,453,8.2,207,,190,,,,252,,,,,,,,,,58,10,35,6,,,,,,,,,,,,,,,,,,,,,,,,
1967-08-01,,48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1967-08-01,,48,,525,8.2,223,,190,,,,272,,,,,,,,,,53,13,47,7.4,,,,,,,,,,,,,,,,,,,,,,,,
1967-09-01,,18,,508,8.1,210,,180,,,,256,,,,,,,,,,46,15,45,7.9,,,,,,,,,,,,,,,,,,,,,,,,
1967-10-01,,20,,556,8.4,221,,180,322,,,255,7,,,,,,,32,,47,15,45,8.2,16,0.6,24,,,,,,,,,,,,,,,,,,,,,
1967-10-01,,7.7,,546,8.4,213,,190,340,,,260,4,,,,0.02,,,43,,48,16,46,8.5,19,0.5,27,,,,,,,,,,,,,,,,,,,,,
1967-10-09,,27,,612,8.4,242,,190,,,,281,7,,,,,,,,,53,15,57,11,,,,,,,,,,,,,,,,,,,,,,,,
1967-10-17,,23,,586,8.2,238,,190,,,,290,,,,,,,,,,53,15,49,8.3,,,,,,,,,,,,,,,,,,,,,,,,
1967-11-01,,36,,582,8.4,238,,190,,,,278,6,,,,,,,,,55,14,50,7.8,,,,,,,,,,,,,,,,,,,,,,,,
1967-12-01,,44,,565,8.2,239,,170,,,,292,,,,,,,,,,50,12,53,8.2,,,,,,,,,,,,,,,,,,,,,,,,
1967-12-09,,41,,626,8.2,271,,210,,,,330,,,,,,,,,,60,14,57,8.7,,,,,,,,,,,,,,,,,,,,,,,,
1967-12-19,,46,,553,8.2,241,,180,,,,294,,,,,,,,,,54,12,51,8.3,,,,,,,,,,,,,,,,,,,,,,,,
1968-01-01,,50,,519,8.6,218,,170,334,,,238,14,,,,,,,41,,45,13,50,7.9,16,0.6,28,,,,,,,,,,,,,,,,,,,,,
1968-01-15,,48,,590,8.3,251,,200,372,,,298,4,,,,,,,43,,57,13,52,8.6,16,0.6,30,,,,,,,,,,,,,,,,,,,,,
1968-02-01,,70,,406,7.8,142,,110,,,,173,,,,,,,,,,27,11,40,6.6,,,,,,,,,,,,,,,,,,,,,,,,
1968-02-11,,221,,480,8.3,195,,160,,,,232,3,,,,,,,,,44,11,43,6.3,,,,,,,,,,,,,,,,,,,,,,,,
1968-03-01,,259,,503,8.4,204,,160,,,,241,4,,,,,,,,,45,11,46,6.9,,,,,,,,,,,,,,,,,,,,,,,,
1968-03-09,,221,,471,8.3,191,,150,,,,227,3,,,,,,,,,44,10,40,6.4,,,,,,,,,,,,,,,,,,,,,,,,
1968-03-15,,229,,479,8,195,,160,,,,238,,,,,,,,,,44,12,41,6.4,,,,,,,,,,,,,,,,,,,,,,,,
1968-04-01,,169,,415,8.2,157,,130,271,,,192,,,,,,,,35,,36,10,38,5.8,18,0.5,27,,,,,,,,,,,,,,,,,,,,,
1968-05-01,,160,,381,8.4,163,,130,,,,193,3,,,,,,,26,,38,8.6,33,5.5,,,,,,,,,,,,,,,,,,,,,,,,
1968-05-15,,164,,380,8,154,,120,235,,,188,,,,,,,,26,,33,8.7,36,5.8,12,0.4,19,,,,,,,,,,,,,,,,,,,,,
1968-06-01,,498,,311,7.8,138,,120,,,,168,,,,,,,,,,37,5.8,20,,,,,,,,,,,,,,,,,,,,,,,,,
1968-06-07,,914,,394,8.2,174,,140,,,,212,,,,,,,,,,42,8.5,31,,,,,,,,,,,,,,,,,,,,,,,,,
1968-06-15,,985,,424,7.7,184,,150,,,,224,,,,,,,,,,45,9.2,33,,,,,,,,,,,,,,,,,,,,,,,,,
1968-07-01,,140,,436,8.2,195,,150,273,,,238,,,,,,,,21,,44,10,37,5.7,11,1.1,24,,,,,,,,,,,,,,,,,,,,,
1968-08-01,,20,,520,8.6,213,,170,,,,240,10,,,,,,,,,45,13,49,,,,,,,,,,,,,,,,,,,,,,,,,
1968-08-15,,12,,566,8.2,231,,180,,,,282,,,,,,,,,,50,14,52,,,,,,,,,,,,,,,,,,,,,,,,,
1968-08-24,,44,,473,8.6,205,,160,,,,226,12,,,,,,,,,45,11,42,,,,,,,,,,,,,,,,,,,,,,,,,
1968-09-01,,29,,512,8.4,220,,170,,,,256,6,,,,,,,,,45,13,48,,,,,,,,,,,,,,,,,,,,,,,,,
1968-09-15,,28,,536,8.1,225,,170,,,,274,,,,,,,,,,47,13,49,,,,,,,,,,,,,,,,,,,,,,,,,
1977-05-20,16,126,,483,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1977-06-29,23.5,149,,445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1977-07-29,24.5,14,,517,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1977-08-11,15.5,14,,464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1977-10-06,10.5,17,,506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1977-11-09,,26,,496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1977-12-19,,26,,551,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-01-24,,64,,527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-02-27,6.5,156,,541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-03-24,10,557,,407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-04-20,12,579,,389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-07-07,17.5,370,,384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-08-22,19.5,19,,550,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-09-21,16.5,52,,498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1978-11-21,4,77,,490,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-01-04,0.5,54,,619,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-02-05,0.5,103,,495,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-03-02,3.5,338,,536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-04-26,13,724,,359,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-04-26,13,724,54,374,8.6,210,,140,273,90,8.6,230,13,0.6,0.46,0.03,,,,24,6.1,41,8.8,29,5.5,11,0.4,27,,,,,,,,,,,,,,,,,,,,,
1979-05-29,16.5,2160,84,366,8.2,280,,130,234,53,7.9,340,,1.1,1,0.01,,,,23,,39,8.3,28,7.1,9.7,0.4,28,6,100,,,,,,20,,20,,,,,,20,,,1,,0.1
1979-07-09,22,204,13,418,8.6,215,,160,283,89,8,240,11,0.43,0.37,0.01,,,,21,5.1,46,11,33,5.6,9.3,0.4,27,,,,,,,,,,,,,,,,,,,,,
1979-07-09,22,204,,377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-07-30,26,88,,448,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-07-30,26,80,10,458,8.8,235,,160,299,83,7.5,280,3,0.66,0.62,0.01,,,,24,,44,11,35,7.1,11,0.4,25,8,90,,2,,3,,10,,1,,,,,,3,,,1,,0.1
1979-08-29,21.5,29,,518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1979-08-29,21.5,19,13,509,8.4,185,,160,201,78,8.5,220,3,0.69,0.42,0.01,,,,39,5.4,46,11,48,9.6,21,0.6,26,,,,,,,,,,,,,,,,,,,,,
1979-10-26,12.5,42,7,610,8.9,259,,190,395,,11,230,42,0.81,0.49,0.09,,,,48,,54,14,55,11,27,0.6,30,10,100,,,,3,,10,,,,,,,,3,,,,,0.2
1979-11-27,1,74,10,521,8.6,204,,170,327,,12.2,220,14,0.59,0.39,0.01,,,,33,4,50,11,44,7.3,17,0.6,31,,,,,,,,,,,,,,,,,,,,,
1979-12-21,1,96,6,514,8.5,,,170,,,12,,,1.2,0.92,,,,,33,7.8,50,12,43,7.2,17,0.5,32,,,,,,,,,,,,,,,,,,,,,
1980-02-05,3,227,21,522,7.9,205,,180,333,89,11.2,250,,0.85,0.49,0.04,,,,39,,51,13,48,7.4,17,0.5,33,6,90,,1,,3,,30,,,,,,,,3,,,,,0.2
1980-02-28,8,612,60,550,,197,,170,331,83,9.6,240,,1.2,0.88,,,,,48,9.1,47,12,47,8.2,20,0.5,29,,,,,,,,,,,,,,,,,,,,,
1980-03-27,5,299,19,505,7.6,197,,160,307,,,240,,0.56,0.48,0.01,,,,37,13,46,12,42,6.4,15,0.4,30,,,,,,,,,,,,,,,,,,,,,
1980-04-24,12,652,130,308,8.3,,,100,,81,8.3,,,1.8,1.6,0.08,,,,18,13,30,7.1,25,5.3,9.9,0.3,28,,,,,,,,,,,,,,,,,,,,,
1980-05-29,14.5,3180,88,449,7.8,190,,140,281,,7.6,233,,1.4,1.2,0.06,,,,29,,42,9.6,41,7.9,13,0.2,29,7,90,,1,,3,10,20,,10,,,,,,,,,,,0.1
1980-06-23,20.5,2070,23,362,,,,140,,,6.7,,,,,0.04,,,,11,8.9,42,9.2,24,5.2,7.9,0.3,32,,,,,,,,,,,,,,,,,,,,,
1980-07-29,24,203,11,427,8.2,180,,160,257,,8.1,230,,0.63,0.5,0.01,,,,20,,46,9.9,31,5.9,10,0.5,25,7,90,,,,3,,40,,20,,,,,,10,,,,,
1980-08-25,19.5,49,11,545,7.7,216,,170,342,,9.2,260,,1.4,1.1,,,,,40,10,50,12,47,8.2,20,0.8,26,,,,,,,,,,,,,,,,,,,,,
1980-09-30,17,47,6,530,8.4,223,,170,336,,11,260,6,0.82,0.74,,,,,36,4.4,47,12,50,8.8,23,0.6,25,,,,,,,,,,,,,,,,,,,,,
1980-11-21,5,108,10,553,8.5,210,,180,346,,12.2,220,8,0.6,0.46,0.02,,,,52,,52,12,45,8.5,21,0.5,28,7,100,,1,,3,,10,,10,,,,,,3,,,,,
1981-01-28,3.5,127,14,527,8.7,210,,170,355,,12,270,19,,,0.05,,,,38,,50,12,46,6.5,19,0.5,30,8,100,,1,,3,,30,,20,,,,,,10,,,,,
1981-03-25,9.5,110,24,600,8.8,230,,200,364,,11.8,250,17,0.55,0.51,0.05,,,,47,4.9,56,14,53,6.8,20,0.5,28,,,,,,,,,,,,,,,,,,,,,
1981-05-20,13,117,16,474,8.6,200,,180,,,,230,7,1,0.94,0.08,,,,,,55,10,36,7.7,14,0.5,27,9,,,1,,3,,20,,20,,,,,,,7,,,,
1981-07-21,25.5,5.7,9,616,8.6,240,,200,382,,9.2,270,12,1,0.71,0.13,,,,43,,54,15,55,10,25,0.5,34,14,120,,1,,3,,10,,10,,,,,,,,,,,0.1
1981-09-17,21.5,9.5,3,487,8.1,200,,170,292,,10.2,250,,1.2,0.97,0.13,,,,25,,44,14,37,8,15,0.6,24,9,100,,1,,3,,10,,20,,,,,,10,,,1,,
1981-12-04,3,23,4,623,8.8,240,,210,406,,13,260,15,0.57,0.38,0.09,0.1,0.02,,56,,62,13,58,9.5,33,0.6,30,9,100,,1,10,3,1,30,1,20,,,1,,,3,,,1,,0.1
1982-01-27,1,61,8,529,8.2,210,,180,342,,12.5,,,0.88,,,,,0.55,42,,52,11,43,7.7,23,0.5,32,,,,,,,,,,,,,,,,,,,,,
1982-03-12,7,649,96,395,8.4,160,,130,248,87,10.7,,,0.7,0.48,0.13,,,0.31,22,,37,9.3,33,7.4,15,0.4,28,9,82,,1,10,,,,,,,,1,,,4,,,1,,0.1
1982-05-26,19,1530,56,367,8.2,158,,130,235,70,7,,,1.3,1.1,0.12,0.1,0.02,0.31,22,,37,8.5,26,5.4,9.6,0.4,31,7,84,,1,10,1,,80,1,,,1,1,,,20,,,1,,0.1
1982-07-20,25,349,26,418,8,190,,150,253,,7.5,,,1.7,1.5,0.14,,,0.31,17,,45,9.3,25,6,9,0.4,27,,,,,,,,,,,,,,,,,,,,,
1982-09-21,20.5,56,6,510,8.6,208,,160,318,,,,,1,0.84,0.06,,,0.37,32,,49,10,46,8.5,20,0.5,27,9,100,,1,10,1,,,1,,,,1,,,10,,,1,,0.1
1982-11-24,,281,25,486,8.6,213,,180,313,,12.7,,,1.1,0.87,0.13,,,0.09,35,,50,12,38,5.6,14,0.4,29,5,81,0.5,1,1,3,,,1,,10,1,1,300,6,10,10,30,1,,0.1
1983-01-25,2,270,20,476,8.4,195,,160,294,,13.1,,,0.83,0.58,0.12,,,0.25,30,,47,11,37,5.5,16,0.4,30,,,,,,,,,,,,,,,,,,,,,
1983-03-06,5,4800,200,308,7.6,109,,79.9,175,79,,,,2.1,1.6,0.33,,,0.521,21,,23,5.4,26,6.9,10,0.3,15,6,49,0.5,1,1,3,,40,,20,10,,1,150,6,,40,20,1,,0.1
1983-05-24,19.5,1500,110,359,8.2,153,,127,232,86,7.2,,,1.2,0.97,0.13,,,0.184,20,,36,9,26,5.4,12,0.3,30,6,73,0.5,1,1,3,,30,,,10,,1,220,6,40,40,20,1,,0.1
1983-07-20,19.5,499,26,369,8.4,176,,143,231,,7.7,,,1,0.77,0.13,,,0.245,16,,43,8.7,23,4.4,7.1,0.4,22,,,,,,,,,,,,,,,,,,,,,
1983-09-20,16,75,4,505,,212,,175,323,,11,,,0.9,0.8,0.1,,,0.368,31,,50,12,47,7.1,22,0.5,25,8,98,0.5,1,1,3,,3,,10,10,,1,390,6,,10,40,1,,0.1
1983-11-15,5.5,335,13,512,8.4,230,,193,333,88,12.2,,,1.1,0.87,0.13,,,0.123,32,,54,14,43,5.8,17,0.4,28,5,87,0.5,1,1,3,,,1,10,10,1,1,350,6,,10,40,1,,0.1
1984-01-24,,369,32,532,7.7,222,,190,335,80,10.7,,,0.97,0.68,0.12,,,0.184,37,,53,14,40,5.6,20,0.4,31,,,,,,,,,,,,,,,,,,,,,
1984-03-14,3.5,2630,640,361,8.1,134,,111,215,80,11,,,2.6,2.4,0.13,,,0.245,24,,32,7.4,30,5.9,13,0.2,20,5,67,0.5,,1,3,,130,1,20,10,1,1,200,6,130,130,20,1,,0.1
1984-05-18,15,8130,180,352,8.1,140,,111,224,,8.4,,,2.1,1.9,0.07,,,0.153,22,,31,8,28,10,13,0.4,27,5,59,1,1,1,3,,,,10,10,1,1,190,6,3,10,20,1,,0.1
1984-07-25,24,1200,39,327,,142,,126,196,,,,,0.93,0.71,0.09,,,0.153,14,,38,7.5,19,3.7,6.9,0.3,21,,,,,,,,,,,,,,,,,,,,,
1984-09-27,13.5,160,8,519,8.5,205,,179,318,,10.6,,,0.4,0.27,0.03,,,0.245,37,,50,13,39,7.7,21,0.5,26,7,110,1,1,1,3,,,,10,10,,1,370,6,,10,30,1,,0.1
1984-11-30,2.5,335,15,600,8.4,239,,208,,,14,,,1.1,0.95,0.05,,,0.153,,,55,17,48,6.9,,0.4,30,5,110,0.5,1,,3,,,,20,10,,1,390,6,3,10,30,1,,0.1
1985-01-22,0.5,218,31,404,8.2,129,,144,294,,12.2,,,0.86,0.49,0.11,,,0.276,29,,41,10,29,4.3,19,0.3,21,,,,,,,,,,,,,,,,,,,,,
1985-03-27,3.5,1040,90,476,8.3,169,,166,305,88,12.8,,,1.5,1.2,0.07,,,0.215,41,,45,13,38,6.8,20,0.3,27,6,83,0.6,1,1,3,5,91,7,11,10,2,1,260,6,18,180,26,1,,0.1
1985-05-21,17.5,606,27,430,8.4,166,,151,268,90,8.7,,,0.6,0.42,0.08,,,0.031,28,,44,10,30,6.1,14,0.4,28,5,80,0.5,1,1,3,6,27,4,8,10,1,1,260,6,28,30,22,1,,0.1
1985-07-23,23.5,54,17,577,8.6,215,,176,358,98,8.7,,,0.7,0.57,0.03,,,0.276,42,,49,13,50,9.9,26,0.5,29,,,,,,,,,,,,,,,,,,,,,
1985-09-26,13.5,33,9,675,8.8,229,,188,394,,10.6,,,0.9,0.72,0.08,,,1.17,57,,52,14,66,13,40,0.6,27,12,140,0.5,1,1,3,1,4,1,11,10,1,1,590,6,14,10,67,1,,0.1
1985-11-19,,76,11,579,8.7,205,,190,361,,15.2,,,0.61,0.37,0.1,0.09,0.02,0.429,51,,56,13,50,7.1,26,0.6,33,8,110,0.5,1,1,3,3,9,1,56,10,2,1,440,6,10,10,37,1,,0.2
1986-01-22,1,247,50,451,8.3,165,,140,273,86,12.6,,,1.1,0.66,0.2,0.16,0.02,0.521,38,,39,11,36,7.1,16,0.4,26,,,,,,,,,,,,,,,,,,,,,
1986-03-27,12,822,47,399,8.2,146,,131,240,89,9.8,,,0.6,0.45,0.05,0.09,0.01,0.153,29,,36,10,29,5.3,16,0.4,28,7,68,0.5,1,1,3,4,47,1,8,10,1,1,230,6,7,50,18,1,,0.1
1986-05-22,12,942,40,384,8.4,149,,131,229,,8,,,1,0.85,0.01,0.1,0.01,0.184,24,,38,8.8,25,4.8,11,0.3,28,,,,,,,,,,,,,,,,,,,,,
1986-07-23,21,126,10,443,8.7,190,,159,276,,8.9,,,0.5,0.36,0.02,0.09,0.01,0.245,30,,47,10,33,6.3,14,0.4,23,,,,,,,,,,,,,,,,,,,,,
1986-09-23,15,29,1,612,8.8,231,,201,400,,12.3,,,,,0.02,0.1,0.01,0.797,54,,57,14,59,13,34,0.6,28,10,120,0.5,1,1,3,1,9,,12,10,1,1,510,6,9,10,55,1,,0.1
1986-11-25,4,152,10,475,8.3,198,,177,303,,11.6,,,0.3,0.19,0.01,0.1,0.01,0.061,32,,51,12,39,6,16,0.4,29,7,90,0.5,1,1,3,1,79,5,13,10,1,1,360,6,17,10,32,1,,0.1
1987-01-28,0.5,74,3,506,8.3,,,177,309,,11.8,235,4,0.6,0.48,0.01,0.1,0.01,0.092,37,,51,12,39,6.1,16,0.5,28,,,,,,,,,,,,,,,,,,,,,
1987-03-24,7.5,300,33,603,8.6,230,,209,392,79,10.5,,,0.56,0.44,0.01,0.06,0.01,0.123,60,,57,16,55,7.4,27,0.6,33,8,100,0.5,1,1,3,1,19,5,11,10,2,1,370,6,4,10,39,1,,0.5
1987-06-01,18.5,483,31,460,8.5,193,,159,285,,,,,1.3,1.1,0.02,0.1,0.01,0.123,28,,47,10,34,5.9,16,0.4,31,3,82,0.5,1,3,3,8,27,5,8,10,1,1,280,6,12,10,26,1,,0.1
1987-08-26,21,10,5,491,8.6,,,171,306,85,9.8,,,0.3,0.17,0.01,0.1,0.01,0.031,30,,45,14,41,8.6,21,0.8,24,9,100,0.5,1,2,3,2,4,5,12,10,1,1,490,6,7,10,45,1,,0.1
1987-11-03,14.5,8,3,521,8.5,205,,173,307,98,,,,0.6,0.47,0.02,0.1,0.01,0.031,38,,46,14,46,7.8,20,0.5,20,10,110,0.5,1,1,3,2,3,5,10,10,4,1,420,6,3,10,41,1,,0.1
1988-01-28,0.5,33,1,588,8.6,235,,201,369,,14,,,0.3,0.18,0.01,0.09,0.01,0.031,46,,59,13,48,8.3,22,0.5,31,,,,,,,,,,,,,,,,,,,,,
1988-03-31,8.5,234,33,409,8.6,181,,162,322,98,,,,0.6,0.47,0.03,0.1,0.01,0.153,44,,45,12,38,6.5,17,0.4,30,6,74,0.5,1,2,3,1,10,5,10,10,5,1,310,6,5,10,24,1,,0.1
1988-05-26,17.5,384,41,300,8.4,114,,109,195,98,8.3,,,0.4,0.28,0.02,0.1,0.01,0.061,26,,34,5.9,20,4.3,8.5,0.4,21,4,55,0.5,1,1,3,1,9,5,3,10,1,1,210,6,8,10,17,1,,0.1
1988-08-24,24.5,11,4,479,8.5,187,,171,298,85,8.8,,,1,0.9,0.01,0.1,0.01,0.031,28,,45,14,40,7.8,15,0.5,24,8,100,0.5,1,1,3,1,5,5,14,10,3,1,480,6,3,10,51,1,,0.1
1988-11-29,0.5,64,13,548,8.6,225,223,187,358,,12.4,,,1,0.36,0.04,0.62,0.01,0.276,44,,53,13,52,7.8,22,0.5,28,8,100,0.5,1,1,3,1,16,5,11,10,5,1,490,6,3,10,47,1,,0.1
1988-11-29,0.5,,,545,8.6,,,,,,12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1988-11-29,0.5,,,550,8.6,,,,,,12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1988-11-29,0.5,,,545,8.6,,,,,,12.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-01-26,,29,68,615,8.5,,270,217,383,,14.9,,,0.3,0.16,0.03,0.1,0.01,0.031,36,,64,14,51,8.9,17,0.3,30,,,,,,,,,,,,,,,,,,,,,
1989-04-08,14.5,905,50,395,8.3,136,136,128,240,80,8.4,,,0.8,0.65,0.04,0.1,0.01,0.031,35,,36,9.2,29,5.8,15,0.3,28,7,67,0.5,1,1,3,8,47,5,7,10,6,1,230,6,6,60,17,1,,0.1
1989-05-31,17,524,12,378,8.2,167,166,142,242,90,7.6,,,0.6,0.46,0.01,0.1,0.01,0.123,21,,42,9,27,5.2,8.9,0.3,28,5,69,0.5,1,1,3,4,13,1,6,10,2,1,250,6,5,20,20,1,,0.1
1989-05-31,17,,,380,8.2,,,,,,7.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-05-31,17,,,378,8.1,,,,,,7.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-05-31,17,,,376,8.1,,,,,,7.6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-07-31,24.5,19,4,530,8.7,201,,163,320,72,9.1,,,0.6,0.5,0.01,0.1,0.01,0.031,43,,44,13,47,8.7,20,0.7,23,,,,,,,,,,,,,,,,,,,,,
1989-08-28,23,15,4,445,8.5,189,188,152,272,87,9.4,,,0.4,0.29,0.01,0.1,0.01,0.031,27,,41,12,36,8,14,0.5,21,8,98,0.5,1,1,3,1,6,1,8,10,1,1,440,6,4,10,43,1,,0.1
1989-08-28,22,,,445,8.5,,,,,,9.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-08-28,22,,,445,8.5,,,,,,9.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-08-28,22,,,442,8.6,,,,,,9.4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1989-11-29,0.5,49,12,502,8.3,208,240,175,350,68,13.2,,,0.5,0.39,0.01,0.1,0.01,0.061,41,,50,12,45,7.9,21,0.6,29,8,99,0.5,1,1,3,5,16,1,14,10,5,1,450,6,3,20,41,1,,0.1
1990-01-17,1,71,15,459,8.7,183,190,160,294,64,12.2,,,0.4,0.28,0.01,0.1,0.01,0.061,36,,46,11,38,5.5,17,0.5,27,,,,,,,,,,,,,,,,,,,,,
1990-03-30,11,307,26,420,8.5,175,168,149,272,99,9.9,,,0.9,0.77,0.01,0.1,0.01,0.092,32,,43,10,34,6.6,17,0.3,28,7,72,0.7,1,5,3,10,29,10,6,10,10,1,280,6,3,50,24,1,,0.1
1990-05-22,20,195,9,396,8.6,164,169,133,242,83,9.2,,,0.7,0.6,0.01,0.1,0.01,0.061,24,,40,7.9,29,5.9,13,0.1,21,5,62,0.5,1,1,3,2,11,1,2,10,1,1,260,6,8,10,22,1,,0.1
1990-06-28,20,238,10,378,8.7,174,173,139,235,93,8,,,0.5,0.37,0.03,0.1,0.01,0.031,16,,42,8.4,26,4.6,11,0.4,23,,,,,,,,,,,,,,,,,,,,,
1990-08-21,19.5,11,5,437,8.4,183,186,150,272,93,9.5,,,0.8,0.69,0.02,0.09,0.01,0.031,29,,40,12,34,7.3,14,1.3,22,7,84,0.5,1,1,3,2,3,1,4,10,1,1,430,6,3,20,42,1,,0.1
1990-11-01,7,14,3,576,8.6,209,210,179,302,61,13.6,,,0.6,0.49,0.01,0.1,0.01,0.061,30,,50,13,38,7.3,16,0.2,21,7,120,0.5,1,1,3,1,6,1,12,10,1,1,500,6,27,10,44,1,,0.1
1991-01-03,,15,3,623,7.8,264,265,219,397,43,13.2,,,0.4,0.25,0.03,0.1,0.01,0.031,52,,63,15,48,9.7,22,0.6,27,,,,,,,,,,,,,,,,,,,,,
1991-02-22,7,152,20,478,7.9,195,170,156,290,86,11.8,,,0.7,0.57,0.01,0.1,0.01,0.092,46,,46,9.9,39,6.3,14,0.4,25,6,79,0.5,1,1,3,3,25,1,11,10,1,1,330,6,5,40,30,1,,0.4
1991-04-29,11.5,119,14,488,8.7,199,193,159,295,99,10.4,,,0.56,0.48,0.03,0.05,0.01,0.031,34,,47,10,39,6.7,15,0.6,26,6,80,0.5,1,1,3,2,18,1,15,10,2,1,350,6,7,20,32,1,,0.1
1991-06-28,17,487,18,365,8.5,167,163,144,222,83,6.8,,,0.65,0.58,0.01,0.05,0.01,0.031,16,,45,7.6,22,3.8,8,0.1,22,,,,,,,,,,,,,,,,,,,,,
1991-08-28,17.5,16,3,455,8.5,199,190,160,279,97,11.2,,,0.45,0.4,0.01,0.05,0.01,0.031,28,,44,12,34,6.8,17,0.6,22,6,95,0.5,1,1,3,1,6,1,14,10,1,1,450,6,13,30,43,1,,0.1
1991-11-01,4,32,2,458,8.5,208,194,181,326,76,12.3,,,0.26,0.18,0.02,0.063,0.01,0.031,48,,51,13,40,7.7,21,0.6,27,,110,,,,3,,7,,12,10,1,1,430,6,,10,63,1,,
1991-12-28,0.5,68,5,506,8.2,230,159,189,308,65,12.2,,,0.25,0.19,0.01,0.05,0.01,0.031,47,,56,12,45,7.1,20,0.5,25,,,,,,,,,,,,,,,,,,,,0.01,
1992-02-26,7,215,49,412,8.4,184,185,154,274,97,11.2,,,0.55,0.48,0.01,0.05,0.01,0.061,27,,46,9.4,32,7.2,16,0.5,25,,71,,,,3,,64,,8,10,2,1,290,6,,120,26,1,,
1992-04-28,19.5,149,13,432,8.5,180,178,142,275,79,9.4,,,0.35,0.28,0.01,0.05,0.01,0.031,31,,42,9,39,6.6,18,0.5,22,,71,,,,3,,10,,5,10,1,1,330,6,,20,30,1,,
1992-08-27,13,11,4,451,8.4,191,190,153,292,95,8.8,,,0.45,0.38,0.02,0.05,0.01,0.031,33,,38,14,41,8.3,20,0.6,23,,110,,,,3,,8,,7,10,1,1,480,6,,10,49,1,0.01,
1992-10-27,11,16,2,474,8.4,208,208,170,299,72,9.4,,,0.35,0.29,0.01,0.05,0.01,0.031,30,,48,12,36,7.7,15,0.5,20,,120,,,,3,,6,,8,10,1,1,500,6,,20,45,1,0.01,
1993-02-26,1,40,4,543,8.4,213,245,175,365,64,11.8,,,0.44,0.26,0.04,0.13,0.01,0.031,49,,50,12,50,7.5,22,0.4,26,,96,,,,3,,8,,12,10,1,1,500,6,,10,51,1,0.01,
1993-04-29,12,485,18,452,8.4,155,149,144,274,86,8.4,,,0.65,0.58,0.02,0.05,0.01,0.123,46,,41,10,35,6.7,20,0.4,25,,72,,,,3,,16,,6,10,1,1,270,6,,20,23,1,0.01,
1993-06-16,15,967,16,392,8.4,163,166,146,247,60,7.5,,,0.55,0.48,0.02,0.05,0.01,0.123,22,,44,8.8,28,4.9,11,0.4,24,5,63,1,1,1,1,1,,1,3,3,1,1,,,3,22,,,0.01,
1993-08-25,19.5,26,2,486,8.4,187,191,158,289,72,8.6,,,0.55,0.48,0.02,0.05,0.01,0.031,37,,45,11,38,7,17,0.5,18,,92,,,,3,,6,,16,10,1,1,410,6,,20,39,1,0.01,
1993-10-26,8,36,1,595,8.6,210,171,188,349,62,11.4,,,0.25,0.18,0.02,0.05,0.01,0.031,63,,52,14,58,8.7,26,0.7,23,,110,,,,3,,6,,7,10,1,1,530,6,,10,53,1,0.01,
1994-02-09,,66,3,573,8.6,232,228,199,367,43,12.3,,,0.45,0.38,0.02,0.05,0.01,0.031,52,,58,13,50,7.9,22,0.6,26,,99,,,,3,,5,,22,10,1,1,480,6,,10,40,1,0.01,
1994-04-28,9.5,231,8,409,8.4,175,168,145,248,61,9.8,,,0.45,0.39,0.01,0.05,0.01,0.031,26,,43,9,31,5.8,13,0.4,19,,67,,,,3,,13,,8,10,1,1,290,6,,10,22,1,0.01,
1994-06-07,15,690,11,363,8.4,171,160,139,218,62,,,,0.55,0.48,0.02,0.05,0.01,0.092,16,,43,7.6,22,4.1,7.6,0.3,20,4,56,0.5,1,1,3,2,19,1,5,10,1,1,240,6,3,40,13,1,0.01,
1994-09-01,18.5,6.2,5,446,8.6,193,185,152,276,66,8.1,,,0.45,0.38,0.02,0.05,0.01,0.245,24,,41,12,36,8.1,16,0.5,24,,130,,,,3,,65,,160,10,1,1,410,6,,50,42,1,0.01,
1994-10-27,9,15,6,476,8.5,212,203,167,284,,10.8,,,,,0.02,0.05,0.01,0.031,26,,47,12,36,7.3,15,0.5,17,8,130,0.5,1,1,3,1,14,1,58,10,1,1,480,6,4,10,39,1,0.01,0.1
1994-12-21,,18,7,630,8.5,245,240,200,383,54,12.6,,,0.25,0.2,0.02,0.05,0.01,0.061,56,,57,14,55,9.9,23,0.7,27,,,,,,,,,,,,,,,,,,,,0.01,
1995-03-02,6,245,18,434,8.3,170,165,151,269,95,10.2,,,0.55,0.5,0.02,0.05,0.01,0.092,35,,44,10,34,6.1,16,0.4,24,,78,,,,3,,25,,13,10,1,1,310,6,,20,27,1,0.01,
1995-04-27,11.5,453,17,411,8.4,160,142,147,243,84,9.3,,,0.25,0.2,0.02,0.05,0.01,0.061,30,,44,9,32,5,14,0.5,23,,72,,,,4,,60,,5,10,1,1,260,7,,90,24,1,0.01,
1995-06-28,21,2180,32,370,8.1,161,154,135,217,65,6.5,,,0.65,0.6,0.02,0.05,0.01,0.153,15,,41,8,22,4.3,9.3,0.3,25,,,,,,,,,,,,,,,,,,,,0.01,
1995-08-22,22,80,2,390,8.6,165,156,141,238,73,9,,,0.35,0.3,0.02,0.05,0.01,0.031,26,,42,8.7,30,5.5,14,0.4,17,,71,,,,3,,11,,10,10,1,1,320,6,,20,21,1,0.01,
1995-10-27,7.5,74,6,481,8.3,200,174,165,294,,10.2,,,,,0.02,0.05,0.01,0.031,37,,46,12,38,6.5,16,0.5,21,7,88,0.5,1,1,3,1,13,1,14,10,1,1,400,6,3,20,31,1,0.01,0.1
1996-04-19,8,1100,35,370,8.5,147,142,117,224,,11.2,,,,,0.02,0.05,0.01,0.123,28,,34,7.8,27,4.4,13,0.4,24,4,60,0.5,1,1,3,12,58,1,4,10,2,1,210,6,8,70,17,1,0.01,0.1
1996-09-09,16,24,1,448,8.5,198,186,158,261,,9.2,,,,,0.02,0.06,0.01,0.031,24,,45,11,30,6.5,14,0.5,18,6,88,0.5,1,1,3,4,11,1,8,10,1,1,430,6,4,20,33,1,0.01,0.1
1996-11-22,,,12,,,210,,175,328,,,,,,,0.02,0.09,0.01,0.061,46,,50,12,43,7,20,0.6,22,7,90,0.5,1,1,3,8,24,1,6,10,1,1,410,6,3,40,30,1,0.01,0.1
1997-04-17,13,698,34,416,8.3,159,146,135,248,,8.9,,,,,0.02,0.05,0.01,0.123,32.5,,38.3,9.6,32.3,5.4,14.9,0.5,26,6,70.5,0.5,1,1,3,1.5,3,1,3.19,13.5,1,1,252,7.5,3,5,19.5,1,0.01,0.1
1997-08-25,22,140,2,313,8.7,134,131,101,182,,9.5,,,,,0.02,0.05,0.01,0.113,15.5,,30.7,5.97,20.8,4.3,9,0.3,16.6,6,47.4,0.5,1,1,3,1,4.3,1,8.53,10,1,1,225,6,8.1,5,18.1,1,0.01,0.1
1997-12-17,0.5,66,4,585,8.4,229,229,194,360,,12.2,,,,,0.02,0.05,0.01,0.055,50.1,,55.8,13.1,46.9,6.9,21,0.5,28.5,7,94.9,1,8,1,12,1,10,1,11.2,60,1,1,409,10,20,10,35.4,1,0.01,0.1
1998-04-07,7,864,44,413,8.8,159,162,141,262,,,,,,,0.02,0.067,0.01,0.159,34.7,,39,10.4,33,5.4,15.7,0.3,26.2,6,74.2,1,8,1,12,3.8,18.8,1,4.7,60,1,1,254,10,20,10,21,1,0.01,0.1
1998-08-27,18,62,2,532,8.5,212,200,170,314,,9.7,,,,,0.02,0.05,0.01,0.043,40.3,,47.8,12.2,44.2,7.7,19.2,0.6,21.6,10,97.4,1,8,1,12,1.9,10,1,9,60,1,1,401,10,20,10,37.2,1,0.01,0.1
2015-05-01,16.5,80,,390,8.7,,,,,,8.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2015-06-26,21.6,94,,422,8.2,,,,,,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,

From sarah.goslee at gmail.com  Mon Jul 20 21:08:58 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 20 Jul 2015 15:08:58 -0400
Subject: [R] jaccards index
In-Reply-To: <8FDC9F4E-09D6-4ADA-97BE-98A0D10C71E1@u.washington.edu>
References: <1437369783364-4710057.post@n4.nabble.com>
	<8FDC9F4E-09D6-4ADA-97BE-98A0D10C71E1@u.washington.edu>
Message-ID: <CAM_vjukXfR9MzZFp4+DX6MLHDxgGKAyrxPmTZ3RKrFzos0yJWA@mail.gmail.com>

On Mon, Jul 20, 2015 at 2:02 PM, Don McKenzie <dmck at u.washington.edu> wrote:
> Sarah Goslee?s package ?ecodist? will compute a Jaccard index, I believe.

Indeed it does, and for that matter so does the dist() function in
base R. But I couldn't see how that index could be used to normalize
data, being a dissimilarity on pairwise samples, so I left the
question alone hoping it made more sense to someone else on the list.

> You are unlikely to get much help, however, unless you provide more details as to what you are trying to accomplish.  See
>
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>
> for how to create a reproducible example, as requested in the PostingGuide.

Please do!

You might also try reading the documentation about data import and
boxplots. http://www.rseek.org might help you find the appropriate
docs.

Sarah


>> On Jul 19, 2015, at 10:23 PM, sreenath <sreenath.rajur at macfast.ac.in> wrote:
>>
>> hi..
>> I have a csv file containing 35 coloumns and 193 rows.i want to generate
>> jaccards index to normalise these data.how can i do this also from these
>> data i want to draw boxplot.plz help
>>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Mon Jul 20 21:11:01 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 20 Jul 2015 15:11:01 -0400
Subject: [R] Printing row and column names of cells with specific value
 in a big matrix
In-Reply-To: <CANGdRbHKxER3bOtYQEitL=XVpHfP7De3J5ymvvCaeF04P-iZFw@mail.gmail.com>
References: <CANGdRbHKxER3bOtYQEitL=XVpHfP7De3J5ymvvCaeF04P-iZFw@mail.gmail.com>
Message-ID: <CAM_vju=Es4Z_qm0AWRZM_v4-vqELps1SRaPo4QbGiWK91eRU1w@mail.gmail.com>

Without a reproducible example, or at least a non-mangled one (please
don't post in HTML), I'm not inclined to try it, but why not use
sig_values to index row.names() and col.names() if you're after the
names?

Sarah

On Mon, Jul 20, 2015 at 1:44 PM, gaurav kandoi <kandoigaurav at gmail.com> wrote:
> Hi All
>
> I've two big matrices (5k*4k) with the same structure, i.e. :
>
>   mRNA1 mRNA2 mRNA3  lncRNA1 0.395646 0.94995 0.76177  lncRNA2 0.03791
> 0.661258 0.558658  lncRNA3 0.67846 0.652364 0.359054  lncRNA4 0.57769 0.003
> 0.459127
> Now, I would like to extract the names of the row,col pairs whose value is
> less than 0.05. In this case, I should get the output as (lncRNA2,mRNA1)
> and (lncRNA4,mRNA2) alongwith their values (0.03791 and 0.003). Since the
> structure of both the matrix is same, I would also like to retrieve the
> corresponding values and row,col names from the second matrix.
> (lncRNA2,mRNA1 and lncRNA4,mRNA2 alongwith their values in the second
> matrix.)
>
> I'm using the following code:
>
> Pmatrix = read.table("pmatrix.csv", header=T, sep="," , row.names=1)
>> sig_values <- which(Pmatrix<0.05, arr.ind=TRUE)
>> sig_values
>> Corr_Matrix = read.csv("corr_matrix.csv", header = T, row.names=1)
>> Corr_Matrix[sig_values]
>
>
> However, it only prints the row,col number (sig_values command) or only the
> values (Corr_Matrix[sig_values]) command. How can I get the row and column
> names alongwith their values?
>
> Regards
>
> --
> *Gaurav Kandoi*
>
>         [[alternative HTML version deleted]]

-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Mon Jul 20 21:14:07 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 20 Jul 2015 15:14:07 -0400
Subject: [R] error reading file DESCRIPTION
In-Reply-To: <6BF38763-6545-4D6F-85ED-CEE2529BDE0B@wooster.edu>
References: <6BF38763-6545-4D6F-85ED-CEE2529BDE0B@wooster.edu>
Message-ID: <CAM_vjun21pAOhRS=afY7sQLY=xjR7tYa1Zg6c5H=HxrW+WdE-A@mail.gmail.com>

On Mon, Jul 20, 2015 at 1:42 PM, William Morgan <WMORGAN at wooster.edu> wrote:
> Hello,
>
> After downloading imview from SourceForge.net <http://sourceforge.net/projects/imview/?source=directory>,
> I get the following error and warning when trying to install this package:

That would be because it isn't an R package. That's the source code
for stand-alone image processing software.

>> install.packages("~/Downloads/imview-src-1.0.1.tar.gz", repos = NULL, type = "source")
> Error: error reading file '/var/folders/57/76qlq1g9607g8nm3pv7txydw0000gp/T//Rtmp9Xr791/R.INSTALL4b47344defc/imview-1.0.1/DESCRIPTION'
> Warning in install.packages :
>   installation of package ?/Users/wmorgan/Downloads/imview-src-1.0.1.tar.gz? had non-zero exit status
>
> Any suggestions on how to solve this problem?
>
> Thanks,
> Bill
>
> P.S. I?m using R Studio, on a Mac Airbook with OS 10.10.4.
>
>
> William R. Morgan, Ph.D.
> Theron L. Peterson and Dorothy R. Peterson Professor of Biology
> The College of Wooster
> Department of Biology
> 931 College Mall
> Wooster, OH 44691
> 330-263-2379
> wmorgan at wooster.edu
>
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From thierry.onkelinx at inbo.be  Mon Jul 20 21:19:58 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 20 Jul 2015 21:19:58 +0200
Subject: [R] ggplot2 geom_boxplot limits
In-Reply-To: <alpine.OSX.2.20.1507191849540.9451@sombscqb055040a.local>
References: <alpine.OSX.2.20.1507191849540.9451@sombscqb055040a.local>
Message-ID: <CAJuCY5znuktiWT3=bq2x0bjVdA8hTUAa4ExwT_5Ctf5KR-xP6g@mail.gmail.com>

Limits in scales set values outside the limits to NA. Hence the boxplots,
smoothers,... change. Use coord_cartesian() to "zoom in".

Op 20-jul.-2015 20:29 schreef "Jacob Wegelin" <jacobwegelin at fastmail.fm>:
>
> With base graphics, one can use the "ylim" argument to zoom in on a
boxplot.
>
> With ggplot2, using "limits" to try to zoom in on a boxplot *changes the
box*.  Since the box usually indicates the 25th and 75th percentiles of a
quantitative variable, this is puzzling.
>
> The toy code below demonstrates this. In ggplot2, "zooming in" causes the
two boxes to overlap, when they did not overlap in the full plot.  Also,
the center lines --- which usually indicate the median of the variable ---
change when one zooms in.
>
> In base graphics, "zooming in" does not cause the boxes to overlap or, as
far as I can see, the median line to move relative to the scale.
>
> What is going on here?
>
> pdf(file="toy-example.pdf")
> set.seed(1)
> toy1<-data.frame(Y=rnorm(500, mean=3), A="one")
> toy2<-data.frame(Y=rnorm(500, mean=1.6), A="two")
> toy<-rbind(toy1,toy2)
> toy$A<-factor(toy$A)
> library(ggplot2)
> mybreaks<-signif(seq(from=min(toy$Y),to=max(toy$Y),by=0.5),digits=2)
> mylimits<-c(0.61,3.7)
> print(myplot<-ggplot(toy, aes(x=A,y=Y)) +
geom_boxplot()+scale_y_continuous(breaks=mybreaks)+theme_bw())
> print(myplot+scale_y_continuous(breaks=mybreaks,limits=mylimits))
> boxplot(toy1$Y,toy2$Y)
> boxplot(toy1$Y,toy2$Y, ylim=mylimits)
> graphics.off()
>
>> sessionInfo()
>
> R version 3.2.1 (2015-06-18)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.4 (Yosemite)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] ggplot2_1.0.1
>
> loaded via a namespace (and not attached):
>  [1] MASS_7.3-40      colorspace_1.2-6 scales_0.2.5     magrittr_1.5
 plyr_1.8.3       tools_3.2.1      gtable_0.1.2     reshape2_1.4.1
>  [9] Rcpp_0.11.6      stringi_0.5-5    grid_3.2.1       stringr_1.0.0
digest_0.6.8     proto_0.3-10     munsell_0.4.2
>
>
> Jacob A. Wegelin
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Jul 20 21:27:10 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 20 Jul 2015 21:27:10 +0200
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <alpine.LNX.2.11.1507201145510.1876@localhost>
References: <alpine.LNX.2.11.1507201047240.1876@localhost>
	<CAJuCY5zjgVK9NkcdXaGd5Y=7gOBWYSQwBsdU1iPQvNSBYc28Dw@mail.gmail.com>
	<alpine.LNX.2.11.1507201145510.1876@localhost>
Message-ID: <CAJuCY5wiOPuV2n3MpHnLk4h=mmo=fod6unep=H=d3WdLQXJaKw@mail.gmail.com>

We need the source file. Not the output. And please try to make it as small
as possible while still reproducing the problem. The smaller the example,
the easier it is for us to help you.
Op 20-jul.-2015 21:17 schreef "Rich Shepard" <rshepard at appl-ecosys.com>:

> On Mon, 20 Jul 2015, Thierry Onkelinx wrote:
>
>  Have you tried echo = FALSE instead of echo = F. If that doesn't solve
>> your problem, please provide a minimal reproducible example.
>>
>
>   Yes, I have.
>
>   Attached is a TeX file renamed to sample.txt (rather than .tex to ensure
> it is not stripped), a PDF of the compiled page, and the source data that's
> being read in the sample doc.
>
> Thanks,
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kandoigaurav at gmail.com  Mon Jul 20 21:29:04 2015
From: kandoigaurav at gmail.com (gaurav kandoi)
Date: Mon, 20 Jul 2015 14:29:04 -0500
Subject: [R] Printing row and column names of cells with specific value
 in a big matrix
In-Reply-To: <CAM_vju=Es4Z_qm0AWRZM_v4-vqELps1SRaPo4QbGiWK91eRU1w@mail.gmail.com>
References: <CANGdRbHKxER3bOtYQEitL=XVpHfP7De3J5ymvvCaeF04P-iZFw@mail.gmail.com>
	<CAM_vju=Es4Z_qm0AWRZM_v4-vqELps1SRaPo4QbGiWK91eRU1w@mail.gmail.com>
Message-ID: <CANGdRbG0yd24pd0Frfj5EQ0Ykr7XSvXsB1eRLT0MKd3rUmEYoA@mail.gmail.com>

Hi Sarah, sorry for posting in HTML.

I've two big matrices (5k*4k) with the same structure, i.e. :

,mRNA1,mRNA2,mRNA3
lncRNA1,0.395646498,0.949950035,0.761770206
lncRNA2,0.037909944,0.661258022,0.558657799
lncRNA3,0.678459646,0.652364052,0.359053653

Now, I would like to extract the names of the row,col pairs whose
value is less than 0.05. In this case, I should get the output as
(lncRNA2,mRNA1) and (lncRNA4,mRNA2) alongwith their values (0.03791
and 0.003). Since the structure of both the matrix is same, I would
also like to retrieve the corresponding values and row,col names from
the second matrix. (lncRNA2,mRNA1 and lncRNA4,mRNA2 alongwith their
values in the second matrix.)

I'm using the following code:

> Pmatrix = read.table("pmatrix.csv", header=T, sep="," , row.names=1)
> sig_values <- which(Pmatrix<0.05, arr.ind=TRUE)
> sig_values
> Corr_Matrix = read.csv("corr_matrix.csv", header = T, row.names=1)
> Corr_Matrix[sig_values]

However, it only prints the row,col number (sig_values command) or
only the values (Corr_Matrix[sig_values]) command. How can I get the
row and column names alongwith their values?

I've also tried printing using the following print command:

>paste(rownames(Pmatrix)[sig_values[1]], colnames(Pmatrix)[sig_values[2]], sep=", ")

But it gives a output like this:

[1] "lncRNA2, NA"


Sample input files available for download: https://goo.gl/xR6XDg

Regards

On Mon, Jul 20, 2015 at 2:11 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Without a reproducible example, or at least a non-mangled one (please
> don't post in HTML), I'm not inclined to try it, but why not use
> sig_values to index row.names() and col.names() if you're after the
> names?
>
> Sarah
>
> On Mon, Jul 20, 2015 at 1:44 PM, gaurav kandoi <kandoigaurav at gmail.com> wrote:
>> Hi All
>>
>> I've two big matrices (5k*4k) with the same structure, i.e. :
>>
>>   mRNA1 mRNA2 mRNA3  lncRNA1 0.395646 0.94995 0.76177  lncRNA2 0.03791
>> 0.661258 0.558658  lncRNA3 0.67846 0.652364 0.359054  lncRNA4 0.57769 0.003
>> 0.459127
>> Now, I would like to extract the names of the row,col pairs whose value is
>> less than 0.05. In this case, I should get the output as (lncRNA2,mRNA1)
>> and (lncRNA4,mRNA2) alongwith their values (0.03791 and 0.003). Since the
>> structure of both the matrix is same, I would also like to retrieve the
>> corresponding values and row,col names from the second matrix.
>> (lncRNA2,mRNA1 and lncRNA4,mRNA2 alongwith their values in the second
>> matrix.)
>>
>> I'm using the following code:
>>
>> Pmatrix = read.table("pmatrix.csv", header=T, sep="," , row.names=1)
>>> sig_values <- which(Pmatrix<0.05, arr.ind=TRUE)
>>> sig_values
>>> Corr_Matrix = read.csv("corr_matrix.csv", header = T, row.names=1)
>>> Corr_Matrix[sig_values]
>>
>>
>> However, it only prints the row,col number (sig_values command) or only the
>> values (Corr_Matrix[sig_values]) command. How can I get the row and column
>> names alongwith their values?
>>
>> Regards
>>
>> --
>> *Gaurav Kandoi*
>>
>>         [[alternative HTML version deleted]]
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org



-- 
Gaurav Kandoi


From rshepard at appl-ecosys.com  Mon Jul 20 21:35:46 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Mon, 20 Jul 2015 12:35:46 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <CANROs4fLO_7rM9A0XhKE+ozMRv9hKyDkUgGVwAhYuctSxv6RVw@mail.gmail.com>
References: <alpine.LNX.2.11.1507201047240.1876@localhost>
	<CANROs4fLO_7rM9A0XhKE+ozMRv9hKyDkUgGVwAhYuctSxv6RVw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507201228420.1876@localhost>

On Mon, 20 Jul 2015, Yihui Xie wrote:

> Section 5.1.3 of the book "Dynamic Documents with R and knitr" is
> titled "Global Options". I don't know how to make it more clear for
> readers to find information on global options in the book.

   Yes, I've read that and have not found where 'opts_chunk$set(echo =
FALSE)' should be inserted in the document.

   If I put it in the first chunk, outside the options box, pdflatex throws
an error. If it is in it's own chunk, either by itself or in the options
box, there's the same verbose error when trying to compile it. When I put it
in the body of the document it's printed as is.

   Where should that command be entered in the document?

Rich


From sarah.goslee at gmail.com  Mon Jul 20 21:55:24 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 20 Jul 2015 15:55:24 -0400
Subject: [R] Printing row and column names of cells with specific value
 in a big matrix
In-Reply-To: <CANGdRbG0yd24pd0Frfj5EQ0Ykr7XSvXsB1eRLT0MKd3rUmEYoA@mail.gmail.com>
References: <CANGdRbHKxER3bOtYQEitL=XVpHfP7De3J5ymvvCaeF04P-iZFw@mail.gmail.com>
	<CAM_vju=Es4Z_qm0AWRZM_v4-vqELps1SRaPo4QbGiWK91eRU1w@mail.gmail.com>
	<CANGdRbG0yd24pd0Frfj5EQ0Ykr7XSvXsB1eRLT0MKd3rUmEYoA@mail.gmail.com>
Message-ID: <CAM_vju=MoRn-FXyzMFo9OwPKc4Dh6Svy_e2Jf+Ksi50XjeDhPg@mail.gmail.com>

Subsetting error. See below.

On Mon, Jul 20, 2015 at 3:29 PM, gaurav kandoi <kandoigaurav at gmail.com> wrote:
> Hi Sarah, sorry for posting in HTML.
>
> I've two big matrices (5k*4k) with the same structure, i.e. :
>
> ,mRNA1,mRNA2,mRNA3
> lncRNA1,0.395646498,0.949950035,0.761770206
> lncRNA2,0.037909944,0.661258022,0.558657799
> lncRNA3,0.678459646,0.652364052,0.359053653
>
> Now, I would like to extract the names of the row,col pairs whose
> value is less than 0.05. In this case, I should get the output as
> (lncRNA2,mRNA1) and (lncRNA4,mRNA2) alongwith their values (0.03791
> and 0.003). Since the structure of both the matrix is same, I would
> also like to retrieve the corresponding values and row,col names from
> the second matrix. (lncRNA2,mRNA1 and lncRNA4,mRNA2 alongwith their
> values in the second matrix.)
>
> I'm using the following code:
>
>> Pmatrix = read.table("pmatrix.csv", header=T, sep="," , row.names=1)
>> sig_values <- which(Pmatrix<0.05, arr.ind=TRUE)
>> sig_values
>> Corr_Matrix = read.csv("corr_matrix.csv", header = T, row.names=1)
>> Corr_Matrix[sig_values]
>
> However, it only prints the row,col number (sig_values command) or
> only the values (Corr_Matrix[sig_values]) command. How can I get the
> row and column names alongwith their values?
>
> I've also tried printing using the following print command:
>
>>paste(rownames(Pmatrix)[sig_values[1]], colnames(Pmatrix)[sig_values[2]], sep=", ")

> But it gives a output like this:
>
> [1] "lncRNA2, NA"

Well, yes.

sig_values[1]

> sig_values[1]
[1] 2
> sig_values[2]
[1] 8

And there is no column 8, so no name.

paste(rownames(Pmatrix)[sig_values[,1]],
colnames(Pmatrix)[sig_values[,2]], sep=", ")
[1] "lncRNA2, mRNA1" "lncRNA8, mRNA1" "lncRNA4, mRNA2" "lncRNA7,
mRNA2" "lncRNA1, mRNA4"
[6] "lncRNA3, mRNA4" "lncRNA5, mRNA5"

> Sample input files available for download: https://goo.gl/xR6XDg

dput() is preferred to expecting people to download things from unknown sources.

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


From rshepard at appl-ecosys.com  Mon Jul 20 21:58:51 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Mon, 20 Jul 2015 12:58:51 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <CAJuCY5wiOPuV2n3MpHnLk4h=mmo=fod6unep=H=d3WdLQXJaKw@mail.gmail.com>
References: <alpine.LNX.2.11.1507201047240.1876@localhost>
	<CAJuCY5zjgVK9NkcdXaGd5Y=7gOBWYSQwBsdU1iPQvNSBYc28Dw@mail.gmail.com>
	<alpine.LNX.2.11.1507201145510.1876@localhost>
	<CAJuCY5wiOPuV2n3MpHnLk4h=mmo=fod6unep=H=d3WdLQXJaKw@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1507201255450.1876@localhost>

On Mon, 20 Jul 2015, Thierry Onkelinx wrote:

> We need the source file.

   Attached to the original message was the TeX output called, 'sample.txt'.
I've attached it again, but with the .tex extension. Also, the .lyx file is
attached.

Rich
-------------- next part --------------
#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrreprt
\begin_preamble
\date{}
\usepackage{textcomp,url,multicol}
%\setkomafont{sectioning}{\rmfamily}
\end_preamble
\options abstract=on
\use_default_options false
\begin_modules
natbibapa
knitr
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style humannat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
In 1986 Ward described the status of water quality assessment as "data rich
 and information poor" 
\begin_inset CommandInset citation
LatexCommand citep
key "Ward1986"

\end_inset

.
 The authors addressed the lack of science-based design of monitoring location
 networks; this situation still applies.
 They defined water quality monitoring as "any effort by a government or
 private enterprise to obtain an understanding of the physical, chemical,
 and biological characteristics of water via statistical sampling." Of course,
 their definition also applies to appropriate statistical analyses of the
 collected data.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status open

\begin_layout Plain Layout

\begin_inset Argument 1
status open

\begin_layout Plain Layout
opts_chunk$set(echo = FALSE)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

load('.RData')
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
tidy=T, tidy.opts=list(width.cutoff=60)
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The data available for the Carlin site (USGS station number 10321000 are
 in Appendix A.
 Many documents on data analysis and statistics use small sample sets to
 illustrate the points the author wants the reader to learn.
 Real-world environmental data sets frequently are very large so this document
 uses 52 variables from the total available from the USGS's web site.
\end_layout

\begin_layout Standard
The first step in analyzing water chemistry data for CWA compliance is reading
 it into the analytical software and converting column data types as necessary.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

carlin <- read.csv("./carlin.csv", header=T, sep=",", stringsAsFactors=F)
\end_layout

\begin_layout Plain Layout

carlin$sampdate <- as.Date(carlin$sampdate)
\begin_inset Argument 1
status open

\begin_layout Plain Layout
'input_data'
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Next, check that the data are what you expect to see and convert dates from
 factors.
 The site ID number is retained for use when examining multiple stations
 along the Humboldt River for longitudinal changes and other variables that
 might affect the measured values.
\end_layout

\begin_layout Standard
\begin_inset Flex Chunk
status collapsed

\begin_layout Plain Layout

\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout
'data_set_structure'
\end_layout

\end_inset

str(carlin, width=60, strict.width='cut')
\end_layout

\end_inset


\end_layout

\end_body
\end_document

From kandoigaurav at gmail.com  Mon Jul 20 22:13:26 2015
From: kandoigaurav at gmail.com (gaurav kandoi)
Date: Mon, 20 Jul 2015 15:13:26 -0500
Subject: [R] Printing row and column names of cells with specific value
 in a big matrix
In-Reply-To: <CAM_vju=MoRn-FXyzMFo9OwPKc4Dh6Svy_e2Jf+Ksi50XjeDhPg@mail.gmail.com>
References: <CANGdRbHKxER3bOtYQEitL=XVpHfP7De3J5ymvvCaeF04P-iZFw@mail.gmail.com>
	<CAM_vju=Es4Z_qm0AWRZM_v4-vqELps1SRaPo4QbGiWK91eRU1w@mail.gmail.com>
	<CANGdRbG0yd24pd0Frfj5EQ0Ykr7XSvXsB1eRLT0MKd3rUmEYoA@mail.gmail.com>
	<CAM_vju=MoRn-FXyzMFo9OwPKc4Dh6Svy_e2Jf+Ksi50XjeDhPg@mail.gmail.com>
Message-ID: <CANGdRbEOG=4V-Uuu6JS9k3YaqEDtA1dqZKpNDtG7REbCVW9rLg@mail.gmail.com>

Thanks a lot Sarah. I think I've got what I wanted.

On Mon, Jul 20, 2015 at 2:55 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Subsetting error. See below.
>
> On Mon, Jul 20, 2015 at 3:29 PM, gaurav kandoi <kandoigaurav at gmail.com> wrote:
>> Hi Sarah, sorry for posting in HTML.
>>
>> I've two big matrices (5k*4k) with the same structure, i.e. :
>>
>> ,mRNA1,mRNA2,mRNA3
>> lncRNA1,0.395646498,0.949950035,0.761770206
>> lncRNA2,0.037909944,0.661258022,0.558657799
>> lncRNA3,0.678459646,0.652364052,0.359053653
>>
>> Now, I would like to extract the names of the row,col pairs whose
>> value is less than 0.05. In this case, I should get the output as
>> (lncRNA2,mRNA1) and (lncRNA4,mRNA2) alongwith their values (0.03791
>> and 0.003). Since the structure of both the matrix is same, I would
>> also like to retrieve the corresponding values and row,col names from
>> the second matrix. (lncRNA2,mRNA1 and lncRNA4,mRNA2 alongwith their
>> values in the second matrix.)
>>
>> I'm using the following code:
>>
>>> Pmatrix = read.table("pmatrix.csv", header=T, sep="," , row.names=1)
>>> sig_values <- which(Pmatrix<0.05, arr.ind=TRUE)
>>> sig_values
>>> Corr_Matrix = read.csv("corr_matrix.csv", header = T, row.names=1)
>>> Corr_Matrix[sig_values]
>>
>> However, it only prints the row,col number (sig_values command) or
>> only the values (Corr_Matrix[sig_values]) command. How can I get the
>> row and column names alongwith their values?
>>
>> I've also tried printing using the following print command:
>>
>>>paste(rownames(Pmatrix)[sig_values[1]], colnames(Pmatrix)[sig_values[2]], sep=", ")
>
>> But it gives a output like this:
>>
>> [1] "lncRNA2, NA"
>
> Well, yes.
>
> sig_values[1]
>
>> sig_values[1]
> [1] 2
>> sig_values[2]
> [1] 8
>
> And there is no column 8, so no name.
>
> paste(rownames(Pmatrix)[sig_values[,1]],
> colnames(Pmatrix)[sig_values[,2]], sep=", ")
> [1] "lncRNA2, mRNA1" "lncRNA8, mRNA1" "lncRNA4, mRNA2" "lncRNA7,
> mRNA2" "lncRNA1, mRNA4"
> [6] "lncRNA3, mRNA4" "lncRNA5, mRNA5"
>
>> Sample input files available for download: https://goo.gl/xR6XDg
>
> dput() is preferred to expecting people to download things from unknown sources.
>
> Sarah
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org



-- 
Gaurav Kandoi


From dwinsemius at comcast.net  Mon Jul 20 22:16:56 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 20 Jul 2015 13:16:56 -0700
Subject: [R] Value passing in print option
In-Reply-To: <CADcgpJdw+iYF0-Dccp+bjTtGSOaQ7EJ0xYm7mHV5UVN8=MFKsA@mail.gmail.com>
References: <CADcgpJdw+iYF0-Dccp+bjTtGSOaQ7EJ0xYm7mHV5UVN8=MFKsA@mail.gmail.com>
Message-ID: <9DAAF670-549E-4B6C-8A3A-DFEA656D6371@comcast.net>


On Jul 19, 2015, at 11:48 PM, Partha Sinha wrote:

> i want to pass a value with print option
> x<-10
> y<2*x
> print("Current value of y is " ) # confused dont know how to pass value
> 
> i want output as Current value of y is 10

With that code we really do not have any assurance that y is 10 (or anything for that matter), and if you meant to type y <- 2*x it would be 20, not 10.

The sprintf function is often used:

 x<-10
y <- 2*x
 sprintf("Current value of y is %d", y)
[1] "Current value of y is 20"


> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jacobwegelin at fastmail.fm  Mon Jul 20 22:19:46 2015
From: jacobwegelin at fastmail.fm (Jacob Wegelin)
Date: Mon, 20 Jul 2015 16:19:46 -0400
Subject: [R] ggplot2 geom_boxplot limits
In-Reply-To: <CAJuCY5znuktiWT3=bq2x0bjVdA8hTUAa4ExwT_5Ctf5KR-xP6g@mail.gmail.com>
References: <alpine.OSX.2.20.1507191849540.9451@sombscqb055040a.local>
	<CAJuCY5znuktiWT3=bq2x0bjVdA8hTUAa4ExwT_5Ctf5KR-xP6g@mail.gmail.com>
Message-ID: <alpine.OSX.2.20.1507201616340.23854@qqt.local>

On 2015-07-20 Mon 15:19, Thierry Onkelinx wrote:
> 
> Limits in scales set values outside the limits to NA. Hence the boxplots, smoothers,... change. Use coord_cartesian() to "zoom in".

Thanks. What do I do if I also want to use coord_flip(), that is, if I want the boxes to lie horizontally *and* to zoom in?

myplot+coord_cartesian(ylim=mylimits) # zooms in

myplot+coord_cartesian(ylim=mylimits) + coord_flip() # flips but does not zoom

myplot + coord_flip()+coord_cartesian(ylim=mylimits) # zooms but does not flip

Jacob Wegelin

> Op 20-jul.-2015 20:29 schreef "Jacob Wegelin" <jacobwegelin at fastmail.fm>:
> >
> > With base graphics, one can use the "ylim" argument to zoom in on a boxplot.
> >
> > With ggplot2, using "limits" to try to zoom in on a boxplot *changes the box*.? Since the box usually indicates the 25th and 75th percentiles of a
> quantitative variable, this is puzzling.
> >
> > The toy code below demonstrates this. In ggplot2, "zooming in" causes the two boxes to overlap, when they did not overlap in the full plot.? Also, the
> center lines --- which usually indicate the median of the variable --- change when one zooms in.
> >
> > In base graphics, "zooming in" does not cause the boxes to overlap or, as far as I can see, the median line to move relative to the scale.
> >
> > What is going on here?
> >
> > pdf(file="toy-example.pdf")
> > set.seed(1)
> > toy1<-data.frame(Y=rnorm(500, mean=3), A="one")
> > toy2<-data.frame(Y=rnorm(500, mean=1.6), A="two")
> > toy<-rbind(toy1,toy2)
> > toy$A<-factor(toy$A)
> > library(ggplot2)
> > mybreaks<-signif(seq(from=min(toy$Y),to=max(toy$Y),by=0.5),digits=2)
> > mylimits<-c(0.61,3.7)
> > print(myplot<-ggplot(toy, aes(x=A,y=Y)) + geom_boxplot()+scale_y_continuous(breaks=mybreaks)+theme_bw())
> > print(myplot+scale_y_continuous(breaks=mybreaks,limits=mylimits))
> > boxplot(toy1$Y,toy2$Y)
> > boxplot(toy1$Y,toy2$Y, ylim=mylimits)
> > graphics.off()
> >
> >> sessionInfo()
> >
> > R version 3.2.1 (2015-06-18)
> > Platform: x86_64-apple-darwin13.4.0 (64-bit)
> > Running under: OS X 10.10.4 (Yosemite)
> >
> > locale:
> > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> >
> > attached base packages:
> > [1] stats? ? ?graphics? grDevices utils? ? ?datasets? methods? ?base
> >
> > other attached packages:
> > [1] ggplot2_1.0.1
> >
> > loaded via a namespace (and not attached):
> > ?[1] MASS_7.3-40? ? ? colorspace_1.2-6 scales_0.2.5? ? ?magrittr_1.5? ? ?plyr_1.8.3? ? ? ?tools_3.2.1? ? ? gtable_0.1.2? ? ?reshape2_1.4.1
> > ?[9] Rcpp_0.11.6? ? ? stringi_0.5-5? ? grid_3.2.1? ? ? ?stringr_1.0.0? ? digest_0.6.8? ? ?proto_0.3-10? ? ?munsell_0.4.2
> >
> >
> > Jacob A. Wegelin
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
>

From wht_crl at yahoo.com  Mon Jul 20 22:24:41 2015
From: wht_crl at yahoo.com (carol white)
Date: Mon, 20 Jul 2015 20:24:41 +0000 (UTC)
Subject: [R] change text size on a graphics
Message-ID: <166669763.1146304.1437423881249.JavaMail.yahoo@mail.yahoo.com>

Hi,How is it possible to increase the size of a histogram labels (displayed on the top of the bars)? I thought that if I use cex > 1, it will increase all text size on a plot (axis labels, axis annotation, title of the graphics and histogram labels) which I want but it doesn't. 

Regards,
Carol

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Mon Jul 20 22:51:14 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Mon, 20 Jul 2015 22:51:14 +0200
Subject: [R] ggplot2 geom_boxplot limits
In-Reply-To: <alpine.OSX.2.20.1507201616340.23854@qqt.local>
References: <alpine.OSX.2.20.1507191849540.9451@sombscqb055040a.local>
	<CAJuCY5znuktiWT3=bq2x0bjVdA8hTUAa4ExwT_5Ctf5KR-xP6g@mail.gmail.com>
	<alpine.OSX.2.20.1507201616340.23854@qqt.local>
Message-ID: <CAJuCY5y1wb=OhzaA1_esCcTAq8fxvQ5C3nNPVzpEu1SWVqYijQ@mail.gmail.com>

Here is the answer: http://rpubs.com/INBOstats/zoom_in

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-20 22:19 GMT+02:00 Jacob Wegelin <jacobwegelin at fastmail.fm>:

> On 2015-07-20 Mon 15:19, Thierry Onkelinx wrote:
>
>>
>> Limits in scales set values outside the limits to NA. Hence the boxplots,
>> smoothers,... change. Use coord_cartesian() to "zoom in".
>>
>
> Thanks. What do I do if I also want to use coord_flip(), that is, if I
> want the boxes to lie horizontally *and* to zoom in?
>
> myplot+coord_cartesian(ylim=mylimits) # zooms in
>
> myplot+coord_cartesian(ylim=mylimits) + coord_flip() # flips but does not
> zoom
>
> myplot + coord_flip()+coord_cartesian(ylim=mylimits) # zooms but does not
> flip
>
> Jacob Wegelin
>
>
>  Op 20-jul.-2015 20:29 schreef "Jacob Wegelin" <jacobwegelin at fastmail.fm>:
>> >
>> > With base graphics, one can use the "ylim" argument to zoom in on a
>> boxplot.
>> >
>> > With ggplot2, using "limits" to try to zoom in on a boxplot *changes
>> the box*.  Since the box usually indicates the 25th and 75th percentiles of
>> a
>> quantitative variable, this is puzzling.
>> >
>> > The toy code below demonstrates this. In ggplot2, "zooming in" causes
>> the two boxes to overlap, when they did not overlap in the full plot.
>> Also, the
>> center lines --- which usually indicate the median of the variable ---
>> change when one zooms in.
>> >
>> > In base graphics, "zooming in" does not cause the boxes to overlap or,
>> as far as I can see, the median line to move relative to the scale.
>> >
>> > What is going on here?
>> >
>> > pdf(file="toy-example.pdf")
>> > set.seed(1)
>> > toy1<-data.frame(Y=rnorm(500, mean=3), A="one")
>> > toy2<-data.frame(Y=rnorm(500, mean=1.6), A="two")
>> > toy<-rbind(toy1,toy2)
>> > toy$A<-factor(toy$A)
>> > library(ggplot2)
>> > mybreaks<-signif(seq(from=min(toy$Y),to=max(toy$Y),by=0.5),digits=2)
>> > mylimits<-c(0.61,3.7)
>> > print(myplot<-ggplot(toy, aes(x=A,y=Y)) +
>> geom_boxplot()+scale_y_continuous(breaks=mybreaks)+theme_bw())
>> > print(myplot+scale_y_continuous(breaks=mybreaks,limits=mylimits))
>> > boxplot(toy1$Y,toy2$Y)
>> > boxplot(toy1$Y,toy2$Y, ylim=mylimits)
>> > graphics.off()
>> >
>> >> sessionInfo()
>> >
>> > R version 3.2.1 (2015-06-18)
>> > Platform: x86_64-apple-darwin13.4.0 (64-bit)
>> > Running under: OS X 10.10.4 (Yosemite)
>> >
>> > locale:
>> > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>> >
>> > attached base packages:
>> > [1] stats     graphics  grDevices utils     datasets  methods   base
>> >
>> > other attached packages:
>> > [1] ggplot2_1.0.1
>> >
>> > loaded via a namespace (and not attached):
>> >  [1] MASS_7.3-40      colorspace_1.2-6 scales_0.2.5     magrittr_1.5
>>  plyr_1.8.3       tools_3.2.1      gtable_0.1.2     reshape2_1.4.1
>> >  [9] Rcpp_0.11.6      stringi_0.5-5    grid_3.2.1       stringr_1.0.0
>>   digest_0.6.8     proto_0.3-10     munsell_0.4.2
>> >
>> >
>> > Jacob A. Wegelin
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Jul 20 23:02:37 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 20 Jul 2015 14:02:37 -0700
Subject: [R] change text size on a graphics
In-Reply-To: <166669763.1146304.1437423881249.JavaMail.yahoo@mail.yahoo.com>
References: <166669763.1146304.1437423881249.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <A06CE8AE-A76E-4F54-BE17-3E9E828C973C@comcast.net>


On Jul 20, 2015, at 1:24 PM, carol white via R-help wrote:

> Hi,How is it possible to increase the size of a histogram labels (displayed on the top of the bars)? I thought that if I use cex > 1, it will increase all text size on a plot (axis labels, axis annotation, title of the graphics and histogram labels) which I want but it doesn't. 
> 

Need to know which function you are using to determine where your thinking (or failure to read the documentation) has gone wrong.


> Regards,
> Carol
> 
> 	[[alternative HTML version deleted]]

You have been advised to stop using HTML in the past. What sort of incentive is needed?


> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From r.turner at auckland.ac.nz  Mon Jul 20 23:23:44 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 21 Jul 2015 09:23:44 +1200
Subject: [R] [FORGED]  change text size on a graphics
In-Reply-To: <166669763.1146304.1437423881249.JavaMail.yahoo@mail.yahoo.com>
References: <166669763.1146304.1437423881249.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <55AD66E0.90801@auckland.ac.nz>

On 21/07/15 08:24, carol white via R-help wrote:
> Hi,How is it possible to increase the size of a histogram labels
> (displayed on the top of the bars)? I thought that if I use cex > 1,
> it will increase all text size on a plot (axis labels, axis
> annotation, title of the graphics and histogram labels) which I want
> but it doesn't.

***What*** labels "displayed on the top of the bars"???  I don't see any 
such labels when I plot a histogram.

Reproducible example?

And please don't post in HTML.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From jfox at mcmaster.ca  Tue Jul 21 00:31:58 2015
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 20 Jul 2015 18:31:58 -0400
Subject: [R] R GUI tklistbox get value
In-Reply-To: <1437388147621-4710064.post@n4.nabble.com>
References: <1437388147621-4710064.post@n4.nabble.com>
Message-ID: <web-565940781@cgpsrv2.cis.mcmaster.ca>

Dear j.para.fernandez,

Try

    selecvar <- dat[, as.numeric(tkcurselection(tl))+1]

Omitting the comma returns a one-column data frame, not a numeric vector.

I hope this helps,
 John

------------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/

On Mon, 20 Jul 2015 03:29:07 -0700 (PDT)
 jpara3 <j.para.fernandez at hotmail.com> wrote:
> Hi, i have a dataframe, dat, with 2 variables, one and two.
> 
> I want to print in R the mean of the selected variable of the dataframe. You
> can select it with a tklistbox, but when you click OK button, the mean is
> not displayed, just NA
> 
> 
> ########
> 
> 
> one<-c(5,5,6,9,5,8)
> two<-c(12,13,14,12,14,12)
> dat<-data.frame(uno,dos)
> 
> require(tcltk)
> tt<-tktoplevel()
> tl<-tklistbox(tt,height=4,selectmode="single")
> tkgrid(tklabel(tt,text="Selecciona la variable para calcular media"))
> tkgrid(tl)
> for (i in (1:4))
> {
>     tkinsert(tl,"end",colnames(dat[i]))
> }
>  
> OnOK <- function()
> {
> 
>   selecvar <- dat[as.numeric(tkcurselection(tl))+1]
>    
>   print(mean(selecvar))
> 
> }
> OK.but <-tkbutton(tt,text="   OK   ",command=OnOK)
> tkgrid(OK.but)
> tkfocus(tt)
> 
> #################
> 
> Can someone please help me?? Thanks!!! 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/R-GUI-tklistbox-get-value-tp4710064.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marongiu.luigi at gmail.com  Tue Jul 21 00:38:31 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Mon, 20 Jul 2015 23:38:31 +0100
Subject: [R] plot auto key and text into panels using lattice
Message-ID: <CAMk+s2QZyYxojd7C=81sXGjM0kStk7taG0uniXT+yEMF8kx+0w@mail.gmail.com>

Dear all,
I am writing some text into several panels which I can do with this
script (in capital the variables):
xyplot(Y ~ X | Z,
               data = DATAFRAME,
               groups = Z,
               ylab= "Y",
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               as.table = TRUE,
               layout = LAYOUT,
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               type = "l",
               col = 3,
               panel = function(x, y,...) {
                   panel.xyplot(x,y,...)
                   panel.text(MIN.X+(0.1*MAX.X),
                              MAX.Y-(0.1*MAX.Y),
                              labels=LABELS[panel.number()],
                              cex=0.3
                              )
               }
        )

A similar plot also add the autokey because it takes in account two
different Y values, but the plot is not drawn rather the function is
implemented but the plot remains empty:
xyplot(Y1+Y2 ~ X | Z,
               data = DATAFRAME,
               ylab= "Y",
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               as.table = TRUE,
               layout = LAYOUT,
               auto.key= list(space = "centre"),
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               type = "l",
               col =  c(4,2),
               key = list(
                   space="top", columns=2,
                   text=list(TEXT_FOR_AUTOKEY, col="black"),
                   lines=list(col=c(4,2)),
                   panel = panel.superpose
                    ),
               panel = function(x, y,...)
               {
                   panel.xyplot(x,y,...)
                   panel.text(MIN.X+(0.1*MAX.X),
                              MAX.Y-(0.1*MAX.Y),
                              labels=LAB[panel.number()],
                              cex=0.3,
                              panel = panel.superpose
                              )

               }
        )


I am not attaching actual data because I believe the problem is in the
actual call, maybe I am using twice panel.superimpose, although
several combination I made (for instance moving the key argument into
the panel function) did not solve the problem.
Could you tell me where I am getting it wrong?
Thank you.
best regards
Luigi


From cdetermanjr at gmail.com  Tue Jul 21 01:54:53 2015
From: cdetermanjr at gmail.com (Charles Determan)
Date: Mon, 20 Jul 2015 18:54:53 -0500
Subject: [R] modifying a package installed via GitHub
In-Reply-To: <CACH0XT_nh9HAkaFXN-5V+Ju8tyS9yQGhWZL4167HsEG8NH_Tkw@mail.gmail.com>
References: <1437178330378-4710016.post@n4.nabble.com>
	<CAJ4QxaOOqCrC=BTSmE5pRVtX1Pgj=gj6GxtuA9i2c8M7UXQZxQ@mail.gmail.com>
	<CAKxd1KPv_Q4jZpvkGuNrWv1MK=rTnPj7Ro1t5sxs=uk6RDaEsg@mail.gmail.com>
	<CACH0XT_nh9HAkaFXN-5V+Ju8tyS9yQGhWZL4167HsEG8NH_Tkw@mail.gmail.com>
Message-ID: <CAKxd1KNY66RhZC-rdUhagBTfDRuOwgtWqVwH+p5Wefrf1gxPAw@mail.gmail.com>

You essentially have it but you can just click the 'build and install'
button to rebuild on the changes you made. But technically it would still
work pushing to your repo and using devtools.


On Monday, July 20, 2015, Stevan Earl <searl at vt.edu> wrote:

> Bob and Charles,
>
> Thanks very much for taking the time to write, I greatly appreciate your
> help. I have been so spoiled by Rstudio for so long that I cannot recall
> the last time I had to use R CMD install. Although I installed this package
> from GitHub using devtools, I do not see that an .Rproj exists, and the R
> code is in the .rdb and .rdx formats.
>
> However, if I understand Charles correctly, one approach would be to (1)
> fork the repo, (2) clone it, (3) make my edits, (4) push the edits to my
> fork of the repo, then (5) (re)install the package from my forked repo
> (e.g., install_github("myreponame/packagename"))...then I should be able
> to call all the functions with my edits. If I wanted to go back to the
> original, published version of the package, then I can just reinstall from
> the source (e.g.,install_github("author/packagename"), and that will
> overwrite what I have done locally. Do I have that right?
>
> Thanks again for your thoughtful advice!
>
>
> Steve
>
> On Mon, Jul 20, 2015 at 5:52 AM, Charles Determan <cdetermanjr at gmail.com
> <javascript:_e(%7B%7D,'cvml','cdetermanjr at gmail.com');>> wrote:
>
>> Steve,
>>
>> You are able to work with a github package the same as any github repo.
>> If you clone the repo:
>>
>> git clone https://github.com/user/repo.git
>>
>> If using RStudio it is simple enough to create a new project in that new
>> directory (if the .Rproj file does not exist, otherwise open that).  Once
>> you have the project open for that directory you can modify source files
>> and rebuild and install as you like.  If at the CMD line, you do as Bob
>> instructed with R CMD install .
>>
>> I recommend, however, either creating a new branch for you changes (if
>> you familiar with git management) or at least make sure to change the
>> subversion of the package so it doesn't conflict with the 'original'.  That
>> way you 'know' which version of the package is installed at a given time.
>>
>> Naturally, if you feel your modifications are valuable you may want to
>> actually fork the package on github and create a pull request of your
>> changes for the maintainer to incorporate in to the next release.
>>
>> Hope this helps clarify things,
>>
>> Charles
>>
>>
>>
>> On Sat, Jul 18, 2015 at 8:49 AM, boB Rudis <bob at rudis.net
>> <javascript:_e(%7B%7D,'cvml','bob at rudis.net');>> wrote:
>>
>>> You can go to the package directory:
>>>
>>>     cd /some/path/to/package
>>>
>>> and do
>>>
>>>     R CMD install .
>>>
>>> from a command-line there.
>>>
>>> Many github-based packages are also made using RStudio and you can
>>> just open the .Rproj file (i.e. load it into R studio) and build the
>>> package there which will install it.
>>>
>>> The same-named package will overwrite what you have previously installed.
>>>
>>> Just:
>>>
>>>    devtools::install_github("owner/package")
>>>
>>> to go back to the original.
>>>
>>> On Fri, Jul 17, 2015 at 8:12 PM, Steve E. <searl at vt.edu
>>> <javascript:_e(%7B%7D,'cvml','searl at vt.edu');>> wrote:
>>> > Hi Folks,
>>> >
>>> > I am working with a package installed via GitHub that I would like to
>>> > modify. However, I am not sure how I would go about loading a 'local'
>>> > version of the package after I have modified it, and whether that
>>> process
>>> > would including uninstalling the original unmodified package (and,
>>> > conversely, how to uninstall my local, modified version if I wanted to
>>> go
>>> > back to the unmodified version available on GitHub).
>>> >
>>> > Any advice would be appreciated.
>>> >
>>> >
>>> > Thanks,
>>> > Steve
>>> >
>>> >
>>> >
>>> > --
>>> > View this message in context:
>>> http://r.789695.n4.nabble.com/modifying-a-package-installed-via-GitHub-tp4710016.html
>>> > Sent from the R help mailing list archive at Nabble.com.
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org
>>> <javascript:_e(%7B%7D,'cvml','R-help at r-project.org');> mailing list --
>>> To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org
>>> <javascript:_e(%7B%7D,'cvml','R-help at r-project.org');> mailing list --
>>> To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From searl at vt.edu  Mon Jul 20 23:05:01 2015
From: searl at vt.edu (Stevan Earl)
Date: Mon, 20 Jul 2015 14:05:01 -0700
Subject: [R] modifying a package installed via GitHub
In-Reply-To: <CAKxd1KPv_Q4jZpvkGuNrWv1MK=rTnPj7Ro1t5sxs=uk6RDaEsg@mail.gmail.com>
References: <1437178330378-4710016.post@n4.nabble.com>
	<CAJ4QxaOOqCrC=BTSmE5pRVtX1Pgj=gj6GxtuA9i2c8M7UXQZxQ@mail.gmail.com>
	<CAKxd1KPv_Q4jZpvkGuNrWv1MK=rTnPj7Ro1t5sxs=uk6RDaEsg@mail.gmail.com>
Message-ID: <CACH0XT_nh9HAkaFXN-5V+Ju8tyS9yQGhWZL4167HsEG8NH_Tkw@mail.gmail.com>

Bob and Charles,

Thanks very much for taking the time to write, I greatly appreciate your
help. I have been so spoiled by Rstudio for so long that I cannot recall
the last time I had to use R CMD install. Although I installed this package
from GitHub using devtools, I do not see that an .Rproj exists, and the R
code is in the .rdb and .rdx formats.

However, if I understand Charles correctly, one approach would be to (1)
fork the repo, (2) clone it, (3) make my edits, (4) push the edits to my
fork of the repo, then (5) (re)install the package from my forked repo
(e.g., install_github("myreponame/packagename"))...then I should be able to
call all the functions with my edits. If I wanted to go back to the
original, published version of the package, then I can just reinstall from
the source (e.g.,install_github("author/packagename"), and that will
overwrite what I have done locally. Do I have that right?

Thanks again for your thoughtful advice!


Steve

On Mon, Jul 20, 2015 at 5:52 AM, Charles Determan <cdetermanjr at gmail.com>
wrote:

> Steve,
>
> You are able to work with a github package the same as any github repo.
> If you clone the repo:
>
> git clone https://github.com/user/repo.git
>
> If using RStudio it is simple enough to create a new project in that new
> directory (if the .Rproj file does not exist, otherwise open that).  Once
> you have the project open for that directory you can modify source files
> and rebuild and install as you like.  If at the CMD line, you do as Bob
> instructed with R CMD install .
>
> I recommend, however, either creating a new branch for you changes (if you
> familiar with git management) or at least make sure to change the
> subversion of the package so it doesn't conflict with the 'original'.  That
> way you 'know' which version of the package is installed at a given time.
>
> Naturally, if you feel your modifications are valuable you may want to
> actually fork the package on github and create a pull request of your
> changes for the maintainer to incorporate in to the next release.
>
> Hope this helps clarify things,
>
> Charles
>
>
>
> On Sat, Jul 18, 2015 at 8:49 AM, boB Rudis <bob at rudis.net> wrote:
>
>> You can go to the package directory:
>>
>>     cd /some/path/to/package
>>
>> and do
>>
>>     R CMD install .
>>
>> from a command-line there.
>>
>> Many github-based packages are also made using RStudio and you can
>> just open the .Rproj file (i.e. load it into R studio) and build the
>> package there which will install it.
>>
>> The same-named package will overwrite what you have previously installed.
>>
>> Just:
>>
>>    devtools::install_github("owner/package")
>>
>> to go back to the original.
>>
>> On Fri, Jul 17, 2015 at 8:12 PM, Steve E. <searl at vt.edu> wrote:
>> > Hi Folks,
>> >
>> > I am working with a package installed via GitHub that I would like to
>> > modify. However, I am not sure how I would go about loading a 'local'
>> > version of the package after I have modified it, and whether that
>> process
>> > would including uninstalling the original unmodified package (and,
>> > conversely, how to uninstall my local, modified version if I wanted to
>> go
>> > back to the unmodified version available on GitHub).
>> >
>> > Any advice would be appreciated.
>> >
>> >
>> > Thanks,
>> > Steve
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> http://r.789695.n4.nabble.com/modifying-a-package-installed-via-GitHub-tp4710016.html
>> > Sent from the R help mailing list archive at Nabble.com.
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From angelo.arcadi at virgilio.it  Tue Jul 21 02:45:41 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 21 Jul 2015 02:45:41 +0200 (CEST)
Subject: [R] R: Re: Differences in output of lme() when introducing
 interactions
Message-ID: <14eae12bffe.angelo.arcadi@virgilio.it>

Dear Michael, 
thanks for your answer. Despite it answers to my initial question, it does not help me in finding the solution to my problem unfortunately.

Could you please tell me which analysis of the two models should I trust then?
My goal is to know whether participants? choices
 of the dependent variable are linearly related to their own weight, height, shoe size and
 the combination of those effects. 
Would the analysis of model 2 be more 
correct than that of model 1? Which of the two analysis should I trust according to my goal? 
What is your recommendation?
                

Thanks in advance

Angelo





----Messaggio originale----
Da: lists at dewey.myzen.co.uk
Data: 20-lug-2015 17.56
A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, <r-help at r-project.org>
Ogg: Re: [R] Differences in output of lme() when introducing interactions

In-line

On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
> Dear List Members,
>
>
>
> I am searching for correlations between a dependent variable and a
> factor or a combination of factors in a repeated measure design. So I
> use lme() function in R. However, I am getting very different results
> depending on whether I add on the lme formula various factors compared
> to when only one is present. If a factor is found to be significant,
> shouldn't remain significant also when more factors are introduced in
> the model?
>

The short answer is 'No'.

The long answer is contained in any good book on statistics which you 
really need to have by your side as the long answer is too long to 
include in an email.

>
> I give an example of the outputs I get using the two models. In the first model I use one single factor:
>
> library(nlme)
> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
> Linear mixed-effects model fit by REML
>   Data: Gravel_ds
>        AIC      BIC   logLik
>    2119.28 2130.154 -1055.64
>
> Random effects:
>   Formula: ~1 | Subject
>          (Intercept) Residual
> StdDev:    1952.495 2496.424
>
> Fixed effects: Mode ~ Weight
>                  Value Std.Error DF   t-value p-value
> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
> Weight        -99.036   32.3094 17 -3.065233   0.007
>   Correlation:
>         (Intr)
> Weight -0.976
>
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>
> Number of Observations: 114
> Number of Groups: 19
>
>
> As you can see the p-value for factor Weight is significant.
> This is the second model, in which I add various factors for searching their correlations:
>
> library(nlme)
> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data = Gravel_ds, random = ~1 | Subject))
> Linear mixed-effects model fit by REML
>   Data: Gravel_ds
>         AIC      BIC    logLik
>    1975.165 2021.694 -969.5825
>
> Random effects:
>   Formula: ~1 | Subject
>          (Intercept) Residual
> StdDev:    1.127993 2494.826
>
> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>                                  Value Std.Error DF    t-value p-value
> (Intercept)                   5115955  10546313 95  0.4850941  0.6287
> Weight                      -13651237   6939242  3 -1.9672518  0.1438
> Height                         -18678     53202  3 -0.3510740  0.7487
> Shoe_Size                       93427    213737  3  0.4371115  0.6916
> BMI                         -13011088   7148969  3 -1.8199949  0.1663
> Weight:Height                   28128     14191  3  1.9820883  0.1418
> Weight:Shoe_Size               351453    186304  3  1.8864467  0.1557
> Height:Shoe_Size                 -783      1073  3 -0.7298797  0.5183
> Weight:BMI                      19475     11425  3  1.7045450  0.1868
> Height:BMI                     226512    118364  3  1.9136867  0.1516
> Shoe_Size:BMI                  329377    190294  3  1.7308827  0.1819
> Weight:Height:Shoe_Size          -706       371  3 -1.9014817  0.1534
> Weight:Height:BMI                -109        63  3 -1.7258742  0.1828
> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421  0.2671
> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771  0.1646
> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782  0.2589
>   Correlation:
>                              (Intr) Weight Height Sho_Sz BMI    Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
> Weight                      -0.895
> Height                      -0.996  0.869
> Shoe_Size                   -0.930  0.694  0.933
> BMI                         -0.911  0.998  0.887  0.720
> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999  0.995
> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641  0.609  0.619
> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972  0.980  0.965  0.637
> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999  0.999  0.999  0.622  0.973
> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997  0.988  0.998  0.649  0.958  0.995
> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000 -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974 -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940 -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998 -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947  0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931
>
> Standardized Within-Group Residuals:
>          Min          Q1         Med          Q3         Max
> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>
> Number of Observations: 114
> Number of Groups: 19
>
>
> This time the p-value associated to Weight is not significant anymore. Why? Which analysis should I trust?
>
>
> In addition, while in the first output the field "value" (which
> should give me the slope) is -99.036 in the second output it is
> -13651237. Why they are so different? The one in the first output is the
>   one that seems definitively more reasonable to me.
> I would very grateful if someone could give me an answer
>
>
> Thanks in advance
>
>
> Angelo
>
>
>
>
>
>
>
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html



  
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Jul 21 03:45:19 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 20 Jul 2015 18:45:19 -0700
Subject: [R] R: Re: Differences in output of lme() when introducing
	interactions
In-Reply-To: <14eae12bffe.angelo.arcadi@virgilio.it>
References: <14eae12bffe.angelo.arcadi@virgilio.it>
Message-ID: <CAGxFJbQVZRr-xc5O6wbPgYc_BPGKGBkxY77F68oaC9NjEQm-3w@mail.gmail.com>

I believe Michael's point is that you need to STOP asking such
questions and START either learning some statistics or work with
someone who already knows some. You should not be doing such analyses
on your own given your present state of statistical ignorance.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 20, 2015 at 5:45 PM, angelo.arcadi at virgilio.it
<angelo.arcadi at virgilio.it> wrote:
> Dear Michael,
> thanks for your answer. Despite it answers to my initial question, it does not help me in finding the solution to my problem unfortunately.
>
> Could you please tell me which analysis of the two models should I trust then?
> My goal is to know whether participants? choices
>  of the dependent variable are linearly related to their own weight, height, shoe size and
>  the combination of those effects.
> Would the analysis of model 2 be more
> correct than that of model 1? Which of the two analysis should I trust according to my goal?
> What is your recommendation?
>
>
> Thanks in advance
>
> Angelo
>
>
>
>
>
> ----Messaggio originale----
> Da: lists at dewey.myzen.co.uk
> Data: 20-lug-2015 17.56
> A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, <r-help at r-project.org>
> Ogg: Re: [R] Differences in output of lme() when introducing interactions
>
> In-line
>
> On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
>> Dear List Members,
>>
>>
>>
>> I am searching for correlations between a dependent variable and a
>> factor or a combination of factors in a repeated measure design. So I
>> use lme() function in R. However, I am getting very different results
>> depending on whether I add on the lme formula various factors compared
>> to when only one is present. If a factor is found to be significant,
>> shouldn't remain significant also when more factors are introduced in
>> the model?
>>
>
> The short answer is 'No'.
>
> The long answer is contained in any good book on statistics which you
> really need to have by your side as the long answer is too long to
> include in an email.
>
>>
>> I give an example of the outputs I get using the two models. In the first model I use one single factor:
>>
>> library(nlme)
>> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
>> Linear mixed-effects model fit by REML
>>   Data: Gravel_ds
>>        AIC      BIC   logLik
>>    2119.28 2130.154 -1055.64
>>
>> Random effects:
>>   Formula: ~1 | Subject
>>          (Intercept) Residual
>> StdDev:    1952.495 2496.424
>>
>> Fixed effects: Mode ~ Weight
>>                  Value Std.Error DF   t-value p-value
>> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
>> Weight        -99.036   32.3094 17 -3.065233   0.007
>>   Correlation:
>>         (Intr)
>> Weight -0.976
>>
>> Standardized Within-Group Residuals:
>>          Min          Q1         Med          Q3         Max
>> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>>
>> Number of Observations: 114
>> Number of Groups: 19
>>
>>
>> As you can see the p-value for factor Weight is significant.
>> This is the second model, in which I add various factors for searching their correlations:
>>
>> library(nlme)
>> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data = Gravel_ds, random = ~1 | Subject))
>> Linear mixed-effects model fit by REML
>>   Data: Gravel_ds
>>         AIC      BIC    logLik
>>    1975.165 2021.694 -969.5825
>>
>> Random effects:
>>   Formula: ~1 | Subject
>>          (Intercept) Residual
>> StdDev:    1.127993 2494.826
>>
>> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>>                                  Value Std.Error DF    t-value p-value
>> (Intercept)                   5115955  10546313 95  0.4850941  0.6287
>> Weight                      -13651237   6939242  3 -1.9672518  0.1438
>> Height                         -18678     53202  3 -0.3510740  0.7487
>> Shoe_Size                       93427    213737  3  0.4371115  0.6916
>> BMI                         -13011088   7148969  3 -1.8199949  0.1663
>> Weight:Height                   28128     14191  3  1.9820883  0.1418
>> Weight:Shoe_Size               351453    186304  3  1.8864467  0.1557
>> Height:Shoe_Size                 -783      1073  3 -0.7298797  0.5183
>> Weight:BMI                      19475     11425  3  1.7045450  0.1868
>> Height:BMI                     226512    118364  3  1.9136867  0.1516
>> Shoe_Size:BMI                  329377    190294  3  1.7308827  0.1819
>> Weight:Height:Shoe_Size          -706       371  3 -1.9014817  0.1534
>> Weight:Height:BMI                -109        63  3 -1.7258742  0.1828
>> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421  0.2671
>> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771  0.1646
>> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782  0.2589
>>   Correlation:
>>                              (Intr) Weight Height Sho_Sz BMI    Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
>> Weight                      -0.895
>> Height                      -0.996  0.869
>> Shoe_Size                   -0.930  0.694  0.933
>> BMI                         -0.911  0.998  0.887  0.720
>> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
>> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999  0.995
>> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641  0.609  0.619
>> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972  0.980  0.965  0.637
>> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999  0.999  0.999  0.622  0.973
>> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997  0.988  0.998  0.649  0.958  0.995
>> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000 -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
>> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974 -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
>> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940 -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
>> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998 -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
>> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947  0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931
>>
>> Standardized Within-Group Residuals:
>>          Min          Q1         Med          Q3         Max
>> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>>
>> Number of Observations: 114
>> Number of Groups: 19
>>
>>
>> This time the p-value associated to Weight is not significant anymore. Why? Which analysis should I trust?
>>
>>
>> In addition, while in the first output the field "value" (which
>> should give me the slope) is -99.036 in the second output it is
>> -13651237. Why they are so different? The one in the first output is the
>>   one that seems definitively more reasonable to me.
>> I would very grateful if someone could give me an answer
>>
>>
>> Thanks in advance
>>
>>
>> Angelo
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Tue Jul 21 04:09:37 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 20 Jul 2015 18:09:37 -0800
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <alpine.LNX.2.11.1507201255450.1876@localhost>
References: <alpine.lnx.2.11.1507201047240.1876@localhost>
	<alpine.lnx.2.11.1507201145510.1876@localhost>
	<cajucy5zjgvk9nkcdxagd5y=7gobwysqwbsdu1ipqvnsbyc28dw@mail.gmail.com>
	<cajucy5wiopuv2n3mphnlk4h=mmo=fod6unep=h=d3wdlqxjakw@mail.gmail.com>
Message-ID: <12E5FD3CA16.00000DCEjrkrideau@inbox.com>

Hi Rich,
I have no idea wha that chunk is not working but I think you can get the same result using the old method
Stick the following in an ERT (Insert > Tex Code) 

<<set-ops, echo = FALSE>>=
opts_chunk$set(echo = FALSE)
@

Heck, I've only been using LyX for 4-5 years and already I'm sounding crotchety.
John Kane
Kingston ON Canada


> -----Original Message-----
> From: rshepard at appl-ecosys.com
> Sent: Mon, 20 Jul 2015 12:58:51 -0700 (PDT)
> To: r-help at r-project.org
> Subject: Re: [R] Knitr: setting echo = FALSE globally
> 
> On Mon, 20 Jul 2015, Thierry Onkelinx wrote:
> 
>> We need the source file.
> 
>    Attached to the original message was the TeX output called,
> 'sample.txt'.
> I've attached it again, but with the .tex extension. Also, the .lyx file
> is
> attached.
> 
> Rich
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From drjimlemon at gmail.com  Tue Jul 21 09:45:45 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 21 Jul 2015 17:45:45 +1000
Subject: [R] [FORGED] change text size on a graphics
In-Reply-To: <55AD66E0.90801@auckland.ac.nz>
References: <166669763.1146304.1437423881249.JavaMail.yahoo@mail.yahoo.com>
	<55AD66E0.90801@auckland.ac.nz>
Message-ID: <CA+8X3fUEcizOUJkxOQkptNunnPipbMh_PtW0LGjy9gUCG5z93w@mail.gmail.com>

Hi Carol,
If you are using the "barlabels" function in the plotrix package, just
add the usual "cex" argument to the call. Using the first example in
the help page:

heights<-c(14,20,9,31,17)
barpos<-barplot(heights,main="A redundant bar plot")
barlabels(barpos,heights+1,prop=1,cex=1.5)

Jim


On Tue, Jul 21, 2015 at 7:23 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> On 21/07/15 08:24, carol white via R-help wrote:
>>
>> Hi,How is it possible to increase the size of a histogram labels
>> (displayed on the top of the bars)? I thought that if I use cex > 1,
>> it will increase all text size on a plot (axis labels, axis
>> annotation, title of the graphics and histogram labels) which I want
>> but it doesn't.
>
>
> ***What*** labels "displayed on the top of the bars"???  I don't see any
> such labels when I plot a histogram.
>
> Reproducible example?
>
> And please don't post in HTML.
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>
> and provide commented, minimal, self-contained, reproducible code.


From thierry.onkelinx at inbo.be  Tue Jul 21 09:52:58 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 21 Jul 2015 09:52:58 +0200
Subject: [R] using dcast with a function of multiple arguments
In-Reply-To: <0765308CD028654885F30322557308D81EEFB7F3@NYCSM0208.rth.ad.rothschild.com>
References: <0765308CD028654885F30322557308D81EEFB7F3@NYCSM0208.rth.ad.rothschild.com>
Message-ID: <CAJuCY5wJu1rb9LHKwoPb-QKzTgMzqo8ZArSOW3bnNLj5BX3FJA@mail.gmail.com>

Here is a solution using dplyr

require(data.table)
dt <- as.data.table(mtcars)
library(dplyr)
library(tidyr)
dt %>%
  group_by(carb, cyl) %>%
  summarise(WM = weighted.mean(x = mpg, w = wt)) %>%
  spread(cyl, WM)

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-20 20:52 GMT+02:00 Bos, Roger <roger.bos at rothschild.com>:

> I am trying to figure out how to use dcast.data.table with a function with
> multiple arguments.  Here is my reproducible example for a simple function
> of one argument:
>
> require(data.table)
> dt <- as.data.table(mtcars)
> dcast.data.table(dt, carb ~ cyl, value.var='mpg', fun=mean)
>
> If I instead want to use, say, weighted.mean(x, w), how do I do so?
>
> The docs say
> ...
> Any other arguments that maybe passed to the aggregating function.
>
> So I tried:
>
> > dcast.data.table(dt, carb ~ cyl, value.var='mpg', fun=weighted.mean,
> w="wt")
> Error in weighted.mean.default(data[[value.var]][0], ...) :
>   'x' and 'w' must have the same length
>
> The docs also say that value.var can be a list, so I tried that:
>
>  In cases where value.var is a list, the function should be able to handle
> a list input and provide a single value or list of length one as output.
>
> > dcast.data.table(dt, carb ~ cyl, value.var=list('mpg','wt'),
> fun=weighted.mean)
> Error in dcast.data.table(dt, carb ~ cyl, value.var = list("mpg", "wt"),  :
>   'value.var' must be a character vector of length 1.
>
> I didn't actually expect that to work, but without an example I don't know
> what else to try.  Any hints would be greatly appreciated.
>
> Thanks,
>
> Roger
>
>
>
> ***************************************************************
> This message and any attachments are for the intended recipient's use only.
> This message may contain confidential, proprietary or legally privileged
> information. No right to confidential or privileged treatment
> of this message is waived or lost by an error in transmission.
> If you have received this message in error, please immediately
> notify the sender by e-mail, delete the message, any attachments and all
> copies from your system and destroy any hard copies.  You must
> not, directly or indirectly, use, disclose, distribute,
> print or copy any part of this message or any attachments if you are not
> the intended recipient.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wht_crl at yahoo.com  Tue Jul 21 09:56:40 2015
From: wht_crl at yahoo.com (carol white)
Date: Tue, 21 Jul 2015 07:56:40 +0000 (UTC)
Subject: [R] [FORGED] change text size on a graphics
In-Reply-To: <CA+8X3fUEcizOUJkxOQkptNunnPipbMh_PtW0LGjy9gUCG5z93w@mail.gmail.com>
References: <CA+8X3fUEcizOUJkxOQkptNunnPipbMh_PtW0LGjy9gUCG5z93w@mail.gmail.com>
Message-ID: <165325612.1436759.1437465400195.JavaMail.yahoo@mail.yahoo.com>

Hi,I use hist.
If I usehist.result = hist(c(14,20,9,31,17), plot = F)plot(hist.result, labels = T, cex = 2) 
#doesn't increase the font size of all text on the plot: axis annotation, axis labels, count labels on top of the bars 

plot(hist.result, labels = T, cex.axis = 2, cex.label = 2, cex.main = 2) 
# how to increase the size of count labels that are displayed on the top of the bars by labels = T?
 Regards,


     On Tuesday, July 21, 2015 9:45 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
   

 Hi Carol,
If you are using the "barlabels" function in the plotrix package, just
add the usual "cex" argument to the call. Using the first example in
the help page:

heights<-c(14,20,9,31,17)
barpos<-barplot(heights,main="A redundant bar plot")
barlabels(barpos,heights+1,prop=1,cex=1.5)

Jim


On Tue, Jul 21, 2015 at 7:23 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> On 21/07/15 08:24, carol white via R-help wrote:
>>
>> Hi,How is it possible to increase the size of a histogram labels
>> (displayed on the top of the bars)? I thought that if I use cex > 1,
>> it will increase all text size on a plot (axis labels, axis
>> annotation, title of the graphics and histogram labels) which I want
>> but it doesn't.
>
>
> ***What*** labels "displayed on the top of the bars"???? I don't see any
> such labels when I plot a histogram.
>
> Reproducible example?
>
> And please don't post in HTML.
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>
> and provide commented, minimal, self-contained, reproducible code.


  
	[[alternative HTML version deleted]]


From j.para.fernandez at hotmail.com  Tue Jul 21 09:13:53 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Tue, 21 Jul 2015 00:13:53 -0700 (PDT)
Subject: [R] R GUI tklistbox get value
In-Reply-To: <web-565940781@cgpsrv2.cis.mcmaster.ca>
References: <1437388147621-4710064.post@n4.nabble.com>
	<web-565940781@cgpsrv2.cis.mcmaster.ca>
Message-ID: <1437462833661-4710123.post@n4.nabble.com>

Thanks mr FOX, now it works perfect!!!





--
View this message in context: http://r.789695.n4.nabble.com/R-GUI-tklistbox-get-value-tp4710064p4710123.html
Sent from the R help mailing list archive at Nabble.com.


From drjimlemon at gmail.com  Tue Jul 21 10:39:33 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 21 Jul 2015 18:39:33 +1000
Subject: [R] [FORGED] change text size on a graphics
In-Reply-To: <165325612.1436759.1437465400195.JavaMail.yahoo@mail.yahoo.com>
References: <CA+8X3fUEcizOUJkxOQkptNunnPipbMh_PtW0LGjy9gUCG5z93w@mail.gmail.com>
	<165325612.1436759.1437465400195.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fVsMUm_j1McmFP61sB6zOwAZwg2q1vfr2bRtW+jtHr=rQ@mail.gmail.com>

Hi Carol,
You can use "hist" to get the counts in each interval, but try
"barplot" to get the larger bar labels:

barpos<-barplot(hist.result$counts,space=0)
barlabels(barpos,hist.result$counts+0.1,hist.result$counts,
 prop=1,border=NA,cex=2)
axis(1,at=1:6-0.5,labels=paste(hist.result$breaks[1:6],
 hist.result$breaks[2:7],sep="-"))

You can't just send any old argument to any old function. At best you
get an error message, often you just get nonsense.

Jim


On Tue, Jul 21, 2015 at 5:56 PM, carol white <wht_crl at yahoo.com> wrote:
> Hi,
> I use hist.
>
> If I use
> hist.result = hist(c(14,20,9,31,17), plot = F)
> plot(hist.result, labels = T, cex = 2)
> #doesn't increase the font size of all text on the plot: axis annotation,
> axis labels, count labels on top of the bars
>
> plot(hist.result, labels = T, cex.axis = 2, cex.label = 2, cex.main = 2)
> # how to increase the size of count labels that are displayed on the top of
> the bars by labels = T?
> Regards,
>
>
> On Tuesday, July 21, 2015 9:45 AM, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>
> Hi Carol,
> If you are using the "barlabels" function in the plotrix package, just
> add the usual "cex" argument to the call. Using the first example in
> the help page:
>
> heights<-c(14,20,9,31,17)
> barpos<-barplot(heights,main="A redundant bar plot")
> barlabels(barpos,heights+1,prop=1,cex=1.5)
>
> Jim
>
>
> On Tue, Jul 21, 2015 at 7:23 AM, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
>> On 21/07/15 08:24, carol white via R-help wrote:
>>>
>>> Hi,How is it possible to increase the size of a histogram labels
>>> (displayed on the top of the bars)? I thought that if I use cex > 1,
>>> it will increase all text size on a plot (axis labels, axis
>>> annotation, title of the graphics and histogram labels) which I want
>>> but it doesn't.
>>
>>
>> ***What*** labels "displayed on the top of the bars"???  I don't see any
>> such labels when I plot a histogram.
>>
>> Reproducible example?
>>
>> And please don't post in HTML.
>>
>> cheers,
>>
>> Rolf Turner
>>
>> --
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>


From angelo.arcadi at virgilio.it  Tue Jul 21 11:04:40 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 21 Jul 2015 11:04:40 +0200 (CEST)
Subject: [R] R: Re: R: Re: Differences in output of lme() when introducing
 interactions
Message-ID: <14eafdb9695.angelo.arcadi@virgilio.it>

Dear Bert,
thank you for your feedback. Can you please provide some references online so I can improve "my ignorance"?
Anyways, please notice that it is not true that I do not know statistics and regressions at all, and I am strongly
convinced that my question can be of interest for some one else in the future. 

This is what forums serve for, isn't it? This is why people help each other, isn't it?

Moreover, don't you think that I would not have asked to this R forum if I had the possibility to ask or pay a statician?
Don't you think I have done already my best to study and learn before posting this message? Trust me, I have read different
online tutorials on lme and lmer, and I am confident that I have got the basic concepts. Still I have not found the answer
to solve my problem, so if you know the answer can you please give me some suggestions that can help me?

I do not have a book where to learn and unfortunately I have to analyze the results soon. Any help? Any online reference to-the-point
that can help me in solving this problem?

Thank you in advance

Best regards

Angelo 





----Messaggio originale----
Da: bgunter.4567 at gmail.com
Data: 21-lug-2015 3.45
A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>
Cc: <lists at dewey.myzen.co.uk>, <r-help at r-project.org>
Ogg: Re: [R] R: Re: Differences in output of lme() when introducing interactions

I believe Michael's point is that you need to STOP asking such
questions and START either learning some statistics or work with
someone who already knows some. You should not be doing such analyses
on your own given your present state of statistical ignorance.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 20, 2015 at 5:45 PM, angelo.arcadi at virgilio.it
<angelo.arcadi at virgilio.it> wrote:
> Dear Michael,
> thanks for your answer. Despite it answers to my initial question, it does not help me in finding the solution to my problem unfortunately.
>
> Could you please tell me which analysis of the two models should I trust then?
> My goal is to know whether participants? choices
>  of the dependent variable are linearly related to their own weight, height, shoe size and
>  the combination of those effects.
> Would the analysis of model 2 be more
> correct than that of model 1? Which of the two analysis should I trust according to my goal?
> What is your recommendation?
>
>
> Thanks in advance
>
> Angelo
>
>
>
>
>
> ----Messaggio originale----
> Da: lists at dewey.myzen.co.uk
> Data: 20-lug-2015 17.56
> A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, <r-help at r-project.org>
> Ogg: Re: [R] Differences in output of lme() when introducing interactions
>
> In-line
>
> On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
>> Dear List Members,
>>
>>
>>
>> I am searching for correlations between a dependent variable and a
>> factor or a combination of factors in a repeated measure design. So I
>> use lme() function in R. However, I am getting very different results
>> depending on whether I add on the lme formula various factors compared
>> to when only one is present. If a factor is found to be significant,
>> shouldn't remain significant also when more factors are introduced in
>> the model?
>>
>
> The short answer is 'No'.
>
> The long answer is contained in any good book on statistics which you
> really need to have by your side as the long answer is too long to
> include in an email.
>
>>
>> I give an example of the outputs I get using the two models. In the first model I use one single factor:
>>
>> library(nlme)
>> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
>> Linear mixed-effects model fit by REML
>>   Data: Gravel_ds
>>        AIC      BIC   logLik
>>    2119.28 2130.154 -1055.64
>>
>> Random effects:
>>   Formula: ~1 | Subject
>>          (Intercept) Residual
>> StdDev:    1952.495 2496.424
>>
>> Fixed effects: Mode ~ Weight
>>                  Value Std.Error DF   t-value p-value
>> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
>> Weight        -99.036   32.3094 17 -3.065233   0.007
>>   Correlation:
>>         (Intr)
>> Weight -0.976
>>
>> Standardized Within-Group Residuals:
>>          Min          Q1         Med          Q3         Max
>> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>>
>> Number of Observations: 114
>> Number of Groups: 19
>>
>>
>> As you can see the p-value for factor Weight is significant.
>> This is the second model, in which I add various factors for searching their correlations:
>>
>> library(nlme)
>> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data = Gravel_ds, random = ~1 | Subject))
>> Linear mixed-effects model fit by REML
>>   Data: Gravel_ds
>>         AIC      BIC    logLik
>>    1975.165 2021.694 -969.5825
>>
>> Random effects:
>>   Formula: ~1 | Subject
>>          (Intercept) Residual
>> StdDev:    1.127993 2494.826
>>
>> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>>                                  Value Std.Error DF    t-value p-value
>> (Intercept)                   5115955  10546313 95  0.4850941  0.6287
>> Weight                      -13651237   6939242  3 -1.9672518  0.1438
>> Height                         -18678     53202  3 -0.3510740  0.7487
>> Shoe_Size                       93427    213737  3  0.4371115  0.6916
>> BMI                         -13011088   7148969  3 -1.8199949  0.1663
>> Weight:Height                   28128     14191  3  1.9820883  0.1418
>> Weight:Shoe_Size               351453    186304  3  1.8864467  0.1557
>> Height:Shoe_Size                 -783      1073  3 -0.7298797  0.5183
>> Weight:BMI                      19475     11425  3  1.7045450  0.1868
>> Height:BMI                     226512    118364  3  1.9136867  0.1516
>> Shoe_Size:BMI                  329377    190294  3  1.7308827  0.1819
>> Weight:Height:Shoe_Size          -706       371  3 -1.9014817  0.1534
>> Weight:Height:BMI                -109        63  3 -1.7258742  0.1828
>> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421  0.2671
>> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771  0.1646
>> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782  0.2589
>>   Correlation:
>>                              (Intr) Weight Height Sho_Sz BMI    Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
>> Weight                      -0.895
>> Height                      -0.996  0.869
>> Shoe_Size                   -0.930  0.694  0.933
>> BMI                         -0.911  0.998  0.887  0.720
>> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
>> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999  0.995
>> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641  0.609  0.619
>> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972  0.980  0.965  0.637
>> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999  0.999  0.999  0.622  0.973
>> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997  0.988  0.998  0.649  0.958  0.995
>> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000 -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
>> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974 -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
>> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940 -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
>> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998 -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
>> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947  0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931
>>
>> Standardized Within-Group Residuals:
>>          Min          Q1         Med          Q3         Max
>> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>>
>> Number of Observations: 114
>> Number of Groups: 19
>>
>>
>> This time the p-value associated to Weight is not significant anymore. Why? Which analysis should I trust?
>>
>>
>> In addition, while in the first output the field "value" (which
>> should give me the slope) is -99.036 in the second output it is
>> -13651237. Why they are so different? The one in the first output is the
>>   one that seems definitively more reasonable to me.
>> I would very grateful if someone could give me an answer
>>
>>
>> Thanks in advance
>>
>>
>> Angelo
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



  
	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Tue Jul 21 11:58:50 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 21 Jul 2015 10:58:50 +0100
Subject: [R] R: Re: R: Re: Differences in output of lme() when
 introducing interactions
In-Reply-To: <14eafdb9695.angelo.arcadi@virgilio.it>
References: <14eafdb9695.angelo.arcadi@virgilio.it>
Message-ID: <55AE17DA.3020704@dewey.myzen.co.uk>

Dear Angelo

I suggest you do an online search for marginality which may help to 
explain the relationship between main effects and interactions. As I 
said in my original email this is a complicated subject which we are not 
going to retype for you.

If you are doing this as a student I suggest you sue your university for 
failing to train you appropriately and if it is part of your employment 
I suggest you find a better employer.

On 21/07/2015 10:04, angelo.arcadi at virgilio.it wrote:
> Dear Bert,
> thank you for your feedback. Can you please provide some references
> online so I can improve "my ignorance"?
> Anyways, please notice that it is not true that I do not know statistics
> and regressions at all, and I am strongly
> convinced that my question can be of interest for some one else in the
> future.
>
> This is what forums serve for, isn't it? This is why people help each
> other, isn't it?
>
> Moreover, don't you think that I would not have asked to this R forum if
> I had the possibility to ask or pay a statician?
> Don't you think I have done already my best to study and learn before
> posting this message? Trust me, I have read different
> online tutorials on lme and lmer, and I am confident that I have got the
> basic concepts. Still I have not found the answer
> to solve my problem, so if you know the answer can you please give me
> some suggestions that can help me?
>
> I do not have a book where to learn and unfortunately I have to analyze
> the results soon. Any help? Any online reference to-the-point
> that can help me in solving this problem?
>
> Thank you in advance
>
> Best regards
>
> Angelo
>
>
>     ----Messaggio originale----
>     Da: bgunter.4567 at gmail.com
>     Data: 21-lug-2015 3.45
>     A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>
>     Cc: <lists at dewey.myzen.co.uk>, <r-help at r-project.org>
>     Ogg: Re: [R] R: Re: Differences in output of lme() when introducing
>     interactions
>
>     I believe Michael's point is that you need to STOP asking such
>     questions and START either learning some statistics or work with
>     someone who already knows some. You should not be doing such analyses
>     on your own given your present state of statistical ignorance.
>
>     Cheers,
>     Bert
>
>
>     Bert Gunter
>
>     "Data is not information. Information is not knowledge. And knowledge
>     is certainly not wisdom."
>         -- Clifford Stoll
>
>
>     On Mon, Jul 20, 2015 at 5:45 PM, angelo.arcadi at virgilio.it
>     <angelo.arcadi at virgilio.it> wrote:
>      > Dear Michael,
>      > thanks for your answer. Despite it answers to my initial
>     question, it does not help me in finding the solution to my problem
>     unfortunately.
>      >
>      > Could you please tell me which analysis of the two models should
>     I trust then?
>      > My goal is to know whether participants? choices
>      >  of the dependent variable are linearly related to their own
>     weight, height, shoe size and
>      >  the combination of those effects.
>      > Would the analysis of model 2 be more
>      > correct than that of model 1? Which of the two analysis should I
>     trust according to my goal?
>      > What is your recommendation?
>      >
>      >
>      > Thanks in advance
>      >
>      > Angelo
>      >
>      >
>      >
>      >
>      >
>      > ----Messaggio originale----
>      > Da: lists at dewey.myzen.co.uk
>      > Data: 20-lug-2015 17.56
>      > A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>,
>     <r-help at r-project.org>
>      > Ogg: Re: [R] Differences in output of lme() when introducing
>     interactions
>      >
>      > In-line
>      >
>      > On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
>      >> Dear List Members,
>      >>
>      >>
>      >>
>      >> I am searching for correlations between a dependent variable and a
>      >> factor or a combination of factors in a repeated measure design.
>     So I
>      >> use lme() function in R. However, I am getting very different
>     results
>      >> depending on whether I add on the lme formula various factors
>     compared
>      >> to when only one is present. If a factor is found to be significant,
>      >> shouldn't remain significant also when more factors are
>     introduced in
>      >> the model?
>      >>
>      >
>      > The short answer is 'No'.
>      >
>      > The long answer is contained in any good book on statistics which you
>      > really need to have by your side as the long answer is too long to
>      > include in an email.
>      >
>      >>
>      >> I give an example of the outputs I get using the two models. In
>     the first model I use one single factor:
>      >>
>      >> library(nlme)
>      >> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
>      >> Linear mixed-effects model fit by REML
>      >>   Data: Gravel_ds
>      >>        AIC      BIC   logLik
>      >>    2119.28 2130.154 -1055.64
>      >>
>      >> Random effects:
>      >>   Formula: ~1 | Subject
>      >>          (Intercept) Residual
>      >> StdDev:    1952.495 2496.424
>      >>
>      >> Fixed effects: Mode ~ Weight
>      >>                  Value Std.Error DF   t-value p-value
>      >> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
>      >> Weight        -99.036   32.3094 17 -3.065233   0.007
>      >>   Correlation:
>      >>         (Intr)
>      >> Weight -0.976
>      >>
>      >> Standardized Within-Group Residuals:
>      >>          Min          Q1         Med          Q3         Max
>      >> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>      >>
>      >> Number of Observations: 114
>      >> Number of Groups: 19
>      >>
>      >>
>      >> As you can see the p-value for factor Weight is significant.
>      >> This is the second model, in which I add various factors for
>     searching their correlations:
>      >>
>      >> library(nlme)
>      >> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data =
>     Gravel_ds, random = ~1 | Subject))
>      >> Linear mixed-effects model fit by REML
>      >>   Data: Gravel_ds
>      >>         AIC      BIC    logLik
>      >>    1975.165 2021.694 -969.5825
>      >>
>      >> Random effects:
>      >>   Formula: ~1 | Subject
>      >>          (Intercept) Residual
>      >> StdDev:    1.127993 2494.826
>      >>
>      >> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>      >>                                  Value Std.Error DF    t-value
>     p-value
>      >> (Intercept)                   5115955  10546313 95  0.4850941
>     0.6287
>      >> Weight                      -13651237   6939242  3 -1.9672518
>     0.1438
>      >> Height                         -18678     53202  3 -0.3510740
>     0.7487
>      >> Shoe_Size                       93427    213737  3  0.4371115
>     0.6916
>      >> BMI                         -13011088   7148969  3 -1.8199949
>     0.1663
>      >> Weight:Height                   28128     14191  3  1.9820883
>     0.1418
>      >> Weight:Shoe_Size               351453    186304  3  1.8864467
>     0.1557
>      >> Height:Shoe_Size                 -783      1073  3 -0.7298797
>     0.5183
>      >> Weight:BMI                      19475     11425  3  1.7045450
>     0.1868
>      >> Height:BMI                     226512    118364  3  1.9136867
>     0.1516
>      >> Shoe_Size:BMI                  329377    190294  3  1.7308827
>     0.1819
>      >> Weight:Height:Shoe_Size          -706       371  3 -1.9014817
>     0.1534
>      >> Weight:Height:BMI                -109        63  3 -1.7258742
>     0.1828
>      >> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421
>     0.2671
>      >> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771
>     0.1646
>      >> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782
>     0.2589
>      >>   Correlation:
>      >>                              (Intr) Weight Height Sho_Sz BMI
>     Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
>      >> Weight                      -0.895
>      >> Height                      -0.996  0.869
>      >> Shoe_Size                   -0.930  0.694  0.933
>      >> BMI                         -0.911  0.998  0.887  0.720
>      >> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
>      >> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999
>     0.995
>      >> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641
>     0.609  0.619
>      >> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972
>     0.980  0.965  0.637
>      >> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999
>     0.999  0.999  0.622  0.973
>      >> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997
>     0.988  0.998  0.649  0.958  0.995
>      >> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000
>     -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
>      >> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974
>     -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
>      >> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940
>     -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
>      >> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998
>     -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
>      >> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947
>     0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931
>      >>
>      >> Standardized Within-Group Residuals:
>      >>          Min          Q1         Med          Q3         Max
>      >> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>      >>
>      >> Number of Observations: 114
>      >> Number of Groups: 19
>      >>
>      >>
>      >> This time the p-value associated to Weight is not significant
>     anymore. Why? Which analysis should I trust?
>      >>
>      >>
>      >> In addition, while in the first output the field "value" (which
>      >> should give me the slope) is -99.036 in the second output it is
>      >> -13651237. Why they are so different? The one in the first
>     output is the
>      >>   one that seems definitively more reasonable to me.
>      >> I would very grateful if someone could give me an answer
>      >>
>      >>
>      >> Thanks in advance
>      >>
>      >>
>      >> Angelo
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>       [[alternative HTML version deleted]]
>      >>
>      >> ______________________________________________
>      >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      >> https://stat.ethz.ch/mailman/listinfo/r-help
>      >> PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      >> and provide commented, minimal, self-contained, reproducible code.
>      >>
>      >
>      > --
>      > Michael
>      > http://www.dewey.myzen.co.uk/home.html
>      >
>      >
>      >
>      >
>      >         [[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From angelo.arcadi at virgilio.it  Tue Jul 21 12:12:58 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Tue, 21 Jul 2015 12:12:58 +0200 (CEST)
Subject: [R] R: Re: R: Re: R: Re: Differences in output of lme() when
 introducing interactions
Message-ID: <14eb01a1c64.angelo.arcadi@virgilio.it>

Dear Michael,
thanks a lot. I am studying the marginality and I came across to this post:

http://www.ats.ucla.edu/stat/r/faq/type3.htm

Do you think that the procedure there described is the right one to solve my problem?

Would you have any other online resources to suggest especially dealing with R?

My department does not have a statician, so I have to find a solution with my own capacities.

Thanks in advance

Angelo




----Messaggio originale----
Da: lists at dewey.myzen.co.uk
Data: 21-lug-2015 11.58
A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>, <bgunter.4567 at gmail.com>
Cc: <r-help at r-project.org>
Ogg: Re: R: Re: [R] R: Re: Differences in output of lme() when introducing interactions

Dear Angelo

I suggest you do an online search for marginality which may help to 
explain the relationship between main effects and interactions. As I 
said in my original email this is a complicated subject which we are not 
going to retype for you.

If you are doing this as a student I suggest you sue your university for 
failing to train you appropriately and if it is part of your employment 
I suggest you find a better employer.

On 21/07/2015 10:04, angelo.arcadi at virgilio.it wrote:
> Dear Bert,
> thank you for your feedback. Can you please provide some references
> online so I can improve "my ignorance"?
> Anyways, please notice that it is not true that I do not know statistics
> and regressions at all, and I am strongly
> convinced that my question can be of interest for some one else in the
> future.
>
> This is what forums serve for, isn't it? This is why people help each
> other, isn't it?
>
> Moreover, don't you think that I would not have asked to this R forum if
> I had the possibility to ask or pay a statician?
> Don't you think I have done already my best to study and learn before
> posting this message? Trust me, I have read different
> online tutorials on lme and lmer, and I am confident that I have got the
> basic concepts. Still I have not found the answer
> to solve my problem, so if you know the answer can you please give me
> some suggestions that can help me?
>
> I do not have a book where to learn and unfortunately I have to analyze
> the results soon. Any help? Any online reference to-the-point
> that can help me in solving this problem?
>
> Thank you in advance
>
> Best regards
>
> Angelo
>
>
>     ----Messaggio originale----
>     Da: bgunter.4567 at gmail.com
>     Data: 21-lug-2015 3.45
>     A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>
>     Cc: <lists at dewey.myzen.co.uk>, <r-help at r-project.org>
>     Ogg: Re: [R] R: Re: Differences in output of lme() when introducing
>     interactions
>
>     I believe Michael's point is that you need to STOP asking such
>     questions and START either learning some statistics or work with
>     someone who already knows some. You should not be doing such analyses
>     on your own given your present state of statistical ignorance.
>
>     Cheers,
>     Bert
>
>
>     Bert Gunter
>
>     "Data is not information. Information is not knowledge. And knowledge
>     is certainly not wisdom."
>         -- Clifford Stoll
>
>
>     On Mon, Jul 20, 2015 at 5:45 PM, angelo.arcadi at virgilio.it
>     <angelo.arcadi at virgilio.it> wrote:
>      > Dear Michael,
>      > thanks for your answer. Despite it answers to my initial
>     question, it does not help me in finding the solution to my problem
>     unfortunately.
>      >
>      > Could you please tell me which analysis of the two models should
>     I trust then?
>      > My goal is to know whether participants? choices
>      >  of the dependent variable are linearly related to their own
>     weight, height, shoe size and
>      >  the combination of those effects.
>      > Would the analysis of model 2 be more
>      > correct than that of model 1? Which of the two analysis should I
>     trust according to my goal?
>      > What is your recommendation?
>      >
>      >
>      > Thanks in advance
>      >
>      > Angelo
>      >
>      >
>      >
>      >
>      >
>      > ----Messaggio originale----
>      > Da: lists at dewey.myzen.co.uk
>      > Data: 20-lug-2015 17.56
>      > A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>,
>     <r-help at r-project.org>
>      > Ogg: Re: [R] Differences in output of lme() when introducing
>     interactions
>      >
>      > In-line
>      >
>      > On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
>      >> Dear List Members,
>      >>
>      >>
>      >>
>      >> I am searching for correlations between a dependent variable and a
>      >> factor or a combination of factors in a repeated measure design.
>     So I
>      >> use lme() function in R. However, I am getting very different
>     results
>      >> depending on whether I add on the lme formula various factors
>     compared
>      >> to when only one is present. If a factor is found to be significant,
>      >> shouldn't remain significant also when more factors are
>     introduced in
>      >> the model?
>      >>
>      >
>      > The short answer is 'No'.
>      >
>      > The long answer is contained in any good book on statistics which you
>      > really need to have by your side as the long answer is too long to
>      > include in an email.
>      >
>      >>
>      >> I give an example of the outputs I get using the two models. In
>     the first model I use one single factor:
>      >>
>      >> library(nlme)
>      >> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 | Subject))
>      >> Linear mixed-effects model fit by REML
>      >>   Data: Gravel_ds
>      >>        AIC      BIC   logLik
>      >>    2119.28 2130.154 -1055.64
>      >>
>      >> Random effects:
>      >>   Formula: ~1 | Subject
>      >>          (Intercept) Residual
>      >> StdDev:    1952.495 2496.424
>      >>
>      >> Fixed effects: Mode ~ Weight
>      >>                  Value Std.Error DF   t-value p-value
>      >> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
>      >> Weight        -99.036   32.3094 17 -3.065233   0.007
>      >>   Correlation:
>      >>         (Intr)
>      >> Weight -0.976
>      >>
>      >> Standardized Within-Group Residuals:
>      >>          Min          Q1         Med          Q3         Max
>      >> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>      >>
>      >> Number of Observations: 114
>      >> Number of Groups: 19
>      >>
>      >>
>      >> As you can see the p-value for factor Weight is significant.
>      >> This is the second model, in which I add various factors for
>     searching their correlations:
>      >>
>      >> library(nlme)
>      >> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data =
>     Gravel_ds, random = ~1 | Subject))
>      >> Linear mixed-effects model fit by REML
>      >>   Data: Gravel_ds
>      >>         AIC      BIC    logLik
>      >>    1975.165 2021.694 -969.5825
>      >>
>      >> Random effects:
>      >>   Formula: ~1 | Subject
>      >>          (Intercept) Residual
>      >> StdDev:    1.127993 2494.826
>      >>
>      >> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>      >>                                  Value Std.Error DF    t-value
>     p-value
>      >> (Intercept)                   5115955  10546313 95  0.4850941
>     0.6287
>      >> Weight                      -13651237   6939242  3 -1.9672518
>     0.1438
>      >> Height                         -18678     53202  3 -0.3510740
>     0.7487
>      >> Shoe_Size                       93427    213737  3  0.4371115
>     0.6916
>      >> BMI                         -13011088   7148969  3 -1.8199949
>     0.1663
>      >> Weight:Height                   28128     14191  3  1.9820883
>     0.1418
>      >> Weight:Shoe_Size               351453    186304  3  1.8864467
>     0.1557
>      >> Height:Shoe_Size                 -783      1073  3 -0.7298797
>     0.5183
>      >> Weight:BMI                      19475     11425  3  1.7045450
>     0.1868
>      >> Height:BMI                     226512    118364  3  1.9136867
>     0.1516
>      >> Shoe_Size:BMI                  329377    190294  3  1.7308827
>     0.1819
>      >> Weight:Height:Shoe_Size          -706       371  3 -1.9014817
>     0.1534
>      >> Weight:Height:BMI                -109        63  3 -1.7258742
>     0.1828
>      >> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421
>     0.2671
>      >> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771
>     0.1646
>      >> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782
>     0.2589
>      >>   Correlation:
>      >>                              (Intr) Weight Height Sho_Sz BMI
>     Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S: H:S_S:
>      >> Weight                      -0.895
>      >> Height                      -0.996  0.869
>      >> Shoe_Size                   -0.930  0.694  0.933
>      >> BMI                         -0.911  0.998  0.887  0.720
>      >> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
>      >> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999
>     0.995
>      >> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641
>     0.609  0.619
>      >> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972
>     0.980  0.965  0.637
>      >> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999
>     0.999  0.999  0.622  0.973
>      >> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997
>     0.988  0.998  0.649  0.958  0.995
>      >> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000
>     -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
>      >> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974
>     -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
>      >> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940
>     -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
>      >> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998
>     -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
>      >> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947
>     0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999 -0.931
>      >>
>      >> Standardized Within-Group Residuals:
>      >>          Min          Q1         Med          Q3         Max
>      >> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>      >>
>      >> Number of Observations: 114
>      >> Number of Groups: 19
>      >>
>      >>
>      >> This time the p-value associated to Weight is not significant
>     anymore. Why? Which analysis should I trust?
>      >>
>      >>
>      >> In addition, while in the first output the field "value" (which
>      >> should give me the slope) is -99.036 in the second output it is
>      >> -13651237. Why they are so different? The one in the first
>     output is the
>      >>   one that seems definitively more reasonable to me.
>      >> I would very grateful if someone could give me an answer
>      >>
>      >>
>      >> Thanks in advance
>      >>
>      >>
>      >> Angelo
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>
>      >>       [[alternative HTML version deleted]]
>      >>
>      >> ______________________________________________
>      >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      >> https://stat.ethz.ch/mailman/listinfo/r-help
>      >> PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      >> and provide commented, minimal, self-contained, reproducible code.
>      >>
>      >
>      > --
>      > Michael
>      > http://www.dewey.myzen.co.uk/home.html
>      >
>      >
>      >
>      >
>      >         [[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html



  
	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jul 21 13:03:41 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 21 Jul 2015 07:03:41 -0400
Subject: [R] help with contributed package MTS
In-Reply-To: <001801d0c39c$ca1ce470$5e56ad50$@rcumariacristina.com>
References: <000601d0c316$2ca804d0$85f80e70$@rcumariacristina.com>
	<CAM_vju=_zxvXOt2RQNSBWxcC9-GsbW7nqB40xAVbYrtez0iC0A@mail.gmail.com>
	<001801d0c39c$ca1ce470$5e56ad50$@rcumariacristina.com>
Message-ID: <CAM_vjunKaXxm5wKEXjvsuHyaTSATA6LoKwmaj3SJYHEy65R-SQ@mail.gmail.com>

Please reply to the list, not just me.

Have you installed the requirements?

Please also do the other thing I originally suggested, which is post your
sessionInfo.().

On Tuesday, July 21, 2015, AGUSTIN ALONSO RODRIGUEZ <
aalonso at rcumariacristina.com> wrote:

> Dear Ms Goslee:
>
> Thank you very much for answering my email.
>
> However, I do not succed installing MTS. When I type:
> install.packages("MTS") I receive the message "not available for R.3.2.1".
>
> Also, if I intend to install from "packages -> install packages" after
> connecting to a CRAN Mirror, a window is open with
>
> GLMMGibbs
> RDCOMClient
> Rsterm
> Survnnet
> Yags
>
> This is something new for me. What should  I do with these?


I don't even know what you mean. What R commands are you issuing? Or are
you using RStudio or something besides R? That's also important information.


>
> If I install M from a local zip, I continue receiving the message: not
> available for R.3.2.1.


Where did you get this zip?


> I do not know what to do.


Provide complete information, follow previous suggestions, read the
directions.

Sarah



>
> Sincerely yours
>
> Agustin Alonso
>
> -----Mensaje original-----
> De: Sarah Goslee [mailto:sarah.goslee at gmail.com <javascript:;>]
> Enviado el: lunes, 20 de julio de 2015 20:17
> Para: AGUSTIN ALONSO RODRIGUEZ
> CC: r-help
> Asunto: Re: [R] help with contributed package MTS
>
> Hi,
>
> On Mon, Jul 20, 2015 at 2:01 PM, AGUSTIN ALONSO RODRIGUEZ <
> aalonso at rcumariacristina.com <javascript:;>> wrote:
>
> > I am using R-3.2.1, and when I type  install.packages(?MTS?), I get
> > the
> > message:
> >
> > package ?MTS? is not available (for R version 3.2.1).
>
> It's on CRAN, and passes check for R-release:
> https://cran.r-project.org/web/checks/check_results_MTS.html
> I didn't have any problem installing it with install.packages().
>
> I'd suggest trying again after installing the requirements (see below),
> and if you still have no success posting your sessionInfo().
>
> > I have tried also to install from local zip files, and I got it, but
> > when I
> > type: library(MTS), I got the message that Rcpp was absent.
>
> That means you need to install the Rcpp package before you can install
> MTS. You can see these requirements listed:
>  https://cran.r-project.org/web/packages/MTS/index.html
>
> > Would you please, tell me how to install MTS in R-3.2.1?
>
> By reading and following the error messages you received.
>
> Sarah
>
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
>

-- 
Sarah Goslee
http://www.stringpage.com
http://www.sarahgoslee.com
http://www.functionaldiversity.org

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Tue Jul 21 13:35:13 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 21 Jul 2015 03:35:13 -0800
Subject: [R] [FORGED] change text size on a graphics
In-Reply-To: <165325612.1436759.1437465400195.JavaMail.yahoo@mail.yahoo.com>
References: <ca+8x3fuecizoujkxoqkptnunnpipbmh_ptw0lgjy9gucg5z93w@mail.gmail.com>
Message-ID: <17D63511C24.00001152jrkrideau@inbox.com>

Following some of the basic advice in these links can save a lot of time and make answering questions much easier.
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html


John Kane
Kingston ON Canada


> -----Original Message-----
> From: r-help at r-project.org
> Sent: Tue, 21 Jul 2015 07:56:40 +0000 (UTC)
> To: drjimlemon at gmail.com
> Subject: Re: [R] [FORGED] change text size on a graphics
> 
> Hi,I use hist.
> If I usehist.result = hist(c(14,20,9,31,17), plot = F)plot(hist.result,
> labels = T, cex = 2)
> #doesn't increase the font size of all text on the plot: axis annotation,
> axis labels, count labels on top of the bars
> 
> plot(hist.result, labels = T, cex.axis = 2, cex.label = 2, cex.main = 2)
> # how to increase the size of count labels that are displayed on the top
> of the bars by labels = T?
>  Regards,
> 
> 
>      On Tuesday, July 21, 2015 9:45 AM, Jim Lemon <drjimlemon at gmail.com>
> wrote:
> 
> 
>  Hi Carol,
> If you are using the "barlabels" function in the plotrix package, just
> add the usual "cex" argument to the call. Using the first example in
> the help page:
> 
> heights<-c(14,20,9,31,17)
> barpos<-barplot(heights,main="A redundant bar plot")
> barlabels(barpos,heights+1,prop=1,cex=1.5)
> 
> Jim
> 
> 
> On Tue, Jul 21, 2015 at 7:23 AM, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
>> On 21/07/15 08:24, carol white via R-help wrote:
>>> 
>>> Hi,How is it possible to increase the size of a histogram labels
>>> (displayed on the top of the bars)? I thought that if I use cex > 1,
>>> it will increase all text size on a plot (axis labels, axis
>>> annotation, title of the graphics and histogram labels) which I want
>>> but it doesn't.
>> 
>> 
>> ***What*** labels "displayed on the top of the bars"???? I don't see any
>> such labels when I plot a histogram.
>> 
>> Reproducible example?
>> 
>> And please don't post in HTML.
>> 
>> cheers,
>> 
>> Rolf Turner
>> 
>> --
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> 
>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From jrkrideau at inbox.com  Tue Jul 21 13:39:31 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 21 Jul 2015 03:39:31 -0800
Subject: [R] R: Re: R: Re: R: Re: Differences in output of lme() when
 introducing interactions
In-Reply-To: <14eb01a1c64.angelo.arcadi@virgilio.it>
Message-ID: <17DFD4C7B1E.0000115Cjrkrideau@inbox.com>

Have you been asking statistics related questiongs on StackExchange?

I must say I had the luxury when at school that we had a very strong (free) stats consulting service. I was the envy of several friends at other universities and I suspect we (many depts of the university) turned out better work.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: angelo.arcadi at virgilio.it
> Sent: Tue, 21 Jul 2015 12:12:58 +0200 (CEST)
> To: lists at dewey.myzen.co.uk, bgunter.4567 at gmail.com
> Subject: [R] R: Re: R: Re: R: Re: Differences in output of lme() when
> introducing interactions
> 
> Dear Michael,
> thanks a lot. I am studying the marginality and I came across to this
> post:
> 
> http://www.ats.ucla.edu/stat/r/faq/type3.htm
> 
> Do you think that the procedure there described is the right one to solve
> my problem?
> 
> Would you have any other online resources to suggest especially dealing
> with R?
> 
> My department does not have a statician, so I have to find a solution
> with my own capacities.
> 
> Thanks in advance
> 
> Angelo
> 
> 
> 
> 
> ----Messaggio originale----
> Da: lists at dewey.myzen.co.uk
> Data: 21-lug-2015 11.58
> A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>,
> <bgunter.4567 at gmail.com>
> Cc: <r-help at r-project.org>
> Ogg: Re: R: Re: [R] R: Re: Differences in output of lme() when
> introducing interactions
> 
> Dear Angelo
> 
> I suggest you do an online search for marginality which may help to
> explain the relationship between main effects and interactions. As I
> said in my original email this is a complicated subject which we are not
> going to retype for you.
> 
> If you are doing this as a student I suggest you sue your university for
> failing to train you appropriately and if it is part of your employment
> I suggest you find a better employer.
> 
> On 21/07/2015 10:04, angelo.arcadi at virgilio.it wrote:
>> Dear Bert,
>> thank you for your feedback. Can you please provide some references
>> online so I can improve "my ignorance"?
>> Anyways, please notice that it is not true that I do not know statistics
>> and regressions at all, and I am strongly
>> convinced that my question can be of interest for some one else in the
>> future.
>> 
>> This is what forums serve for, isn't it? This is why people help each
>> other, isn't it?
>> 
>> Moreover, don't you think that I would not have asked to this R forum if
>> I had the possibility to ask or pay a statician?
>> Don't you think I have done already my best to study and learn before
>> posting this message? Trust me, I have read different
>> online tutorials on lme and lmer, and I am confident that I have got the
>> basic concepts. Still I have not found the answer
>> to solve my problem, so if you know the answer can you please give me
>> some suggestions that can help me?
>> 
>> I do not have a book where to learn and unfortunately I have to analyze
>> the results soon. Any help? Any online reference to-the-point
>> that can help me in solving this problem?
>> 
>> Thank you in advance
>> 
>> Best regards
>> 
>> Angelo
>> 
>> 
>>     ----Messaggio originale----
>>     Da: bgunter.4567 at gmail.com
>>     Data: 21-lug-2015 3.45
>>     A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>
>>     Cc: <lists at dewey.myzen.co.uk>, <r-help at r-project.org>
>>     Ogg: Re: [R] R: Re: Differences in output of lme() when introducing
>>     interactions
>> 
>>     I believe Michael's point is that you need to STOP asking such
>>     questions and START either learning some statistics or work with
>>     someone who already knows some. You should not be doing such
>> analyses
>>     on your own given your present state of statistical ignorance.
>> 
>>     Cheers,
>>     Bert
>> 
>> 
>>     Bert Gunter
>> 
>>     "Data is not information. Information is not knowledge. And
>> knowledge
>>     is certainly not wisdom."
>>         -- Clifford Stoll
>> 
>> 
>>     On Mon, Jul 20, 2015 at 5:45 PM, angelo.arcadi at virgilio.it
>>     <angelo.arcadi at virgilio.it> wrote:
>>      > Dear Michael,
>>      > thanks for your answer. Despite it answers to my initial
>>     question, it does not help me in finding the solution to my problem
>>     unfortunately.
>>      >
>>      > Could you please tell me which analysis of the two models should
>>     I trust then?
>>      > My goal is to know whether participants? choices
>>      >  of the dependent variable are linearly related to their own
>>     weight, height, shoe size and
>>      >  the combination of those effects.
>>      > Would the analysis of model 2 be more
>>      > correct than that of model 1? Which of the two analysis should I
>>     trust according to my goal?
>>      > What is your recommendation?
>>      >
>>      >
>>      > Thanks in advance
>>      >
>>      > Angelo
>>      >
>>      >
>>      >
>>      >
>>      >
>>      > ----Messaggio originale----
>>      > Da: lists at dewey.myzen.co.uk
>>      > Data: 20-lug-2015 17.56
>>      > A: "angelo.arcadi at virgilio.it"<angelo.arcadi at virgilio.it>,
>>     <r-help at r-project.org>
>>      > Ogg: Re: [R] Differences in output of lme() when introducing
>>     interactions
>>      >
>>      > In-line
>>      >
>>      > On 20/07/2015 15:10, angelo.arcadi at virgilio.it wrote:
>>      >> Dear List Members,
>>      >>
>>      >>
>>      >>
>>      >> I am searching for correlations between a dependent variable and
>> a
>>      >> factor or a combination of factors in a repeated measure design.
>>     So I
>>      >> use lme() function in R. However, I am getting very different
>>     results
>>      >> depending on whether I add on the lme formula various factors
>>     compared
>>      >> to when only one is present. If a factor is found to be
>> significant,
>>      >> shouldn't remain significant also when more factors are
>>     introduced in
>>      >> the model?
>>      >>
>>      >
>>      > The short answer is 'No'.
>>      >
>>      > The long answer is contained in any good book on statistics which
>> you
>>      > really need to have by your side as the long answer is too long
>> to
>>      > include in an email.
>>      >
>>      >>
>>      >> I give an example of the outputs I get using the two models. In
>>     the first model I use one single factor:
>>      >>
>>      >> library(nlme)
>>      >> summary(lme(Mode ~ Weight, data = Gravel_ds, random = ~1 |
>> Subject))
>>      >> Linear mixed-effects model fit by REML
>>      >>   Data: Gravel_ds
>>      >>        AIC      BIC   logLik
>>      >>    2119.28 2130.154 -1055.64
>>      >>
>>      >> Random effects:
>>      >>   Formula: ~1 | Subject
>>      >>          (Intercept) Residual
>>      >> StdDev:    1952.495 2496.424
>>      >>
>>      >> Fixed effects: Mode ~ Weight
>>      >>                  Value Std.Error DF   t-value p-value
>>      >> (Intercept) 10308.966 2319.0711 95  4.445299   0.000
>>      >> Weight        -99.036   32.3094 17 -3.065233   0.007
>>      >>   Correlation:
>>      >>         (Intr)
>>      >> Weight -0.976
>>      >>
>>      >> Standardized Within-Group Residuals:
>>      >>          Min          Q1         Med          Q3         Max
>>      >> -1.74326719 -0.41379593 -0.06508451  0.39578734  2.27406649
>>      >>
>>      >> Number of Observations: 114
>>      >> Number of Groups: 19
>>      >>
>>      >>
>>      >> As you can see the p-value for factor Weight is significant.
>>      >> This is the second model, in which I add various factors for
>>     searching their correlations:
>>      >>
>>      >> library(nlme)
>>      >> summary(lme(Mode ~ Weight*Height*Shoe_Size*BMI, data =
>>     Gravel_ds, random = ~1 | Subject))
>>      >> Linear mixed-effects model fit by REML
>>      >>   Data: Gravel_ds
>>      >>         AIC      BIC    logLik
>>      >>    1975.165 2021.694 -969.5825
>>      >>
>>      >> Random effects:
>>      >>   Formula: ~1 | Subject
>>      >>          (Intercept) Residual
>>      >> StdDev:    1.127993 2494.826
>>      >>
>>      >> Fixed effects: Mode ~ Weight * Height * Shoe_Size * BMI
>>      >>                                  Value Std.Error DF    t-value
>>     p-value
>>      >> (Intercept)                   5115955  10546313 95  0.4850941
>>     0.6287
>>      >> Weight                      -13651237   6939242  3 -1.9672518
>>     0.1438
>>      >> Height                         -18678     53202  3 -0.3510740
>>     0.7487
>>      >> Shoe_Size                       93427    213737  3  0.4371115
>>     0.6916
>>      >> BMI                         -13011088   7148969  3 -1.8199949
>>     0.1663
>>      >> Weight:Height                   28128     14191  3  1.9820883
>>     0.1418
>>      >> Weight:Shoe_Size               351453    186304  3  1.8864467
>>     0.1557
>>      >> Height:Shoe_Size                 -783      1073  3 -0.7298797
>>     0.5183
>>      >> Weight:BMI                      19475     11425  3  1.7045450
>>     0.1868
>>      >> Height:BMI                     226512    118364  3  1.9136867
>>     0.1516
>>      >> Shoe_Size:BMI                  329377    190294  3  1.7308827
>>     0.1819
>>      >> Weight:Height:Shoe_Size          -706       371  3 -1.9014817
>>     0.1534
>>      >> Weight:Height:BMI                -109        63  3 -1.7258742
>>     0.1828
>>      >> Weight:Shoe_Size:BMI             -273       201  3 -1.3596421
>>     0.2671
>>      >> Height:Shoe_Size:BMI            -5858      3200  3 -1.8306771
>>     0.1646
>>      >> Weight:Height:Shoe_Size:BMI         2         1  3  1.3891782
>>     0.2589
>>      >>   Correlation:
>>      >>                              (Intr) Weight Height Sho_Sz BMI
>>     Wght:H Wg:S_S Hg:S_S Wg:BMI Hg:BMI S_S:BM Wg:H:S_S W:H:BM W:S_S:
>> H:S_S:
>>      >> Weight                      -0.895
>>      >> Height                      -0.996  0.869
>>      >> Shoe_Size                   -0.930  0.694  0.933
>>      >> BMI                         -0.911  0.998  0.887  0.720
>>      >> Weight:Height                0.894 -1.000 -0.867 -0.692 -0.997
>>      >> Weight:Shoe_Size             0.898 -0.997 -0.873 -0.700 -0.999
>>     0.995
>>      >> Height:Shoe_Size             0.890 -0.612 -0.904 -0.991 -0.641
>>     0.609  0.619
>>      >> Weight:BMI                   0.911 -0.976 -0.887 -0.715 -0.972
>>     0.980  0.965  0.637
>>      >> Height:BMI                   0.900 -1.000 -0.875 -0.703 -0.999
>>     0.999  0.999  0.622  0.973
>>      >> Shoe_Size:BMI                0.912 -0.992 -0.889 -0.726 -0.997
>>     0.988  0.998  0.649  0.958  0.995
>>      >> Weight:Height:Shoe_Size     -0.901  0.999  0.876  0.704  1.000
>>     -0.997 -1.000 -0.623 -0.971 -1.000 -0.997
>>      >> Weight:Height:BMI           -0.908  0.978  0.886  0.704  0.974
>>     -0.982 -0.968 -0.627 -0.999 -0.975 -0.961  0.973
>>      >> Weight:Shoe_Size:BMI        -0.949  0.941  0.928  0.818  0.940
>>     -0.946 -0.927 -0.751 -0.980 -0.938 -0.924  0.935    0.974
>>      >> Height:Shoe_Size:BMI        -0.901  0.995  0.878  0.707  0.998
>>     -0.992 -1.000 -0.627 -0.960 -0.997 -0.999  0.999    0.964  0.923
>>      >> Weight:Height:Shoe_Size:BMI  0.952 -0.948 -0.933 -0.812 -0.947
>>     0.953  0.935  0.747  0.985  0.946  0.932 -0.943   -0.980 -0.999
>> -0.931
>>      >>
>>      >> Standardized Within-Group Residuals:
>>      >>          Min          Q1         Med          Q3         Max
>>      >> -2.03523736 -0.47889716 -0.02149143  0.41118126  2.20012158
>>      >>
>>      >> Number of Observations: 114
>>      >> Number of Groups: 19
>>      >>
>>      >>
>>      >> This time the p-value associated to Weight is not significant
>>     anymore. Why? Which analysis should I trust?
>>      >>
>>      >>
>>      >> In addition, while in the first output the field "value" (which
>>      >> should give me the slope) is -99.036 in the second output it is
>>      >> -13651237. Why they are so different? The one in the first
>>     output is the
>>      >>   one that seems definitively more reasonable to me.
>>      >> I would very grateful if someone could give me an answer
>>      >>
>>      >>
>>      >> Thanks in advance
>>      >>
>>      >>
>>      >> Angelo
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>
>>      >>       [[alternative HTML version deleted]]
>>      >>
>>      >> ______________________________________________
>>      >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>>      >> https://stat.ethz.ch/mailman/listinfo/r-help
>>      >> PLEASE do read the posting guide
>>     http://www.R-project.org/posting-guide.html
>>      >> and provide commented, minimal, self-contained, reproducible
>> code.
>>      >>
>>      >
>>      > --
>>      > Michael
>>      > http://www.dewey.myzen.co.uk/home.html
>>      >
>>      >
>>      >
>>      >
>>      >         [[alternative HTML version deleted]]
>>      >
>>      > ______________________________________________
>>      > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>      > https://stat.ethz.ch/mailman/listinfo/r-help
>>      > PLEASE do read the posting guide
>>     http://www.R-project.org/posting-guide.html
>>      > and provide commented, minimal, self-contained, reproducible
>> code.
>> 
>> 
> 
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From dulcalma at bigpond.com  Tue Jul 21 14:34:19 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Tue, 21 Jul 2015 22:34:19 +1000
Subject: [R]  plot auto key and text into panels using lattice
Message-ID: <001c01d0c3b1$917bc5e0$b47351a0$@bigpond.com>

Forgot to send to list 
Plus forgot to say that I appear to have  no problems with script below

Hi Luigi


It is helpful to use a MWE - you had several errors in the script.

try these  (adapting fig 5.11 from Deepayan Sarkar's book

data(SeatacWeather, package = "latticeExtra")
str(SeatacWeather)

MIN.X= 1
MAX.X = 31
MIN.Y = 15
MAX.Y = 70
LABELS <- paste("Well", 1:3)
TEXT_FOR_AUTOKEY <- paste("Label", 1:3)

xyplot(min.temp + max.temp ~ day | month, data = SeatacWeather,
       ylab = "Temperature and Rainfall",
       as.table = TRUE,
       type = "l",
       lty = 1,
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               layout = c(3,1),
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               col = 3,
               panel = function(x, y,...) {
                   panel.xyplot(x,y,...)
                   pnl = panel.number()

                   panel.text(MIN.X+(0.1*MAX.X),
                              MAX.Y-(0.1*MAX.Y),
                              labels=LABELS[panel.number()],
                              cex=0.3
                              )
               }
        )

xyplot(min.temp + max.temp ~ day | month, data = SeatacWeather,
       ylab = "Temperature and Rainfall",
       as.table = TRUE,
       type = "l",
       lty = 1,
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               layout = c(3,1),
            #   auto.key= list(space = "centre"),
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               col =  c(4,2),
               key = list(
                   text=list(TEXT_FOR_AUTOKEY, col="black"),
                   space="top", columns=2,
                   lines=list(col=c(4,2))
  ##                panel = panel.superpose #wrong place ; duplicated panel
function anyway
                    ),
               panel = function(x, y,...) {
               
                   panel.xyplot(x,y,...)
                   panel.text(MIN.X+(0.1*MAX.X),
                              MAX.Y-(0.1*MAX.Y),
                              labels=LABELS[panel.number()],
                              cex=0.3,
                              panel = panel.superpose
                              )

               }
        )

You cannot have key and auto.key

If you want to have 2 keys/ legends

see ?draw.key and legend argument  lattice::xyplot
https://stat.ethz.ch/pipermail/r-help/2005-February/066264.html

an easier way for text is 

library(grid)

xyplot(min.temp + max.temp ~ day | month, data = SeatacWeather,
       ylab = "Temperature and Rainfall",
       as.table = TRUE,
       type = "l",
       lty = 1,
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               layout = c(3,1),
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               col = 3,
               panel = function(x, y,...) {
                   panel.xyplot(x,y,...)
                   pnl = panel.number()
                  # possibly needs some fine tuning
                   grid.text(label=LABELS[panel.number()], x = unit(0.1,
"npc"), y = unit(0.9, "npc"), gp = gpar(cex = 0.3))
               }
        )

panel.number() used on its own can play up sometimes
better to save the value and use it.

If you are having problems start from the beginning with a minimal script eg

xyplot(min.temp + max.temp ~ day, data = SeatacWeather,
            as.table = T,
           layout = ...,
          groups = ...)

and build up from there. Its amazing what you find out.

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au





-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
Marongiu
Sent: Tuesday, 21 July 2015 08:39
To: r-help
Subject: [R] plot auto key and text into panels using lattice

Dear all,
I am writing some text into several panels which I can do with this
script (in capital the variables):
xyplot(Y ~ X | Z,
               data = DATAFRAME,
               groups = Z,
               ylab= "Y",
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               as.table = TRUE,
               layout = LAYOUT,
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               type = "l",
               col = 3,
               panel = function(x, y,...) {
                   panel.xyplot(x,y,...)
                   panel.text(MIN.X+(0.1*MAX.X),
                              MAX.Y-(0.1*MAX.Y),
                              labels=LABELS[panel.number()],
                              cex=0.3
                              )
               }
        )

A similar plot also add the autokey because it takes in account two
different Y values, but the plot is not drawn rather the function is
implemented but the plot remains empty:
xyplot(Y1+Y2 ~ X | Z,
               data = DATAFRAME,
               ylab= "Y",
               xlab="X",
               main="TITLE",
               scales = list(
                   x = list(draw = FALSE),
                   y = list(draw = FALSE),
                   relation="same",
                   alternating=TRUE),
               as.table = TRUE,
               layout = LAYOUT,
               auto.key= list(space = "centre"),
               par.settings = list(
                   strip.background=list(col="white"),
                   axis.text = list(cex = 0.6),
                   par.xlab.text = list(cex = 0.75),
                   par.ylab.text = list(cex = 0.75),
                   par.main.text = list(cex = 0.8),
                   superpose.symbol = list(pch = ".", cex = 1)
               ),
               strip    = FALSE,
               type = "l",
               col =  c(4,2),
               key = list(
                   space="top", columns=2,
                   text=list(TEXT_FOR_AUTOKEY, col="black"),
                   lines=list(col=c(4,2)),
                   panel = panel.superpose
                    ),
               panel = function(x, y,...)
               {
                   panel.xyplot(x,y,...)
                   panel.text(MIN.X+(0.1*MAX.X),
                              MAX.Y-(0.1*MAX.Y),
                              labels=LAB[panel.number()],
                              cex=0.3,
                              panel = panel.superpose
                              )

               }
        )


I am not attaching actual data because I believe the problem is in the
actual call, maybe I am using twice panel.superimpose, although
several combination I made (for instance moving the key argument into
the panel function) did not solve the problem.
Could you tell me where I am getting it wrong?
Thank you.
best regards
Luigi

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From emmanuel.levy at gmail.com  Tue Jul 21 15:21:42 2015
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Tue, 21 Jul 2015 16:21:42 +0300
Subject: [R] combining columns into a "combination index" of the same length
Message-ID: <CAMUS-MuZwZzNLonWFv_1uQwzzpQvEOjtodVQHceFUSb0Dr46pw@mail.gmail.com>

Hi,

The answer to this is probably straightforward, I have a dataframe and I'd
like to build an index of column combinations, e.g.

col1 col2  --> col3 (the index I need)
A     1           1
A     1           1
A     2           2
B     1           3
B     2           4
B     2           4


At the moment I use:
col3 <- apply(mat[,sel.col], 1, paste0)

But I wonder if another approach could be faster?

Thanks,

Emmanuel

	[[alternative HTML version deleted]]


From itsme at CorinaLogan.com  Tue Jul 21 11:20:03 2015
From: itsme at CorinaLogan.com (Corina)
Date: Tue, 21 Jul 2015 02:20:03 -0700 (PDT)
Subject: [R] Why does dredge() rank models differently for lmer() and
 MCMCglmm()?
In-Reply-To: <loom.20150718T165928-763@post.gmane.org>
References: <1437197412639-4710020.post@n4.nabble.com>
	<loom.20150718T165928-763@post.gmane.org>
Message-ID: <12AE139A-96D5-4246-B65D-891747BDBE21@corinalogan.com>

Hi Ben,
Thank you so much for your helpful advice! I changed the lmer model to REML=FALSE and the best-fitting model in dredge is MUCH more similar to what I found with MCMCglmm.

My best,
Corina

-- 
Dr. Corina Logan 
Leverhulme Early Career Research Fellow 
Department of Zoology
University of Cambridge
itsme at CorinaLogan.com 
www.CorinaLogan.com


On Jul 18, 2015, at 18:06, bbolker [via R] <ml-node+s789695n4710029h10 at n4.nabble.com> wrote:

Corina <itsme <at> CorinaLogan.com> writes: 

> 
> Hello, 
> I am running my full model (fm) through lmer() and MCMCglmm() using the 
> default settings: 
> 
> model.lmer <- lmer(fm) 
> model.MCMCglmm <- MCMCglmm(fm) 
> 

  [snip] 

> However, when I run the models through dredge(): 
> 
> dredge(model.lmer) 
> dredge(model.MCMCglmm) 
  
> It ranks the models very differently for lmer() and MCMCglmm() in 
> the model selection tables, even though I used the default settings 
> for dredge() as well (ranked by AICc). I would assume the difference 
> is because lmer() uses REML and MCMCglmm() uses Bayes Rule, however 
> if this is the case then why weren?t the summary outputs different 
> as well? 
  
  [snip] 

   This question *might* be better on [hidden email] <x-msg://2/user/SendEmail.jtp?type=node&node=4710029&i=0> ... 
You shouldn't be comparing models with fixed effects that are fitted 
with REML -- try  refitting with REML=FALSE.  The model comparison 
functions in the lme4 package (e.g. anova()) try to stop you from 
making this mistake, but I'm not sure the MuMIn::dredge does. 

______________________________________________ 
[hidden email] <x-msg://2/user/SendEmail.jtp?type=node&node=4710029&i=1> mailing list -- To UNSUBSCRIBE and more, see 
https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code. 

If you reply to this email, your message will be added to the discussion below:
http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020p4710029.html <http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020p4710029.html>
To unsubscribe from Why does dredge() rank models differently for lmer() and MCMCglmm()?, click here <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4710020&code=aXRzbWVAQ29yaW5hTG9nYW4uY29tfDQ3MTAwMjB8LTIwNjUwNjEyMzQ=>.
NAML <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
View this message in context: http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020p4710129.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From itsme at CorinaLogan.com  Tue Jul 21 11:22:34 2015
From: itsme at CorinaLogan.com (Corina)
Date: Tue, 21 Jul 2015 02:22:34 -0700 (PDT)
Subject: [R] Why does dredge() rank models differently for lmer() and
 MCMCglmm()?
In-Reply-To: <CAGxFJbTL+WCFe75afOHNtnejskNCawuE60YLx3KXKDPV-QVzvA@mail.gmail.com>
References: <1437197412639-4710020.post@n4.nabble.com>
	<loom.20150718T165928-763@post.gmane.org>
	<CAGxFJbTL+WCFe75afOHNtnejskNCawuE60YLx3KXKDPV-QVzvA@mail.gmail.com>
Message-ID: <A2F1A56C-9D97-442B-AB15-34A0AF53B9DC@corinalogan.com>

Hi Bert,
Thanks for the model selection philosophy! It?s definitely not a perfect world in terms of trying to understand the ?truth", but I find that there are so many opinions about how to do model selection that I just choose one and go with it :)

My best,
Corina

-- 
Dr. Corina Logan 
Leverhulme Early Career Research Fellow 
Department of Zoology
University of Cambridge
itsme at CorinaLogan.com 
www.CorinaLogan.com

On Jul 18, 2015, at 21:26, Bert Gunter-2 [via R] <ml-node+s789695n4710033h34 at n4.nabble.com> wrote:

(slightly off topic) 

One might also add that different model fitting criteria might produce 
rankings that are quite different, but with model predictions that are 
very similar. This reflects the inconvenient reality that empirical 
models are merely data interpolators and not representations of 
"truth." Ergo quite different looking models might fit essentially 
equally well, the differences in fits and therefore rankings being 
nothing more than noise. 

Cheers, 
Bert 
Bert Gunter 

"Data is not information. Information is not knowledge. And knowledge 
is certainly not wisdom." 
   -- Clifford Stoll 


On Sat, Jul 18, 2015 at 10:03 AM, Ben Bolker <[hidden email] <x-msg://4/user/SendEmail.jtp?type=node&node=4710033&i=0>> wrote:

> Corina <itsme <at> CorinaLogan.com> writes: 
> 
>> 
>> Hello, 
>> I am running my full model (fm) through lmer() and MCMCglmm() using the 
>> default settings: 
>> 
>> model.lmer <- lmer(fm) 
>> model.MCMCglmm <- MCMCglmm(fm) 
>> 
> 
>   [snip] 
> 
>> However, when I run the models through dredge(): 
>> 
>> dredge(model.lmer) 
>> dredge(model.MCMCglmm) 
> 
>> It ranks the models very differently for lmer() and MCMCglmm() in 
>> the model selection tables, even though I used the default settings 
>> for dredge() as well (ranked by AICc). I would assume the difference 
>> is because lmer() uses REML and MCMCglmm() uses Bayes Rule, however 
>> if this is the case then why weren?t the summary outputs different 
>> as well? 
> 
>   [snip] 
> 
>    This question *might* be better on [hidden email] <x-msg://4/user/SendEmail.jtp?type=node&node=4710033&i=1> ... 
> You shouldn't be comparing models with fixed effects that are fitted 
> with REML -- try  refitting with REML=FALSE.  The model comparison 
> functions in the lme4 package (e.g. anova()) try to stop you from 
> making this mistake, but I'm not sure the MuMIn::dredge does. 
> 
> ______________________________________________ 
> [hidden email] <x-msg://4/user/SendEmail.jtp?type=node&node=4710033&i=2> mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________ 
[hidden email] <x-msg://4/user/SendEmail.jtp?type=node&node=4710033&i=3> mailing list -- To UNSUBSCRIBE and more, see 
https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code. 

If you reply to this email, your message will be added to the discussion below:
http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020p4710033.html <http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020p4710033.html>
To unsubscribe from Why does dredge() rank models differently for lmer() and MCMCglmm()?, click here <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=4710020&code=aXRzbWVAQ29yaW5hTG9nYW4uY29tfDQ3MTAwMjB8LTIwNjUwNjEyMzQ=>.
NAML <http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
View this message in context: http://r.789695.n4.nabble.com/Why-does-dredge-rank-models-differently-for-lmer-and-MCMCglmm-tp4710020p4710130.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From micallefpierre at hotmail.com  Tue Jul 21 14:38:40 2015
From: micallefpierre at hotmail.com (Pierre Micallef)
Date: Tue, 21 Jul 2015 12:38:40 +0000
Subject: [R] barplot -issues with axis and labels not appearing
Message-ID: <DUB118-W359E7EA81A03A5C6A9DE74D3840@phx.gbl>

Hi
 
I am experiencing a few issues with the barplot function.
 
I have written the following code;
 
  barplot(as.matrix(GEP.data2), beside=TRUE, main="Global Portfolio Weights", col.main="gray", col=blues9, 
          cex.axis=0.1, ylim=c(-1,1), las=2, cex.lab=1, cex=0.8)
 
 
where;
> GEP.data2 =
  VGSIX.equity VUSTX.equity VGTSX.equity VFISX.equity VTSMX.equity VFITX.equity VEIEX.equity VIPSX.equity
1  -0.08645095   0.08991793   0.03548216         0.45         0.45         0.45   -0.1689109   -0.2200382 However i  am having the following issues; (1) neither x or y axis appear on my graph (bars appear as if they are floating)(2) no axis labels are showing up (3) chart title is too high on graph and is being cut off from view Please can you help solve these issues? Thanks Pierre 		 	   		  
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplot01.png
Type: image/png
Size: 2641 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150721/5dc4ab2c/attachment.png>

From thierry.onkelinx at inbo.be  Tue Jul 21 15:32:57 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 21 Jul 2015 15:32:57 +0200
Subject: [R] combining columns into a "combination index" of the same
	length
In-Reply-To: <CAMUS-MuZwZzNLonWFv_1uQwzzpQvEOjtodVQHceFUSb0Dr46pw@mail.gmail.com>
References: <CAMUS-MuZwZzNLonWFv_1uQwzzpQvEOjtodVQHceFUSb0Dr46pw@mail.gmail.com>
Message-ID: <CAJuCY5zw0otPBAX3nCNsLapMmYk0O8jNAMX7ST5MufLNDSg2eA@mail.gmail.com>

Yes. paste0() can work on vectors. So paste0(mat[, col1], mat[, col2])

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-21 15:21 GMT+02:00 Emmanuel Levy <emmanuel.levy at gmail.com>:

> Hi,
>
> The answer to this is probably straightforward, I have a dataframe and I'd
> like to build an index of column combinations, e.g.
>
> col1 col2  --> col3 (the index I need)
> A     1           1
> A     1           1
> A     2           2
> B     1           3
> B     2           4
> B     2           4
>
>
> At the moment I use:
> col3 <- apply(mat[,sel.col], 1, paste0)
>
> But I wonder if another approach could be faster?
>
> Thanks,
>
> Emmanuel
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Tue Jul 21 16:00:32 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Tue, 21 Jul 2015 16:00:32 +0200
Subject: [R] combining columns into a "combination index" of the same
	length
In-Reply-To: <CAMUS-Mv+zz8cP9B8z1ps7-TC_4je0fmNRroCjnwPjOcs5_0iog@mail.gmail.com>
References: <CAMUS-MuZwZzNLonWFv_1uQwzzpQvEOjtodVQHceFUSb0Dr46pw@mail.gmail.com>
	<CAJuCY5zw0otPBAX3nCNsLapMmYk0O8jNAMX7ST5MufLNDSg2eA@mail.gmail.com>
	<CAMUS-Mv+zz8cP9B8z1ps7-TC_4je0fmNRroCjnwPjOcs5_0iog@mail.gmail.com>
Message-ID: <CAJuCY5wG77h=-tXHi5D3dTdBppj5smYygL_A6T2EVSFkR80eAg@mail.gmail.com>

Please always keep the mailing list in cc.

If mat is a data.frame, then you can use do.call. Then the number of
columns doesn't matter.

do.call(paste, mtcars[, c("mpg", "cyl")])
do.call(paste, mtcars[, c("mpg", "cyl", "disp")])
do.call(paste, mtcars)


ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey

2015-07-21 15:43 GMT+02:00 Emmanuel Levy <emmanuel.levy at gmail.com>:

> Thanks! -- this is indeed much faster (plus I made a mistake, one has to
> use paste with the option collapse="".
>
> The thing is I'm looking for a solution *without paste*. The reason is
> that* there may be two or more columns*.
>
>
>
> On 21 July 2015 at 16:32, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
>> Yes. paste0() can work on vectors. So paste0(mat[, col1], mat[, col2])
>>
>> ir. Thierry Onkelinx
>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>> and Forest
>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>> Kliniekstraat 25
>> 1070 Anderlecht
>> Belgium
>>
>> To call in the statistician after the experiment is done may be no more
>> than asking him to perform a post-mortem examination: he may be able to say
>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>> The plural of anecdote is not data. ~ Roger Brinner
>> The combination of some data and an aching desire for an answer does not
>> ensure that a reasonable answer can be extracted from a given body of data.
>> ~ John Tukey
>>
>> 2015-07-21 15:21 GMT+02:00 Emmanuel Levy <emmanuel.levy at gmail.com>:
>>
>>> Hi,
>>>
>>> The answer to this is probably straightforward, I have a dataframe and
>>> I'd
>>> like to build an index of column combinations, e.g.
>>>
>>> col1 col2  --> col3 (the index I need)
>>> A     1           1
>>> A     1           1
>>> A     2           2
>>> B     1           3
>>> B     2           4
>>> B     2           4
>>>
>>>
>>> At the moment I use:
>>> col3 <- apply(mat[,sel.col], 1, paste0)
>>>
>>> But I wonder if another approach could be faster?
>>>
>>> Thanks,
>>>
>>> Emmanuel
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From emmanuel.levy at gmail.com  Tue Jul 21 17:01:31 2015
From: emmanuel.levy at gmail.com (Emmanuel Levy)
Date: Tue, 21 Jul 2015 18:01:31 +0300
Subject: [R] combining columns into a "combination index" of the same
	length
In-Reply-To: <CAJuCY5wG77h=-tXHi5D3dTdBppj5smYygL_A6T2EVSFkR80eAg@mail.gmail.com>
References: <CAMUS-MuZwZzNLonWFv_1uQwzzpQvEOjtodVQHceFUSb0Dr46pw@mail.gmail.com>
	<CAJuCY5zw0otPBAX3nCNsLapMmYk0O8jNAMX7ST5MufLNDSg2eA@mail.gmail.com>
	<CAMUS-Mv+zz8cP9B8z1ps7-TC_4je0fmNRroCjnwPjOcs5_0iog@mail.gmail.com>
	<CAJuCY5wG77h=-tXHi5D3dTdBppj5smYygL_A6T2EVSFkR80eAg@mail.gmail.com>
Message-ID: <CAMUS-MvH9mBQOUAkUZmiTZp1dzJKNb9xHCmkOus87k4+JnkWMA@mail.gmail.com>

Thanks Thierry, you made my day :)


On 21 July 2015 at 17:00, Thierry Onkelinx <thierry.onkelinx at inbo.be> wrote:

> Please always keep the mailing list in cc.
>
> If mat is a data.frame, then you can use do.call. Then the number of
> columns doesn't matter.
>
> do.call(paste, mtcars[, c("mpg", "cyl")])
> do.call(paste, mtcars[, c("mpg", "cyl", "disp")])
> do.call(paste, mtcars)
>
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> 2015-07-21 15:43 GMT+02:00 Emmanuel Levy <emmanuel.levy at gmail.com>:
>
>> Thanks! -- this is indeed much faster (plus I made a mistake, one has to
>> use paste with the option collapse="".
>>
>> The thing is I'm looking for a solution *without paste*. The reason is
>> that* there may be two or more columns*.
>>
>>
>>
>> On 21 July 2015 at 16:32, Thierry Onkelinx <thierry.onkelinx at inbo.be>
>> wrote:
>>
>>> Yes. paste0() can work on vectors. So paste0(mat[, col1], mat[, col2])
>>>
>>> ir. Thierry Onkelinx
>>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
>>> and Forest
>>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
>>> Kliniekstraat 25
>>> 1070 Anderlecht
>>> Belgium
>>>
>>> To call in the statistician after the experiment is done may be no more
>>> than asking him to perform a post-mortem examination: he may be able to say
>>> what the experiment died of. ~ Sir Ronald Aylmer Fisher
>>> The plural of anecdote is not data. ~ Roger Brinner
>>> The combination of some data and an aching desire for an answer does not
>>> ensure that a reasonable answer can be extracted from a given body of data.
>>> ~ John Tukey
>>>
>>> 2015-07-21 15:21 GMT+02:00 Emmanuel Levy <emmanuel.levy at gmail.com>:
>>>
>>>> Hi,
>>>>
>>>> The answer to this is probably straightforward, I have a dataframe and
>>>> I'd
>>>> like to build an index of column combinations, e.g.
>>>>
>>>> col1 col2  --> col3 (the index I need)
>>>> A     1           1
>>>> A     1           1
>>>> A     2           2
>>>> B     1           3
>>>> B     2           4
>>>> B     2           4
>>>>
>>>>
>>>> At the moment I use:
>>>> col3 <- apply(mat[,sel.col], 1, paste0)
>>>>
>>>> But I wonder if another approach could be faster?
>>>>
>>>> Thanks,
>>>>
>>>> Emmanuel
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>
>

	[[alternative HTML version deleted]]


From friendly at yorku.ca  Tue Jul 21 17:14:15 2015
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 21 Jul 2015 11:14:15 -0400
Subject: [R] calculate adjacent log odds for a table
Message-ID: <55AE61C7.50602@yorku.ca>

This is a question about array and data frame manipulation and 
calculation, in the
context of models for log odds in contingency tables.

I have a data frame representing a 3-way frequency table, of size 5 
(litter) x 2 (treatment) x 3 (deaths).
"Freq" is the frequency in each cell, and deaths is the response variable.

     Mice <-
     structure(list(litter = c(7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L,
     11L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 7L, 7L, 8L,
     8L, 9L, 9L, 10L, 10L, 11L, 11L), treatment = structure(c(1L,
     2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L,
     2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L), .Label = c("A",
     "B"), class = "factor"), deaths = structure(c(1L, 1L, 1L, 1L,
     1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
     3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), .Label = c("0", "1",
     "2+"), class = "factor"), Freq = c(58L, 75L, 49L, 58L, 33L, 45L,
     15L, 39L, 4L, 5L, 11L, 19L, 14L, 17L, 18L, 22L, 13L, 22L, 12L,
     15L, 5L, 7L, 10L, 8L, 15L, 10L, 15L, 18L, 17L, 8L)), .Names = 
c("litter",
     "treatment", "deaths", "Freq"), row.names = c(NA, 30L), class = 
"data.frame")


 From this, I want to calculate the log odds for adjacent categories of 
the last variable (deaths)
and have this value in a data frame with factors litter (5), treatment 
(2), and contrast (2), as detailed below.

The data can be seen in xtabs() form:

     mice.tab <- xtabs(Freq ~ litter + treatment + deaths, data=Mice)
     ftable(mice.tab)

                      deaths  0  1 2+
     litter treatment
     7      A                58 11  5
            B                75 19  7
     8      A                49 14 10
            B                58 17  8
     9      A                33 18 15
            B                45 22 10
     10     A                15 13 15
            B                39 22 18
     11     A                 4 12 17
            B                 5 15  8
     >


 From this, I want to calculate the (adjacent) log odds of 0 vs. 1 and 1 
vs.2+ deaths, which is easy in
array format,

     odds1 <- log(mice.tab[,,1]/mice.tab[,,2])  # contrast 0:1
     odds2 <- log(mice.tab[,,2]/mice.tab[,,3])  # contrast 1:2+

     odds1
           treatment
     litter          A          B
         7   1.6625477  1.3730491
         8   1.2527630  1.2272297
         9   0.6061358  0.7156200
         10  0.1431008  0.5725192
         11 -1.0986123 -1.0986123
     >

But, for analysis, I want to have these in a data frame, with factors 
litter, treatment and contrast
and a column, 'logodds' containing the entries in the odds1 and odds2 
tables, suitably strung out.

For this problem, the desired result is given by

 > result <- data.frame(expand.grid(litter=factor(7:11), 
treatment=c("A","B"), deaths=c("0:1", "1:2+")),
                        logodds=c(odds1, odds2))
 > result
    litter treatment deaths    logodds
1       7         A    0:1  1.6625477
2       8         A    0:1  1.2527630
3       9         A    0:1  0.6061358
4      10         A    0:1  0.1431008
5      11         A    0:1 -1.0986123
6       7         B    0:1  1.3730491
7       8         B    0:1  1.2272297
8       9         B    0:1  0.7156200
9      10         B    0:1  0.5725192
10     11         B    0:1 -1.0986123
11      7         A   1:2+  0.7884574
12      8         A   1:2+  0.3364722
13      9         A   1:2+  0.1823216
14     10         A   1:2+ -0.1431008
15     11         A   1:2+ -0.3483067
16      7         B   1:2+  0.9985288
17      8         B   1:2+  0.7537718
18      9         B   1:2+  0.7884574
19     10         B   1:2+  0.2006707
20     11         B   1:2+  0.6286087
 >


More generally, for an I x J x K table, where the last factor is the 
response, my desired result
is a data frame of IJ(K-1) rows, with adjacent log odds in a 'logodds' 
column, and ideally, I'd like
to have a general function to do this.

Note that if T is the 10 x 3 matrix of frequencies shown by ftable(), 
the calculation is essentially

log(T) %*% matrix(c(1, -1, 0,
                     0,  1, -1))
followed by reshaping and labeling.

Can anyone help with this?

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, Quantitative Methods
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:http://www.datavis.ca
Toronto, ONT  M3J 1P3 CANADA


From rshepard at appl-ecosys.com  Tue Jul 21 17:30:09 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Tue, 21 Jul 2015 08:30:09 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally [RESOLVED]
In-Reply-To: <12E5FD3CA16.00000DCEjrkrideau@inbox.com>
References: <alpine.lnx.2.11.1507201047240.1876@localhost>
	<alpine.lnx.2.11.1507201145510.1876@localhost>
	<cajucy5zjgvk9nkcdxagd5y=7gobwysqwbsdu1ipqvnsbyc28dw@mail.gmail.com>
	<cajucy5wiopuv2n3mphnlk4h=mmo=fod6unep=h=d3wdlqxjakw@mail.gmail.com>
	<12E5FD3CA16.00000DCEjrkrideau@inbox.com>
Message-ID: <alpine.LNX.2.11.1507210823360.15112@localhost>

On Mon, 20 Jul 2015, John Kane wrote:

> I have no idea wha that chunk is not working but I think you can get the
> same result using the old method Stick the following in an ERT box:
>
> <<set-ops, echo = FALSE>>=
> opts_chunk$set(echo = FALSE)
> @

John,

   My original response was too large (3 attached PDF files) and was held for
moderator approval which is no longer needed.

   I found the magic solution; looks awkward, but it works.

   Near the top of the document (well before any R code) I insert a knitr
chunk containing the directive, opts_chunk$set(echo=FALSE). This is
immediately followed by an ERT box as above. The double whammy prevents R
code from echoing ... except for the contents of the leading chunk itself.

   Adding the option, echo=FALSE, to that leading chunk prevents it from
displaying.

   Note that both the chunk with opts_chunk$set() and the ERT is required to
prevent R code from displaying. No chunk but ERT box does not stop the
echoing.

Thanks for the pointer,

Rich


From jrkrideau at inbox.com  Tue Jul 21 17:36:15 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 21 Jul 2015 07:36:15 -0800
Subject: [R] Knitr: setting echo = FALSE globally [RESOLVED]
In-Reply-To: <alpine.LNX.2.11.1507210823360.15112@localhost>
References: <alpine.lnx.2.11.1507201047240.1876@localhost>
	<alpine.lnx.2.11.1507201145510.1876@localhost>
	<cajucy5zjgvk9nkcdxagd5y=7gobwysqwbsdu1ipqvnsbyc28dw@mail.gmail.com>
	<12e5fd3ca16.00000dcejrkrideau@inbox.com>
	<cajucy5wiopuv2n3mphnlk4h=mmo=fod6unep=h=d3wdlqxjakw@mail.gmail.com>
Message-ID: <19F0F85DC1D.00000243jrkrideau@inbox.com>

Hi Rich,
It sounds like your approach with the chunks is equivalent to my approach. Somewhere I remember reading that the global chunk commands apply to everything 'after' the global command--which once one thinks about it is sensible . 

Glad it's working.  R +knitr does save a lot of time once you get it working.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: rshepard at appl-ecosys.com
> Sent: Tue, 21 Jul 2015 08:30:09 -0700 (PDT)
> To: r-help at r-project.org
> Subject: Re: [R] Knitr: setting echo = FALSE globally [RESOLVED]
> 
> On Mon, 20 Jul 2015, John Kane wrote:
> 
>> I have no idea wha that chunk is not working but I think you can get the
>> same result using the old method Stick the following in an ERT box:
>> 
>> <<set-ops, echo = FALSE>>=
>> opts_chunk$set(echo = FALSE)
>> @
> 
> John,
> 
>    My original response was too large (3 attached PDF files) and was held
> for
> moderator approval which is no longer needed.
> 
>    I found the magic solution; looks awkward, but it works.
> 
>    Near the top of the document (well before any R code) I insert a knitr
> chunk containing the directive, opts_chunk$set(echo=FALSE). This is
> immediately followed by an ERT box as above. The double whammy prevents R
> code from echoing ... except for the contents of the leading chunk
> itself.
> 
>    Adding the option, echo=FALSE, to that leading chunk prevents it from
> displaying.
> 
>    Note that both the chunk with opts_chunk$set() and the ERT is required
> to
> prevent R code from displaying. No chunk but ERT box does not stop the
> echoing.
> 
> Thanks for the pointer,
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Publish your photos in seconds for FREE
TRY IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if4


From rshepard at appl-ecosys.com  Tue Jul 21 17:43:13 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Tue, 21 Jul 2015 08:43:13 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally [RESOLVED]
In-Reply-To: <19F0F85DC1D.00000243jrkrideau@inbox.com>
References: <alpine.lnx.2.11.1507201047240.1876@localhost>
	<alpine.lnx.2.11.1507201145510.1876@localhost>
	<cajucy5zjgvk9nkcdxagd5y=7gobwysqwbsdu1ipqvnsbyc28dw@mail.gmail.com>
	<12e5fd3ca16.00000dcejrkrideau@inbox.com>
	<cajucy5wiopuv2n3mphnlk4h=mmo=fod6unep=h=d3wdlqxjakw@mail.gmail.com>
	<19F0F85DC1D.00000243jrkrideau@inbox.com>
Message-ID: <alpine.LNX.2.11.1507210841060.15112@localhost>

On Tue, 21 Jul 2015, John Kane wrote:

> It sounds like your approach with the chunks is equivalent to my approach.
> Somewhere I remember reading that the global chunk commands apply to
> everything 'after' the global command--which once one thinks about it is
> sensible .

John,

   Yes, it makes sense. So it should work if echo=FALSE is in a chunk _or_ in
the ERT box. Needing to be in both is not expected.

> Glad it's working. R +knitr does save a lot of time once you get it
> working.

   I'm looking forward to that time savings. :-)

Many thanks,

Rich


From WMORGAN at wooster.edu  Tue Jul 21 17:43:15 2015
From: WMORGAN at wooster.edu (William Morgan)
Date: Tue, 21 Jul 2015 15:43:15 +0000
Subject: [R] SPOT install error
Message-ID: <BE194EBB-2252-4E82-B6F4-9B0ADEDCF542@wooster.edu>

I?m having trouble installing the SPOT R package for microarray image analysis. I?m awaiting a response from the packager developer, but wanted to see if anyone here has successfully used CSIRO?s SPOT software.

I?ve been following the developer?s "Spot Installation Instructions for Linux and Unix" <http://www.hca-vision.com/Spot_Documentation/Spot_Install_UNIX.pdf>: 

- Downloaded the Spot package (Spot_3.2_x86_64.tar) to my hard disk.

- Started R: 

     R version 3.2.1 (2015-06-18) -- "World-Famous Astronaut"
     Copyright (C) 2015 The R Foundation for Statistical Computing
     Platform: x86_64-apple-darwin13.4.0 (64-bit)
     ?

 - As instructed, executed the command: 
     >install.packages("Spot_3.2_x86_64.tar.gz", CRAN = NULL)

  This produced:
     Warning in install.packages :
       package ?Spot_3.2_x86_64.tar.gz? is not available (for R version 3.2.1)

Presuming that R was looking for the package in a repository and not my hard disk, I modified the command:
     > install.packages("~/Downloads/Spot_3.2_x86_64.tar.gz", repos = NULL, type = "source")

  This produced:
     * installing *source* package ?Spot? ...
     ERROR: a 'NAMESPACE' file is required
     * removing ?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/Spot?
     Warning in install.packages :
       installation of package ?/Users/wmorgan/Downloads/Spot_3.2_x86_64.tar.gz? had non-zero exit status

(I also tried type = ?mac.binary? or ?mac.binary.maverick?, but this only produced an error message that it's is not a mac binary file.)


William R. Morgan, Ph.D.
The College of Wooster
Department of Biology
Wooster, OH 44691




From murdoch.duncan at gmail.com  Tue Jul 21 18:01:48 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 21 Jul 2015 12:01:48 -0400
Subject: [R] SPOT install error
In-Reply-To: <BE194EBB-2252-4E82-B6F4-9B0ADEDCF542@wooster.edu>
References: <BE194EBB-2252-4E82-B6F4-9B0ADEDCF542@wooster.edu>
Message-ID: <55AE6CEC.2050801@gmail.com>

On 21/07/2015 11:43 AM, William Morgan wrote:
> I?m having trouble installing the SPOT R package for microarray image analysis. I?m awaiting a response from the packager developer, but wanted to see if anyone here has successfully used CSIRO?s SPOT software.
> 
> I?ve been following the developer?s "Spot Installation Instructions for Linux and Unix" <http://www.hca-vision.com/Spot_Documentation/Spot_Install_UNIX.pdf>: 
> 
> - Downloaded the Spot package (Spot_3.2_x86_64.tar) to my hard disk.
> 
> - Started R: 
> 
>      R version 3.2.1 (2015-06-18) -- "World-Famous Astronaut"
>      Copyright (C) 2015 The R Foundation for Statistical Computing
>      Platform: x86_64-apple-darwin13.4.0 (64-bit)
>      ?
> 
>  - As instructed, executed the command: 
>      >install.packages("Spot_3.2_x86_64.tar.gz", CRAN = NULL)
> 
>   This produced:
>      Warning in install.packages :
>        package ?Spot_3.2_x86_64.tar.gz? is not available (for R version 3.2.1)
> 
> Presuming that R was looking for the package in a repository and not my hard disk, I modified the command:
>      > install.packages("~/Downloads/Spot_3.2_x86_64.tar.gz", repos = NULL, type = "source")
> 
>   This produced:
>      * installing *source* package ?Spot? ...
>      ERROR: a 'NAMESPACE' file is required
>      * removing ?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/Spot?
>      Warning in install.packages :
>        installation of package ?/Users/wmorgan/Downloads/Spot_3.2_x86_64.tar.gz? had non-zero exit status

NAMESPACE files have been required in R since R 3.0.0.  If that package
doesn't have one, it sounds as though it hasn't been actively
maintained.  Will definitely be a case of "some assembly required", not
to mention the usual "use at your own risk".

Duncan Murdoch

> 
> (I also tried type = ?mac.binary? or ?mac.binary.maverick?, but this only produced an error message that it's is not a mac binary file.)
> 
> 
> William R. Morgan, Ph.D.
> The College of Wooster
> Department of Biology
> Wooster, OH 44691
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From unwin at math.uni-augsburg.de  Tue Jul 21 18:23:07 2015
From: unwin at math.uni-augsburg.de (Antony Unwin)
Date: Tue, 21 Jul 2015 18:23:07 +0200
Subject: [R] R Course in Dublin (September 14-16)
Message-ID: <D522A433-C61A-47AE-B980-FBFA22E5C7CA@math.uni-augsburg.de>

Details at  

http://insightsc.ie/training/r-statistical-software/ <http://insightsc.ie/training/r-statistical-software/>

Antony Unwin
University of Augsburg, Germany and Insight Statistical Consulting, Dublin, Ireland
	[[alternative HTML version deleted]]


From rshepard at appl-ecosys.com  Tue Jul 21 16:37:16 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Tue, 21 Jul 2015 07:37:16 -0700 (PDT)
Subject: [R] Knitr: setting echo = FALSE globally
In-Reply-To: <12E5FD3CA16.00000DCEjrkrideau@inbox.com>
References: <alpine.lnx.2.11.1507201047240.1876@localhost>
	<alpine.lnx.2.11.1507201145510.1876@localhost>
	<cajucy5zjgvk9nkcdxagd5y=7gobwysqwbsdu1ipqvnsbyc28dw@mail.gmail.com>
	<cajucy5wiopuv2n3mphnlk4h=mmo=fod6unep=h=d3wdlqxjakw@mail.gmail.com>
	<12E5FD3CA16.00000DCEjrkrideau@inbox.com>
Message-ID: <alpine.LNX.2.11.1507210730370.15112@localhost>

On Mon, 20 Jul 2015, John Kane wrote:

> I have no idea wha that chunk is not working but I think you can get the
> same result using the old method Stick the following in an ERT ...
>
> <<set-ops, echo = FALSE>>=
> opts_chunk$set(echo = FALSE)
> @

> Heck, I've only been using LyX for 4-5 years and already I'm sounding
> crotchety.

John,

   Crotchety is appropriate for situations like this.

   The above _almost_ works. Attached three PDF files show the results of
tests, all with the LaTeX code inserted.

   sample-no-chunk.pdf: removing chunk2 with its contents,
opts_chunk$set(echo=FALSE), did not keep the R code from displaying.

   sample-chunk-2-above.pdf and sample-chunk-2-below.pdf: chunk2 was prior
to, and following, the LaTeX code and all other R code did not echo, only
the chunk2 code displayed.

Thanks,

Rich
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample-no-chunk.pdf
Type: application/pdf
Size: 69845 bytes
Desc: 
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150721/7bff7fa4/attachment.pdf>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample-chunk-2-above.pdf
Type: application/pdf
Size: 68938 bytes
Desc: 
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150721/7bff7fa4/attachment-0001.pdf>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample-chunk-2-below.pdf
Type: application/pdf
Size: 68938 bytes
Desc: 
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150721/7bff7fa4/attachment-0002.pdf>

From raquelgmendes at gmail.com  Tue Jul 21 18:28:34 2015
From: raquelgmendes at gmail.com (Raquel Mendes)
Date: Tue, 21 Jul 2015 17:28:34 +0100
Subject: [R] Kruskal Wallis and pos-hoc Kruskal Wallis
Message-ID: <CA+Ds2vs0hA9F8zq0_k6MU3Eo9iP7RnJkYbM_oQgNAb0xyPsvTg@mail.gmail.com>

Hi all,

I performed a Kruskal-Wallis test on 4 groups, 13 variables, using
kruskal.test. I then applied a pos-hoc on the variables significant at
p<0.05, using kruskalmc. The problem is that two of the significant
variables on the kruskal-wallis test (with p=0.03276 and p=0.03537) didn?t
showed significant diferences between any groups in the pos hoc test.

I'm fairly new to R and not exactly a statistic genius, so I don?t know if
I am doing something wrong or just misinterpreting the results.

I am missing something here? Is there some p adjustment in these tests that
I don?t now about?

Thank you,

Raquel Mendes

	[[alternative HTML version deleted]]


From abhinabaroy09 at gmail.com  Tue Jul 21 18:50:37 2015
From: abhinabaroy09 at gmail.com (Abhinaba Roy)
Date: Tue, 21 Jul 2015 22:20:37 +0530
Subject: [R] getURL not working in loop
Message-ID: <CANtKHPUnNM3+RtV_2U=WQ5QzDeupBx1SLCcGNvJYTT5FPzR33g@mail.gmail.com>

Hi R helpers,

I am trying to extract customer feedback from an e-commerce site and
subsequently use it for creating a word cloud. Below is the code I have

#web-crawling
library(RCurl)
library(XML)
library(rvest)

#web-crawling
init="
http://www.flipkart.com/moto-g-2nd-generation/product-reviews/ITME7YBANGAWQZZX?pid=MOBDYGZ6SHNB7RFC&type=all
"
crawlcandidate="start="
base="http://www.flipkart.com"
num=10

doclist=list()
anchorlist=vector()

j=0

while(j<num){
  print(j)
  if(j==0){
    doclist[j+1]=getURL(init)
  }else{
    doclist[j+1]=getURL(paste(base,anchorlist[j+1],sep=""))
  }
  doc=htmlParse(doclist[[j+1]])
  anchor=getNodeSet(doc,"//a")
  anchor=sapply(anchor,function(x)xmlGetAttr(x,"href"))
  anchor=anchor[grep(crawlcandidate,anchor)]
  anchorlist=c(anchorlist,anchor)
  anchorlist=unique(anchorlist)
  j=j+1
}

#html_text is for extracting only reviews and ratings
reviews=c()
ratings=c()
for(i in 1:10){
  doc=htmlParse(doclist[[i]])
  l=getNodeSet(doc,"//div/p/span[@class='review-text']")
  l1=html_text(l)
  rateNodes=getNodeSet(doc,"//div[@class='fk-stars']")
  rates=sapply(rateNodes,function(x)xmlGetAttr(x,'title'))
  ratings=c(ratings,rates)
  reviews=c(reviews,l1)
}
View(reviews)
View(ratings)

#creating wordcloud
#tm,wordcloud
corpus=Corpus(VectorSource(reviews[1:100]))
corpus=tm_map(corpus,tolower)
corpus=tm_map(corpus,removePunctuation)
corpus=tm_map(corpus,removeNumbers)
corpus=tm_map(corpus,removeWords,stopwords("en"))
corpus=Corpus(VectorSource(corpus))
tdm=TermDocumentMatrix(corpus)
m=as.matrix(tdm)
v=sort(rowSums(m),decreasing=T)
d=data.frame(words=names(v),freq=v)
wordcloud(d$words,d$freq,max.words=300,colors=brewer.pal(10,"Dark2"),scale=c(3,0.5),random.order=F)

But I am getting the error

Error in which(value == defs) :
  argument "code" is missing, with no default
In addition: Warning message:
XML content does not seem to be XML: ''

How can I resolve this error??

Help will be appreciated.

Regards,
Abhi

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Jul 21 18:51:53 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 21 Jul 2015 09:51:53 -0700
Subject: [R] Kruskal Wallis and pos-hoc Kruskal Wallis
In-Reply-To: <CA+Ds2vs0hA9F8zq0_k6MU3Eo9iP7RnJkYbM_oQgNAb0xyPsvTg@mail.gmail.com>
References: <CA+Ds2vs0hA9F8zq0_k6MU3Eo9iP7RnJkYbM_oQgNAb0xyPsvTg@mail.gmail.com>
Message-ID: <CAGxFJbRAdfpbpxBkGn+vtsrW+wEJgG3Bb=Azzftud1TN9bVf+w@mail.gmail.com>

Yes you are missing something -- the underlying ideas of post hoc
tests. This is a statistics issue, not an R issue, and so off topic
here. Try posting on a statistics forum like stats.stackexchange.com
instead.


Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Jul 21, 2015 at 9:28 AM, Raquel Mendes <raquelgmendes at gmail.com> wrote:
> Hi all,
>
> I performed a Kruskal-Wallis test on 4 groups, 13 variables, using
> kruskal.test. I then applied a pos-hoc on the variables significant at
> p<0.05, using kruskalmc. The problem is that two of the significant
> variables on the kruskal-wallis test (with p=0.03276 and p=0.03537) didn?t
> showed significant diferences between any groups in the pos hoc test.
>
> I'm fairly new to R and not exactly a statistic genius, so I don?t know if
> I am doing something wrong or just misinterpreting the results.
>
> I am missing something here? Is there some p adjustment in these tests that
> I don?t now about?
>
> Thank you,
>
> Raquel Mendes
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From aalonso at rcumariacristina.com  Tue Jul 21 19:46:31 2015
From: aalonso at rcumariacristina.com (AGUSTIN ALONSO RODRIGUEZ)
Date: Tue, 21 Jul 2015 19:46:31 +0200
Subject: [R] Cannot install contributed package MTS
Message-ID: <000301d0c3dd$2ddcfaf0$8996f0d0$@rcumariacristina.com>

Dear Sirs:

 

I have attempted to install the contributed package MTS, but without
success.

 

I am using R.3.2.1, on Windows 8.1, after selecting Espa?ol as the
instalation language, on a Lenovo PC, 64 bits.

 

If I use the instruction:  install.packages(?MTS?)

 

i get:

 

> install.packages("MTS")

Installing package into ?C:/Users/Usuario/Documents/R/win-library/3.2?

(as ?lib? is unspecified)

Warning: unable to access index for repository
https://cran.r-project.org/src/contrib

Warning: unable to access index for repository
https://cran.r-project.org/bin/windows/contrib/3.2

Warning message:

package ?MTS? is not available (for R version 3.2.1) 

 

If I use the men? bar: Packages, install packages, after selecting a CRAN
mirror, I get  a window with the packages:

GLMMGibbs

RDCOMClient

Rsterm

Survnnet

yags

 

As I do not select any of the above, I get 

 

> utils:::menuInstallPkgs()

--- Please select a CRAN mirror for use in this session ---

Warning: unable to access index for repository
https://cran.r-project.org/src/contrib

Error in install.packages(NULL, .libPaths()[1L], dependencies = NA, type =
type) : 

  no packages were specified

 

 

I have been using R for more than fifteen years. I never had a problem
installing packages, and I am so confused that  do not know what to do.

 

Would you, please, help me?

 

Thanking you for your attention, I am sincerely yours

 

Agustin Alonso


	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue Jul 21 21:41:32 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 21 Jul 2015 15:41:32 -0400
Subject: [R] Cannot install contributed package MTS
In-Reply-To: <000301d0c3dd$2ddcfaf0$8996f0d0$@rcumariacristina.com>
References: <000301d0c3dd$2ddcfaf0$8996f0d0$@rcumariacristina.com>
Message-ID: <55AEA06C.5020208@gmail.com>

On 21/07/2015 1:46 PM, AGUSTIN ALONSO RODRIGUEZ wrote:
> Dear Sirs:
> 
>  
> 
> I have attempted to install the contributed package MTS, but without
> success.
> 
>  
> 
> I am using R.3.2.1, on Windows 8.1, after selecting Espa?ol as the
> instalation language, on a Lenovo PC, 64 bits.
> 
>  
> 
> If I use the instruction:  install.packages(?MTS?)
> 
>  
> 
> i get:
> 
>  
> 
>> install.packages("MTS")
> 
> Installing package into ?C:/Users/Usuario/Documents/R/win-library/3.2?
> 
> (as ?lib? is unspecified)
> 
> Warning: unable to access index for repository
> https://cran.r-project.org/src/contrib

The issue is that CRAN now prefers you to use https access, but your
system is having problems with that.  Since you are on a recent version
of Windows, the problem should go away if you use

setInternet2(TRUE)

This will be the default in R 3.2.2 (and is the default already in
R-patched).

Duncan Murdoch
> 
> Warning: unable to access index for repository
> https://cran.r-project.org/bin/windows/contrib/3.2
> 
> Warning message:
> 
> package ?MTS? is not available (for R version 3.2.1) 
> 
>  
> 
> If I use the men? bar: Packages, install packages, after selecting a CRAN
> mirror, I get  a window with the packages:
> 
> GLMMGibbs
> 
> RDCOMClient
> 
> Rsterm
> 
> Survnnet
> 
> yags
> 
>  
> 
> As I do not select any of the above, I get 
> 
>  
> 
>> utils:::menuInstallPkgs()
> 
> --- Please select a CRAN mirror for use in this session ---
> 
> Warning: unable to access index for repository
> https://cran.r-project.org/src/contrib
> 
> Error in install.packages(NULL, .libPaths()[1L], dependencies = NA, type =
> type) : 
> 
>   no packages were specified
> 
>  
> 
>  
> 
> I have been using R for more than fifteen years. I never had a problem
> installing packages, and I am so confused that  do not know what to do.
> 
>  
> 
> Would you, please, help me?
> 
>  
> 
> Thanking you for your attention, I am sincerely yours
> 
>  
> 
> Agustin Alonso
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Tue Jul 21 21:49:31 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 21 Jul 2015 12:49:31 -0700
Subject: [R] SPOT install error
In-Reply-To: <BE194EBB-2252-4E82-B6F4-9B0ADEDCF542@wooster.edu>
References: <BE194EBB-2252-4E82-B6F4-9B0ADEDCF542@wooster.edu>
Message-ID: <24C2692C-BF1E-4AE1-B9E7-7E74C34A27D5@comcast.net>


On Jul 21, 2015, at 8:43 AM, William Morgan wrote:

> I?m having trouble installing the SPOT R package for microarray image analysis. I?m awaiting a response from the packager developer, but wanted to see if anyone here has successfully used CSIRO?s SPOT software.
> 
> I?ve been following the developer?s "Spot Installation Instructions for Linux and Unix" <http://www.hca-vision.com/Spot_Documentation/Spot_Install_UNIX.pdf>: 
> 
> - Downloaded the Spot package (Spot_3.2_x86_64.tar) to my hard disk.
> 
> - Started R: 
> 
>     R version 3.2.1 (2015-06-18) -- "World-Famous Astronaut"
>     Copyright (C) 2015 The R Foundation for Statistical Computing
>     Platform: x86_64-apple-darwin13.4.0 (64-bit)
>     ?
> 
> - As instructed, executed the command: 
>> install.packages("Spot_3.2_x86_64.tar.gz", CRAN = NULL)
> 
>  This produced:
>     Warning in install.packages :
>       package ?Spot_3.2_x86_64.tar.gz? is not available (for R version 3.2.1)
> 
> Presuming that R was looking for the package in a repository and not my hard disk, I modified the command:
>> install.packages("~/Downloads/Spot_3.2_x86_64.tar.gz", repos = NULL, type = "source")
> 
>  This produced:
>     * installing *source* package ?Spot? ...
>     ERROR: a 'NAMESPACE' file is required

If your package does not have a NAMESPACE (which was acceptable several R versions ago) then you either need to revert to an older R from the same era as this was written, or you need to edit the source of the package to create a NAMESPACE.

-- 
David.

>     * removing ?/Library/Frameworks/R.framework/Versions/3.2/Resources/library/Spot?
>     Warning in install.packages :
>       installation of package ?/Users/wmorgan/Downloads/Spot_3.2_x86_64.tar.gz? had non-zero exit status
> 
> (I also tried type = ?mac.binary? or ?mac.binary.maverick?, but this only produced an error message that it's is not a mac binary file.)
> 
> 


David Winsemius
Alameda, CA, USA


From rhelpmaillist at 163.com  Tue Jul 21 23:37:31 2015
From: rhelpmaillist at 163.com (PO SU)
Date: Wed, 22 Jul 2015 05:37:31 +0800 (CST)
Subject: [R]  A strange problem using pls package
Message-ID: <29095cae.dad9.14eb28cd716.Coremail.rhelpmaillist@163.com>



Dear expeRts,


? ? ?Today i used the pls package to do the PLSR, ?i don' t know if anyone happen to have the same problem as me, it is so strange, my problem is:
?when i used codes like:
?suppose data has 20 columns
? traindata <- data[ 1:10, 1:10]
?testdata <- data[11:15,1:10]
? pls.fit <- plsr(y~x, ncomp = 5,?data = traindata, method= "simpls", scale = FALSE, model = TRUE, validation = "CV")
ok, i get some result, the srange thing happens when i redo the plsr, i mean, i use

?traindata <- data[ 1:10, 1:20]
?testdata <- data[11:15,1:20]
?pls.fit <- plsr(y~x, ncomp = 5,?data = traindata, method= "simpls", scale = FALSE, model = TRUE, validation = "CV")


I get the same result as the first one!!! ?i explore it a lot, even restart my computer, but still the same, however i changed the data, the result is the same!!
I think the pls package must write the result into some files, and ?read it back ~~ but i can't say~~ PLZ help me if you happen to know it





--

PO SU
mail: desolator88 at 163.com 
Majored in Statistics from SJTU

From marammagdysalem at gmail.com  Tue Jul 21 23:40:56 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Tue, 21 Jul 2015 23:40:56 +0200
Subject: [R] Warning message with maxLik()
In-Reply-To: <CAMTWbJh0LJ6aQLg8z-gD-DSWQ8A4PcgyAOp941LLmrf+jL5jCA@mail.gmail.com>
References: <CAPLSCn0v=xZiDGzZZcfQNFe277fvEKO7OowTYgHzJ0mnb5VGPw@mail.gmail.com>
	<CAMTWbJh0LJ6aQLg8z-gD-DSWQ8A4PcgyAOp941LLmrf+jL5jCA@mail.gmail.com>
Message-ID: <E0909349-EAF6-4476-A6B0-F780A8A9AF77@gmail.com>

Dear Arne,

The elements of the theta vector are indeed strictly positive. I've just tried to use instead : lamda = log (theta), which means that theta = exp (lamda),  so as to get rid of the log() function that appears in the log-likelihood and is causing the 50 warnings, but still the estimates I got for lamda and then those I got for theta (using theta=exp(lamda)) are irrelvant and their standard errors are infinite, which means that therer is still a problem that I can't yet figure out.

Thanks,
Maram

> On 18 July 2015 at 08:01, Arne Henningsen <arne.henningsen at gmail.com> wrote:
> Dear Maram
> 
> - Please do not start a new thread for the same issue but reply to
> previous messages in this thread [1].
> 
> - Please read my previous responses [1] more carefully, e.g. to use
> "theta <- exp( param )" which guarantees that all elements of "theta"
> are always positive.
> 
> [1] http://r.789695.n4.nabble.com/NaN-produced-from-log-with-positive-input-td4709463.html
> 
> Best regards,
> Arne
> 
> 
> 
> 2015-07-18 2:46 GMT+02:00 Maram SAlem <marammagdysalem at gmail.com>:
> > Dear All,
> > I'm trying to get the MLe for a certain distribution using maxLik ()
> > function. I wrote the log-likelihood function as follows:
> > theta <-vector(mode = "numeric", length = 3)
> > r<- 17
> > n <-30
> >  T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> > C<-
> > c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> > # The  loglik. func.
> > loglik <- function(param) {
> >  theta[1]<- param[1]
> >  theta[2]<- param[2]
> >  theta[3]<- param[3]
> >  l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+
> > (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+
> > (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
> > return(l)
> >  }
> >
> > then, I evaluated it at theta<- c(40,50,2)
> >
> > v<-loglik(param=theta)
> > v
> > [1] -56.66653
> >
> > I used this same log-likelihood function, once with analytic gradient and
> > another time with numerical one, with the maxLik function, and in both
> > cases I got the same 50 warning messages and an MLE which is completely
> > unrealistic as per my applied example.
> >
> > a <- maxLik(loglik, gradlik, hesslik, start=c(40,50,2))
> >
> > where gradlik and hesslik are the analytic gradient and Hessian matrix,
> > respectively, given by:
> >
> > U <- vector(mode="numeric",length=3)
> > gradlik<-function(param = theta,n, T,C)
> >  {
> > U <- vector(mode="numeric",length=3)
> > theta[1] <- param[1]
> > theta[2] <- param[2]
> > theta[3] <- param[3]
> > r<- 17
> > n <-30
> > T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> > C<-
> > c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> >  U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+(
> > -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
> > (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+
> > (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
> > (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+
> > (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
> > return(U)
> > }
> > hesslik<-function(param=theta,n,T,C)
> > {
> > theta[1] <- param[1]
> > theta[2] <- param[2]
> > theta[3] <- param[3]
> > r<- 17
> > n <-30
> > T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
> > C<-
> > c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
> > G<- matrix(nrow=3,ncol=3)
> > G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+
> > (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
> > theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> > G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+
> > (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+
> > (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> > G[2,1]<-G[1,2]
> > G[1,3]<-(n/theta[1])+(-1)*sum(
> > (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > G[3,1]<-G[1,3]
> > G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+
> > (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
> > theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
> > G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
> > G[3,2]<-G[2,3]
> > G[3,3]<-((-1*r)/(theta[3])^2)
> > return(G)
> > }
> >
> > and using numeric gradient and hessian matrix:
> >
> > a <- maxLik(loglik, start=c(40,50,2))
> > Warning messages:
> > 1: In log(theta[3]) : NaNs produced
> > 2: In log(theta[1] + theta[2]) : NaNs produced
> > 3: In log(theta[1]) : NaNs produced
> > 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
> > produced
> > 5: In log((C * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
> > produced
> > 6: In log(theta[3]) : NaNs produced
> > 7: In log(theta[1] + theta[2]) : NaNs produced
> > and so on?..
> >
> > I don't know why I get these 50 warnings although:
> > 1- The inputs of the log() function are strictly positive.
> > 2- When I evaluated the log-likelihood fuction at the very begining it gave
> > me a number(which is -56.66) and not (NAN).
> >
> > I've also tried to:
> > 1- Reparamtrize my model using lamda(i)= log(theta(i)), for i=1,2,3, so
> > that it may solve the problem, but it didn't.
> > 2- I've used the comparederivitive() function, and the analytic and numeric
> > gradients were so close.
> >
> > Any help please?
> > Maram Salem
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> --
> Arne Henningsen
> http://www.arne-henningsen.name


	[[alternative HTML version deleted]]


From l.shulman at hotmail.com  Tue Jul 21 19:39:45 2015
From: l.shulman at hotmail.com (lstat)
Date: Tue, 21 Jul 2015 10:39:45 -0700 (PDT)
Subject: [R] How to make a persistence landscape in R?
Message-ID: <1437500385524-4710159.post@n4.nabble.com>

Hi there, 
I am trying to construct a persistence landscape in R that shows all of the
overlapping triangles -- not just the overall silhouette-- how can you do
this?
Everytime I plug in the code I get one big triangle that corresponds to the
largest barcode, but I cannot see any of the smaller barcodes/ overlapping
isosceles trianges. 
Thanks!



--
View this message in context: http://r.789695.n4.nabble.com/How-to-make-a-persistence-landscape-in-R-tp4710159.html
Sent from the R help mailing list archive at Nabble.com.


From matthew.jones at kellogg.ox.ac.uk  Tue Jul 21 20:56:58 2015
From: matthew.jones at kellogg.ox.ac.uk (matthewjones43)
Date: Tue, 21 Jul 2015 11:56:58 -0700 (PDT)
Subject: [R] glm help - final predictor variable NA
Message-ID: <1437505018036-4710161.post@n4.nabble.com>

Hi, I am not a statistician and so I am sure whatever it is I am doing wrong
must be an obvious error for those who are...Basically I can not understand
why I get NA for variable 'CDSTotal' when running a glm? Does anyone have an
idea of why this might be happening?

Call:  glm(formula = cbind(SRAS - 26, 182 - SRAS) ~ Age + Gender + LOC + 
    PC + Stability + CDSTotal, family = binomial, data = Controlgroup)

Coefficients:
(Intercept)          Age       Gender          LOC           PC    Stability  
  -2.575071     0.009148     0.354143     0.018295    -0.011317     0.090759  
   CDSTotal  
         NA  

Degrees of Freedom: 64 Total (i.e. Null);  59 Residual
Null Deviance:	    2015 
Residual Deviance: 1264 	AIC: 1614

Thanks
Matthew



--
View this message in context: http://r.789695.n4.nabble.com/glm-help-final-predictor-variable-NA-tp4710161.html
Sent from the R help mailing list archive at Nabble.com.


From klerer at sxmail.de  Tue Jul 21 20:55:40 2015
From: klerer at sxmail.de (klerer at sxmail.de)
Date: Tue, 21 Jul 2015 20:55:40 +0200
Subject: [R] R version update
Message-ID: <a5605b357787a0fb7fc2fcee8242dd36@www.sxmail.de>

Dear Ladies and Gentlemen,
as a beginner in R, I encountered a sort of "naturally given limits"concerning the process of amending R with packages.
I apparently own version 3.0.2 and I principally decided to use R via
the RKWard GUI on Linux Kubuntu Trusty Tahr; the R version installed on
my computer is admittedly not far from the basics provided by my
distribution's package management (also throughout multiple rounds of
updates/ upgrades of the system).
What would I have to do to finally update/ upgrade beyond that R
version? Would that process (generally?) affect any/ my GUI?
This problem appeared to me as I was busy installing packages an
received the error message "package ?ATLAS? is not available (for R
version 3.0.2) ".
I am quite desperate and would look forward to be indicated a path to a
sustainable solution or be told how to mitigate/ circumvent (these/
such) problems.
Thanks a lot!

Best regards,Markus Hofstetter


	[[alternative HTML version deleted]]


From prof7bit at gmail.com  Tue Jul 21 21:28:33 2015
From: prof7bit at gmail.com (Bernd)
Date: Tue, 21 Jul 2015 21:28:33 +0200
Subject: [R] problems with rgp,
	examples from documentation won't produce expected results
Message-ID: <CAAkg2-nr=86ahhzScFVGDANq-hBWzxphmoCfCHztqrCFhK_MCg@mail.gmail.com>

I am currently playing with the rgp package, trying to find out how
the symbolic regression works but I am having trouble getting any
reasonable results.

I have found this pdf:
https://cran.r-project.org/web/packages/rgp/vignettes/rgp_introduction.pdf

and am currently trying to follow the pendulum example in chapter 4.3
(after I failed to make *any* of my own experiments produce any
results at all).

>From the PDF in chapter 4.3 I have copy-pasted the relevant code
snippets into one file, now I have the following:


# -----8<-----8<-----8<---- cut here

library("rgp")

set.seed(1)

makeDampedPendulum <- function(A0 = 1, g = 9.81, l = 0.1, phi = pi,
gamma = 0.5) {
  omega <- sqrt(g/l)
  function(t) A0 * exp(-gamma * t) * cos(omega * t + phi)
}

pendulum1 <- makeDampedPendulum(l = 0.5)

xs1 <- seq(from = 1, to = 10, length.out = 512)
pendulum1Data <- data.frame(time = xs1,
  deflection = pendulum1(xs1) + rnorm(length(xs1), sd = 0.01))


modelSet1 <- symbolicRegression(deflection ~ time, data = pendulum1Data,
                                 stopCondition = makeStepsStopCondition(8000))
bestModel1 <- modelSet1$population[[which.min(modelSet1$fitnessValues)]]
plot(y = bestModel1(xs1), x = xs1, type = "l", lty = 1, xlab = "x", ylab = "y")
lines(y = pendulum1(xs1), x = xs1, lty = 2)

# -----8<-----8<-----8<---- cut here


In the PDF it says it would produce a function (even after only 2
minutes), that fits to the data like the one depicted in fig. 1 on
page 3 but all I get is:

> bestModel1
function (time)
-2.97507503535599/(exp(time)/(0.280210268683732/time))

Sometimes it even produces just a constant term and no dependency on
time at all! What am I missing here, what needs to be done to make it
try more complicated expressions, to make the example in the PDF work
as described?

TIA,
Bernd


From tk9830 at gmail.com  Tue Jul 21 22:00:37 2015
From: tk9830 at gmail.com (thok12)
Date: Tue, 21 Jul 2015 13:00:37 -0700 (PDT)
Subject: [R] removing entries from a character vector
Message-ID: <CA+AQxOpTC0owkq6qMiw5RCT9Tnw9a6hOqMB-+8VbTfuJG_=LMw@mail.gmail.com>

ok this is a textbook question but i feel kind of lost so any help would be
appreciated.

I have a character vector with couple of hundreds of observations

about 60 of them are in the following fashion
"total June15"
"total July 15"
"total July 17"
etc.

now I want to remove these entries but not one by one.
I want to write some code that will remove all the entries with the
word/string "total" in them.
I tried few things but with no success.

thanks in advance




--
View this message in context: http://r.789695.n4.nabble.com/removing-entries-from-a-character-vector-tp4710165.html
Sent from the R help mailing list archive at Nabble.com.
	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Jul 22 03:40:01 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 21 Jul 2015 21:40:01 -0400
Subject: [R] removing entries from a character vector
In-Reply-To: <CA+AQxOpTC0owkq6qMiw5RCT9Tnw9a6hOqMB-+8VbTfuJG_=LMw@mail.gmail.com>
References: <CA+AQxOpTC0owkq6qMiw5RCT9Tnw9a6hOqMB-+8VbTfuJG_=LMw@mail.gmail.com>
Message-ID: <CAM_vjum69sVz5EdeHtM+LwfeZX6q2JLHkMjugUPFeCPK_6dkhQ@mail.gmail.com>

See ?grepl

If you'd like a more detailed answer, here are some suggestions for
creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

Sarah


On Tue, Jul 21, 2015 at 4:00 PM, thok12 <tk9830 at gmail.com> wrote:
> ok this is a textbook question but i feel kind of lost so any help would be
> appreciated.
>
> I have a character vector with couple of hundreds of observations
>
> about 60 of them are in the following fashion
> "total June15"
> "total July 15"
> "total July 17"
> etc.
>
> now I want to remove these entries but not one by one.
> I want to write some code that will remove all the entries with the
> word/string "total" in them.
> I tried few things but with no success.
>
> thanks in advance
>


-- 
Sarah Goslee
http://www.functionaldiversity.org


From r.turner at auckland.ac.nz  Wed Jul 22 04:30:03 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 22 Jul 2015 14:30:03 +1200
Subject: [R] glm help - final predictor variable NA
In-Reply-To: <1437505018036-4710161.post@n4.nabble.com>
References: <1437505018036-4710161.post@n4.nabble.com>
Message-ID: <55AF002B.8070100@auckland.ac.nz>


Psigh!  Why do people think that it is perfectly OK to undertake 
statistical analyses without knowing or understanding any statistics?
(I guess it's slightly less dangerous than undertaking to do your own 
wiring without knowing anything about being an electrician, but still ....)

However, to stop venting and answer your question:  It is because 
"CDSTotal" is perfectly confounded (in the given design) with the other 
predictors. That is, CDSTotal is exactly equal to a linear combination 
of the other predictors (and the constant "1").

Try:

lm(CDSTotal ~ Age + Gender + LOC + PC + Stability, data=Controlgroup)

and you will find that the error sum of squares is zero (to within 
numerical tolerance).

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 22/07/15 06:56, matthewjones43 wrote:

> Hi, I am not a statistician and so I am sure whatever it is I am doing wrong
> must be an obvious error for those who are...Basically I can not understand
> why I get NA for variable 'CDSTotal' when running a glm? Does anyone have an
> idea of why this might be happening?
>
> Call:  glm(formula = cbind(SRAS - 26, 182 - SRAS) ~ Age + Gender + LOC +
>      PC + Stability + CDSTotal, family = binomial, data = Controlgroup)
>
> Coefficients:
> (Intercept)          Age       Gender          LOC           PC    Stability
>    -2.575071     0.009148     0.354143     0.018295    -0.011317     0.090759
>     CDSTotal
>           NA
>
> Degrees of Freedom: 64 Total (i.e. Null);  59 Residual
> Null Deviance:	    2015
> Residual Deviance: 1264 	AIC: 1614


From dwinsemius at comcast.net  Wed Jul 22 04:49:48 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 21 Jul 2015 19:49:48 -0700
Subject: [R] How to make a persistence landscape in R?
In-Reply-To: <1437500385524-4710159.post@n4.nabble.com>
References: <1437500385524-4710159.post@n4.nabble.com>
Message-ID: <97ECB264-2ABC-41BB-BF19-1BB12B0E456B@comcast.net>


On Jul 21, 2015, at 10:39 AM, lstat wrote:

> Hi there, 
> I am trying to construct a persistence landscape in R that shows all of the
> overlapping triangles -- not just the overall silhouette-- how can you do
> this?
> Everytime I plug in the code I get one big triangle that corresponds to the
> largest barcode, but I cannot see any of the smaller barcodes/ overlapping
> isosceles trianges. 

You need to get out more. Talk to people who don't share your somewhat constricted view of the world. Learn to talk to a more diverse group of human beings,... say statisticians who might never have heard of a "persistent landscape"? 

It all sounds quite fascinating ... but there was no "the code". Perhaps this is due to the Nabble interface which masquerades as R-help but is only a feeble imitation.


Do read the fine posting guide:  http://www.R-project.org/posting-guide.html
> 
-- 

David Winsemius
Alameda, CA, USA


From liuwensui at gmail.com  Wed Jul 22 05:21:32 2015
From: liuwensui at gmail.com (Wensui Liu)
Date: Tue, 21 Jul 2015 23:21:32 -0400
Subject: [R] model non-integer count outcomes
Message-ID: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>

Dear Lister
When the count outcomes are integers, we could use either Poisson or
NB regression to model them. However, there are cases that the count
outcomes are non-integers, e.g. average counts.
I am wondering if it still makes sense to use Poisson or NB regression
to model these non-integer outcomes.

Truly appreciate your attention and insight!


From dwinsemius at comcast.net  Wed Jul 22 05:38:01 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 21 Jul 2015 20:38:01 -0700
Subject: [R] model non-integer count outcomes
In-Reply-To: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
References: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
Message-ID: <500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>


On Jul 21, 2015, at 8:21 PM, Wensui Liu wrote:

> Dear Lister
> When the count outcomes are integers, we could use either Poisson or
> NB regression to model them. However, there are cases that the count
> outcomes are non-integers, e.g. average counts.
> I am wondering if it still makes sense to use Poisson or NB regression
> to model these non-integer outcomes.

There is a quasi-binomial error model that accepts non-integer outcomes.

-- 

David Winsemius
Alameda, CA, USA


From dmck at u.washington.edu  Wed Jul 22 04:33:38 2015
From: dmck at u.washington.edu (Don McKenzie)
Date: Tue, 21 Jul 2015 19:33:38 -0700
Subject: [R] glm help - final predictor variable NA
In-Reply-To: <55AF002B.8070100@auckland.ac.nz>
References: <1437505018036-4710161.post@n4.nabble.com>
	<55AF002B.8070100@auckland.ac.nz>
Message-ID: <606524B8-BAE8-4722-BF35-7E12BF50640D@u.washington.edu>


> On Jul 21, 2015, at 7:30 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
> Psigh!  Why do people think that it is perfectly OK to undertake statistical analyses without knowing or understanding any statistics?
> (I guess it's slightly less dangerous than undertaking to do your own wiring without knowing anything about being an electrician, but still ?.)

Fortune?





From dmck at u.washington.edu  Wed Jul 22 05:41:47 2015
From: dmck at u.washington.edu (Don McKenzie)
Date: Tue, 21 Jul 2015 20:41:47 -0700
Subject: [R] model non-integer count outcomes
In-Reply-To: <500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>
References: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
	<500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>
Message-ID: <7A828A98-8AC4-4DDA-B5A3-3C60276F84AC@u.washington.edu>

Or if there are enough averages of enough counts, the CLT provides another option.

> On Jul 21, 2015, at 8:38 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> On Jul 21, 2015, at 8:21 PM, Wensui Liu wrote:
> 
>> Dear Lister
>> When the count outcomes are integers, we could use either Poisson or
>> NB regression to model them. However, there are cases that the count
>> outcomes are non-integers, e.g. average counts.
>> I am wondering if it still makes sense to use Poisson or NB regression
>> to model these non-integer outcomes.
> 
> There is a quasi-binomial error model that accepts non-integer outcomes.
> 
> -- 
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




From dmck at uw.edu  Wed Jul 22 06:48:46 2015
From: dmck at uw.edu (Don McKenzie)
Date: Tue, 21 Jul 2015 21:48:46 -0700
Subject: [R] model non-integer count outcomes
In-Reply-To: <CAKyN3iA=9JZW42xPSz5YVJDQP4Z2VOqB_F5uLHwm209Pac_uyg@mail.gmail.com>
References: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
	<500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>
	<7A828A98-8AC4-4DDA-B5A3-3C60276F84AC@u.washington.edu>
	<CAKyN3iA=9JZW42xPSz5YVJDQP4Z2VOqB_F5uLHwm209Pac_uyg@mail.gmail.com>
Message-ID: <445E64E7-A752-4C59-9258-3BECBBFED4FC@uw.edu>

Sorry. Central limit theorem. Enough averaging and you get a normal distribution (simply stated, perhaps too simply). If so others will correct me before long.  :-(

Sent from my iPad

> On Jul 21, 2015, at 8:52 PM, Wensui Liu <liuwensui at gmail.com> wrote:
> 
> what does CLT stand for?
> 
>> On Tue, Jul 21, 2015 at 11:41 PM, Don McKenzie <dmck at u.washington.edu> wrote:
>> Or if there are enough averages of enough counts, the CLT provides another option.
>> 
>>> On Jul 21, 2015, at 8:38 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>> 
>>> On Jul 21, 2015, at 8:21 PM, Wensui Liu wrote:
>>> 
>>>> Dear Lister
>>>> When the count outcomes are integers, we could use either Poisson or
>>>> NB regression to model them. However, there are cases that the count
>>>> outcomes are non-integers, e.g. average counts.
>>>> I am wondering if it still makes sense to use Poisson or NB regression
>>>> to model these non-integer outcomes.
>>> 
>>> There is a quasi-binomial error model that accepts non-integer outcomes.
>>> 
>>> -- 
>>> 
>>> David Winsemius
>>> Alameda, CA, USA
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
> 
> 
> 
> -- 
> WenSui Liu
> https://statcompute.wordpress.com/

From thierry.onkelinx at inbo.be  Wed Jul 22 10:37:14 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Wed, 22 Jul 2015 10:37:14 +0200
Subject: [R] model non-integer count outcomes
In-Reply-To: <7A828A98-8AC4-4DDA-B5A3-3C60276F84AC@u.washington.edu>
References: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
	<500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>
	<7A828A98-8AC4-4DDA-B5A3-3C60276F84AC@u.washington.edu>
Message-ID: <CAJuCY5wxYwk-d71-XDVxt-cJDtc2MW_PdNJ740ZR35_U=w1MmQ@mail.gmail.com>

If you know the number of counts (n) used to calculate the average then you
can still use a poisson distribution.

Total = average * n
glm(total ~ offset(n), family = poisson)

?
ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
Op 22 jul. 2015 08:38 schreef "Don McKenzie" <dmck at u.washington.edu>:

> Or if there are enough averages of enough counts, the CLT provides another
> option.
>
> > On Jul 21, 2015, at 8:38 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >
> > On Jul 21, 2015, at 8:21 PM, Wensui Liu wrote:
> >
> >> Dear Lister
> >> When the count outcomes are integers, we could use either Poisson or
> >> NB regression to model them. However, there are cases that the count
> >> outcomes are non-integers, e.g. average counts.
> >> I am wondering if it still makes sense to use Poisson or NB regression
> >> to model these non-integer outcomes.
> >
> > There is a quasi-binomial error model that accepts non-integer outcomes.
> >
> > --
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Jul 22 11:20:53 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 22 Jul 2015 19:20:53 +1000
Subject: [R] barplot -issues with axis and labels not appearing
In-Reply-To: <DUB118-W359E7EA81A03A5C6A9DE74D3840@phx.gbl>
References: <DUB118-W359E7EA81A03A5C6A9DE74D3840@phx.gbl>
Message-ID: <CA+8X3fXvgnL3qEm0rWW3BE9hVDzES5Q+ewb7mxK-HKsWH3bhHQ@mail.gmail.com>

Hi Pierre,
I get a reasonable plot using the following code:

par(mar=c(6,4,4,2))
barpos<-barplot(unlist(GEP.data2),
 main="Global Portfolio Weights",
 col.main="gray", col=blues9,
 cex.axis=1, ylim=c(-1,1), las=2,
 cex.lab=1, cex=0.8)
axis(1,at=barpos,labels=rep("",8))

For one thing, you don't need the "beside=TRUE" argument as there is
only one vector of values to display. The small value for "cex.axis"
made the tick labels unreadable on my display. If you would like to
have the bar labels horizontal, have a look at the "staxlab" function
in the plotrix package.

Jim


On Tue, Jul 21, 2015 at 10:38 PM, Pierre Micallef
<micallefpierre at hotmail.com> wrote:
> Hi
>
> I am experiencing a few issues with the barplot function.
>
> I have written the following code;
>
>   barplot(as.matrix(GEP.data2), beside=TRUE, main="Global Portfolio Weights", col.main="gray", col=blues9,
>           cex.axis=0.1, ylim=c(-1,1), las=2, cex.lab=1, cex=0.8)
>
>
> where;
>> GEP.data2 =
>   VGSIX.equity VUSTX.equity VGTSX.equity VFISX.equity VTSMX.equity VFITX.equity VEIEX.equity VIPSX.equity
> 1  -0.08645095   0.08991793   0.03548216         0.45         0.45         0.45   -0.1689109   -0.2200382 However i  am having the following issues; (1) neither x or y axis appear on my graph (bars appear as if they are floating)(2) no axis labels are showing up (3) chart title is too high on graph and is being cut off from view Please can you help solve these issues? Thanks Pierre
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Wed Jul 22 11:44:31 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 22 Jul 2015 19:44:31 +1000
Subject: [R] How to make a persistence landscape in R?
In-Reply-To: <97ECB264-2ABC-41BB-BF19-1BB12B0E456B@comcast.net>
References: <1437500385524-4710159.post@n4.nabble.com>
	<97ECB264-2ABC-41BB-BF19-1BB12B0E456B@comcast.net>
Message-ID: <CA+8X3fVzyOfWA+ZysJK1iO=k3h0i4cxjVh7=Gbn-xf3DB76j2Q@mail.gmail.com>

Hi lstat,
The problem may be that as you are adding your connector triangles
there is a fill color. Try reprogramming with col=NA if you are using
"polygon" to draw the connectors.

Jim


On Wed, Jul 22, 2015 at 12:49 PM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
> On Jul 21, 2015, at 10:39 AM, lstat wrote:
>
>> Hi there,
>> I am trying to construct a persistence landscape in R that shows all of the
>> overlapping triangles -- not just the overall silhouette-- how can you do
>> this?
>> Everytime I plug in the code I get one big triangle that corresponds to the
>> largest barcode, but I cannot see any of the smaller barcodes/ overlapping
>> isosceles trianges.
>
> You need to get out more. Talk to people who don't share your somewhat constricted view of the world. Learn to talk to a more diverse group of human beings,... say statisticians who might never have heard of a "persistent landscape"?
>
> It all sounds quite fascinating ... but there was no "the code". Perhaps this is due to the Nabble interface which masquerades as R-help but is only a feeble imitation.
>
>
> Do read the fine posting guide:  http://www.R-project.org/posting-guide.html
>>
> --
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Jul 22 12:41:27 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 22 Jul 2015 12:41:27 +0200
Subject: [R] model non-integer count outcomes
In-Reply-To: <445E64E7-A752-4C59-9258-3BECBBFED4FC@uw.edu>
References: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
	<500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>
	<7A828A98-8AC4-4DDA-B5A3-3C60276F84AC@u.washington.edu>
	<CAKyN3iA=9JZW42xPSz5YVJDQP4Z2VOqB_F5uLHwm209Pac_uyg@mail.gmail.com>
	<445E64E7-A752-4C59-9258-3BECBBFED4FC@uw.edu>
Message-ID: <A1A4393A-ECD9-4820-86B6-5C93B644FC66@gmail.com>


> On 22 Jul 2015, at 06:48 , Don McKenzie <dmck at uw.edu> wrote:
> 
> Sorry. Central limit theorem.

Or some sort of vegetarian sandwich. Celery, Lettuce, Tomato sounds almost edible with sufficient mayo. ;-)

> Enough averaging and you get a normal distribution (simply stated, perhaps too simply). If so others will correct me before long.  :-(

Well, your punctuation doesn't quite work -- ')' comes too early. Otherwise it is close enough for jazz, although there are distributions that you can average forever and still not get a normal, and some might want to stress that it is the parameter estimators that become approximately normal.  (Students sometimes get confused and believe that the original data magically become normally distributed when you have a lot of them.) 

In practice, one should ensure that one has "many" data for all the relevant averages (996 males and 4 females is no good), and also that one gets the variance structure at least roughly right.

-pd



> 
> Sent from my iPad
> 
>> On Jul 21, 2015, at 8:52 PM, Wensui Liu <liuwensui at gmail.com> wrote:
>> 
>> what does CLT stand for?
>> 
>>> On Tue, Jul 21, 2015 at 11:41 PM, Don McKenzie <dmck at u.washington.edu> wrote:
>>> Or if there are enough averages of enough counts, the CLT provides another option.
>>> 
>>>> On Jul 21, 2015, at 8:38 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>> 
>>>> 
>>>> On Jul 21, 2015, at 8:21 PM, Wensui Liu wrote:
>>>> 
>>>>> Dear Lister
>>>>> When the count outcomes are integers, we could use either Poisson or
>>>>> NB regression to model them. However, there are cases that the count
>>>>> outcomes are non-integers, e.g. average counts.
>>>>> I am wondering if it still makes sense to use Poisson or NB regression
>>>>> to model these non-integer outcomes.
>>>> 
>>>> There is a quasi-binomial error model that accepts non-integer outcomes.
>>>> 
>>>> -- 
>>>> 
>>>> David Winsemius
>>>> Alameda, CA, USA
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>>> 
>>> 
>> 
>> 
>> 
>> -- 
>> WenSui Liu
>> https://statcompute.wordpress.com/
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From liuwensui at gmail.com  Wed Jul 22 13:36:44 2015
From: liuwensui at gmail.com (Wensui Liu)
Date: Wed, 22 Jul 2015 07:36:44 -0400
Subject: [R] model non-integer count outcomes
In-Reply-To: <CAJuCY5wxYwk-d71-XDVxt-cJDtc2MW_PdNJ740ZR35_U=w1MmQ@mail.gmail.com>
References: <CAKyN3iB7bVDsFq5fHi6AmmpL6qbL3HAOwvqKA6d7FgEv2n6tGA@mail.gmail.com>
	<500A4054-6D4C-4BC7-94A2-5BBC7B28B476@comcast.net>
	<7A828A98-8AC4-4DDA-B5A3-3C60276F84AC@u.washington.edu>
	<CAJuCY5wxYwk-d71-XDVxt-cJDtc2MW_PdNJ740ZR35_U=w1MmQ@mail.gmail.com>
Message-ID: <CAKyN3iDBGWTQGicMPuBsumopB1UG+HsCOTf2Ecooyyj58dY+Bw@mail.gmail.com>

Thanks Thierry
What if I don't know the n in the offset term?

On Wednesday, July 22, 2015, Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> If you know the number of counts (n) used to calculate the average then you
> can still use a poisson distribution.
>
> Total = average * n
> glm(total ~ offset(n), family = poisson)
>
> ?
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
> Forest
> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
>
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> Op 22 jul. 2015 08:38 schreef "Don McKenzie" <dmck at u.washington.edu
> <javascript:;>>:
>
> > Or if there are enough averages of enough counts, the CLT provides
> another
> > option.
> >
> > > On Jul 21, 2015, at 8:38 PM, David Winsemius <dwinsemius at comcast.net
> <javascript:;>>
> > wrote:
> > >
> > >
> > > On Jul 21, 2015, at 8:21 PM, Wensui Liu wrote:
> > >
> > >> Dear Lister
> > >> When the count outcomes are integers, we could use either Poisson or
> > >> NB regression to model them. However, there are cases that the count
> > >> outcomes are non-integers, e.g. average counts.
> > >> I am wondering if it still makes sense to use Poisson or NB regression
> > >> to model these non-integer outcomes.
> > >
> > > There is a quasi-binomial error model that accepts non-integer
> outcomes.
> > >
> > > --
> > >
> > > David Winsemius
> > > Alameda, CA, USA
> > >
> > > ______________________________________________
> > > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
> and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
WenSui Liu
https://statcompute.wordpress.com/

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Wed Jul 22 14:24:44 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 22 Jul 2015 08:24:44 -0400
Subject: [R] R version update
In-Reply-To: <a5605b357787a0fb7fc2fcee8242dd36@www.sxmail.de>
References: <a5605b357787a0fb7fc2fcee8242dd36@www.sxmail.de>
Message-ID: <CA+vqiLEfknTDPgBnq0SGkx_n6p52Uvs=7SqXX=k3SnJXAbagBw@mail.gmail.com>

On Jul 21, 2015 9:30 PM, <klerer at sxmail.de> wrote:
>
> Dear Ladies and Gentlemen,
> as a beginner in R, I encountered a sort of "naturally given
limits"concerning the process of amending R with packages.
> I apparently own version 3.0.2 and I principally decided to use R via
> the RKWard GUI on Linux Kubuntu Trusty Tahr; the R version installed on
> my computer is admittedly not far from the basics provided by my
> distribution's package management (also throughout multiple rounds of
> updates/ upgrades of the system).
> What would I have to do to finally update/ upgrade beyond that R
> version?

https://cran.r-project.org/bin/linux/ubuntu/

Would that process (generally?) affect any/ my GUI?

possibly.

> This problem appeared to me as I was busy installing packages an
> received the error message "package ?ATLAS? is not available (for R
> version 3.0.2) ".

I don't think there is any package on CRAN with that name. What makes you
think there is?

> I am quite desperate and would look forward to be indicated a path to a
> sustainable solution or be told how to mitigate/ circumvent (these/
> such) problems.

I'm not actually sure exactly what the problem is...

> Thanks a lot!
>
> Best regards,Markus Hofstetter
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From marammagdysalem at gmail.com  Wed Jul 22 14:40:23 2015
From: marammagdysalem at gmail.com (marammagdysalem at gmail.com)
Date: Wed, 22 Jul 2015 14:40:23 +0200
Subject: [R] Fwd:  Warning message with maxLik()
References: <E0909349-EAF6-4476-A6B0-F780A8A9AF77@gmail.com>
Message-ID: <800EDD2F-335D-46F9-A878-7065C5591E62@gmail.com>



Sent from my iPhone

Begin forwarded message:

> From: Maram SAlem <marammagdysalem at gmail.com>
> Date: July 21, 2015 at 11:40:56 PM GMT+2
> To: Arne Henningsen <arne.henningsen at gmail.com>
> Cc: "r-help at r-project.org" <r-help at r-project.org>
> Subject: Re: [R] Warning message with maxLik()
> 
> Dear Arne,
> 
> The elements of the theta vector are indeed strictly positive. I've just tried to use instead : lamda = log (theta), which means that theta = exp (lamda),  so as to get rid of the log() function that appears in the log-likelihood and is causing the 50 warnings, but still the estimates I got for lamda and then those I got for theta (using theta=exp(lamda)) are irrelvant and their standard errors are infinite, which means that therer is still a problem that I can't yet figure out.
> 
> Thanks,
> Maram
> 
>> On 18 July 2015 at 08:01, Arne Henningsen <arne.henningsen at gmail.com> wrote:
>> Dear Maram
>> 
>> - Please do not start a new thread for the same issue but reply to
>> previous messages in this thread [1].
>> 
>> - Please read my previous responses [1] more carefully, e.g. to use
>> "theta <- exp( param )" which guarantees that all elements of "theta"
>> are always positive.
>> 
>> [1] http://r.789695.n4.nabble.com/NaN-produced-from-log-with-positive-input-td4709463.html
>> 
>> Best regards,
>> Arne
>> 
>> 
>> 
>> 2015-07-18 2:46 GMT+02:00 Maram SAlem <marammagdysalem at gmail.com>:
>> > Dear All,
>> > I'm trying to get the MLe for a certain distribution using maxLik ()
>> > function. I wrote the log-likelihood function as follows:
>> > theta <-vector(mode = "numeric", length = 3)
>> > r<- 17
>> > n <-30
>> >  T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
>> > C<-
>> > c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>> > # The  loglik. func.
>> > loglik <- function(param) {
>> >  theta[1]<- param[1]
>> >  theta[2]<- param[2]
>> >  theta[3]<- param[3]
>> >  l<-(r*log(theta[3]))+(r*log(theta[1]+theta[2]))+(n*theta[3]*log(theta[1]))+(n*theta[3]*log(theta[2]))+
>> > (-1*(theta[3]+1))*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+
>> > (-1*theta[3]*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2]))))
>> > return(l)
>> >  }
>> >
>> > then, I evaluated it at theta<- c(40,50,2)
>> >
>> > v<-loglik(param=theta)
>> > v
>> > [1] -56.66653
>> >
>> > I used this same log-likelihood function, once with analytic gradient and
>> > another time with numerical one, with the maxLik function, and in both
>> > cases I got the same 50 warning messages and an MLE which is completely
>> > unrealistic as per my applied example.
>> >
>> > a <- maxLik(loglik, gradlik, hesslik, start=c(40,50,2))
>> >
>> > where gradlik and hesslik are the analytic gradient and Hessian matrix,
>> > respectively, given by:
>> >
>> > U <- vector(mode="numeric",length=3)
>> > gradlik<-function(param = theta,n, T,C)
>> >  {
>> > U <- vector(mode="numeric",length=3)
>> > theta[1] <- param[1]
>> > theta[2] <- param[2]
>> > theta[3] <- param[3]
>> > r<- 17
>> > n <-30
>> > T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
>> > C<-
>> > c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>> >  U[1]<- (r/(theta[1]+theta[2]))+((n*theta[3])/theta[1])+(
>> > -1*(theta[3]+1))*sum((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
>> > (-1*(theta[3]))*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> > U[2]<-(r/(theta[1]+theta[2]))+((n*theta[3])/theta[2])+
>> > (-1*(theta[3]+1))*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+
>> > (-1*(theta[3]))*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> > U[3]<-(r/theta[3])+(n*log(theta[1]*theta[2]))+
>> > (-1)*sum(log((T*(theta[1]+theta[2]))+(theta[1]*theta[2])))+(-1)*sum(log((C*(theta[1]+theta[2]))+(theta[1]*theta[2])))
>> > return(U)
>> > }
>> > hesslik<-function(param=theta,n,T,C)
>> > {
>> > theta[1] <- param[1]
>> > theta[2] <- param[2]
>> > theta[3] <- param[3]
>> > r<- 17
>> > n <-30
>> > T<-c(7.048,0.743,2.404,1.374,2.233,1.52,23.531,5.182,4.502,1.362,1.15,1.86,1.692,11.659,1.631,2.212,5.451)
>> > C<-
>> > c(0.562,5.69,12.603,3.999,6.156,4.004,5.248,4.878,7.122,17.069,23.996,1.538,7.792)
>> > G<- matrix(nrow=3,ncol=3)
>> > G[1,1]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[1])^2)+
>> > (theta[3]+1)*sum(((T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
>> > theta[3])*sum(((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
>> > G[1,2]<-((-1*r)/((theta[1]+theta[2])^2))+
>> > (theta[3]+1)*sum(((T)/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+
>> > (theta[3])*sum(((C)/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
>> > G[2,1]<-G[1,2]
>> > G[1,3]<-(n/theta[1])+(-1)*sum(
>> > (T+theta[2])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[2])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> > G[3,1]<-G[1,3]
>> > G[2,2]<-((-1*r)/((theta[1]+theta[2])^2))+((-1*n*theta[3])/(theta[2])^2)+
>> > (theta[3]+1)*sum(((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))^2)+(
>> > theta[3])*sum(((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))^2)
>> > G[2,3]<-(n/theta[2])+(-1)*sum((T+theta[1])/((theta[1]+theta[2])*T+(theta[1]*theta[2])))+(-1)*sum((C+theta[1])/((theta[1]+theta[2])*C+(theta[1]*theta[2])))
>> > G[3,2]<-G[2,3]
>> > G[3,3]<-((-1*r)/(theta[3])^2)
>> > return(G)
>> > }
>> >
>> > and using numeric gradient and hessian matrix:
>> >
>> > a <- maxLik(loglik, start=c(40,50,2))
>> > Warning messages:
>> > 1: In log(theta[3]) : NaNs produced
>> > 2: In log(theta[1] + theta[2]) : NaNs produced
>> > 3: In log(theta[1]) : NaNs produced
>> > 4: In log((T * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
>> > produced
>> > 5: In log((C * (theta[1] + theta[2])) + (theta[1] * theta[2])) : NaNs
>> > produced
>> > 6: In log(theta[3]) : NaNs produced
>> > 7: In log(theta[1] + theta[2]) : NaNs produced
>> > and so on?..
>> >
>> > I don't know why I get these 50 warnings although:
>> > 1- The inputs of the log() function are strictly positive.
>> > 2- When I evaluated the log-likelihood fuction at the very begining it gave
>> > me a number(which is -56.66) and not (NAN).
>> >
>> > I've also tried to:
>> > 1- Reparamtrize my model using lamda(i)= log(theta(i)), for i=1,2,3, so
>> > that it may solve the problem, but it didn't.
>> > 2- I've used the comparederivitive() function, and the analytic and numeric
>> > gradients were so close.
>> >
>> > Any help please?
>> > Maram Salem
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>> 
>> 
>> 
>> --
>> Arne Henningsen
>> http://www.arne-henningsen.name
> 

	[[alternative HTML version deleted]]


From therneau at mayo.edu  Wed Jul 22 15:15:04 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Wed, 22 Jul 2015 08:15:04 -0500
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <mailman.7.1437559202.1794.r-help@r-project.org>
References: <mailman.7.1437559202.1794.r-help@r-project.org>
Message-ID: <2f3a88$12la7r@ironport10.mayo.edu>

"Type III" is a peculiarity of SAS, which has taken root in the world.  There are 3 main 
questions wrt to it:

1. How to compute it (outside of SAS).  There is a trick using contr.treatment coding that 
works if the design has no missing factor combinations, your post has a link to such a 
description.  The SAS documentation is very obtuse, thus almost no one knows how to 
compute the general case.

2. What is it?  It is a population average.  The predicted average treatment effect in a 
balanced population-- one where all the factor combinations appeared the same number of 
times.  One way to compute 'type 3' is to create such a data set, get all the predicted 
values, and then take the average prediction for treatment A, average for treatment B, 
average for C, ...  and test "are these averages the same".   The algorithm of #1 above 
leads to another explanation which is a false trail, in my opinion.

3. Should you ever use it?  No.  There is a very strong inverse correlation between 
"understand what it really is" and "recommend its use".   Stephen Senn has written very 
intellgently on the issues.

Terry Therneau


On 07/22/2015 05:00 AM, r-help-request at r-project.org wrote:
> Dear Michael,
> thanks a lot. I am studying the marginality and I came across to this post:
>
> http://www.ats.ucla.edu/stat/r/faq/type3.htm
>
> Do you think that the procedure there described is the right one to solve my problem?
>
> Would you have any other online resources to suggest especially dealing with R?
>
> My department does not have a statician, so I have to find a solution with my own capacities.
>
> Thanks in advance
>
> Angelo


From infojomy at gmail.com  Wed Jul 22 15:50:09 2015
From: infojomy at gmail.com (Jomy Jose)
Date: Wed, 22 Jul 2015 19:20:09 +0530
Subject: [R] Sensitivity,Specificity and Youden Index
Message-ID: <CADGufDEMHVkD8=3vCGoZAojz0rqupqd9AdJAOR8OautEgcU07w@mail.gmail.com>

How to calculate the sensitivity,specificity,Youden index for 18 factors
and their combination (6 factors in each) with an outcome measure.

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Jul 22 16:00:57 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 22 Jul 2015 10:00:57 -0400
Subject: [R] Sensitivity,Specificity and Youden Index
In-Reply-To: <CADGufDEMHVkD8=3vCGoZAojz0rqupqd9AdJAOR8OautEgcU07w@mail.gmail.com>
References: <CADGufDEMHVkD8=3vCGoZAojz0rqupqd9AdJAOR8OautEgcU07w@mail.gmail.com>
Message-ID: <CAM_vjummE8Kjub-HCG5dwU0LxvLQiaxbeYxaQC2cUULFvAWXhg@mail.gmail.com>

On Wed, Jul 22, 2015 at 9:50 AM, Jomy Jose <infojomy at gmail.com> wrote:
> How to calculate the sensitivity,specificity,Youden index for 18 factors
> and their combination (6 factors in each) with an outcome measure.

www.rseek.org turns up a bunch of references to Youden index,
including packages.

Without a reproducible example that includes some sample data (fake is
fine), the code you used, and some clear idea of what output you
expect, it's impossible to figure out how to help you. Here are some
suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


From angelo.arcadi at virgilio.it  Wed Jul 22 16:05:02 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Wed, 22 Jul 2015 16:05:02 +0200 (CEST)
Subject: [R] R: Re: Differences in output of lme() when introducing
 interactions
Message-ID: <14eb614f0ea.angelo.arcadi@virgilio.it>

Dear Terry,
I am very grateful to you for such a detailed and helpful answer.
Following your recommendation then I will skip the method presented at http://www.ats.ucla.edu/stat/r/faq/type3.htm

So far, based on my understanding of R I arrived to the conclusion that the correct way to see if there is
a correlation between my dependent variable (spectral centroid of a sound) and weight, height, and interaction
between weight and height of participants asked to create those sounds (in a repeated measure design) is:


lme_centroid <- lme(Centroid ~ Weight*Height*Shoe_Size, data = My_data, random = ~1 | Subject)

anova.lme(lme_centroid,type = "marginal")


Can anyone please confirm me that those formulas are actually correct and give the significant or
non significant p-values for the main effects and their interactions? I would prefer to use lme(), not lmer().

I am making the assumption of course that the model I am using (Centroid ~ Weight*Height*Shoe_Size) is 
the best fit for my data.

Thanks in advance

Angelo




----Messaggio originale----
Da: therneau at mayo.edu
Data: 22-lug-2015 15.15
A: <r-help at r-project.org>, <angelo.arcadi at virgilio.it>
Ogg: Re:  Differences in output of lme() when introducing interactions

"Type III" is a peculiarity of SAS, which has taken root in the world.  There are 3 main 
questions wrt to it:

1. How to compute it (outside of SAS).  There is a trick using contr.treatment coding that 
works if the design has no missing factor combinations, your post has a link to such a 
description.  The SAS documentation is very obtuse, thus almost no one knows how to 
compute the general case.

2. What is it?  It is a population average.  The predicted average treatment effect in a 
balanced population-- one where all the factor combinations appeared the same number of 
times.  One way to compute 'type 3' is to create such a data set, get all the predicted 
values, and then take the average prediction for treatment A, average for treatment B, 
average for C, ...  and test "are these averages the same".   The algorithm of #1 above 
leads to another explanation which is a false trail, in my opinion.

3. Should you ever use it?  No.  There is a very strong inverse correlation between 
"understand what it really is" and "recommend its use".   Stephen Senn has written very 
intellgently on the issues.

Terry Therneau


On 07/22/2015 05:00 AM, r-help-request at r-project.org wrote:
> Dear Michael,
> thanks a lot. I am studying the marginality and I came across to this post:
>
> http://www.ats.ucla.edu/stat/r/faq/type3.htm
>
> Do you think that the procedure there described is the right one to solve my problem?
>
> Would you have any other online resources to suggest especially dealing with R?
>
> My department does not have a statician, so I have to find a solution with my own capacities.
>
> Thanks in advance
>
> Angelo



   
	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Wed Jul 22 16:29:59 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Wed, 22 Jul 2015 09:29:59 -0500
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <2f3a88$12la7r@ironport10.mayo.edu>
References: <mailman.7.1437559202.1794.r-help@r-project.org>
	<2f3a88$12la7r@ironport10.mayo.edu>
Message-ID: <0F70E84A-B95F-4F76-B09C-B01149AB79B7@me.com>

Hi,

In addition to Terry?s great comments below, as this subject has come up frequently over the years, there is also a great document by Bill Venables that is valuable reading:

  Exegeses on Linear Models
  http://www.stats.ox.ac.uk/pub/MASS3/Exegeses.pdf


Regards,

Marc Schwartz


> On Jul 22, 2015, at 8:15 AM, Therneau, Terry M., Ph.D. <therneau at mayo.edu> wrote:
> 
> "Type III" is a peculiarity of SAS, which has taken root in the world.  There are 3 main questions wrt to it:
> 
> 1. How to compute it (outside of SAS).  There is a trick using contr.treatment coding that works if the design has no missing factor combinations, your post has a link to such a description.  The SAS documentation is very obtuse, thus almost no one knows how to compute the general case.
> 
> 2. What is it?  It is a population average.  The predicted average treatment effect in a balanced population-- one where all the factor combinations appeared the same number of times.  One way to compute 'type 3' is to create such a data set, get all the predicted values, and then take the average prediction for treatment A, average for treatment B, average for C, ...  and test "are these averages the same".   The algorithm of #1 above leads to another explanation which is a false trail, in my opinion.
> 
> 3. Should you ever use it?  No.  There is a very strong inverse correlation between "understand what it really is" and "recommend its use".   Stephen Senn has written very intellgently on the issues.
> 
> Terry Therneau
> 
> 
> On 07/22/2015 05:00 AM, r-help-request at r-project.org wrote:
>> Dear Michael,
>> thanks a lot. I am studying the marginality and I came across to this post:
>> 
>> http://www.ats.ucla.edu/stat/r/faq/type3.htm
>> 
>> Do you think that the procedure there described is the right one to solve my problem?
>> 
>> Would you have any other online resources to suggest especially dealing with R?
>> 
>> My department does not have a statician, so I have to find a solution with my own capacities.
>> 
>> Thanks in advance
>> 
>> Angelo


From jrkrideau at inbox.com  Wed Jul 22 16:32:33 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 22 Jul 2015 06:32:33 -0800
Subject: [R] How to make a persistence landscape in R?
In-Reply-To: <1437500385524-4710159.post@n4.nabble.com>
Message-ID: <25F5374ABB3.000010C8jrkrideau@inbox.com>

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html


John Kane
Kingston ON Canada


> -----Original Message-----
> From: l.shulman at hotmail.com
> Sent: Tue, 21 Jul 2015 10:39:45 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] How to make a persistence landscape in R?
> 
> Hi there,
> I am trying to construct a persistence landscape in R that shows all of
> the
> overlapping triangles -- not just the overall silhouette-- how can you do
> this?
> Everytime I plug in the code I get one big triangle that corresponds to
> the
> largest barcode, but I cannot see any of the smaller barcodes/
> overlapping
> isosceles trianges.
> Thanks!
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/How-to-make-a-persistence-landscape-in-R-tp4710159.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From friendly at yorku.ca  Wed Jul 22 16:33:29 2015
From: friendly at yorku.ca (Michael Friendly)
Date: Wed, 22 Jul 2015 10:33:29 -0400
Subject: [R] calculate adjacent log odds for a table
In-Reply-To: <55AE61C7.50602@yorku.ca>
References: <55AE61C7.50602@yorku.ca>
Message-ID: <55AFA9B9.2030803@yorku.ca>

On 7/21/2015 11:14 AM, Michael Friendly wrote:
> More generally, for an I x J x K table, where the last factor is the
> response, my desired result
> is a data frame of IJ(K-1) rows, with adjacent log odds in a 'logodds'
> column, and ideally, I'd like
> to have a general function to do this.
>
> Note that if T is the 10 x 3 matrix of frequencies shown by ftable(),
> the calculation is essentially
>
> log(T) %*% matrix(c(1, -1, 0,
>                      0,  1, -1))
> followed by reshaping and labeling.

No one answered, but as I often find, writing it out as a MWE was 
helpful.  Here is a simpler approach for my sample case, that should be
easier to generalize

# reshape to matrix
T <- matrix(mice.tab, nrow=prod(dim(mice.tab)[1:2]), ncol=dim(mice.tab)[3])
colnames(T) <- dimnames(mice.tab)[[3]]
rn <- expand.grid(litter=factor(7:11), treatment=c("A","B"))
rownames(T) <- apply(rn, 1, paste, collapse=":")
T
# calculate log odds as contrasts
C <- matrix(c(1, -1, 0,
               0,  1, -1), nrow=3)
lodds <- log(T) %*% C
colnames(lodds) <- c("0:1", "1:2+")
lodds

 > lodds
             0:1       1:2+
7:A   1.6625477  0.7884574
8:A   1.2527630  0.3364722
9:A   0.6061358  0.1823216
10:A  0.1431008 -0.1431008
11:A -1.0986123 -0.3483067
7:B   1.3730491  0.9985288
8:B   1.2272297  0.7537718
9:B   0.7156200  0.7884574
10:B  0.5725192  0.2006707
11:B -1.0986123  0.6286087
 >

# make a data frame
DF2 <- data.frame(expand.grid(litter=factor(7:11), treatment=c("A","B"), 
deaths=c("0:1", "1:2+")),
	logodds=c(lodds))


From YAOJIALU at FOXMAIL.COM  Wed Jul 22 09:50:54 2015
From: YAOJIALU at FOXMAIL.COM (JIALU YAO)
Date: Wed, 22 Jul 2015 00:50:54 -0700 (PDT)
Subject: [R] Error in "rownames"
Message-ID: <1437551454053-4710183.post@n4.nabble.com>

Hello,

When I type the code *rownames(gene_exp_matrix)<-levels(geneidfactor)*,the
error happens: Error in "rownames<-(*tmp*,value=character(0)):can not set
object to 'rownames'.

I am using Mac OS X10.10.2.

How can I fix this problem?

Thank a lot.

Yao



--
View this message in context: http://r.789695.n4.nabble.com/Error-in-rownames-tp4710183.html
Sent from the R help mailing list archive at Nabble.com.


From dpmeddings at gmail.com  Wed Jul 22 10:20:47 2015
From: dpmeddings at gmail.com (Daniel Meddings)
Date: Wed, 22 Jul 2015 09:20:47 +0100
Subject: [R] How to simulate informative censoring in a Cox PH model?
Message-ID: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>

I wish to simulate event times where the censoring is informative, and to
compare parameter estimator quality from a Cox PH model with estimates
obtained from event times generated with non-informative censoring. However
I am struggling to do this, and I conclude rather than a technical flaw in
my code I instead do not understand what is meant by informative and
un-informative censoring.

My approach is to simulate an event time T dependent on a vector of
covariates x having hazard function h(t|x)=lambda*exp(beta'*x)v*t^{v-1}.
This corresponds to T~ Weibull(lambda(x),v), where the scale parameter
lambda(x)=lambda*exp(beta'*x) depends on x and the shape parameter v is
fixed. I have N subjects where T_{i}~ Weibull(lambda(x_{i}),v_{T}),
lambda(x_{i})=lambda_{T}*exp(beta_{T}'*x_{i}), for i=1,...,N. Here I assume
the regression coefficients are p-dimensional.

I generate informative censoring times C_i~ Weibull(lambda(x_i),v_C),
lambda(x_i)=lambda_C*exp(beta_C'*x_i) and compute Y_inf_i=min(T_i,C_i) and
a censored flag delta_inf_i=1 if Y_inf_i <= C_i (an observed event), and
delta_inf_i=0 if Y_inf_i > C_i (informatively censored: event not
observed). I am convinced this is informative censoring because as long as
beta_T~=0 and beta_C~=0 then for each subject the data generating process
for T and C both depend on x.

In contrast I generate non-informative censoring times
D_i~Weibull(lambda_D*exp(beta_D),v_D), and compute Y_ninf_i=min(T_i,D_i)
and a censored flag delta_ninf_i=1 if Y_ninf_i <= D_i (an observed event),
and delta_ninf_i=0 if Y_ninf_i > D_i (non-informatively censored: event not
observed). Here beta_D is a scalar. I "scale" the simulation by choosing
the lambda_T, lambda_C and lambda_D parameters such that on average T_i<C_i
and T_i<D_i to achieve X% of censored subjects for both Y_inf_i and
Y_ninf_i.

The problem is that even for say 30% censoring (which I think is high), the
Cox PH parameter estimates using both Y_inf and Y_ninf are unbiased when I
expected the estimates using Y_inf to be biased, and I think I see why:
however different beta_C is from beta_T, a censored subject can presumably
influence the estimation of beta_T only by affecting the set of subjects at
risk at any time t, but this does not change the fact that every single
Y_inf_i with delta_inf_i=1 will have been generated using beta_T only. Thus
I do not see how my simulation can possibly produce biased estimates for
beta_T using Y_inf.

But then what is informative censoring if not based on this approach?

Any help would be greatly appreciated.

	[[alternative HTML version deleted]]


From amgv0009 at red.ujaen.es  Tue Jul 21 14:04:22 2015
From: amgv0009 at red.ujaen.es (amgv0009 at red.ujaen.es)
Date: Tue, 21 Jul 2015 12:04:22 +0000
Subject: [R] =?utf-8?q?=5BR-pkgs=5D_SDR_package_for_Subgroup_Discovery_on_?=
	=?utf-8?q?CRAN?=
Message-ID: <55ae3574.8261b40a.5d367.66b4@mx.google.com>


Package SDR for subgroup discovery data mining is available on CRAN now. 





More info of the package on the CRAN page:

https://cran.r-project.org/web/packages/SDR/index.html


?ngel.
	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From antony.akkara at ge.com  Wed Jul 22 14:09:21 2015
From: antony.akkara at ge.com (R_Antony)
Date: Wed, 22 Jul 2015 05:09:21 -0700 (PDT)
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
Message-ID: <1437566961986-4710192.post@n4.nabble.com>

Hi,

Here i am having a ".xlsx" file and it contains various columns including
date-column[mm/dd/yy]-but it is not in the date format. I have to read this
excel[.xlsx] file and need to get in dataframe. So i used "xlsx"-liabrary
and it was fine to read data. But the problem is, values in the date column
is converting to some other value.

for eg:- 

FF DATE
-----------
3/31/2016
2/26/2016
--
1/2/2016

[Values like "--" will come in the column to indicate that there is no date
mentioned ]

and i getting result like this,

FF DATE
-----------
42460
42426

42125

this is the code i am using for it,

theData<-data.frame(read.xlsx2(InputFilePath, sheetIndex,
sheetName="Workflow_Report", startRow=3,colIndex=NULL, endRow=NULL,
as.data.frame=TRUE, header=TRUE))

Aim :- I have to get actual "date-column" values in dataframe from xlsx
file.

I tried many ways, Could someone please help ?

Thanks in advance,
Antony.




--
View this message in context: http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192.html
Sent from the R help mailing list archive at Nabble.com.


From sarah.goslee at gmail.com  Wed Jul 22 17:44:48 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 22 Jul 2015 11:44:48 -0400
Subject: [R] Error in "rownames"
In-Reply-To: <1437551454053-4710183.post@n4.nabble.com>
References: <1437551454053-4710183.post@n4.nabble.com>
Message-ID: <CAM_vjumw=5Ls+ZKbcugr_=77hQU=WeGusnXZ4fK9i_u2+Hkefw@mail.gmail.com>

Without a reproducible example that includes some sample data (fake is
fine), the code you used, and some clear idea of what output you
expect, it's impossible to figure out how to help you. Here are some
suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example



On Wed, Jul 22, 2015 at 3:50 AM, JIALU YAO <YAOJIALU at foxmail.com> wrote:
> Hello,
>
> When I type the code *rownames(gene_exp_matrix)<-levels(geneidfactor)*,the
> error happens: Error in "rownames<-(*tmp*,value=character(0)):can not set
> object to 'rownames'.
>
> I am using Mac OS X10.10.2.
>
> How can I fix this problem?
>
> Thank a lot.
>
> Yao
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Error-in-rownames-tp4710183.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee
http://www.functionaldiversity.org


From rshepard at appl-ecosys.com  Wed Jul 22 17:50:15 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Wed, 22 Jul 2015 08:50:15 -0700 (PDT)
Subject: [R] Langelier-Ludwig Plots
Message-ID: <alpine.LNX.2.11.1507220847520.27006@localhost>

   My web search for an R package producing Langelier-Ludwig plots found no
hits. Has this been implemented in base graphics, lattice, ggplot2, or
another package?

   The reference:

Langelier, W., and Ludwig, H., 1942, Graphical methods for indicating the
mineral character of natural waters: J. Am. Water Ass., 34, p. 335-352.

Rich


From ivan.calandra at univ-reims.fr  Wed Jul 22 17:51:28 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Wed, 22 Jul 2015 17:51:28 +0200
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
In-Reply-To: <1437566961986-4710192.post@n4.nabble.com>
References: <1437566961986-4710192.post@n4.nabble.com>
Message-ID: <55AFBC00.6030606@univ-reims.fr>

Hi Antony,

I am not sure it could work easily with package xlsx. Try using the 
function read_excel() from package readxl. This function allows for 
Dates to be read.

HTH,
Ivan

--
Ivan Calandra, ATER
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 22/07/15 14:09, R_Antony a ?crit :
> Hi,
>
> Here i am having a ".xlsx" file and it contains various columns including
> date-column[mm/dd/yy]-but it is not in the date format. I have to read this
> excel[.xlsx] file and need to get in dataframe. So i used "xlsx"-liabrary
> and it was fine to read data. But the problem is, values in the date column
> is converting to some other value.
>
> for eg:-
>
> FF DATE
> -----------
> 3/31/2016
> 2/26/2016
> --
> 1/2/2016
>
> [Values like "--" will come in the column to indicate that there is no date
> mentioned ]
>
> and i getting result like this,
>
> FF DATE
> -----------
> 42460
> 42426
>
> 42125
>
> this is the code i am using for it,
>
> theData<-data.frame(read.xlsx2(InputFilePath, sheetIndex,
> sheetName="Workflow_Report", startRow=3,colIndex=NULL, endRow=NULL,
> as.data.frame=TRUE, header=TRUE))
>
> Aim :- I have to get actual "date-column" values in dataframe from xlsx
> file.
>
> I tried many ways, Could someone please help ?
>
> Thanks in advance,
> Antony.
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From boris.steipe at utoronto.ca  Wed Jul 22 18:04:52 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Wed, 22 Jul 2015 12:04:52 -0400
Subject: [R] Langelier-Ludwig Plots
In-Reply-To: <alpine.LNX.2.11.1507220847520.27006@localhost>
References: <alpine.LNX.2.11.1507220847520.27006@localhost>
Message-ID: <9AE2FD3B-EEBD-4FF3-985A-DD560F653141@utoronto.ca>

According to  ...
   http://info.ngwa.org/gwol/pdf/721000139.PDF (Graphical Interpretation of Water Quality Data)
... a Langelier-Ludwig plot is simply a scatterplot of cations vs. anions (in percent).
Surely that would be beyond trivial to produce in R. Or am I missing a subtle something?


Cheers,
B.



On Jul 22, 2015, at 11:50 AM, Rich Shepard <rshepard at appl-ecosys.com> wrote:

>  My web search for an R package producing Langelier-Ludwig plots found no
> hits. Has this been implemented in base graphics, lattice, ggplot2, or
> another package?
> 
>  The reference:
> 
> Langelier, W., and Ludwig, H., 1942, Graphical methods for indicating the
> mineral character of natural waters: J. Am. Water Ass., 34, p. 335-352.
> 
> Rich
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From john.archie.mckown at gmail.com  Wed Jul 22 18:13:17 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Wed, 22 Jul 2015 11:13:17 -0500
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
In-Reply-To: <1437566961986-4710192.post@n4.nabble.com>
References: <1437566961986-4710192.post@n4.nabble.com>
Message-ID: <CAAJSdjh2HDroRRTyN3aruAnWFZBym7Cu=JHzF35q+XEVa4sOTg@mail.gmail.com>

Those numbers are a "serial" number of days. A value of 1 maps to Jan 1,
1900. ref:
https://support.office.com/en-za/article/DATE-function-e36c0c8c-4104-49da-ab83-82328b832349

A formula such as: as.Date('1900-01-01')+excel_date-1 should convert the
serial value to a date value.

On Wed, Jul 22, 2015 at 7:09 AM, R_Antony <antony.akkara at ge.com> wrote:

> Hi,
>
> Here i am having a ".xlsx" file and it contains various columns including
> date-column[mm/dd/yy]-but it is not in the date format. I have to read this
> excel[.xlsx] file and need to get in dataframe. So i used "xlsx"-liabrary
> and it was fine to read data. But the problem is, values in the date column
> is converting to some other value.
>
> for eg:-
>
> FF DATE
> -----------
> 3/31/2016
> 2/26/2016
> --
> 1/2/2016
>
> [Values like "--" will come in the column to indicate that there is no date
> mentioned ]
>
> and i getting result like this,
>
> FF DATE
> -----------
> 42460
> 42426
>
> 42125
>
> this is the code i am using for it,
>
> theData<-data.frame(read.xlsx2(InputFilePath, sheetIndex,
> sheetName="Workflow_Report", startRow=3,colIndex=NULL, endRow=NULL,
> as.data.frame=TRUE, header=TRUE))
>
> Aim :- I have to get actual "date-column" values in dataframe from xlsx
> file.
>
> I tried many ways, Could someone please help ?
>
> Thanks in advance,
> Antony.
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Jul 22 18:22:02 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 22 Jul 2015 09:22:02 -0700
Subject: [R] Error in "rownames"
In-Reply-To: <1437551454053-4710183.post@n4.nabble.com>
References: <1437551454053-4710183.post@n4.nabble.com>
Message-ID: <7388C5D6-75F3-4382-888A-9C8C2AA06780@comcast.net>


On Jul 22, 2015, at 12:50 AM, JIALU YAO wrote:

> Hello,
> 
> When I type the code *rownames(gene_exp_matrix)<-levels(geneidfactor)*,the
> error happens: Error in "rownames<-(*tmp*,value=character(0)):can not set
> object to 'rownames'.
> 
> I am using Mac OS X10.10.2.
> 
> How can I fix this problem?
> 

The error suggests that levels(geneidfactor) is returning a vector of length 0.

How do you fix that? The first thing I would do is check to see if spelling might be an issue and then see what str(geneidfactor) returns. If this process makes any sense (noting that the order of a dataframe factor variable might not be the same as the level-names), then my guess is that you failed to include the dataframe name where the factor vector resides.
> 

-- 

David Winsemius
Alameda, CA, USA


From jholtman at gmail.com  Wed Jul 22 18:22:47 2015
From: jholtman at gmail.com (jim holtman)
Date: Wed, 22 Jul 2015 12:22:47 -0400
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
In-Reply-To: <55AFBC00.6030606@univ-reims.fr>
References: <1437566961986-4710192.post@n4.nabble.com>
	<55AFBC00.6030606@univ-reims.fr>
Message-ID: <CAAxdm-6fkaOien4FaOOsg9A7O1rSWsW1QJWawGauZSMJyW09ZQ@mail.gmail.com>

forgot the reply to all:

These are serial dates within EXCEL.  Here is a way of converting them:

> as.Date(c(42460, 42426), origin = '1899-12-30')
[1] "2016-03-31" "2016-02-26"



Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Wed, Jul 22, 2015 at 11:51 AM, Ivan Calandra <ivan.calandra at univ-reims.fr
> wrote:

> Hi Antony,
>
> I am not sure it could work easily with package xlsx. Try using the
> function read_excel() from package readxl. This function allows for Dates
> to be read.
>
> HTH,
> Ivan
>
> --
> Ivan Calandra, ATER
> University of Reims Champagne-Ardenne
> GEGENAA - EA 3795
> CREA - 2 esplanade Roland Garros
> 51100 Reims, France
> +33(0)3 26 77 36 89
> ivan.calandra at univ-reims.fr
> https://www.researchgate.net/profile/Ivan_Calandra
>
> Le 22/07/15 14:09, R_Antony a ?crit :
>
>> Hi,
>>
>> Here i am having a ".xlsx" file and it contains various columns including
>> date-column[mm/dd/yy]-but it is not in the date format. I have to read
>> this
>> excel[.xlsx] file and need to get in dataframe. So i used "xlsx"-liabrary
>> and it was fine to read data. But the problem is, values in the date
>> column
>> is converting to some other value.
>>
>> for eg:-
>>
>> FF DATE
>> -----------
>> 3/31/2016
>> 2/26/2016
>> --
>> 1/2/2016
>>
>> [Values like "--" will come in the column to indicate that there is no
>> date
>> mentioned ]
>>
>> and i getting result like this,
>>
>> FF DATE
>> -----------
>> 42460
>> 42426
>>
>> 42125
>>
>> this is the code i am using for it,
>>
>> theData<-data.frame(read.xlsx2(InputFilePath, sheetIndex,
>> sheetName="Workflow_Report", startRow=3,colIndex=NULL, endRow=NULL,
>> as.data.frame=TRUE, header=TRUE))
>>
>> Aim :- I have to get actual "date-column" values in dataframe from xlsx
>> file.
>>
>> I tried many ways, Could someone please help ?
>>
>> Thanks in advance,
>> Antony.
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
>
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Jul 22 18:43:54 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 22 Jul 2015 12:43:54 -0400
Subject: [R] Sensitivity,Specificity and Youden Index
In-Reply-To: <CADGufDFX+yFb95sDrNXEfADyPoDFSpeR8OP41zDzMjNYnbb8jg@mail.gmail.com>
References: <CADGufDEMHVkD8=3vCGoZAojz0rqupqd9AdJAOR8OautEgcU07w@mail.gmail.com>
	<CAM_vjummE8Kjub-HCG5dwU0LxvLQiaxbeYxaQC2cUULFvAWXhg@mail.gmail.com>
	<CADGufDFX+yFb95sDrNXEfADyPoDFSpeR8OP41zDzMjNYnbb8jg@mail.gmail.com>
Message-ID: <CAM_vjukFK2i3ZYYJon6tgGurjv9rqEkGXpLX0vqKZ2P7O0Vt1w@mail.gmail.com>

Please reply to the list, not me. The list doesn't allow most
attachments (and most of the list recipients are highly unlikely to
open unsolicited binary files anyway). Do see the link I provided for
the appropriate way to create reproducible examples, including
providing data (hint: use dput(), and include your code).

Some evidence that you've looked at the packages suggested by rseek
would also be useful. Nobody's going to do your work for you.

Sarah

On Wed, Jul 22, 2015 at 12:34 PM, Jomy Jose <infojomy at gmail.com> wrote:
> Thank you for your reply
>
> I have attached my dataset,here 'malnut' is the
> outcome,.sensitivity,specificity and youden index of each factor from chew
> to cc(18 factors) and combination of these factors (any 6 out of 18) with
> the outcome measure 'malnut' has to be generated
>
> On Wed, Jul 22, 2015 at 7:30 PM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
>>
>> On Wed, Jul 22, 2015 at 9:50 AM, Jomy Jose <infojomy at gmail.com> wrote:
>> > How to calculate the sensitivity,specificity,Youden index for 18 factors
>> > and their combination (6 factors in each) with an outcome measure.
>>
>> www.rseek.org turns up a bunch of references to Youden index,
>> including packages.
>>
>> Without a reproducible example that includes some sample data (fake is
>> fine), the code you used, and some clear idea of what output you
>> expect, it's impossible to figure out how to help you. Here are some
>> suggestions for creating a good reproducible example:
>>
>> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
>>
>> Sarah
>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org

-- 
Sarah Goslee
http://www.functionaldiversity.org


From rshepard at appl-ecosys.com  Wed Jul 22 18:49:50 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Wed, 22 Jul 2015 09:49:50 -0700 (PDT)
Subject: [R] Langelier-Ludwig Plots
In-Reply-To: <9AE2FD3B-EEBD-4FF3-985A-DD560F653141@utoronto.ca>
References: <alpine.LNX.2.11.1507220847520.27006@localhost>
	<9AE2FD3B-EEBD-4FF3-985A-DD560F653141@utoronto.ca>
Message-ID: <alpine.LNX.2.11.1507220946210.27006@localhost>

On Wed, 22 Jul 2015, Boris Steipe wrote:

> According to  ...
>   http://info.ngwa.org/gwol/pdf/721000139.PDF (Graphical Interpretation of Water Quality Data)
> ... a Langelier-Ludwig plot is simply a scatterplot of cations vs. anions (in percent).
> Surely that would be beyond trivial to produce in R. Or am I missing a subtle something?

Boris,

   Yes, that's what it is. I'll see just how trivial it is to produce it in
the four-quadrant format I've seen used. It's a different layout from the
basic two-variable scatterplot.

   Thanks for the URL. I worked from a reference in a 2003 paper to the
original 1942 paper.

Rich


From angelat416 at yahoo.com  Wed Jul 22 19:15:39 2015
From: angelat416 at yahoo.com (Angela)
Date: Wed, 22 Jul 2015 10:15:39 -0700
Subject: [R] Ordering in Sankey diagram using R and googleVis
Message-ID: <1437585339.71836.YahooMailBasic@web161506.mail.bf1.yahoo.com>

Hello,

I am trying to figure out if there is a way to order the left side of a Sankey diagram from most frequent to least frequent. I am using R version 3.2.1 and using googleVis version 0.5.9 for the Sankey. I've tried sorting, but that does not work. Is there anyway to force it to arrange the left ("before") side in decreasing frequency? Something I am missing? Does not have to be using googleVis. Thank you!

-Angela

Example of the data I have, in a csv file:

before??? after
A??? B
A??? B
A??? B
A??? C
A??? A
A??? A
A??? B
D??? E
F??? B
F??? B
F??? F
G??? H
G??? A

I reformat the data in R so it looks like this:

before??? after??? count
A??? B??? 4
A??? C??? 1
A??? A??? 2
D??? E??? 1
F??? B??? 2
F??? F??? 1
G??? H??? 1
G??? A??? 1

Then plot using this:
plot( gvisSankey (data, from="before", to="after", weight="freq", options=list(width=600, height=800, 
??? sankey="{iterations: 2}")))


From amelia_marsh08 at yahoo.com  Wed Jul 22 18:41:29 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Wed, 22 Jul 2015 16:41:29 +0000 (UTC)
Subject: [R] Statistical distribution not fitting
Message-ID: <451818237.417248.1437583289428.JavaMail.yahoo@mail.yahoo.com>

Hello! 

(I dont know if I can raise this query here on this forum, but I had already raised on teh finance forum, but have not received any sugegstion, so now raising on this list. Sorry for the same. The query is about what to do, if no statistical distribution is fitting to data).

I am into risk management and deal with Operatioanl risk. As a part of BASEL II guidelines, we need to arrive at the capital charge the banks must set aside to counter any operational risk, if it happens. As a part of Loss Distribution Approach (LDA), we need to collate past loss events and use these loss amounts. The usual process as being practised in the industry is as follows - 

Using these historical loss amounts and using the various statistical tests like KS test, AD test, PP plot, QQ plot etc, we try to identify best statistical (continuous) distribution fitting this historical loss data. Then using these estimated parameters w.r.t. the statistical distribution, we simulate say 1 miliion loss anounts and then taking appropriate percentile (say 99.9%), we arrive at the capital charge. 

However, many a times, loss data is such that fitting of distribution to loss data is not possible. May be loss data is multimodal or has significant variability, making the fitting of distribution impossible. Can someone guide me how to deal with such data and what can be done to simulate losses using this historical loss data in R. 

My data is as follows - 

mydat <- c(829.53,4000,6000,1000,1063904,102400,22000,4000,4200,2000,10000,400, 459006, 7276,4000,100,4000,10000,613803.36, 825,1000,5000,4000,3000,84500,200, 2000,68000,97400,6267.8, 49500,27000,2100,10489.92,2200,2000,2000,1000,1900, 6000,5600,100,4000,14300,100,94100,1200,7000,2000,3000,1100,6900,1000,18500,6000,2000,4000,8400,11200,1000,15100,23300,4000,13100,4500,200,2000,50000,3900,3200,2000,2000,67000,2000,500,2000,1000,1900,10400,1900,2000,3200,6500,10000,2900,1000,14300,1000,2700,1500,12000,40000,25000,2800,5000,15000,4000,1000,21000,15000,16000,54000,1500,19200,2000,2000,1000,39000,5000,1100,18000,10000,3500,1000,10000,5000,14000,1800,4000,1000,300,4000,1000,100,1000,4400,2000,2000,12000,200,100,1000,1000,2000,1600,2000,4000,14000,4000,13500,1000,200,200,1000,18000,23000,41400,60000,500,3000,21000,6900,14600,1900,4000,4500,1000,2000,2000,1000,4100,2000,1000,2000,8000,3000,1500,2000,2000,3500,2000,2000,1000,3800,30000,55000,500,1000,1000,2000,62400,2000,3000,200,200! 
0,3500,2000,2000,500,3000,4500,1000,10000,2000,3000,3600,1000,2000,2000,5000,23000,2000,1900,2000,60000,2000,60000,20000,2000,2000,4600,1000,2000,1000,18000,6000,62000,68000,26800,50000,45900,16900,21500,2000,22700,2000,2000,32000,10000,5000,138000,159700,13000,2000,17619,2000,1000,4000,2000,1500,4000,20000,158900,74100,6000,24900,60000,500,1000,40000,10000,50000,800,4000,4900,6500,5000,400,500,3000,32300,24000,300,11500,2000,5000,1000,500,5000,5500,17450,56800,2000,1000,21400,22000,60000,3000,7500,3000,1000,1000,2000,1500,83700,2000,4000,170005,70000,6700,1500,3500,2000,10563.97,1500,25000,2000,2000,2267.57,1100,3100,2000,3500,10000,2000,6000,1500,200,20000,4000,46400,296900,150000,3700,7500,20000,48500,3500,12000,2500,4000,8500,1000,14500,1000,11000,2000,2000,120000,20000,7600,3000,2000,8000,1600,40000,2000,5000,34187.67,279100,9900,31300,814000,43500,5100,49500,4500,6262.38,100,10400,2400,1500,5000,2500,15000,40000,32500,41100,358600,109600,514300,258200,225900,402700,27! 
4300,75000,1000,56000,10000,4100,1000,15000,100,40000,7900,5000,105000 
,15100,2000,1100,2900,1500,600,500,1300,100,5000,5000,10000,10100,7000,40000,10500,5000,9500,1000,15200,2000,10000,10000,100,7800,3500,189900,58000,345000,151700,11000,6000,7000,15700,6000,3000,5000,10000,2000,1000,36000,1000,500,8000,9000,6000,2000,26500,6000,5000,97200,2000,5100,17000,2500,25500,24000,5400,90000,41500,6200,7500,5000,7000,41000,25000,1500,40000,5000,10000,21500,100,32000,32500,70000,500,66400,21000,5000,5000,12600,3000,6200,38900,10000,1000,60000,41100,1200,31300,2500,58000,4100,58000,42500) 

Sorry for the inconvenience. I do understand fitting of distribution to such data is not a full proof method, but this is what is the procedure that has been followed in the risk management risk industry. Please note that my question is not pertaining to operational risk. My question is if distributions are not fitting to a particular data, how do we proceed further to simualte data based on this data. 

Regards 

Amelia Marsh


From avsrivatsa at gmail.com  Wed Jul 22 20:13:40 2015
From: avsrivatsa at gmail.com (Abiram Srivatsa)
Date: Wed, 22 Jul 2015 14:13:40 -0400
Subject: [R] ggplot2 - Specifying Colors Manually
Message-ID: <CAKRqKSd63NJabh5XnEgZZoaVeATJY4W5cJBDCEVp8g9dABaDEQ@mail.gmail.com>

Hi,

Given a data frame, I'm trying to graph multiple lines on one graph, each
line being a different color and each colored line corresponding to a
specific name in the legend. Here is a very basic data sample to work with:

 x <- seq(0,40,10)
 y1 <- sample(1:50,5)
 y2 <- sample(1:50,5)

 mydf <- data.frame(x,y1,y2)

 p <- ggplot(mydf,aes(x=mydf$x)) +
geom_line(aes(y=mydf$y1,colour="green4")) +
geom_line(aes(y=mydf$y2,colour="blue2"))  +

 scale_color_manual(name="legend",values=c(y1="green4",y2="blue2"))


 p


When I run this, the entire plot is blank. What I WANT to show up is two
lines, one being the color of green4 and the other being blue2. Besides
that, I'm trying to associate the colors with the names "y1" and "y2" in
the legend, but my codes don't seem to be working.

I'm very new to R/ggplot2, and I really appreciate any and all help I can
get.

Thank you!

	[[alternative HTML version deleted]]


From bbolker at gmail.com  Wed Jul 22 21:44:45 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 22 Jul 2015 19:44:45 +0000
Subject: [R] glm help - final predictor variable NA
References: <1437505018036-4710161.post@n4.nabble.com>
Message-ID: <loom.20150722T210502-371@post.gmane.org>

matthewjones43 <matthew.jones <at> kellogg.ox.ac.uk> writes:

> 
> Hi, I am not a statistician and so I am sure whatever it is I 
> am doing wrong
> must be an obvious error for those who are...Basically I can
>  not understand
> why I get NA for variable 'CDSTotal' when running a glm? 
> Does anyone have an
> idea of why this might be happening?

It might be useful to look at 
http://stackoverflow.com/questions/7337761/
 linear-regression-na-estimate-just-for-last-coefficient/7341074#7341074

(broken URL).  You are overfitting the model by including
a predictor that can be expressed as a linear combination of
other predictors, and R is trying to handle it automatically.


From boris.steipe at utoronto.ca  Wed Jul 22 22:01:32 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Wed, 22 Jul 2015 16:01:32 -0400
Subject: [R] Statistical distribution not fitting
In-Reply-To: <451818237.417248.1437583289428.JavaMail.yahoo@mail.yahoo.com>
References: <451818237.417248.1437583289428.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <45F25AA6-BDA8-42F4-B6F9-2CF58334AFFA@utoronto.ca>

Try:

qqnorm(log(mydat))

That doesn't look too bad, does it? Now: where is the problem?

Cheers,
B.


On Jul 22, 2015, at 12:41 PM, Amelia Marsh <amelia_marsh08 at yahoo.com> wrote:

> Hello! 
> 
> (I dont know if I can raise this query here on this forum, but I had already raised on teh finance forum, but have not received any sugegstion, so now raising on this list. Sorry for the same. The query is about what to do, if no statistical distribution is fitting to data).
> 
> I am into risk management and deal with Operatioanl risk. As a part of BASEL II guidelines, we need to arrive at the capital charge the banks must set aside to counter any operational risk, if it happens. As a part of Loss Distribution Approach (LDA), we need to collate past loss events and use these loss amounts. The usual process as being practised in the industry is as follows - 
> 
> Using these historical loss amounts and using the various statistical tests like KS test, AD test, PP plot, QQ plot etc, we try to identify best statistical (continuous) distribution fitting this historical loss data. Then using these estimated parameters w.r.t. the statistical distribution, we simulate say 1 miliion loss anounts and then taking appropriate percentile (say 99.9%), we arrive at the capital charge. 
> 
> However, many a times, loss data is such that fitting of distribution to loss data is not possible. May be loss data is multimodal or has significant variability, making the fitting of distribution impossible. Can someone guide me how to deal with such data and what can be done to simulate losses using this historical loss data in R. 
> 
> My data is as follows - 
> 
> mydat <- c(829.53,4000,6000,1000,1063904,102400,22000,4000,4200,2000,10000,400, 459006, 7276,4000,100,4000,10000,613803.36, 825,1000,5000,4000,3000,84500,200, 2000,68000,97400,6267.8, 49500,27000,2100,10489.92,2200,2000,2000,1000,1900, 6000,5600,100,4000,14300,100,94100,1200,7000,2000,3000,1100,6900,1000,18500,6000,2000,4000,8400,11200,1000,15100,23300,4000,13100,4500,200,2000,50000,3900,3200,2000,2000,67000,2000,500,2000,1000,1900,10400,1900,2000,3200,6500,10000,2900,1000,14300,1000,2700,1500,12000,40000,25000,2800,5000,15000,4000,1000,21000,15000,16000,54000,1500,19200,2000,2000,1000,39000,5000,1100,18000,10000,3500,1000,10000,5000,14000,1800,4000,1000,300,4000,1000,100,1000,4400,2000,2000,12000,200,100,1000,1000,2000,1600,2000,4000,14000,4000,13500,1000,200,200,1000,18000,23000,41400,60000,500,3000,21000,6900,14600,1900,4000,4500,1000,2000,2000,1000,4100,2000,1000,2000,8000,3000,1500,2000,2000,3500,2000,2000,1000,3800,30000,55000,500,1000,1000,2000,62400,2000,3000,200,200!
> ! 
> 0,3500,2000,2000,500,3000,4500,1000,10000,2000,3000,3600,1000,2000,2000,5000,23000,2000,1900,2000,60000,2000,60000,20000,2000,2000,4600,1000,2000,1000,18000,6000,62000,68000,26800,50000,45900,16900,21500,2000,22700,2000,2000,32000,10000,5000,138000,159700,13000,2000,17619,2000,1000,4000,2000,1500,4000,20000,158900,74100,6000,24900,60000,500,1000,40000,10000,50000,800,4000,4900,6500,5000,400,500,3000,32300,24000,300,11500,2000,5000,1000,500,5000,5500,17450,56800,2000,1000,21400,22000,60000,3000,7500,3000,1000,1000,2000,1500,83700,2000,4000,170005,70000,6700,1500,3500,2000,10563.97,1500,25000,2000,2000,2267.57,1100,3100,2000,3500,10000,2000,6000,1500,200,20000,4000,46400,296900,150000,3700,7500,20000,48500,3500,12000,2500,4000,8500,1000,14500,1000,11000,2000,2000,120000,20000,7600,3000,2000,8000,1600,40000,2000,5000,34187.67,279100,9900,31300,814000,43500,5100,49500,4500,6262.38,100,10400,2400,1500,5000,2500,15000,40000,32500,41100,358600,109600,514300,258200,225900,402700,27! 
> 4300,75000,1000,56000,10000,4100,1000,15000,100,40000,7900,5000,105000 
> ,15100,2000,1100,2900,1500,600,500,1300,100,5000,5000,10000,10100,7000,40000,10500,5000,9500,1000,15200,2000,10000,10000,100,7800,3500,189900,58000,345000,151700,11000,6000,7000,15700,6000,3000,5000,10000,2000,1000,36000,1000,500,8000,9000,6000,2000,26500,6000,5000,97200,2000,5100,17000,2500,25500,24000,5400,90000,41500,6200,7500,5000,7000,41000,25000,1500,40000,5000,10000,21500,100,32000,32500,70000,500,66400,21000,5000,5000,12600,3000,6200,38900,10000,1000,60000,41100,1200,31300,2500,58000,4100,58000,42500) 
> 
> Sorry for the inconvenience. I do understand fitting of distribution to such data is not a full proof method, but this is what is the procedure that has been followed in the risk management risk industry. Please note that my question is not pertaining to operational risk. My question is if distributions are not fitting to a particular data, how do we proceed further to simualte data based on this data. 
> 
> Regards 
> 
> Amelia Marsh
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Wed Jul 22 22:04:31 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 22 Jul 2015 15:04:31 -0500
Subject: [R] ggplot2 - Specifying Colors Manually
In-Reply-To: <CAKRqKSd63NJabh5XnEgZZoaVeATJY4W5cJBDCEVp8g9dABaDEQ@mail.gmail.com>
References: <CAKRqKSd63NJabh5XnEgZZoaVeATJY4W5cJBDCEVp8g9dABaDEQ@mail.gmail.com>
Message-ID: <CABdHhvEECQkmP+oxTXJp=A9rn6Gdx7kvKp1iM0D2k3ACKCntLg@mail.gmail.com>

Try this:

ggplot(mydf,aes(x)) +
  geom_line(aes(y = y1, colour = "y1")) +
  geom_line(aes(y = y2, colour = "y2"))  +
  scale_color_manual(values = c(y1 = "green4", y2 = "blue2"))

Note that you don't need to use `mydf` and names in the manual scale
should match the values in the aes() calls.
Alsoit'smucheasiertoreadyourcodeifyouusespaces;)

Hadley

On Wed, Jul 22, 2015 at 1:13 PM, Abiram Srivatsa <avsrivatsa at gmail.com> wrote:
> Hi,
>
> Given a data frame, I'm trying to graph multiple lines on one graph, each
> line being a different color and each colored line corresponding to a
> specific name in the legend. Here is a very basic data sample to work with:
>
>  x <- seq(0,40,10)
>  y1 <- sample(1:50,5)
>  y2 <- sample(1:50,5)
>
>  mydf <- data.frame(x,y1,y2)
>
>  p <- ggplot(mydf,aes(x=mydf$x)) +
> geom_line(aes(y=mydf$y1,colour="green4")) +
> geom_line(aes(y=mydf$y2,colour="blue2"))  +
>
>  scale_color_manual(name="legend",values=c(y1="green4",y2="blue2"))
>
>
>  p
>
>
> When I run this, the entire plot is blank. What I WANT to show up is two
> lines, one being the color of green4 and the other being blue2. Besides
> that, I'm trying to associate the colors with the names "y1" and "y2" in
> the legend, but my codes don't seem to be working.
>
> I'm very new to R/ggplot2, and I really appreciate any and all help I can
> get.
>
> Thank you!
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://had.co.nz/


From ignited at yandex.ru  Wed Jul 22 21:38:14 2015
From: ignited at yandex.ru (jckhmmr)
Date: Wed, 22 Jul 2015 12:38:14 -0700 (PDT)
Subject: [R] Same results in arules after changing support and confidence
Message-ID: <1437593894430-4710218.post@n4.nabble.com>

Hello, everyone!

I'm new to R, so I'm sorry in advance if it's something obvious, but I still
can't figure it out.

I've started experimenting with apriori and everything was working fine,
i.e. I changed support and confidence and results were different depending
on combination.

But on the next after running the same code and same dataset I'm getting 0
rules no matter which combination I'm using. 

My dataset looks like this (foodmarket data):

Trans_Id,Product
3694728,Washington Berry Juice
3779788,Washington Berry Juice
4146666,Washington Berry Juice
4405313,Washington Berry Juice
etc.

My code is:

library('arules')
transactions <-
read.transactions(file="transactions.csv",format="single",sep=",",cols=c(1,2),rm.duplicates="false")

basket_rules <- apriori(transactions, parameter = list(sup = 0.05, conf =
0.01, target="rules",minlen=2))

With following results:
http://postimg.org/image/8qw4e1hw7/

And another one with different parameters but same result:
http://postimg.org/image/b9k7xfq91/


So, I have no idea how is that possible. Any input will be appreciated.

Thank you.



--
View this message in context: http://r.789695.n4.nabble.com/Same-results-in-arules-after-changing-support-and-confidence-tp4710218.html
Sent from the R help mailing list archive at Nabble.com.


From bbolker at gmail.com  Wed Jul 22 21:56:58 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 22 Jul 2015 19:56:58 +0000
Subject: [R] Statistical distribution not fitting
References: <451818237.417248.1437583289428.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <loom.20150722T215005-916@post.gmane.org>

Amelia Marsh <amelia_marsh08 <at> yahoo.com> writes:

 
> Hello!  (I dont know if I can raise this query here on this forum,
> but I had already raised on teh finance forum, but have not received
> any sugegstion, so now raising on this list. Sorry for the same. The
> query is about what to do, if no statistical distribution is fitting
> to data).
 
> I am into risk management and deal with Operatioanl risk. As a part
> of BASEL II guidelines, we need to arrive at the capital charge the
> banks must set aside to counter any operational risk, if it
> happens. As a part of Loss Distribution Approach (LDA), we need to
> collate past loss events and use these loss amounts. The usual
> process as being practised in the industry is as follows -
 
> Using these historical loss amounts and using the various
> statistical tests like KS test, AD test, PP plot, QQ plot etc, we
> try to identify best statistical (continuous) distribution fitting
> this historical loss data. Then using these estimated parameters
> w.r.t. the statistical distribution, we simulate say 1 miliion loss
> anounts and then taking appropriate percentile (say 99.9%), we
> arrive at the capital charge.
 
> However, many a times, loss data is such that fitting of
> distribution to loss data is not possible. May be loss data is
> multimodal or has significant variability, making the fitting of
> distribution impossible.  Can someone guide me how to deal with such
> data and what can be done to simulate losses using this historical
> loss data in R.
 
A skew-(log)-normal fit doesn't look too bad ... (whenever you
have positive data that are this strongly skewed, log-transforming
is a good step)

hist(log10(mydat),col="gray",breaks="FD",freq=FALSE)
## default breaks are much coarser:
## hist(log10(mydat),col="gray",breaks="Sturges",freq=FALSE)
lines(density(log10(mydat)),col=2,lwd=2)
library(fGarch)
ss <- snormFit(log10(mydat))
xvec <- seq(2,6.5,length=101)
lines(xvec,do.call(dsnorm,c(list(x=xvec),as.list(ss$par))),
      col="blue",lwd=2)
## or try a skew-Student-t: not very different:
ss2 <- sstdFit(log10(mydat))
lines(xvec,do.call(dsstd,c(list(x=xvec),as.list(ss2$estimate))),
      col="purple",lwd=2)

There are more flexible distributional families (Johnson,
log-spline ...)

Multimodal data are a different can of worms -- consider
fitting a finite mixture model ...


From boris.steipe at utoronto.ca  Wed Jul 22 22:50:31 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Wed, 22 Jul 2015 16:50:31 -0400
Subject: [R] Statistical distribution not fitting
In-Reply-To: <loom.20150722T215005-916@post.gmane.org>
References: <451818237.417248.1437583289428.JavaMail.yahoo@mail.yahoo.com>
	<loom.20150722T215005-916@post.gmane.org>
Message-ID: <BE620735-64F9-46E4-8423-A36532B479D7@utoronto.ca>

So - as you can see, your data can be modelled.

Now the interesting question is: what do you do with that knowledge. I know nearly nothing about your domain, but given that the data looks log-normal, I am curious abut the following:

 - Most of the events are in the small-loss category. But most of the damage is done by the rare large losses. Is it even meaningful to guard against a single 1/1000 event? Shouldn't you be saying: my contingency funds need to be large enough to allow survival of, say, a fiscal year with 99.9 % probability? This is a very different question.

 - If a loss occurs, in what time do the funds need to be replenished? Do you need to take series of events into account?

 - The model assumes that the data are independent. This is probably a poor (and dangerous) assumption.

Cheers,
B.





On Jul 22, 2015, at 3:56 PM, Ben Bolker <bbolker at gmail.com> wrote:

> Amelia Marsh <amelia_marsh08 <at> yahoo.com> writes:
> 
> 
>> Hello!  (I dont know if I can raise this query here on this forum,
>> but I had already raised on teh finance forum, but have not received
>> any sugegstion, so now raising on this list. Sorry for the same. The
>> query is about what to do, if no statistical distribution is fitting
>> to data).
> 
>> I am into risk management and deal with Operatioanl risk. As a part
>> of BASEL II guidelines, we need to arrive at the capital charge the
>> banks must set aside to counter any operational risk, if it
>> happens. As a part of Loss Distribution Approach (LDA), we need to
>> collate past loss events and use these loss amounts. The usual
>> process as being practised in the industry is as follows -
> 
>> Using these historical loss amounts and using the various
>> statistical tests like KS test, AD test, PP plot, QQ plot etc, we
>> try to identify best statistical (continuous) distribution fitting
>> this historical loss data. Then using these estimated parameters
>> w.r.t. the statistical distribution, we simulate say 1 miliion loss
>> anounts and then taking appropriate percentile (say 99.9%), we
>> arrive at the capital charge.
> 
>> However, many a times, loss data is such that fitting of
>> distribution to loss data is not possible. May be loss data is
>> multimodal or has significant variability, making the fitting of
>> distribution impossible.  Can someone guide me how to deal with such
>> data and what can be done to simulate losses using this historical
>> loss data in R.
> 
> A skew-(log)-normal fit doesn't look too bad ... (whenever you
> have positive data that are this strongly skewed, log-transforming
> is a good step)
> 
> hist(log10(mydat),col="gray",breaks="FD",freq=FALSE)
> ## default breaks are much coarser:
> ## hist(log10(mydat),col="gray",breaks="Sturges",freq=FALSE)
> lines(density(log10(mydat)),col=2,lwd=2)
> library(fGarch)
> ss <- snormFit(log10(mydat))
> xvec <- seq(2,6.5,length=101)
> lines(xvec,do.call(dsnorm,c(list(x=xvec),as.list(ss$par))),
>      col="blue",lwd=2)
> ## or try a skew-Student-t: not very different:
> ss2 <- sstdFit(log10(mydat))
> lines(xvec,do.call(dsstd,c(list(x=xvec),as.list(ss2$estimate))),
>      col="purple",lwd=2)
> 
> There are more flexible distributional families (Johnson,
> log-spline ...)
> 
> Multimodal data are a different can of worms -- consider
> fitting a finite mixture model ...
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Thu Jul 23 01:02:17 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 23 Jul 2015 11:02:17 +1200
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <2f3a88$12la7r@ironport10.mayo.edu>
References: <mailman.7.1437559202.1794.r-help@r-project.org>
	<2f3a88$12la7r@ironport10.mayo.edu>
Message-ID: <55B020F9.7020706@auckland.ac.nz>

On 23/07/15 01:15, Therneau, Terry M., Ph.D. wrote:

<SNIP>

> 3. Should you ever use it [i.e. Type III SS]?  No.  There is a very strong inverse
> correlation between "understand what it really is" and "recommend its
> use".   Stephen Senn has written very intellgently on the issues.

Terry --- can you please supply an explicit citation?  Ta.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From 538280 at gmail.com  Thu Jul 23 01:33:05 2015
From: 538280 at gmail.com (Greg Snow)
Date: Wed, 22 Jul 2015 17:33:05 -0600
Subject: [R] How to simulate informative censoring in a Cox PH model?
In-Reply-To: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>
References: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>
Message-ID: <CAFEqCdzk8cSwed-VdBrob1bzLQtx7rknK9UkNEU_bwt8auoD7A@mail.gmail.com>

I think that the Cox model still works well when the only information
in the censoring is conditional on variables in the model.  What you
describe could be called non-informative conditional on x.

To really see the difference you need informative censoring that
depends on something not included in the model.  One option would be
to use copulas to generate dependent data and then transform the
values using your Weibul.  Or you could generate your event times and
censoring times based on x1 and x2, but then only include x1 in the
model.

On Wed, Jul 22, 2015 at 2:20 AM, Daniel Meddings <dpmeddings at gmail.com> wrote:
> I wish to simulate event times where the censoring is informative, and to
> compare parameter estimator quality from a Cox PH model with estimates
> obtained from event times generated with non-informative censoring. However
> I am struggling to do this, and I conclude rather than a technical flaw in
> my code I instead do not understand what is meant by informative and
> un-informative censoring.
>
> My approach is to simulate an event time T dependent on a vector of
> covariates x having hazard function h(t|x)=lambda*exp(beta'*x)v*t^{v-1}.
> This corresponds to T~ Weibull(lambda(x),v), where the scale parameter
> lambda(x)=lambda*exp(beta'*x) depends on x and the shape parameter v is
> fixed. I have N subjects where T_{i}~ Weibull(lambda(x_{i}),v_{T}),
> lambda(x_{i})=lambda_{T}*exp(beta_{T}'*x_{i}), for i=1,...,N. Here I assume
> the regression coefficients are p-dimensional.
>
> I generate informative censoring times C_i~ Weibull(lambda(x_i),v_C),
> lambda(x_i)=lambda_C*exp(beta_C'*x_i) and compute Y_inf_i=min(T_i,C_i) and
> a censored flag delta_inf_i=1 if Y_inf_i <= C_i (an observed event), and
> delta_inf_i=0 if Y_inf_i > C_i (informatively censored: event not
> observed). I am convinced this is informative censoring because as long as
> beta_T~=0 and beta_C~=0 then for each subject the data generating process
> for T and C both depend on x.
>
> In contrast I generate non-informative censoring times
> D_i~Weibull(lambda_D*exp(beta_D),v_D), and compute Y_ninf_i=min(T_i,D_i)
> and a censored flag delta_ninf_i=1 if Y_ninf_i <= D_i (an observed event),
> and delta_ninf_i=0 if Y_ninf_i > D_i (non-informatively censored: event not
> observed). Here beta_D is a scalar. I "scale" the simulation by choosing
> the lambda_T, lambda_C and lambda_D parameters such that on average T_i<C_i
> and T_i<D_i to achieve X% of censored subjects for both Y_inf_i and
> Y_ninf_i.
>
> The problem is that even for say 30% censoring (which I think is high), the
> Cox PH parameter estimates using both Y_inf and Y_ninf are unbiased when I
> expected the estimates using Y_inf to be biased, and I think I see why:
> however different beta_C is from beta_T, a censored subject can presumably
> influence the estimation of beta_T only by affecting the set of subjects at
> risk at any time t, but this does not change the fact that every single
> Y_inf_i with delta_inf_i=1 will have been generated using beta_T only. Thus
> I do not see how my simulation can possibly produce biased estimates for
> beta_T using Y_inf.
>
> But then what is informative censoring if not based on this approach?
>
> Any help would be greatly appreciated.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From timcke at hotmail.de  Thu Jul 23 01:35:32 2015
From: timcke at hotmail.de (Marie-Louise)
Date: Wed, 22 Jul 2015 16:35:32 -0700 (PDT)
Subject: [R] interactive Map: Popups
Message-ID: <1437608132059-4710226.post@n4.nabble.com>

Hello,
I am trying to build a map of a country which shows informations to its
regions in a popup window as soon as someone clicks on a region. 
Thank you



--
View this message in context: http://r.789695.n4.nabble.com/interactive-Map-Popups-tp4710226.html
Sent from the R help mailing list archive at Nabble.com.


From macqueen1 at llnl.gov  Thu Jul 23 02:03:35 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 23 Jul 2015 00:03:35 +0000
Subject: [R] interactive Map: Popups
In-Reply-To: <1437608132059-4710226.post@n4.nabble.com>
References: <1437608132059-4710226.post@n4.nabble.com>
Message-ID: <D1D57CDB.13235D%macqueen1@llnl.gov>

I would start by looking at the "Graphics" and "Web Technologies" entries
in "Task Views" on CRAN. In addition, I suspect there might be some
packages listed in the Spatial task view that could do this.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/22/15, 4:35 PM, "R-help on behalf of Marie-Louise"
<r-help-bounces at r-project.org on behalf of timcke at hotmail.de> wrote:

>Hello,
>I am trying to build a map of a country which shows informations to its
>regions in a popup window as soon as someone clicks on a region.
>Thank you
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/interactive-Map-Popups-tp4710226.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From amelia_marsh08 at yahoo.com  Thu Jul 23 08:05:56 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Thu, 23 Jul 2015 06:05:56 +0000
Subject: [R] Statistical distribution not fitting
In-Reply-To: <BE620735-64F9-46E4-8423-A36532B479D7@utoronto.ca>
References: <BE620735-64F9-46E4-8423-A36532B479D7@utoronto.ca>
Message-ID: <289188321.256721.1437631556986.JavaMail.yahoo@mail.yahoo.com>

Dear Sir,

Thanks for your great guidance. Made me realize that I need to think out of box. 

As regards the low losses, BASEL guidelines do say to get rid of such low losses which create noise in analysing the losses caused by Operational Loss events. 
Its the right tail events do matter which represent low frequency high magnitude nature losses.
But my client is so adamant about it, that although we have shown them research papers about threshold limits which need to apply to arrive at some meaningful analyses, he is insisting that we do include these low losses too and fit some distribution.

Lastly using the command 



rsnorm(10000, mean = m, sd = s, xi = x) 

where m, s and x are the estimated parameters obtained from loss data. The usual procedure is to arrange these simulated values in descending order and select an observation representing (say 99.9%) and this is Value at Risk (VaR) which is say 'p'. 

My understanding is to this value 'p', I need to apply the transformation 10^p to arrive at the value which is in line with my original loss data. Am I right? 

Thanks again sir for your great help. I have something to look ahead now. 


Regards

Amelia

_____________________________________________________________________________



On Thursday, 23 July 2015 2:20 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote:
So - as you can see, your data can be modelled.

Now the interesting question is: what do you do with that knowledge. I know nearly nothing about your domain, but given that the data looks log-normal, I am curious abut the following:

- Most of the events are in the small-loss category. But most of the damage is done by the rare large losses. Is it even meaningful to guard against a single 1/1000 event? Shouldn't you be saying: my contingency funds need to be large enough to allow survival of, say, a fiscal year with 99.9 % probability? This is a very different question.

- If a loss occurs, in what time do the funds need to be replenished? Do you need to take series of events into account?

- The model assumes that the data are independent. This is probably a poor (and dangerous) assumption.

Cheers,
B.






On Jul 22, 2015, at 3:56 PM, Ben Bolker <bbolker at gmail.com> wrote:

> Amelia Marsh <amelia_marsh08 <at> yahoo.com> writes:
> 
> 
>> Hello!  (I dont know if I can raise this query here on this forum,
>> but I had already raised on teh finance forum, but have not received
>> any sugegstion, so now raising on this list. Sorry for the same. The
>> query is about what to do, if no statistical distribution is fitting
>> to data).
> 
>> I am into risk management and deal with Operatioanl risk. As a part
>> of BASEL II guidelines, we need to arrive at the capital charge the
>> banks must set aside to counter any operational risk, if it
>> happens. As a part of Loss Distribution Approach (LDA), we need to
>> collate past loss events and use these loss amounts. The usual
>> process as being practised in the industry is as follows -
> 
>> Using these historical loss amounts and using the various
>> statistical tests like KS test, AD test, PP plot, QQ plot etc, we
>> try to identify best statistical (continuous) distribution fitting
>> this historical loss data. Then using these estimated parameters
>> w.r.t. the statistical distribution, we simulate say 1 miliion loss
>> anounts and then taking appropriate percentile (say 99.9%), we
>> arrive at the capital charge.
> 
>> However, many a times, loss data is such that fitting of
>> distribution to loss data is not possible. May be loss data is
>> multimodal or has significant variability, making the fitting of
>> distribution impossible.  Can someone guide me how to deal with such
>> data and what can be done to simulate losses using this historical
>> loss data in R.
> 
> A skew-(log)-normal fit doesn't look too bad ... (whenever you
> have positive data that are this strongly skewed, log-transforming
> is a good step)
> 
> hist(log10(mydat),col="gray",breaks="FD",freq=FALSE)
> ## default breaks are much coarser:
> ## hist(log10(mydat),col="gray",breaks="Sturges",freq=FALSE)
> lines(density(log10(mydat)),col=2,lwd=2)
> library(fGarch)
> ss <- snormFit(log10(mydat))
> xvec <- seq(2,6.5,length=101)
> lines(xvec,do.call(dsnorm,c(list(x=xvec),as.list(ss$par))),
>      col="blue",lwd=2)
> ## or try a skew-Student-t: not very different:
> ss2 <- sstdFit(log10(mydat))
> lines(xvec,do.call(dsstd,c(list(x=xvec),as.list(ss2$estimate))),
>      col="purple",lwd=2)
> 
> There are more flexible distributional families (Johnson,
> log-spline ...)
> 
> Multimodal data are a different can of worms -- consider
> fitting a finite mixture model ...
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From antony.akkara at ge.com  Thu Jul 23 08:50:14 2015
From: antony.akkara at ge.com (R_Antony)
Date: Wed, 22 Jul 2015 23:50:14 -0700 (PDT)
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
In-Reply-To: <55AFBC00.6030606@univ-reims.fr>
References: <1437566961986-4710192.post@n4.nabble.com>
	<55AFBC00.6030606@univ-reims.fr>
Message-ID: <1437634214624-4710232.post@n4.nabble.com>

Hi Ivan,

This way i would've tried but i am using R 2.15 - ReadXL package will
support R >=3 versions.

Thanks,
Antony.



--
View this message in context: http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192p4710232.html
Sent from the R help mailing list archive at Nabble.com.


From antony.akkara at ge.com  Thu Jul 23 09:10:15 2015
From: antony.akkara at ge.com (R_Antony)
Date: Thu, 23 Jul 2015 00:10:15 -0700 (PDT)
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
In-Reply-To: <CAAxdm-6fkaOien4FaOOsg9A7O1rSWsW1QJWawGauZSMJyW09ZQ@mail.gmail.com>
References: <1437566961986-4710192.post@n4.nabble.com>
	<55AFBC00.6030606@univ-reims.fr>
	<CAAxdm-6fkaOien4FaOOsg9A7O1rSWsW1QJWawGauZSMJyW09ZQ@mail.gmail.com>
Message-ID: <1437635415233-4710233.post@n4.nabble.com>

Hi Jim,

My requirement is simple. I have to read date-values from the excel file
into dataframe, that's all.

and i tried using the way you mentioned and it works. Thank you very much !



--
View this message in context: http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192p4710233.html
Sent from the R help mailing list archive at Nabble.com.


From highstat at highstat.com  Thu Jul 23 11:41:18 2015
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 23 Jul 2015 17:41:18 +0800
Subject: [R] Stats courses in Queensland, Australia
Message-ID: <55B0B6BE.8090305@highstat.com>

Apologies for cross-posting


There are various remaining seats on the following two statistics courses:

Course 1:
Introduction to Generalized Linear Models with R. -Bayesian and 
frequentist approaches -
Dates: 11 - 14 August, 2015
Location: Hotel Grand Chancellor, Coral Coast Drive, Palm Cove. Australia


Course 2:
Introduction to Zero Inflated Models with R. - Frequentist and Bayesian 
approaches -
Dates: 17 - 21 August, 2015
Location: Hotel Grand Chancellor, Coral Coast Drive, Palm Cove. Australia


Course website:
http://highstat.com/statscourse.htm

Course flyers:
http://highstat.com/Courses/Flyers/Flyer2015_08PalmCoveI.pdf
http://highstat.com/Courses/Flyers/Flyer2015_08PalmCoveII.pdf

Both courses are pre-required knowledge for our 'Introduction to GLMs 
with spatial and temporal correlation'.


Kind regards,

Alain Zuur



-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From ivan.calandra at univ-reims.fr  Thu Jul 23 12:08:21 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Thu, 23 Jul 2015 12:08:21 +0200
Subject: [R] Read ".xlsx" and convert date-column value into Dataframe
In-Reply-To: <1437634214624-4710232.post@n4.nabble.com>
References: <1437566961986-4710192.post@n4.nabble.com>
	<55AFBC00.6030606@univ-reims.fr>
	<1437634214624-4710232.post@n4.nabble.com>
Message-ID: <55B0BD15.2050901@univ-reims.fr>

Hi Antony,

Except if you have good reasons to use R2.15, it is generally a good 
idea to upgrade to the latest version.
In any case, the solutions that were proposed on the list will 
definitely work fine.

There are however advantages of using readxl, in my opinion. readxl does 
not use dependencies; it correctly converts to dates; and if you have 
empty cells in numeric columns, xlsx will convert to NaN and not NA as 
would be expected (correct me if this behavior has been modified in the 
latest version). The problem with readxl is that it only reads! A 
similar write function would be nice!

Ivan

--
Ivan Calandra, ATER
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 23/07/15 08:50, R_Antony a ?crit :
> Hi Ivan,
>
> This way i would've tried but i am using R 2.15 - ReadXL package will
> support R >=3 versions.
>
> Thanks,
> Antony.
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Read-xlsx-and-convert-date-column-value-into-Dataframe-tp4710192p4710232.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Thu Jul 23 12:31:37 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 23 Jul 2015 20:31:37 +1000
Subject: [R] Langelier-Ludwig Plots
In-Reply-To: <alpine.LNX.2.11.1507220946210.27006@localhost>
References: <alpine.LNX.2.11.1507220847520.27006@localhost>
	<9AE2FD3B-EEBD-4FF3-985A-DD560F653141@utoronto.ca>
	<alpine.LNX.2.11.1507220946210.27006@localhost>
Message-ID: <CA+8X3fUcpk-ydtbsTSZhPaQ3vAXKaaHY05zFaLx6hosV+44smg@mail.gmail.com>

Hi Rich,
Thanks to the link provided by Boris, I now realize that the third
example in the radial.plot function help page is almost a Tickell
diagram. Another plotting function that is close to the illustrations
in that paper is starPie. Learn something every day. Hope this is
helpful.

Jim


On Thu, Jul 23, 2015 at 2:49 AM, Rich Shepard <rshepard at appl-ecosys.com> wrote:
> On Wed, 22 Jul 2015, Boris Steipe wrote:
>
>> According to  ...
>>   http://info.ngwa.org/gwol/pdf/721000139.PDF (Graphical Interpretation of
>> Water Quality Data)
>> ... a Langelier-Ludwig plot is simply a scatterplot of cations vs. anions
>> (in percent).
>> Surely that would be beyond trivial to produce in R. Or am I missing a
>> subtle something?
>
>
> Boris,
>
>   Yes, that's what it is. I'll see just how trivial it is to produce it in
> the four-quadrant format I've seen used. It's a different layout from the
> basic two-variable scatterplot.
>
>   Thanks for the URL. I worked from a reference in a 2003 paper to the
> original 1942 paper.
>
> Rich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Thu Jul 23 12:43:30 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 23 Jul 2015 20:43:30 +1000
Subject: [R] Ordering in Sankey diagram using R and googleVis
In-Reply-To: <1437585339.71836.YahooMailBasic@web161506.mail.bf1.yahoo.com>
References: <1437585339.71836.YahooMailBasic@web161506.mail.bf1.yahoo.com>
Message-ID: <CA+8X3fXoxpf6ojw2_vc4J7EzDbCb4vtpTFDaCx19wHiwq7ZnDQ@mail.gmail.com>

Hi Angela,
Assuming that your reformatted data is named "data", have you tried:

data[order(data$count,data$before,decreasing=TRUE),]

Jim

On Thu, Jul 23, 2015 at 3:15 AM, Angela via R-help <r-help at r-project.org> wrote:
> Hello,
>
> I am trying to figure out if there is a way to order the left side of a Sankey diagram from most frequent to least frequent. I am using R version 3.2.1 and using googleVis version 0.5.9 for the Sankey. I've tried sorting, but that does not work. Is there anyway to force it to arrange the left ("before") side in decreasing frequency? Something I am missing? Does not have to be using googleVis. Thank you!
>
> -Angela
>
> Example of the data I have, in a csv file:
>
> before    after
> A    B
> A    B
> A    B
> A    C
> A    A
> A    A
> A    B
> D    E
> F    B
> F    B
> F    F
> G    H
> G    A
>
> I reformat the data in R so it looks like this:
>
> before    after    count
> A    B    4
> A    C    1
> A    A    2
> D    E    1
> F    B    2
> F    F    1
> G    H    1
> G    A    1
>
> Then plot using this:
> plot( gvisSankey (data, from="before", to="after", weight="freq", options=list(width=600, height=800,
>     sankey="{iterations: 2}")))
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erich.neuwirth at univie.ac.at  Thu Jul 23 13:28:56 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 23 Jul 2015 13:28:56 +0200
Subject: [R] interactive Map: Popups
In-Reply-To: <1437608132059-4710226.post@n4.nabble.com>
References: <1437608132059-4710226.post@n4.nabble.com>
Message-ID: <3D7241A7-DD00-44EF-9DB7-1642B4235C0C@univie.ac.at>

I am quite happy with that package leaflet which is not yet on CRAN
but available on Githib.

https://github.com/rstudio/leaflet <https://github.com/rstudio/leaflet>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 670 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150723/4aac61fb/attachment.bin>

From angelat416 at yahoo.com  Thu Jul 23 17:57:57 2015
From: angelat416 at yahoo.com (Angela)
Date: Thu, 23 Jul 2015 08:57:57 -0700
Subject: [R] Ordering in Sankey diagram using R and googleVis
In-Reply-To: <CA+8X3fXoxpf6ojw2_vc4J7EzDbCb4vtpTFDaCx19wHiwq7ZnDQ@mail.gmail.com>
Message-ID: <1437667077.64279.YahooMailBasic@web161506.mail.bf1.yahoo.com>

Hi Jim,

I tried it and it while it does make the diagram look more like what I want, there are a few categories still out of order.  Thank you for your help!

-Angela

--------------------------------------------
On Thu, 7/23/15, Jim Lemon <drjimlemon at gmail.com> wrote:

 Subject: Re: [R] Ordering in Sankey diagram using R and googleVis

 Cc: "r-help mailing list" <r-help at r-project.org>
 Date: Thursday, July 23, 2015, 6:43 AM

 Hi Angela,
 Assuming that your reformatted data is named
 "data", have you tried:

 data[order(data$count,data$before,decreasing=TRUE),]

 Jim

 On
 Thu, Jul 23, 2015 at 3:15 AM, Angela via R-help <r-help at r-project.org>
 wrote:
 > Hello,
 >
 > I am trying to figure out if there is a
 way to order the left side of a Sankey diagram from most
 frequent to least frequent. I am using R version 3.2.1 and
 using googleVis version 0.5.9 for the Sankey. I've tried
 sorting, but that does not work. Is there anyway to force it
 to arrange the left ("before") side in decreasing
 frequency? Something I am missing? Does not have to be using
[[elided Yahoo spam]]
 >
 > -Angela
 >
 > Example of the data I have, in a csv
 file:
 >
 > before? ?
 after
 > A? ? B
 > A?
 ? B
 > A? ? B
 > A?
 ? C
 > A? ? A
 > A?
 ? A
 > A? ? B
 > D?
 ? E
 > F? ? B
 > F?
 ? B
 > F? ? F
 > G?
 ? H
 > G? ? A
 >
 > I reformat the data in R so it looks like
 this:
 >
 > before? ?
 after? ? count
 > A? ? B? ? 4
 > A? ? C? ? 1
 > A?
 ? A? ? 2
 > D? ? E? ? 1
 > F? ? B? ? 2
 > F?
 ? F? ? 1
 > G? ? H? ? 1
 > G? ? A? ? 1
 >
 > Then plot using this:
 >
 plot( gvisSankey (data, from="before",
 to="after", weight="freq",
 options=list(width=600, height=800,
 >?
 ???sankey="{iterations: 2}")))
 >
 >
 ______________________________________________
 > R-help at r-project.org
 mailing list -- To UNSUBSCRIBE and more, see
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
 > and provide commented, minimal,
 self-contained, reproducible code.


From 538280 at gmail.com  Thu Jul 23 18:07:13 2015
From: 538280 at gmail.com (Greg Snow)
Date: Thu, 23 Jul 2015 10:07:13 -0600
Subject: [R] interactive Map: Popups
In-Reply-To: <1437608132059-4710226.post@n4.nabble.com>
References: <1437608132059-4710226.post@n4.nabble.com>
Message-ID: <CAFEqCdxyN5bjYY1p8YLHT4QwqkQhayOOx1xi0Uv1JXC_aDmB6Q@mail.gmail.com>

You might be interested in the HWidentify and HTKidentify functions in
the TeachingDemos package.  They currently don't do maps, but since
the functions are pure R code it would not be hard to modify them.

On Wed, Jul 22, 2015 at 5:35 PM, Marie-Louise <timcke at hotmail.de> wrote:
> Hello,
> I am trying to build a map of a country which shows informations to its
> regions in a popup window as soon as someone clicks on a region.
> Thank you
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/interactive-Map-Popups-tp4710226.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From mbressan at arpa.veneto.it  Thu Jul 23 18:14:32 2015
From: mbressan at arpa.veneto.it (Massimo Bressan)
Date: Thu, 23 Jul 2015 18:14:32 +0200
Subject: [R] shift by one column given rows in a dataframe
Message-ID: <55B112E8.2090103@arpa.veneto.it>

by considering the following reproducible example:

v0<-c("a","xxx","c",rep("xxx",2))
v1<-c(1,"b",3,"d","e")
v2<-c(6,2,8,4,5)
v3<-c("xxx",7,"xxx",9,10)

df_start<-data.frame(v0,v1,v2,v3)
df_start

v0<-letters[1:5]
v1<-1:5
v2<-6:10

df_end<-data.frame(v0,v1,v2)
df_end

I need  to shift by one column some given rows in the initial data frame 
called "df_start" so that to get the final structure as in "df_end";
please consider that the value "xxx" in the rows of "df_start" can be 
anything so that I necessarly need to apply by row index position (in my 
reproducible example rows: 2, 3, 5);

I'm really stuck with that problem and I can not conceive any viable 
solution up to now

any hints?

best regards

m


From sarah.goslee at gmail.com  Thu Jul 23 18:27:31 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 23 Jul 2015 12:27:31 -0400
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <55B112E8.2090103@arpa.veneto.it>
References: <55B112E8.2090103@arpa.veneto.it>
Message-ID: <CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>

With one minor change to your reproducible example (thank you!):

df_start <- data.frame(v0,v1,v2,v3, stringsAsFactors=FALSE)
data.frame(t(apply(df_start, 1, function(i)i[!grepl("xxx", i)])),
stringsAsFactors=FALSE)

I'll leave it to you to deal with columns that you'd like to have
numeric. (You might also try str(df_start)).

Sarah

On Thu, Jul 23, 2015 at 12:14 PM, Massimo Bressan
<mbressan at arpa.veneto.it> wrote:
> by considering the following reproducible example:
>
> v0<-c("a","xxx","c",rep("xxx",2))
> v1<-c(1,"b",3,"d","e")
> v2<-c(6,2,8,4,5)
> v3<-c("xxx",7,"xxx",9,10)
>
> df_start<-data.frame(v0,v1,v2,v3)
> df_start
>
> v0<-letters[1:5]
> v1<-1:5
> v2<-6:10
>
> df_end<-data.frame(v0,v1,v2)
> df_end
>
> I need  to shift by one column some given rows in the initial data frame
> called "df_start" so that to get the final structure as in "df_end";
> please consider that the value "xxx" in the rows of "df_start" can be
> anything so that I necessarly need to apply by row index position (in my
> reproducible example rows: 2, 3, 5);
>
> I'm really stuck with that problem and I can not conceive any viable
> solution up to now
>
> any hints?
>
> best regards
>
> m
>
-- 
Sarah Goslee
http://www.functionaldiversity.org


From bbolker at gmail.com  Thu Jul 23 19:47:37 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 23 Jul 2015 17:47:37 +0000
Subject: [R] Statistical distribution not fitting
References: <BE620735-64F9-46E4-8423-A36532B479D7@utoronto.ca>
	<289188321.256721.1437631556986.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <loom.20150723T194405-191@post.gmane.org>

Amelia Marsh via R-help <r-help <at> r-project.org> writes:

> 
> Dear Sir,
> 

 [snip]
 
> Lastly using the command rsnorm(10000, mean = m, sd = s, xi = x)
> where m, s and x are the estimated parameters obtained from loss
> data. The usual procedure is to arrange these simulated values in
> descending order and select an observation representing (say 99.9%)
> and this is Value at Risk (VaR) which is say 'p'.

> My understanding is to this value 'p', I need to apply the
> transformation 10^p to arrive at the value which is in line with my
> original loss data. Am I right?

 [snip; sorry to remove context, but Gmane doesn't like it]

(1) you can probably calculate the 0.999 quantile directly from
qsnorm(0.999, [params]) rather than by simulating ...
(2) ... I believe that my original example used log(), so you
would need to use exp() (not 10^x) to get back to the original scale ...
(3) ... if you're concerned about extreme events it would be
a very good idea to use the skew-t rather than the skew-Normal
(4) you should certainly consider Boris Steipe's concerns about
non-independence (although I have to admit that without more
information and further time/effort/thought I don't have any
simple suggestions how ...)

  cheers
    Ben Bolker


From stxfc at nottingham.ac.uk  Thu Jul 23 12:48:37 2015
From: stxfc at nottingham.ac.uk (=?ISO-8859-1?Q?Fran=E7ois_Collin?=)
Date: Thu, 23 Jul 2015 11:48:37 +0100
Subject: [R] Contr.sum and coefficient tests
Message-ID: <2E3BD18875277F48A593197016F0B10441800709E8@EXCHANGE2.ad.nottingham.ac.uk>

Dear all,

I would like to run a linear model which includes two factors:
- The first one has two levels, including a reference level. Thus I have to use the treatment contrast (contr.treatment, reference level effect = 0, then the intercept).
- The second is a 6-level factor without reference contrast nor order. So, I would like to use sum contrat: sum of the effects = 0.

The problem arises when it comes to the coefficient test. I understand it is not relevant to test the reference level for the first factor as the reference level is set to 0. However, using sum contrast for the second factor, I would have expected the test of each level to be included in the classical summary print of the lm function result but it is not. And here is my problem, how can I have every coefficients tested and printed in the summary output when my factor is studied from this sum.contrast standpoint?


Here is an example from a previous thread:
(http://comments.gmane.org/gmane.comp.lang.r.general/258886)

==============================
> x <- as.factor(c(1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3))
> y <- c(1.1,1.15,1.2,1.1,1.1,1.1,1.2,1.2,1.2,2.1,2.2,2.3,2.4,2.5,
> +          2.6,2.7,2.8,2.9,3,3.1)
> test <- data.frame(x,y)
> reg1 <- lm(y~C(x,contr.sum),data=test)
> summary(reg1)

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)   
(Intercept)       1.63333    0.06577  24.834 8.48e-15 ***
C(x, contr.sum)1 -0.48333    0.10792  -4.479  0.00033 ***
C(x, contr.sum)2 -0.48333    0.08936  -5.409 4.70e-05 ***
==============================

How can I include the third factor level in this table?

I fill the answer provided in this previous thread is relevant for contr.treatment but not for contr.sum. Am I right? Or can you explain me?

Many thanks,
Fanch



This message and any attachment are intended solely for the addressee
and may contain confidential information. If you have received this
message in error, please send it back to me, and immediately delete it. 

Please do not use, copy or disclose the information contained in this
message or in any attachment.  Any views or opinions expressed by the
author of this email do not necessarily reflect the views of the
University of Nottingham.

This message has been checked for viruses but the contents of an
attachment may still contain software viruses which could damage your
computer system, you are advised to perform your own checks. Email
communications with the University of Nottingham may be monitored as
permitted by UK legislation.


From yangwenyue900780 at 163.com  Thu Jul 23 17:35:21 2015
From: yangwenyue900780 at 163.com (wenyueyang)
Date: Thu, 23 Jul 2015 08:35:21 -0700 (PDT)
Subject: [R] How to calculate the average direct effect,
 average total effect and average indirect effect for spatial
 regression models with spatial lag of dependent variable
Message-ID: <1437665721814-4710253.post@n4.nabble.com>

Hi,

I am using four spatial regression models (SAR, SEM, SAC and SDM) to
evaluate the spillover effect of some factors. LeSage and Pace (2009)
pointed out that when the spatial lag of the dependent variable is included
in the model, parameter estimates lose their conventional interpretation as
marginal effects, because the spatial lag gives rise to a series of feedback
loops and spillover effects across regions. Therefore, I need to calculate
the three different marginal effects: average direct effect, average total
effect and average indirect (spillover) effect, and this is what I don't
know how to perform in R.
Can you teach me about how to calculate the average direct effect, average
total effect and average indirect (spillover) effect for the spatial
regression models in R and tell me the related R code? 
I would like to express my great appreciation to you!

Thank you very much and best regards.

Yours sincerely,
Wenyue Yang



--
View this message in context: http://r.789695.n4.nabble.com/How-to-calculate-the-average-direct-effect-average-total-effect-and-average-indirect-effect-for-spate-tp4710253.html
Sent from the R help mailing list archive at Nabble.com.


From timcke at hotmail.de  Thu Jul 23 19:18:41 2015
From: timcke at hotmail.de (Marie-Louise)
Date: Thu, 23 Jul 2015 10:18:41 -0700 (PDT)
Subject: [R] interactive Map: Popups
In-Reply-To: <CAFEqCdxyN5bjYY1p8YLHT4QwqkQhayOOx1xi0Uv1JXC_aDmB6Q@mail.gmail.com>
References: <1437608132059-4710226.post@n4.nabble.com>
	<CAFEqCdxyN5bjYY1p8YLHT4QwqkQhayOOx1xi0Uv1JXC_aDmB6Q@mail.gmail.com>
Message-ID: <1437671921926-4710257.post@n4.nabble.com>

Thank you both very much for your tips, and excuse my horrible english
skills. 

R is new to me so I tried to work with code that was already there.
I managed to get close to what I wanted with

require(googleVis)
G4 = gvisGeoChart(con2, 
                  locationvar = "DE", 
                  colorvar = "data_7", hovervar = "data_6", options=list(
                  width=800, height=600, region="DE", 
displayMode="regions", 
                  resolution="provinces"))
                  
plot(G4)

but it seems like googleVis does not support resolution="provinces" for
Germany :(
Do you know anything that simple as this code to get to what I wanted? 
Thank you very much



--
View this message in context: http://r.789695.n4.nabble.com/interactive-Map-Popups-tp4710226p4710257.html
Sent from the R help mailing list archive at Nabble.com.


From therneau at mayo.edu  Thu Jul 23 21:07:00 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 23 Jul 2015 14:07:00 -0500
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <55B020F9.7020706@auckland.ac.nz>
References: <mailman.7.1437559202.1794.r-help@r-project.org>
	<2f3a88$12la7r@ironport10.mayo.edu> <55B020F9.7020706@auckland.ac.nz>
Message-ID: <2f3a88$1324qq@ironport10.mayo.edu>

The following are in parody (but like all good parody correct wrt the salient features). 
The musings of
Guernsey McPhearson
    http://www.senns.demon.co.uk/wprose.html#Mixed
    http://www.senns.demon.co.uk/wprose.html#FDA


In formal publication:
  Senn, Statistical Issues in Drug Development, second edition, Chapter 14: Multicentre Trials
  Senn, The many modes of meta, Drug information journal, 34:535-549, 2000.

The second points out that in a meta analysis no one would ever consider giving both large 
and small trials equal weights, and relates that to several other bits of standard 
practice.  The 'equal weights' notion embedded in a fixed effects model + SAS type 3 is an 
isolated backwater.

Terry T.

PS. The "Devils' Drug Development Dictionary" at the same source has some gems. Three 
rather random choices:

Bayesian - One who, vaguely expecting a horse and catching a glimpse of a donkey, strongly 
concludes he has seen a mule.

Medical Statistician - One who won't accept that Columbus discovered America because he 
said he was looking for India in the trial Plan.

Trend Towards Significance - An ever present help in times of trouble.



On 07/22/2015 06:02 PM, Rolf Turner wrote:
> On 23/07/15 01:15, Therneau, Terry M., Ph.D. wrote:
>
> <SNIP>
>
>> 3. Should you ever use it [i.e. Type III SS]?  No.  There is a very strong inverse
>> correlation between "understand what it really is" and "recommend its
>> use".   Stephen Senn has written very intellgently on the issues.
>
> Terry --- can you please supply an explicit citation?  Ta.
>
> cheers,
>
> Rolf
>


From varinsacha at yahoo.fr  Thu Jul 23 21:50:38 2015
From: varinsacha at yahoo.fr (varin sacha)
Date: Thu, 23 Jul 2015 19:50:38 +0000 (UTC)
Subject: [R] Confidence intervals of G&K gamma statistics using bootstrap
Message-ID: <1773326778.910146.1437681038747.JavaMail.yahoo@mail.yahoo.com>

Dear R-Experts,

I am trying to calculate the confidence intervals of the Goodman & Kruskal gamma statistics using bootstrap. There is no gamma function in the boot package. There is a gamma function in the base package, but it is the usual mathematical function.


So, I decide to try to calculate the confidence intervals using this site : 
http://www.statmethods.net/advstats/bootstrapping.html

I get a "normal" warning message, but my R code is not working and I don't understand where my mistake(s) is (are).
Here is the reproducible example with imaginary/fake data.

install.packages("ryouready") 
library(ryouready) 
a=c("satisfait", "pas satisfait", "tres satisfait","satisfait","tres satisfait","pas satisfait","satisfait","satisfait","tres satisfait","pas satisfait") 
b=c("grand", "petit", "petit", "grand", "petit", "grand", "grand", "petit", "petit", "grand")
x=table(a,b) 
ord.gamma(x) 
# calculate Goodman & Kruskal gamma using bootstrap 
library(boot) 
GK <- function(formula, data, indices) { 
d <- data[indices,] # allows boot to select sample 
tab <- xtabs(formula, data=d) 
stat <- ord.gamma(tab) 
return(stat)
} 
# bootstrapping with 2000 replications 
results <- boot(data=d, statistic=GK,R=2000) 
# view results 
results
plot(results) 
# get 95% confidence interval 
boot.ci(results, type="all") 

Best Regards, thanks for your precious help!

SV


From mbressan at arpa.veneto.it  Thu Jul 23 21:56:05 2015
From: mbressan at arpa.veneto.it (maxbre)
Date: Thu, 23 Jul 2015 12:56:05 -0700 (PDT)
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
Message-ID: <1437681365994-4710271.post@n4.nabble.com>

Hi

thank you for your reply: it's a neat solution but unfortunately not
applicable to my specific case;

in fact as I specified in my first post (I may have been not enough clear,
sorry for that!) I can not rely on any search method grep-like because the
value "xxx" in the rows of "df_start" can be anything (string or numeric and
always different) so that I necessarely need to apply by row index position
(i.e. in my reproducible example rows: 2, 3, 5); 

thank you again for your kind help but still searching for a solution...

best



--
View this message in context: http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710271.html
Sent from the R help mailing list archive at Nabble.com.


From erich.neuwirth at univie.ac.at  Thu Jul 23 22:00:18 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 23 Jul 2015 22:00:18 +0200
Subject: [R] interactive Map: Popups
In-Reply-To: <CACxE24n6NN+7=rGavaiiAAUyj9W-bD_NUR5V4RpND8KCp3PSPw@mail.gmail.com>
References: <1437608132059-4710226.post@n4.nabble.com>
	<3D7241A7-DD00-44EF-9DB7-1642B4235C0C@univie.ac.at>
	<CACxE24n6NN+7=rGavaiiAAUyj9W-bD_NUR5V4RpND8KCp3PSPw@mail.gmail.com>
Message-ID: <31D66263-1EAB-4DC4-B4EB-087EA2294A05@univie.ac.at>

Some shapefiles for Germany can be found here

http://www.statsilk.com/maps/download-free-shapefile-maps <http://www.statsilk.com/maps/download-free-shapefile-maps>

> On Jul 23, 2015, at 21:36, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> 
> Hello Erich:
> 
> I just looked at your leaflet package and its examples.  It is awesome!
> 
> Thanks,
> Erin
> 
> 
> On Thu, Jul 23, 2015 at 6:28 AM, Erich Neuwirth <erich.neuwirth at univie.ac.at <mailto:erich.neuwirth at univie.ac.at>> wrote:
> I am quite happy with that package leaflet which is not yet on CRAN
> but available on Githib.
> 
> https://github.com/rstudio/leaflet <https://github.com/rstudio/leaflet> <https://github.com/rstudio/leaflet <https://github.com/rstudio/leaflet>>
> 
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> 
> --
> Erin Hodgess
> Associate Professor
> Department of Mathematical and Statistics
> University of Houston - Downtown
> mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 670 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150723/70b01028/attachment.bin>

From bgunter.4567 at gmail.com  Thu Jul 23 22:06:15 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 23 Jul 2015 13:06:15 -0700
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <1437681365994-4710271.post@n4.nabble.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
Message-ID: <CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>

Ah, so apparently you require some sort of psychic abilities...

For how else would one choose which three values to keep in a row that was:

a  2   b  5

based on your specification that "xxx could be anything."

Cheers,

Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 23, 2015 at 12:56 PM, maxbre <mbressan at arpa.veneto.it> wrote:
> Hi
>
> thank you for your reply: it's a neat solution but unfortunately not
> applicable to my specific case;
>
> in fact as I specified in my first post (I may have been not enough clear,
> sorry for that!) I can not rely on any search method grep-like because the
> value "xxx" in the rows of "df_start" can be anything (string or numeric and
> always different) so that I necessarely need to apply by row index position
> (i.e. in my reproducible example rows: 2, 3, 5);
>
> thank you again for your kind help but still searching for a solution...
>
> best
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710271.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From daniel319 at gmail.com  Thu Jul 23 22:06:37 2015
From: daniel319 at gmail.com (daniel)
Date: Thu, 23 Jul 2015 17:06:37 -0300
Subject: [R] interactive Map: Popups
In-Reply-To: <31D66263-1EAB-4DC4-B4EB-087EA2294A05@univie.ac.at>
References: <1437608132059-4710226.post@n4.nabble.com>
	<3D7241A7-DD00-44EF-9DB7-1642B4235C0C@univie.ac.at>
	<CACxE24n6NN+7=rGavaiiAAUyj9W-bD_NUR5V4RpND8KCp3PSPw@mail.gmail.com>
	<31D66263-1EAB-4DC4-B4EB-087EA2294A05@univie.ac.at>
Message-ID: <CAPfrkhmH+poL6MN=gAVeTavEsk6uaeFz0A9u9G5ZYvjMp=fVUA@mail.gmail.com>

Marie-Louise,

As long sa I know you have to gie googleVis the ISO code for provinces.

GR <- data.frame( ISO = c("DE-BW",
"DE-BY",
"DE-BE",
"DE-BB",
"DE-HB",
"DE-HH",
"DE-HE",
"DE-MV",
"DE-NI",
"DE-NW",
"DE-RP",
"DE-SL",
"DE-SN",
"DE-ST",
"DE-SH",
"DE-TH"),
name = c("Baden-W?rttemberg",
"Bayern",
"Berlin",
"Brandenburg",
"Bremen",
"Hamburg",
"Hessen",
"Mecklenburg-Vorpommern",
"Niedersachsen",
"Nordrhein-Westfalen",
"Rheinland-Pfalz",
"Saarland",
"Sachsen",
"Sachsen-Anhalt",
"Schleswig-Holstein",
"Th?ringen"), value = 1:16)

G3 <- gvisGeoMap(GR, locationvar='ISO',
numvar='value', options=list(region="DE",
displayMode="regions", resolution="provinces,"))
plot(G3)

Daniel Merino


2015-07-23 17:00 GMT-03:00 Erich Neuwirth <erich.neuwirth at univie.ac.at>:

> Some shapefiles for Germany can be found here
>
> http://www.statsilk.com/maps/download-free-shapefile-maps <
> http://www.statsilk.com/maps/download-free-shapefile-maps>
>
> > On Jul 23, 2015, at 21:36, Erin Hodgess <erinm.hodgess at gmail.com> wrote:
> >
> > Hello Erich:
> >
> > I just looked at your leaflet package and its examples.  It is awesome!
> >
> > Thanks,
> > Erin
> >
> >
> > On Thu, Jul 23, 2015 at 6:28 AM, Erich Neuwirth <
> erich.neuwirth at univie.ac.at <mailto:erich.neuwirth at univie.ac.at>> wrote:
> > I am quite happy with that package leaflet which is not yet on CRAN
> > but available on Githib.
> >
> > https://github.com/rstudio/leaflet <https://github.com/rstudio/leaflet>
> <https://github.com/rstudio/leaflet <https://github.com/rstudio/leaflet>>
> >
> >
> > ______________________________________________
> > R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help <
> https://stat.ethz.ch/mailman/listinfo/r-help>
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html <
> http://www.r-project.org/posting-guide.html>
> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > Erin Hodgess
> > Associate Professor
> > Department of Mathematical and Statistics
> > University of Houston - Downtown
> > mailto: erinm.hodgess at gmail.com <mailto:erinm.hodgess at gmail.com>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Daniel

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Jul 23 22:17:53 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 23 Jul 2015 16:17:53 -0400
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <1437681365994-4710271.post@n4.nabble.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
Message-ID: <CAM_vjum_y-GGZFV5qXSW3rs1MLanRaRceOWwP8xogUUFUnatpA@mail.gmail.com>

On Thu, Jul 23, 2015 at 3:56 PM, maxbre <mbressan at arpa.veneto.it> wrote:
> Hi
>
> thank you for your reply: it's a neat solution but unfortunately not
> applicable to my specific case;

I'm going to assume you're replying to me, although there's no context
whatsoever in your response (this is the R-help email list, not
Nabble).

> in fact as I specified in my first post (I may have been not enough clear,
> sorry for that!) I can not rely on any search method grep-like because the
> value "xxx" in the rows of "df_start" can be anything (string or numeric and
> always different) so that I necessarely need to apply by row index position
> (i.e. in my reproducible example rows: 2, 3, 5);

Then how do you know which positions? Do you have another R object
that specifies row and column number? Or do you guess? I can easily
remove a random element from each row... If you aren't removing based
on value, then you didn't provide a reproducible example after all,
and you need to supply the index for removal.

Always different? Different within a single data frame? Different
between data frames? (Then you can simply change "xxx" to "whatever".)
Is telepathy required? Or perhaps precognition?

> thank you again for your kind help but still searching for a solution...
>
> best

-- 
Sarah Goslee
http://www.functionaldiversity.org


From mbressan at arpa.veneto.it  Thu Jul 23 22:19:56 2015
From: mbressan at arpa.veneto.it (maxbre)
Date: Thu, 23 Jul 2015 13:19:56 -0700 (PDT)
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
	<CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>
Message-ID: <1437682796009-4710276.post@n4.nabble.com>

sorry but honestly I do not get your point

I need to shift to left by one position (i.e. one column) the entire rows
2,4,5 of "df_start" so that to obtain as final result the structure
indicated in "df_end"

I know in advance the rows that I need to shift

hope it clears a bit, now




--
View this message in context: http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710276.html
Sent from the R help mailing list archive at Nabble.com.


From bbolker at gmail.com  Thu Jul 23 22:59:04 2015
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 23 Jul 2015 20:59:04 +0000
Subject: [R] Contr.sum and coefficient tests
References: <2E3BD18875277F48A593197016F0B10441800709E8@EXCHANGE2.ad.nottingham.ac.uk>
Message-ID: <loom.20150723T225338-197@post.gmane.org>

Fran?ois Collin <stxfc <at> nottingham.ac.uk> writes:

> 
> Dear all,
 
> I would like to run a linear model which includes two factors:

> - The first one has two levels, including a reference level. Thus I
> have to use the treatment contrast (contr.treatment, reference level
> effect = 0, then the intercept).

> - The second is a 6-level factor without reference contrast nor
> order. So, I would like to use sum contrat: sum of the effects = 0.

> The problem arises when it comes to the coefficient test. I
> understand it is not relevant to test the reference level for the
> first factor as the reference level is set to 0. However, using sum
> contrast for the second factor, I would have expected the test of
> each level to be included in the classical summary print of the lm
> function result but it is not. And here is my problem, how can I
> have every coefficients tested and printed in the summary output
> when my factor is studied from this sum.contrast standpoint?

  [some context snipped to make gmane happy -- sorry ]

  I think you should look at the effects package or the lsmeans
package (or possibly the multcomp package, if you want to be careful
about the number of (non-orthogonal) tests) -- the issue is that
summary.lm always reports on the *parameters* estimated.  If some
parameters are not independently estimable (e.g. the effect corresponding
to the last level can be reconstructed from the parameter values
for all of the preceding levels), then summary.lm() won't give
them to you.

  In fact, these packages will (probably) give you the results you
want even if the original model uses default treatment contrasts.

From john.maindonald at anu.edu.au  Thu Jul 23 22:59:40 2015
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 23 Jul 2015 20:59:40 +0000
Subject: [R] R: Re: Differences in output of lme() when introducing
	interactions
In-Reply-To: <mailman.6.1437559202.1794.r-help@r-project.org>
References: <mailman.6.1437559202.1794.r-help@r-project.org>
Message-ID: <08F55850-54EE-4B59-AB6A-6DEAD6CA496A@anu.edu.au>

Do you have legal advice that suing the University (if that is the right context)
would actually be a fruitful way forwards, that it would achieve anything
useful within reasonable time and without causing the student severe
financial risk?

What may work in that context is for students to collectively complain that
important aspects of their training and support are being neglected.
With the rapidity of recent technological change, the issue is widespread.
To an extent, able post-docs and PhDs have to lead the charge in getting
training and support updated and brought into the modern world.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>

On 22/07/2015, at 22:00, r-help-request at r-project.org<mailto:r-help-request at r-project.org> wrote:

Da: lists at dewey.myzen.co.uk<mailto:lists at dewey.myzen.co.uk>
Data: 21-lug-2015 11.58
A: "angelo.arcadi at virgilio.it<mailto:angelo.arcadi at virgilio.it>"<angelo.arcadi at virgilio.it<mailto:angelo.arcadi at virgilio.it>>, <bgunter.4567 at gmail.com<mailto:bgunter.4567 at gmail.com>>
Cc: <r-help at r-project.org<mailto:r-help at r-project.org>>
Ogg: Re: R: Re: [R] R: Re: Differences in output of lme() when introducing interactions

Dear Angelo

I suggest you do an online search for marginality which may help to
explain the relationship between main effects and interactions. As I
said in my original email this is a complicated subject which we are not
going to retype for you.

If you are doing this as a student I suggest you sue your university for
failing to train you appropriately and if it is part of your employment
I suggest you find a better employer.

On 21/07/2015 10:04, angelo.arcadi at virgilio.it<mailto:angelo.arcadi at virgilio.it> wrote:
Dear Bert,
thank you for your feedback. Can you please provide some references
online so I can improve "my ignorance"?
Anyways, please notice that it is not true that I do not know statistics
and regressions at all, and I am strongly
convinced that my question can be of interest for some one else in the
future.

This is what forums serve for, isn't it? This is why people help each
other, isn't it?

Moreover, don't you think that I would not have asked to this R forum if
I had the possibility to ask or pay a statician?
Don't you think I have done already my best to study and learn before
posting this message? Trust me, I have read different
online tutorials on lme and lmer, and I am confident that I have got the
basic concepts. Still I have not found the answer
to solve my problem, so if you know the answer can you please give me
some suggestions that can help me?

I do not have a book where to learn and unfortunately I have to analyze
the results soon. Any help? Any online reference to-the-point
that can help me in solving this problem?

Thank you in advance

Best regards

Angelo


	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Jul 23 23:44:22 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 23 Jul 2015 14:44:22 -0700
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <1437682796009-4710276.post@n4.nabble.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
	<CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>
	<1437682796009-4710276.post@n4.nabble.com>
Message-ID: <CAF8bMcZSti9VYJh4r+YEMTrOJZ=BzEx2iem-kuASOLk51Q8D9Q@mail.gmail.com>

You could do something like the following
  > rowsToShiftLeft <- c(2,4,5) # 4, not the 3 that was in the original post
  > mat <- as.matrix(df_start)
  > mat[rowsToShiftLeft, 1:3] <- mat[rowsToShiftLeft, 2:4]
  > result <- data.frame(mat[, 1:3], stringsAsFactors=FALSE)
  > str(result)
  'data.frame':   5 obs. of  3 variables:
   $ v0: chr  "a" "b" "c" "d" ...
   $ v1: chr  "1" "2" "3" "4" ...
   $ v2: chr  "6" "7" "8" "9" ...
You will then have to convert the columns which ought to be numeric
to numeric.  (All the columns in df_start were factors because of the
extra xxx that offset some of them.)


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Jul 23, 2015 at 1:19 PM, maxbre <mbressan at arpa.veneto.it> wrote:

> sorry but honestly I do not get your point
>
> I need to shift to left by one position (i.e. one column) the entire rows
> 2,4,5 of "df_start" so that to obtain as final result the structure
> indicated in "df_end"
>
> I know in advance the rows that I need to shift
>
> hope it clears a bit, now
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710276.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Jul 23 23:51:33 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 23 Jul 2015 14:51:33 -0700
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <CAF8bMcZSti9VYJh4r+YEMTrOJZ=BzEx2iem-kuASOLk51Q8D9Q@mail.gmail.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
	<CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>
	<1437682796009-4710276.post@n4.nabble.com>
	<CAF8bMcZSti9VYJh4r+YEMTrOJZ=BzEx2iem-kuASOLk51Q8D9Q@mail.gmail.com>
Message-ID: <CAGxFJbSUi_ewTqpYC9=ELQGgkCQA8wv4KRe_kRY4WFwq+XLfkg@mail.gmail.com>

Oops, Bill's reply and mine crossed in the email. His is essentially
the same as mine except probably more efficient.


-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 23, 2015 at 2:44 PM, William Dunlap <wdunlap at tibco.com> wrote:
> You could do something like the following
>   > rowsToShiftLeft <- c(2,4,5) # 4, not the 3 that was in the original post
>   > mat <- as.matrix(df_start)
>   > mat[rowsToShiftLeft, 1:3] <- mat[rowsToShiftLeft, 2:4]
>   > result <- data.frame(mat[, 1:3], stringsAsFactors=FALSE)
>   > str(result)
>   'data.frame':   5 obs. of  3 variables:
>    $ v0: chr  "a" "b" "c" "d" ...
>    $ v1: chr  "1" "2" "3" "4" ...
>    $ v2: chr  "6" "7" "8" "9" ...
> You will then have to convert the columns which ought to be numeric
> to numeric.  (All the columns in df_start were factors because of the
> extra xxx that offset some of them.)
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Jul 23, 2015 at 1:19 PM, maxbre <mbressan at arpa.veneto.it> wrote:
>
>> sorry but honestly I do not get your point
>>
>> I need to shift to left by one position (i.e. one column) the entire rows
>> 2,4,5 of "df_start" so that to obtain as final result the structure
>> indicated in "df_end"
>>
>> I know in advance the rows that I need to shift
>>
>> hope it clears a bit, now
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710276.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marissa.fahlberg at gmail.com  Thu Jul 23 23:08:10 2015
From: marissa.fahlberg at gmail.com (Marissa Fahlberg)
Date: Thu, 23 Jul 2015 16:08:10 -0500
Subject: [R] Table Looks Funny
Message-ID: <CANgFV8+B0D3rqLNOG8eC1J56hrXser9V-oNUtiqH+xVKr91ibg@mail.gmail.com>

Hey, I can't seem to import my csv file in such a way that the table looks
"normal". The dimensions are correct, 4x7, and I set my header =TRUE, but
it still looks weird. Attached is the picture. Any ideas??

Thanks!!

-Marissa

From derawang at udel.edu  Fri Jul 24 00:33:19 2015
From: derawang at udel.edu (Dera)
Date: Thu, 23 Jul 2015 15:33:19 -0700 (PDT)
Subject: [R] =?utf-8?b?4oCcRXJyb3IgaW4gaWYgKGFicyh4IC0gb2xkeCkgPCBmdG9s?=
 =?utf-8?b?KeKAnSB3aGVuIHVzaW5nIOKAnGxvZ25vcm1hbOKAnSBkaXN0cmlidXRpb24g?=
 =?utf-8?q?in_mixed_logit?=
Message-ID: <1437690799874-4710284.post@n4.nabble.com>

Hi, I have a question about how to use the mlogit package in R to do analysis
of discrete choice survey data. Our survey is about asking people to choose
from different insurance policies(with two attributes of deductible and
premium).

The code I used to fit mixed logit is:


[1] ml <- mlogit.data (mydata, choice="choice", shape = "wide", id =
"individual", 
               opposite =c ('deductible', 'premium'),varying = 5:10)

[2] ml.w5 <- mlogit (choice~deductible+premium|0, ml, panel = TRUE, 
             rpar = c(deductible='ln', premium='ln'), 
             R = 100, halton = NA, print.level=0)

I try to use lognormal because we hope the coefficients for both deductible
and premium are negative. And I use "opposite" in [1] to reverse the sign
because lognormal is always positive.

But I always get the error warning:

"Error in if (abs(x - oldx) < ftol) { : missing value where TRUE/FALSE
needed
In addition: Warning message: In log(start[ln]) : NaNs produced"
I double check the data and am sure there isn't any missing data. And if I
change the lognormal "ln" to "n" or "cn", it will work without any warning.

Does anyone know how to deal with this? Thank you for your help.



--
View this message in context: http://r.789695.n4.nabble.com/Error-in-if-abs-x-oldx-ftol-when-using-lognormal-distribution-in-mixed-logit-tp4710284.html
Sent from the R help mailing list archive at Nabble.com.


From marissa.fahlberg at gmail.com  Fri Jul 24 02:18:43 2015
From: marissa.fahlberg at gmail.com (Marissa Fahlberg)
Date: Thu, 23 Jul 2015 19:18:43 -0500
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <CAGxFJbSUi_ewTqpYC9=ELQGgkCQA8wv4KRe_kRY4WFwq+XLfkg@mail.gmail.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
	<CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>
	<1437682796009-4710276.post@n4.nabble.com>
	<CAF8bMcZSti9VYJh4r+YEMTrOJZ=BzEx2iem-kuASOLk51Q8D9Q@mail.gmail.com>
	<CAGxFJbSUi_ewTqpYC9=ELQGgkCQA8wv4KRe_kRY4WFwq+XLfkg@mail.gmail.com>
Message-ID: <CANgFV8LFwqJK5P52yiHzcfQPqMDNjxs90466i0i6nR=YsNxu8g@mail.gmail.com>

Turns out my column and row names were too long to fit in the console and
when I widened the window the table didn't automatically adjust and widen
as well. I had to widen the console first and then retype the same command
so it fit. Silly mistake. I usually keep the console pretty narrow on my
laptop and never considered that would change the format.

Thanks for all the help - I'm sure I'll be back again very soon with more
questions!!
On Jul 23, 2015 4:53 PM, "Bert Gunter" <bgunter.4567 at gmail.com> wrote:

> Oops, Bill's reply and mine crossed in the email. His is essentially
> the same as mine except probably more efficient.
>
>
> -- Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Jul 23, 2015 at 2:44 PM, William Dunlap <wdunlap at tibco.com> wrote:
> > You could do something like the following
> >   > rowsToShiftLeft <- c(2,4,5) # 4, not the 3 that was in the original
> post
> >   > mat <- as.matrix(df_start)
> >   > mat[rowsToShiftLeft, 1:3] <- mat[rowsToShiftLeft, 2:4]
> >   > result <- data.frame(mat[, 1:3], stringsAsFactors=FALSE)
> >   > str(result)
> >   'data.frame':   5 obs. of  3 variables:
> >    $ v0: chr  "a" "b" "c" "d" ...
> >    $ v1: chr  "1" "2" "3" "4" ...
> >    $ v2: chr  "6" "7" "8" "9" ...
> > You will then have to convert the columns which ought to be numeric
> > to numeric.  (All the columns in df_start were factors because of the
> > extra xxx that offset some of them.)
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> > On Thu, Jul 23, 2015 at 1:19 PM, maxbre <mbressan at arpa.veneto.it> wrote:
> >
> >> sorry but honestly I do not get your point
> >>
> >> I need to shift to left by one position (i.e. one column) the entire
> rows
> >> 2,4,5 of "df_start" so that to obtain as final result the structure
> >> indicated in "df_end"
> >>
> >> I know in advance the rows that I need to shift
> >>
> >> hope it clears a bit, now
> >>
> >>
> >>
> >>
> >> --
> >> View this message in context:
> >>
> http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710276.html
> >> Sent from the R help mailing list archive at Nabble.com.
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Fri Jul 24 04:26:02 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 24 Jul 2015 14:26:02 +1200
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <2f3a88$1324qp@ironport10.mayo.edu>
References: <mailman.7.1437559202.1794.r-help@r-project.org>
	<2f3a88$12la7r@ironport10.mayo.edu> <55B020F9.7020706@auckland.ac.nz>
	<2f3a88$1324qp@ironport10.mayo.edu>
Message-ID: <55B1A23A.8000400@auckland.ac.nz>

On 24/07/15 07:07, Therneau, Terry M., Ph.D. wrote:
> The following are in parody (but like all good parody correct wrt the
> salient features). The musings of
> Guernsey McPhearson
>     http://www.senns.demon.co.uk/wprose.html#Mixed
>     http://www.senns.demon.co.uk/wprose.html#FDA
>
>
> In formal publication:
>   Senn, Statistical Issues in Drug Development, second edition, Chapter
> 14: Multicentre Trials
>   Senn, The many modes of meta, Drug information journal, 34:535-549, 2000.
>
> The second points out that in a meta analysis no one would ever consider
> giving both large and small trials equal weights, and relates that to
> several other bits of standard practice.  The 'equal weights' notion
> embedded in a fixed effects model + SAS type 3 is an isolated backwater.
>
> Terry T.
>
> PS. The "Devils' Drug Development Dictionary" at the same source has
> some gems. Three rather random choices:
>
> Bayesian - One who, vaguely expecting a horse and catching a glimpse of
> a donkey, strongly concludes he has seen a mule.
>
> Medical Statistician - One who won't accept that Columbus discovered
> America because he said he was looking for India in the trial Plan.
>
> Trend Towards Significance - An ever present help in times of trouble.
>
>
>
> On 07/22/2015 06:02 PM, Rolf Turner wrote:
>> On 23/07/15 01:15, Therneau, Terry M., Ph.D. wrote:
>>
>> <SNIP>
>>
>>> 3. Should you ever use it [i.e. Type III SS]?  No.  There is a very
>>> strong inverse
>>> correlation between "understand what it really is" and "recommend its
>>> use".   Stephen Senn has written very intellgently on the issues.
>>
>> Terry --- can you please supply an explicit citation?  Ta.

Thanks Terry!

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From annijanh at gmail.com  Fri Jul 24 05:23:39 2015
From: annijanh at gmail.com (Janh Anni)
Date: Thu, 23 Jul 2015 23:23:39 -0400
Subject: [R] Infinite Series
Message-ID: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>

Dear All,

Does anyone know of any R functions that compute partial sums of series?

Thanks in advance!

Janh

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Fri Jul 24 05:51:48 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 23 Jul 2015 20:51:48 -0700
Subject: [R] Infinite Series
In-Reply-To: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
Message-ID: <682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>

?cumsum
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com> wrote:
>Dear All,
>
>Does anyone know of any R functions that compute partial sums of
>series?
>
>Thanks in advance!
>
>Janh
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From mbressan at arpa.veneto.it  Fri Jul 24 08:55:02 2015
From: mbressan at arpa.veneto.it (maxbre)
Date: Thu, 23 Jul 2015 23:55:02 -0700 (PDT)
Subject: [R] shift by one column given rows in a dataframe
In-Reply-To: <CAGxFJbSUi_ewTqpYC9=ELQGgkCQA8wv4KRe_kRY4WFwq+XLfkg@mail.gmail.com>
References: <55B112E8.2090103@arpa.veneto.it>
	<CAM_vju=5GyZuRp=gNqcSwdnS-qS5pXT+LYrPdEX5RnEz3srwfQ@mail.gmail.com>
	<1437681365994-4710271.post@n4.nabble.com>
	<CAGxFJbTCtpcV5SuY7Z4VNjuYY_uZE5a9_3uKdapu-gDsxg2+bA@mail.gmail.com>
	<1437682796009-4710276.post@n4.nabble.com>
	<CAF8bMcZSti9VYJh4r+YEMTrOJZ=BzEx2iem-kuASOLk51Q8D9Q@mail.gmail.com>
	<CAGxFJbSUi_ewTqpYC9=ELQGgkCQA8wv4KRe_kRY4WFwq+XLfkg@mail.gmail.com>
Message-ID: <1437720902456-4710294.post@n4.nabble.com>

hi

thank you all for the great replies, very useful indeed even if some of them
a bit too aggressive (which is never, ever a good approach in my very humble
opinion... but that's a matter of taste and style I do not want to discuss
here); sorry again for bothering someone with such a trivial and
ill-conceived question

finally, I'm posting here my solution as a reference to the problem so that
to close this long and winding thread; 

hoping this code might be somehow useful for someonelse, sometime,
somewhere...

## start code

v0<-c("a","xxx","c",rep("xxx",2))
v1<-c(1,"b",3,"d","e")
v2<-c(6,2,8,4,5)
v3<-c("xxx",7,"xxx",9,10)
 
df_start<-data.frame(v0,v1,v2,v3, stringsAsFactors = FALSE)

df_start

# set vector of rows to be shifted left
shIftLeft<-c(2,4,5)

# shift selected rows
df_start[shIftLeft,1:3]<-df_start[shIftLeft,2:4]

# final result
df_end<-df_start[,1:3]

df_end

## end code

thanks




--
View this message in context: http://r.789695.n4.nabble.com/shift-by-one-column-given-rows-in-a-dataframe-tp4710256p4710294.html
Sent from the R help mailing list archive at Nabble.com.


From j.para.fernandez at hotmail.com  Fri Jul 24 10:43:01 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Fri, 24 Jul 2015 01:43:01 -0700 (PDT)
Subject: [R] R GUI plot by color
Message-ID: <1437727381033-4710297.post@n4.nabble.com>

Hi, 

I want to do a plot from a variable (which i select from a listbox) with the
color factor of the variable that i have selected from another listbox.

To be not very heavy pasting all the code, i will only paste real important
parts:

"data1" and "data2" are the extacted parts of the dataframes "data" from a
tcltk listbox:

data1<<- dataframe[as.numeric(tkcurselection(tl))+1]
data2<<- dataframe[as.numeric(tkcurselection(tl))+1]

As i want to plot the data1 with the color of data2, i use this code:

plot(data1,col=colnames(data2))

This works perfect for plotting the data1 variable, but it do not change the
col of the dots by data2. 

I also have probed with 

plot(data1,col=factor(colnames(data2)))

But nothing happens with the color. 

Thanks!!






--
View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297.html
Sent from the R help mailing list archive at Nabble.com.


From drjimlemon at gmail.com  Fri Jul 24 11:30:08 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 24 Jul 2015 19:30:08 +1000
Subject: [R] Table Looks Funny
In-Reply-To: <CANgFV8+B0D3rqLNOG8eC1J56hrXser9V-oNUtiqH+xVKr91ibg@mail.gmail.com>
References: <CANgFV8+B0D3rqLNOG8eC1J56hrXser9V-oNUtiqH+xVKr91ibg@mail.gmail.com>
Message-ID: <CA+8X3fWgKkNe8tzAztkrgoQWajxmOz+5nV1uAowqri21DAF1Sw@mail.gmail.com>

Hi Marissa,
Unfortunately we didn't "get the picture". If you want to send an
image, PDF is probably your best bet. If that is not possible, perhaps
a description of the weirdness that has confronted you will allow
someone to suggest a solution.

Jim

On Fri, Jul 24, 2015 at 7:08 AM, Marissa Fahlberg
<marissa.fahlberg at gmail.com> wrote:
> Hey, I can't seem to import my csv file in such a way that the table looks
> "normal". The dimensions are correct, 4x7, and I set my header =TRUE, but
> it still looks weird. Attached is the picture. Any ideas??
>
> Thanks!!
>
> -Marissa
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jul 24 11:43:55 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 24 Jul 2015 19:43:55 +1000
Subject: [R] R GUI plot by color
In-Reply-To: <1437727381033-4710297.post@n4.nabble.com>
References: <1437727381033-4710297.post@n4.nabble.com>
Message-ID: <CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>

Hi jpara3,
Hmmm. It's becoming clearer, yes I think the answer is emerging from
the swirling clouds of uncertainty.

1) The column names of data2 are not numbers

2) The number of columns in data2 is not equal to the number of values
in data1 that you are plotting

3) You probably want to plot colors determined by the values in data2,
not the names of its columns

Please send payment for this psychic reading to

Jim


On Fri, Jul 24, 2015 at 6:43 PM, jpara3 <j.para.fernandez at hotmail.com> wrote:
> Hi,
>
> I want to do a plot from a variable (which i select from a listbox) with the
> color factor of the variable that i have selected from another listbox.
>
> To be not very heavy pasting all the code, i will only paste real important
> parts:
>
> "data1" and "data2" are the extacted parts of the dataframes "data" from a
> tcltk listbox:
>
> data1<<- dataframe[as.numeric(tkcurselection(tl))+1]
> data2<<- dataframe[as.numeric(tkcurselection(tl))+1]
>
> As i want to plot the data1 with the color of data2, i use this code:
>
> plot(data1,col=colnames(data2))
>
> This works perfect for plotting the data1 variable, but it do not change the
> col of the dots by data2.
>
> I also have probed with
>
> plot(data1,col=factor(colnames(data2)))
>
> But nothing happens with the color.
>
> Thanks!!
>
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j.para.fernandez at hotmail.com  Fri Jul 24 11:53:40 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Fri, 24 Jul 2015 02:53:40 -0700 (PDT)
Subject: [R] R GUI plot by color
In-Reply-To: <CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>
References: <1437727381033-4710297.post@n4.nabble.com>
	<CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>
Message-ID: <1437731620874-4710300.post@n4.nabble.com>

I have done a trial with a dataframe like this:
one<-c(3,2,2)
two<-c(a,b,b)
data<-dataframe(uno,dos)

plot(data$one,col=data$two)

and it plots perfect.

If I try it with the code that i have post in the first message, selecting
data1 and data2 as i nthis example, the plot is plotted, but all dots with
the same color.

Thanks for the answer but noone of the 3 topics is the root problem.





--
View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297p4710300.html
Sent from the R help mailing list archive at Nabble.com.


From holtermann at hwwi.org  Fri Jul 24 12:02:40 2015
From: holtermann at hwwi.org (Linus Holtermann)
Date: Fri, 24 Jul 2015 10:02:40 +0000
Subject: [R] How to calculate the average direct effect,
 average total effect and average indirect effect for spatial
 regression models with spatial lag of dependent variable
In-Reply-To: <1437665721814-4710253.post@n4.nabble.com>
References: <1437665721814-4710253.post@n4.nabble.com>
Message-ID: <0275b88c032e4762b88b0da6be96921c@winhexbeeu15.win.mail>

Hello,

the command "impacts" in the spdep-packeage should help you.


Mit freundlichen Gr??en


Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
?
Amtsgericht Hamburg HRB 94303
Gesch?ftsf?hrer: PD Dr. Christian Growitsch | Prof. Dr. Henning V?pel
Prokura: Dipl. Kauffrau Alexis Malchin
Umsatzsteuer-ID: DE 241849425

-----Urspr?ngliche Nachricht-----
Von: R-help [mailto:r-help-bounces at r-project.org] Im Auftrag von wenyueyang
Gesendet: Donnerstag, 23. Juli 2015 17:35
An: r-help at r-project.org
Betreff: [R] How to calculate the average direct effect, average total effect and average indirect effect for spatial regression models with spatial lag of dependent variable

Hi,

I am using four spatial regression models (SAR, SEM, SAC and SDM) to evaluate the spillover effect of some factors. LeSage and Pace (2009) pointed out that when the spatial lag of the dependent variable is included in the model, parameter estimates lose their conventional interpretation as marginal effects, because the spatial lag gives rise to a series of feedback loops and spillover effects across regions. Therefore, I need to calculate the three different marginal effects: average direct effect, average total effect and average indirect (spillover) effect, and this is what I don't know how to perform in R.
Can you teach me about how to calculate the average direct effect, average total effect and average indirect (spillover) effect for the spatial regression models in R and tell me the related R code? 
I would like to express my great appreciation to you!

Thank you very much and best regards.

Yours sincerely,
Wenyue Yang



--
View this message in context: http://r.789695.n4.nabble.com/How-to-calculate-the-average-direct-effect-average-total-effect-and-average-indirect-effect-for-spate-tp4710253.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From entropy053 at gmail.com  Fri Jul 24 13:20:47 2015
From: entropy053 at gmail.com (Yasin Gocgun)
Date: Fri, 24 Jul 2015 14:20:47 +0300
Subject: [R] how to install and use Rcplex package
Message-ID: <CAJJuoERM6p_gr=KE0H9sLjhE0viEa+2ziuTMkA1GKBXmubx3bA@mail.gmail.com>

Hi,

I need to know how to completely install Rcplex to be able to use
cplex in R. According to the instructions given in the following
webpage, I edited the Makevars.win file:

https://cran.r-project.org/web/packages/Rcplex/INSTALL

The revised Makevars.win file includes only the following lines:

PKG_CPPFLAGS=-I/C:/ilog/CPLEX_Studio122/include
PKG_LIBS=-L/C:/ilog/CPLEX_Studio122/cplex/lib/x64_windows_vs2008/stat_mda-lcplex122
-lm

First, I am not sure whether the above lines are completely correct.
Second, I don'y know what is the next step in installing and using
Rcplex. So can someone help me about this issue?

Thank you in advance,

-- 
Yasin Gocgun


From drjimlemon at gmail.com  Fri Jul 24 13:23:29 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 24 Jul 2015 21:23:29 +1000
Subject: [R] R GUI plot by color
In-Reply-To: <1437731620874-4710300.post@n4.nabble.com>
References: <1437727381033-4710297.post@n4.nabble.com>
	<CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>
	<1437731620874-4710300.post@n4.nabble.com>
Message-ID: <CA+8X3fXaC2hhVp7zD2Wc+NEHP0cfKXdE=fSYa_jFibxQKLbXFw@mail.gmail.com>

Hi jpara3,
Your example, when I got it to go:

one<-c(3,2,2)
two<-c("a","b","b")
data<-dataframe(one,two)
plot(data$one,col=data$two)

does indeed work, and I'll explain how. You are plotting the values of
data$one against the _values_ of data$two (see point 3 of my
response). In this case, the values of data$two are of class "factor",
which means that they have numeric values attached to the levels (a,
b) of the factor. When you pass these values as the "col" argument,
they are silently converted to their numeric values (1,2,2). In the
default palette, these numbers represent the colors - black, red, red.
Those are the colors in which the points are plotted. So far, so good.
Let's look at the other two points that I guessed.

1) The column names of data2 are not numbers

colnames(data)
[1] "one" "two"

As you can see, the column names are character variables, and they
don't translate to numbers:

as.numeric(colnames(data))
[1] NA NA

2) The number of columns in data2 is not equal to the number of values
in data1 that you are plotting

It's pretty obvious that there are two values in the column names and
three in the vector of values that you are plotting in your
example.So, I think I got three out of three without knowing what the
data were.

Jim


On Fri, Jul 24, 2015 at 7:53 PM, jpara3 <j.para.fernandez at hotmail.com> wrote:
> I have done a trial with a dataframe like this:
> one<-c(3,2,2)
> two<-c(a,b,b)
> data<-dataframe(uno,dos)
>
> plot(data$one,col=data$two)
>
> and it plots perfect.
>
> If I try it with the code that i have post in the first message, selecting
> data1 and data2 as i nthis example, the plot is plotted, but all dots with
> the same color.
>
> Thanks for the answer but noone of the 3 topics is the root problem.
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297p4710300.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ruipbarradas at sapo.pt  Fri Jul 24 15:04:38 2015
From: ruipbarradas at sapo.pt (Rui Barradas)
Date: Fri, 24 Jul 2015 14:04:38 +0100
Subject: [R] how to install and use Rcplex package
In-Reply-To: <CAJJuoERM6p_gr=KE0H9sLjhE0viEa+2ziuTMkA1GKBXmubx3bA@mail.gmail.com>
References: <CAJJuoERM6p_gr=KE0H9sLjhE0viEa+2ziuTMkA1GKBXmubx3bA@mail.gmail.com>
Message-ID: <55B237E6.9050100@sapo.pt>

Hello,

Try

install.packages("Rcplex")

That's it.

Hope this helps,

Rui Barradas

Em 24-07-2015 12:20, Yasin Gocgun escreveu:
> Hi,
>
> I need to know how to completely install Rcplex to be able to use
> cplex in R. According to the instructions given in the following
> webpage, I edited the Makevars.win file:
>
> https://cran.r-project.org/web/packages/Rcplex/INSTALL
>
> The revised Makevars.win file includes only the following lines:
>
> PKG_CPPFLAGS=-I/C:/ilog/CPLEX_Studio122/include
> PKG_LIBS=-L/C:/ilog/CPLEX_Studio122/cplex/lib/x64_windows_vs2008/stat_mda-lcplex122
> -lm
>
> First, I am not sure whether the above lines are completely correct.
> Second, I don'y know what is the next step in installing and using
> Rcplex. So can someone help me about this issue?
>
> Thank you in advance,
>


From jrkrideau at inbox.com  Fri Jul 24 15:37:20 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 24 Jul 2015 05:37:20 -0800
Subject: [R] Differences in output of lme() when introducing interactions
In-Reply-To: <2f3a88$1324qq@ironport10.mayo.edu>
References: <2f3a88$12la7r@ironport10.mayo.edu>
	<55b020f9.7020706@auckland.ac.nz>
	<mailman.7.1437559202.1794.r-help@r-project.org>
Message-ID: <3E9F19E1883.000006B9jrkrideau@inbox.com>

I clearly am going to have to improve my stats knowledge by reading McPhearson. To heck with Senn- too complicated. :)

Thanks Terry.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: therneau at mayo.edu
> Sent: Thu, 23 Jul 2015 14:07:00 -0500
> To: r.turner at auckland.ac.nz, therneau at mayo.edu
> Subject: Re: [R] Differences in output of lme() when introducing
> interactions
> 
> The following are in parody (but like all good parody correct wrt the
> salient features).
> The musings of
> Guernsey McPhearson
>     http://www.senns.demon.co.uk/wprose.html#Mixed
>     http://www.senns.demon.co.uk/wprose.html#FDA
> 
> 
> In formal publication:
>   Senn, Statistical Issues in Drug Development, second edition, Chapter
> 14: Multicentre Trials
>   Senn, The many modes of meta, Drug information journal, 34:535-549,
> 2000.
> 
> The second points out that in a meta analysis no one would ever consider
> giving both large
> and small trials equal weights, and relates that to several other bits of
> standard
> practice.  The 'equal weights' notion embedded in a fixed effects model +
> SAS type 3 is an
> isolated backwater.
> 
> Terry T.
> 
> PS. The "Devils' Drug Development Dictionary" at the same source has some
> gems. Three
> rather random choices:
> 
> Bayesian - One who, vaguely expecting a horse and catching a glimpse of a
> donkey, strongly
> concludes he has seen a mule.
> 
> Medical Statistician - One who won't accept that Columbus discovered
> America because he
> said he was looking for India in the trial Plan.
> 
> Trend Towards Significance - An ever present help in times of trouble.
> 
> 
> 
> On 07/22/2015 06:02 PM, Rolf Turner wrote:
>> On 23/07/15 01:15, Therneau, Terry M., Ph.D. wrote:
>> 
>> <SNIP>
>> 
>>> 3. Should you ever use it [i.e. Type III SS]?  No.  There is a very
>>> strong inverse
>>> correlation between "understand what it really is" and "recommend its
>>> use".   Stephen Senn has written very intellgently on the issues.
>> 
>> Terry --- can you please supply an explicit citation?  Ta.
>> 
>> cheers,
>> 
>> Rolf
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jrkrideau at inbox.com  Fri Jul 24 15:43:28 2015
From: jrkrideau at inbox.com (John Kane)
Date: Fri, 24 Jul 2015 05:43:28 -0800
Subject: [R] Table Looks Funny
In-Reply-To: <CANgFV8+B0D3rqLNOG8eC1J56hrXser9V-oNUtiqH+xVKr91ibg@mail.gmail.com>
Message-ID: <3EACCF3562B.000006C5jrkrideau@inbox.com>

Hi Marrisa,

As a follow-up to Jim's point,  it might be a good idea to supply the data. Since you don't trust the imported data our preferred method of using dput() won't work but if you rename the csv file to whatever.txt and attached it should make it through.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: marissa.fahlberg at gmail.com
> Sent: Thu, 23 Jul 2015 16:08:10 -0500
> To: r-help at r-project.org
> Subject: [R] Table Looks Funny
> 
> Hey, I can't seem to import my csv file in such a way that the table
> looks
> "normal". The dimensions are correct, 4x7, and I set my header =TRUE, but
> it still looks weird. Attached is the picture. Any ideas??
> 
> Thanks!!
> 
> -Marissa

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From entropy053 at gmail.com  Fri Jul 24 15:54:02 2015
From: entropy053 at gmail.com (Yasin Gocgun)
Date: Fri, 24 Jul 2015 16:54:02 +0300
Subject: [R] how to install and use Rcplex package
In-Reply-To: <55B237E6.9050100@sapo.pt>
References: <CAJJuoERM6p_gr=KE0H9sLjhE0viEa+2ziuTMkA1GKBXmubx3bA@mail.gmail.com>
	<55B237E6.9050100@sapo.pt>
Message-ID: <CAJJuoEQwwk-SAqqAt31FvSASOtDP5fu55zwetYNxaLKwVBhD0w@mail.gmail.com>

Hi,

Thanks for your email. When I use that line, I received the following
messages from R:

package ?Rcplex? is available as a source package but not as a binary

package ?Rcplex? is not available (for R version 3.1.1)

So, do you see what the problem is?

Thank you,

Yasin

On Fri, Jul 24, 2015 at 4:04 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
> Hello,
>
> Try
>
> install.packages("Rcplex")
>
> That's it.
>
> Hope this helps,
>
> Rui Barradas
>
> Em 24-07-2015 12:20, Yasin Gocgun escreveu:
>>
>> Hi,
>>
>> I need to know how to completely install Rcplex to be able to use
>> cplex in R. According to the instructions given in the following
>> webpage, I edited the Makevars.win file:
>>
>> https://cran.r-project.org/web/packages/Rcplex/INSTALL
>>
>> The revised Makevars.win file includes only the following lines:
>>
>> PKG_CPPFLAGS=-I/C:/ilog/CPLEX_Studio122/include
>>
>> PKG_LIBS=-L/C:/ilog/CPLEX_Studio122/cplex/lib/x64_windows_vs2008/stat_mda-lcplex122
>> -lm
>>
>> First, I am not sure whether the above lines are completely correct.
>> Second, I don'y know what is the next step in installing and using
>> Rcplex. So can someone help me about this issue?
>>
>> Thank you in advance,
>>
>



-- 
Yasin Gocgun


From profjcnash at gmail.com  Fri Jul 24 16:38:27 2015
From: profjcnash at gmail.com (ProfJCNash)
Date: Fri, 24 Jul 2015 10:38:27 -0400
Subject: [R] how to install and use Rcplex package
In-Reply-To: <CAJJuoEQwwk-SAqqAt31FvSASOtDP5fu55zwetYNxaLKwVBhD0w@mail.gmail.com>
References: <CAJJuoERM6p_gr=KE0H9sLjhE0viEa+2ziuTMkA1GKBXmubx3bA@mail.gmail.com>	<55B237E6.9050100@sapo.pt>
	<CAJJuoEQwwk-SAqqAt31FvSASOtDP5fu55zwetYNxaLKwVBhD0w@mail.gmail.com>
Message-ID: <55B24DE3.8000700@gmail.com>

It's important to look at the CRAN documentation, where it is quite
clear there are NO Windows binaries for this package.

My suggestion -- set up the (freeware) VirtualBox or a similar Virtual
Machine environment, install a Linux OS virtually, and install there. If
there is no Windows binary on CRAN, there is likely a very good reason.

You may actually find that something like Lubuntu or Linux Mint are more
convenient for working with R.

JN

On 15-07-24 09:54 AM, Yasin Gocgun wrote:
> Hi,
> 
> Thanks for your email. When I use that line, I received the following
> messages from R:
> 
> package ?Rcplex? is available as a source package but not as a binary
> 
> package ?Rcplex? is not available (for R version 3.1.1)
> 
> So, do you see what the problem is?
> 
> Thank you,
> 
> Yasin
> 
> On Fri, Jul 24, 2015 at 4:04 PM, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>> Hello,
>>
>> Try
>>
>> install.packages("Rcplex")
>>
>> That's it.
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> Em 24-07-2015 12:20, Yasin Gocgun escreveu:
>>>
>>> Hi,
>>>
>>> I need to know how to completely install Rcplex to be able to use
>>> cplex in R. According to the instructions given in the following
>>> webpage, I edited the Makevars.win file:
>>>
>>> https://cran.r-project.org/web/packages/Rcplex/INSTALL
>>>
>>> The revised Makevars.win file includes only the following lines:
>>>
>>> PKG_CPPFLAGS=-I/C:/ilog/CPLEX_Studio122/include
>>>
>>> PKG_LIBS=-L/C:/ilog/CPLEX_Studio122/cplex/lib/x64_windows_vs2008/stat_mda-lcplex122
>>> -lm
>>>
>>> First, I am not sure whether the above lines are completely correct.
>>> Second, I don'y know what is the next step in installing and using
>>> Rcplex. So can someone help me about this issue?
>>>
>>> Thank you in advance,
>>>
>>
> 
> 
>


From angelo.arcadi at virgilio.it  Fri Jul 24 17:45:59 2015
From: angelo.arcadi at virgilio.it (angelo.arcadi at virgilio.it)
Date: Fri, 24 Jul 2015 17:45:59 +0200 (CEST)
Subject: [R] Which is the best way of reporting results of lme() in two
 different possible cases?
Message-ID: <14ec0be160f.angelo.arcadi@virgilio.it>

 
Dear List Memebers,
I need some advise about how to report the output of lme(). When searching for correlations between between a dependent variable 
and a factor or a combination of factors in a repeated measure design 
with lme() I noticed that I can encounter two types of results, and I am
 wondering which is the best way to report each of them in a journal 
publication. It is not clear to me when I should report the values of 
the beta coefficient together with the t-test value and p-value, or the 
beta coefficient with F value and p-value.


Let?s have as a reference the following two models:


MODEL TYPE 1: fixed effects only 

lme_Weigth <- lme(Sound_Feature ~ Weight, data = My_Data, random = ~1 | Subject)
summary(lme_Weigth)

lme_Height <- lme(Sound_Feature ~ Height, data = My_Data, random = ~1 | Subject)
summary(lme_Height)


MODEL TYPE 2: Fixed and interaction effects together

lme_Interaction <- lme(Sound_Feature ~ Weight*Height, data = My_Data, random = ~1 | Subject)

summary(lme_Interaction)  
anova.lme(lme_Interaction, type = "marginal").


RESULTS CASE 1: Applying model type 2 I do not get any significant p-value so there is no interaction effect. Therefore I check
the simplified model type 1, and I get for both Height and Weight significant p-values.


RESULTS CASE 2: Applying model type 2 I get a significant p-value so 
there is an interaction effect. Therefore I do not check
the simplified model type 1 for the two factors separately. Moreover, in
 the results of model type 2 I can also see that the fixed effects of 
both factors are significant.


I am not sure if in presence of an interaction it is correct to 
report the significant interactions of the separate factors, since I 
read somewhere that it does not make too much sense. Am I wrong?


My attempt in reporting the results for the two cases is the following. Can you please tell me it I am right?


?We performed a linear mixed effects analysis of the relationship 
between Sound_Feature and Height and Weight. As fixed effects, we 
entered Height and Weight (without interaction term) into a first model,
 and we included the interaction effect into a second model. As random 
effects, we had intercepts for subjects.?


RESULTS CASE 1: ?Results showed that Sound_Feature was linearly 
related to Height (beta = value, t(df)= value, p < 0.05) and Weight 
(beta = value, t(df)= value, p < 0.05), but no to their interaction 
effect.?


RESULTS CASE 2:  ?Results showed that Sound_Feature was linearly 
related to Height (beta = value, F(df)= value, p < 0.05) and Weight 
(beta = value, F(df)= value, p < 0.05), and to their interaction 
effect (beta = value, F(df)= value, p < 0.05).?


Basically I used for reporting the beta value in the 2 cases I use 
the output of summary(). In the case 1, I report the value of the 
t-test, still taken from summary. But for case 2 I do not report the 
t-test, I report the F value as result of anova.lme(lme_Interaction, 
type = "marginal").


Is this the correct way of proceeding in the results reporting?


I give an example of the outputs I get using the two models for the three cases:


RESULTS CASE 1:

> ############### Sound_Level_Peak vs Weight*Height ###############
> 
>
> 
> library(nlme)
> lme_Sound_Level_Peak <- lme(Sound_Level_Peak ~ Weight*Height, data = My_Data1, random = ~1 | Subject)
> 
> summary(lme_Sound_Level_Peak)
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  716.2123 732.4152 -352.1061

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.470027 4.246533

Fixed effects: Sound_Level_Peak ~ Weight * Height 
                  Value Std.Error DF    t-value p-value
(Intercept)   -7.185833  97.56924 95 -0.0736485  0.9414
Weight         0.993543   1.63151 15  0.6089715  0.5517
Height        -0.076300   0.55955 15 -0.1363592  0.8934
Weight:Height -0.005403   0.00898 15 -0.6017421  0.5563
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.95289464 -0.51041805 -0.06414148  0.48562230  2.95415889 

Number of Observations: 114
Number of Groups: 19 


> anova.lme(lme_Sound_Level_Peak,type = "marginal")
              numDF denDF   F-value p-value
(Intercept)       1    95 0.0054241  0.9414
Weight            1    15 0.3708463  0.5517
Height            1    15 0.0185938  0.8934
Weight:Height     1    15 0.3620936  0.5563
> 
> 





> ############### Sound_Level_Peak vs Weight ###############
> 
> library(nlme)
> summary(lme(Sound_Level_Peak ~ Weight, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  706.8101 717.6841 -349.4051

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.717712 4.246533

Fixed effects: Sound_Level_Peak ~ Weight 
                Value Std.Error DF    t-value p-value
(Intercept) -3.393843  6.291036 95 -0.5394728  0.5908
Weight      -0.196214  0.087647 17 -2.2386822  0.0388
 Correlation: 
       (Intr)
Weight -0.976

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.90606493 -0.51419643 -0.05659565  0.56770327  3.00098859 

Number of Observations: 114
Number of Groups: 19 
> 
> 
> 
> 
> 
> 
> ############### Sound_Level_Peak vs Height ###############
> 
> library(nlme)
> summary(lme(Sound_Level_Peak ~ Height, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC   logLik
  702.9241 713.7981 -347.462

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.174077 4.246533

Fixed effects: Sound_Level_Peak ~ Height 
               Value Std.Error DF   t-value p-value
(Intercept) 46.36896 20.764187 95  2.233122  0.0279
Height      -0.36643  0.119588 17 -3.064113  0.0070
 Correlation: 
       (Intr)
Height -0.998

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.93697776 -0.50963502 -0.06774953  0.50428597  2.97007576 

Number of Observations: 114
Number of Groups: 19 
> 
> 


So, I will report the results in this way: ?Results showed that 
Sound_Level_Peak was linearly related to Height (beta = -0.36643, t(17)=
 -3.064113, p = 0.007) and Weight (beta = -0.196214, t(17)= -2.2386822, p
 < 0.0388), but no to their interaction effect.?


RESULTS CASE 2:

> ############### Centroid vs Weight*Height ###############
> 
> 
> 
> library(nlme)
> lme_Centroid <- lme(Centroid ~ Weight*Height, data = My_Data2, random = ~1 | Subject)
> 
> summary(lme_Centroid)
Linear mixed-effects model fit by REML
 Data: My_Data2 
       AIC      BIC    logLik
  1904.563 1920.766 -946.2817

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1180.301 945.3498

Fixed effects: Centroid ~ Weight * Height 
                  Value Std.Error DF   t-value p-value
(Intercept)   -45019.39 21114.912 95 -2.132113  0.0356
Weight           710.53   353.074 15  2.012414  0.0625
Height           330.61   121.092 15  2.730246  0.0155
Weight:Height     -4.34     1.943 15 -2.233779  0.0411
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.16255520 -0.60084449 -0.02651629  0.54377042  1.92638924 

Number of Observations: 114
Number of Groups: 19 


> anova.lme(lme_Centroid,type = "marginal")
              numDF denDF  F-value p-value
(Intercept)       1    95 4.545908  0.0356
Weight            1    15 4.049810  0.0625
Height            1    15 7.454243  0.0155
Weight:Height     1    15 4.989769  0.0411
> 
> 
> 


So, I will report the results in this way:  ?Results showed that 
Centroid was linearly related to the interaction effect of Weight and 
Height (beta = -4.34, F(1,15)= 4.989769, p = 0.0411), and to Height 
(beta = 330.61, F(1,15)= 7.454243, p = 0.0155). 



Thanks in advance


Best


Angelo







    
  
	[[alternative HTML version deleted]]


From dpmeddings at gmail.com  Fri Jul 24 18:14:13 2015
From: dpmeddings at gmail.com (Daniel Meddings)
Date: Fri, 24 Jul 2015 17:14:13 +0100
Subject: [R] How to simulate informative censoring in a Cox PH model?
In-Reply-To: <CAFEqCdzk8cSwed-VdBrob1bzLQtx7rknK9UkNEU_bwt8auoD7A@mail.gmail.com>
References: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>
	<CAFEqCdzk8cSwed-VdBrob1bzLQtx7rknK9UkNEU_bwt8auoD7A@mail.gmail.com>
Message-ID: <CABrUXPVi2=GnEg14xZQ19XjJOnuW=61NVZc+Vkbfo2HDN08L8g@mail.gmail.com>

Hi Greg

Many thanks for taking the time to respond to my query. You are right about
pointing out the distinction between what variables are and are not
included in the event times process and in the censoring process. I clearly
forgot this important aspect. I amended my code to do as you advise and now
I am indeed getting biased estimates when using the informatively censored
responses. The problem is now that the estimates from the independently
censored responses are the same - i.e. they are just as biased. Thus the
bias seems to be due entirely to model mis-specification and not the
informative censoring.


To give a concrete example I simulate event times T_i and censoring times
C_i from the following models;


T_i~ Weibull(lambda_t(x),v_t),    lambda_t(x)=lambda_t*exp( beta_t_0 +
(beta_t_1*Treat_i) + (beta_t_2*Z_i) + (beta_t_3*Treat_i*Z_i)  )

C_i~ Weibull(lambda_c(x),v_c),    lambda_c(x)=lambda_c*exp( beta_c_0 +
(beta_c_1*Treat_i) + (beta_c_2*Z_i) + (beta_c_3*Treat_i*Z_i)  )

D_i~Weibull(lambda_d(x),v_D), lambda_d(x)=lamda_d*exp( beta_d_0)

where ;

beta_t_0 = 1,  beta_t_1 = -1,   beta_t_2 = 1,  beta_t_3 = -2,   lambda_t=0.5

beta_c_0 = 0.2,  beta_c_1 = -2,   beta_c_2 = 2,  beta_c_3 = -2,
lambda_c=0.5

beta_d_0 = -0.7,  lambda_d=0.1

When I fit the cox model to both the informatively censored responses and
the independent censored responses I include only the Treatment covariate
in the model.

I simulate Treatment from a Bernoulli distribution with p=0.5 and Z_i from
a beta distribution so that Z ranges from 0 to 1 (I like to think of Z as a
"poor" prognosis probability where Z_i=1 means a subject is 100% certain to
have a poor prognosis and Z_i=0 means zero chance). These parameter choices
give approximately 27% and 25% censoring for the informatively censored
responses (using C_i) and the independent censored responses (using D_i)
respectively. I use N=2000 subjects and 2000 simulation replications.

The above simulation I get estimates of beta_t_2 of -1.526 and -1.537 for
the informatively censored responses and the independent censored responses
respectively.

Furthermore when I fit a cox model to the full responses (no censoring at
all) I get an estimate of beta_t_2 of -1.542. This represents the best that
can possibly be done given that Z and Treat*Z are not in the model. Clearly
censoring is not making much of a difference here - model mis-specification
dominates.

I still must be doing something wrong but I cannot figure this one out.

Thanks

Dan



On Thu, Jul 23, 2015 at 12:33 AM, Greg Snow <538280 at gmail.com> wrote:

> I think that the Cox model still works well when the only information
> in the censoring is conditional on variables in the model.  What you
> describe could be called non-informative conditional on x.
>
> To really see the difference you need informative censoring that
> depends on something not included in the model.  One option would be
> to use copulas to generate dependent data and then transform the
> values using your Weibul.  Or you could generate your event times and
> censoring times based on x1 and x2, but then only include x1 in the
> model.
>
> On Wed, Jul 22, 2015 at 2:20 AM, Daniel Meddings <dpmeddings at gmail.com>
> wrote:
> > I wish to simulate event times where the censoring is informative, and to
> > compare parameter estimator quality from a Cox PH model with estimates
> > obtained from event times generated with non-informative censoring.
> However
> > I am struggling to do this, and I conclude rather than a technical flaw
> in
> > my code I instead do not understand what is meant by informative and
> > un-informative censoring.
> >
> > My approach is to simulate an event time T dependent on a vector of
> > covariates x having hazard function h(t|x)=lambda*exp(beta'*x)v*t^{v-1}.
> > This corresponds to T~ Weibull(lambda(x),v), where the scale parameter
> > lambda(x)=lambda*exp(beta'*x) depends on x and the shape parameter v is
> > fixed. I have N subjects where T_{i}~ Weibull(lambda(x_{i}),v_{T}),
> > lambda(x_{i})=lambda_{T}*exp(beta_{T}'*x_{i}), for i=1,...,N. Here I
> assume
> > the regression coefficients are p-dimensional.
> >
> > I generate informative censoring times C_i~ Weibull(lambda(x_i),v_C),
> > lambda(x_i)=lambda_C*exp(beta_C'*x_i) and compute Y_inf_i=min(T_i,C_i)
> and
> > a censored flag delta_inf_i=1 if Y_inf_i <= C_i (an observed event), and
> > delta_inf_i=0 if Y_inf_i > C_i (informatively censored: event not
> > observed). I am convinced this is informative censoring because as long
> as
> > beta_T~=0 and beta_C~=0 then for each subject the data generating process
> > for T and C both depend on x.
> >
> > In contrast I generate non-informative censoring times
> > D_i~Weibull(lambda_D*exp(beta_D),v_D), and compute Y_ninf_i=min(T_i,D_i)
> > and a censored flag delta_ninf_i=1 if Y_ninf_i <= D_i (an observed
> event),
> > and delta_ninf_i=0 if Y_ninf_i > D_i (non-informatively censored: event
> not
> > observed). Here beta_D is a scalar. I "scale" the simulation by choosing
> > the lambda_T, lambda_C and lambda_D parameters such that on average
> T_i<C_i
> > and T_i<D_i to achieve X% of censored subjects for both Y_inf_i and
> > Y_ninf_i.
> >
> > The problem is that even for say 30% censoring (which I think is high),
> the
> > Cox PH parameter estimates using both Y_inf and Y_ninf are unbiased when
> I
> > expected the estimates using Y_inf to be biased, and I think I see why:
> > however different beta_C is from beta_T, a censored subject can
> presumably
> > influence the estimation of beta_T only by affecting the set of subjects
> at
> > risk at any time t, but this does not change the fact that every single
> > Y_inf_i with delta_inf_i=1 will have been generated using beta_T only.
> Thus
> > I do not see how my simulation can possibly produce biased estimates for
> > beta_T using Y_inf.
> >
> > But then what is informative censoring if not based on this approach?
> >
> > Any help would be greatly appreciated.
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Fri Jul 24 19:43:40 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Fri, 24 Jul 2015 13:43:40 -0400
Subject: [R] R version update
In-Reply-To: <5415fa6a82047c040f1321499132adb2@www.sxmail.de>
References: <5415fa6a82047c040f1321499132adb2@www.sxmail.de>
Message-ID: <CA+vqiLHvLQZ4uDbnkWQXr-yx2mdQShrqgYA4TdFdX-SMw0it4w@mail.gmail.com>

Hi Markus,

Please keep the list copied. See responses in line.


On Fri, Jul 24, 2015 at 1:22 PM,  <klerer at sxmail.de> wrote:
> Dear Ista Zahn,
>
> Thank you that you answered.
> Kubuntu delivered some packages of R and I obviously got R version 3.0.2 by
> the time (early 2015) I installed R fully on my Linux Kubuntu Trusty Tahr.

Yes, I understood that much. If you want to upgrade to a more recent
version follow the instructions in the first link I sent you last time
(https://cran.r-project.org/bin/linux/ubuntu/).

> The package "ATLAS" is a recommended one from recent R-explaining literature
> that I intend to work through to get accustomed and tackle R as fully as I
> can.

Well, AFAIK there is no such package on CRAN. What literature exactly
are you referring to?

 I had to experience that beyond a package called "ATLAS" a certain
> number of further other very useful packages were not available because
> being "not for R version 3.0.2" as I appended when speaking of the problem
> "ATLAS".
> My conclusion up to now is: I cannot use R fully because I have no
> opportunity to update my basic R version AS SUCH. I hope there is a way of
> solving this without destroying too much of the wealth of packages I have
> already integrated.

OK, so follow the instructions for updating.

Best,
ISta

>
> Best regards,
> Markus Hofstetter
>
>
> Ista Zahn <istazahn at gmail.com> schrieb (22.07.2015 14:24):
>
>
> On Jul 21, 2015 9:30 PM, <klerer at sxmail.de> wrote:
>>
>> Dear Ladies and Gentlemen,
>> as a beginner in R, I encountered a sort of "naturally given
>> limits"concerning the process of amending R with packages.
>> I apparently own version 3.0.2 and I principally decided to use R via
>> the RKWard GUI on Linux Kubuntu Trusty Tahr; the R version installed on
>> my computer is admittedly not far from the basics provided by my
>> distribution's package management (also throughout multiple rounds of
>> updates/ upgrades of the system).
>> What would I have to do to finally update/ upgrade beyond that R
>> version?
>
> https://cran.r-project.org/bin/linux/ubuntu/
>
> Would that process (generally?) affect any/ my GUI?
>
> possibly.
>
>> This problem appeared to me as I was busy installing packages an
>> received the error message "package ?ATLAS? is not available (for R
>> version 3.0.2) ".
>
> I don't think there is any package on CRAN with that name. What makes you
> think there is?
>
>> I am quite desperate and would look forward to be indicated a path to a
>> sustainable solution or be told how to mitigate/ circumvent (these/
>> such) problems.
>
> I'm not actually sure exactly what the problem is...
>
>> Thanks a lot!
>>
>> Best regards,Markus Hofstetter
>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Jul 24 19:58:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 24 Jul 2015 10:58:53 -0700
Subject: [R] R version update
In-Reply-To: <a5605b357787a0fb7fc2fcee8242dd36@www.sxmail.de>
References: <a5605b357787a0fb7fc2fcee8242dd36@www.sxmail.de>
Message-ID: <845CA85B-1D52-47FD-8CAF-32B7AF960F5D@comcast.net>


On Jul 21, 2015, at 11:55 AM, klerer at sxmail.de wrote:

> Dear Ladies and Gentlemen,
> as a beginner in R, I encountered a sort of "naturally given limits"concerning the process of amending R with packages.
> I apparently own version 3.0.2 and I principally decided to use R via
> the RKWard GUI on Linux Kubuntu Trusty Tahr; the R version installed on
> my computer is admittedly not far from the basics provided by my
> distribution's package management (also throughout multiple rounds of
> updates/ upgrades of the system).
> What would I have to do to finally update/ upgrade beyond that R
> version? Would that process (generally?) affect any/ my GUI?
> This problem appeared to me as I was busy installing packages an
> received the error message "package ?ATLAS? is not available (for R
> version 3.0.2) ".

ATLAS is not an R package:

https://launchpad.net/ubuntu/+source/atlas

> I am quite desperate and would look forward to be indicated a path to a
> sustainable solution or be told how to mitigate/ circumvent (these/
> such) problems.
> Thanks a lot!
> 
> Best regards,Markus Hofstetter
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.CA.us  Fri Jul 24 20:02:57 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Fri, 24 Jul 2015 11:02:57 -0700
Subject: [R] Infinite Series
In-Reply-To: <CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
Message-ID: <B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>

Please reply-all so the mailing list stays in the loop.

cumsum(1/(1:100)^2)

gives you the partial sums up through i=100.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
>Hello Jeff,
>
>Thank you so much for the suggestion,  I searched cumsum as suggested
>but
>not sure it is what I had in mind.  For instance if I had the infinite
>series:    [image: Inline image 1]
>
>and want to compute the sum of the, say, first 100 terms, how could I
>use
>cusum to do that?
>
>Thanks again,
>
>Janh
>
>
>On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
><jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> ?cumsum
>>
>---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go
>Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                       Live:   OO#.. Dead: OO#.. 
>Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>>
>---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
>wrote:
>> >Dear All,
>> >
>> >Does anyone know of any R functions that compute partial sums of
>> >series?
>> >
>> >Thanks in advance!
>> >
>> >Janh
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> >______________________________________________
>> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >PLEASE do read the posting guide
>> >http://www.R-project.org/posting-guide.html
>> >and provide commented, minimal, self-contained, reproducible code.
>>
>>


From j.para.fernandez at hotmail.com  Fri Jul 24 21:23:32 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Fri, 24 Jul 2015 12:23:32 -0700 (PDT)
Subject: [R] R GUI plot by color
In-Reply-To: <1437731620874-4710300.post@n4.nabble.com>
References: <1437727381033-4710297.post@n4.nabble.com>
	<CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>
	<1437731620874-4710300.post@n4.nabble.com>
Message-ID: <1437765812248-4710320.post@n4.nabble.com>

Yes, you were right!! 

Thanks.




--
View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297p4710320.html
Sent from the R help mailing list archive at Nabble.com.


From annijanh at gmail.com  Sat Jul 25 01:37:00 2015
From: annijanh at gmail.com (Janh Anni)
Date: Fri, 24 Jul 2015 19:37:00 -0400
Subject: [R] Infinite Series
In-Reply-To: <B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
Message-ID: <CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>

Hello Jeff,

Thanks a lot.  I tried it and see that it prints out the entire 100 partial
sums, so I can take the last value as the partial sum for the first 100
terms. Would there be any way cumsum can print only the nth partial sum,
i.e. the last value in the array, instead of printing the entire array?
Thanks again.

Joseph

On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Please reply-all so the mailing list stays in the loop.
>
> cumsum(1/(1:100)^2)
>
> gives you the partial sums up through i=100.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
> >Hello Jeff,
> >
> >Thank you so much for the suggestion,  I searched cumsum as suggested
> >but
> >not sure it is what I had in mind.  For instance if I had the infinite
> >series:    [image: Inline image 1]
> >
> >and want to compute the sum of the, say, first 100 terms, how could I
> >use
> >cusum to do that?
> >
> >Thanks again,
> >
> >Janh
> >
> >
> >On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
> ><jdnewmil at dcn.davis.ca.us>
> >wrote:
> >
> >> ?cumsum
> >>
>
> >---------------------------------------------------------------------------
> >> Jeff Newmiller                        The     .....       .....  Go
> >Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >> Go...
> >>                                       Live:   OO#.. Dead: OO#..
> >Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> >rocks...1k
> >>
>
> >---------------------------------------------------------------------------
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
> >wrote:
> >> >Dear All,
> >> >
> >> >Does anyone know of any R functions that compute partial sums of
> >> >series?
> >> >
> >> >Thanks in advance!
> >> >
> >> >Janh
> >> >
> >> >       [[alternative HTML version deleted]]
> >> >
> >> >______________________________________________
> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >PLEASE do read the posting guide
> >> >http://www.R-project.org/posting-guide.html
> >> >and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Jul 25 01:57:23 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 24 Jul 2015 16:57:23 -0700
Subject: [R] Infinite Series
In-Reply-To: <CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
	<CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
Message-ID: <CAGxFJbShKai0-ib6mCmBE5DYPN-cqPxfSLwTKCzipaG8wCyEKQ@mail.gmail.com>

Janh:

It sounds like you really need to go through an R tutorial or two
before posting further, as this is a pretty basic query. Or am I wrong
about this?

An answer: Just use indexing

cumsum(1/seq_len(100)^2)[seq(10, to = 100,by = 10)] ## keeps every 10th

 [1] 1.549768 1.596163 1.612150 1.620244 1.625133 1.628406 1.630750
1.632512 1.633884
[10] 1.634984


But beware FAQ 7.31 for long series.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Jul 24, 2015 at 4:37 PM, Janh Anni <annijanh at gmail.com> wrote:
> Hello Jeff,
>
> Thanks a lot.  I tried it and see that it prints out the entire 100 partial
> sums, so I can take the last value as the partial sum for the first 100
> terms. Would there be any way cumsum can print only the nth partial sum,
> i.e. the last value in the array, instead of printing the entire array?
> Thanks again.
>
> Joseph
>
> On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
>
>> Please reply-all so the mailing list stays in the loop.
>>
>> cumsum(1/(1:100)^2)
>>
>> gives you the partial sums up through i=100.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                       Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>>
>> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
>> >Hello Jeff,
>> >
>> >Thank you so much for the suggestion,  I searched cumsum as suggested
>> >but
>> >not sure it is what I had in mind.  For instance if I had the infinite
>> >series:    [image: Inline image 1]
>> >
>> >and want to compute the sum of the, say, first 100 terms, how could I
>> >use
>> >cusum to do that?
>> >
>> >Thanks again,
>> >
>> >Janh
>> >
>> >
>> >On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
>> ><jdnewmil at dcn.davis.ca.us>
>> >wrote:
>> >
>> >> ?cumsum
>> >>
>>
>> >---------------------------------------------------------------------------
>> >> Jeff Newmiller                        The     .....       .....  Go
>> >Live...
>> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> >> Go...
>> >>                                       Live:   OO#.. Dead: OO#..
>> >Playing
>> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> >> /Software/Embedded Controllers)               .OO#.       .OO#.
>> >rocks...1k
>> >>
>>
>> >---------------------------------------------------------------------------
>> >> Sent from my phone. Please excuse my brevity.
>> >>
>> >> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
>> >wrote:
>> >> >Dear All,
>> >> >
>> >> >Does anyone know of any R functions that compute partial sums of
>> >> >series?
>> >> >
>> >> >Thanks in advance!
>> >> >
>> >> >Janh
>> >> >
>> >> >       [[alternative HTML version deleted]]
>> >> >
>> >> >______________________________________________
>> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> >https://stat.ethz.ch/mailman/listinfo/r-help
>> >> >PLEASE do read the posting guide
>> >> >http://www.R-project.org/posting-guide.html
>> >> >and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >>
>>
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Jul 25 02:07:27 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 24 Jul 2015 17:07:27 -0700
Subject: [R] Infinite Series
In-Reply-To: <CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
	<CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
Message-ID: <A42E6348-D5C3-4AE5-9D0A-B734401556D8@comcast.net>


On Jul 24, 2015, at 4:37 PM, Janh Anni wrote:

> Hello Jeff,
> 
> Thanks a lot.  I tried it and see that it prints out the entire 100 partial
> sums, so I can take the last value as the partial sum for the first 100
> terms. Would there be any way cumsum can print only the nth partial sum,
> i.e. the last value in the array, instead of printing the entire array?
> Thanks again.

Wouldn't that just mean using sum instead of cumsum?????

Can even check the error from the analytical limit.

> sum(1/(1:100)^2) - pi^2/6
[1] -0.009950167


> 
> Joseph
> 
> On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Please reply-all so the mailing list stays in the loop.
>> 
>> cumsum(1/(1:100)^2)
>> 
>> gives you the partial sums up through i=100.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                      Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
>>> Hello Jeff,
>>> 
>>> Thank you so much for the suggestion,  I searched cumsum as suggested
>>> but
>>> not sure it is what I had in mind.  For instance if I had the infinite
>>> series:    [image: Inline image 1]
>>> 
>>> and want to compute the sum of the, say, first 100 terms, how could I
>>> use
>>> cusum to do that?
>>> 
>>> Thanks again,
>>> 
>>> Janh
>>> 
>>> 
>>> On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
>>> <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>> 
>>>> ?cumsum
>>>> 
>> 
>>> ---------------------------------------------------------------------------
>>>> Jeff Newmiller                        The     .....       .....  Go
>>> Live...
>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>> Go...
>>>>                                      Live:   OO#.. Dead: OO#..
>>> Playing
>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> rocks...1k
>>>> 
>> 
>>> ---------------------------------------------------------------------------
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
>>> wrote:
>>>>> Dear All,
>>>>> 
>>>>> Does anyone know of any R functions that compute partial sums of
>>>>> series?
>>>>> 
>>>>> Thanks in advance!
>>>>> 
>>>>> Janh
>>>>> 
>>>>>      [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From timcke at hotmail.de  Fri Jul 24 08:59:15 2015
From: timcke at hotmail.de (Marie-Louise)
Date: Thu, 23 Jul 2015 23:59:15 -0700 (PDT)
Subject: [R] interactive Map: Popups
In-Reply-To: <CAPfrkhmH+poL6MN=gAVeTavEsk6uaeFz0A9u9G5ZYvjMp=fVUA@mail.gmail.com>
References: <1437608132059-4710226.post@n4.nabble.com>
	<3D7241A7-DD00-44EF-9DB7-1642B4235C0C@univie.ac.at>
	<31D66263-1EAB-4DC4-B4EB-087EA2294A05@univie.ac.at>
	<CAPfrkhmH+poL6MN=gAVeTavEsk6uaeFz0A9u9G5ZYvjMp=fVUA@mail.gmail.com>
Message-ID: <1437721155533-4710295.post@n4.nabble.com>

You guys are awesome thank you all very very much!



--
View this message in context: http://r.789695.n4.nabble.com/interactive-Map-Popups-tp4710226p4710295.html
Sent from the R help mailing list archive at Nabble.com.


From sreenath.rajur at macfast.ac.in  Fri Jul 24 09:37:57 2015
From: sreenath.rajur at macfast.ac.in (sreenath)
Date: Fri, 24 Jul 2015 00:37:57 -0700 (PDT)
Subject: [R] Labeling world map
Message-ID: <1437723477804-4710296.post@n4.nabble.com>

I draw world map using 
library(maptools)
> library(ggmap)
> library(mapdata)
> library(maps)
>map("world",fill=TRUE,col="White",bg="light
blue",ylim=c(-60,90),mar=c(0,0,0,0))
>native <- c("brazil","sao paulo state")
> nat <-geocode(native)
>nat.x <- nat$lon
> nat.y <- nat$lat
>points(nat.x,nat.y,col="yellow",pch=16)
How can i put lilte for this map and label points




--
View this message in context: http://r.789695.n4.nabble.com/Labeling-world-map-tp4710296.html
Sent from the R help mailing list archive at Nabble.com.


From sreenath.rajur at macfast.ac.in  Fri Jul 24 13:04:11 2015
From: sreenath.rajur at macfast.ac.in (sreenath)
Date: Fri, 24 Jul 2015 04:04:11 -0700 (PDT)
Subject: [R] P value from jaccard's index matrix
Message-ID: <1437735851182-4710302.post@n4.nabble.com>

My table having 40 raw and 4 columns, in that 4 columns first column belongs
to one group and the remaining constitute the other group. using following
commands for calculating jaccard's index
x <- read.csv(file name,header=T, sep= ) 
jac <- vegdist(x,method="jaccard") 
from this out file(jac) how can i find the p value for two groups ? and how
can i plot notched box plot of these two groups? when i use boxes
(as.matrix(jac)~x$first column,notch=TRUE) it showing 40 box plots.why it
so?




--
View this message in context: http://r.789695.n4.nabble.com/P-value-from-jaccard-s-index-matrix-tp4710302.html
Sent from the R help mailing list archive at Nabble.com.


From shah_suraj at hotmail.co.uk  Fri Jul 24 13:08:08 2015
From: shah_suraj at hotmail.co.uk (SRS)
Date: Fri, 24 Jul 2015 04:08:08 -0700 (PDT)
Subject: [R] apply kendall tau to a split data set
Message-ID: <1437736088378-4710303.post@n4.nabble.com>

$`19179222`
   Unique.ID    Start.Year                    L1.Risk.Category                     
Gross.amount.sum
17  19179222       2013 Execution, Delivery & Process Management        
161212.1
18  19179222       2015 Execution, Delivery & Process Management        
110880.0

$`25182498`
   Unique.ID   Start.Year                    L1.Risk.Category                       
Gross.amount.sum
19  25182498       2014   Clients, Products & Business Practices                 
59384
20  25182498       2014 Execution, Delivery & Process Management          
355000
21  25182498       2015 Execution, Delivery & Process Management           
27720

$`32506027`
   Unique.ID    Start.Year                L1.Risk.Category                        
Gross.amount.sum
22  32506027       2003 Execution, Delivery & Process Management       
3600000.0
23  32506027       2013 Execution, Delivery & Process Management        
161212.1
24  32506027       2014   Clients, Products & Business Practices               
14846.0


Hi, I have split a data frame and output is as above. 

I would like to apply Kendall's tau to calculate correlations. For example,
for the last Unique.ID 32506027, I want to create a time series for each L1
risk category, and then calculate the correlation between each L1 risk
category using the Kendall tau approach. 

Any help would be much appreciated. 

Thanks
s




--
View this message in context: http://r.789695.n4.nabble.com/apply-kendall-tau-to-a-split-data-set-tp4710303.html
Sent from the R help mailing list archive at Nabble.com.


From nilsson.henric at gmail.com  Fri Jul 24 13:16:27 2015
From: nilsson.henric at gmail.com (Henric Winell)
Date: Fri, 24 Jul 2015 13:16:27 +0200
Subject: [R] =?utf-8?b?4oCcRXJyb3IgaW4gaWYgKGFicyh4IC0gb2xkeCkgPCBmdG9s?=
 =?utf-8?b?KeKAnSB3aGVuIHVzaW5nIOKAnGxvZ25vcm1hbOKAnSBkaXN0cmlidXRpb24g?=
 =?utf-8?q?in_mixed_logit?=
In-Reply-To: <1437690799874-4710284.post@n4.nabble.com>
References: <1437690799874-4710284.post@n4.nabble.com>
Message-ID: <55B21E8B.3010303@gmail.com>

Den 2015-07-24 kl. 00:33, skrev Dera:

> Hi, I have a question about how to use the mlogit package in R to do analysis
> of discrete choice survey data. Our survey is about asking people to choose
> from different insurance policies(with two attributes of deductible and
> premium).
>
> The code I used to fit mixed logit is:
>
>
> [1] ml <- mlogit.data (mydata, choice="choice", shape = "wide", id =
> "individual",
>                 opposite =c ('deductible', 'premium'),varying = 5:10)
>
> [2] ml.w5 <- mlogit (choice~deductible+premium|0, ml, panel = TRUE,
>               rpar = c(deductible='ln', premium='ln'),
>               R = 100, halton = NA, print.level=0)
>
> I try to use lognormal because we hope the coefficients for both deductible
> and premium are negative. And I use "opposite" in [1] to reverse the sign
> because lognormal is always positive.
>
> But I always get the error warning:
>
> "Error in if (abs(x - oldx) < ftol) { : missing value where TRUE/FALSE
> needed
> In addition: Warning message: In log(start[ln]) : NaNs produced"
> I double check the data and am sure there isn't any missing data. And if I
> change the lognormal "ln" to "n" or "cn", it will work without any warning.
>
> Does anyone know how to deal with this? Thank you for your help.

I've never used the 'mlogit' package but that error message has likely 
nothing to do with missingness in your data.

The error message is pretty clear: for some reason 'if (abs(x - oldx) < 
ftol)', which looks like a check for convergence, doesn't evaluate to 
TRUE or FALSE as expected.  Perhaps due to the warning message about 
problems with NaN in the starting values?  Have you checked your data? 
That warning message is typical for when you try to take logs of a 
negative number:

 > log(-1)
[1] NaN
Warning message:
In log(-1) : NaNs produced

Unless this helps you identify the problem, we need a (preferably small) 
reproducible example demonstrating the issue to be able to provide 
further help.

BTW, have you asked the package maintainer (as per the posting guide)?


Henric Winell



>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Error-in-if-abs-x-oldx-ftol-when-using-lognormal-distribution-in-mixed-logit-tp4710284.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From tulls4472 at gmail.com  Fri Jul 24 17:22:54 2015
From: tulls4472 at gmail.com (Martin Tully)
Date: Fri, 24 Jul 2015 16:22:54 +0100
Subject: [R] building a quicksort function in rcpp
Message-ID: <CAE0RWP-Sv97Loq2SAd0R0medjyfO+HfKoLJu19Uyb-gDUCTNqA@mail.gmail.com>

Hi I am using RCPP to build a C++ function for quicksort called qsort.
This function is compiled and loaded through the cxxfunction in R
I am getting the message in R error: no matching function for call to
'qsort(int*&)' The code is below.
It will not run for me and I was wondering if you could help?



library(Rcpp)
library(inline)


    incl <- 'int qsort(int xx[], int left, int right) {

            int i = left,
            j = right;
            int tmp;
            int pivot = xx[(left + right) / 2];

            /* partition */
              while (i <= j) {
                while (xx[i] < pivot)
                  i++;
                while (xx[j] > pivot)
                  j--;
                if (i <= j) {
                  tmp = xx[i];
                  xx[i] = xx[j];
                  xx[j] = tmp;
                  i++;
                  j--;
                }
              }

            /* recursion */
              if (left < j){
                qsort(xx, left, j);
              }
            if (i < right){
              qsort(xx, i, right);
            }

        return (qsort(xx));
          }
          '

          sortCpp <- cxxfunction(signature( x = "integer",left = "integer",
                                            right = "integer"),
                                 body = 'IntegerVector arr(x);
                                         int a = as<int>(left);
                                         int b = as<int>(right);
                                        return wrap(qsort(arr));',
                                 include = incl,
                                 plugin = "Rcpp",
                                 verbose = TRUE)

	[[alternative HTML version deleted]]


From yangwenyue900780 at 163.com  Fri Jul 24 18:17:41 2015
From: yangwenyue900780 at 163.com (wenyueyang)
Date: Fri, 24 Jul 2015 09:17:41 -0700 (PDT)
Subject: [R] How to calculate the average direct effect,
 average total effect and average indirect effect for spatial
 regression models with spatial lag of dependent variable
In-Reply-To: <0275b88c032e4762b88b0da6be96921c@winhexbeeu15.win.mail>
References: <1437665721814-4710253.post@n4.nabble.com>
	<0275b88c032e4762b88b0da6be96921c@winhexbeeu15.win.mail>
Message-ID: <1437754661918-4710315.post@n4.nabble.com>

Dear Linus Holtermann,

thank a lot for telling me the key to calculate the effects. I will try it.
thank you for your help!

best regards,

Wenyue Yang





--
View this message in context: http://r.789695.n4.nabble.com/How-to-calculate-the-average-direct-effect-average-total-effect-and-average-indirect-effect-for-spate-tp4710253p4710315.html
Sent from the R help mailing list archive at Nabble.com.


From gmoyeyemi at gmail.com  Fri Jul 24 21:59:48 2015
From: gmoyeyemi at gmail.com (gmoyeyemi)
Date: Fri, 24 Jul 2015 20:59:48 +0100
Subject: [R] Simulating multivariate gamma
Message-ID: <sgqnrtfcb23affjx6jk65bcb.1437767988207@email.android.com>

Hi,
I'm having problem simulating multivariate gamma. Is there anyone to assist in simulating multivariate gamma.
I know that for multivariate normal, we can use;
mvrnorm (n, means, sigma)

Thanks.

From timcke at hotmail.de  Fri Jul 24 23:24:29 2015
From: timcke at hotmail.de (Marie-Louise)
Date: Fri, 24 Jul 2015 14:24:29 -0700 (PDT)
Subject: [R] interactive Map with option to slide through years
Message-ID: <1437773069430-4710322.post@n4.nabble.com>

Hello,
first of all: my english is horrible but I try to explain what I want to
say.

I am new to R and thank to your help I managed to create an interactive map
of Germany for some weather data. I used 

States = data.frame(Datensatz)
require(googleVis)
G4 = gvisGeoChart(States, 
                  locationvar = "ISO", 
                  colorvar = "Hoechsttemperatur", hovervar = "name",
options=list(
                  width=800, height=600, region="DE", 
displayMode="regions", 
                  resolution="provinces", colorAxis="{colors:
['yellow','red']}"))
plot(G4)                  
plot(G4, 'chart')

for the map. Now I would like to create a map that shows the data for
different years. I would like to do something like this:
<http://r.789695.n4.nabble.com/file/n4710322/Bildschirmfoto_2015-07-24_um_23.png> 
Is there an easy possibility (I am really really new to R) to do so? Again,
I am very sorry for my bad english.

Thank you very very much,
Marie-Louise



--
View this message in context: http://r.789695.n4.nabble.com/interactive-Map-with-option-to-slide-through-years-tp4710322.html
Sent from the R help mailing list archive at Nabble.com.


From derawang at udel.edu  Sat Jul 25 00:27:58 2015
From: derawang at udel.edu (Dong Wang)
Date: Fri, 24 Jul 2015 18:27:58 -0400
Subject: [R] =?utf-8?b?4oCcRXJyb3IgaW4gaWYgKGFicyh4IC0gb2xkeCkgPCBmdG9s?=
	=?utf-8?b?KeKAnSB3aGVuIHVzaW5nIOKAnGxvZ25vcm1hbOKAnSBkaXN0cmlidXRp?=
	=?utf-8?q?on_in_mixed_logit?=
In-Reply-To: <55B21E8B.3010303@gmail.com>
References: <1437690799874-4710284.post@n4.nabble.com>
	<55B21E8B.3010303@gmail.com>
Message-ID: <CAD38Qvr8D24Vt3ZzM-0N27GAHYoepa82SeE-jg_JViVOZ1YUfQ@mail.gmail.com>

Hi Henric,

Thank you for your reply! It really helps. I should have noticed this
earlier. I don't have a negative input, the error is because our survey
allowed people to choose neither of the choices we gave, so we added a
choice "Neither" with both deductible and premium being 0, which caused
this problem. I tried to delete the "0" entries just now and it worked! I
think I need to find a way do deal with the "Neither" choice in another way.

Thank you so much and have a nice weekend!

Best, Dong

On Fri, Jul 24, 2015 at 7:16 AM, Henric Winell <nilsson.henric at gmail.com>
wrote:

> Den 2015-07-24 kl. 00:33, skrev Dera:
>
>  Hi, I have a question about how to use the mlogit package in R to do
>> analysis
>> of discrete choice survey data. Our survey is about asking people to
>> choose
>> from different insurance policies(with two attributes of deductible and
>> premium).
>>
>> The code I used to fit mixed logit is:
>>
>>
>> [1] ml <- mlogit.data (mydata, choice="choice", shape = "wide", id =
>> "individual",
>>                 opposite =c ('deductible', 'premium'),varying = 5:10)
>>
>> [2] ml.w5 <- mlogit (choice~deductible+premium|0, ml, panel = TRUE,
>>               rpar = c(deductible='ln', premium='ln'),
>>               R = 100, halton = NA, print.level=0)
>>
>> I try to use lognormal because we hope the coefficients for both
>> deductible
>> and premium are negative. And I use "opposite" in [1] to reverse the sign
>> because lognormal is always positive.
>>
>> But I always get the error warning:
>>
>> "Error in if (abs(x - oldx) < ftol) { : missing value where TRUE/FALSE
>> needed
>> In addition: Warning message: In log(start[ln]) : NaNs produced"
>> I double check the data and am sure there isn't any missing data. And if I
>> change the lognormal "ln" to "n" or "cn", it will work without any
>> warning.
>>
>> Does anyone know how to deal with this? Thank you for your help.
>>
>
> I've never used the 'mlogit' package but that error message has likely
> nothing to do with missingness in your data.
>
> The error message is pretty clear: for some reason 'if (abs(x - oldx) <
> ftol)', which looks like a check for convergence, doesn't evaluate to TRUE
> or FALSE as expected.  Perhaps due to the warning message about problems
> with NaN in the starting values?  Have you checked your data? That warning
> message is typical for when you try to take logs of a negative number:
>
> > log(-1)
> [1] NaN
> Warning message:
> In log(-1) : NaNs produced
>
> Unless this helps you identify the problem, we need a (preferably small)
> reproducible example demonstrating the issue to be able to provide further
> help.
>
> BTW, have you asked the package maintainer (as per the posting guide)?
>
>
> Henric Winell
>
>
>
>
>>
>>
>> --
>> View this message in context:
>> http://r.789695.n4.nabble.com/Error-in-if-abs-x-oldx-ftol-when-using-lognormal-distribution-in-mixed-logit-tp4710284.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>

	[[alternative HTML version deleted]]


From dmck at u.washington.edu  Sat Jul 25 01:48:39 2015
From: dmck at u.washington.edu (Don McKenzie)
Date: Fri, 24 Jul 2015 16:48:39 -0700
Subject: [R] Infinite Series
In-Reply-To: <CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
	<CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
Message-ID: <EFC01D42-CD87-426F-9D8F-BA125127912C@u.washington.edu>

cumsum(1/(1:100)^2)[100]


> On Jul 24, 2015, at 4:37 PM, Janh Anni <annijanh at gmail.com> wrote:
> 
> Hello Jeff,
> 
> Thanks a lot.  I tried it and see that it prints out the entire 100 partial
> sums, so I can take the last value as the partial sum for the first 100
> terms. Would there be any way cumsum can print only the nth partial sum,
> i.e. the last value in the array, instead of printing the entire array?
> Thanks again.
> 
> Joseph
> 
> On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> Please reply-all so the mailing list stays in the loop.
>> 
>> cumsum(1/(1:100)^2)
>> 
>> gives you the partial sums up through i=100.
>> ---------------------------------------------------------------------------
>> Jeff Newmiller                        The     .....       .....  Go Live...
>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>> Go...
>>                                      Live:   OO#.. Dead: OO#..  Playing
>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
>> ---------------------------------------------------------------------------
>> Sent from my phone. Please excuse my brevity.
>> 
>> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
>>> Hello Jeff,
>>> 
>>> Thank you so much for the suggestion,  I searched cumsum as suggested
>>> but
>>> not sure it is what I had in mind.  For instance if I had the infinite
>>> series:    [image: Inline image 1]
>>> 
>>> and want to compute the sum of the, say, first 100 terms, how could I
>>> use
>>> cusum to do that?
>>> 
>>> Thanks again,
>>> 
>>> Janh
>>> 
>>> 
>>> On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
>>> <jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>> 
>>>> ?cumsum
>>>> 
>> 
>>> ---------------------------------------------------------------------------
>>>> Jeff Newmiller                        The     .....       .....  Go
>>> Live...
>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>> Go...
>>>>                                      Live:   OO#.. Dead: OO#..
>>> Playing
>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>> rocks...1k
>>>> 
>> 
>>> ---------------------------------------------------------------------------
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
>>> wrote:
>>>>> Dear All,
>>>>> 
>>>>> Does anyone know of any R functions that compute partial sums of
>>>>> series?
>>>>> 
>>>>> Thanks in advance!
>>>>> 
>>>>> Janh
>>>>> 
>>>>>      [[alternative HTML version deleted]]
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




From annijanh at gmail.com  Sat Jul 25 04:18:19 2015
From: annijanh at gmail.com (Janh Anni)
Date: Fri, 24 Jul 2015 22:18:19 -0400
Subject: [R] Infinite Series
In-Reply-To: <CAGxFJbShKai0-ib6mCmBE5DYPN-cqPxfSLwTKCzipaG8wCyEKQ@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
	<CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
	<CAGxFJbShKai0-ib6mCmBE5DYPN-cqPxfSLwTKCzipaG8wCyEKQ@mail.gmail.com>
Message-ID: <CAFCoDdA6RNnfwSA-g47zpv7YeFHac4BGH0i1utfZebU9pq-f3g@mail.gmail.com>

Thanks Bert!

On Fri, Jul 24, 2015 at 7:57 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Janh:
>
> It sounds like you really need to go through an R tutorial or two
> before posting further, as this is a pretty basic query. Or am I wrong
> about this?
>
> An answer: Just use indexing
>
> cumsum(1/seq_len(100)^2)[seq(10, to = 100,by = 10)] ## keeps every 10th
>
>  [1] 1.549768 1.596163 1.612150 1.620244 1.625133 1.628406 1.630750
> 1.632512 1.633884
> [10] 1.634984
>
>
> But beware FAQ 7.31 for long series.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Fri, Jul 24, 2015 at 4:37 PM, Janh Anni <annijanh at gmail.com> wrote:
> > Hello Jeff,
> >
> > Thanks a lot.  I tried it and see that it prints out the entire 100
> partial
> > sums, so I can take the last value as the partial sum for the first 100
> > terms. Would there be any way cumsum can print only the nth partial sum,
> > i.e. the last value in the array, instead of printing the entire array?
> > Thanks again.
> >
> > Joseph
> >
> > On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> > wrote:
> >
> >> Please reply-all so the mailing list stays in the loop.
> >>
> >> cumsum(1/(1:100)^2)
> >>
> >> gives you the partial sums up through i=100.
> >>
> ---------------------------------------------------------------------------
> >> Jeff Newmiller                        The     .....       .....  Go
> Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >> Go...
> >>                                       Live:   OO#.. Dead: OO#..  Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >>
> ---------------------------------------------------------------------------
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
> >> >Hello Jeff,
> >> >
> >> >Thank you so much for the suggestion,  I searched cumsum as suggested
> >> >but
> >> >not sure it is what I had in mind.  For instance if I had the infinite
> >> >series:    [image: Inline image 1]
> >> >
> >> >and want to compute the sum of the, say, first 100 terms, how could I
> >> >use
> >> >cusum to do that?
> >> >
> >> >Thanks again,
> >> >
> >> >Janh
> >> >
> >> >
> >> >On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
> >> ><jdnewmil at dcn.davis.ca.us>
> >> >wrote:
> >> >
> >> >> ?cumsum
> >> >>
> >>
> >>
> >---------------------------------------------------------------------------
> >> >> Jeff Newmiller                        The     .....       .....  Go
> >> >Live...
> >> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.
> Live
> >> >> Go...
> >> >>                                       Live:   OO#.. Dead: OO#..
> >> >Playing
> >> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> >> >rocks...1k
> >> >>
> >>
> >>
> >---------------------------------------------------------------------------
> >> >> Sent from my phone. Please excuse my brevity.
> >> >>
> >> >> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
> >> >wrote:
> >> >> >Dear All,
> >> >> >
> >> >> >Does anyone know of any R functions that compute partial sums of
> >> >> >series?
> >> >> >
> >> >> >Thanks in advance!
> >> >> >
> >> >> >Janh
> >> >> >
> >> >> >       [[alternative HTML version deleted]]
> >> >> >
> >> >> >______________________________________________
> >> >> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> >https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> >PLEASE do read the posting guide
> >> >> >http://www.R-project.org/posting-guide.html
> >> >> >and provide commented, minimal, self-contained, reproducible code.
> >> >>
> >> >>
> >>
> >>
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From annijanh at gmail.com  Sat Jul 25 04:24:42 2015
From: annijanh at gmail.com (Janh Anni)
Date: Fri, 24 Jul 2015 22:24:42 -0400
Subject: [R] Infinite Series
In-Reply-To: <A42E6348-D5C3-4AE5-9D0A-B734401556D8@comcast.net>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
	<CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
	<A42E6348-D5C3-4AE5-9D0A-B734401556D8@comcast.net>
Message-ID: <CAFCoDdB0v4-mS1O69HbgfR1+86HCDERvNw_RddoADgUe=m5-Kw@mail.gmail.com>

Wow! So many (simpler) ways to skin a cat.  Thanks!

On Fri, Jul 24, 2015 at 8:07 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On Jul 24, 2015, at 4:37 PM, Janh Anni wrote:
>
> > Hello Jeff,
> >
> > Thanks a lot.  I tried it and see that it prints out the entire 100
> partial
> > sums, so I can take the last value as the partial sum for the first 100
> > terms. Would there be any way cumsum can print only the nth partial sum,
> > i.e. the last value in the array, instead of printing the entire array?
> > Thanks again.
>
> Wouldn't that just mean using sum instead of cumsum?????
>
> Can even check the error from the analytical limit.
>
> > sum(1/(1:100)^2) - pi^2/6
> [1] -0.009950167
>
>
> >
> > Joseph
> >
> > On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> > wrote:
> >
> >> Please reply-all so the mailing list stays in the loop.
> >>
> >> cumsum(1/(1:100)^2)
> >>
> >> gives you the partial sums up through i=100.
> >>
> ---------------------------------------------------------------------------
> >> Jeff Newmiller                        The     .....       .....  Go
> Live...
> >> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >> Go...
> >>                                      Live:   OO#.. Dead: OO#..  Playing
> >> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >> /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >>
> ---------------------------------------------------------------------------
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
> >>> Hello Jeff,
> >>>
> >>> Thank you so much for the suggestion,  I searched cumsum as suggested
> >>> but
> >>> not sure it is what I had in mind.  For instance if I had the infinite
> >>> series:    [image: Inline image 1]
> >>>
> >>> and want to compute the sum of the, say, first 100 terms, how could I
> >>> use
> >>> cusum to do that?
> >>>
> >>> Thanks again,
> >>>
> >>> Janh
> >>>
> >>>
> >>> On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
> >>> <jdnewmil at dcn.davis.ca.us>
> >>> wrote:
> >>>
> >>>> ?cumsum
> >>>>
> >>
> >>>
> ---------------------------------------------------------------------------
> >>>> Jeff Newmiller                        The     .....       .....  Go
> >>> Live...
> >>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >>>> Go...
> >>>>                                      Live:   OO#.. Dead: OO#..
> >>> Playing
> >>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >>>> /Software/Embedded Controllers)               .OO#.       .OO#.
> >>> rocks...1k
> >>>>
> >>
> >>>
> ---------------------------------------------------------------------------
> >>>> Sent from my phone. Please excuse my brevity.
> >>>>
> >>>> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
> >>> wrote:
> >>>>> Dear All,
> >>>>>
> >>>>> Does anyone know of any R functions that compute partial sums of
> >>>>> series?
> >>>>>
> >>>>> Thanks in advance!
> >>>>>
> >>>>> Janh
> >>>>>
> >>>>>      [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>>
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Sat Jul 25 05:28:45 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 25 Jul 2015 15:28:45 +1200
Subject: [R] [FORGED]  Simulating multivariate gamma
In-Reply-To: <sgqnrtfcb23affjx6jk65bcb.1437767988207@email.android.com>
References: <sgqnrtfcb23affjx6jk65bcb.1437767988207@email.android.com>
Message-ID: <55B3026D.9030001@auckland.ac.nz>


On 25/07/15 07:59, gmoyeyemi wrote:

> Hi, I'm having problem simulating multivariate gamma. Is there
> anyone to assist in simulating multivariate gamma. I know that for
> multivariate normal, we can use; mvrnorm (n, means, sigma)

For starters you have to specify exactly what you mean by "multivariate 
gamma"; there is not a *unique* multivariate gamma distribution.

A little bit of googling on "multivariate gamma" will lead you to some 
useful material.  In particular there is a reference to the copula 
package which apparently provides a means of simulating a multivariate
distribution with gamma marginals.

It is not however clear to me how one specifies the desired correlation 
structure, or what the limitations on such a structure are.  I find the 
documentation to be opaque.

Perhaps someone with knowledge and insight on this issue will chime in.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From waqas1518 at gmail.com  Sat Jul 25 06:41:27 2015
From: waqas1518 at gmail.com (Waqas Shafqat)
Date: Sat, 25 Jul 2015 09:41:27 +0500
Subject: [R] about R 3.2.0
Message-ID: <CADHyn5hLVdOTMQORoJy0QNsOpkxup20fEnmr=G0Zf5LxwUSKwA@mail.gmail.com>

Dear sir,

I am using currently R 3.1.3. But i wnat to upgrade this version to R 3.2.0
but i am feared that the libraries which i have installed may be remove?
please guide me.

thanks
-- 
Waqas Shafqat Chattha
Ph. D Scholar
Department of Plant Breeding and Genetics
University of Agriculture, Faisalabad
Pakistan

	[[alternative HTML version deleted]]


From sa.filahi at gmail.com  Fri Jul 24 14:05:23 2015
From: sa.filahi at gmail.com (Said Filahi)
Date: Fri, 24 Jul 2015 13:05:23 +0100
Subject: [R] (no subject)
Message-ID: <CABNiEmmN9UFWCiYQbkrbC1yhTXzPQ6noQoeD+TbCWjmyFEGvdg@mail.gmail.com>

hello

i'm trying to plot multiple image (.png) and plot as a single image using
mfrow().

i have already image1.png  image2.png image.png image4.png and a can't plot
them in single fig1.png?


thank you


Said FILAHI

morocco

	[[alternative HTML version deleted]]


From dmck at u.washington.edu  Sat Jul 25 06:14:15 2015
From: dmck at u.washington.edu (Don McKenzie)
Date: Fri, 24 Jul 2015 21:14:15 -0700
Subject: [R] Infinite Series
In-Reply-To: <CAFCoDdB0v4-mS1O69HbgfR1+86HCDERvNw_RddoADgUe=m5-Kw@mail.gmail.com>
References: <CAFCoDdDT8bQjxRB7_ULf90jCyDbpv9Y+rTRdcrLJUj40mYvRhg@mail.gmail.com>
	<682B443D-A48A-4CDC-9B72-E3F95192C0C4@dcn.davis.CA.us>
	<CAFCoDdCP1DAs0=khNq-wsDKmbkb+GjzN6EA=6aE8C3r1BrJ03A@mail.gmail.com>
	<B5CEFAA6-DF1A-4B18-B6EC-EE1615DE25B3@dcn.davis.CA.us>
	<CAFCoDdDTPJUC813-Lc_c2uNtFJ4s_vVrEKQQSLO5t7ZO2XYiug@mail.gmail.com>
	<A42E6348-D5C3-4AE5-9D0A-B734401556D8@comcast.net>
	<CAFCoDdB0v4-mS1O69HbgfR1+86HCDERvNw_RddoADgUe=m5-Kw@mail.gmail.com>
Message-ID: <1E6A2AF0-995A-45C2-A832-A3C8873033F6@u.washington.edu>

but you get a different hide with sum vs. cumsum.  David is right if you want the sum of n terms.

> sum(1/(1:100)^2) - pi^2/6
[1] -0.009950167
> sum(1/(1:1000)^2) - pi^2/6
[1] -0.0009995002

etc.

> On Jul 24, 2015, at 7:24 PM, Janh Anni <annijanh at gmail.com> wrote:
> 
> Wow! So many (simpler) ways to skin a cat.  Thanks!
> 
> On Fri, Jul 24, 2015 at 8:07 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> 
>> 
>> On Jul 24, 2015, at 4:37 PM, Janh Anni wrote:
>> 
>>> Hello Jeff,
>>> 
>>> Thanks a lot.  I tried it and see that it prints out the entire 100
>> partial
>>> sums, so I can take the last value as the partial sum for the first 100
>>> terms. Would there be any way cumsum can print only the nth partial sum,
>>> i.e. the last value in the array, instead of printing the entire array?
>>> Thanks again.
>> 
>> Wouldn't that just mean using sum instead of cumsum?????
>> 
>> Can even check the error from the analytical limit.
>> 
>>> sum(1/(1:100)^2) - pi^2/6
>> [1] -0.009950167
>> 
>> 
>>> 
>>> Joseph
>>> 
>>> On Fri, Jul 24, 2015 at 2:02 PM, Jeff Newmiller <
>> jdnewmil at dcn.davis.ca.us>
>>> wrote:
>>> 
>>>> Please reply-all so the mailing list stays in the loop.
>>>> 
>>>> cumsum(1/(1:100)^2)
>>>> 
>>>> gives you the partial sums up through i=100.
>>>> 
>> ---------------------------------------------------------------------------
>>>> Jeff Newmiller                        The     .....       .....  Go
>> Live...
>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>> Go...
>>>>                                     Live:   OO#.. Dead: OO#..  Playing
>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>> rocks...1k
>>>> 
>> ---------------------------------------------------------------------------
>>>> Sent from my phone. Please excuse my brevity.
>>>> 
>>>> On July 24, 2015 10:30:09 AM PDT, Janh Anni <annijanh at gmail.com> wrote:
>>>>> Hello Jeff,
>>>>> 
>>>>> Thank you so much for the suggestion,  I searched cumsum as suggested
>>>>> but
>>>>> not sure it is what I had in mind.  For instance if I had the infinite
>>>>> series:    [image: Inline image 1]
>>>>> 
>>>>> and want to compute the sum of the, say, first 100 terms, how could I
>>>>> use
>>>>> cusum to do that?
>>>>> 
>>>>> Thanks again,
>>>>> 
>>>>> Janh
>>>>> 
>>>>> 
>>>>> On Thu, Jul 23, 2015 at 11:51 PM, Jeff Newmiller
>>>>> <jdnewmil at dcn.davis.ca.us>
>>>>> wrote:
>>>>> 
>>>>>> ?cumsum
>>>>>> 
>>>> 
>>>>> 
>> ---------------------------------------------------------------------------
>>>>>> Jeff Newmiller                        The     .....       .....  Go
>>>>> Live...
>>>>>> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>>>>>> Go...
>>>>>>                                     Live:   OO#.. Dead: OO#..
>>>>> Playing
>>>>>> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>>>>>> /Software/Embedded Controllers)               .OO#.       .OO#.
>>>>> rocks...1k
>>>>>> 
>>>> 
>>>>> 
>> ---------------------------------------------------------------------------
>>>>>> Sent from my phone. Please excuse my brevity.
>>>>>> 
>>>>>> On July 23, 2015 8:23:39 PM PDT, Janh Anni <annijanh at gmail.com>
>>>>> wrote:
>>>>>>> Dear All,
>>>>>>> 
>>>>>>> Does anyone know of any R functions that compute partial sums of
>>>>>>> series?
>>>>>>> 
>>>>>>> Thanks in advance!
>>>>>>> 
>>>>>>> Janh
>>>>>>> 
>>>>>>>     [[alternative HTML version deleted]]
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide
>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>>> 
>>>> 
>>>> 
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> David Winsemius
>> Alameda, CA, USA
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




From erich.neuwirth at univie.ac.at  Sat Jul 25 09:24:13 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Sat, 25 Jul 2015 09:24:13 +0200
Subject: [R] about R 3.2.0
In-Reply-To: <CADHyn5hLVdOTMQORoJy0QNsOpkxup20fEnmr=G0Zf5LxwUSKwA@mail.gmail.com>
References: <CADHyn5hLVdOTMQORoJy0QNsOpkxup20fEnmr=G0Zf5LxwUSKwA@mail.gmail.com>
Message-ID: <1059F851-82AC-481B-95F6-90DBBA60F52E@univie.ac.at>

If you are running R on Windows,
there is the package installr which will help you with the process
of upgrading.

And item 2.8 in the R for Windows FAQ has some information about upgrading also.
> On 25 Jul 2015, at 06:41, Waqas Shafqat <waqas1518 at gmail.com> wrote:
> 
> Dear sir,
> 
> I am using currently R 3.1.3. But i wnat to upgrade this version to R 3.2.0
> but i am feared that the libraries which i have installed may be remove?
> please guide me.
> 
> thanks
> --
> Waqas Shafqat Chattha
> Ph. D Scholar
> Department of Plant Breeding and Genetics
> University of Agriculture, Faisalabad
> Pakistan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 670 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20150725/56028b36/attachment.bin>

From jdnewmil at dcn.davis.CA.us  Sat Jul 25 09:56:58 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sat, 25 Jul 2015 00:56:58 -0700
Subject: [R] about R 3.2.0
In-Reply-To: <CADHyn5hLVdOTMQORoJy0QNsOpkxup20fEnmr=G0Zf5LxwUSKwA@mail.gmail.com>
References: <CADHyn5hLVdOTMQORoJy0QNsOpkxup20fEnmr=G0Zf5LxwUSKwA@mail.gmail.com>
Message-ID: <E89A52E2-4EAD-4E5F-A572-7680A19536E5@dcn.davis.CA.us>

You have not included the output of session info() which suggests you have nit read the Posting Guide... go do that now.

I am not aware of any install procedure that deletes R packages. You can usually install multiple versions of R. Each X.Y version of R by default refers to a different library directory. Therefore when you run a new version of R you are likely to find that it does not find your packages.

However that is not the end of the world... just reinstall them if you don't feel like paying attention to what the fine R Installation guide tells you. You may find that sometimes package authors don't update their packages when a new version of R comes out, so CRAN may archive those packages making it a bit less easy to use them. However, there is no way for us to guide you before that happens about your best approach afterward... some of your options may include learning how to fix packages yourself, rolling back to an earlier version of R (we generally can't help you if you take this route because the Posting Guide warns you that the current released version of R is expected here), or using a different package or your own code to perform the task.

Usually you can copy your old library directory onto the new one and update.packages to update them.  
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 24, 2015 9:41:27 PM PDT, Waqas Shafqat <waqas1518 at gmail.com> wrote:
>Dear sir,
>
>I am using currently R 3.1.3. But i wnat to upgrade this version to R
>3.2.0
>but i am feared that the libraries which i have installed may be
>remove?
>please guide me.
>
>thanks


From tal.galili at gmail.com  Sat Jul 25 10:34:44 2015
From: tal.galili at gmail.com (Tal Galili)
Date: Sat, 25 Jul 2015 11:34:44 +0300
Subject: [R] about R 3.2.0
In-Reply-To: <1059F851-82AC-481B-95F6-90DBBA60F52E@univie.ac.at>
References: <CADHyn5hLVdOTMQORoJy0QNsOpkxup20fEnmr=G0Zf5LxwUSKwA@mail.gmail.com>
	<1059F851-82AC-481B-95F6-90DBBA60F52E@univie.ac.at>
Message-ID: <CANdJ3dUBscz1-S-=9+iZt0CLQBn30wBuCSW01ber-cimsZg9wA@mail.gmail.com>

Following what Erich wrote, if you are on Windows, there is also a
step-by-step screenshot guide for upgrading R using the installr package:

http://www.r-statistics.com/2015/06/a-step-by-step-screenshots-tutorial-for-upgrading-r-on-windows/



----------------Contact
Details:-------------------------------------------------------
Contact me: Tal.Galili at gmail.com |
Read me: www.talgalili.com (Hebrew) | www.biostatistics.co.il (Hebrew) |
www.r-statistics.com (English)
----------------------------------------------------------------------------------------------


On Sat, Jul 25, 2015 at 10:24 AM, Erich Neuwirth <
erich.neuwirth at univie.ac.at> wrote:

> If you are running R on Windows,
> there is the package installr which will help you with the process
> of upgrading.
>
> And item 2.8 in the R for Windows FAQ has some information about upgrading
> also.
> > On 25 Jul 2015, at 06:41, Waqas Shafqat <waqas1518 at gmail.com> wrote:
> >
> > Dear sir,
> >
> > I am using currently R 3.1.3. But i wnat to upgrade this version to R
> 3.2.0
> > but i am feared that the libraries which i have installed may be remove?
> > please guide me.
> >
> > thanks
> > --
> > Waqas Shafqat Chattha
> > Ph. D Scholar
> > Department of Plant Breeding and Genetics
> > University of Agriculture, Faisalabad
> > Pakistan
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ligges at statistik.tu-dortmund.de  Sat Jul 25 11:05:05 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sat, 25 Jul 2015 11:05:05 +0200
Subject: [R] building a quicksort function in rcpp
In-Reply-To: <CAE0RWP-Sv97Loq2SAd0R0medjyfO+HfKoLJu19Uyb-gDUCTNqA@mail.gmail.com>
References: <CAE0RWP-Sv97Loq2SAd0R0medjyfO+HfKoLJu19Uyb-gDUCTNqA@mail.gmail.com>
Message-ID: <55B35141.8090801@statistik.tu-dortmund.de>

Actually sort() is already there ....

Best,
Uwe Ligges



On 24.07.2015 17:22, Martin Tully wrote:
> Hi I am using RCPP to build a C++ function for quicksort called qsort.
> This function is compiled and loaded through the cxxfunction in R
> I am getting the message in R error: no matching function for call to
> 'qsort(int*&)' The code is below.
> It will not run for me and I was wondering if you could help?
>
>
>
> library(Rcpp)
> library(inline)
>
>
>      incl <- 'int qsort(int xx[], int left, int right) {
>
>              int i = left,
>              j = right;
>              int tmp;
>              int pivot = xx[(left + right) / 2];
>
>              /* partition */
>                while (i <= j) {
>                  while (xx[i] < pivot)
>                    i++;
>                  while (xx[j] > pivot)
>                    j--;
>                  if (i <= j) {
>                    tmp = xx[i];
>                    xx[i] = xx[j];
>                    xx[j] = tmp;
>                    i++;
>                    j--;
>                  }
>                }
>
>              /* recursion */
>                if (left < j){
>                  qsort(xx, left, j);
>                }
>              if (i < right){
>                qsort(xx, i, right);
>              }
>
>          return (qsort(xx));
>            }
>            '
>
>            sortCpp <- cxxfunction(signature( x = "integer",left = "integer",
>                                              right = "integer"),
>                                   body = 'IntegerVector arr(x);
>                                           int a = as<int>(left);
>                                           int b = as<int>(right);
>                                          return wrap(qsort(arr));',
>                                   include = incl,
>                                   plugin = "Rcpp",
>                                   verbose = TRUE)
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drjimlemon at gmail.com  Sat Jul 25 12:06:02 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 25 Jul 2015 20:06:02 +1000
Subject: [R] (no subject)
In-Reply-To: <CABNiEmmN9UFWCiYQbkrbC1yhTXzPQ6noQoeD+TbCWjmyFEGvdg@mail.gmail.com>
References: <CABNiEmmN9UFWCiYQbkrbC1yhTXzPQ6noQoeD+TbCWjmyFEGvdg@mail.gmail.com>
Message-ID: <CA+8X3fU0Qq7ZO66qeY0mEwAunbTVOv4usRW-OX2YOZzyRPEftg@mail.gmail.com>

Hi Said,
Try this:

# some data please, maestro
fatdat<-data.frame(age=sample(12:50,100,TRUE),BMI=runif(100,12,40))
# get your graphic device going
png("fatplots.png",width=800,height=800)
# get a 2x2 division of the device space
par(mfrow=c(2,2))
# start plotting
plot(age~BMI,data=fatdat)
hist(fatdat$BMI)
plot(sort(fatdat$BMI),type="l")
fatdat$agecat<-cut(fatdat$age,breaks=c(10,20,30,40,50))
barplot(by(fatdat$BMI,fatdat$agecat,FUN=mean))
dev.off()

Jim


On Fri, Jul 24, 2015 at 10:05 PM, Said Filahi <sa.filahi at gmail.com> wrote:
> hello
>
> i'm trying to plot multiple image (.png) and plot as a single image using
> mfrow().
>
> i have already image1.png  image2.png image.png image4.png and a can't plot
> them in single fig1.png?
>
>
> thank you
>
>
> Said FILAHI
>
> morocco
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Sat Jul 25 13:26:03 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 25 Jul 2015 13:26:03 +0200
Subject: [R] building a quicksort function in rcpp
In-Reply-To: <CAE0RWP-Sv97Loq2SAd0R0medjyfO+HfKoLJu19Uyb-gDUCTNqA@mail.gmail.com>
References: <CAE0RWP-Sv97Loq2SAd0R0medjyfO+HfKoLJu19Uyb-gDUCTNqA@mail.gmail.com>
Message-ID: <10C01F31-66DE-41C0-9D63-9798675A407F@gmail.com>


> On 24 Jul 2015, at 17:22 , Martin Tully <tulls4472 at gmail.com> wrote:
> 
> Hi I am using RCPP to build a C++ function for quicksort called qsort.
> This function is compiled and loaded through the cxxfunction in R
> I am getting the message in R error: no matching function for call to
> 'qsort(int*&)' The code is below.
> It will not run for me and I was wondering if you could help?

I'm too old to  be good at C++, but this looks wrong:

> 
> 
> 
> library(Rcpp)
> library(inline)
> 
> 
>    incl <- 'int qsort(int xx[], int left, int right) {
> 
> ......
>        return (qsort(xx));
>          }
>          '

It looks wrong on two counts: qsort() calls itself with no conditioning, and even if that is intentional, the call doesn't match the definition. Shouldn't it just be return(xx); ?

-pd

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jrkrideau at inbox.com  Sat Jul 25 13:52:48 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sat, 25 Jul 2015 03:52:48 -0800
Subject: [R] apply kendall tau to a split data set
In-Reply-To: <1437736088378-4710303.post@n4.nabble.com>
Message-ID: <4A481CE410F.000001A5jrkrideau@inbox.com>

It helps to have a bit more detail and some sample data. Have a look at Reproducibility
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html
for some ideas of how to pose a question on the list.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: shah_suraj at hotmail.co.uk
> Sent: Fri, 24 Jul 2015 04:08:08 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] apply kendall tau to a split data set
> 
> $`19179222`
>    Unique.ID    Start.Year                    L1.Risk.Category
> Gross.amount.sum
> 17  19179222       2013 Execution, Delivery & Process Management
> 161212.1
> 18  19179222       2015 Execution, Delivery & Process Management
> 110880.0
> 
> $`25182498`
>    Unique.ID   Start.Year                    L1.Risk.Category
> Gross.amount.sum
> 19  25182498       2014   Clients, Products & Business Practices
> 59384
> 20  25182498       2014 Execution, Delivery & Process Management
> 355000
> 21  25182498       2015 Execution, Delivery & Process Management
> 27720
> 
> $`32506027`
>    Unique.ID    Start.Year                L1.Risk.Category
> Gross.amount.sum
> 22  32506027       2003 Execution, Delivery & Process Management
> 3600000.0
> 23  32506027       2013 Execution, Delivery & Process Management
> 161212.1
> 24  32506027       2014   Clients, Products & Business Practices
> 14846.0
> 
> 
> Hi, I have split a data frame and output is as above.
> 
> I would like to apply Kendall's tau to calculate correlations. For
> example,
> for the last Unique.ID 32506027, I want to create a time series for each
> L1
> risk category, and then calculate the correlation between each L1 risk
> category using the Kendall tau approach.
> 
> Any help would be much appreciated.
> 
> Thanks
> s
> 
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/apply-kendall-tau-to-a-split-data-set-tp4710303.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From cecilia.larrosa10 at imperial.ac.uk  Sat Jul 25 11:54:00 2015
From: cecilia.larrosa10 at imperial.ac.uk (Larrosa, Cecilia)
Date: Sat, 25 Jul 2015 09:54:00 +0000
Subject: [R] Writing output of a looped process with pdfs
Message-ID: <B71DF3B3-5DAD-4F06-B50A-AB7FBB55F706@imperial.ac.uk>

Hi,

I have created a list of spdfs, and I am looping a process for each of them. The process is using dDistance to calculate distances between features within each spdf. I want to write the output tables using the name of the spdf at each loop, but cannot find a way to do this. It seems a rather basic thing to do, and I am not very proficient in R, but I have spent several hours looking for a way to do this and failed. I am using R studio on a Mac.

Here is the code so far (not the most efficient code):

# Load libraries
library(rgdal)
library(gdistance)

#Read forest shape files
setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/split_fnp/")
shps<- dir(getwd(), "*.shp")
shps <- gsub('.{4}$', '', shps)
for (shp in shps) assign(shp, readOGR(".",layer=shp))

#Create list (I did this manually because I could not find another way)
fnps <- list(a_1, a_10, a_100, a_101, a_102, a_103, a_104, a_105, a_106, a_107, a_108, a_109, a_11, a_110,
             a_111, a_112, a_113, a_12, a_13, a_14, a_15, a_16, a_17, a_18, a_19, a_2, a_20, a_21, a_22, a_23,
             a_24, a_25,  a_26, a_27,  a_28, a_29, a_3, a_30, a_31, a_32, a_33, a_34, a_35, a_36, a_37,
             a_38, a_39, a_4, a_42, a_43, a_44, a_45, a_46, a_47, a_48, a_49, a_5, a_50, a_51, a_52,
             a_53, a_54, a_55,  a_56, a_57, a_6,  a_69, a_7, a_70,  a_73, a_79, a_8,  a_80, a_81, a_82,
             a_83,  a_84, a_85, a_86, a_87, a_88, a_89, a_9,  a_90, a_91, a_94, a_95, a_96, a_98, a_99)


###  Calculate distance between all polygons
for (fnp in fnps)
{
  distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
  row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);# did this because gDistance changed the IDs of the features from [1 to ...] to [0 to ...], not sure why
  colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?); # same as above
  dists.melt <- melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,]; #use only lower triangle of the distances matrix
  outfile <- file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/", paste0("distances_", fnp, ".txt")); # this is the bit that is not working
  write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
}

And this is the error message:

Error in as.character.default(<S4 object of class "SpatialPolygonsDataFrame">) :
  no method for coercing this S4 class to a vector

Can anyone help me with solving the issue? How to call the name of the looped spdf to be included in the title of the output table? I really appreciate your time!

Cheers
Cecilia


	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Sat Jul 25 15:01:25 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sat, 25 Jul 2015 05:01:25 -0800
Subject: [R] Writing output of a looped process with pdfs
In-Reply-To: <B71DF3B3-5DAD-4F06-B50A-AB7FBB55F706@imperial.ac.uk>
Message-ID: <4AE178D4A90.0000020Fjrkrideau@inbox.com>

Hi Cecilia,
I am of no help with your actual problem but here are a few suggestions to help improve your question. 
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html

I see you say that you created fnps list by hand. There is a much easier way to do this. You can use the dput() command to provide us with an identical copy of the data as it appears on your machine. Just do dput(fnps) and copy the output into your email.  dput() is discussed in the links above or see ?dput. Actually we probably would like to see the shps in its form as soon as you load it.  

Sorry not to have been of any substantive help.


By the way, the problem  is very well stated but a major issue, at least to me, is the "list" you supply. I may just be obtuse but I don't see how you can read it in as a list() nor do I understand why. It may be a misunderstanding of data structures in R (which are great once you get your mind wrapped around them)

I can read in fnps if I treat it as a character vector. What I have done here is use the Hmisc package's function Cs to read in a character vector without having to have quotes "" around each item. It is the equivalent of fnps  <-  c("a_1", "a_10" ... )
library(Hmisc)
fnps <- Cs(a_1, a_10, a_100, a_101, a_102, a_103, a_104, a_105, a_106, a_107, a_108, a_109, a_11, a_110,   a_111, a_112, a_113, a_12, a_13, a_14, a_15, a_16, a_17, a_18, a_19, a_2, a_20, a_21, a_22, a_23,   a_24, a_25,  a_26, a_27,  a_28, a_29, a_3, a_30, a_31, a_32, a_33, a_34, a_35, a_36, a_37,  a_38, a_39, a_4, a_42, a_43, a_44, a_45, a_46, a_47, a_48, a_49, a_5, a_50, a_51, a_52,   a_53, a_54, a_55,  a_56, a_57, a_6,  a_69, a_7, a_70,  a_73, a_79, a_8,
a_80, a_81, a_82, a_83,  a_84, a_85, a_86, a_87, a_88, a_89, a_9,  a_90, a_91, a_94, a_95, a_96, a_98, a_99)

John Kane
Kingston ON Canada


> -----Original Message-----
> From: cecilia.larrosa10 at imperial.ac.uk
> Sent: Sat, 25 Jul 2015 09:54:00 +0000
> To: r-help at r-project.org
> Subject: [R] Writing output of a looped process with pdfs
> 
> Hi,
> 
> I have created a list of spdfs, and I am looping a process for each of
> them. The process is using dDistance to calculate distances between
> features within each spdf. I want to write the output tables using the
> name of the spdf at each loop, but cannot find a way to do this. It seems
> a rather basic thing to do, and I am not very proficient in R, but I have
> spent several hours looking for a way to do this and failed. I am using R
> studio on a Mac.
> 
> Here is the code so far (not the most efficient code):
> 
> # Load libraries
> library(rgdal)
> library(gdistance)
> 
> #Read forest shape files
> setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/split_fnp/")
> shps<- dir(getwd(), "*.shp")
> shps <- gsub('.{4}$', '', shps)
> for (shp in shps) assign(shp, readOGR(".",layer=shp))
> 
> #Create list (I did this manually because I could not find another way)
> fnps <- list(a_1, a_10, a_100, a_101, a_102, a_103, a_104, a_105, a_106,
> a_107, a_108, a_109, a_11, a_110,
>              a_111, a_112, a_113, a_12, a_13, a_14, a_15, a_16, a_17,
> a_18, a_19, a_2, a_20, a_21, a_22, a_23,
>              a_24, a_25,  a_26, a_27,  a_28, a_29, a_3, a_30, a_31, a_32,
> a_33, a_34, a_35, a_36, a_37,
>              a_38, a_39, a_4, a_42, a_43, a_44, a_45, a_46, a_47, a_48,
> a_49, a_5, a_50, a_51, a_52,
>              a_53, a_54, a_55,  a_56, a_57, a_6,  a_69, a_7, a_70,  a_73,
> a_79, a_8,  a_80, a_81, a_82,
>              a_83,  a_84, a_85, a_86, a_87, a_88, a_89, a_9,  a_90, a_91,
> a_94, a_95, a_96, a_98, a_99)
> 
> 
> ###  Calculate distance between all polygons
> for (fnp in fnps)
> {
>   distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
>   row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);#
> did this because gDistance changed the IDs of the features from [1 to
> ...] to [0 to ...], not sure why
>   colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?); #
> same as above
>   dists.melt <-
> melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,]; #use only
> lower triangle of the distances matrix
>   outfile <-
file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/",
> paste0("distances_", fnp, ".txt")); # this is the bit that is not working
>   write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
> }
> 
> And this is the error message:
> 
> Error in as.character.default(<S4 object of class
> "SpatialPolygonsDataFrame">) :
>   no method for coercing this S4 class to a vector
> 
> Can anyone help me with solving the issue? How to call the name of the
> looped spdf to be included in the title of the output table? I really
> appreciate your time!
> 
> Cheers
> Cecilia
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From pmaclean2011 at yahoo.com  Sat Jul 25 21:30:09 2015
From: pmaclean2011 at yahoo.com (Peter Maclean)
Date: Sat, 25 Jul 2015 19:30:09 +0000 (UTC)
Subject: [R] Eye Diagram from a Topic Model
Message-ID: <544514760.2313693.1437852609350.JavaMail.yahoo@mail.yahoo.com>

Is it possible to reproduce the eye-diagram described here from the topic model/
https://github.com/ouzor/eyediagram/blob/master/example.R 


and shown here:
https://github.com/ouzor/eyediagram/blob/master/example/ExampleEyeDiagram.pdf

The code do not work in processing as suggested by the author and the outhor seems 
not to be interested in developing the code further. Here is a toy
topic model using yahoo headline news to start with. 
###############################################################
library(XML)
source <- "http://finance.yahoo.com/q/h?s=AAPL+Headlines"
d <- htmlParse(source)
text <- as.data.frame(xpathSApply(d, "//ul[contains(@class,'newsheadlines')]/following::ul/li/a", xmlValue))
colnames(text) <- c("topics")
fix(text)
library(stm)
#Stemming/stopword removal
textss <- textProcessor(text$topics, metadata=text)
#str(textss)
model <- stm(textss$documents, textss$vocab, K=10, data=textss$meta, seed=5678)
labelTopics(model)
plot.STM(model, text.cex=0.8, type='summary', xlim=c(0, 0.5), n=4)
#How can we create an eye-diagram with document-topic-words


From lid.zigh at gmail.com  Sat Jul 25 22:03:21 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Sat, 25 Jul 2015 15:03:21 -0500
Subject: [R] Reading some csv files from different folders and add the name
 of each files to the first column of files
In-Reply-To: <CAMqbV1AoHGRSa_EHH5_oqVWvoKn=6R8y8Xg9P5HzpcpTe=PCmA@mail.gmail.com>
References: <CAMqbV1D1AhwjhWo-8iUEYBTd8iBezPqPO5SMyeonXiD2_Mv6ug@mail.gmail.com>
	<CAMqbV1CDTEJcQjaFJ59n7VYORPfdDVXxSWsFPcnoJhX5a-F+LQ@mail.gmail.com>
	<CAMqbV1AxbxQEVJAuOq6fFXzGs2b+Umho9Y3T4TajikcNQxy_eA@mail.gmail.com>
	<CAMqbV1Cygs7qJV+7CH8CEdHy_soUeUCoj4jGenJF2uKqn4Gwww@mail.gmail.com>
	<CAMqbV1A1wWG0jQL1e=XQQOMKvivvvtBb3goaM2ObeTfbVZ4ahQ@mail.gmail.com>
	<CAMqbV1BOv6CrZhJ2Doya5fFnkJq0bzcWYNHVM1YJVG-4jUG+qQ@mail.gmail.com>
	<CAMqbV1BkBeqNq+4LwuKHKbD-ds6dRs6a2rNAu48M7gWTgpDFyw@mail.gmail.com>
	<CAMqbV1DXk=NC5=V7cXTzHADsaChEKFFGmyDDGkXTb7jUo-GPsg@mail.gmail.com>
	<CAMqbV1AxNoGw0YLJR9V6V-dE8RcCv1yKK4YSUPGBdd7_W5P-Ww@mail.gmail.com>
	<CAMqbV1DoF5ffnN6AE8rruoqPKFgVvBbu_RyBdNipwiU+_c_0fQ@mail.gmail.com>
	<CAMqbV1AoHGRSa_EHH5_oqVWvoKn=6R8y8Xg9P5HzpcpTe=PCmA@mail.gmail.com>
Message-ID: <CAMqbV1CXN0=R+HxmaYS40sMymGdbhDQK1V+n5bHmBPgBr3M2Yg@mail.gmail.com>

I have 600 folders in which there are 3 csv files. The name of folders are
as follows: EA_aa, EA_bb, EA_cc, EA_dd,....

In each folder there are 3 csv files:
in folder EA_aa there are:
  EA_sing_aa.csvqwerty
  EA_ska_aa.csv
  EA_tat_aa.csv

In folder EA_bb:
  EA_sing_bb.csv
  EA_ska_bb.csv
  EA_tat_bb.csv
...

I need to read all the same kind of files (for example sing files) in one
dataframe but before that I should add a column to each files with the name
of file as a row names!

So in output I should have just 3 csv files such as
EA_sing.csv: (rbind all EA_sing_*.csv file)
EA_ska.csv: (rbind all EA_ska_*.csv files)
EA_tat.csv: (rbind all EA_tat_*.csv files)

And in each file the first column should be added as row names which
containing the file name! So the format will be as follow:
EA_sing.csv:
1st column   2nd column  3rd column
   aa                    yhhh                ghj
   aa                     k ki                   Fyh
   bb                     k ki                   vgd
   bb                     k gki                  Fyh
   bb                    k reci                  Fyh
   cc                     k hcd                  hyd
   dd                     lmb                     Fyh

EA_ska.csv:
1st column   2nd column  3rd column
   aa                    yhhh                ghj
   aa                     k ki                   Fyh
   bb                     k ki                   Fyh
   cc                     k gki                  Fyh
   cc                     k reci                  oki
   cc                     k hcd                  Fyh
   dd                     lmb                     dsf

EA_tat.csv:
1st column   2nd column  3rd column
   aa                    yhhh                ghj
   aa                     k ki                   Fyh
   bb                     k ki                   Fyh
   cc                     k gki                  Fyh
   cc                     k reci                  oki
   dd                     k hcd                  Fyh
   dd                     lmb                     dsf

Would you please help me how to can I do that?
Thanks

	[[alternative HTML version deleted]]


From attenka at utu.fi  Sat Jul 25 21:49:55 2015
From: attenka at utu.fi (Atte Tenkanen)
Date: Sat, 25 Jul 2015 22:49:55 +0300
Subject: [R] Opposite color in R
Message-ID: <55B3E863.2010208@utu.fi>

Hi,

I have tried to find a way to find opposite or complementary colors in R.

I would like to form a color circle with R like this one: 
http://nobetty.net/dandls/colorwheel/complementary_colors.jpg

If you just make a basic color wheel in R, the colors do not form 
complementary color circle:

palette(rainbow(24))
Colors=palette()
pie(rep(1, 24), col = Colors)

There is a package ?colortools? where you can find function opposite(), 
but it doesn?t work as is said. I tried

library(colortools)
opposite("violet") and got green instead of yellow and

opposite("blue") and got yellow instead of orange.

Do you know any solutions?

Atte Tenkanen


From pdalgd at gmail.com  Sun Jul 26 02:04:04 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 26 Jul 2015 02:04:04 +0200
Subject: [R] Opposite color in R
In-Reply-To: <55B3E863.2010208@utu.fi>
References: <55B3E863.2010208@utu.fi>
Message-ID: <0567E180-AFE9-4F70-9753-736963486025@gmail.com>


> On 25 Jul 2015, at 21:49 , Atte Tenkanen <attenka at utu.fi> wrote:
> 
> Hi,
> 
> I have tried to find a way to find opposite or complementary colors in R.
> 
> I would like to form a color circle with R like this one: http://nobetty.net/dandls/colorwheel/complementary_colors.jpg
> 
> If you just make a basic color wheel in R, the colors do not form complementary color circle:
> 
> palette(rainbow(24))
> Colors=palette()
> pie(rep(1, 24), col = Colors)
> 
> There is a package ?colortools? where you can find function opposite(), but it doesn?t work as is said. I tried
> 
> library(colortools)
> opposite("violet") and got green instead of yellow and
> 
> opposite("blue") and got yellow instead of orange.
> 
> Do you know any solutions?

Not directly, but a few hints: 

First read up on "complementary colors" in Wikipedia. In particular, note that the traditional color circle does not satisfy the modern definition of opposite-ness. E.g. red paint mixed with green paint is brown, not black or grey.

The construction of the color circle is simple in principle: red, blue, yellow go at 0, 120, 240 degrees, the other colors on the circle are formed by mixing two primaries in varying proportions: green (at 180 deg) is an equal mixture of blue and yellow, violet (at 60 deg) of blue and red, orange (at 300 deg) of red and yellow. Blue-green (at 150 deg) would be half blue, half green, alias three quarter blue, one quarter yellow. Etc.

The tricky bit is that the above mixtures are subtractive mixtures (mixing paint rather than light beams) and I don't know how to make a subtractive color mixture in the additive RGB space that we usually work in. Maybe there are tools in the colortools package?

-pd

> 
> Atte Tenkanen
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Ray.Brownrigg at ecs.vuw.ac.nz  Sun Jul 26 02:42:58 2015
From: Ray.Brownrigg at ecs.vuw.ac.nz (Ray Brownrigg)
Date: Sun, 26 Jul 2015 12:42:58 +1200
Subject: [R] Labeling world map
In-Reply-To: <1437723477804-4710296.post@n4.nabble.com>
References: <1437723477804-4710296.post@n4.nabble.com>
Message-ID: <55B42D12.5050904@ecs.vuw.ac.nz>



On 24/07/2015 7:37 p.m., sreenath wrote:
> I draw world map using
> library(maptools)
>> library(ggmap)
>> library(mapdata)
>> library(maps)
>> map("world",fill=TRUE,col="White",bg="light
> blue",ylim=c(-60,90),mar=c(0,0,0,0))
>> native <- c("brazil","sao paulo state")
>> nat <-geocode(native)
>> nat.x <- nat$lon
>> nat.y <- nat$lat
>> points(nat.x,nat.y,col="yellow",pch=16)
> How can i put lilte for this map and label points
title("This is a title")
text(nat.x, nat.y, c("A", "B"), col=2)

If this isn't what you want then you need to be more specific.

Ray Brownrigg


From glennmschultz at me.com  Sun Jul 26 00:52:07 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Sat, 25 Jul 2015 22:52:07 +0000 (GMT)
Subject: [R] ggplot percent format for interest rates
Message-ID: <7254487a-deb6-4eda-9ee1-52ae6a3290d1@me.com>

Hello All,

The data is as follows below with ggplot command. ?I would like there to be two digits to the right of the decimal on the y axis - this would be consistent with plotting interest rates. ?I have not had much luck. ?Any ideas are appreciated

-glenn


structure(list(Tenor = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,?
12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,?
28, 29, 30), Rate = c(0.226560966039639, 0.34, 0.44, 0.58, 0.79,?
1.05, 1.30483402347606, 1.54, 1.7472935509309, 1.92899362434552,?
2.09, 2.23452744367452, 2.36405066524527, 2.47935936061244, 2.58124322567624,?
2.67049195633686, 2.7478952484945, 2.81424279804937, 2.87032430090165,?
2.91692945295155, 2.95484795009927, 2.984869488245, 3.00778376328894,?
3.0243804711313, 3.03544930767227, 3.04177996881204, 3.04416215045082,?
3.04338554848881, 3.04023985882621, 3.0355147773632, 3.03)), .Names = c("Tenor",?
"Rate"), row.names = c(NA, -31L), class = "data.frame")

ggplot(SwapCurve, aes(x = Tenor, y= Rate/100, colour = "#E69F00")) +
? geom_line() +
? geom_line(size = 1.5) +
? ylab("Fixed Rate Payer Side") +
? xlab("Maturity (years)") +
? theme_minimal()+
? theme(panel.grid.major = element_line(size = .25, color = "grey")) +
? scale_colour_manual(values = cbbPalette, guide = FALSE) +
? #scale_y_continuous(labels = percent)+
? theme(axis.text = element_text(size = 15)) +
? theme(axis.title = element_text(size = 20)) +
? theme(legend.text=element_text(size= 15))

From ken.knoblauch at inserm.fr  Sun Jul 26 08:45:34 2015
From: ken.knoblauch at inserm.fr (ken knoblauch)
Date: Sun, 26 Jul 2015 06:45:34 +0000
Subject: [R] Opposite color in R
References: <55B3E863.2010208@utu.fi>
	<0567E180-AFE9-4F70-9753-736963486025@gmail.com>
Message-ID: <loom.20150726T083402-956@post.gmane.org>

peter dalgaard <pdalgd <at> gmail.com> writes:

> 
> 
> > On 25 Jul 2015, at 21:49 , Atte Tenkanen 
<attenka <at> utu.fi> wrote:
> > 
> > Hi,
> > 
> > I have tried to find a way to find opposite 
or complementary colors in R.
> > 
> > I would like to form a color circle with R 
like this one: http://nobetty.net/dandls/
colorwheel/complementary_colors.jpg
> > 
> > If you just make a basic color wheel in R, 
the colors do not form complementary color circle:
> > 
> > palette(rainbow(24))
> > Colors=palette()
> > pie(rep(1, 24), col = Colors)
> > 
> > There is a package ?colortools? where 
you can find function opposite(), but it doesn?t work as is
> said. I tried
> > 
> > library(colortools)
> > opposite("violet") and got green instead of yellow and
> > 
> > opposite("blue") and got yellow instead of orange.
> > 
> > Do you know any solutions?
> 
> Not directly, but a few hints: 
> 
> First read up on "complementary colors" in
 Wikipedia. In particular, note that the traditional color
> circle does not satisfy the modern definition 
of opposite-ness. E.g. red paint mixed with green paint is
> brown, not black or grey.
> 
> The construction of the color circle is simple
 in principle: red, blue, yellow go at 0, 120, 240 degrees, the
> other colors on the circle are formed by mixing
 two primaries in varying proportions: green (at 180 deg) is
> an equal mixture of blue and yellow, violet 
(at 60 deg) of blue and red, orange (at 300 deg) 
of red and yellow.
> Blue-green (at 150 deg) would be half blue, 
half green, alias three quarter blue, one quarter
 yellow. Etc.
> 
> The tricky bit is that the above mixtures are 
subtractive mixtures (mixing paint rather than light beams)
> and I don't know how to make a subtractive 
color mixture in the additive RGB space 
that we usually work in.
> Maybe there are tools in the colortools package?
> 
> -pd
> 
> > 
> > Atte Tenkanen

To start with, you should be specifying your "colors"
or lights actually in an additive color space like
CIE 1931 xy,
https://en.wikipedia.org/wiki/CIE_1931_color_space
which you can do in the colorspace package.
But this is based on an average observer and
the results are unlikely to match a given 
individual's vision.  On top of that, decisions made
when this norm was specified are such that it
deviates from human vision for short wavelengths
so that you would be better off using a corrected
version like that proposed by Judd in the 1950's
or for the most recent suggestion see
ww.cvrl.org
under 
New CIE XYZ functions transformed 
from the CIE (2006) LMS functions

best, 

Ken

-- 
Kenneth Knoblauch
Inserm U846
Stem-cell and Brain Research Institute
Department of Integrative Neurosciences
18 avenue du Doyen L?pine
69500 Bron
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: +33 (0)6 84 10 64 10
http://www.sbri.fr/members/kenneth-knoblauch.html

From drjimlemon at gmail.com  Sun Jul 26 13:20:33 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 26 Jul 2015 21:20:33 +1000
Subject: [R] Opposite color in R
In-Reply-To: <loom.20150726T083402-956@post.gmane.org>
References: <55B3E863.2010208@utu.fi>
	<0567E180-AFE9-4F70-9753-736963486025@gmail.com>
	<loom.20150726T083402-956@post.gmane.org>
Message-ID: <CA+8X3fW5a+bFOBi4+eQoWA-BvMO7iP1Lj96fe7UggOpqpTmhhQ@mail.gmail.com>

Hi Atte,
If you look at the colors produced by rainbow(12):

rainbow(12)
 [1] "#FF0000FF" "#FF8000FF" "#FFFF00FF" "#80FF00FF" "#00FF00FF" "#00FF80FF"
 [7] "#00FFFFFF" "#0080FFFF" "#0000FFFF" "#8000FFFF" "#FF00FFFF" "#FF0080FF"

they are complementary additive colors. That is, in the RGB color
space, the colors at the opposite sides of the wheel would add to
white (#FFFFFF) if mixed. The colors in the diagram you mentioned
don't look like additive colors. Perhaps that diagram represents a
subtractive (i.e. pigment) color space but based on the additive (red,
green blue) primaries. Also remember that WYSINNWOPG (what you see is
not necessarily what other people get)

Jim


On Sun, Jul 26, 2015 at 4:45 PM, ken knoblauch <ken.knoblauch at inserm.fr> wrote:
> peter dalgaard <pdalgd <at> gmail.com> writes:
>
>>
>>
>> > On 25 Jul 2015, at 21:49 , Atte Tenkanen
> <attenka <at> utu.fi> wrote:
>> >
>> > Hi,
>> >
>> > I have tried to find a way to find opposite
> or complementary colors in R.
>> >
>> > I would like to form a color circle with R
> like this one: http://nobetty.net/dandls/
> colorwheel/complementary_colors.jpg
>> >
>> > If you just make a basic color wheel in R,
> the colors do not form complementary color circle:
>> >
>> > palette(rainbow(24))
>> > Colors=palette()
>> > pie(rep(1, 24), col = Colors)
>> >
>> > There is a package ?colortools? where
> you can find function opposite(), but it doesn?t work as is
>> said. I tried
>> >
>> > library(colortools)
>> > opposite("violet") and got green instead of yellow and
>> >
>> > opposite("blue") and got yellow instead of orange.
>> >
>> > Do you know any solutions?
>>
>> Not directly, but a few hints:
>>
>> First read up on "complementary colors" in
>  Wikipedia. In particular, note that the traditional color
>> circle does not satisfy the modern definition
> of opposite-ness. E.g. red paint mixed with green paint is
>> brown, not black or grey.
>>
>> The construction of the color circle is simple
>  in principle: red, blue, yellow go at 0, 120, 240 degrees, the
>> other colors on the circle are formed by mixing
>  two primaries in varying proportions: green (at 180 deg) is
>> an equal mixture of blue and yellow, violet
> (at 60 deg) of blue and red, orange (at 300 deg)
> of red and yellow.
>> Blue-green (at 150 deg) would be half blue,
> half green, alias three quarter blue, one quarter
>  yellow. Etc.
>>
>> The tricky bit is that the above mixtures are
> subtractive mixtures (mixing paint rather than light beams)
>> and I don't know how to make a subtractive
> color mixture in the additive RGB space
> that we usually work in.
>> Maybe there are tools in the colortools package?
>>
>> -pd
>>
>> >
>> > Atte Tenkanen
>
> To start with, you should be specifying your "colors"
> or lights actually in an additive color space like
> CIE 1931 xy,
> https://en.wikipedia.org/wiki/CIE_1931_color_space
> which you can do in the colorspace package.
> But this is based on an average observer and
> the results are unlikely to match a given
> individual's vision.  On top of that, decisions made
> when this norm was specified are such that it
> deviates from human vision for short wavelengths
> so that you would be better off using a corrected
> version like that proposed by Judd in the 1950's
> or for the most recent suggestion see
> ww.cvrl.org
> under
> New CIE XYZ functions transformed
> from the CIE (2006) LMS functions
>
> best,
>
> Ken
>
> --
> Kenneth Knoblauch
> Inserm U846
> Stem-cell and Brain Research Institute
> Department of Integrative Neurosciences
> 18 avenue du Doyen L?pine
> 69500 Bron
> France
> tel: +33 (0)4 72 91 34 77
> fax: +33 (0)4 72 91 34 61
> portable: +33 (0)6 84 10 64 10
> http://www.sbri.fr/members/kenneth-knoblauch.html
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rbaer at atsu.edu  Sun Jul 26 13:36:55 2015
From: rbaer at atsu.edu (Robert Baer)
Date: Sun, 26 Jul 2015 06:36:55 -0500
Subject: [R] R GUI plot by color
In-Reply-To: <CA+8X3fXaC2hhVp7zD2Wc+NEHP0cfKXdE=fSYa_jFibxQKLbXFw@mail.gmail.com>
References: <1437727381033-4710297.post@n4.nabble.com>
	<CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>
	<1437731620874-4710300.post@n4.nabble.com>
	<CA+8X3fXaC2hhVp7zD2Wc+NEHP0cfKXdE=fSYa_jFibxQKLbXFw@mail.gmail.com>
Message-ID: <55B4C657.5000808@atsu.edu>



On 7/24/2015 6:23 AM, Jim Lemon wrote:
> Hi jpara3,
> Your example, when I got it to go:
>
> one<-c(3,2,2)
> two<-c("a","b","b")
> data<-dataframe(one,two)
> plot(data$one,col=data$two)
Wow Jim. Psychic indeed!  Not only did you answer with NO reproducible 
example, but on round 2 you fixed a non-working example and explained 
why it was an accident that it works.  What is the stock market about to 
do? :)

jpara3 - Those of us without Jim's talent can be more helpful if you 
read and follow the guide at the bottom of each email.:

PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




> does indeed work, and I'll explain how. You are plotting the values of
> data$one against the _values_ of data$two (see point 3 of my
> response). In this case, the values of data$two are of class "factor",
> which means that they have numeric values attached to the levels (a,
> b) of the factor. When you pass these values as the "col" argument,
> they are silently converted to their numeric values (1,2,2). In the
> default palette, these numbers represent the colors - black, red, red.
> Those are the colors in which the points are plotted. So far, so good.
> Let's look at the other two points that I guessed.
>
> 1) The column names of data2 are not numbers
>
> colnames(data)
> [1] "one" "two"
>
> As you can see, the column names are character variables, and they
> don't translate to numbers:
>
> as.numeric(colnames(data))
> [1] NA NA
>
> 2) The number of columns in data2 is not equal to the number of values
> in data1 that you are plotting
>
> It's pretty obvious that there are two values in the column names and
> three in the vector of values that you are plotting in your
> example.So, I think I got three out of three without knowing what the
> data were.
>
> Jim
>
>
> On Fri, Jul 24, 2015 at 7:53 PM, jpara3 <j.para.fernandez at hotmail.com> wrote:
>> I have done a trial with a dataframe like this:
>> one<-c(3,2,2)
>> two<-c(a,b,b)
>> data<-dataframe(uno,dos)
>>
>> plot(data$one,col=data$two)
>>
>> and it plots perfect.
If you paste the code above in R, it has errors and does NOT plot 
perfectly.   I still did not understand what you were trying to do. You 
owe Jim big time.

>> If I try it with the code that i have post in the first message, selecting
>> data1 and data2 as i nthis example, the plot is plotted, but all dots with
>> the same color.
>>
>> Thanks for the answer but noone of the 3 topics is the root problem.
>>
>>
>>
>>
>>
>> --
>> View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297p4710300.html
>> Sent from the R help mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Sun Jul 26 14:26:07 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sun, 26 Jul 2015 04:26:07 -0800
Subject: [R] ggplot percent format for interest rates
In-Reply-To: <7254487a-deb6-4eda-9ee1-52ae6a3290d1@me.com>
Message-ID: <57253C4A36E.0000010Bjrkrideau@inbox.com>

Hi Glenn,
I don't understand what you are after. If I run your code I get two figures to the right of the decimal point.  Can you give us an example of the layout you want? Clearly I am missing the point. 

Note I removed the palate command from the code,

ggplot(SwapCurve, aes(x = Tenor, y= Rate/100, colour = "#E69F00")) +
  geom_line() +
  geom_line(size = 1.5) +
  ylab("Fixed Rate Payer Side") +
  xlab("Maturity (years)") +
  theme_minimal()+
  theme(panel.grid.major = element_line(size = .25, color = "grey")) +
  theme(axis.text = element_text(size = 15)) +
  theme(axis.title = element_text(size = 20)) +
  theme(legend.text=element_text(size= 15))




John Kane
Kingston ON Canada


> -----Original Message-----
> From: glennmschultz at me.com
> Sent: Sat, 25 Jul 2015 22:52:07 +0000 (GMT)
> To: r-help at r-project.org
> Subject: [R] ggplot percent format for interest rates
> 
> Hello All,
> 
> The data is as follows below with ggplot command. ?I would like there to
> be two digits to the right of the decimal on the y axis - this would be
> consistent with plotting interest rates. ?I have not had much luck. ?Any
> ideas are appreciated
> 
> -glenn
> 
> 
> structure(list(Tenor = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
> 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,
> 28, 29, 30), Rate = c(0.226560966039639, 0.34, 0.44, 0.58, 0.79,
> 1.05, 1.30483402347606, 1.54, 1.7472935509309, 1.92899362434552,
> 2.09, 2.23452744367452, 2.36405066524527, 2.47935936061244,
> 2.58124322567624,
> 2.67049195633686, 2.7478952484945, 2.81424279804937, 2.87032430090165,
> 2.91692945295155, 2.95484795009927, 2.984869488245, 3.00778376328894,
> 3.0243804711313, 3.03544930767227, 3.04177996881204, 3.04416215045082,
> 3.04338554848881, 3.04023985882621, 3.0355147773632, 3.03)), .Names =
> c("Tenor",
> "Rate"), row.names = c(NA, -31L), class = "data.frame")
> 
> ggplot(SwapCurve, aes(x = Tenor, y= Rate/100, colour = "#E69F00")) +
> ? geom_line() +
> ? geom_line(size = 1.5) +
> ? ylab("Fixed Rate Payer Side") +
> ? xlab("Maturity (years)") +
> ? theme_minimal()+
> ? theme(panel.grid.major = element_line(size = .25, color = "grey")) +
> ? scale_colour_manual(values = cbbPalette, guide = FALSE) +
> ? #scale_y_continuous(labels = percent)+
> ? theme(axis.text = element_text(size = 15)) +
> ? theme(axis.title = element_text(size = 20)) +
> ? theme(legend.text=element_text(size= 15))
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From admin.dslcomputer at gmail.com  Sun Jul 26 10:38:24 2015
From: admin.dslcomputer at gmail.com (admin.dslcomputer at gmail.com)
Date: Sun, 26 Jul 2015 08:38:24 +0000
Subject: [R] =?utf-8?q?Compute_z?=
Message-ID: <55b49d00.4676460a.5615.ffffdc34@mx.google.com>

Hi Everyone:


How do I correctly compute z? 



z = 0;
for i = 1:7
  z = z + v(i) * w(i)
end


If there are two column vectors v and w, each with 7 elements (i.e., they have dimensions 7x1). 


Regards,

Hal






Sent from Surface
	[[alternative HTML version deleted]]


From attenka at utu.fi  Sun Jul 26 05:30:27 2015
From: attenka at utu.fi (Atte Tenkanen)
Date: Sun, 26 Jul 2015 06:30:27 +0300
Subject: [R] Opposite color in R
Message-ID: <55B45453.8010408@utu.fi>

Thanks,

This helps me going on.

The description in "complementary {colortools}" was/is somehow misleading:
"Complementary or opposite color scheme is formed by colors that are opposite each other on the color wheel (example: red and green)."

Atte T.

>/  On 25 Jul 2015, at 21:49 , Atte Tenkanen <attenka at utu.fi  <https://stat.ethz.ch/mailman/listinfo/r-help>> wrote:
/>/  
/>/  Hi,
/>/  
/>/  I have tried to find a way to find opposite or complementary colors in R.
/>/  
/>/  I would like to form a color circle with R like this one:http://nobetty.net/dandls/colorwheel/complementary_colors.jpg
/>/  
/>/  If you just make a basic color wheel in R, the colors do not form complementary color circle:
/>/  
/>/  palette(rainbow(24))
/>/  Colors=palette()
/>/  pie(rep(1, 24), col = Colors)
/>/  
/>/  There is a package ?colortools? where you can find function opposite(), but it doesn?t work as is said. I tried
/>/  
/>/  library(colortools)
/>/  opposite("violet") and got green instead of yellow and
/>/  
/>/  opposite("blue") and got yellow instead of orange.
/>/  
/>/  Do you know any solutions?
/
Not directly, but a few hints:

First read up on "complementary colors" in Wikipedia. In particular, note that the traditional color circle does not satisfy the modern definition of opposite-ness. E.g. red paint mixed with green paint is brown, not black or grey.

The construction of the color circle is simple in principle: red, blue, yellow go at 0, 120, 240 degrees, the other colors on the circle are formed by mixing two primaries in varying proportions: green (at 180 deg) is an equal mixture of blue and yellow, violet (at 60 deg) of blue and red, orange (at 300 deg) of red and yellow. Blue-green (at 150 deg) would be half blue, half green, alias three quarter blue, one quarter yellow. Etc.

The tricky bit is that the above mixtures are subtractive mixtures (mixing paint rather than light beams) and I don't know how to make a subtractive color mixture in the additive RGB space that we usually work in. Maybe there are tools in the colortools package?

-pd

>/  
/>/  Atte Tenkanen
/>/  
/>/  ______________________________________________
/>/  R-help at r-project.org  <https://stat.ethz.ch/mailman/listinfo/r-help>  mailing list -- To UNSUBSCRIBE and more, see
/>/  https://stat.ethz.ch/mailman/listinfo/r-help
/>/  PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
/>/  and provide commented, minimal, self-contained, reproducible code.
/
-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email:pd.mes at cbs.dk  <https://stat.ethz.ch/mailman/listinfo/r-help>   Priv:PDalgd at gmail.com  <https://stat.ethz.ch/mailman/listinfo/r-help>


	[[alternative HTML version deleted]]


From yangwenyue900780 at 163.com  Sun Jul 26 07:43:03 2015
From: yangwenyue900780 at 163.com (wenyueyang)
Date: Sat, 25 Jul 2015 22:43:03 -0700 (PDT)
Subject: [R] Why I get the massage of "Error in impacts.sarlm(s.lag,
 mat2listw(swmmat)) : Only row-standardised weights supported"
Message-ID: <1437889383825-4710356.post@n4.nabble.com>

Hi,all

I am using the spdep-package to estimate the SAR(or called SLM) and SDM in
R. I can get the estimation results of SAR and SDM with the command of
"lagsarlm". But when I perform the "impact" command to calculate the direct
effect, indirect effect and total effect, it feedback the following
sentences:"Error in impacts.sarlm(s.lag, mat2listw(swmmat)) : Only
row-standardised weights supported".
I used the same matrix, whcih I built in ArcGIS 10.1 based on the principle
of "inversed distance" with row standarised, and then it was converted into
R.
I have tried many times. Can you tell me what else I need to do to calculate
the impacts and tell me the codes. Thank you so much!

Best regards. 

Yours sincerely, 
Wenyue Yang

*The codes I put into R is presented as follow:*

>v=read.csv(file=file.choose(),header=T)
>library(spdep)
>swm<-read.table(file=file.choose(),header=T,sep=",")# I choose the text
file of matrix built in ArcGIS
>n=length(unique(swm[[3]]))
>swmmat<-matrix(0,n,n)
>apply(swm,1,function(x)swmmat[x[3],x[4]]<<-x[5])
>setequal(swm$ID, v$ID)
>dimnames(swmmat)<-list(v$ID,v$ID)
>swmmat<-swmmat[v$ID,v$ID]
>fm=SJgap~BSD+MSD+RND
>s.lag<-lagsarlm(fm,data=v,listw=mat2listw(swmmat))
>impacts(s.lag, mat2listw(swmmat))
Error in impacts.sarlm(s.lag, mat2listw(swmmat)) : 
  Only row-standardised weights supported

OR
> W <- as(mat2listw(swmmat), "CsparseMatrix")
> trMatc <- trW(W, type="mult")
> impacts(s.lag, trMatc)
Error in impacts.sarlm(s.lag, trMatc) : 
  Only row-standardised weights supported




--
View this message in context: http://r.789695.n4.nabble.com/Why-I-get-the-massage-of-Error-in-impacts-sarlm-s-lag-mat2listw-swmmat-Only-row-standardised-weights-tp4710356.html
Sent from the R help mailing list archive at Nabble.com.


From lists at dewey.myzen.co.uk  Sun Jul 26 17:00:08 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sun, 26 Jul 2015 16:00:08 +0100
Subject: [R] Compute z
In-Reply-To: <55b49d00.4676460a.5615.ffffdc34@mx.google.com>
References: <55b49d00.4676460a.5615.ffffdc34@mx.google.com>
Message-ID: <55B4F5F8.9080809@dewey.myzen.co.uk>

Dear Hal

Are you looking for %*% by any chance?

On 26/07/2015 09:38, admin.dslcomputer at gmail.com wrote:
> Hi Everyone:
>
>
> How do I correctly compute z?
>
>
>
> z = 0;
> for i = 1:7
>    z = z + v(i) * w(i)
> end
>
>
> If there are two column vectors v and w, each with 7 elements (i.e., they have dimensions 7x1).
>
>
> Regards,
>
> Hal
>
>
>
>
>
>
> Sent from Surface
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From jdnewmil at dcn.davis.CA.us  Sun Jul 26 17:19:59 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 26 Jul 2015 08:19:59 -0700
Subject: [R] Compute z
In-Reply-To: <55b49d00.4676460a.5615.ffffdc34@mx.google.com>
References: <55b49d00.4676460a.5615.ffffdc34@mx.google.com>
Message-ID: <A2F7B866-42DC-4E84-BE64-D76DAE80134C@dcn.davis.CA.us>

sum(v*w)

There are no "column vectors" in R... there are vectors (that have no "direction"), and there are data frames that might only have one column, and matrices that might have many rows but only one column, and a piece of matrix or data frame is often converted to a vector when indexing is used to extract a column or row (e.g. mat[ , 1 ]).

It may feel too dense to absorb at first, but the Introduction to R document that comes with R actually explains all this. Try (re)reading that occasionally until it sinks in.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 26, 2015 1:38:24 AM PDT, admin.dslcomputer at gmail.com wrote:
>Hi Everyone:
>
>
>How do I correctly compute z? 
>
>
>
>z = 0;
>for i = 1:7
>  z = z + v(i) * w(i)
>end
>
>
>If there are two column vectors v and w, each with 7 elements (i.e.,
>they have dimensions 7x1). 
>
>
>Regards,
>
>Hal
>
>
>
>
>
>
>Sent from Surface
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bretschr at xs4all.nl  Sun Jul 26 17:53:05 2015
From: bretschr at xs4all.nl (Franklin Bretschneider)
Date: Sun, 26 Jul 2015 17:53:05 +0200
Subject: [R] Opposite color in R
In-Reply-To: <55B3E863.2010208@utu.fi>
References: <55B3E863.2010208@utu.fi>
Message-ID: <5B6B0638-E435-4822-B6C3-4A3306E00842@xs4all.nl>

Dear Atte Tenkanen,


Re:
> Hi,
> 
> I have tried to find a way to find opposite or complementary colors in R.
> 
> I would like to form a color circle with R like this one: http://nobetty.net/dandls/colorwheel/complementary_colors.jpg
> 
> If you just make a basic color wheel in R, the colors do not form complementary color circle:
> 
> palette(rainbow(24))
> Colors=palette()
> pie(rep(1, 24), col = Colors)
> 
> There is a package ?colortools? where you can find function opposite(), but it doesn?t work as is said. I tried
> 
> library(colortools)
> opposite("violet") and got green instead of yellow and
> 
> opposite("blue") and got yellow instead of orange.
> 
> Do you know any solutions?
> 
> Atte Tenkanen


Actually, yellow and blue are complementary colours, but red and green aren't. 

The human visual system has three types of cones: red-sensitive, green-sensitive and blue-sensitive.
(the labels are approximate, e.g. red-sensitive cones have their optimum sensitivity at a wavelength we might call orange, but for understanding colours, R-G-B is the useful standard designation).
A certain combination of these three together, such as in sunlight, is seen as white. In the digital domain, the three "colour channels" of an image are usually scaled to 8-bit numbers, i.e. from zero up to and including 255. So, all three channels 255 makes white.

Leaving one of the three colors out yields yellow (no blue), magenta (no green) and cyan (no red). The pairs yellow-blue, magenta-green and cyan-red  are truly complementary colours.

Colours are the result of the wavelength of the light, so one would expect colours to lie on a linear scale, from about 700 nm (red), through 550 (green) to about 440 nm (blue).

There is a complication, however: the photosensitive pigment of our red cones has a second action peak past that of the blue cones, so past pure blue we see a sort of reddish blue, in other words violet or purple. Therefore, the colours can be plotted in a circle, where violet and purple fill the gap between blue and red.

Using a combination of the three ground colors R, G and B, any desired colour shade can be composed. Orange, for example, consists of (approximately) all red and half green. 

- - - - - - - - -

R has ample possibilities to compose colours or colour palettes, with which one can create (almost continuous) gradients or stepwise colour patches.
Examples are col2rgb():

>col2rgb("orange")
      [,1]
red    255
green  165
blue     0

>col2rgb("violet")
      [,1]
red    238
green  130
blue   238

Cindy Brewer wrote a fine set of colour functions, adapted to R by Erich Neuwirth. See package "RColorBrewer".

And much can be done with the standard R distribution:
 
The following code plots a some colours in a circle, with the complementary colours at opposite sides (so crudely what you're after):


# define colour triplets
reds =   c( 255, 255,  255,    0,    0,    0,    0,  128)
greens = c(   0, 127,  255,  255,  255,  127,    0,    0)
blues=   c(   0,   0,    0,    0,  255,  255,  255,  255)
n = length(reds)
#  compute circle to plot in
stp = 2*pi/n
th = seq(0,2*pi-stp, length.out=n)
x = cos(th); y=sin(th)
#  plot (on a Mac, for other OSses call the appropriate grahics window
quartz(w=5, h=5)
par(xpd=NA)
plot(x,y,pch=15, cex=8, col=rgb(reds, greens, blues, maxColorValue = 255), asp=1, axes=FALSE, xlab='', ylab='')
points(x,y,pch=0, cex=8, col="black")
# arrows connect the complementary colours
arrows(0,0, 0.7*x, 0.7*y, length = 0.25, col = "grey")

Hope this helps;
Best wishes,


Frank
------

Franklin Bretschneider
Dept of Biology
Utrecht University
bretschr at xs4all.nl


From aurora.gonzalez2 at um.es  Sun Jul 26 20:16:41 2015
From: aurora.gonzalez2 at um.es (AURORA GONZALEZ VIDAL)
Date: Sun, 26 Jul 2015 20:16:41 +0200
Subject: [R] graphviz, Rmarkdown, colorBrewer
Message-ID: <20150726201641.Horde.w_YSdJlvkopAIrM9D4IZ1w2@webmail.um.es>

Hello. I am drawing a graph using graphviz. It works but now, I am trying
to use some palettes from the RColorBrewer pakcage. Any idea why this
diagram works when the code (in .Rmd) is

```{r, engine='dot', echo=F}
digraph unix{
? size=30;
? ratio=compress;
?
? param [label="? Contrastes param?tricos? ", shape=oval,
?????????? style="filled,rounded,diagonals",
fillcolor=dodgerblue3,
?????????? fontcolor=gray90];
?
```

but it doesn't work if I try to use some colors of any palette

```{r, echo=FALSE}
library("RColorBrewer")
colores <- brewer.pal(11,"PiYG")
```

```{r, engine='dot', echo=F}
digraph unix{
? size=30;
? ratio=compress;
?
? param [label="? Contrastes param?tricos? ", shape=oval,
?????????? style="filled,rounded,diagonals",
fillcolor=colores[1],
?????????? fontcolor=gray90];
?
```

Thank you very much!!


------
Aurora Gonz?lez Vidal

Secci?n Apoyo Estad?stico.
Servicio de Apoyo a la Investigaci?n (SAI).
Vicerrectorado de Investigaci?n.
Universidad de Murcia
Edif. SACE . Campus de Espinardo.
30100 Murcia

@. aurora.gonzalez2 at um.es
T. 868 88 7315
F. 868 88 7302
www.um.es/sai
www.um.es/ae

	[[alternative HTML version deleted]]


From cecilia.larrosa10 at imperial.ac.uk  Sun Jul 26 21:00:15 2015
From: cecilia.larrosa10 at imperial.ac.uk (Larrosa, Cecilia)
Date: Sun, 26 Jul 2015 19:00:15 +0000
Subject: [R] Varying name of output tables from looped process of list of
 spdf objects
Message-ID: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>

Hi,

This is a repost from here (http://r.789695.n4.nabble.com/Writing-output-of-a-looped-process-with-pdfs-tt4710348.html), due to the post not being complete originally. I am running R studio on OS X Yosemite 10.10.4 (Mac). I appreciate you help very much!

The objective: I have 100 shapefiles that need to undergo the same process.

The process: I use gDistance{rgdal} to calculate the distance between all features (polygons) within each layer, and output a txt file.

The problem: I need the name of the output txt file to contain the name of the shapefile, but the shapefiles are read into R as SpatialPolygonsDataFrames (spdf) and I cannot find a way to use the name of the spdf objects as character in order to make it vary with each iteration.

My questions to you: Do you know a way to solve the problem or an alternative way to fulfil the objective? I have come to determine the problem after searching about the error message, have I interpreted correctly?


Here is a minimal dataset for replicability:

> dput(a_1)
new("SpatialPolygonsDataFrame"
    , data = structure(list(ID = 1:3, GRIDCODE = c(1L, 1L, 1L), Shape_Leng = c(3349.48347556,
1618.93904903, 893.268790786), Shape_Area = c(309430.38861, 90015.8325676,
47507.0325775), Count = c(1L, 1L, 1L)), .Names = c("ID", "GRIDCODE",
"Shape_Leng", "Shape_Area", "Count"), row.names = 0:2, class = "data.frame")
    , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>)
    , plotOrder = 1:3
    , bbox = structure(c(476685.625393809, 311791.86152084, 508519.585393809,
312935.41622084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
), c("min", "max")))
    , proj4string = new("CRS"
    , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
)
)
> dput(a_10)
new("SpatialPolygonsDataFrame"
    , data = structure(list(ID = 1:5, GRIDCODE = c(1L, 1L, 1L, 1L, 1L), Shape_Leng = c(1691.7247095,
2305.45647624, 1022.64650591, 1172.27848042, 94.2722341164),
    Shape_Area = c(6.47354525991, 92111.8528756, 65.7173995386,
    19042.7776647, 415.253663691), Count = c(1L, 1L, 1L, 1L,
    1L)), .Names = c("ID", "GRIDCODE", "Shape_Leng", "Shape_Area",
"Count"), row.names = 0:4, class = "data.frame")
    , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>)
    , plotOrder = c(2L, 4L, 5L, 3L, 1L)
    , bbox = structure(c(825796.904693809, 815666.86152084, 831270.106493809,
816562.46752084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
), c("min", "max")))
    , proj4string = new("CRS"
    , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
)
)


Here is the code that I have been using:

###Load packages
library(rgdal)
library(gdistance)

###Read forest shape files
setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/R_Quest/")
shps<- dir(getwd(), "*.shp")
shps <- gsub('.{4}$', '', shps)
for (shp in shps) assign(shp, readOGR(".",layer=shp))

###Create list of spdf objects
fnps<- mget(ls(pattern= "a_"))

###For each spatial layer (object in the list), calculate distance between all polygons within layer
for (fnp in fnps)
{
  distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
  row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);        # did this because gDistance changed the IDs of the features from [1 to ...] to [0 to ...], not sure why
  colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?);            # same as above
  dists.melt <- melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,];  #use only lower triangle of the distances matrix
  outfile <- file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/", paste0("distances_", fnp, ".txt"));
  write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
}

And this is the error message:

Error in as.character.default(<S4 object of class "SpatialPolygonsDataFrame">) :
  no method for coercing this S4 class to a vector

Thank you very much!!
Cecilia


	[[alternative HTML version deleted]]


From cecilia.larrosa10 at imperial.ac.uk  Sun Jul 26 21:06:38 2015
From: cecilia.larrosa10 at imperial.ac.uk (Larrosa, Cecilia)
Date: Sun, 26 Jul 2015 19:06:38 +0000
Subject: [R] Varying name of output tables from looped process of list of
 spdf objects
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
Message-ID: <5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>

Hi,

This is a repost from here (http://r.789695.n4.nabble.com/Writing-output-of-a-looped-process-with-pdfs-tt4710348.html), due to the post not being complete originally. I am running R studio on OS X Yosemite 10.10.4 (Mac). I appreciate you help very much!

The objective: I have 100 shapefiles that need to undergo the same process.

The process: I use gDistance{rgdal} to calculate the distance between all features (polygons) within each layer, and output a txt file.

The problem: I need the name of the output txt file to contain the name of the shapefile, but the shapefiles are read into R as SpatialPolygonsDataFrames (spdf) and I cannot find a way to use the name of the spdf objects as character in order to make it vary with each iteration.

My questions to you: Do you know a way to solve the problem or an alternative way to fulfil the objective? I have come to determine the problem after searching about the error message, have I interpreted correctly?


Here is a minimal dataset for replicability:

> dput(a_1)
new("SpatialPolygonsDataFrame"
    , data = structure(list(ID = 1:3, GRIDCODE = c(1L, 1L, 1L), Shape_Leng = c(3349.48347556,
1618.93904903, 893.268790786), Shape_Area = c(309430.38861, 90015.8325676,
47507.0325775), Count = c(1L, 1L, 1L)), .Names = c("ID", "GRIDCODE",
"Shape_Leng", "Shape_Area", "Count"), row.names = 0:2, class = "data.frame")
    , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>)
    , plotOrder = 1:3
    , bbox = structure(c(476685.625393809, 311791.86152084, 508519.585393809,
312935.41622084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
), c("min", "max")))
    , proj4string = new("CRS"
    , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
)
)
> dput(a_10)
new("SpatialPolygonsDataFrame"
    , data = structure(list(ID = 1:5, GRIDCODE = c(1L, 1L, 1L, 1L, 1L), Shape_Leng = c(1691.7247095,
2305.45647624, 1022.64650591, 1172.27848042, 94.2722341164),
    Shape_Area = c(6.47354525991, 92111.8528756, 65.7173995386,
    19042.7776647, 415.253663691), Count = c(1L, 1L, 1L, 1L,
    1L)), .Names = c("ID", "GRIDCODE", "Shape_Leng", "Shape_Area",
"Count"), row.names = 0:4, class = "data.frame")
    , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>,
    <S4 object of class structure("Polygons", package = "sp")>)
    , plotOrder = c(2L, 4L, 5L, 3L, 1L)
    , bbox = structure(c(825796.904693809, 815666.86152084, 831270.106493809,
816562.46752084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
), c("min", "max")))
    , proj4string = new("CRS"
    , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
)
)


Here is the code that I have been using:

###Load packages
library(rgdal)
library(gdistance)

###Read forest shape files
setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/R_Quest/")
shps<- dir(getwd(), "*.shp")
shps <- gsub('.{4}$', '', shps)
for (shp in shps) assign(shp, readOGR(".",layer=shp))

###Create list of spdf objects
fnps<- mget(ls(pattern= "a_"))

###For each spatial layer (object in the list), calculate distance between all polygons within layer
for (fnp in fnps)
{
  distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
  row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);        # did this because gDistance changed the IDs of the features from [1 to ...] to [0 to ...], not sure why
  colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?);            # same as above
  dists.melt <- melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,];  #use only lower triangle of the distances matrix
  outfile <- file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/", paste0("distances_", fnp, ".txt"));
  write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
}

And this is the error message:

Error in as.character.default(<S4 object of class "SpatialPolygonsDataFrame">) :
  no method for coercing this S4 class to a vector

Thank you very much!!
Cecilia


	[[alternative HTML version deleted]]


From liuwensui at gmail.com  Sun Jul 26 23:00:08 2015
From: liuwensui at gmail.com (Wensui Liu)
Date: Sun, 26 Jul 2015 17:00:08 -0400
Subject: [R] VIF threshold implying multicollinearity
Message-ID: <CAKyN3iD0AZK+G-YwWyf9B8He02MM=cmOP6YqZrTnHy1K-h1NPA@mail.gmail.com>

Dear All
I have a general question about VIF.
While there are multiple rules of thumb about the threshold value of
VIF, e.g. 4 or 10, implying multicollinearity, I am wondering if
anyone can point me to some literature supporting these rules of
thumb.

Thank you so much!
wensui


From a.mosnier at gmail.com  Mon Jul 27 01:04:23 2015
From: a.mosnier at gmail.com (Arnaud Mosnier)
Date: Sun, 26 Jul 2015 19:04:23 -0400
Subject: [R] Reading some csv files from different folders and add the
 name of each files to the first column of files
Message-ID: <CANkFkEefVQrNotZbsv12SqFQJ4G2N_HhqCLHyAv5gSW-9w9_wA@mail.gmail.com>

Hi Lida,

You can try this:

d<- choose.dir() # choose the folder with the subdirectories containing the
csv files
f <- list.files(d, full.names = TRUE, recursive = TRUE)

# Here the example for the "sing" files
selsing <- grep("sing",f) #Select the files notaining the word sing

allsing <- data.frame() #Create an empty data frame

# Loop among the selected files
for (i in 1:length(f[selsing])){
  dat <- read.csv(file=f[selsing][i])  # suppose that the csv files have a
header
  allsing <- rbind(allsing,data.frame(FileID =
gsub("EA_sing_|.txt","",basename(f[selsing][i])), dat)) #Combine the file
ID with the other columns and add the result to the all sing object
}

Now, you just have to do it for the other cases and save your final object.

Hope this help !

Arnaud

###########################################################################

Date: Sat, 25 Jul 2015 15:03:21 -0500
From: Lida Zeighami <lid.zigh at gmail.com>
To: r-help at r-project.org
Subject: [R] Reading some csv files from different folders and add the
        name of each files to the first column of files
Message-ID:
        <CAMqbV1CXN0=R+HxmaYS40sMymGdbhDQK1V+n5bHmBPgBr3M2Yg at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

I have 600 folders in which there are 3 csv files. The name of folders are
as follows: EA_aa, EA_bb, EA_cc, EA_dd,....

In each folder there are 3 csv files:
in folder EA_aa there are:
  EA_sing_aa.csvqwerty
  EA_ska_aa.csv
  EA_tat_aa.csv

In folder EA_bb:
  EA_sing_bb.csv
  EA_ska_bb.csv
  EA_tat_bb.csv
...

I need to read all the same kind of files (for example sing files) in one
dataframe but before that I should add a column to each files with the name
of file as a row names!

So in output I should have just 3 csv files such as
EA_sing.csv: (rbind all EA_sing_*.csv file)
EA_ska.csv: (rbind all EA_ska_*.csv files)
EA_tat.csv: (rbind all EA_tat_*.csv files)

And in each file the first column should be added as row names which
containing the file name! So the format will be as follow:
EA_sing.csv:
1st column   2nd column  3rd column
   aa                    yhhh                ghj
   aa                     k ki                   Fyh
   bb                     k ki                   vgd
   bb                     k gki                  Fyh
   bb                    k reci                  Fyh
   cc                     k hcd                  hyd
   dd                     lmb                     Fyh

EA_ska.csv:
1st column   2nd column  3rd column
   aa                    yhhh                ghj
   aa                     k ki                   Fyh
   bb                     k ki                   Fyh
   cc                     k gki                  Fyh
   cc                     k reci                  oki
   cc                     k hcd                  Fyh
   dd                     lmb                     dsf

EA_tat.csv:
1st column   2nd column  3rd column
   aa                    yhhh                ghj
   aa                     k ki                   Fyh
   bb                     k ki                   Fyh
   cc                     k gki                  Fyh
   cc                     k reci                  oki
   dd                     k hcd                  Fyh
   dd                     lmb                     dsf

Would you please help me how to can I do that?
Thanks

	[[alternative HTML version deleted]]


From syen04 at gmail.com  Mon Jul 27 03:10:34 2015
From: syen04 at gmail.com (Steven Yen)
Date: Sun, 26 Jul 2015 21:10:34 -0400
Subject: [R] Judging if a matrix contains any NA
In-Reply-To: <CAGxFJbQ0f6wdPpKLr3APuERuGVwvBia-fvo45MHrDX+QaqR7dA@mail.gmail.com>
References: <5589D070.4070900@gmail.com> <5589D219.5070001@gmail.com>
	<CAGxFJbQ0f6wdPpKLr3APuERuGVwvBia-fvo45MHrDX+QaqR7dA@mail.gmail.com>
Message-ID: <55B5850A.7030301@gmail.com>

How do I judge if a matrix contain any NA or otherwise non-missing, 
non-numerical?
In the following, I would like to deliver ONE logical of TRUE or FALSE, 
rather than a 4 x 4 matrix containing TRUE or FALSE. Thank you.

 > a<-matrix(1:16,nrow=4)
 > diag(a)<-NA
 > a
      [,1] [,2] [,3] [,4]
[1,]   NA    5    9   13
[2,]    2   NA   10   14
[3,]    3    7   NA   15
[4,]    4    8   12   NA
 > is.na(a)
       [,1]  [,2]  [,3]  [,4]
[1,]  TRUE FALSE FALSE FALSE
[2,] FALSE  TRUE FALSE FALSE
[3,] FALSE FALSE  TRUE FALSE
[4,] FALSE FALSE FALSE  TRUE


From dwinsemius at comcast.net  Mon Jul 27 03:19:09 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sun, 26 Jul 2015 18:19:09 -0700
Subject: [R] Judging if a matrix contains any NA
In-Reply-To: <55B5850A.7030301@gmail.com>
References: <5589D070.4070900@gmail.com> <5589D219.5070001@gmail.com>
	<CAGxFJbQ0f6wdPpKLr3APuERuGVwvBia-fvo45MHrDX+QaqR7dA@mail.gmail.com>
	<55B5850A.7030301@gmail.com>
Message-ID: <F768EC33-6B39-430E-94A3-D2AAD859C471@comcast.net>


On Jul 26, 2015, at 6:10 PM, Steven Yen wrote:

> How do I judge if a matrix contain any NA or otherwise non-missing, non-numerical?
> In the following, I would like to deliver ONE logical of TRUE or FALSE, rather than a 4 x 4 matrix containing TRUE or FALSE. Thank you.
> 
> > a<-matrix(1:16,nrow=4)
> > diag(a)<-NA
> > a
>     [,1] [,2] [,3] [,4]
> [1,]   NA    5    9   13
> [2,]    2   NA   10   14
> [3,]    3    7   NA   15
> [4,]    4    8   12   NA

> any(is.na( a))
[1] TRUE


> > is.na(a)
>      [,1]  [,2]  [,3]  [,4]
> [1,]  TRUE FALSE FALSE FALSE
> [2,] FALSE  TRUE FALSE FALSE
> [3,] FALSE FALSE  TRUE FALSE
> [4,] FALSE FALSE FALSE  TRUE
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From murdoch.duncan at gmail.com  Mon Jul 27 03:29:06 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 26 Jul 2015 21:29:06 -0400
Subject: [R] Judging if a matrix contains any NA
In-Reply-To: <55B5850A.7030301@gmail.com>
References: <5589D070.4070900@gmail.com>
	<5589D219.5070001@gmail.com>	<CAGxFJbQ0f6wdPpKLr3APuERuGVwvBia-fvo45MHrDX+QaqR7dA@mail.gmail.com>
	<55B5850A.7030301@gmail.com>
Message-ID: <55B58962.9080708@gmail.com>

On 26/07/2015 9:10 PM, Steven Yen wrote:
> How do I judge if a matrix contain any NA or otherwise non-missing, 
> non-numerical?

David told you about any().  You may also want to use !is.finite()
instead of is.na().

Duncan Murdoch

> In the following, I would like to deliver ONE logical of TRUE or FALSE, 
> rather than a 4 x 4 matrix containing TRUE or FALSE. Thank you.
> 
>  > a<-matrix(1:16,nrow=4)
>  > diag(a)<-NA
>  > a
>       [,1] [,2] [,3] [,4]
> [1,]   NA    5    9   13
> [2,]    2   NA   10   14
> [3,]    3    7   NA   15
> [4,]    4    8   12   NA
>  > is.na(a)
>        [,1]  [,2]  [,3]  [,4]
> [1,]  TRUE FALSE FALSE FALSE
> [2,] FALSE  TRUE FALSE FALSE
> [3,] FALSE FALSE  TRUE FALSE
> [4,] FALSE FALSE FALSE  TRUE
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From cflynch at ncsu.edu  Mon Jul 27 03:36:36 2015
From: cflynch at ncsu.edu (Collin Lynch)
Date: Sun, 26 Jul 2015 21:36:36 -0400
Subject: [R] VIF threshold implying multicollinearity
In-Reply-To: <CAKyN3iD0AZK+G-YwWyf9B8He02MM=cmOP6YqZrTnHy1K-h1NPA@mail.gmail.com>
References: <CAKyN3iD0AZK+G-YwWyf9B8He02MM=cmOP6YqZrTnHy1K-h1NPA@mail.gmail.com>
Message-ID: <CAE=6FXazREK-MnUMDLqGxLXgPBhCBLd-dzyFRThjsApku8wRzQ@mail.gmail.com>

The following sources discuss the issues generally and may be a goof
pointer to the literature on VIF.  Particularly the Schroeder paper.

@article{Yi:Evaluation,
   AUTHOR = {Youjae Yi},
   TITLE  = {On the Evaluation of Main Effects in Multiplicative
             Regression Models.},
   JOURNAL = {Journal of the Market Research Society},
   VOLUME  = {31},
   NUMBER  = {1},
   MONTH   = {January},
   YEAR    = {1989},
   PAGES   = {133-138}
}


@article{Gordon:Issues,
   AUTHOR  = {Robert A. Gordon},
   TITLE   = {Issues in Multiple Regression},
   JOURNAL = {American Journal of Sociology},
   VOLUME  = {73},
   NUMBER  = {5},
   MONTH   = {March},
   YEAR    = {1968},
   PAGES   = {592-616}
}


@misc{Lynch:Multicollinearity,
   author = {Scott M. Lynch},
   title  = {Multicollinearity},
   year   = {2003},
   url    = {\url{
http://www.princeton.edu/~slynch/soc504/multicollinearity.pdf}},
   note   = "[Online; accessed 11-October-2013]"
 }


@article{Schroeder:Multicollinearity,
   AUTHOR  = {Mary Ann Schroeder
               and Janice Lander
               and Stacey Levine-Silverman},
   TITLE   = {Diagnosing and Dealing with Multicollinearity},
   JOURNAL = {Western Journal of Nursing Research},
   VOLUME  = {12},
   NUMBER  = {2},
   YEAR    = {1990},
   PAGES   = {175-187}
}


@book{Afifi:Computer,
  AUTHOR    = {A. Afifi and V. Clark},
  TITLE     = {Computer-aided Multivariate Analysis},
  PUBLISHER = {Wadsworth, Belmont California},
  YEAR      = {1984}
}


On Sun, Jul 26, 2015 at 5:00 PM, Wensui Liu <liuwensui at gmail.com> wrote:

> Dear All
> I have a general question about VIF.
> While there are multiple rules of thumb about the threshold value of
> VIF, e.g. 4 or 10, implying multicollinearity, I am wondering if
> anyone can point me to some literature supporting these rules of
> thumb.
>
> Thank you so much!
> wensui
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Mon Jul 27 05:12:43 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 27 Jul 2015 15:12:43 +1200
Subject: [R] VIF threshold implying multicollinearity
In-Reply-To: <CAE=6FXazREK-MnUMDLqGxLXgPBhCBLd-dzyFRThjsApku8wRzQ@mail.gmail.com>
References: <CAKyN3iD0AZK+G-YwWyf9B8He02MM=cmOP6YqZrTnHy1K-h1NPA@mail.gmail.com>
	<CAE=6FXazREK-MnUMDLqGxLXgPBhCBLd-dzyFRThjsApku8wRzQ@mail.gmail.com>
Message-ID: <55B5A1AB.1030808@auckland.ac.nz>


On 27/07/15 13:36, Collin Lynch wrote:

> The following sources discuss the issues generally and may be a goof
> pointer to the literature ...

<SNIP>

I think that the foregoing merits fortune status! :-)

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From liuning.1982 at qq.com  Mon Jul 27 03:18:39 2015
From: liuning.1982 at qq.com (=?ISO-8859-1?B?TGl1TmluZw==?=)
Date: Mon, 27 Jul 2015 09:18:39 +0800
Subject: [R] Judging if a matrix contains any NA
In-Reply-To: <55B5850A.7030301@gmail.com>
References: <5589D070.4070900@gmail.com> <5589D219.5070001@gmail.com>
	<CAGxFJbQ0f6wdPpKLr3APuERuGVwvBia-fvo45MHrDX+QaqR7dA@mail.gmail.com>
	<55B5850A.7030301@gmail.com>
Message-ID: <tencent_12CF1E6D7EC7DB4A494DECD0@qq.com>

> all(is.na(a)) [1] FALSE

 

 ------------------ Original ------------------
  From:  "Steven Yen";<syen04 at gmail.com>;
 Date:  Mon, Jul 27, 2015 09:10 AM
 To:  "r-help mailing list"<r-help at r-project.org>; 
 
 Subject:  [R] Judging if a matrix contains any NA

 

How do I judge if a matrix contain any NA or otherwise non-missing, 
non-numerical?
In the following, I would like to deliver ONE logical of TRUE or FALSE, 
rather than a 4 x 4 matrix containing TRUE or FALSE. Thank you.

 > a<-matrix(1:16,nrow=4)
 > diag(a)<-NA
 > a
      [,1] [,2] [,3] [,4]
[1,]   NA    5    9   13
[2,]    2   NA   10   14
[3,]    3    7   NA   15
[4,]    4    8   12   NA
 > is.na(a)
       [,1]  [,2]  [,3]  [,4]
[1,]  TRUE FALSE FALSE FALSE
[2,] FALSE  TRUE FALSE FALSE
[3,] FALSE FALSE  TRUE FALSE
[4,] FALSE FALSE FALSE  TRUE

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From amelia_marsh08 at yahoo.com  Mon Jul 27 08:00:54 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Mon, 27 Jul 2015 06:00:54 +0000 (UTC)
Subject: [R] Sample Weights
Message-ID: <584050594.1919334.1437976854475.JavaMail.yahoo@mail.yahoo.com>

Dear R Forum

I have a data.frame as


mydat = c(6,6,5,6,4,6,8,4,6,6,6,3,4,6,5,7,7,4,3,5,5,5,3,6,7,4,4,7,4,3,4,6,4,6,5,4,4,7,6,8,5,6,5,5,8,2,3,5,7,5)

Is there any library or way in R to allocate weights to these values? Actually I am having a large data, but for illustrative purpose, have considered just a small part of the same. 

Regards

Amelia


From jdnewmil at dcn.davis.CA.us  Mon Jul 27 08:19:25 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Sun, 26 Jul 2015 23:19:25 -0700
Subject: [R] Sample Weights
In-Reply-To: <584050594.1919334.1437976854475.JavaMail.yahoo@mail.yahoo.com>
References: <584050594.1919334.1437976854475.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <C52BA23A-D6EE-4163-80D6-705A47CE310D@dcn.davis.CA.us>

If you have a clear idea what meaning those weights have (?) in the context of a specific calculation (?), and you know what the weights are (?), then it is usually trivially easy to do in R. However,  your question is vague on all of those points, so offering you a solution seems like an invitation for you to mis-use any particular solution offered. Please try to clarify what you are doing that "weights" will help with, and yes, there may just be a weights argument to the function that does that analysis that we can point you to.

You also are unclear what the difference between a data frame and a vector is... if it helps, a data frame is a list of vectors (typically referred to as "columns") all with the same length...  your "mydat" is a vector, not a data frame.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 26, 2015 11:00:54 PM PDT, Amelia Marsh via R-help <r-help at r-project.org> wrote:
>Dear R Forum
>
>I have a data.frame as
>
>
>mydat =
>c(6,6,5,6,4,6,8,4,6,6,6,3,4,6,5,7,7,4,3,5,5,5,3,6,7,4,4,7,4,3,4,6,4,6,5,4,4,7,6,8,5,6,5,5,8,2,3,5,7,5)
>
>Is there any library or way in R to allocate weights to these values?
>Actually I am having a large data, but for illustrative purpose, have
>considered just a small part of the same. 
>
>Regards
>
>Amelia
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From j.para.fernandez at hotmail.com  Mon Jul 27 09:26:49 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Mon, 27 Jul 2015 00:26:49 -0700 (PDT)
Subject: [R] R GUI plot by color
In-Reply-To: <55B4C657.5000808@atsu.edu>
References: <1437727381033-4710297.post@n4.nabble.com>
	<CA+8X3fXqko3UsPA9CVOhHO=q-FnGPyO3sM-JzE-=p94+hhegdg@mail.gmail.com>
	<1437731620874-4710300.post@n4.nabble.com>
	<CA+8X3fXaC2hhVp7zD2Wc+NEHP0cfKXdE=fSYa_jFibxQKLbXFw@mail.gmail.com>
	<55B4C657.5000808@atsu.edu>
Message-ID: <1437982009687-4710382.post@n4.nabble.com>

Ok,  I will take it into account in the future.

Thanks!! 



--
View this message in context: http://r.789695.n4.nabble.com/R-GUI-plot-by-color-tp4710297p4710382.html
Sent from the R help mailing list archive at Nabble.com.


From jdnewmil at dcn.davis.CA.us  Mon Jul 27 09:37:38 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 27 Jul 2015 00:37:38 -0700
Subject: [R] Sample Weights
In-Reply-To: <549596576.1945472.1437979051201.JavaMail.yahoo@mail.yahoo.com>
References: <C52BA23A-D6EE-4163-80D6-705A47CE310D@dcn.davis.CA.us>
	<549596576.1945472.1437979051201.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <FEB0CADC-83FB-4DBC-B6EC-8195C135E523@dcn.davis.CA.us>

R is a computing tool, and each package has implemented algorithms that have history and books and papers that allow those algorithms to be used in a variety if computing environments... from Fortran to Excel to Java to ... R, and probably beyond.

>From your description I am going to hazard a guess that perhaps you are considering regression, where the ranges of some variables may be much smaller than others, and yes, among the many types of regression there are many that accept weights... constant values used to scale each "column" of data so that they all have similar ranges. You might be able to get along just fine with the lm function from base R [1], or you might need something more specialized as [2] might discuss. Or you may have something else in mind, but only you can clarify what that might be.

It may be wise to do a bit of reading so you know which algorithms you want to apply, and then give us a reproducible, small example of what you have and what you want to get out of this analysis. To do that, you will really need to read and follow the advice in [3].

[1] type "?lm" at the R prompt to get details about that function. If that is too terse, get a text book.
[2] https://cran.r-project.org/web/views/Multivariate.html
[3] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example 
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 26, 2015 11:37:31 PM PDT, Amelia Marsh <amelia_marsh08 at yahoo.com> wrote:
>Dear Sir,
>
>I do appreciate your views. Yes even I was also aware about the non
>clarity in the question. Actaully, I have a large data having lots of
>data of low magnitude and few of very high magnitude. In order to
>analyse the same, some very senior person in the office suggested me to
>assign weights to these obervations.
>
>Problem is this senior person is on office tour travelling into
>Australia and unfortutely I can't even think of reverting to him owing
>to his seniority. I tried to find through some R libraries if I can get
>something about sample weights. 
>
>But as you have pointed out, yes my question is too vague and feel
>really sorry about the same. And I have recently started with R
>language hence I am trying to learn some basics about R. Thanks for
>pointing the difference between data.frame and vector.
>
>
>Thanks again for your response.
>
>Regards
>
>Amelia
>
>_____________________________________________________________________________________________
>
>
>
>On Monday, 27 July 2015 11:49 AM, Jeff Newmiller
><jdnewmil at dcn.davis.CA.us> wrote:
>If you have a clear idea what meaning those weights have (?) in the
>context of a specific calculation (?), and you know what the weights
>are (?), then it is usually trivially easy to do in R. However,  your
>question is vague on all of those points, so offering you a solution
>seems like an invitation for you to mis-use any particular solution
>offered. Please try to clarify what you are doing that "weights" will
>help with, and yes, there may just be a weights argument to the
>function that does that analysis that we can point you to.
>
>You also are unclear what the difference between a data frame and a
>vector is... if it helps, a data frame is a list of vectors (typically
>referred to as "columns") all with the same length...  your "mydat" is
>a vector, not a data frame.
>---------------------------------------------------------------------------
>Jeff Newmiller                        The     .....       .....  Go
>Live...
>DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>Go...
>                                     Live:   OO#.. Dead: OO#..  Playing
>Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>/Software/Embedded Controllers)               .OO#.       .OO#. 
>rocks...1k
>---------------------------------------------------------------------------
>
>Sent from my phone. Please excuse my brevity.
>
>
>On July 26, 2015 11:00:54 PM PDT, Amelia Marsh via R-help
><r-help at r-project.org> wrote:
>>Dear R Forum
>>
>>I have a data.frame as
>>
>>
>>mydat =
>>c(6,6,5,6,4,6,8,4,6,6,6,3,4,6,5,7,7,4,3,5,5,5,3,6,7,4,4,7,4,3,4,6,4,6,5,4,4,7,6,8,5,6,5,5,8,2,3,5,7,5)
>>
>>Is there any library or way in R to allocate weights to these values?
>>Actually I am having a large data, but for illustrative purpose, have
>>considered just a small part of the same. 
>>
>>Regards
>>
>>Amelia
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Mon Jul 27 10:28:30 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 27 Jul 2015 10:28:30 +0200
Subject: [R] Judging if a matrix contains any NA
In-Reply-To: <tencent_12CF1E6D7EC7DB4A494DECD0@qq.com>
References: <5589D070.4070900@gmail.com> <5589D219.5070001@gmail.com>
	<CAGxFJbQ0f6wdPpKLr3APuERuGVwvBia-fvo45MHrDX+QaqR7dA@mail.gmail.com>
	<55B5850A.7030301@gmail.com>
	<tencent_12CF1E6D7EC7DB4A494DECD0@qq.com>
Message-ID: <ABDCB82A-9916-4A7D-AABA-EAAFC6769DE9@gmail.com>


> On 27 Jul 2015, at 03:18 , LiuNing <liuning.1982 at qq.com> wrote:
> 
>> all(is.na(a)) [1] FALSE
> 
> 

Bzzt! Try !all(is.finite(a))

-pd

> 
> ------------------ Original ------------------
>  From:  "Steven Yen";<syen04 at gmail.com>;
> Date:  Mon, Jul 27, 2015 09:10 AM
> To:  "r-help mailing list"<r-help at r-project.org>; 
> 
> Subject:  [R] Judging if a matrix contains any NA
> 
> 
> 
> How do I judge if a matrix contain any NA or otherwise non-missing, 
> non-numerical?
> In the following, I would like to deliver ONE logical of TRUE or FALSE, 
> rather than a 4 x 4 matrix containing TRUE or FALSE. Thank you.
> 
>> a<-matrix(1:16,nrow=4)
>> diag(a)<-NA
>> a
>      [,1] [,2] [,3] [,4]
> [1,]   NA    5    9   13
> [2,]    2   NA   10   14
> [3,]    3    7   NA   15
> [4,]    4    8   12   NA
>> is.na(a)
>       [,1]  [,2]  [,3]  [,4]
> [1,]  TRUE FALSE FALSE FALSE
> [2,] FALSE  TRUE FALSE FALSE
> [3,] FALSE FALSE  TRUE FALSE
> [4,] FALSE FALSE FALSE  TRUE
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From marongiu.luigi at gmail.com  Mon Jul 27 11:55:17 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Mon, 27 Jul 2015 10:55:17 +0100
Subject: [R] force values for elements of a dataframe below cut-off
Message-ID: <CAMk+s2TZe4nvpAaa4rHVpdJLEcMQn8kk2LV4ckWaxiEPtQvYfQ@mail.gmail.com>

Dear all,
I would like to clip the data of a dataframe by forcing the value of
the elements below a cut-off to 0. I believe that the function to be
used is sapply but I could not have the call working properly.
Would you have any tip?
Best regards
Luigi


>>>
value <- c(5.43, 6.63, 0, 6.2, 5.61, 0, 5.59, 0, 0, 5.49, 18.35, 0,
6.07, 4.54, 4.73, 0, 5.74, 33.02, 4.45, 31.16, 0, 0, 3.12, 0, 0, 4.78,
0, 0, 0, 0, 0, 32.42, 3.35, 3.87, 0, 3.26, 5.75, 1.66, 0, 0, 0, 8.49,
3.08, 0, 0, 3.74, 0, 0, 4.06, 3.8, 0, 0
)
sample <- c("p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
"p.002", "p.002", "p.002", "p.002"
)

df <- as.data.frame(cbind(sample, value))

# cut off
co <- 8.5


From drjimlemon at gmail.com  Mon Jul 27 12:12:32 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 27 Jul 2015 20:12:32 +1000
Subject: [R] force values for elements of a dataframe below cut-off
In-Reply-To: <CAMk+s2TZe4nvpAaa4rHVpdJLEcMQn8kk2LV4ckWaxiEPtQvYfQ@mail.gmail.com>
References: <CAMk+s2TZe4nvpAaa4rHVpdJLEcMQn8kk2LV4ckWaxiEPtQvYfQ@mail.gmail.com>
Message-ID: <CA+8X3fWXk1Uxb=uP5Kp4ZrWJMR_=9d7a-EheOBK5P+7rdFKmoQ@mail.gmail.com>

Hi Luigi,
First, the way you have combined the two vectors coerces df$value to
factor. I think you want:

df<-data.frame(value,sample)

If you just want to change df$value, it's simple:

df$value[df$value<8.5]<-0

If you also want to change df$sample, you will probably want to add
another factor level and change it before you change df$value.

Jim


On Mon, Jul 27, 2015 at 7:55 PM, Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
> Dear all,
> I would like to clip the data of a dataframe by forcing the value of
> the elements below a cut-off to 0. I believe that the function to be
> used is sapply but I could not have the call working properly.
> Would you have any tip?
> Best regards
> Luigi
>
>
>>>>
> value <- c(5.43, 6.63, 0, 6.2, 5.61, 0, 5.59, 0, 0, 5.49, 18.35, 0,
> 6.07, 4.54, 4.73, 0, 5.74, 33.02, 4.45, 31.16, 0, 0, 3.12, 0, 0, 4.78,
> 0, 0, 0, 0, 0, 32.42, 3.35, 3.87, 0, 3.26, 5.75, 1.66, 0, 0, 0, 8.49,
> 3.08, 0, 0, 3.74, 0, 0, 4.06, 3.8, 0, 0
> )
> sample <- c("p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.001", "p.001", "p.001", "p.001", "p.001", "p.001", "p.001",
> "p.002", "p.002", "p.002", "p.002"
> )
>
> df <- as.data.frame(cbind(sample, value))
>
> # cut off
> co <- 8.5
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Mon Jul 27 12:50:23 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 27 Jul 2015 20:50:23 +1000
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
Message-ID: <CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>

Hi Cecilia,
I _think_ that the error is occurring in the call to paste0. You may
be able to get what you want like this:

paste0("distances_",deparse(substitute(fnp)),".txt")

Jim


On Mon, Jul 27, 2015 at 5:06 AM, Larrosa, Cecilia
<cecilia.larrosa10 at imperial.ac.uk> wrote:
> Hi,
>
> This is a repost from here (http://r.789695.n4.nabble.com/Writing-output-of-a-looped-process-with-pdfs-tt4710348.html), due to the post not being complete originally. I am running R studio on OS X Yosemite 10.10.4 (Mac). I appreciate you help very much!
>
> The objective: I have 100 shapefiles that need to undergo the same process.
>
> The process: I use gDistance{rgdal} to calculate the distance between all features (polygons) within each layer, and output a txt file.
>
> The problem: I need the name of the output txt file to contain the name of the shapefile, but the shapefiles are read into R as SpatialPolygonsDataFrames (spdf) and I cannot find a way to use the name of the spdf objects as character in order to make it vary with each iteration.
>
> My questions to you: Do you know a way to solve the problem or an alternative way to fulfil the objective? I have come to determine the problem after searching about the error message, have I interpreted correctly?
>
>
> Here is a minimal dataset for replicability:
>
>> dput(a_1)
> new("SpatialPolygonsDataFrame"
>     , data = structure(list(ID = 1:3, GRIDCODE = c(1L, 1L, 1L), Shape_Leng = c(3349.48347556,
> 1618.93904903, 893.268790786), Shape_Area = c(309430.38861, 90015.8325676,
> 47507.0325775), Count = c(1L, 1L, 1L)), .Names = c("ID", "GRIDCODE",
> "Shape_Leng", "Shape_Area", "Count"), row.names = 0:2, class = "data.frame")
>     , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
>     <S4 object of class structure("Polygons", package = "sp")>,
>     <S4 object of class structure("Polygons", package = "sp")>)
>     , plotOrder = 1:3
>     , bbox = structure(c(476685.625393809, 311791.86152084, 508519.585393809,
> 312935.41622084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
> ), c("min", "max")))
>     , proj4string = new("CRS"
>     , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
> )
> )
>> dput(a_10)
> new("SpatialPolygonsDataFrame"
>     , data = structure(list(ID = 1:5, GRIDCODE = c(1L, 1L, 1L, 1L, 1L), Shape_Leng = c(1691.7247095,
> 2305.45647624, 1022.64650591, 1172.27848042, 94.2722341164),
>     Shape_Area = c(6.47354525991, 92111.8528756, 65.7173995386,
>     19042.7776647, 415.253663691), Count = c(1L, 1L, 1L, 1L,
>     1L)), .Names = c("ID", "GRIDCODE", "Shape_Leng", "Shape_Area",
> "Count"), row.names = 0:4, class = "data.frame")
>     , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
>     <S4 object of class structure("Polygons", package = "sp")>,
>     <S4 object of class structure("Polygons", package = "sp")>,
>     <S4 object of class structure("Polygons", package = "sp")>,
>     <S4 object of class structure("Polygons", package = "sp")>)
>     , plotOrder = c(2L, 4L, 5L, 3L, 1L)
>     , bbox = structure(c(825796.904693809, 815666.86152084, 831270.106493809,
> 816562.46752084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
> ), c("min", "max")))
>     , proj4string = new("CRS"
>     , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
> )
> )
>
>
> Here is the code that I have been using:
>
> ###Load packages
> library(rgdal)
> library(gdistance)
>
> ###Read forest shape files
> setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/R_Quest/")
> shps<- dir(getwd(), "*.shp")
> shps <- gsub('.{4}$', '', shps)
> for (shp in shps) assign(shp, readOGR(".",layer=shp))
>
> ###Create list of spdf objects
> fnps<- mget(ls(pattern= "a_"))
>
> ###For each spatial layer (object in the list), calculate distance between all polygons within layer
> for (fnp in fnps)
> {
>   distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
>   row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);        # did this because gDistance changed the IDs of the features from [1 to ...] to [0 to ...], not sure why
>   colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?);            # same as above
>   dists.melt <- melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,];  #use only lower triangle of the distances matrix
>   outfile <- file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/", paste0("distances_", fnp, ".txt"));
>   write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
> }
>
> And this is the error message:
>
> Error in as.character.default(<S4 object of class "SpatialPolygonsDataFrame">) :
>   no method for coercing this S4 class to a vector
>
> Thank you very much!!
> Cecilia
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From b.h.mevik at usit.uio.no  Mon Jul 27 13:35:12 2015
From: b.h.mevik at usit.uio.no (=?utf-8?Q?Bj=C3=B8rn-Helge_Mevik?=)
Date: Mon, 27 Jul 2015 13:35:12 +0200
Subject: [R] A strange problem using pls package
In-Reply-To: <29095cae.dad9.14eb28cd716.Coremail.rhelpmaillist@163.com> (PO
	SU's message of "Wed, 22 Jul 2015 05:37:31 +0800 (CST)")
References: <29095cae.dad9.14eb28cd716.Coremail.rhelpmaillist@163.com>
Message-ID: <s3sfv49n89b.fsf@slagelg.uio.no>

"PO SU" <rhelpmaillist at 163.com> writes:

> ?suppose data has 20 columns
> ? traindata <- data[ 1:10, 1:10]
> ?testdata <- data[11:15,1:10]
> ? pls.fit <- plsr(y~x, ncomp = 5,?data = traindata, method= "simpls", scale = FALSE, model = TRUE, validation = "CV")
> ok, i get some result, the srange thing happens when i redo the plsr, i mean, i use
>
> ?traindata <- data[ 1:10, 1:20]
> ?testdata <- data[11:15,1:20]
> ?pls.fit <- plsr(y~x, ncomp = 5,?data = traindata, method= "simpls", scale = FALSE, model = TRUE, validation = "CV")
>
>
> I get the same result as the first one!!!

The reason is probably that you ask plsr() to use the coloumn of
traindata called "x" as the predictor.  Then it will only use that
coloumn, no matter how many coloumns traindata contains.

The usual way of using plsr() is to have a data.frame with a _matrix_ as
the predictor "coloumn", for instance like this:

mydata <- data.frame(y = some_vector, X = I(some_matrix))
mymodel <- plsr(y ~ X, ..., data = mydata)

If you want to have the predictors as separate vectors, you must name
all of them in the formula (y ~ x1 + x2 + x3 + ...), or you can use the
following shortcut to regress y on all the remaining coloumns:
plsr(y ~ ., ..., data = mydata)

-- 
Regards,
Bj?rn-Helge Mevik


From jrkrideau at inbox.com  Mon Jul 27 14:14:30 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 27 Jul 2015 04:14:30 -0800
Subject: [R] VIF threshold implying multicollinearity
In-Reply-To: <55B5A1AB.1030808@auckland.ac.nz>
References: <cakyn3id0azk+g-ywwyf9b8he02mm=cmop6yqzrtnhy1k-h1npa@mail.gmail.com>
	<cae=6fxazrek-mnumdlqgxlxgpbhcbld-dzyfrthjsapku8wrzq@mail.gmail.com>
Message-ID: <639DEAA621D.000001FBjrkrideau@inbox.com>

+1
I, originally,  read it as a stringent criticism of the first paper.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r.turner at auckland.ac.nz
> Sent: Mon, 27 Jul 2015 15:12:43 +1200
> To: cflynch at ncsu.edu
> Subject: Re: [R] VIF threshold implying multicollinearity
> 
> 
> On 27/07/15 13:36, Collin Lynch wrote:
> 
>> The following sources discuss the issues generally and may be a goof
>> pointer to the literature ...
> 
> <SNIP>
> 
> I think that the foregoing merits fortune status! :-)
> 
> cheers,
> 
> Rolf
> 
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From emipana at hcmr.gr  Mon Jul 27 10:21:48 2015
From: emipana at hcmr.gr (emiP)
Date: Mon, 27 Jul 2015 01:21:48 -0700 (PDT)
Subject: [R] Segmented model
Message-ID: <1437985308855-4710384.post@n4.nabble.com>

Hi to all,
I used segmented package for my analysis. All is OK, I can find the slope
and the intercept of each segment but I can't find the R2 (square root) of
each segment. Any idea?
Thank you in advance,
Emilia



--
View this message in context: http://r.789695.n4.nabble.com/Segmented-model-tp4710384.html
Sent from the R help mailing list archive at Nabble.com.


From peter.anthoni at kit.edu  Mon Jul 27 15:14:41 2015
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Mon, 27 Jul 2015 13:14:41 +0000
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
	<CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
Message-ID: <8A18FABE-B192-402A-9101-F3311F74A0A1@kit.edu>

Hi Cecilia,

Alternative solution

#...
afiles <- ls(pattern= "a_")

for (ifile in 1:length(afiles))
{
  fnp = mget(afiles[ifile])
#... do something with fnp

  outfile <- file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/", paste0("distances_", afiles[ifile], ".txt"));
#...
}

cheers
Peter



> On 27 Jul 2015, at 12:50, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Cecilia,
> I _think_ that the error is occurring in the call to paste0. You may
> be able to get what you want like this:
> 
> paste0("distances_",deparse(substitute(fnp)),".txt")
> 
> Jim
> 
> 
> On Mon, Jul 27, 2015 at 5:06 AM, Larrosa, Cecilia
> <cecilia.larrosa10 at imperial.ac.uk> wrote:
>> Hi,
>> 
>> This is a repost from here (http://r.789695.n4.nabble.com/Writing-output-of-a-looped-process-with-pdfs-tt4710348.html), due to the post not being complete originally. I am running R studio on OS X Yosemite 10.10.4 (Mac). I appreciate you help very much!
>> 
>> The objective: I have 100 shapefiles that need to undergo the same process.
>> 
>> The process: I use gDistance{rgdal} to calculate the distance between all features (polygons) within each layer, and output a txt file.
>> 
>> The problem: I need the name of the output txt file to contain the name of the shapefile, but the shapefiles are read into R as SpatialPolygonsDataFrames (spdf) and I cannot find a way to use the name of the spdf objects as character in order to make it vary with each iteration.
>> 
>> My questions to you: Do you know a way to solve the problem or an alternative way to fulfil the objective? I have come to determine the problem after searching about the error message, have I interpreted correctly?
>> 
>> 
>> Here is a minimal dataset for replicability:
>> 
>>> dput(a_1)
>> new("SpatialPolygonsDataFrame"
>>    , data = structure(list(ID = 1:3, GRIDCODE = c(1L, 1L, 1L), Shape_Leng = c(3349.48347556,
>> 1618.93904903, 893.268790786), Shape_Area = c(309430.38861, 90015.8325676,
>> 47507.0325775), Count = c(1L, 1L, 1L)), .Names = c("ID", "GRIDCODE",
>> "Shape_Leng", "Shape_Area", "Count"), row.names = 0:2, class = "data.frame")
>>    , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
>>    <S4 object of class structure("Polygons", package = "sp")>,
>>    <S4 object of class structure("Polygons", package = "sp")>)
>>    , plotOrder = 1:3
>>    , bbox = structure(c(476685.625393809, 311791.86152084, 508519.585393809,
>> 312935.41622084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
>> ), c("min", "max")))
>>    , proj4string = new("CRS"
>>    , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
>> )
>> )
>>> dput(a_10)
>> new("SpatialPolygonsDataFrame"
>>    , data = structure(list(ID = 1:5, GRIDCODE = c(1L, 1L, 1L, 1L, 1L), Shape_Leng = c(1691.7247095,
>> 2305.45647624, 1022.64650591, 1172.27848042, 94.2722341164),
>>    Shape_Area = c(6.47354525991, 92111.8528756, 65.7173995386,
>>    19042.7776647, 415.253663691), Count = c(1L, 1L, 1L, 1L,
>>    1L)), .Names = c("ID", "GRIDCODE", "Shape_Leng", "Shape_Area",
>> "Count"), row.names = 0:4, class = "data.frame")
>>    , polygons = list(<S4 object of class structure("Polygons", package = "sp")>,
>>    <S4 object of class structure("Polygons", package = "sp")>,
>>    <S4 object of class structure("Polygons", package = "sp")>,
>>    <S4 object of class structure("Polygons", package = "sp")>,
>>    <S4 object of class structure("Polygons", package = "sp")>)
>>    , plotOrder = c(2L, 4L, 5L, 3L, 1L)
>>    , bbox = structure(c(825796.904693809, 815666.86152084, 831270.106493809,
>> 816562.46752084), .Dim = c(2L, 2L), .Dimnames = list(c("x", "y"
>> ), c("min", "max")))
>>    , proj4string = new("CRS"
>>    , projargs = "+proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0 +y_0=0 +ellps=aust_SA +units=m +no_defs"
>> )
>> )
>> 
>> 
>> Here is the code that I have been using:
>> 
>> ###Load packages
>> library(rgdal)
>> library(gdistance)
>> 
>> ###Read forest shape files
>> setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/R_Quest/")
>> shps<- dir(getwd(), "*.shp")
>> shps <- gsub('.{4}$', '', shps)
>> for (shp in shps) assign(shp, readOGR(".",layer=shp))
>> 
>> ###Create list of spdf objects
>> fnps<- mget(ls(pattern= "a_"))
>> 
>> ###For each spatial layer (object in the list), calculate distance between all polygons within layer
>> for (fnp in fnps)
>> {
>>  distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
>>  row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);        # did this because gDistance changed the IDs of the features from [1 to ...] to [0 to ...], not sure why
>>  colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?);            # same as above
>>  dists.melt <- melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,];  #use only lower triangle of the distances matrix
>>  outfile <- file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/", paste0("distances_", fnp, ".txt"));
>>  write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
>> }
>> 
>> And this is the error message:
>> 
>> Error in as.character.default(<S4 object of class "SpatialPolygonsDataFrame">) :
>>  no method for coercing this S4 class to a vector
>> 
>> Thank you very much!!
>> Cecilia
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From j.para.fernandez at hotmail.com  Mon Jul 27 15:52:25 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Mon, 27 Jul 2015 06:52:25 -0700 (PDT)
Subject: [R] Type of variable
Message-ID: <1438005145912-4710395.post@n4.nabble.com>

I have a dataframe like this one:

one     two    three
3.2      2.5     a
3.5      2.3     a
3.7      2.2     b

How can I implment and If that detects the type of data that the variables
one, two and three has?

Thanks!!!



--
View this message in context: http://r.789695.n4.nabble.com/Type-of-variable-tp4710395.html
Sent from the R help mailing list archive at Nabble.com.


From j.para.fernandez at hotmail.com  Mon Jul 27 15:55:11 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Mon, 27 Jul 2015 06:55:11 -0700 (PDT)
Subject: [R] Type of variable
In-Reply-To: <1438005145912-4710395.post@n4.nabble.com>
References: <1438005145912-4710395.post@n4.nabble.com>
Message-ID: <1438005311329-4710396.post@n4.nabble.com>

Ok, stupid question. 

is.factor is the solution.

Thanks!



--
View this message in context: http://r.789695.n4.nabble.com/Type-of-variable-tp4710395p4710396.html
Sent from the R help mailing list archive at Nabble.com.


From marammagdysalem at gmail.com  Mon Jul 27 16:07:07 2015
From: marammagdysalem at gmail.com (Maram SAlem)
Date: Mon, 27 Jul 2015 16:07:07 +0200
Subject: [R] Generating Progressively censored samples
Message-ID: <CAPLSCn3yQtN+gNz9FkWr7qo-ttNjEc1o=BQMq_EocH4OB=oSvQ@mail.gmail.com>

Dear All,

Is there some built-in function in R that can be used to generate
progressively censored sample from a certain distribution, for example, the
Weibull distribution? OR Do I have to write the code of the algorithm
myself?

Thanks for helping.
Maram Salem

	[[alternative HTML version deleted]]


From sally.cmp at gmail.com  Mon Jul 27 15:57:19 2015
From: sally.cmp at gmail.com (Sally Chan)
Date: Mon, 27 Jul 2015 14:57:19 +0100
Subject: [R] Multilevel mediated moderation
Message-ID: <CAOp9oGCGiCUPky7M_uvoYqKda9eVNPd8mBC0JQ7mOn=HjZqZ=Q@mail.gmail.com>

Dear All,

I want to test a multilevel/cross-level mediated moderation model (Level 1:
IV, DV; and Level 2: Mod, Med). The dataset can be grouped by firm_id and I
use mediate{mediation} with lmer class to do it...

Can anyone suggest if the following models are specified correctly?
I don't know whether a Level 2 variable can be specified in a lmer
model...any idea?
mod.m <- lmer(Med ~ IV*Mod + (IV | firm_id), data, REML=FALSE)
mod.y <- lmer(DV ~ Med*IV*Mod + (IV | firm_id), data, REML=FALSE)

Thanks so much for your help!!!

Best regards,
Sally

	[[alternative HTML version deleted]]


From liuning.1982 at qq.com  Mon Jul 27 16:10:54 2015
From: liuning.1982 at qq.com (=?ISO-8859-1?B?TGl1TmluZw==?=)
Date: Mon, 27 Jul 2015 22:10:54 +0800
Subject: [R] Generating Progressively censored samples
In-Reply-To: <CAPLSCn3yQtN+gNz9FkWr7qo-ttNjEc1o=BQMq_EocH4OB=oSvQ@mail.gmail.com>
References: <CAPLSCn3yQtN+gNz9FkWr7qo-ttNjEc1o=BQMq_EocH4OB=oSvQ@mail.gmail.com>
Message-ID: <tencent_283A2CC36507AE982425CCD5@qq.com>

try these:




dweibull(x, shape, scale = 1, log = FALSE)
pweibull(q, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
qweibull(p, shape, scale = 1, lower.tail = TRUE, log.p = FALSE)
rweibull(n, shape, scale = 1)



------------------ Original ------------------
From:  "Maram SAlem";<marammagdysalem at gmail.com>;
Date:  Mon, Jul 27, 2015 10:07 PM
To:  "r-help"<r-help at r-project.org>; 

Subject:  [R] Generating Progressively censored samples



Dear All,

Is there some built-in function in R that can be used to generate
progressively censored sample from a certain distribution, for example, the
Weibull distribution? OR Do I have to write the code of the algorithm
myself?

Thanks for helping.
Maram Salem

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
	[[alternative HTML version deleted]]


From ignacio82 at gmail.com  Mon Jul 27 17:21:51 2015
From: ignacio82 at gmail.com (Ignacio Martinez)
Date: Mon, 27 Jul 2015 15:21:51 +0000
Subject: [R] R package with Fortran module on Windows? undefined reference
	to `__stack_chk_fail'
Message-ID: <CAJA1VFzMBib=0zO=HZEHUvVUrchpMUgG4YAVKPiPSKc9qHwgFw@mail.gmail.com>

Hi,

I created a R library that uses a Fortran module. Everything works like a
charm on linux.

Now I'm trying to make it work on Windows. I cloned my git repository
<https://github.com/ignacio82/MyPi> on a windows computer, and when I press
the build and reload button on Rstudio I get these errors:

==> Rcmd.exe INSTALL --no-multiarch --with-keep.source MyPi
* installing to library
'C:/Users/IMartinez/Documents/R/R-3.2.1/library'* installing *source*
package 'MyPi' ...** libs
gfortran -m64 -shared -s -static-libgcc -o MyPi.dll tmp.def Fpi.o
-Ld:/RCompile/r-compiling/local/local320/lib/x64
-Ld:/RCompile/r-compiling/local/local320/lib
-LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR
Fpi.o: In function `__fortranpi_MOD_dboard':
Fpi.f90:(.text+0xd7): undefined reference to `__stack_chk_fail'
Fpi.o: In function `pi_':
Fpi.f90:(.text+0x249): undefined reference to `__stack_chk_fail'
collect2: ld returned 1 exit status
no DLL was created
ERROR: compilation failed for package 'MyPi'* removing
'C:/Users/IMartinez/Documents/R/R-3.2.1/library/MyPi'

Exited with status 1.


This is  the Fortran code:


Module Fortranpi
IMPLICIT NONE
contains
subroutine dboard(darts, dartsscore)
  integer, intent(in)                    :: darts
  double precision, intent(out)          :: dartsscore
  double precision                       :: x_coord, y_coord
  integer                                :: score, n

score = 0
do n = 1, darts
  call random_number(x_coord)
  call random_number(y_coord)

  if ((x_coord**2 + y_coord**2) <= 1.0d0) then
  score = score + 1
  end if
end do

dartsscore = 4.0d0*score/darts

end subroutine dboard

subroutine pi(avepi, DARTS, ROUNDS) bind(C, name="pi_")
  use, intrinsic                         :: iso_c_binding, only :
c_double, c_int
  real(c_double), intent(out)            ::  avepi
  integer(c_int), intent(in)             ::  DARTS, ROUNDS
  integer                                ::  MASTER, rank, i, n
  integer, allocatable                   ::  seed(:)
  double precision                       ::  pi_est, homepi, pirecv, pisum
! we set it to zero in the sequential run
rank = 0! initialize the random number generator! we make sure the
seed is different for each task
call random_seed()
call random_seed(size = n)
allocate(seed(n))
seed = 12 + rank*11
call random_seed(put=seed(1:n))
deallocate(seed)

avepi = 0
do i = 0, ROUNDS-1
  call dboard(darts, pi_est)
  ! calculate the average value of pi over all iterations
  avepi = ((avepi*i) + pi_est)/(i + 1)
end do
end subroutine pi

end module Fortranpi


I tried adding <http://i.stack.imgur.com/lC82X.png> -fno-stack-protector
-lssp but it did not help.

I also tried doing this "by hand" <http://i.stack.imgur.com/WY4VD.png> and
I get these errors:


> system("R CMD SHLIB -fno-stack-protector -lssp ./src/Fpi.f90")
gfortran -m64 -shared -s -static-libgcc -o src/Fpi.dll tmp.def
./src/Fpi.o -fno-stack-protector -lssp
-Ld:/RCompile/r-compiling/local/local320/lib/x64
-Ld:/RCompile/r-compiling/local/local320/lib
-LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR>
dyn.load("./src/Fpi.dll")
Error in inDL(x, as.logical(local), as.logical(now), ...) :
  unable to load shared object 'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
  LoadLibrary failure:  %1 is not a valid Win32 application.
'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
  LoadLibrary failure:  %1 is not a valid Win32 application.


Thanks for the help!


Ignacio


PS: I posted this question in stackoverflow with no luck.
<http://stackoverflow.com/questions/31638934/r-package-with-fortran-module-on-windows-undefined-reference-to-stack-chk-fa>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Jul 27 17:57:50 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 27 Jul 2015 15:57:50 +0000
Subject: [R] Sample Weights
In-Reply-To: <FEB0CADC-83FB-4DBC-B6EC-8195C135E523@dcn.davis.CA.us>
References: <C52BA23A-D6EE-4163-80D6-705A47CE310D@dcn.davis.CA.us>
	<549596576.1945472.1437979051201.JavaMail.yahoo@mail.yahoo.com>
	<FEB0CADC-83FB-4DBC-B6EC-8195C135E523@dcn.davis.CA.us>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C37042@SRVEXCHMBX.precheza.cz>

Hi

<snip>

> <amelia_marsh08 at yahoo.com> wrote:
> >Dear Sir,
> >
> >I do appreciate your views. Yes even I was also aware about the non
> >clarity in the question. Actaully, I have a large data having lots of
> >data of low magnitude and few of very high magnitude. In order to

can you explain what is low or high magnitude? Or better to use

dput(head(yourdata, 20))

and copy the result so as we might see better what you mean.

Just a guess, from your previous post are you iterested in some summary for your data?

> x<-c(6,6,5,6,4,6,8,4,6,6,6,3,4,6,5,7,7,4,3,5,5,5,3,6,7,4,4,7,4,3,4,6,4,6,5,4,4,7,6,8,5,6,5,5,8,2,3,5,7,5)
> table(x)
x
 2  3  4  5  6  7  8
 1  5 11 11 13  6  3

Cheers
Petr


> >analyse the same, some very senior person in the office suggested me
> to
> >assign weights to these obervations.
> >
> >Problem is this senior person is on office tour travelling into
> >Australia and unfortutely I can't even think of reverting to him owing
> >to his seniority. I tried to find through some R libraries if I can
> get
> >something about sample weights.
> >
> >But as you have pointed out, yes my question is too vague and feel
> >really sorry about the same. And I have recently started with R
> >language hence I am trying to learn some basics about R. Thanks for
> >pointing the difference between data.frame and vector.
> >
> >
> >Thanks again for your response.
> >
> >Regards
> >
> >Amelia
> >
> >______________________________________________________________________
> _
> >______________________
> >
> >
> >
> >On Monday, 27 July 2015 11:49 AM, Jeff Newmiller
> ><jdnewmil at dcn.davis.CA.us> wrote:
> >If you have a clear idea what meaning those weights have (?) in the
> >context of a specific calculation (?), and you know what the weights
> >are (?), then it is usually trivially easy to do in R. However,  your
> >question is vague on all of those points, so offering you a solution
> >seems like an invitation for you to mis-use any particular solution
> >offered. Please try to clarify what you are doing that "weights" will
> >help with, and yes, there may just be a weights argument to the
> >function that does that analysis that we can point you to.
> >
> >You also are unclear what the difference between a data frame and a
> >vector is... if it helps, a data frame is a list of vectors (typically
> >referred to as "columns") all with the same length...  your "mydat" is
> >a vector, not a data frame.
> >----------------------------------------------------------------------
> -----
> >Jeff Newmiller                        The     .....       .....  Go
> >Live...
> >DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> >Go...
> >                                     Live:   OO#.. Dead: OO#..
> Playing
> >Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> >/Software/Embedded Controllers)               .OO#.       .OO#.
> >rocks...1k
> >----------------------------------------------------------------------
> -
> >----
> >
> >Sent from my phone. Please excuse my brevity.
> >
> >
> >On July 26, 2015 11:00:54 PM PDT, Amelia Marsh via R-help
> ><r-help at r-project.org> wrote:
> >>Dear R Forum
> >>
> >>I have a data.frame as
> >>
> >>
> >>mydat =
> >>c(6,6,5,6,4,6,8,4,6,6,6,3,4,6,5,7,7,4,3,5,5,5,3,6,7,4,4,7,4,3,4,6,4,6
> ,
> >>5,4,4,7,6,8,5,6,5,5,8,2,3,5,7,5)
> >>
> >>Is there any library or way in R to allocate weights to these values?
> >>Actually I am having a large data, but for illustrative purpose, have
> >>considered just a small part of the same.
> >>
> >>Regards
> >>
> >>Amelia
> >>
> >>______________________________________________
> >>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jdnewmil at dcn.davis.CA.us  Mon Jul 27 17:58:50 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Mon, 27 Jul 2015 10:58:50 -0500
Subject: [R] R package with Fortran module on Windows? undefined
	reference	to `__stack_chk_fail'
In-Reply-To: <CAJA1VFzMBib=0zO=HZEHUvVUrchpMUgG4YAVKPiPSKc9qHwgFw@mail.gmail.com>
References: <CAJA1VFzMBib=0zO=HZEHUvVUrchpMUgG4YAVKPiPSKc9qHwgFw@mail.gmail.com>
Message-ID: <E015A78D-7373-4D14-930D-C0061950CB47@dcn.davis.CA.us>

You went to all that trouble to find a mailing list to ask your question on and failed to find R-packages or R-devel?
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 27, 2015 10:21:51 AM CDT, Ignacio Martinez <ignacio82 at gmail.com> wrote:
>Hi,
>
>I created a R library that uses a Fortran module. Everything works like
>a
>charm on linux.
>
>Now I'm trying to make it work on Windows. I cloned my git repository
><https://github.com/ignacio82/MyPi> on a windows computer, and when I
>press
>the build and reload button on Rstudio I get these errors:
>
>==> Rcmd.exe INSTALL --no-multiarch --with-keep.source MyPi
>* installing to library
>'C:/Users/IMartinez/Documents/R/R-3.2.1/library'* installing *source*
>package 'MyPi' ...** libs
>gfortran -m64 -shared -s -static-libgcc -o MyPi.dll tmp.def Fpi.o
>-Ld:/RCompile/r-compiling/local/local320/lib/x64
>-Ld:/RCompile/r-compiling/local/local320/lib
>-LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR
>Fpi.o: In function `__fortranpi_MOD_dboard':
>Fpi.f90:(.text+0xd7): undefined reference to `__stack_chk_fail'
>Fpi.o: In function `pi_':
>Fpi.f90:(.text+0x249): undefined reference to `__stack_chk_fail'
>collect2: ld returned 1 exit status
>no DLL was created
>ERROR: compilation failed for package 'MyPi'* removing
>'C:/Users/IMartinez/Documents/R/R-3.2.1/library/MyPi'
>
>Exited with status 1.
>
>
>This is  the Fortran code:
>
>
>Module Fortranpi
>IMPLICIT NONE
>contains
>subroutine dboard(darts, dartsscore)
>  integer, intent(in)                    :: darts
>  double precision, intent(out)          :: dartsscore
>  double precision                       :: x_coord, y_coord
>  integer                                :: score, n
>
>score = 0
>do n = 1, darts
>  call random_number(x_coord)
>  call random_number(y_coord)
>
>  if ((x_coord**2 + y_coord**2) <= 1.0d0) then
>  score = score + 1
>  end if
>end do
>
>dartsscore = 4.0d0*score/darts
>
>end subroutine dboard
>
>subroutine pi(avepi, DARTS, ROUNDS) bind(C, name="pi_")
>  use, intrinsic                         :: iso_c_binding, only :
>c_double, c_int
>  real(c_double), intent(out)            ::  avepi
>  integer(c_int), intent(in)             ::  DARTS, ROUNDS
>  integer                                ::  MASTER, rank, i, n
>  integer, allocatable                   ::  seed(:)
>double precision                       ::  pi_est, homepi, pirecv,
>pisum
>! we set it to zero in the sequential run
>rank = 0! initialize the random number generator! we make sure the
>seed is different for each task
>call random_seed()
>call random_seed(size = n)
>allocate(seed(n))
>seed = 12 + rank*11
>call random_seed(put=seed(1:n))
>deallocate(seed)
>
>avepi = 0
>do i = 0, ROUNDS-1
>  call dboard(darts, pi_est)
>  ! calculate the average value of pi over all iterations
>  avepi = ((avepi*i) + pi_est)/(i + 1)
>end do
>end subroutine pi
>
>end module Fortranpi
>
>
>I tried adding <http://i.stack.imgur.com/lC82X.png>
>-fno-stack-protector
>-lssp but it did not help.
>
>I also tried doing this "by hand" <http://i.stack.imgur.com/WY4VD.png>
>and
>I get these errors:
>
>
>> system("R CMD SHLIB -fno-stack-protector -lssp ./src/Fpi.f90")
>gfortran -m64 -shared -s -static-libgcc -o src/Fpi.dll tmp.def
>./src/Fpi.o -fno-stack-protector -lssp
>-Ld:/RCompile/r-compiling/local/local320/lib/x64
>-Ld:/RCompile/r-compiling/local/local320/lib
>-LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR>
>dyn.load("./src/Fpi.dll")
>Error in inDL(x, as.logical(local), as.logical(now), ...) :
>unable to load shared object
>'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
>  LoadLibrary failure:  %1 is not a valid Win32 application.
>'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
>  LoadLibrary failure:  %1 is not a valid Win32 application.
>
>
>Thanks for the help!
>
>
>Ignacio
>
>
>PS: I posted this question in stackoverflow with no luck.
><http://stackoverflow.com/questions/31638934/r-package-with-fortran-module-on-windows-undefined-reference-to-stack-chk-fa>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Mon Jul 27 18:07:06 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 27 Jul 2015 16:07:06 +0000
Subject: [R] Type of variable
In-Reply-To: <1438005311329-4710396.post@n4.nabble.com>
References: <1438005145912-4710395.post@n4.nabble.com>
	<1438005311329-4710396.post@n4.nabble.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C3708F@SRVEXCHMBX.precheza.cz>

Hi

sapply(yourdata, class)

would be probably quicker then to ask all variables one by one.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of jpara3
> Sent: Monday, July 27, 2015 3:55 PM
> To: r-help at r-project.org
> Subject: Re: [R] Type of variable
>
> Ok, stupid question.
>
> is.factor is the solution.
>
> Thanks!
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Type-of-
> variable-tp4710395p4710396.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From cecilia.larrosa10 at imperial.ac.uk  Mon Jul 27 18:26:18 2015
From: cecilia.larrosa10 at imperial.ac.uk (SisoL)
Date: Mon, 27 Jul 2015 09:26:18 -0700 (PDT)
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
	<CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
Message-ID: <1438014378037-4710406.post@n4.nabble.com>

Hi Jim,

I've tried your suggestion but it just prints "fnp" and not the iterating
names (a_1, a_10, etc). Any other suggestions?

Thank you very much

Siso



--
View this message in context: http://r.789695.n4.nabble.com/Varying-name-of-output-tables-from-looped-process-of-list-of-spdf-objects-tp4710369p4710406.html
Sent from the R help mailing list archive at Nabble.com.


From cecilia.larrosa10 at imperial.ac.uk  Mon Jul 27 18:33:42 2015
From: cecilia.larrosa10 at imperial.ac.uk (SisoL)
Date: Mon, 27 Jul 2015 09:33:42 -0700 (PDT)
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <8A18FABE-B192-402A-9101-F3311F74A0A1@kit.edu>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
	<CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
	<8A18FABE-B192-402A-9101-F3311F74A0A1@kit.edu>
Message-ID: <1438014822138-4710408.post@n4.nabble.com>

Hi Peter,

Thank you for your reply. The method for looping seems to work, but
gDistance will not recognise the input. I am puzzled because when I
print(fnp), and print (a_10) they look exactly the same, but when I try to
run gDistance{rgeos} with a_10 it works, but with fnp it throws an error.
Please see below. Any other suggestions?

>afiles <- ls(pattern= "a_") #OK
>print(afiles)
[1] "a_1"  "a_10"

>fnp<- mget(afiles[ifile]) #OK

> print(fnp)
$a_10
class       : SpatialPolygonsDataFrame 
features    : 5 
extent      : 825796.9, 831270.1, 815666.9, 816562.5  (xmin, xmax, ymin,
ymax)
coord. ref. : +proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0
+y_0=0 +ellps=aust_SA +units=m +no_defs 
variables   : 5
names       : ID, GRIDCODE,    Shape_Leng,    Shape_Area, Count 
min values  :  1,        1, 94.2722341164, 6.47354525991,     1 
max values  :  5,        1, 2305.45647624, 92111.8528756,     1 

> print(a_10)
class       : SpatialPolygonsDataFrame 
features    : 5 
extent      : 825796.9, 831270.1, 815666.9, 816562.5  (xmin, xmax, ymin,
ymax)
coord. ref. : +proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0
+y_0=0 +ellps=aust_SA +units=m +no_defs 
variables   : 5
names       : ID, GRIDCODE,    Shape_Leng,    Shape_Area, Count 
min values  :  1,        1, 94.2722341164, 6.47354525991,     1 
max values  :  5,        1, 2305.45647624, 92111.8528756,     1   

>distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T)
Error in (function (classes, fdef, mtable)  : 
  unable to find an inherited method for function ?is.projected? for
signature ?"list"?




--
View this message in context: http://r.789695.n4.nabble.com/Varying-name-of-output-tables-from-looped-process-of-list-of-spdf-objects-tp4710369p4710408.html
Sent from the R help mailing list archive at Nabble.com.


From lid.zigh at gmail.com  Mon Jul 27 19:53:36 2015
From: lid.zigh at gmail.com (Lida Zeighami)
Date: Mon, 27 Jul 2015 12:53:36 -0500
Subject: [R] Reading some csv files from different folders and add the
 name of each files to the first column of files
In-Reply-To: <CANkFkEefVQrNotZbsv12SqFQJ4G2N_HhqCLHyAv5gSW-9w9_wA@mail.gmail.com>
References: <CANkFkEefVQrNotZbsv12SqFQJ4G2N_HhqCLHyAv5gSW-9w9_wA@mail.gmail.com>
Message-ID: <CAMqbV1B5QReQDwtxdmkSP6Zi1c3i7TAhS=J=9f5BjbEHdxeW7g@mail.gmail.com>

Dear Arnaud,

Thank you so much for your reply! It works great!

On Sun, Jul 26, 2015 at 6:04 PM, Arnaud Mosnier <a.mosnier at gmail.com> wrote:

> Hi Lida,
>
> You can try this:
>
> d<- choose.dir() # choose the folder with the subdirectories containing
> the csv files
> f <- list.files(d, full.names = TRUE, recursive = TRUE)
>
> # Here the example for the "sing" files
> selsing <- grep("sing",f) #Select the files notaining the word sing
>
> allsing <- data.frame() #Create an empty data frame
>
> # Loop among the selected files
> for (i in 1:length(f[selsing])){
>   dat <- read.csv(file=f[selsing][i])  # suppose that the csv files have a
> header
>   allsing <- rbind(allsing,data.frame(FileID =
> gsub("EA_sing_|.txt","",basename(f[selsing][i])), dat)) #Combine the file
> ID with the other columns and add the result to the all sing object
> }
>
> Now, you just have to do it for the other cases and save your final object.
>
> Hope this help !
>
> Arnaud
>
> ###########################################################################
>
> Date: Sat, 25 Jul 2015 15:03:21 -0500
> From: Lida Zeighami <lid.zigh at gmail.com>
> To: r-help at r-project.org
> Subject: [R] Reading some csv files from different folders and add the
>         name of each files to the first column of files
> Message-ID:
>         <CAMqbV1CXN0=
> R+HxmaYS40sMymGdbhDQK1V+n5bHmBPgBr3M2Yg at mail.gmail.com>
> Content-Type: text/plain; charset="UTF-8"
>
>
> I have 600 folders in which there are 3 csv files. The name of folders are
> as follows: EA_aa, EA_bb, EA_cc, EA_dd,....
>
> In each folder there are 3 csv files:
> in folder EA_aa there are:
>   EA_sing_aa.csvqwerty
>   EA_ska_aa.csv
>   EA_tat_aa.csv
>
> In folder EA_bb:
>   EA_sing_bb.csv
>   EA_ska_bb.csv
>   EA_tat_bb.csv
> ...
>
> I need to read all the same kind of files (for example sing files) in one
> dataframe but before that I should add a column to each files with the name
> of file as a row names!
>
> So in output I should have just 3 csv files such as
> EA_sing.csv: (rbind all EA_sing_*.csv file)
> EA_ska.csv: (rbind all EA_ska_*.csv files)
> EA_tat.csv: (rbind all EA_tat_*.csv files)
>
> And in each file the first column should be added as row names which
> containing the file name! So the format will be as follow:
> EA_sing.csv:
> 1st column   2nd column  3rd column
>    aa                    yhhh                ghj
>    aa                     k ki                   Fyh
>    bb                     k ki                   vgd
>    bb                     k gki                  Fyh
>    bb                    k reci                  Fyh
>    cc                     k hcd                  hyd
>    dd                     lmb                     Fyh
>
> EA_ska.csv:
> 1st column   2nd column  3rd column
>    aa                    yhhh                ghj
>    aa                     k ki                   Fyh
>    bb                     k ki                   Fyh
>    cc                     k gki                  Fyh
>    cc                     k reci                  oki
>    cc                     k hcd                  Fyh
>    dd                     lmb                     dsf
>
> EA_tat.csv:
> 1st column   2nd column  3rd column
>    aa                    yhhh                ghj
>    aa                     k ki                   Fyh
>    bb                     k ki                   Fyh
>    cc                     k gki                  Fyh
>    cc                     k reci                  oki
>    dd                     k hcd                  Fyh
>    dd                     lmb                     dsf
>
> Would you please help me how to can I do that?
> Thanks
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Mon Jul 27 19:55:17 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 27 Jul 2015 19:55:17 +0200
Subject: [R] R package with Fortran module on Windows? undefined
	reference to `__stack_chk_fail'
In-Reply-To: <E015A78D-7373-4D14-930D-C0061950CB47@dcn.davis.CA.us>
References: <CAJA1VFzMBib=0zO=HZEHUvVUrchpMUgG4YAVKPiPSKc9qHwgFw@mail.gmail.com>
	<E015A78D-7373-4D14-930D-C0061950CB47@dcn.davis.CA.us>
Message-ID: <FADC9373-4505-454C-9D55-38E65852BE36@gmail.com>

You went through the trouble to chastise a beginner and failed to spot the difference between R-packages and R-package-devel?

(I think R-devel is better for now. R-package-devel is more for people trying to make CRAN happy.)

-pd

PS: The by-hand example suggests that there is a 32/64 bit mixup.

> On 27 Jul 2015, at 17:58 , Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> You went to all that trouble to find a mailing list to ask your question on and failed to find R-packages or R-devel?
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> --------------------------------------------------------------------------- 
> Sent from my phone. Please excuse my brevity.
> 
> On July 27, 2015 10:21:51 AM CDT, Ignacio Martinez <ignacio82 at gmail.com> wrote:
>> Hi,
>> 
>> I created a R library that uses a Fortran module. Everything works like
>> a
>> charm on linux.
>> 
>> Now I'm trying to make it work on Windows. I cloned my git repository
>> <https://github.com/ignacio82/MyPi> on a windows computer, and when I
>> press
>> the build and reload button on Rstudio I get these errors:
>> 
>> ==> Rcmd.exe INSTALL --no-multiarch --with-keep.source MyPi
>> * installing to library
>> 'C:/Users/IMartinez/Documents/R/R-3.2.1/library'* installing *source*
>> package 'MyPi' ...** libs
>> gfortran -m64 -shared -s -static-libgcc -o MyPi.dll tmp.def Fpi.o
>> -Ld:/RCompile/r-compiling/local/local320/lib/x64
>> -Ld:/RCompile/r-compiling/local/local320/lib
>> -LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR
>> Fpi.o: In function `__fortranpi_MOD_dboard':
>> Fpi.f90:(.text+0xd7): undefined reference to `__stack_chk_fail'
>> Fpi.o: In function `pi_':
>> Fpi.f90:(.text+0x249): undefined reference to `__stack_chk_fail'
>> collect2: ld returned 1 exit status
>> no DLL was created
>> ERROR: compilation failed for package 'MyPi'* removing
>> 'C:/Users/IMartinez/Documents/R/R-3.2.1/library/MyPi'
>> 
>> Exited with status 1.
>> 
>> 
>> This is  the Fortran code:
>> 
>> 
>> Module Fortranpi
>> IMPLICIT NONE
>> contains
>> subroutine dboard(darts, dartsscore)
>> integer, intent(in)                    :: darts
>> double precision, intent(out)          :: dartsscore
>> double precision                       :: x_coord, y_coord
>> integer                                :: score, n
>> 
>> score = 0
>> do n = 1, darts
>> call random_number(x_coord)
>> call random_number(y_coord)
>> 
>> if ((x_coord**2 + y_coord**2) <= 1.0d0) then
>> score = score + 1
>> end if
>> end do
>> 
>> dartsscore = 4.0d0*score/darts
>> 
>> end subroutine dboard
>> 
>> subroutine pi(avepi, DARTS, ROUNDS) bind(C, name="pi_")
>> use, intrinsic                         :: iso_c_binding, only :
>> c_double, c_int
>> real(c_double), intent(out)            ::  avepi
>> integer(c_int), intent(in)             ::  DARTS, ROUNDS
>> integer                                ::  MASTER, rank, i, n
>> integer, allocatable                   ::  seed(:)
>> double precision                       ::  pi_est, homepi, pirecv,
>> pisum
>> ! we set it to zero in the sequential run
>> rank = 0! initialize the random number generator! we make sure the
>> seed is different for each task
>> call random_seed()
>> call random_seed(size = n)
>> allocate(seed(n))
>> seed = 12 + rank*11
>> call random_seed(put=seed(1:n))
>> deallocate(seed)
>> 
>> avepi = 0
>> do i = 0, ROUNDS-1
>> call dboard(darts, pi_est)
>> ! calculate the average value of pi over all iterations
>> avepi = ((avepi*i) + pi_est)/(i + 1)
>> end do
>> end subroutine pi
>> 
>> end module Fortranpi
>> 
>> 
>> I tried adding <http://i.stack.imgur.com/lC82X.png>
>> -fno-stack-protector
>> -lssp but it did not help.
>> 
>> I also tried doing this "by hand" <http://i.stack.imgur.com/WY4VD.png>
>> and
>> I get these errors:
>> 
>> 
>>> system("R CMD SHLIB -fno-stack-protector -lssp ./src/Fpi.f90")
>> gfortran -m64 -shared -s -static-libgcc -o src/Fpi.dll tmp.def
>> ./src/Fpi.o -fno-stack-protector -lssp
>> -Ld:/RCompile/r-compiling/local/local320/lib/x64
>> -Ld:/RCompile/r-compiling/local/local320/lib
>> -LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR>
>> dyn.load("./src/Fpi.dll")
>> Error in inDL(x, as.logical(local), as.logical(now), ...) :
>> unable to load shared object
>> 'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
>> LoadLibrary failure:  %1 is not a valid Win32 application.
>> 'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
>> LoadLibrary failure:  %1 is not a valid Win32 application.
>> 
>> 
>> Thanks for the help!
>> 
>> 
>> Ignacio
>> 
>> 
>> PS: I posted this question in stackoverflow with no luck.
>> <http://stackoverflow.com/questions/31638934/r-package-with-fortran-module-on-windows-undefined-reference-to-stack-chk-fa>
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sarah.goslee at gmail.com  Mon Jul 27 20:11:19 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 27 Jul 2015 14:11:19 -0400
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <1438014822138-4710408.post@n4.nabble.com>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
	<CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
	<8A18FABE-B192-402A-9101-F3311F74A0A1@kit.edu>
	<1438014822138-4710408.post@n4.nabble.com>
Message-ID: <CAM_vjunGJZygrxGZ=wiTAg1iTH=sODGAX+aY-JVRE83NU+XVzw@mail.gmail.com>

Ah, but they don't look exactly the same. Look closer at the first
line of the print() output.

You might also look at

class(fnp)
class(a_10)

for a clue.

What happens if you run:

distance.matrix<- gDistance(fnp[[1]], spgeom2= NULL, byid=T)

Sarah

On Mon, Jul 27, 2015 at 12:33 PM, SisoL
<cecilia.larrosa10 at imperial.ac.uk> wrote:
> Hi Peter,
>
> Thank you for your reply. The method for looping seems to work, but
> gDistance will not recognise the input. I am puzzled because when I
> print(fnp), and print (a_10) they look exactly the same, but when I try to
> run gDistance{rgeos} with a_10 it works, but with fnp it throws an error.
> Please see below. Any other suggestions?
>
>>afiles <- ls(pattern= "a_") #OK
>>print(afiles)
> [1] "a_1"  "a_10"
>
>>fnp<- mget(afiles[ifile]) #OK
>
>> print(fnp)
> $a_10
> class       : SpatialPolygonsDataFrame
> features    : 5
> extent      : 825796.9, 831270.1, 815666.9, 816562.5  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0
> +y_0=0 +ellps=aust_SA +units=m +no_defs
> variables   : 5
> names       : ID, GRIDCODE,    Shape_Leng,    Shape_Area, Count
> min values  :  1,        1, 94.2722341164, 6.47354525991,     1
> max values  :  5,        1, 2305.45647624, 92111.8528756,     1
>
>> print(a_10)
> class       : SpatialPolygonsDataFrame
> features    : 5
> extent      : 825796.9, 831270.1, 815666.9, 816562.5  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0
> +y_0=0 +ellps=aust_SA +units=m +no_defs
> variables   : 5
> names       : ID, GRIDCODE,    Shape_Leng,    Shape_Area, Count
> min values  :  1,        1, 94.2722341164, 6.47354525991,     1
> max values  :  5,        1, 2305.45647624, 92111.8528756,     1
>
>>distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T)
> Error in (function (classes, fdef, mtable)  :
>   unable to find an inherited method for function ?is.projected? for
> signature ?"list"?
>
-- 
Sarah Goslee
http://www.functionaldiversity.org


From ignacio82 at gmail.com  Mon Jul 27 20:04:39 2015
From: ignacio82 at gmail.com (Ignacio Martinez)
Date: Mon, 27 Jul 2015 18:04:39 +0000
Subject: [R] R package with Fortran module on Windows? undefined
 reference to `__stack_chk_fail'
In-Reply-To: <FADC9373-4505-454C-9D55-38E65852BE36@gmail.com>
References: <CAJA1VFzMBib=0zO=HZEHUvVUrchpMUgG4YAVKPiPSKc9qHwgFw@mail.gmail.com>
	<E015A78D-7373-4D14-930D-C0061950CB47@dcn.davis.CA.us>
	<FADC9373-4505-454C-9D55-38E65852BE36@gmail.com>
Message-ID: <CAJA1VFy9SP5cVRBqGExAf8v6yCp8ASJ1zh5CBYknx00bSZJtMg@mail.gmail.com>

:( I sent this to R-package... I hope people are not hating me for spamming
the lists... I will email R-devel

Thanks!

On Mon, Jul 27, 2015 at 1:55 PM peter dalgaard <pdalgd at gmail.com> wrote:

> You went through the trouble to chastise a beginner and failed to spot the
> difference between R-packages and R-package-devel?
>
> (I think R-devel is better for now. R-package-devel is more for people
> trying to make CRAN happy.)
>
> -pd
>
> PS: The by-hand example suggests that there is a 32/64 bit mixup.
>
> > On 27 Jul 2015, at 17:58 , Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> >
> > You went to all that trouble to find a mailing list to ask your question
> on and failed to find R-packages or R-devel?
> >
> ---------------------------------------------------------------------------
> > Jeff Newmiller                        The     .....       .....  Go
> Live...
> > DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
> >                                      Live:   OO#.. Dead: OO#..  Playing
> > Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> > /Software/Embedded Controllers)               .OO#.       .OO#.
> rocks...1k
> >
> ---------------------------------------------------------------------------
> > Sent from my phone. Please excuse my brevity.
> >
> > On July 27, 2015 10:21:51 AM CDT, Ignacio Martinez <ignacio82 at gmail.com>
> wrote:
> >> Hi,
> >>
> >> I created a R library that uses a Fortran module. Everything works like
> >> a
> >> charm on linux.
> >>
> >> Now I'm trying to make it work on Windows. I cloned my git repository
> >> <https://github.com/ignacio82/MyPi> on a windows computer, and when I
> >> press
> >> the build and reload button on Rstudio I get these errors:
> >>
> >> ==> Rcmd.exe INSTALL --no-multiarch --with-keep.source MyPi
> >> * installing to library
> >> 'C:/Users/IMartinez/Documents/R/R-3.2.1/library'* installing *source*
> >> package 'MyPi' ...** libs
> >> gfortran -m64 -shared -s -static-libgcc -o MyPi.dll tmp.def Fpi.o
> >> -Ld:/RCompile/r-compiling/local/local320/lib/x64
> >> -Ld:/RCompile/r-compiling/local/local320/lib
> >> -LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR
> >> Fpi.o: In function `__fortranpi_MOD_dboard':
> >> Fpi.f90:(.text+0xd7): undefined reference to `__stack_chk_fail'
> >> Fpi.o: In function `pi_':
> >> Fpi.f90:(.text+0x249): undefined reference to `__stack_chk_fail'
> >> collect2: ld returned 1 exit status
> >> no DLL was created
> >> ERROR: compilation failed for package 'MyPi'* removing
> >> 'C:/Users/IMartinez/Documents/R/R-3.2.1/library/MyPi'
> >>
> >> Exited with status 1.
> >>
> >>
> >> This is  the Fortran code:
> >>
> >>
> >> Module Fortranpi
> >> IMPLICIT NONE
> >> contains
> >> subroutine dboard(darts, dartsscore)
> >> integer, intent(in)                    :: darts
> >> double precision, intent(out)          :: dartsscore
> >> double precision                       :: x_coord, y_coord
> >> integer                                :: score, n
> >>
> >> score = 0
> >> do n = 1, darts
> >> call random_number(x_coord)
> >> call random_number(y_coord)
> >>
> >> if ((x_coord**2 + y_coord**2) <= 1.0d0) then
> >> score = score + 1
> >> end if
> >> end do
> >>
> >> dartsscore = 4.0d0*score/darts
> >>
> >> end subroutine dboard
> >>
> >> subroutine pi(avepi, DARTS, ROUNDS) bind(C, name="pi_")
> >> use, intrinsic                         :: iso_c_binding, only :
> >> c_double, c_int
> >> real(c_double), intent(out)            ::  avepi
> >> integer(c_int), intent(in)             ::  DARTS, ROUNDS
> >> integer                                ::  MASTER, rank, i, n
> >> integer, allocatable                   ::  seed(:)
> >> double precision                       ::  pi_est, homepi, pirecv,
> >> pisum
> >> ! we set it to zero in the sequential run
> >> rank = 0! initialize the random number generator! we make sure the
> >> seed is different for each task
> >> call random_seed()
> >> call random_seed(size = n)
> >> allocate(seed(n))
> >> seed = 12 + rank*11
> >> call random_seed(put=seed(1:n))
> >> deallocate(seed)
> >>
> >> avepi = 0
> >> do i = 0, ROUNDS-1
> >> call dboard(darts, pi_est)
> >> ! calculate the average value of pi over all iterations
> >> avepi = ((avepi*i) + pi_est)/(i + 1)
> >> end do
> >> end subroutine pi
> >>
> >> end module Fortranpi
> >>
> >>
> >> I tried adding <http://i.stack.imgur.com/lC82X.png>
> >> -fno-stack-protector
> >> -lssp but it did not help.
> >>
> >> I also tried doing this "by hand" <http://i.stack.imgur.com/WY4VD.png>
> >> and
> >> I get these errors:
> >>
> >>
> >>> system("R CMD SHLIB -fno-stack-protector -lssp ./src/Fpi.f90")
> >> gfortran -m64 -shared -s -static-libgcc -o src/Fpi.dll tmp.def
> >> ./src/Fpi.o -fno-stack-protector -lssp
> >> -Ld:/RCompile/r-compiling/local/local320/lib/x64
> >> -Ld:/RCompile/r-compiling/local/local320/lib
> >> -LC:/Users/IMARTI~1/DOCUME~1/R/R-32~1.1/bin/x64 -lR>
> >> dyn.load("./src/Fpi.dll")
> >> Error in inDL(x, as.logical(local), as.logical(now), ...) :
> >> unable to load shared object
> >> 'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
> >> LoadLibrary failure:  %1 is not a valid Win32 application.
> >> 'C:/Users/IMartinez/Projects/MyPi/./src/Fpi.dll':
> >> LoadLibrary failure:  %1 is not a valid Win32 application.
> >>
> >>
> >> Thanks for the help!
> >>
> >>
> >> Ignacio
> >>
> >>
> >> PS: I posted this question in stackoverflow with no luck.
> >> <
> http://stackoverflow.com/questions/31638934/r-package-with-fortran-module-on-windows-undefined-reference-to-stack-chk-fa
> >
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From cflynch at ncsu.edu  Mon Jul 27 20:35:06 2015
From: cflynch at ncsu.edu (Collin Lynch)
Date: Mon, 27 Jul 2015 14:35:06 -0400
Subject: [R] VIF threshold implying multicollinearity
In-Reply-To: <639DEAA621D.000001FBjrkrideau@inbox.com>
References: <cakyn3id0azk+g-ywwyf9b8he02mm=cmop6yqzrtnhy1k-h1npa@mail.gmail.com>
	<cae=6fxazrek-mnumdlqgxlxgpbhcbld-dzyfrthjsapku8wrzq@mail.gmail.com>
	<55B5A1AB.1030808@auckland.ac.nz>
	<639DEAA621D.000001FBjrkrideau@inbox.com>
Message-ID: <CAE=6FXaZ1asX9XimTsKX_dCqV9kuFOzmzm2fnrvKSO9YQQ0mHA@mail.gmail.com>

No actually it is a quiet good paper! :)

On Mon, Jul 27, 2015 at 8:14 AM, John Kane <jrkrideau at inbox.com> wrote:

> +1
> I, originally,  read it as a stringent criticism of the first paper.
>
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: r.turner at auckland.ac.nz
> > Sent: Mon, 27 Jul 2015 15:12:43 +1200
> > To: cflynch at ncsu.edu
> > Subject: Re: [R] VIF threshold implying multicollinearity
> >
> >
> > On 27/07/15 13:36, Collin Lynch wrote:
> >
> >> The following sources discuss the issues generally and may be a goof
> >> pointer to the literature ...
> >
> > <SNIP>
> >
> > I think that the foregoing merits fortune status! :-)
> >
> > cheers,
> >
> > Rolf
> >
> > --
> > Technical Editor ANZJS
> > Department of Statistics
> > University of Auckland
> > Phone: +64-9-373-7599 ext. 88276
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE ONLINE PHOTOSHARING - Share your photos online with your friends and
> family!
> Visit http://www.inbox.com/photosharing to find out more!
>
>
>

	[[alternative HTML version deleted]]


From chris.barker at barkerstats.com  Mon Jul 27 21:38:10 2015
From: chris.barker at barkerstats.com (Chris)
Date: Mon, 27 Jul 2015 19:38:10 +0000 (UTC)
Subject: [R] setwd() command on windows vs. linux/unix
In-Reply-To: <839891721.3232879.1438025798655.JavaMail.yahoo@mail.yahoo.com>
References: <839891721.3232879.1438025798655.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>


I have a script that runs correctly (no errors) on Windows, the first command is a "setwd(.....windows directory)"I installed the script on a Linux machine and changed the windows directory reference to the correct directory on the linux machine.when I ran the script I got a message that "cannot change working directory".?I confirmed I have read and write access to the directory.I mainly run R on windows infrequently use linux/unix and would appreciate suggestions.thanks in advance?Chris Barker, Ph.D.
Adjunct Associate Professor of Biostatistics - UIC-SPH




  
	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Mon Jul 27 21:38:22 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Mon, 27 Jul 2015 15:38:22 -0400
Subject: [R] "unfurling" rankings into a matrix of preferences
Message-ID: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>

I have 5 items in total (1:5), but I show a person only 4 items (1:4)
and ask this person to rank items 1:4 in terms of preferences (1 is
best, 2 is second best, 4 is worst), and I get a vector of ranks:
ranks <- c(2,4,3,1)

# That means that this person liked item 4 best and item 2 worst.

I would like to "unfirl" this vector of ranks into a matrix of
preferences where if the row item prefers the column item, then it's a
1. Otherwise, it's a zero. So, the output should be a 5 by 5 matrix
(because overall we have 5 items, not 4, but item 5 did not
participate in rankings), and it would always have zeros in a
diagonal.:

0    1    1    0 NA
0    0    0    0 NA
0    1    0    0 NA
1    1    1    0 NA
NA NA NA NA 0

I can loop through all possible pairs the person saw and fill the
matrix accordingly, but it seems like a lot of looping. Could one do
it in a more elegant way?

Thank you very much!


-- 
Dimitri Liakhovitski


From jholtman at gmail.com  Mon Jul 27 21:52:12 2015
From: jholtman at gmail.com (jim holtman)
Date: Mon, 27 Jul 2015 15:52:12 -0400
Subject: [R] setwd() command on windows vs. linux/unix
In-Reply-To: <2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>
References: <839891721.3232879.1438025798655.JavaMail.yahoo@mail.yahoo.com>
	<2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAAxdm-5-PS7uecOaiU8QczO3q0gkpDkQcf502U+9aush=AvT-g@mail.gmail.com>

Try 'choose.dir()' to see if you can navigate to the given directory, or
take baby steps by doing one directory at a time.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Mon, Jul 27, 2015 at 3:38 PM, Chris <chris.barker at barkerstats.com> wrote:

>
> I have a script that runs correctly (no errors) on Windows, the first
> command is a "setwd(.....windows directory)"I installed the script on a
> Linux machine and changed the windows directory reference to the correct
> directory on the linux machine.when I ran the script I got a message that
> "cannot change working directory". I confirmed I have read and write access
> to the directory.I mainly run R on windows infrequently use linux/unix and
> would appreciate suggestions.thanks in advance Chris Barker, Ph.D.
> Adjunct Associate Professor of Biostatistics - UIC-SPH
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Mon Jul 27 22:02:30 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Mon, 27 Jul 2015 15:02:30 -0500
Subject: [R] setwd() command on windows vs. linux/unix
In-Reply-To: <2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>
References: <839891721.3232879.1438025798655.JavaMail.yahoo@mail.yahoo.com>
	<2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAAJSdjiXUtpDTO8ag==rjY3QjUZYR9ro_+STSQzpysDyGEaLvw@mail.gmail.com>

On Mon, Jul 27, 2015 at 2:38 PM, Chris <chris.barker at barkerstats.com> wrote:

>
> I have a script that runs correctly (no errors) on Windows, the first
> command is a "setwd(.....windows directory)"I installed the script on a
> Linux machine and changed the windows directory reference to the correct
> directory on the linux machine.when I ran the script I got a message that
> "cannot change working directory". I confirmed I have read and write access
> to the directory.I mainly run R on windows infrequently use linux/unix and
> would appreciate suggestions.thanks in advance Chris Barker, Ph.D.
> Adjunct Associate Professor of Biostatistics - UIC-SPH
>
>
?I took a quick look at the source where I _think_ the message is coming
from: src/main/util.c . That message comes out when the chdir() function (C
library routine) returns any kind of error.? Because you said that have
read & write access (you should also have execute in most cases), then the
only return which makes sense is ENAMETOOLONG (although it could also be
ENOMEM). How long is the path name? The usual limit is 255 bytes.



-- 

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From syen04 at gmail.com  Mon Jul 27 22:04:32 2015
From: syen04 at gmail.com (Steven Yen)
Date: Mon, 27 Jul 2015 16:04:32 -0400
Subject: [R] Element-by-element division
Message-ID: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>

I need help with element-by-element division. Below, matrices a and c are
both 5 x 2 and element-by-element division works as (I) expected. What if
matrix is 1 by 2: to divide first column of a by b[1] and second column of
a by b[2]. I had to go around (two ways) to make it work. In Gauss, these
can be dine by a./b and a./c. Any such simple way in R? Thank!

> a<-matrix(1:10,nrow=5); a
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
> b<-matrix(c(0.5,0.25),nrow=1); b
     [,1] [,2]
[1,]  0.5 0.25
> c<-matrix(rep(c(0.5,0.25),5),nrow=5,byrow=T); c
     [,1] [,2]
[1,]  0.5 0.25
[2,]  0.5 0.25
[3,]  0.5 0.25
[4,]  0.5 0.25
[5,]  0.5 0.25

> one<-a/c; one     [,1] [,2]
[1,]    2   24
[2,]    4   28
[3,]    6   32
[4,]    8   36
[5,]   10   40


> two<-a/b
Error in a/b : non-conformable arrays
> two<-cbind(a[,1]/b[1],a[,2]/b[2]); two
     [,1] [,2]
[1,]    2   24
[2,]    4   28
[3,]    6   32
[4,]    8   36
[5,]   10   40

> b2<-matrix(rep(b,5),nrow=5,byrow=T); b2     [,1] [,2]
[1,]  0.5 0.25
[2,]  0.5 0.25
[3,]  0.5 0.25
[4,]  0.5 0.25
[5,]  0.5 0.25> a/b2     [,1] [,2]
[1,]    2   24
[2,]    4   28
[3,]    6   32
[4,]    8   36
[5,]   10   40

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Mon Jul 27 22:17:07 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 27 Jul 2015 16:17:07 -0400
Subject: [R] Element-by-element division
In-Reply-To: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>
References: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>
Message-ID: <CAM_vjunk9ObntqZ2D+23xVp9c0_mB7YU-9YjYTsR1DXXO8MBoA@mail.gmail.com>

Hi,

See ?sweep

For instance, to get your matrix two:

> sweep(a, 2, b, "/")
     [,1] [,2]
[1,]    2   24
[2,]    4   28
[3,]    6   32
[4,]    8   36
[5,]   10   40


Sarah

On Mon, Jul 27, 2015 at 4:04 PM, Steven Yen <syen04 at gmail.com> wrote:
> I need help with element-by-element division. Below, matrices a and c are
> both 5 x 2 and element-by-element division works as (I) expected. What if
> matrix is 1 by 2: to divide first column of a by b[1] and second column of
> a by b[2]. I had to go around (two ways) to make it work. In Gauss, these
> can be dine by a./b and a./c. Any such simple way in R? Thank!
>
>> a<-matrix(1:10,nrow=5); a
>      [,1] [,2]
> [1,]    1    6
> [2,]    2    7
> [3,]    3    8
> [4,]    4    9
> [5,]    5   10
>> b<-matrix(c(0.5,0.25),nrow=1); b
>      [,1] [,2]
> [1,]  0.5 0.25
>> c<-matrix(rep(c(0.5,0.25),5),nrow=5,byrow=T); c
>      [,1] [,2]
> [1,]  0.5 0.25
> [2,]  0.5 0.25
> [3,]  0.5 0.25
> [4,]  0.5 0.25
> [5,]  0.5 0.25
>
>> one<-a/c; one     [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>
>> two<-a/b
> Error in a/b : non-conformable arrays
>> two<-cbind(a[,1]/b[1],a[,2]/b[2]); two
>      [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>> b2<-matrix(rep(b,5),nrow=5,byrow=T); b2     [,1] [,2]
> [1,]  0.5 0.25
> [2,]  0.5 0.25
> [3,]  0.5 0.25
> [4,]  0.5 0.25
> [5,]  0.5 0.25> a/b2     [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40

-- 
Sarah Goslee
http://www.functionaldiversity.org


From wdunlap at tibco.com  Mon Jul 27 22:29:56 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 27 Jul 2015 13:29:56 -0700
Subject: [R] setwd() command on windows vs. linux/unix
In-Reply-To: <2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>
References: <839891721.3232879.1438025798655.JavaMail.yahoo@mail.yahoo.com>
	<2096312924.3226800.1438025890056.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAF8bMcZLM9jfx-hXtK=auXS5V1tKiVVeHrnogxqZmwBifcXjmg@mail.gmail.com>

You can get a more informative error message from system("bash -c 'cd
yourDirectory'"), although it will not take R to that directory if there
are no problems.  E.g.,  I did in a shell
  % mkdir -p dir/subdir
  % chmod -x dir
to make an untraversable directory 'dir' and a subdirectory of it.  Then in
R I get:
  > setwd("dir/subdir")
  Error in setwd("dir/subdir") : cannot change working directory
  > system("bash -c 'cd dir/subdir'")
  bash: line 0: cd: dir/subdir: Permission denied
And for a nonexistent directory I get:
  > setwd("no/such/directory")
  Error in setwd("no/such/directory") : cannot change working directory
  > system("bash -c 'cd no/such/directory'")
  bash: line 0: cd: no/such/directory: No such file or directory

(It would be nice if the name of the offending directory and a reason were
given in setwd's error message, but that would require someone to write the
code.)



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 27, 2015 at 12:38 PM, Chris <chris.barker at barkerstats.com>
wrote:

>
> I have a script that runs correctly (no errors) on Windows, the first
> command is a "setwd(.....windows directory)"I installed the script on a
> Linux machine and changed the windows directory reference to the correct
> directory on the linux machine.when I ran the script I got a message that
> "cannot change working directory". I confirmed I have read and write access
> to the directory.I mainly run R on windows infrequently use linux/unix and
> would appreciate suggestions.thanks in advance Chris Barker, Ph.D.
> Adjunct Associate Professor of Biostatistics - UIC-SPH
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From Scott.Waichler at pnnl.gov  Mon Jul 27 22:32:18 2015
From: Scott.Waichler at pnnl.gov (Waichler, Scott R)
Date: Mon, 27 Jul 2015 20:32:18 +0000
Subject: [R] write.table with append=T after using cat on same file
Message-ID: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>

Hi, 

For years I've been writing text to the beginning of files with cat(append=F) , then following that text with data written by write.table(append=T).  It is now giving me an error message.  I'm using R-3.1.2.  What gives?

df <- data.frame(x = 1, y = 1:10, z = 10:1)
cat(file="junk.txt", sep="", "# An introductory note.\n")
write.table(df, file="junk.txt", sep=",", append=T, quote=F, row.names=F, col.names=F)

Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
Richland, WA  USA


From mayukh.dass at gmail.com  Mon Jul 27 22:41:17 2015
From: mayukh.dass at gmail.com (Mayukh Dass)
Date: Mon, 27 Jul 2015 15:41:17 -0500
Subject: [R] Reading Json data
Message-ID: <CAHWpsEFi_15m4jbw5A=3NQchF9kFC3GnU9LRY-QfH771DXeJgA@mail.gmail.com>

Hello,

I am trying to read a set of json files containing tweets using the
following code:

json_data <- fromJSON(paste(readLines(json_file))

Unfortunately, it only reads the first record on the file. For example, in
the file below, it only reads the first record starting with "id":"tag:
search.twitter.com,2005:3318539389". What is the best way to retrieve these
records? I have 20 such json files with varying number of tweets in it.
Thank you in advance.

Best,
Mayukh

{"id":"tag:search.twitter.com
,2005:3318539389","objectType":"activity","actor":{"objectType":"person","id":"id:
twitter.com:2859421","link":"http://www.twitter.com/meetjenn","displayName":"Jenn","postedTime":"2007-01-29T17:06:00.000Z","image":"06-19-07_2010.jpg","summary":"I
say 'like' a lot. I fall down a lot. I walk into everything. Love Pgh Pens,
NE Pats, Fundraising, Dogs & History. Craft Beer & Running
Novice.","links":[{"href":"http://meetjenn.tumblr.com","rel":"me"}],"friendsCount":0,"followersCount":0,"listedCount":0,"statusesCount":0,"twitterTimeZone":"Eastern
Time (US &
Canada)","verified":false,"utcOffset":"0","preferredUsername":"meetjenn","languages":["en"],"location":{"objectType":"place","displayName":"Pgh/Philajersey"},"favoritesCount":0},"verb":"post","postedTime":"2009-08-15T00:00:12.000Z","generator":{"displayName":"tweetdeck","link":"
http://twitter.com
"},"provider":{"objectType":"service","displayName":"Twitter","link":"
http://www.twitter.com"},"link":"
http://twitter.com/meetjenn/statuses/3318539389","body":"Cool story about
the man who created the @Starbucks logo. Additional link at the bottom on
how it came to be:  http://bit.ly/16bOJk
","object":{"objectType":"note","id":"object:search.twitter.com,2005:3318539389","summary":"Cool
story about the man who created the @Starbucks logo. Additional link at the
bottom on how it came to be:  http://bit.ly/16bOJk","link":"
http://twitter.com/meetjenn/statuses/3318539389
","postedTime":"2009-08-15T00:00:12.000Z"},"twitter_entities":{"urls":[{"expanded_url":null,"indices":[111,131],"url":"
http://bit.ly/16bOJk
"}],"hashtags":[],"user_mentions":[{"id":null,"name":null,"indices":[41,51],"screen_name":"@Starbucks","id_str":null}]},"retweetCount":0,"gnip":{"matching_rules":[{"value":"Starbucks","tag":null}]}}
{"id":"tag:search.twitter.com
,2005:3318543260","objectType":"activity","actor":{"objectType":"person","id":"id:
twitter.com:61595468","link":"http://www.twitter.com/FastestFood","displayName":"FastFood
Bob","postedTime":"2009-01-30T20:51:10.000Z","image":"","summary":"Just A
little food for
thought","links":[{"href":"http://www.TeamSantilli.com","rel":"me"}],"friendsCount":0,"followersCount":0,"listedCount":0,"statusesCount":0,"twitterTimeZone":"Pacific
Time (US &
Canada)","verified":false,"utcOffset":"0","preferredUsername":"FastestFood","languages":["en"],"location":{"objectType":"place","displayName":"eating
some
thoughts"},"favoritesCount":0},"verb":"post","postedTime":"2009-08-15T00:00:23.000Z","generator":{"displayName":"oauth:17","link":"
http://twitter.com
"},"provider":{"objectType":"service","displayName":"Twitter","link":"
http://www.twitter.com"},"link":"
http://twitter.com/FastestFood/statuses/3318543260","body":"Oregon Biz
Report ? How Starbucks saved millions. Oregon closures ...
http://u.mavrev.com/02bdj","object":{"objectType":"note","id":"object:
search.twitter.com,2005:3318543260","summary":"Oregon Biz Report ? How
Starbucks saved millions. Oregon closures ... http://u.mavrev.com/02bdj
","link":"http://twitter.com/FastestFood/statuses/3318543260
","postedTime":"2009-08-15T00:00:23.000Z"},"twitter_entities":{"urls":[{"expanded_url":null,"indices":[70,95],"url":"
http://u.mavrev.com/02bdj
"}],"hashtags":[],"user_mentions":[]},"retweetCount":0,"gnip":{"matching_rules":[{"value":"Starbucks","tag":null}]}}
{"info":{"message":"Replay Request
Completed","sent":"2015-02-18T00:05:15+00:00","activity_count":2}}

	[[alternative HTML version deleted]]


From msharp at txbiomed.org  Mon Jul 27 22:46:38 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Mon, 27 Jul 2015 20:46:38 +0000
Subject: [R] write.table with append=T after using cat on same file
In-Reply-To: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
References: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
Message-ID: <629F9A41-0885-4C3D-8BE0-EB5BD8A90FC5@txbiomed.org>

I do not get an error with R-3.2.1 on Mac OS. You may have done something prior to this code so that perhaps F is not FALSE or T is not TRUE.

R. Mark Sharp, Ph.D.
msharp at TxBiomed.org





> On Jul 27, 2015, at 3:32 PM, Waichler, Scott R <Scott.Waichler at pnnl.gov> wrote:
> 
> Hi, 
> 
> For years I've been writing text to the beginning of files with cat(append=F) , then following that text with data written by write.table(append=T).  It is now giving me an error message.  I'm using R-3.1.2.  What gives?
> 
> df <- data.frame(x = 1, y = 1:10, z = 10:1)
> cat(file="junk.txt", sep="", "# An introductory note.\n")
> write.table(df, file="junk.txt", sep=",", append=T, quote=F, row.names=F, col.names=F)
> 
> Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument
> 
> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> Richland, WA  USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Mon Jul 27 22:50:39 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 27 Jul 2015 13:50:39 -0700
Subject: [R] write.table with append=T after using cat on same file
In-Reply-To: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
References: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
Message-ID: <CAF8bMcZvh-WS5p35KmUX__B1H9QbFXM9m70q=+Tz8reLWQVjcQ@mail.gmail.com>

This will happen if you have redefined 'T':
  > T <- 101:104
  > write.table(df, file="junk.txt", sep=",", append=T, quote=F,
row.names=F, col.names=F)
  Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument
Use 'TRUE' and 'FALSE' instead of 'T' and 'F' this sort of problem.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 27, 2015 at 1:32 PM, Waichler, Scott R <Scott.Waichler at pnnl.gov>
wrote:

> Hi,
>
> For years I've been writing text to the beginning of files with
> cat(append=F) , then following that text with data written by
> write.table(append=T).  It is now giving me an error message.  I'm using
> R-3.1.2.  What gives?
>
> df <- data.frame(x = 1, y = 1:10, z = 10:1)
> cat(file="junk.txt", sep="", "# An introductory note.\n")
> write.table(df, file="junk.txt", sep=",", append=T, quote=F, row.names=F,
> col.names=F)
>
> Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument
>
> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> Richland, WA  USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Jul 27 22:51:46 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 27 Jul 2015 13:51:46 -0700
Subject: [R] write.table with append=T after using cat on same file
In-Reply-To: <CAF8bMcZvh-WS5p35KmUX__B1H9QbFXM9m70q=+Tz8reLWQVjcQ@mail.gmail.com>
References: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
	<CAF8bMcZvh-WS5p35KmUX__B1H9QbFXM9m70q=+Tz8reLWQVjcQ@mail.gmail.com>
Message-ID: <CAF8bMcZ7UYFBjuG9XZjazhGx8OiJ743vrp+SGKzT1AnKU=ZV7A@mail.gmail.com>

Missing words:
  Use 'TRUE' and 'FALSE' instead of 'T' and 'F' >to avoid< this sort of
problem

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 27, 2015 at 1:50 PM, William Dunlap <wdunlap at tibco.com> wrote:

> This will happen if you have redefined 'T':
>   > T <- 101:104
>   > write.table(df, file="junk.txt", sep=",", append=T, quote=F,
> row.names=F, col.names=F)
>   Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument
> Use 'TRUE' and 'FALSE' instead of 'T' and 'F' this sort of problem.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Mon, Jul 27, 2015 at 1:32 PM, Waichler, Scott R <
> Scott.Waichler at pnnl.gov> wrote:
>
>> Hi,
>>
>> For years I've been writing text to the beginning of files with
>> cat(append=F) , then following that text with data written by
>> write.table(append=T).  It is now giving me an error message.  I'm using
>> R-3.1.2.  What gives?
>>
>> df <- data.frame(x = 1, y = 1:10, z = 10:1)
>> cat(file="junk.txt", sep="", "# An introductory note.\n")
>> write.table(df, file="junk.txt", sep=",", append=T, quote=F, row.names=F,
>> col.names=F)
>>
>> Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument
>>
>> Thanks,
>> Scott Waichler
>> Pacific Northwest National Laboratory
>> Richland, WA  USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Jul 27 23:28:25 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 27 Jul 2015 14:28:25 -0700
Subject: [R] "unfurling" rankings into a matrix of preferences
In-Reply-To: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>
References: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>
Message-ID: <CAGxFJbRGQoTEUG=+Ov1OEM9=FB-2gWvYx07-uw2wgGnnAks7Dg@mail.gmail.com>

## I leave it to you to add the NA edges

> rk <- c(2,4,3,1)

> outer(rk,rk,"<")+0

     [,1] [,2] [,3] [,4]
[1,]    0    1    1    0
[2,]    0    0    0    0
[3,]    0    1    0    0
[4,]    1    1    1    0



Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 27, 2015 at 12:38 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> I have 5 items in total (1:5), but I show a person only 4 items (1:4)
> and ask this person to rank items 1:4 in terms of preferences (1 is
> best, 2 is second best, 4 is worst), and I get a vector of ranks:
> ranks <- c(2,4,3,1)
>
> # That means that this person liked item 4 best and item 2 worst.
>
> I would like to "unfirl" this vector of ranks into a matrix of
> preferences where if the row item prefers the column item, then it's a
> 1. Otherwise, it's a zero. So, the output should be a 5 by 5 matrix
> (because overall we have 5 items, not 4, but item 5 did not
> participate in rankings), and it would always have zeros in a
> diagonal.:
>
> 0    1    1    0 NA
> 0    0    0    0 NA
> 0    1    0    0 NA
> 1    1    1    0 NA
> NA NA NA NA 0
>
> I can loop through all possible pairs the person saw and fill the
> matrix accordingly, but it seems like a lot of looping. Could one do
> it in a more elegant way?
>
> Thank you very much!
>
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Mon Jul 27 23:31:12 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Mon, 27 Jul 2015 17:31:12 -0400
Subject: [R] "unfurling" rankings into a matrix of preferences
In-Reply-To: <CAGxFJbRGQoTEUG=+Ov1OEM9=FB-2gWvYx07-uw2wgGnnAks7Dg@mail.gmail.com>
References: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>
	<CAGxFJbRGQoTEUG=+Ov1OEM9=FB-2gWvYx07-uw2wgGnnAks7Dg@mail.gmail.com>
Message-ID: <CAN2xGJYVGWoLawboDqkH=sxwNrhmEgN4eqLBTTyH-yfrVDmykQ@mail.gmail.com>

Wow!

On Mon, Jul 27, 2015 at 5:28 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> ## I leave it to you to add the NA edges
>
>> rk <- c(2,4,3,1)
>
>> outer(rk,rk,"<")+0
>
>      [,1] [,2] [,3] [,4]
> [1,]    0    1    1    0
> [2,]    0    0    0    0
> [3,]    0    1    0    0
> [4,]    1    1    1    0
>
>
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Mon, Jul 27, 2015 at 12:38 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> I have 5 items in total (1:5), but I show a person only 4 items (1:4)
>> and ask this person to rank items 1:4 in terms of preferences (1 is
>> best, 2 is second best, 4 is worst), and I get a vector of ranks:
>> ranks <- c(2,4,3,1)
>>
>> # That means that this person liked item 4 best and item 2 worst.
>>
>> I would like to "unfirl" this vector of ranks into a matrix of
>> preferences where if the row item prefers the column item, then it's a
>> 1. Otherwise, it's a zero. So, the output should be a 5 by 5 matrix
>> (because overall we have 5 items, not 4, but item 5 did not
>> participate in rankings), and it would always have zeros in a
>> diagonal.:
>>
>> 0    1    1    0 NA
>> 0    0    0    0 NA
>> 0    1    0    0 NA
>> 1    1    1    0 NA
>> NA NA NA NA 0
>>
>> I can loop through all possible pairs the person saw and fill the
>> matrix accordingly, but it seems like a lot of looping. Could one do
>> it in a more elegant way?
>>
>> Thank you very much!
>>
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Mon Jul 27 23:32:51 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Mon, 27 Jul 2015 17:32:51 -0400
Subject: [R] "unfurling" rankings into a matrix of preferences
In-Reply-To: <CAN2xGJYVGWoLawboDqkH=sxwNrhmEgN4eqLBTTyH-yfrVDmykQ@mail.gmail.com>
References: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>
	<CAGxFJbRGQoTEUG=+Ov1OEM9=FB-2gWvYx07-uw2wgGnnAks7Dg@mail.gmail.com>
	<CAN2xGJYVGWoLawboDqkH=sxwNrhmEgN4eqLBTTyH-yfrVDmykQ@mail.gmail.com>
Message-ID: <CAN2xGJaZQW83OP8EBJoaM5biM=cT+7rAC_5zUYHZXyMYxDHALw@mail.gmail.com>

With NAs it'd be:

rk <- c(2,NA,4,3,1, NA)
outer(rk, rk, "<") + 0

Wow, I still can't believe it - just one line!


On Mon, Jul 27, 2015 at 5:31 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Wow!
>
> On Mon, Jul 27, 2015 at 5:28 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> ## I leave it to you to add the NA edges
>>
>>> rk <- c(2,4,3,1)
>>
>>> outer(rk,rk,"<")+0
>>
>>      [,1] [,2] [,3] [,4]
>> [1,]    0    1    1    0
>> [2,]    0    0    0    0
>> [3,]    0    1    0    0
>> [4,]    1    1    1    0
>>
>>
>>
>> Cheers,
>> Bert
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>    -- Clifford Stoll
>>
>>
>> On Mon, Jul 27, 2015 at 12:38 PM, Dimitri Liakhovitski
>> <dimitri.liakhovitski at gmail.com> wrote:
>>> I have 5 items in total (1:5), but I show a person only 4 items (1:4)
>>> and ask this person to rank items 1:4 in terms of preferences (1 is
>>> best, 2 is second best, 4 is worst), and I get a vector of ranks:
>>> ranks <- c(2,4,3,1)
>>>
>>> # That means that this person liked item 4 best and item 2 worst.
>>>
>>> I would like to "unfirl" this vector of ranks into a matrix of
>>> preferences where if the row item prefers the column item, then it's a
>>> 1. Otherwise, it's a zero. So, the output should be a 5 by 5 matrix
>>> (because overall we have 5 items, not 4, but item 5 did not
>>> participate in rankings), and it would always have zeros in a
>>> diagonal.:
>>>
>>> 0    1    1    0 NA
>>> 0    0    0    0 NA
>>> 0    1    0    0 NA
>>> 1    1    1    0 NA
>>> NA NA NA NA 0
>>>
>>> I can loop through all possible pairs the person saw and fill the
>>> matrix accordingly, but it seems like a lot of looping. Could one do
>>> it in a more elegant way?
>>>
>>> Thank you very much!
>>>
>>>
>>> --
>>> Dimitri Liakhovitski
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From bgunter.4567 at gmail.com  Mon Jul 27 23:46:26 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 27 Jul 2015 14:46:26 -0700
Subject: [R] "unfurling" rankings into a matrix of preferences
In-Reply-To: <CAN2xGJaZQW83OP8EBJoaM5biM=cT+7rAC_5zUYHZXyMYxDHALw@mail.gmail.com>
References: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>
	<CAGxFJbRGQoTEUG=+Ov1OEM9=FB-2gWvYx07-uw2wgGnnAks7Dg@mail.gmail.com>
	<CAN2xGJYVGWoLawboDqkH=sxwNrhmEgN4eqLBTTyH-yfrVDmykQ@mail.gmail.com>
	<CAN2xGJaZQW83OP8EBJoaM5biM=cT+7rAC_5zUYHZXyMYxDHALw@mail.gmail.com>
Message-ID: <CAGxFJbRd9Tmy6rXUm7Uo_EupDH0S7Kz6-6UF-MtBvP5P5cwzGA@mail.gmail.com>

No it wouldn't.

Presumably you have a typo and meant

rk <- c(2,4,3,1,NA)

## and set the (5,5) entry to 0 after

-- Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 27, 2015 at 2:32 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> With NAs it'd be:
>
> rk <- c(2,NA,4,3,1, NA)
> outer(rk, rk, "<") + 0
>
> Wow, I still can't believe it - just one line!
>
>
> On Mon, Jul 27, 2015 at 5:31 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Wow!
>>
>> On Mon, Jul 27, 2015 at 5:28 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>> ## I leave it to you to add the NA edges
>>>
>>>> rk <- c(2,4,3,1)
>>>
>>>> outer(rk,rk,"<")+0
>>>
>>>      [,1] [,2] [,3] [,4]
>>> [1,]    0    1    1    0
>>> [2,]    0    0    0    0
>>> [3,]    0    1    0    0
>>> [4,]    1    1    1    0
>>>
>>>
>>>
>>> Cheers,
>>> Bert
>>>
>>> Bert Gunter
>>>
>>> "Data is not information. Information is not knowledge. And knowledge
>>> is certainly not wisdom."
>>>    -- Clifford Stoll
>>>
>>>
>>> On Mon, Jul 27, 2015 at 12:38 PM, Dimitri Liakhovitski
>>> <dimitri.liakhovitski at gmail.com> wrote:
>>>> I have 5 items in total (1:5), but I show a person only 4 items (1:4)
>>>> and ask this person to rank items 1:4 in terms of preferences (1 is
>>>> best, 2 is second best, 4 is worst), and I get a vector of ranks:
>>>> ranks <- c(2,4,3,1)
>>>>
>>>> # That means that this person liked item 4 best and item 2 worst.
>>>>
>>>> I would like to "unfirl" this vector of ranks into a matrix of
>>>> preferences where if the row item prefers the column item, then it's a
>>>> 1. Otherwise, it's a zero. So, the output should be a 5 by 5 matrix
>>>> (because overall we have 5 items, not 4, but item 5 did not
>>>> participate in rankings), and it would always have zeros in a
>>>> diagonal.:
>>>>
>>>> 0    1    1    0 NA
>>>> 0    0    0    0 NA
>>>> 0    1    0    0 NA
>>>> 1    1    1    0 NA
>>>> NA NA NA NA 0
>>>>
>>>> I can loop through all possible pairs the person saw and fill the
>>>> matrix accordingly, but it seems like a lot of looping. Could one do
>>>> it in a more elegant way?
>>>>
>>>> Thank you very much!
>>>>
>>>>
>>>> --
>>>> Dimitri Liakhovitski
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Dimitri Liakhovitski
>
>
>
> --
> Dimitri Liakhovitski


From msharp at txbiomed.org  Tue Jul 28 00:16:38 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Mon, 27 Jul 2015 22:16:38 +0000
Subject: [R] Reading Json data
In-Reply-To: <CAHWpsEFi_15m4jbw5A=3NQchF9kFC3GnU9LRY-QfH771DXeJgA@mail.gmail.com>
References: <CAHWpsEFi_15m4jbw5A=3NQchF9kFC3GnU9LRY-QfH771DXeJgA@mail.gmail.com>
Message-ID: <B3593265-661A-4E0E-BC63-88EC39E35BD5@txbiomed.org>

Mayukh,

I think you are missing an argument to paste() and a right parenthesis character.

Try 
json_data <- fromJSON(paste(readLines(json_file), collapse = " "))

Mark
R. Mark Sharp, Ph.D.
msharp at TxBiomed.org





> On Jul 27, 2015, at 3:41 PM, Mayukh Dass <mayukh.dass at gmail.com> wrote:
> 
> Hello,
> 
> I am trying to read a set of json files containing tweets using the
> following code:
> 
> json_data <- fromJSON(paste(readLines(json_file))
> 
> Unfortunately, it only reads the first record on the file. For example, in
> the file below, it only reads the first record starting with "id":"tag:
> search.twitter.com,2005:3318539389". What is the best way to retrieve these
> records? I have 20 such json files with varying number of tweets in it.
> Thank you in advance.
> 
> Best,
> Mayukh
> 
> {"id":"tag:search.twitter.com
> ,2005:3318539389","objectType":"activity","actor":{"objectType":"person","id":"id:
> twitter.com:2859421","link":"http://www.twitter.com/meetjenn","displayName":"Jenn","postedTime":"2007-01-29T17:06:00.000Z","image":"06-19-07_2010.jpg","summary":"I
> say 'like' a lot. I fall down a lot. I walk into everything. Love Pgh Pens,
> NE Pats, Fundraising, Dogs & History. Craft Beer & Running
> Novice.","links":[{"href":"http://meetjenn.tumblr.com","rel":"me"}],"friendsCount":0,"followersCount":0,"listedCount":0,"statusesCount":0,"twitterTimeZone":"Eastern
> Time (US &
> Canada)","verified":false,"utcOffset":"0","preferredUsername":"meetjenn","languages":["en"],"location":{"objectType":"place","displayName":"Pgh/Philajersey"},"favoritesCount":0},"verb":"post","postedTime":"2009-08-15T00:00:12.000Z","generator":{"displayName":"tweetdeck","link":"
> http://twitter.com
> "},"provider":{"objectType":"service","displayName":"Twitter","link":"
> http://www.twitter.com"},"link":"
> http://twitter.com/meetjenn/statuses/3318539389","body":"Cool story about
> the man who created the @Starbucks logo. Additional link at the bottom on
> how it came to be:  http://bit.ly/16bOJk
> ","object":{"objectType":"note","id":"object:search.twitter.com,2005:3318539389","summary":"Cool
> story about the man who created the @Starbucks logo. Additional link at the
> bottom on how it came to be:  http://bit.ly/16bOJk","link":"
> http://twitter.com/meetjenn/statuses/3318539389
> ","postedTime":"2009-08-15T00:00:12.000Z"},"twitter_entities":{"urls":[{"expanded_url":null,"indices":[111,131],"url":"
> http://bit.ly/16bOJk
> "}],"hashtags":[],"user_mentions":[{"id":null,"name":null,"indices":[41,51],"screen_name":"@Starbucks","id_str":null}]},"retweetCount":0,"gnip":{"matching_rules":[{"value":"Starbucks","tag":null}]}}
> {"id":"tag:search.twitter.com
> ,2005:3318543260","objectType":"activity","actor":{"objectType":"person","id":"id:
> twitter.com:61595468","link":"http://www.twitter.com/FastestFood","displayName":"FastFood
> Bob","postedTime":"2009-01-30T20:51:10.000Z","image":"","summary":"Just A
> little food for
> thought","links":[{"href":"http://www.TeamSantilli.com","rel":"me"}],"friendsCount":0,"followersCount":0,"listedCount":0,"statusesCount":0,"twitterTimeZone":"Pacific
> Time (US &
> Canada)","verified":false,"utcOffset":"0","preferredUsername":"FastestFood","languages":["en"],"location":{"objectType":"place","displayName":"eating
> some
> thoughts"},"favoritesCount":0},"verb":"post","postedTime":"2009-08-15T00:00:23.000Z","generator":{"displayName":"oauth:17","link":"
> http://twitter.com
> "},"provider":{"objectType":"service","displayName":"Twitter","link":"
> http://www.twitter.com"},"link":"
> http://twitter.com/FastestFood/statuses/3318543260","body":"Oregon Biz
> Report ? How Starbucks saved millions. Oregon closures ...
> http://u.mavrev.com/02bdj","object":{"objectType":"note","id":"object:
> search.twitter.com,2005:3318543260","summary":"Oregon Biz Report ? How
> Starbucks saved millions. Oregon closures ... http://u.mavrev.com/02bdj
> ","link":"http://twitter.com/FastestFood/statuses/3318543260
> ","postedTime":"2009-08-15T00:00:23.000Z"},"twitter_entities":{"urls":[{"expanded_url":null,"indices":[70,95],"url":"
> http://u.mavrev.com/02bdj
> "}],"hashtags":[],"user_mentions":[]},"retweetCount":0,"gnip":{"matching_rules":[{"value":"Starbucks","tag":null}]}}
> {"info":{"message":"Replay Request
> Completed","sent":"2015-02-18T00:05:15+00:00","activity_count":2}}
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dimitri.liakhovitski at gmail.com  Tue Jul 28 00:23:39 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Mon, 27 Jul 2015 18:23:39 -0400
Subject: [R] "unfurling" rankings into a matrix of preferences
In-Reply-To: <CAGxFJbRd9Tmy6rXUm7Uo_EupDH0S7Kz6-6UF-MtBvP5P5cwzGA@mail.gmail.com>
References: <CAN2xGJanUz5=80eZSx032AWBjbuF-LNZw9cE1WwReVPd8wgHLg@mail.gmail.com>
	<CAGxFJbRGQoTEUG=+Ov1OEM9=FB-2gWvYx07-uw2wgGnnAks7Dg@mail.gmail.com>
	<CAN2xGJYVGWoLawboDqkH=sxwNrhmEgN4eqLBTTyH-yfrVDmykQ@mail.gmail.com>
	<CAN2xGJaZQW83OP8EBJoaM5biM=cT+7rAC_5zUYHZXyMYxDHALw@mail.gmail.com>
	<CAGxFJbRd9Tmy6rXUm7Uo_EupDH0S7Kz6-6UF-MtBvP5P5cwzGA@mail.gmail.com>
Message-ID: <CAN2xGJbV45k1GYMwvonva1iVp+M6N0mn9aD3QsVOWM9yHREPCQ@mail.gmail.com>

Yes, correct, thank you, Bert!

On Mon, Jul 27, 2015 at 5:46 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> No it wouldn't.
>
> Presumably you have a typo and meant
>
> rk <- c(2,4,3,1,NA)
>
> ## and set the (5,5) entry to 0 after
>
> -- Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Mon, Jul 27, 2015 at 2:32 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> With NAs it'd be:
>>
>> rk <- c(2,NA,4,3,1, NA)
>> outer(rk, rk, "<") + 0
>>
>> Wow, I still can't believe it - just one line!
>>
>>
>> On Mon, Jul 27, 2015 at 5:31 PM, Dimitri Liakhovitski
>> <dimitri.liakhovitski at gmail.com> wrote:
>>> Wow!
>>>
>>> On Mon, Jul 27, 2015 at 5:28 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>>> ## I leave it to you to add the NA edges
>>>>
>>>>> rk <- c(2,4,3,1)
>>>>
>>>>> outer(rk,rk,"<")+0
>>>>
>>>>      [,1] [,2] [,3] [,4]
>>>> [1,]    0    1    1    0
>>>> [2,]    0    0    0    0
>>>> [3,]    0    1    0    0
>>>> [4,]    1    1    1    0
>>>>
>>>>
>>>>
>>>> Cheers,
>>>> Bert
>>>>
>>>> Bert Gunter
>>>>
>>>> "Data is not information. Information is not knowledge. And knowledge
>>>> is certainly not wisdom."
>>>>    -- Clifford Stoll
>>>>
>>>>
>>>> On Mon, Jul 27, 2015 at 12:38 PM, Dimitri Liakhovitski
>>>> <dimitri.liakhovitski at gmail.com> wrote:
>>>>> I have 5 items in total (1:5), but I show a person only 4 items (1:4)
>>>>> and ask this person to rank items 1:4 in terms of preferences (1 is
>>>>> best, 2 is second best, 4 is worst), and I get a vector of ranks:
>>>>> ranks <- c(2,4,3,1)
>>>>>
>>>>> # That means that this person liked item 4 best and item 2 worst.
>>>>>
>>>>> I would like to "unfirl" this vector of ranks into a matrix of
>>>>> preferences where if the row item prefers the column item, then it's a
>>>>> 1. Otherwise, it's a zero. So, the output should be a 5 by 5 matrix
>>>>> (because overall we have 5 items, not 4, but item 5 did not
>>>>> participate in rankings), and it would always have zeros in a
>>>>> diagonal.:
>>>>>
>>>>> 0    1    1    0 NA
>>>>> 0    0    0    0 NA
>>>>> 0    1    0    0 NA
>>>>> 1    1    1    0 NA
>>>>> NA NA NA NA 0
>>>>>
>>>>> I can loop through all possible pairs the person saw and fill the
>>>>> matrix accordingly, but it seems like a lot of looping. Could one do
>>>>> it in a more elegant way?
>>>>>
>>>>> Thank you very much!
>>>>>
>>>>>
>>>>> --
>>>>> Dimitri Liakhovitski
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>>
>>> --
>>> Dimitri Liakhovitski
>>
>>
>>
>> --
>> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From mayukh.dass at gmail.com  Tue Jul 28 00:31:15 2015
From: mayukh.dass at gmail.com (Mayukh Dass)
Date: Mon, 27 Jul 2015 17:31:15 -0500
Subject: [R] Reading Json data
In-Reply-To: <B3593265-661A-4E0E-BC63-88EC39E35BD5@txbiomed.org>
References: <CAHWpsEFi_15m4jbw5A=3NQchF9kFC3GnU9LRY-QfH771DXeJgA@mail.gmail.com>
	<B3593265-661A-4E0E-BC63-88EC39E35BD5@txbiomed.org>
Message-ID: <81AEA7E5-6C10-4617-8051-A9886A9F2BAA@gmail.com>

Thanks Mark.

I made a mistake when I was coping the code on the email. I have the parentheses in my code.

Best,
Mayukh 



> On Jul 27, 2015, at 5:16 PM, Mark Sharp <msharp at TxBiomed.org> wrote:
> 
> Mayukh,
> 
> I think you are missing an argument to paste() and a right parenthesis character.
> 
> Try 
> json_data <- fromJSON(paste(readLines(json_file), collapse = " "))
> 
> Mark
> R. Mark Sharp, Ph.D.
> msharp at TxBiomed.org
> 
> 
> 
> 
> 
>> On Jul 27, 2015, at 3:41 PM, Mayukh Dass <mayukh.dass at gmail.com> wrote:
>> 
>> Hello,
>> 
>> I am trying to read a set of json files containing tweets using the
>> following code:
>> 
>> json_data <- fromJSON(paste(readLines(json_file))
>> 
>> Unfortunately, it only reads the first record on the file. For example, in
>> the file below, it only reads the first record starting with "id":"tag:
>> search.twitter.com,2005:3318539389". What is the best way to retrieve these
>> records? I have 20 such json files with varying number of tweets in it.
>> Thank you in advance.
>> 
>> Best,
>> Mayukh
>> 
>> {"id":"tag:search.twitter.com
>> ,2005:3318539389","objectType":"activity","actor":{"objectType":"person","id":"id:
>> twitter.com:2859421","link":"http://www.twitter.com/meetjenn","displayName":"Jenn","postedTime":"2007-01-29T17:06:00.000Z","image":"06-19-07_2010.jpg","summary":"I
>> say 'like' a lot. I fall down a lot. I walk into everything. Love Pgh Pens,
>> NE Pats, Fundraising, Dogs & History. Craft Beer & Running
>> Novice.","links":[{"href":"http://meetjenn.tumblr.com","rel":"me"}],"friendsCount":0,"followersCount":0,"listedCount":0,"statusesCount":0,"twitterTimeZone":"Eastern
>> Time (US &
>> Canada)","verified":false,"utcOffset":"0","preferredUsername":"meetjenn","languages":["en"],"location":{"objectType":"place","displayName":"Pgh/Philajersey"},"favoritesCount":0},"verb":"post","postedTime":"2009-08-15T00:00:12.000Z","generator":{"displayName":"tweetdeck","link":"
>> http://twitter.com
>> "},"provider":{"objectType":"service","displayName":"Twitter","link":"
>> http://www.twitter.com"},"link":"
>> http://twitter.com/meetjenn/statuses/3318539389","body":"Cool story about
>> the man who created the @Starbucks logo. Additional link at the bottom on
>> how it came to be:  http://bit.ly/16bOJk
>> ","object":{"objectType":"note","id":"object:search.twitter.com,2005:3318539389","summary":"Cool
>> story about the man who created the @Starbucks logo. Additional link at the
>> bottom on how it came to be:  http://bit.ly/16bOJk","link":"
>> http://twitter.com/meetjenn/statuses/3318539389
>> ","postedTime":"2009-08-15T00:00:12.000Z"},"twitter_entities":{"urls":[{"expanded_url":null,"indices":[111,131],"url":"
>> http://bit.ly/16bOJk
>> "}],"hashtags":[],"user_mentions":[{"id":null,"name":null,"indices":[41,51],"screen_name":"@Starbucks","id_str":null}]},"retweetCount":0,"gnip":{"matching_rules":[{"value":"Starbucks","tag":null}]}}
>> {"id":"tag:search.twitter.com
>> ,2005:3318543260","objectType":"activity","actor":{"objectType":"person","id":"id:
>> twitter.com:61595468","link":"http://www.twitter.com/FastestFood","displayName":"FastFood
>> Bob","postedTime":"2009-01-30T20:51:10.000Z","image":"","summary":"Just A
>> little food for
>> thought","links":[{"href":"http://www.TeamSantilli.com","rel":"me"}],"friendsCount":0,"followersCount":0,"listedCount":0,"statusesCount":0,"twitterTimeZone":"Pacific
>> Time (US &
>> Canada)","verified":false,"utcOffset":"0","preferredUsername":"FastestFood","languages":["en"],"location":{"objectType":"place","displayName":"eating
>> some
>> thoughts"},"favoritesCount":0},"verb":"post","postedTime":"2009-08-15T00:00:23.000Z","generator":{"displayName":"oauth:17","link":"
>> http://twitter.com
>> "},"provider":{"objectType":"service","displayName":"Twitter","link":"
>> http://www.twitter.com"},"link":"
>> http://twitter.com/FastestFood/statuses/3318543260","body":"Oregon Biz
>> Report ? How Starbucks saved millions. Oregon closures ...
>> http://u.mavrev.com/02bdj","object":{"objectType":"note","id":"object:
>> search.twitter.com,2005:3318543260","summary":"Oregon Biz Report ? How
>> Starbucks saved millions. Oregon closures ... http://u.mavrev.com/02bdj
>> ","link":"http://twitter.com/FastestFood/statuses/3318543260
>> ","postedTime":"2009-08-15T00:00:23.000Z"},"twitter_entities":{"urls":[{"expanded_url":null,"indices":[70,95],"url":"
>> http://u.mavrev.com/02bdj
>> "}],"hashtags":[],"user_mentions":[]},"retweetCount":0,"gnip":{"matching_rules":[{"value":"Starbucks","tag":null}]}}
>> {"info":{"message":"Replay Request
>> Completed","sent":"2015-02-18T00:05:15+00:00","activity_count":2}}
>> 
>>    [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


From r.turner at auckland.ac.nz  Tue Jul 28 00:44:14 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 28 Jul 2015 10:44:14 +1200
Subject: [R] Type of variable
In-Reply-To: <1438005311329-4710396.post@n4.nabble.com>
References: <1438005145912-4710395.post@n4.nabble.com>
	<1438005311329-4710396.post@n4.nabble.com>
Message-ID: <55B6B43E.40007@auckland.ac.nz>

On 28/07/15 01:55, jpara3 wrote:
> Ok, stupid question.
>
> is.factor is the solution.

Or more generally, class().

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From marongiu.luigi at gmail.com  Tue Jul 28 00:45:26 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Mon, 27 Jul 2015 23:45:26 +0100
Subject: [R] assign optimal cut-off to variable using Epi/pROC packages
Message-ID: <CAMk+s2T3Ledv1w0z8YLmGV32OjK3ayR8Bkos0aFouO9qRnvagw@mail.gmail.com>

Dear all,
I am calculating the optimal cut-off for a test against a gold
standard measure. I can determine the cut-off (in the following
example 5.905) but I would like to assign it to an atomic variable so
that the cut-off is updated every time the data is changed. I am using
the ROC and roc functions of the packages Epi and pROC respectively.
Would you know how to extract the optimal cut-off from these functions
or a similar way to get such value?
Best regards
Luigi

>>>
query.test <- c(5.43,    6.63,    0,    6.2,    5.61,    0,    5.59,
 0,    0,    5.49,    18.35,    0,    6.07,    4.54,    4.73,    0,
5.74,    33.02,    4.45,    31.16,    0,    0,    3.12,    0,    0,
4.78,    0,    0,    0,    0,    0,    32.42
)
gold.std <- c(0,    1,    0,    1,    0,    0,    0,    0,    0,    0,
   1,    0,    1,    0,    0,    0,    0,    1,    0,    1,    0,
0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1
)
df <- data.frame(cbind(query.test, gold.std))

library(Epi)
library(pROC)
# Epi
ROC(form=gold.std ~ query.test, plot="ROC", data=df, MI=FALSE,
MX=FALSE, PV=TRUE)
# pROC
roc(gold.std ~ query.test, df, smooth=FALSE, plot=TRUE,
auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE, print.auc=TRUE,
print.thres="best")


From r.turner at auckland.ac.nz  Tue Jul 28 00:54:44 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 28 Jul 2015 10:54:44 +1200
Subject: [R] Segmented model
In-Reply-To: <1437985308855-4710384.post@n4.nabble.com>
References: <1437985308855-4710384.post@n4.nabble.com>
Message-ID: <55B6B6B4.1090103@auckland.ac.nz>

On 27/07/15 20:21, emiP wrote:
> Hi to all,
> I used segmented package for my analysis. All is OK, I can find the slope
> and the intercept of each segment but I can't find the R2 (square root) of
> each segment. Any idea?

What *on earth* do you mean???  What is the square root of a segment???

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From steve.taylor at aut.ac.nz  Tue Jul 28 01:23:18 2015
From: steve.taylor at aut.ac.nz (Steve Taylor)
Date: Mon, 27 Jul 2015 23:23:18 +0000
Subject: [R] Opposite color in R
In-Reply-To: <55B3E863.2010208@utu.fi>
References: <55B3E863.2010208@utu.fi>
Message-ID: <CCE952776B6679469977532BD863C39CBD490FC0@Lewis.autuni.aut.ac.nz>

I wonder if the hcl colour space is useful?  Varying hue while keeping chroma and luminosity constant should give varying colours of perceptually the same "colourness" and brightness.

?hcl
pie(rep(1,12),col=hcl((1:12)*30,c=70),border=NA)


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Atte Tenkanen
Sent: Sunday, 26 July 2015 7:50a
To: r-help at r-project.org
Subject: [R] Opposite color in R

Hi,

I have tried to find a way to find opposite or complementary colors in R.

I would like to form a color circle with R like this one: 
http://nobetty.net/dandls/colorwheel/complementary_colors.jpg

If you just make a basic color wheel in R, the colors do not form 
complementary color circle:

palette(rainbow(24))
Colors=palette()
pie(rep(1, 24), col = Colors)

There is a package ?colortools? where you can find function opposite(), 
but it doesn?t work as is said. I tried

library(colortools)
opposite("violet") and got green instead of yellow and

opposite("blue") and got yellow instead of orange.

Do you know any solutions?

Atte Tenkanen

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From bgunter.4567 at gmail.com  Tue Jul 28 01:27:08 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 27 Jul 2015 16:27:08 -0700
Subject: [R] Segmented model
In-Reply-To: <55B6B6B4.1090103@auckland.ac.nz>
References: <1437985308855-4710384.post@n4.nabble.com>
	<55B6B6B4.1090103@auckland.ac.nz>
Message-ID: <CAGxFJbQ8NNEqsu9tMmQFsJZBfvJ8vhFfKfVtfJYELsibQF_jOg@mail.gmail.com>

... Presumably the length of the segment ??

I leave it to the OP or others to look up the answer in their high
school math texts.

-- Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 27, 2015 at 3:54 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> On 27/07/15 20:21, emiP wrote:
>>
>> Hi to all,
>> I used segmented package for my analysis. All is OK, I can find the slope
>> and the intercept of each segment but I can't find the R2 (square root) of
>> each segment. Any idea?
>
>
> What *on earth* do you mean???  What is the square root of a segment???
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From emammendes at gmail.com  Tue Jul 28 01:46:15 2015
From: emammendes at gmail.com (Eduardo M. A. M.Mendes)
Date: Mon, 27 Jul 2015 20:46:15 -0300
Subject: [R] Minor Ticks on ggplot2
Message-ID: <F00BE1DE-89DA-4D45-84F4-968D801E819A@gmail.com>

Hi

Although I could find some answers on how to add minor ticks on a ggplot on stack overflow, I could not figure to how to add them to the following example:

library(ggplot2)

x <- data.frame(V1=runif(1000, -6.0, 6.0),V2=runif(1000, -6.0, 6.0));

               
g<-ggplot(x,aes(V1,V2))+geom_point(size=1.5)+theme_bw()
g<-g+scale_y_continuous(name="y(t)")+scale_x_continuous(name="x(t)")
g<-g+theme(axis.title.x = element_text(face="italic", size=20),axis.title.y = element_text(face="italic", size=20))
g<-g+theme(axis.text.x  = element_text(angle=0, vjust=0.5, size=16),axis.text.y  = element_text(angle=0, vjust=0.5, size=16))
g<-g+coord_cartesian(xlim=c(min(x[,1]),max(x[,1])), ylim=c(min(x[,2]),max(x[,2])))
print(g)

How to add gridlines (or points) to the minor ticks?

Many thanks

Ed


From jrkrideau at inbox.com  Tue Jul 28 04:54:06 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 27 Jul 2015 18:54:06 -0800
Subject: [R] Type of variable
In-Reply-To: <1438005145912-4710395.post@n4.nabble.com>
Message-ID: <6B4BFDECB90.00000D26jrkrideau@inbox.com>

?str perhaps.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: j.para.fernandez at hotmail.com
> Sent: Mon, 27 Jul 2015 06:52:25 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] Type of variable
> 
> I have a dataframe like this one:
> 
> one     two    three
> 3.2      2.5     a
> 3.5      2.3     a
> 3.7      2.2     b
> 
> How can I implment and If that detects the type of data that the
> variables
> one, two and three has?
> 
> Thanks!!!
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Type-of-variable-tp4710395.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From peter.anthoni at kit.edu  Mon Jul 27 19:51:26 2015
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Mon, 27 Jul 2015 17:51:26 +0000
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <1438014822138-4710408.post@n4.nabble.com>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
	<CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
	<8A18FABE-B192-402A-9101-F3311F74A0A1@kit.edu>
	<1438014822138-4710408.post@n4.nabble.com>
Message-ID: <CBB43074-8829-4DAD-81EA-32318DB760A2@kit.edu>

Hi Cecilia,

print(fnp) and print(a_10) are not exactly the same

>> print(fnp)
> $a_10
> class       : SpatialPolygonsDataFrame 
vs.
>> print(a_10)
> class       : SpatialPolygonsDataFrame 


Looks like mget returns a list.

Can you try: 
fnp <- get(afiles[ifile])[1]
print(fnp)
#the $a_10 should be gone.

cheers
Peter




> On 27 Jul 2015, at 18:33, SisoL <cecilia.larrosa10 at imperial.ac.uk> wrote:
> 
> Hi Peter,
> 
> Thank you for your reply. The method for looping seems to work, but
> gDistance will not recognise the input. I am puzzled because when I
> print(fnp), and print (a_10) they look exactly the same, but when I try to
> run gDistance{rgeos} with a_10 it works, but with fnp it throws an error.
> Please see below. Any other suggestions?
> 
>> afiles <- ls(pattern= "a_") #OK
>> print(afiles)
> [1] "a_1"  "a_10"
> 
>> fnp<- mget(afiles[ifile]) #OK
> 
>> print(fnp)
> $a_10
> class       : SpatialPolygonsDataFrame 
> features    : 5 
> extent      : 825796.9, 831270.1, 815666.9, 816562.5  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0
> +y_0=0 +ellps=aust_SA +units=m +no_defs 
> variables   : 5
> names       : ID, GRIDCODE,    Shape_Leng,    Shape_Area, Count 
> min values  :  1,        1, 94.2722341164, 6.47354525991,     1 
> max values  :  5,        1, 2305.45647624, 92111.8528756,     1 
> 
>> print(a_10)
> class       : SpatialPolygonsDataFrame 
> features    : 5 
> extent      : 825796.9, 831270.1, 815666.9, 816562.5  (xmin, xmax, ymin,
> ymax)
> coord. ref. : +proj=aea +lat_1=-5 +lat_2=-42 +lat_0=-32 +lon_0=-60 +x_0=0
> +y_0=0 +ellps=aust_SA +units=m +no_defs 
> variables   : 5
> names       : ID, GRIDCODE,    Shape_Leng,    Shape_Area, Count 
> min values  :  1,        1, 94.2722341164, 6.47354525991,     1 
> max values  :  5,        1, 2305.45647624, 92111.8528756,     1   
> 
>> distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T)
> Error in (function (classes, fdef, mtable)  : 
>  unable to find an inherited method for function ?is.projected? for
> signature ?"list"?
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Varying-name-of-output-tables-from-looped-process-of-list-of-spdf-objects-tp4710369p4710408.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From laptepo at hotmail.com  Mon Jul 27 20:01:20 2015
From: laptepo at hotmail.com (nuria)
Date: Mon, 27 Jul 2015 11:01:20 -0700 (PDT)
Subject: [R] Error in dist.mat[com.names,
	com.names] : subscript out of bounds
Message-ID: <1438020080165-4710412.post@n4.nabble.com>

Hi!

I want to estimate the phylogenetic diversity. I use the SDMTools package.
In the line 3 of the "sntd.a" function appears this error and I don't know
the reason. 

my.phylo<-read.tree("150_BootstrapConsensusTree_Cantabrian_ML.nwk")
my.sample <- read.delim("C:/Filogenia/my.sample.txt")
my.sample<-read.table("my.sample.txt",sep="\t", header=T, row.names=1)
library(SDMTools)
dist.mat<-cophenetic(my.phylo)
sntd.a.function <- function(x){
   com.names <- names(x[x > 0])
   my.com.dist <- dist.mat[com.names, com.names]       
   diag(my.com.dist) = NA                              
   wt.sd(apply(my.com.dist,1,min,na.rm=T), x[x>0])     
}

result<-apply(my.sample, MARGIN = 1, sntd.a.function)

Error in dist.mat[com.names, com.names] : subscript out of bounds
Called from: FUN(newX[, i], ...)
Browse[1]> 

150_BootstrapConsensusTree_Cantabrian_ML.nwk
<http://r.789695.n4.nabble.com/file/n4710412/150_BootstrapConsensusTree_Cantabrian_ML.nwk>  
my.txt <http://r.789695.n4.nabble.com/file/n4710412/my.txt>  


Thanks!!!!




--
View this message in context: http://r.789695.n4.nabble.com/Error-in-dist-mat-com-names-com-names-subscript-out-of-bounds-tp4710412.html
Sent from the R help mailing list archive at Nabble.com.


From ypetscher at fcrr.org  Mon Jul 27 21:03:52 2015
From: ypetscher at fcrr.org (Yaacov Petscher)
Date: Mon, 27 Jul 2015 15:03:52 -0400
Subject: [R] R load error
Message-ID: <73551C12E91468488D61D86CDD49C930026D1C265CE5@fcrrex.fcrr.net>

Greetings - I'm using RStudio and recently updated both it and R. When loadings up, I'm now receiving the following error:

Error: ReadItem: unknown type 63, perhaps written by later version of R

I've tried using rm(list=ls())   rm(list=ls(all.names=TRUE)) and detach() but nothing works in R. Within RStudio it just continues to try and load and I'm unable to interrupt or restart. Any ideas would be greatly appreciated.

Yaacov Petscher, Ph.D.
Associate Director, Florida Center for Reading Research
Senior Research Associate, Regional Educational Laboratory-Southeast (REL-SE) at Florida State University
2010 Levy Ave
Suite 100
Tallahassee, Florida, 32310
850-645-8963 (p)
850-644-9085 (f)
http://www.fcrr.org/for-researchers/petscher2.asp



	[[alternative HTML version deleted]]


From chrismbarker at yahoo.com  Mon Jul 27 21:36:38 2015
From: chrismbarker at yahoo.com (Chris)
Date: Mon, 27 Jul 2015 19:36:38 +0000 (UTC)
Subject: [R] setwd() command on windows vs. linux/unix
Message-ID: <839891721.3232879.1438025798655.JavaMail.yahoo@mail.yahoo.com>

I have a script that runs correctly (no errors) on Windows, the first command is a "setwd(.....windows directory)"I installed the script on a Linux machine and changed the windows directory reference to the correct directory on the linux machine.when I ran the script I got a message that "cannot change working directory".?I confirmed I have read and write access to the directory.I mainly run R on windows infrequently use linux/unix and would appreciate suggestions.thanks in advance?Chris Barker, Ph.D.
Adjunct Associate Professor of Biostatistics - UIC-SPH



	[[alternative HTML version deleted]]


From Boyd.Pete at epa.gov  Mon Jul 27 22:54:27 2015
From: Boyd.Pete at epa.gov (Boyd, Leslie (Pete))
Date: Mon, 27 Jul 2015 20:54:27 +0000
Subject: [R] Latest Xorg updates broke R x11()
Message-ID: <DM2PR09MB02373086EFBB54E26EE0295BE48E0@DM2PR09MB0237.namprd09.prod.outlook.com>

Hello,

       Have 15 RedHat EL6 workstations patched to current.
       Over the weekend the kernel was patched to 6.7 and the xorg-x11-server and our R-3.1.2 will
       not open a xterm window.

        It appears to select a portion of the screen and lock onto it. This section can be moved around
        the screen at will, however; it is not possible to access anything inside the screen. It can only
        be managed by killing the R process.

         Any suggestions for resolution would be greatly appreciated.

         FYI: The kernel was upgraded to 6.7 and xorg-x11-server to 1.15.0-36.el6.x86_64.

         Thank you in advance for any assistance.

	[[alternative HTML version deleted]]


From hussain at touchofmodern.com  Mon Jul 27 22:53:30 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Mon, 27 Jul 2015 13:53:30 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
Message-ID: <1438030410589-4710431.post@n4.nabble.com>

I am trying to use the ggplot2 to build a stacked bar chart for daily Revenue
by category. The chart would look have date on the x-axis, and revenue on
the y axis. The fill would be the categories themselves. I have searched a
great deal and have been unable to find exactly how to do this.



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431.html
Sent from the R help mailing list archive at Nabble.com.


From derawang at udel.edu  Tue Jul 28 00:01:54 2015
From: derawang at udel.edu (Dera)
Date: Mon, 27 Jul 2015 15:01:54 -0700 (PDT)
Subject: [R] =?utf-8?q?Error_=E2=80=9Csystem_is_exactly_singular=3A_U=5B6?=
 =?utf-8?q?=2C6=5D_=3D_0=E2=80=9D_when_doing_nested_logit_using_mlogit_pac?=
 =?utf-8?q?kage_in_R?=
Message-ID: <1438034514863-4710437.post@n4.nabble.com>

 have a problem about doing nested logit using mlogit package in R.

We did a survey about asking people to choose from different insurance
policies. To be specific, respondents were asked ?Would you buy Policy A,
Policy B, or neither?" (Each choice is defined by the deductible and premium
associated with them)

Here is a brief example of our data named "mydata":

RespondentID    Choice    alt      Deductible     Premium
      1         False    PolicyA      500           1000
      1         TRUE     PolicyB     1000            500
      1         False    Neither       0              0
      2         TRUE     PolicyA      250           5000
      2         False    PolicyB      500            500
      2         False    Neither       0              0
      3         TRUE     PolicyA      500           2000
      3         False    PolicyB     5000           1000
      3         False    Neither       0              0
...

I'd like to use nested logit here, because "Policy A" and "Policy B" are
much similar than "Neither". So I tried to use the code to fit a nested
logit model:

nl.1 <- mlogit (Choice~Deductible+Premium, mydata,shape='long', choice =
'Choice', 
         nests=list(buy=c('PolicyA','PolicyA'), no='Neither'), alt.var =
"alt")
But it always gave the error warning:

Error in solve.default(H, g[!fixed]) : Lapack routine dgesv: system is
exactly singular: U[6,6] = 0

If I do like this:

nl.2 <- mlogit (Choice~Deductible+Premium, mydata,shape='long', choice =
'Choice', 
         nests=list(buy=c('PolicyA','PolicyA'), no='Neither'), alt.var =
"alt", un.nest.el = TRUE)
It works. But I don't think the two nests I mention above have unique
elasticity, what "un.nest.el = TRUE" means.

Does anyone know about this? How can I avoid the error in the first code?

Thank you!



--
View this message in context: http://r.789695.n4.nabble.com/Error-system-is-exactly-singular-U-6-6-0-when-doing-nested-logit-using-mlogit-package-in-R-tp4710437.html
Sent from the R help mailing list archive at Nabble.com.


From josephlockhart at hotmail.com  Tue Jul 28 06:06:58 2015
From: josephlockhart at hotmail.com (Jerry)
Date: Mon, 27 Jul 2015 21:06:58 -0700 (PDT)
Subject: [R] Populate data frame for meta-analysis
Message-ID: <1438056418185-4710450.post@n4.nabble.com>



I am trying to model an existing meta-analysis to examine alternative
hypotheses (e.g., doing a random-effects analysis), as well as re-sampling
techniques. There are over 2,000 subjects, but the data is fairly simple: a
binary outcome, success or failure, linked with a score (0-10) on a
structured assessment. I have the frequencies of success or failure for each
score, nested within each study. I am looking for a easier way to create the
dataset rather than keying it in, or using the rep function multiple times.

I would like each row to look something like this: Study_ID,
Test_Result[0-10], Outcome[0 or 1]

For example, let's say I just had two studies and two test levels (1 or 2):
study 1 has 35 successes, and 85 failures for score of "1"; for a score of
"2," 46 successes and 83 failures. In study 2, for a score of "1" there are
78 successes, 246 failures; for a score of "2," 45 successes and 96
failures.

Using just the frequencies provided, how could I most easily create a data
frame with the several hundred lines of data?
Thanks in advance,



--
View this message in context: http://r.789695.n4.nabble.com/Populate-data-frame-for-meta-analysis-tp4710450.html
Sent from the R help mailing list archive at Nabble.com.


From adam.michael.erickson at gmail.com  Tue Jul 28 06:40:22 2015
From: adam.michael.erickson at gmail.com (Adam Erickson)
Date: Mon, 27 Jul 2015 21:40:22 -0700 (PDT)
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <200810082237591675563@flash.net>
References: <mailman.21.1223287205.25404.r-help@r-project.org>,>
	<B46595D426846345AD00664F29B8B2A801625329@MAIL2.ad.uams.edu>,>
	<E3381E449DF449808D92C685A7AA7409@headquarters.silicoinsights>
	<200810082237591675563@flash.net>
Message-ID: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>

I know this is an old thread, but I wrote a simple FOR loop with vectorized 
pattern replacement that is much faster than either of those (it can also 
accept outputs differing in length from the patterns):

  sub2  <- function(pattern, replacement, x) { 
    len   <- length(x)
    y      <- character(length=len)
    patlen <- length(pattern)
    replen <- length(replacement)
    if(patlen != replen) stop('Error: Pattern and replacement length do not 
match')
    for(i in 1:replen) {
      y[which(x==pattern[i])] <- replacement[i]
    }
    return(y)
  }

system.time(test <- sub2(patt, repl, XX))
   user  system elapsed 
      0       0       0 

Cheers,

Adam

On Wednesday, October 8, 2008 at 9:38:01 PM UTC-7, john wrote:
>
> Hello Christos,
>   To my surprise, vectorization actually hurt processing speed!
>
> #Example
> X <- c("ab", "cd", "ef")
> patt <- c("b", "cd", "a")
> repl <- c("B", "CD", "A")
>
> sub2 <- function(pattern, replacement, x) {
>     len <- length(x)
>     if (length(pattern) == 1) 
>         pattern <- rep(pattern, len)
>     if (length(replacement) == 1) 
>         replacement <- rep(replacement, len)
>     FUN <- function(i, ...) {
>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>     }
>     idx <- 1:length(x)
>     sapply(idx, FUN)    
> }
>  
> system.time(  for(i in 1:10000)  sub2(patt, repl, X)  )
>    user  system elapsed 
>    1.18    0.07    1.26 
>
> system.time(  for(i in 1:10000)  mapply(function(p, r, x) sub(p, r, x, 
> fixed = TRUE), p=patt, r=repl, x=X)  )
>    user  system elapsed 
>    1.42    0.05    1.47 
>  
> So much for avoiding loops.
> John Thaden
>
> ======= At 2008-10-07, 14:58:10 Christos wrote: =======
>
> >John,
> >Try the following:
> >
> > mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
> >   b   cd    a 
> >"aB" "CD" "ef"  
> >
> >-Christos
>
> >> -----My Original Message-----
> >> R pattern-matching and replacement functions are
> >> vectorized: they can operate on vectors of targets.
> >> However, they can only use one pattern and replacement.
> >> Here is code to apply a different pattern and replacement for 
> >> every target.  My question: can it be done better?
> >> 
> >> sub2 <- function(pattern, replacement, x) {
> >>     len <- length(x)
> >>     if (length(pattern) == 1) 
> >>         pattern <- rep(pattern, len)
> >>     if (length(replacement) == 1) 
> >>         replacement <- rep(replacement, len)
> >>     FUN <- function(i, ...) {
> >>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
> >>     }
> >>     idx <- 1:length(x)
> >>     sapply(idx, FUN)    
> >> }
> >> 
> >> #Example
> >> X <- c("ab", "cd", "ef")
> >> patt <- c("b", "cd", "a")
> >> repl <- c("B", "CD", "A")
> >> sub2(patt, repl, X)
> >> 
> >> -John
>
> ______________________________________________
> R-h... at r-project.org <javascript:> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>

From mr.ahmeeed at hotmail.com  Tue Jul 28 07:08:25 2015
From: mr.ahmeeed at hotmail.com (Ahmed Al-deeb)
Date: Mon, 27 Jul 2015 22:08:25 -0700 (PDT)
Subject: [R] =?utf-8?b?UiBwYWNrYWdlcyhtYXhMaWsgKCkp4oCP?=
Message-ID: <1438060105916-4710455.post@n4.nabble.com>

Hi all..

I am currently finish recent research to study master's degree in
statistics. And in fact, I faced two problems 
in the practical side using r packages . In the first, generation of the new
distributional data( weibull-lomax dist ) 
and I've successfully overcame them by the following code:


rwl <- function (n,aa,bb,alpha,lambda)
{
 x <- numeric (n)
y1 <- numeric (n)
simlg <- numeric (n)
y1 <- rexp(n, rate = aa)
simlg <- lambda*((y1^(1/bb)+1)^(1/alpha)-1)
return(simlg)
}

library (MASS)
aa <-0.2
bb <- 1.5
alpha <- 6
lambda <- 4
n=1000
x <- numeric (n)
y51 <- numeric (n)
simlg <- numeric(n)
simlg <- rwl(n,aa,bb,alpha,lambda)
tt <- round(max(simlg)+0.5)
ty <- round(min(simlg)-0.5)
for(i in 1:n) x[i] <- tt*((i+ty)/n)
y51 <-
(aa*bb*alpha/lambda)*((1+(x/lambda))^(alpha-1))*(((((1+(x/lambda))^(alpha))-1))^(bb-1))*(exp(-aa*((((1+(x/lambda))^alpha)-1)^bb)))
xrange2 <- range(ty, tt)
yrange2 <- range(density(simlg)$y,y51)
plot(simlg,xlim=xrange2,
ylim=yrange2,axes=TRUE,xlab="X",ylab="pdf",type='n')
lines(x, y51, lty = 2,lwd=2, col =2)





# The second problem, was to estimate the parameters by Maximam likelihood
function.

# I'm trying to get the MLe for a certain distribution using maxLik () 
function.
# I wrote the log-likelihood function as follows:

rwl <- function (n,aa,bb,alpha,lambda)
{
 y1 <- numeric (n)
 simlg <- numeric (n)
 y1 <- rexp(n, rate = aa)
 simlg <- lambda*((y1^(1/bb)+1)^(1/alpha)-1)
 return(simlg)
}

library(maxLik)
set.seed(142)

n <- 200
x <- numeric (n) 
aa <- 0.5
bb <- 4.5
alpha <- 2.7
lambda <- 0.3

x <- rwl(n,aa,bb,alpha,lambda)

param <- numeric(4)
ll <- numeric (n)

maxlikfun<-function(param){
 aa <- param[1]
 bb <- param[2] 
 alpha <- param[3]  
 lambda <- param[4]
 n <- length (x)
 


ll<-(aa*bb*alpha/lambda)*((1+(x/lambda))^(alpha-1))*(((((1+(x/lambda))^(alpha))-1))^(bb-1))*(exp(-aa*((((1+(x/lambda))^alpha)-1)^bb)))

return(log(ll))
 }

param <- numeric(4)
ll <- numeric (n)
  mleBH1 <-
maxBFGS(maxlikfun,start=c(4,5,0.9,3.1),print.level=2,iterlim=200)
  summary (mleBH1)

param <- numeric(4)
ll <- numeric (n)
  mleBH2 <- maxBHHH(maxlikfun,start=c(0.4,5,2.0,0.7),grad =
NULL,finalHessian="BHHH",print.level=2,iterlim=200)
  summary (mleBH2)

param <- numeric(4)
ll <- numeric (n)
  mleBH3 <-
maxLik(maxlikfun,start=c(0.4,5,2.0,0.7),method="BHHH",print.level=2,iterlim=200)
  summary (mleBH3)

 parameters <- c(aa,bb,alpha,lambda)
 coeffs1 <- mleBH1$estimate
 covmat1 <- solve(-(mleBH1$hessian))
 stderr1 <- sqrt(diag(covmat1))
 zscore1 <- coeffs1/stderr1
 pvalue1 <- 2*(1 - pnorm(abs(zscore1)))
 results1 <- cbind(parameters,coeffs1,stderr1,zscore1,pvalue1)
 colnames(results1) <- c("parameters","Coeff.", "Std. Err.", "z", "p value") 
 print(results1)

 parameters <- c(aa,bb,alpha,lambda)
 coeffs2 <- mleBH2$estimate
 covmat2 <- solve(-(mleBH2$hessian))
 stderr2 <- sqrt(diag(covmat2))
 zscore2 <- coeffs2/stderr2
 pvalue2 <- 2*(1 - pnorm(abs(zscore2)))
 results2 <- cbind(parameters,coeffs2,stderr2,zscore2,pvalue2)
 colnames(results2) <- c("parameters","Coeff.", "Std. Err.", "z", "p value") 
 print(results2)

 parameters <- c(aa,bb,alpha,lambda)
 coeffs3 <- mleBH3$estimate
 covmat3 <- solve(-(mleBH3$hessian))
 stderr3 <- sqrt(diag(covmat3))
 zscore3 <- coeffs3/stderr3
 pvalue3 <- 2*(1 - pnorm(abs(zscore3)))
 results3 <- cbind(parameters,coeffs3,stderr3,zscore3,pvalue3)
 colnames(results3) <- c("parameters","Coeff.", "Std. Err.", "z", "p value") 
 print(results3)



But the result estimation was unexpected & the results were usually contain
errors.

Please help me if possible.





--
View this message in context: http://r.789695.n4.nabble.com/R-packages-maxLik-tp4710455.html
Sent from the R help mailing list archive at Nabble.com.


From shreya.cst at gmail.com  Tue Jul 28 07:14:20 2015
From: shreya.cst at gmail.com (shreya ghosh)
Date: Tue, 28 Jul 2015 10:44:20 +0530
Subject: [R] help_ReverseGeocoding
Message-ID: <CAB3ensGkrg_NBJdZ=xsN4s6qExPgHPAbRaysgcLL6yXhaPDvDw@mail.gmail.com>

Hi,
I'm trying to do reversegeocoding on a large dataset. I'm using "RJSONIO"
library and using Google map API to get the location of the given lat-lon
in the dataset. After 100 or 150 successful displaying location information
it is showing
 Warning message - "In readLines(con) : cannot open: HTTP status was '0
(null)'"
and Error : "Error in fromJSON(paste(readLines(con), collapse = "")) :
  error in evaluating the argument 'content' in selecting a method for
function 'fromJSON': Error in readLines(con) : cannot open the connection"

Please help me to solve the issue.

location function is as follows :

location<-function(latlng){
 latlngStr <-  gsub(' ','%20', paste(latlng, collapse=","))
  library("RJSONIO") #Load Library
  #Open Connection
  connectStr <- paste('
http://maps.google.com/maps/api/geocode/json?sensor=false&latlng=',latlngStr,
sep="")
  con <- url(connectStr)
  data.json <- fromJSON(paste(readLines(con), collapse=""))
  close(con)

  data.json <- unlist(data.json)
  if(data.json["status"]=="OK")
    address <- data.json["results.formatted_address"]
  print (address)
}

I'm using R version 3.2.1 and Ubuntu 14.10 OS.

Thank you.



-- 

Shreya Ghosh

*9007448845*

-- The mind is not a vessel to be filled, but a fire to be kindled

	[[alternative HTML version deleted]]


From emipana at hcmr.gr  Tue Jul 28 07:49:56 2015
From: emipana at hcmr.gr (emiP)
Date: Mon, 27 Jul 2015 22:49:56 -0700 (PDT)
Subject: [R] Segmented model
In-Reply-To: <55B6B6B4.1090103@auckland.ac.nz>
References: <1437985308855-4710384.post@n4.nabble.com>
	<55B6B6B4.1090103@auckland.ac.nz>
Message-ID: <1438062596029-4710459.post@n4.nabble.com>

I apologize! I mean R2 (R squared) for each of the two segments-regressions
arising from the model. 



--
View this message in context: http://r.789695.n4.nabble.com/Segmented-model-tp4710384p4710459.html
Sent from the R help mailing list archive at Nabble.com.


From charlotte.hurry at griffithuni.edu.au  Tue Jul 28 08:33:25 2015
From: charlotte.hurry at griffithuni.edu.au (Charlotte)
Date: Mon, 27 Jul 2015 23:33:25 -0700 (PDT)
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
Message-ID: <1438065205709-4710462.post@n4.nabble.com>

Hello

I have count values for abundance which follow a pattern of over-dispersal
with many zero values.  I have read a number of documents which suggest that
I don't use data transforming methods but rather than I run the GLM with the
quasi poisson distribution.  So I have written my script and R is telling me
that Y should be more than 0.

Everything I read tells me to do it this way but I can't get R to agree. 
Did I need to add something else to my script to get it to work and keep my
data untransformed? The script I wrote is as follows:

> fit <- glm(abundance~Gender,data=teminfest,family=binomial())

then I get this error 
Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1

I don't use R a lot so I am having trouble figuring out what to do next.

I would appreciate some help

Many Thanks
Charlotte






--
View this message in context: http://r.789695.n4.nabble.com/R-wont-accept-my-zero-count-values-in-the-GLM-with-quasi-poisson-dsitribution-tp4710462.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Tue Jul 28 08:49:12 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 27 Jul 2015 23:49:12 -0700
Subject: [R] Segmented model
In-Reply-To: <1438062596029-4710459.post@n4.nabble.com>
References: <1437985308855-4710384.post@n4.nabble.com>
	<55B6B6B4.1090103@auckland.ac.nz>
	<1438062596029-4710459.post@n4.nabble.com>
Message-ID: <CAGxFJbRtQ-8SxvX4=4tC73sbnBxgizZ83uxNAL_7hyWWRM8+8Q@mail.gmail.com>

Segmented linear regression with breakpoint locations as parameters is
NON-linear regression. R^2 for nonlinear regression -- and certainly
R^2 for individual segments  in segmented regression-- makes no (or at
best very little) sense. For why this is so, post on a statistics list
like stats.stackexchange.com or consult a local statistical resource.
These are not R issues and so are offtopic here.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Jul 27, 2015 at 10:49 PM, emiP <emipana at hcmr.gr> wrote:
> I apologize! I mean R2 (R squared) for each of the two segments-regressions
> arising from the model.
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Segmented-model-tp4710384p4710459.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From A.Robinson at ms.unimelb.edu.au  Tue Jul 28 08:59:51 2015
From: A.Robinson at ms.unimelb.edu.au (Andrew Robinson)
Date: Tue, 28 Jul 2015 16:59:51 +1000
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <1438065205709-4710462.post@n4.nabble.com>
References: <1438065205709-4710462.post@n4.nabble.com>
Message-ID: <CAHyGmd4ittuyq5CVu9NSCon0s9JPeW70GqEYTou2qqKQ1aV-zw@mail.gmail.com>

You have selected the binomial family in the call to glm.  You should
instead try something like

 family=quasipoisson(link = "log")

I hope this helps

Andrew

On Tue, Jul 28, 2015 at 4:33 PM, Charlotte <
charlotte.hurry at griffithuni.edu.au> wrote:

> Hello
>
> I have count values for abundance which follow a pattern of over-dispersal
> with many zero values.  I have read a number of documents which suggest
> that
> I don't use data transforming methods but rather than I run the GLM with
> the
> quasi poisson distribution.  So I have written my script and R is telling
> me
> that Y should be more than 0.
>
> Everything I read tells me to do it this way but I can't get R to agree.
> Did I need to add something else to my script to get it to work and keep my
> data untransformed? The script I wrote is as follows:
>
> > fit <- glm(abundance~Gender,data=teminfest,family=binomial())
>
> then I get this error
> Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
>
> I don't use R a lot so I am having trouble figuring out what to do next.
>
> I would appreciate some help
>
> Many Thanks
> Charlotte
>
>
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/R-wont-accept-my-zero-count-values-in-the-GLM-with-quasi-poisson-dsitribution-tp4710462.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Andrew Robinson
Deputy Director, CEBRA, School of Biosciences
Reader & Associate Professor in Applied Statistics  Tel: (+61) 0403 138 955
School of Mathematics and Statistics                        Fax: +61-3-8344
4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au
Website: http://www.ms.unimelb.edu.au/~andrewpr

MSME: http://www.crcpress.com/product/isbn/9781439858028
FAwR: http://www.ms.unimelb.edu.au/~andrewpr/FAwR/
SPuR: http://www.ms.unimelb.edu.au/spuRs/

	[[alternative HTML version deleted]]


From goran.brostrom at umu.se  Tue Jul 28 09:05:38 2015
From: goran.brostrom at umu.se (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Tue, 28 Jul 2015 09:05:38 +0200
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <1438065205709-4710462.post@n4.nabble.com>
References: <1438065205709-4710462.post@n4.nabble.com>
Message-ID: <55B729C2.7050904@umu.se>



On 28/07/15 08:33, Charlotte wrote:
> Hello
>
> I have count values for abundance which follow a pattern of
> over-dispersal with many zero values.  I have read a number of
> documents which suggest that I don't use data transforming methods
> but rather than I run the GLM with the quasi poisson distribution.
> So I have written my script and R is telling me that Y should be more
> than 0.

No,  R  is telling you that you must have 0 <= y <= 1 (see below).
For count data you should not use the binomial family, but rather 
'poisson', or 'quasipoisson'.

?family

G?ran

> Everything I read tells me to do it this way but I can't get R to
> agree. Did I need to add something else to my script to get it to
> work and keep my data untransformed? The script I wrote is as
> follows:
>
>> fit <- glm(abundance~Gender,data=teminfest,family=binomial())
>
> then I get this error Error in eval(expr, envir, enclos) : y values
> must be 0 <= y <= 1
>
> I don't use R a lot so I am having trouble figuring out what to do
> next.
>
> I would appreciate some help
>
> Many Thanks Charlotte
>
>
>
>
>
>
> -- View this message in context:
> http://r.789695.n4.nabble.com/R-wont-accept-my-zero-count-values-in-the-GLM-with-quasi-poisson-dsitribution-tp4710462.html
>
>
Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________ R-help at r-project.org
> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide http://www.R-project.org/posting-guide.html and provide
> commented, minimal, self-contained, reproducible code.
>


From murdoch.duncan at gmail.com  Tue Jul 28 11:39:55 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 28 Jul 2015 05:39:55 -0400
Subject: [R] R load error
In-Reply-To: <73551C12E91468488D61D86CDD49C930026D1C265CE5@fcrrex.fcrr.net>
References: <73551C12E91468488D61D86CDD49C930026D1C265CE5@fcrrex.fcrr.net>
Message-ID: <55B74DEB.2010702@gmail.com>

On 27/07/2015 3:03 PM, Yaacov Petscher wrote:
> Greetings - I'm using RStudio and recently updated both it and R. When loadings up, I'm now receiving the following error:
> 
> Error: ReadItem: unknown type 63, perhaps written by later version of R
> 
> I've tried using rm(list=ls())   rm(list=ls(all.names=TRUE)) and detach() but nothing works in R. Within RStudio it just continues to try and load and I'm unable to interrupt or restart. Any ideas would be greatly appreciated.

For the RStudio error (retrying ad infinitum), you should report the
problem to them.  This list is just for R.

The likely cause of that error in R is a corrupted file.  That might be
.Rdata, or whatever file you're trying to load or read.

Duncan Murdoch


From petr.pikal at precheza.cz  Tue Jul 28 13:28:25 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 28 Jul 2015 11:28:25 +0000
Subject: [R] write.table with append=T after using cat on same file
In-Reply-To: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
References: <074C83DAD4825242A20B2D83FDBCB8881A059A8E@EX10MBOX03.pnnl.gov>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C37237@SRVEXCHMBX.precheza.cz>

Hi

Your example works for me. Error is on your side.

Try ?traceback or start with plain R -vanilla or upgrade R.

> sessionInfo()
R Under development (unstable) (2015-06-15 r68521)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows XP (build 2600) Service Pack 3

locale:
[1] LC_COLLATE=Czech_Czech Republic.1250  LC_CTYPE=Czech_Czech Republic.1250
[3] LC_MONETARY=Czech_Czech Republic.1250 LC_NUMERIC=C
[5] LC_TIME=Czech_Czech Republic.1250

attached base packages:
[1] stats     datasets  utils     grDevices graphics  methods   base

other attached packages:
[1] lattice_0.20-31 fun_0.1

loaded via a namespace (and not attached):
 [1] Rcpp_0.11.6      digest_0.6.8     MASS_7.3-40      grid_3.3.0
 [5] plyr_1.8.3       nlme_3.1-120     gtable_0.1.2     magrittr_1.5
 [9] scales_0.2.5     ggplot2_1.0.1    stringi_0.4-1    reshape2_1.4.1
[13] proto_0.3-10     tools_3.3.0      stringr_1.0.0    munsell_0.4.2
[17] colorspace_1.2-6

> version
               _
platform       i386-w64-mingw32
arch           i386
os             mingw32
system         i386, mingw32
status         Under development (unstable)
major          3
minor          3.0
year           2015
month          06
day            15
svn rev        68521
language       R
version.string R Under development (unstable) (2015-06-15 r68521)
nickname       Unsuffered Consequences

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Waichler, Scott R
> Sent: Monday, July 27, 2015 10:32 PM
> To: R. Help
> Subject: [R] write.table with append=T after using cat on same file
>
> Hi,
>
> For years I've been writing text to the beginning of files with
> cat(append=F) , then following that text with data written by
> write.table(append=T).  It is now giving me an error message.  I'm
> using R-3.1.2.  What gives?
>
> df <- data.frame(x = 1, y = 1:10, z = 10:1)
> cat(file="junk.txt", sep="", "# An introductory note.\n")
> write.table(df, file="junk.txt", sep=",", append=T, quote=F,
> row.names=F, col.names=F)
>
> Error in file(file, ifelse(append, "a", "w")) : invalid 'open' argument
>
> Thanks,
> Scott Waichler
> Pacific Northwest National Laboratory
> Richland, WA  USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From drjimlemon at gmail.com  Tue Jul 28 13:52:33 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 28 Jul 2015 21:52:33 +1000
Subject: [R] Populate data frame for meta-analysis
In-Reply-To: <1438056418185-4710450.post@n4.nabble.com>
References: <1438056418185-4710450.post@n4.nabble.com>
Message-ID: <CA+8X3fUiO=+Oa08x1f65R7B589szPcU23ZrZ9+664mZG8-1zcg@mail.gmail.com>

Hi Jerry,
Try this:

jl.df<-read.table(text="ID,score,success,failure
study1,1,35,85
study1,2,46,83
study2,1,78,246
study2,2,45,96",
sep=",",
header=TRUE)

nrows<-dim(jl.df)[1]
jlexp.df<-data.frame()
for(row in 1:nrows) {
 success_rows<-data.frame(ID=rep(jl.df$ID[row],jl.df$success[row]),
  score=rep(jl.df$score[row],jl.df$success[row]),
  outcome=rep("success",jl.df$success[row]))
 failure_rows<-data.frame(ID=rep(jl.df$ID[row],jl.df$failure[row]),
  score=rep(jl.df$score[row],jl.df$failure[row]),
  outcome=rep("failure",jl.df$failure[row]))
 jlexp.df<-rbind(jlexp.df,success_rows,failure_rows)
}

Jim


On Tue, Jul 28, 2015 at 2:06 PM, Jerry <josephlockhart at hotmail.com> wrote:
>
>
> I am trying to model an existing meta-analysis to examine alternative
> hypotheses (e.g., doing a random-effects analysis), as well as re-sampling
> techniques. There are over 2,000 subjects, but the data is fairly simple: a
> binary outcome, success or failure, linked with a score (0-10) on a
> structured assessment. I have the frequencies of success or failure for each
> score, nested within each study. I am looking for a easier way to create the
> dataset rather than keying it in, or using the rep function multiple times.
>
> I would like each row to look something like this: Study_ID,
> Test_Result[0-10], Outcome[0 or 1]
>
> For example, let's say I just had two studies and two test levels (1 or 2):
> study 1 has 35 successes, and 85 failures for score of "1"; for a score of
> "2," 46 successes and 83 failures. In study 2, for a score of "1" there are
> 78 successes, 246 failures; for a score of "2," 45 successes and 96
> failures.
>
> Using just the frequencies provided, how could I most easily create a data
> frame with the several hundred lines of data?
> Thanks in advance,
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Populate-data-frame-for-meta-analysis-tp4710450.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From john.maindonald at anu.edu.au  Tue Jul 28 13:55:39 2015
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Tue, 28 Jul 2015 11:55:39 +0000
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <mailman.2.1438077603.22560.r-help@r-project.org>
References: <mailman.2.1438077603.22560.r-help@r-project.org>
Message-ID: <8F6A2D45-C47F-4E88-AF2E-AC20364FB813@anu.edu.au>

A further point to note is that with a log link, SEs for comparisons with any factor
level where counts are all zero will be huge and meaningless.  This phenomenon
has the name Hauck-Donner effect, though more commonly so identified for
comparisons with categories with very low or very high estimated proportions
for binomial data.  For the poisson or quasipoisson family, use of the sqrt link
avoids this problem.


John Maindonald             email: john.maindonald at anu.edu.au<mailto:john.maindonald at anu.edu.au>


On 28/07/2015, at 22:00, r-help-request at r-project.org<mailto:r-help-request at r-project.org> wrote:

From: Andrew Robinson <A.Robinson at ms.unimelb.edu.au<mailto:A.Robinson at ms.unimelb.edu.au>>
Subject: Re: [R] R wont accept my zero count values in the GLM with quasi_poisson dsitribution
Date: 28 July 2015 18:59:51 NZST
To: Charlotte <charlotte.hurry at griffithuni.edu.au<mailto:charlotte.hurry at griffithuni.edu.au>>
Cc: "R help (r-help at r-project.org<mailto:r-help at r-project.org>)" <r-help at r-project.org<mailto:r-help at r-project.org>>


You have selected the binomial family in the call to glm.  You should
instead try something like

family=quasipoisson(link = "log")

I hope this helps

Andrew

On Tue, Jul 28, 2015 at 4:33 PM, Charlotte <
charlotte.hurry at griffithuni.edu.au<mailto:charlotte.hurry at griffithuni.edu.au>> wrote:

Hello

I have count values for abundance which follow a pattern of over-dispersal
with many zero values.  I have read a number of documents which suggest
that
I don't use data transforming methods but rather than I run the GLM with
the
quasi poisson distribution.  So I have written my script and R is telling
me
that Y should be more than 0.

Everything I read tells me to do it this way but I can't get R to agree.
Did I need to add something else to my script to get it to work and keep my
data untransformed? The script I wrote is as follows:

fit <- glm(abundance~Gender,data=teminfest,family=binomial())

then I get this error
Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1

I don't use R a lot so I am having trouble figuring out what to do next.

I would appreciate some help

Many Thanks
Charlotte






--
View this message in context:
http://r.789695.n4.nabble.com/R-wont-accept-my-zero-count-values-in-the-GLM-with-quasi-poisson-dsitribution-tp4710462.html
Sent from the R help mailing list archive at Nabble.com<http://nabble.com/>.

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
and provide commented, minimal, self-contained, reproducible code.




--
Andrew Robinson
Deputy Director, CEBRA, School of Biosciences
Reader & Associate Professor in Applied Statistics  Tel: (+61) 0403 138 955
School of Mathematics and Statistics                        Fax: +61-3-8344
4599
University of Melbourne, VIC 3010 Australia
Email: a.robinson at ms.unimelb.edu.au<mailto:a.robinson at ms.unimelb.edu.au>
Website: http://www.ms.unimelb.edu.au/~andrewpr

MSME: http://www.crcpress.com/product/isbn/9781439858028
FAwR: http://www.ms.unimelb.edu.au/~andrewpr/FAwR/
SPuR: http://www.ms.unimelb.edu.au/spuRs/


	[[alternative HTML version deleted]]


From j.para.fernandez at hotmail.com  Tue Jul 28 14:22:41 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Tue, 28 Jul 2015 05:22:41 -0700 (PDT)
Subject: [R] Global variables
Message-ID: <1438086161368-4710472.post@n4.nabble.com>

Hi, I want to pass a variable value from one function to another, but not as
a function argument. For this propose I have put <<-, but it doesn?t work.

My code:

one<-function(){

a<<-"variable passed"
}
two<-function(){
print(a)
}

dos()

If I execute dos(), then the error message is:

Error in print(a) : object 'a' not found


Thanks"!




--
View this message in context: http://r.789695.n4.nabble.com/Global-variables-tp4710472.html
Sent from the R help mailing list archive at Nabble.com.


From lists at dewey.myzen.co.uk  Tue Jul 28 14:50:15 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 28 Jul 2015 13:50:15 +0100
Subject: [R] Global variables
In-Reply-To: <1438086161368-4710472.post@n4.nabble.com>
References: <1438086161368-4710472.post@n4.nabble.com>
Message-ID: <55B77A87.2070404@dewey.myzen.co.uk>

In line comments

On 28/07/2015 13:22, jpara3 wrote:
> Hi, I want to pass a variable value from one function to another, but not as
> a function argument. For this propose I have put <<-, but it doesn?t work.
>
> My code:
>
> one<-function(){
>
> a<<-"variable passed"
> }

So you have to execute one() first?

> two<-function(){
> print(a)
> }
>

So go
one()
here
> dos()
>

I suppose you meant two() pero sin problema.

> If I execute dos(), then the error message is:
>
> Error in print(a) : object 'a' not found
>
>
> Thanks"!
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Global-variables-tp4710472.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From jdnewmil at dcn.davis.CA.us  Tue Jul 28 14:56:35 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Tue, 28 Jul 2015 08:56:35 -0400
Subject: [R] Global variables
In-Reply-To: <1438086161368-4710472.post@n4.nabble.com>
References: <1438086161368-4710472.post@n4.nabble.com>
Message-ID: <9C769873-56E0-423E-AFAD-A7F25915C41E@dcn.davis.CA.us>

Please don't. Function arguments are good... global variables are bad.

one <- function(){
  result <- list( a="variable passed" )
  result
}
two <- function( v ){
  print( v$a )
}

x <- one()
two( x )

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 28, 2015 8:22:41 AM EDT, jpara3 <j.para.fernandez at hotmail.com> wrote:
>Hi, I want to pass a variable value from one function to another, but
>not as
>a function argument. For this propose I have put <<-, but it doesn?t
>work.
>
>My code:
>
>one<-function(){
>
>a<<-"variable passed"
>}
>two<-function(){
>print(a)
>}
>
>dos()
>
>If I execute dos(), then the error message is:
>
>Error in print(a) : object 'a' not found
>
>
>Thanks"!
>
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Global-variables-tp4710472.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From kmezhoud at gmail.com  Tue Jul 28 15:00:59 2015
From: kmezhoud at gmail.com (Karim Mezhoud)
Date: Tue, 28 Jul 2015 14:00:59 +0100
Subject: [R] Global variables
In-Reply-To: <1438086161368-4710472.post@n4.nabble.com>
References: <1438086161368-4710472.post@n4.nabble.com>
Message-ID: <CALJKBv8QjEBuOeWYLDa-CNakP39Grafs+Nt8VCzg0AM7oPMLKQ@mail.gmail.com>

normally that works,
BUT <<- is BAD and not accepted in some repositories as Bioconductor.

one<-function(){

a<-"variable passed"
return(a)
}

x <- one()

two<-function(x){
print(x)
}

On Tue, Jul 28, 2015 at 1:22 PM, jpara3 <j.para.fernandez at hotmail.com>
wrote:

> Hi, I want to pass a variable value from one function to another, but not
> as
> a function argument. For this propose I have put <<-, but it doesn?t work.
>
> My code:
>
> one<-function(){
>
> a<<-"variable passed"
> }
> two<-function(){
> print(a)
> }
>
> dos()
>
> If I execute dos(), then the error message is:
>
> Error in print(a) : object 'a' not found
>
>
> Thanks"!
>
>
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Global-variables-tp4710472.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From j.para.fernandez at hotmail.com  Tue Jul 28 15:00:36 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Tue, 28 Jul 2015 06:00:36 -0700 (PDT)
Subject: [R] Stop tkbind
Message-ID: <1438088436481-4710476.post@n4.nabble.com>

Hi, I?m trying this example from the
website(http://www.sciviews.org/_rgui/tcltk/InteractiveTkrPlot.html), but
the problem is that i wnat to integrate to it an stop button that stops the
tkbind. Can someone please help me?

THanks!!

The code->>>

####################################

xCoords<-(-12:13)
yCoords<-xCoords*xCoords
labelsVec <- LETTERS
require(tcltk)
require(tkrplot)
indexLabeled<-c()
labeledPoints <- list()
tt <- tktoplevel()
tkwm.title(tt,"Click on a point to label it")
parPlotSize <- c()
usrCoords <- c()

plotFunction <- function()
{
  params <- par(bg="white")
  plot(xCoords,yCoords,main="Click on a point to label it")
  if (length(indexLabeled)>0)
    for (i in (1:length(indexLabeled)))
    {
      indexClosest <- indexLabeled[i]
      text(xCoords[indexClosest],yCoords[indexClosest],
           labels=labelsVec[indexClosest],pos=3)
    }
  parPlotSize <<- par("plt")
  usrCoords   <<- par("usr")
  par(params)
}

img <- tkrplot(tt,fun=plotFunction,hscale=1.5,vscale=1.5)
tkgrid(img)

labelClosestPoint <- function(xClick,yClick,imgXcoords,imgYcoords)
{
  squared.Distance <- (xClick-imgXcoords)^2 + (yClick-imgYcoords)^2
  indexClosest <- which.min(squared.Distance)
  indexLabeled <<- c(indexLabeled,indexClosest)
  tkrreplot(img)
}

OnLeftClick <- function(x,y)
{
  xClick <- x
  yClick <- y
  require(tcltk)
  width  <- as.numeric(tclvalue(tkwinfo("reqwidth",img)))
  height <- as.numeric(tclvalue(tkwinfo("reqheight",img)))

  xMin <- parPlotSize[1] * width
  xMax <- parPlotSize[2] * width
  yMin <- parPlotSize[3] * height
  yMax <- parPlotSize[4] * height

  rangeX <- usrCoords[2] - usrCoords[1]
  rangeY <- usrCoords[4] - usrCoords[3]

  imgXcoords <- (xCoords-usrCoords[1])*(xMax-xMin)/rangeX + xMin
  imgYcoords <- (yCoords-usrCoords[3])*(yMax-yMin)/rangeY + yMin

  xClick <- as.numeric(xClick)+0.5
  yClick <- as.numeric(yClick)+0.5
  yClick <- height - yClick

  xPlotCoord <- usrCoords[1]+(xClick-xMin)*rangeX/(xMax-xMin)
  yPlotCoord <- usrCoords[3]+(yClick-yMin)*rangeY/(yMax-yMin)

  msg <- paste("Label the point closest to these approximate plot
coordinates: \n",
               "x =",format(xPlotCoord,digits=2),",y
=",format(yPlotCoord,digits=2),"?")
  mbval<- tkmessageBox(title="Label Point Closest to These Approximate Plot
Coordinates",
                       message=msg,type="yesno",icon="question")

  if (tclvalue(mbval)=="yes")
    labelClosestPoint(xClick,yClick,imgXcoords,imgYcoords)
}

tkbind(img, "<Button-1>",OnLeftClick)
tkconfigure(img,cursor="hand2")






--
View this message in context: http://r.789695.n4.nabble.com/Stop-tkbind-tp4710476.html
Sent from the R help mailing list archive at Nabble.com.


From attenka at utu.fi  Tue Jul 28 10:22:00 2015
From: attenka at utu.fi (Atte Tenkanen)
Date: Tue, 28 Jul 2015 11:22:00 +0300
Subject: [R] Opposite color in R
Message-ID: <55B73BA8.7080402@utu.fi>

It seems that there is no implementation for the "traditional artist's 
color circle" in R. However I'm searching for such a wheel, because my 
program needs it.

As said, the description of complementary/opposite-function in package 
"colortools" is misleading since, for example

opposite("green") produces violet, not red, but the description of 
complementary-function says

"Complementary or opposite color scheme is formed by colors that are 
opposite each other on the color wheel (example: red and green)."

So, there must be just a lapse in the text.

I "constrained" such kind of a color wheel, which is enough near of what 
I need:

library(colorspace)

ColorsRYB=rbind(colorRamp(c("red", 
"violet"))((0:4)/4)[1:4,],colorRamp(c("violet", 
"blue"))((0:4)/4)[1:4,],colorRamp(c("blue", 
"green"))((0:4)/4)[1:4,],colorRamp(c("green", 
"yellow"))((0:4)/4)[1:4,],colorRamp(c("yellow", 
"orange"))((0:4)/4)[1:4,],colorRamp(c("orange", "red"))((0:4)/4)[1:4,])

LenCol=length(ColorsRYB[,1])

ColorsRYBhex=rep(0, LenCol)
for(i in 1: LenCol)
{
ColorsRYBhex[i]=rgb(ColorsRYB[i,1]/255,ColorsRYB[i,2]/255,ColorsRYB[i,3]/255)
}

pie(rep(1, 24), col = ColorsRYBhex)

Atte T.


28.7.2015, 2.23, Steve Taylor kirjoitti:
> I wonder if the hcl colour space is useful?  Varying hue while keeping chroma and luminosity constant should give varying colours of perceptually the same "colourness" and brightness.
>
> ?hcl
> pie(rep(1,12),col=hcl((1:12)*30,c=70),border=NA)
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Atte Tenkanen
> Sent: Sunday, 26 July 2015 7:50a
> To: r-help at r-project.org
> Subject: [R] Opposite color in R
>
> Hi,
>
> I have tried to find a way to find opposite or complementary colors in R.
>
> I would like to form a color circle with R like this one:
> http://nobetty.net/dandls/colorwheel/complementary_colors.jpg
>
> If you just make a basic color wheel in R, the colors do not form
> complementary color circle:
>
> palette(rainbow(24))
> Colors=palette()
> pie(rep(1, 24), col = Colors)
>
> There is a package ?colortools? where you can find function opposite(),
> but it doesn?t work as is said. I tried
>
> library(colortools)
> opposite("violet") and got green instead of yellow and
>
> opposite("blue") and got yellow instead of orange.
>
> Do you know any solutions?
>
> Atte Tenkanen
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jan.siebert at ibes.uni-due.de  Tue Jul 28 15:05:28 2015
From: jan.siebert at ibes.uni-due.de (Siebert)
Date: Tue, 28 Jul 2015 06:05:28 -0700 (PDT)
Subject: [R] Problem to optimize a truncated function with auglag (alabama)
Message-ID: <1438088728531-4710478.post@n4.nabble.com>

Hello,
I use the auglag-command of the alabama package to optimize a function under
constraints. The function is truncated at -400. I am afraid, that the
auglag-command has problems to maximize this function.  The solution found
by the auglag-command is not even close to the optimum. It is easy to show
that other values than the solution found by auglag are producing higher
output values and still fulfill the constraints. Does anybody know how I can
optimize that function with auglag or with another R-command?
Thank you very much for any commands and suggestions.

The following explanation is just to understand the background of the
optimization problem. It has nothing to do with the code problem. The
function goes back to an economic intertemporal consumption model. I am
looking for an optimal solution for the consumption-saving-lifetime-plan. 
The utility is the sum of the utilities of 30 periods. The utility in each
period depends on the current consumption and on a habit stock it is never
below -400. The habit stock is a function of the consumption in the last
periods.

# Here is the data I am using:
expectations<-c(124.671, 49.238595, 15.2649945, 85.8809574, 130.43232916875, 
                45.1340382917813, 14.8597845206344, 121.3933676634,
75.1980497225319, 
                76.9945912185161, 184.798095407901, 139.642709300636,
182.57573338504, 
                322.923072570281, 275.238211364469, 155.536259099569,
54.7838218320803, 
                285.968249457082, 644.995614202314, 385.243665085798,
60.2035902594974, 
                218.295491752216, 168.796319321643, 387.193213147244,
110.975689063189, 
                333.047669801961, 81.5607312506053, 253.458002848022,
28.3825973908511, 
                458.282304918595)

habit_ex<-10

# Here is the function I like to optimize:

utility<-function(consum){
  period<<-0
  consum<<-consum
  sum(
    sapply(consum, utility_t)
  )
}

utility_t<-function(consum_t){
  period<<-period+1
  max(c(-400, (40+750*(((consum_t+2.7)/habit(consum, period
  )^0.6)^-2/-2))*5.5))
}

habit<-function(consum, period){if (period==1){habit_ex} else
{0.7*habit(consum, period-1)+consum[(period-1)]}}

# Here are the constraints:

hin<-function(consum){
  h<-rep(NA, 60)
    for (j in 1:30){
      h[j]<-  sum(expectations[1:j]) - sum(consum[1:j])
      h[(j+30)]<-consum[j]
    }
  h
}

# Here is the code to optimize:

install.packages("alabama")
library(alabama)

start<-c(1:30)

ans <- auglag(start, fn=utility, hin=hin, control.optim = list(fnscale =
-1), control.outer = list(sig0 = 100, trace=TRUE))



# Here is an example for input values that have a higher output than the
solution of auglag:  
opt1<-c(15.057, 27.135, 41.067, 55.62, 70.047, 83.79, 96.516, 108.558, 
119.637, 120, 139.707, 148.77, 157.185, 165.339, 174.249, 182.295, 
190.314, 199.359, 208.485, 219.168, 228.816, 240.12, 253.782, 
267.39, 237.705, 254.475, 280.41, 312.8925, 372.9975, 484.432)
utility(opt1)




--
View this message in context: http://r.789695.n4.nabble.com/Problem-to-optimize-a-truncated-function-with-auglag-alabama-tp4710478.html
Sent from the R help mailing list archive at Nabble.com.


From wadud.miah at gmail.com  Tue Jul 28 12:05:33 2015
From: wadud.miah at gmail.com (W. Miah)
Date: Tue, 28 Jul 2015 11:05:33 +0100
Subject: [R] R and Intel compiler
Message-ID: <CAK7hdOAP0FrH1+wh=nVgsmJpQ4QagDsacEy-qSdQzcaWMrOkpQ@mail.gmail.com>

Hello,

Has anyone compared the performance of R built with the Intel and GNU
compilers? The Intel compiler comes with very good vectorisation features
which I imagine will run faster for R. Any experiences with both compilers
will be appreciated.

Best regards,

--------------------
Wadud Miah
Sent via Samsung Galaxy mobile

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Jul 28 15:31:40 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 28 Jul 2015 09:31:40 -0400
Subject: [R] Element-by-element division
In-Reply-To: <55B6D6B1.9060502@gmail.com>
References: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>
	<CAM_vjunk9ObntqZ2D+23xVp9c0_mB7YU-9YjYTsR1DXXO8MBoA@mail.gmail.com>
	<55B6D6B1.9060502@gmail.com>
Message-ID: <CAM_vju=gHAEn_yBD1wt95+ATtszX+tHaBob6kTE-w64mbDCKHw@mail.gmail.com>

Hi,

It's a good idea to keep discussion on R-help, so others can
participate and the results make it into the archives.

On Mon, Jul 27, 2015 at 9:11 PM, Steven Yen <syen04 at gmail.com> wrote:
> Thanks Sarah. That serves my need. I however find ?sweep hard to comprehend.

Heh. The help makes it seem more complicated than it really is. It
isn't that hard:

sweep(a, 2, b, "/")

a: the object to act on
2: the direction to go (1 for rows, 2 for columns - a has 2 columns
and b is of length 2, so you need to choose columns)
b: the vector to use (?sweep calls this "the summary statistic"
because the use case was originally conceived of as being: "divide
columns by standard deviation" and such)
"/": the function to use

so to add vector x to the rows of a, you'd do:
sweep(a, 1, x, "+")

The default FUN is "-", so the first example in the help subtracts the
median from the columns:
     require(stats) # for median
     med.att <- apply(attitude, 2, median)
     sweep(data.matrix(attitude), 2, med.att)  # subtract the column medians

The complicated bits come in if b is an array instead of a vector, or
if the dimensions aren't identical. For your case, and for most cases,
none of that matters.

Sarah

> What am I missing? The S language?
> Steven Yen
>
> On 7/27/2015 4:17 PM, Sarah Goslee wrote:
>
> Hi,
>
> See ?sweep
>
> For instance, to get your matrix two:
>
> sweep(a, 2, b, "/")
>
>      [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>
> Sarah
>
> On Mon, Jul 27, 2015 at 4:04 PM, Steven Yen <syen04 at gmail.com> wrote:
>
> I need help with element-by-element division. Below, matrices a and c are
> both 5 x 2 and element-by-element division works as (I) expected. What if
> matrix is 1 by 2: to divide first column of a by b[1] and second column of
> a by b[2]. I had to go around (two ways) to make it work. In Gauss, these
> can be dine by a./b and a./c. Any such simple way in R? Thank!
>
> a<-matrix(1:10,nrow=5); a
>
>      [,1] [,2]
> [1,]    1    6
> [2,]    2    7
> [3,]    3    8
> [4,]    4    9
> [5,]    5   10
>
> b<-matrix(c(0.5,0.25),nrow=1); b
>
>      [,1] [,2]
> [1,]  0.5 0.25
>
> c<-matrix(rep(c(0.5,0.25),5),nrow=5,byrow=T); c
>
>      [,1] [,2]
> [1,]  0.5 0.25
> [2,]  0.5 0.25
> [3,]  0.5 0.25
> [4,]  0.5 0.25
> [5,]  0.5 0.25
>
> one<-a/c; one     [,1] [,2]
>
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>
> two<-a/b
>
> Error in a/b : non-conformable arrays
>
> two<-cbind(a[,1]/b[1],a[,2]/b[2]); two
>
>      [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
> b2<-matrix(rep(b,5),nrow=5,byrow=T); b2     [,1] [,2]
>
> [1,]  0.5 0.25
> [2,]  0.5 0.25
> [3,]  0.5 0.25
> [4,]  0.5 0.25
> [5,]  0.5 0.25> a/b2     [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>


From friendly at yorku.ca  Tue Jul 28 15:23:33 2015
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 28 Jul 2015 09:23:33 -0400
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <55B729C2.7050904@umu.se>
References: <1438065205709-4710462.post@n4.nabble.com>
	<55B729C2.7050904@umu.se>
Message-ID: <55B78255.10806@yorku.ca>

On 7/28/2015 3:05 AM, G?ran Brostr?m wrote:
>
>
> On 28/07/15 08:33, Charlotte wrote:
>> Hello
>>
>> I have count values for abundance which follow a pattern of
>> over-dispersal with many zero values.  I have read a number of
>> documents which suggest that I don't use data transforming methods
>> but rather than I run the GLM with the quasi poisson distribution.
>> So I have written my script and R is telling me that Y should be more
>> than 0.
>
> No,  R  is telling you that you must have 0 <= y <= 1 (see below).
> For count data you should not use the binomial family, but rather
> 'poisson', or 'quasipoisson'.


With excess zeros, overdispersion is the symptom, but the quasipoisson
model is not the cure.
Even better than quasipoisson would be to use a ZIP (zero-inflated 
poisson) model or a hurdle model to explicitly model the
excess zeros.

These are handled by the countreg package, at present only
available on R-Forge.

  install.packages("countreg", repos="http://R-Forge.R-project.org")


>
>> ______________________________________________ R-help at r-project.org
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
>> posting guide http://www.R-project.org/posting-guide.html and provide
>> commented, minimal, self-contained, reproducible code.
>>
>


From friendly at yorku.ca  Tue Jul 28 15:23:33 2015
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 28 Jul 2015 09:23:33 -0400
Subject: [R] R wont accept my zero count values in the GLM with
 quasi_poisson dsitribution
In-Reply-To: <55B729C2.7050904@umu.se>
References: <1438065205709-4710462.post@n4.nabble.com>
	<55B729C2.7050904@umu.se>
Message-ID: <55B78255.10806@yorku.ca>

On 7/28/2015 3:05 AM, G?ran Brostr?m wrote:
>
>
> On 28/07/15 08:33, Charlotte wrote:
>> Hello
>>
>> I have count values for abundance which follow a pattern of
>> over-dispersal with many zero values.  I have read a number of
>> documents which suggest that I don't use data transforming methods
>> but rather than I run the GLM with the quasi poisson distribution.
>> So I have written my script and R is telling me that Y should be more
>> than 0.
>
> No,  R  is telling you that you must have 0 <= y <= 1 (see below).
> For count data you should not use the binomial family, but rather
> 'poisson', or 'quasipoisson'.


With excess zeros, overdispersion is the symptom, but the quasipoisson
model is not the cure.
Even better than quasipoisson would be to use a ZIP (zero-inflated 
poisson) model or a hurdle model to explicitly model the
excess zeros.

These are handled by the countreg package, at present only
available on R-Forge.

  install.packages("countreg", repos="http://R-Forge.R-project.org")


>
>> ______________________________________________ R-help at r-project.org
>> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
>> posting guide http://www.R-project.org/posting-guide.html and provide
>> commented, minimal, self-contained, reproducible code.
>>
>


From dcarlson at tamu.edu  Tue Jul 28 15:42:41 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 28 Jul 2015 13:42:41 +0000
Subject: [R] Element-by-element division
In-Reply-To: <CAM_vju=gHAEn_yBD1wt95+ATtszX+tHaBob6kTE-w64mbDCKHw@mail.gmail.com>
References: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>
	<CAM_vjunk9ObntqZ2D+23xVp9c0_mB7YU-9YjYTsR1DXXO8MBoA@mail.gmail.com>
	<55B6D6B1.9060502@gmail.com>
	<CAM_vju=gHAEn_yBD1wt95+ATtszX+tHaBob6kTE-w64mbDCKHw@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6ACF89@mb02.ads.tamu.edu>

apply() will also get you there with almost the same arguments in different order (plus t()):

> t(apply(a, 1, "/", b))
     [,1] [,2]
[1,]    2   24
[2,]    4   28
[3,]    6   32
[4,]    8   36
[5,]   10   40

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sarah Goslee
Sent: Tuesday, July 28, 2015 8:32 AM
To: Steven Yen; r-help
Subject: Re: [R] Element-by-element division

Hi,

It's a good idea to keep discussion on R-help, so others can
participate and the results make it into the archives.

On Mon, Jul 27, 2015 at 9:11 PM, Steven Yen <syen04 at gmail.com> wrote:
> Thanks Sarah. That serves my need. I however find ?sweep hard to comprehend.

Heh. The help makes it seem more complicated than it really is. It
isn't that hard:

sweep(a, 2, b, "/")

a: the object to act on
2: the direction to go (1 for rows, 2 for columns - a has 2 columns
and b is of length 2, so you need to choose columns)
b: the vector to use (?sweep calls this "the summary statistic"
because the use case was originally conceived of as being: "divide
columns by standard deviation" and such)
"/": the function to use

so to add vector x to the rows of a, you'd do:
sweep(a, 1, x, "+")

The default FUN is "-", so the first example in the help subtracts the
median from the columns:
     require(stats) # for median
     med.att <- apply(attitude, 2, median)
     sweep(data.matrix(attitude), 2, med.att)  # subtract the column medians

The complicated bits come in if b is an array instead of a vector, or
if the dimensions aren't identical. For your case, and for most cases,
none of that matters.

Sarah

> What am I missing? The S language?
> Steven Yen
>
> On 7/27/2015 4:17 PM, Sarah Goslee wrote:
>
> Hi,
>
> See ?sweep
>
> For instance, to get your matrix two:
>
> sweep(a, 2, b, "/")
>
>      [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>
> Sarah
>
> On Mon, Jul 27, 2015 at 4:04 PM, Steven Yen <syen04 at gmail.com> wrote:
>
> I need help with element-by-element division. Below, matrices a and c are
> both 5 x 2 and element-by-element division works as (I) expected. What if
> matrix is 1 by 2: to divide first column of a by b[1] and second column of
> a by b[2]. I had to go around (two ways) to make it work. In Gauss, these
> can be dine by a./b and a./c. Any such simple way in R? Thank!
>
> a<-matrix(1:10,nrow=5); a
>
>      [,1] [,2]
> [1,]    1    6
> [2,]    2    7
> [3,]    3    8
> [4,]    4    9
> [5,]    5   10
>
> b<-matrix(c(0.5,0.25),nrow=1); b
>
>      [,1] [,2]
> [1,]  0.5 0.25
>
> c<-matrix(rep(c(0.5,0.25),5),nrow=5,byrow=T); c
>
>      [,1] [,2]
> [1,]  0.5 0.25
> [2,]  0.5 0.25
> [3,]  0.5 0.25
> [4,]  0.5 0.25
> [5,]  0.5 0.25
>
> one<-a/c; one     [,1] [,2]
>
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>
> two<-a/b
>
> Error in a/b : non-conformable arrays
>
> two<-cbind(a[,1]/b[1],a[,2]/b[2]); two
>
>      [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
> b2<-matrix(rep(b,5),nrow=5,byrow=T); b2     [,1] [,2]
>
> [1,]  0.5 0.25
> [2,]  0.5 0.25
> [3,]  0.5 0.25
> [4,]  0.5 0.25
> [5,]  0.5 0.25> a/b2     [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
>

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From j.para.fernandez at hotmail.com  Tue Jul 28 15:41:22 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Tue, 28 Jul 2015 06:41:22 -0700 (PDT)
Subject: [R] Global variables
In-Reply-To: <CALJKBv8QjEBuOeWYLDa-CNakP39Grafs+Nt8VCzg0AM7oPMLKQ@mail.gmail.com>
References: <1438086161368-4710472.post@n4.nabble.com>
	<CALJKBv8QjEBuOeWYLDa-CNakP39Grafs+Nt8VCzg0AM7oPMLKQ@mail.gmail.com>
Message-ID: <1438090882002-4710483.post@n4.nabble.com>

Thanks to all!!!



--
View this message in context: http://r.789695.n4.nabble.com/Global-variables-tp4710473p4710483.html
Sent from the R help mailing list archive at Nabble.com.


From sarah.goslee at gmail.com  Tue Jul 28 15:53:05 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 28 Jul 2015 09:53:05 -0400
Subject: [R] Element-by-element division
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6ACF89@mb02.ads.tamu.edu>
References: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>
	<CAM_vjunk9ObntqZ2D+23xVp9c0_mB7YU-9YjYTsR1DXXO8MBoA@mail.gmail.com>
	<55B6D6B1.9060502@gmail.com>
	<CAM_vju=gHAEn_yBD1wt95+ATtszX+tHaBob6kTE-w64mbDCKHw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6ACF89@mb02.ads.tamu.edu>
Message-ID: <CAM_vjumcmZSeFvqYcew=WoJVmY90Rzzpjg495APaAyMUzEH76A@mail.gmail.com>

On Tue, Jul 28, 2015 at 9:42 AM, David L Carlson <dcarlson at tamu.edu> wrote:
> apply() will also get you there with almost the same arguments in different order (plus t()):

Sure, there are lots of ways to do everything in R. But mixing in
apply muddles the issue, since apply() and sweep() use different logic
to determine MARGIN.

It probably doesn't matter much, but sweep is also a lot more
efficient (possibly because of that pesky t()):


> a <- matrix(runif(500), ncol=10)
> b <- runif(10)
>
> system.time(
+ for(i in 1:50000) {
+    aout <- sweep(a, 2, b, "/")
+ })
   user  system elapsed
  2.628   0.000   2.628
>
>
> system.time(
+ for(i in 1:50000) {
+    aout <- t(apply(a, 1, "/", b))
+ })
   user  system elapsed
  8.294   0.025   8.320


>
>> t(apply(a, 1, "/", b))
>      [,1] [,2]
> [1,]    2   24
> [2,]    4   28
> [3,]    6   32
> [4,]    8   36
> [5,]   10   40
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sarah Goslee
> Sent: Tuesday, July 28, 2015 8:32 AM
> To: Steven Yen; r-help
> Subject: Re: [R] Element-by-element division
>
> Hi,
>
> It's a good idea to keep discussion on R-help, so others can
> participate and the results make it into the archives.
>
> On Mon, Jul 27, 2015 at 9:11 PM, Steven Yen <syen04 at gmail.com> wrote:
>> Thanks Sarah. That serves my need. I however find ?sweep hard to comprehend.
>
> Heh. The help makes it seem more complicated than it really is. It
> isn't that hard:
>
> sweep(a, 2, b, "/")
>
> a: the object to act on
> 2: the direction to go (1 for rows, 2 for columns - a has 2 columns
> and b is of length 2, so you need to choose columns)
> b: the vector to use (?sweep calls this "the summary statistic"
> because the use case was originally conceived of as being: "divide
> columns by standard deviation" and such)
> "/": the function to use
>
> so to add vector x to the rows of a, you'd do:
> sweep(a, 1, x, "+")
>
> The default FUN is "-", so the first example in the help subtracts the
> median from the columns:
>      require(stats) # for median
>      med.att <- apply(attitude, 2, median)
>      sweep(data.matrix(attitude), 2, med.att)  # subtract the column medians
>
> The complicated bits come in if b is an array instead of a vector, or
> if the dimensions aren't identical. For your case, and for most cases,
> none of that matters.
>
> Sarah
>
>> What am I missing? The S language?
>> Steven Yen
>>
>> On 7/27/2015 4:17 PM, Sarah Goslee wrote:
>>
>> Hi,
>>
>> See ?sweep
>>
>> For instance, to get your matrix two:
>>
>> sweep(a, 2, b, "/")
>>
>>      [,1] [,2]
>> [1,]    2   24
>> [2,]    4   28
>> [3,]    6   32
>> [4,]    8   36
>> [5,]   10   40
>>
>>
>> Sarah
>>
>> On Mon, Jul 27, 2015 at 4:04 PM, Steven Yen <syen04 at gmail.com> wrote:
>>
>> I need help with element-by-element division. Below, matrices a and c are
>> both 5 x 2 and element-by-element division works as (I) expected. What if
>> matrix is 1 by 2: to divide first column of a by b[1] and second column of
>> a by b[2]. I had to go around (two ways) to make it work. In Gauss, these
>> can be dine by a./b and a./c. Any such simple way in R? Thank!
>>
>> a<-matrix(1:10,nrow=5); a
>>
>>      [,1] [,2]
>> [1,]    1    6
>> [2,]    2    7
>> [3,]    3    8
>> [4,]    4    9
>> [5,]    5   10
>>
>> b<-matrix(c(0.5,0.25),nrow=1); b
>>
>>      [,1] [,2]
>> [1,]  0.5 0.25
>>
>> c<-matrix(rep(c(0.5,0.25),5),nrow=5,byrow=T); c
>>
>>      [,1] [,2]
>> [1,]  0.5 0.25
>> [2,]  0.5 0.25
>> [3,]  0.5 0.25
>> [4,]  0.5 0.25
>> [5,]  0.5 0.25
>>
>> one<-a/c; one     [,1] [,2]
>>
>> [1,]    2   24
>> [2,]    4   28
>> [3,]    6   32
>> [4,]    8   36
>> [5,]   10   40
>>
>>
>> two<-a/b
>>
>> Error in a/b : non-conformable arrays
>>
>> two<-cbind(a[,1]/b[1],a[,2]/b[2]); two
>>
>>      [,1] [,2]
>> [1,]    2   24
>> [2,]    4   28
>> [3,]    6   32
>> [4,]    8   36
>> [5,]   10   40
>>
>> b2<-matrix(rep(b,5),nrow=5,byrow=T); b2     [,1] [,2]
>>
>> [1,]  0.5 0.25
>> [2,]  0.5 0.25
>> [3,]  0.5 0.25
>> [4,]  0.5 0.25
>> [5,]  0.5 0.25> a/b2     [,1] [,2]
>> [1,]    2   24
>> [2,]    4   28
>> [3,]    6   32
>> [4,]    8   36
>> [5,]   10   40
>>


From macqueen1 at llnl.gov  Tue Jul 28 17:18:02 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Tue, 28 Jul 2015 15:18:02 +0000
Subject: [R] Writing output of a looped process with pdfs
Message-ID: <D1DCE662.132B3A%macqueen1@llnl.gov>

Having done this:

setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/split_fnp/")
shps<- dir(getwd(), "*.shp")
shps <- gsub('.{4}$', '', shps)

You can create the list directly, instead of manually, like this (not
tested, and see ?list):

  fnps <- vector('list', length(shps))
  names(fnps) <- shps

  for (shp in shps) fnps[[shp]] <- readOGR(".",layer=shp)

It is not clear from your posting exactly which command in your calculate
distances loop is failing. Have you stepped through them one at a time
manually?

Your calculation of the name outfile is probably failing because fnp is
not a character string. fnp is a spdf, so it makes no sense to use it in
   paste0("distances_", fnp, ".txt")

If you create fnps as I suggest above, then you can modify your loop along
these lines:

for (nm in names(fnps)) {
  fnp <- fnps[[nm]]
...
...  paste0("distances_", nm, ".txt")
...
}
 
Note that then names of the list fnps were calculated from a directory
listing created using dir(), so they are based on the file names. If any
of the file names have space characters in them, you will want to replace
those spaces with dots or underscores. I also don't know if the names
include the .shp suffix; if so you might want to get rid of that in the
outfile names.

In other words, I've outlined an approach, but there are likely some pesky
details in handling the list element names and output file names based on
the input file names.

-Don


-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/25/15, 2:54 AM, "R-help on behalf of Larrosa, Cecilia"
<r-help-bounces at r-project.org on behalf of
cecilia.larrosa10 at imperial.ac.uk> wrote:

>Hi,
>
>I have created a list of spdfs, and I am looping a process for each of
>them. The process is using dDistance to calculate distances between
>features within each spdf. I want to write the output tables using the
>name of the spdf at each loop, but cannot find a way to do this. It seems
>a rather basic thing to do, and I am not very proficient in R, but I have
>spent several hours looking for a way to do this and failed. I am using R
>studio on a Mac.
>
>Here is the code so far (not the most efficient code):
>
># Load libraries
>library(rgdal)
>library(gdistance)
>
>#Read forest shape files
>setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/split_fnp/")
>shps<- dir(getwd(), "*.shp")
>shps <- gsub('.{4}$', '', shps)
>for (shp in shps) assign(shp, readOGR(".",layer=shp))
>
>#Create list (I did this manually because I could not find another way)
>fnps <- list(a_1, a_10, a_100, a_101, a_102, a_103, a_104, a_105, a_106,
>a_107, a_108, a_109, a_11, a_110,
>             a_111, a_112, a_113, a_12, a_13, a_14, a_15, a_16, a_17,
>a_18, a_19, a_2, a_20, a_21, a_22, a_23,
>             a_24, a_25,  a_26, a_27,  a_28, a_29, a_3, a_30, a_31, a_32,
>a_33, a_34, a_35, a_36, a_37,
>             a_38, a_39, a_4, a_42, a_43, a_44, a_45, a_46, a_47, a_48,
>a_49, a_5, a_50, a_51, a_52,
>             a_53, a_54, a_55,  a_56, a_57, a_6,  a_69, a_7, a_70,  a_73,
>a_79, a_8,  a_80, a_81, a_82,
>             a_83,  a_84, a_85, a_86, a_87, a_88, a_89, a_9,  a_90, a_91,
>a_94, a_95, a_96, a_98, a_99)
>
>
>###  Calculate distance between all polygons
>for (fnp in fnps)
>{
>  distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
>  row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="?);#
>did this because gDistance changed the IDs of the features from [1 to
>...] to [0 to ...], not sure why
>  colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="?); #
>same as above
>  dists.melt <- 
>melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,]; #use only
>lower triangle of the distances matrix
>  outfile <- 
>file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/"
>, paste0("distances_", fnp, ".txt")); # this is the bit that is not
>working
>  write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE)
>}
>
>And this is the error message:
>
>Error in as.character.default(<S4 object of class
>"SpatialPolygonsDataFrame">) :
>  no method for coercing this S4 class to a vector
>
>Can anyone help me with solving the issue? How to call the name of the
>looped spdf to be included in the title of the output table? I really
>appreciate your time!
>
>Cheers
>Cecilia
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Tue Jul 28 17:24:27 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Tue, 28 Jul 2015 15:24:27 +0000
Subject: [R] Compute z
In-Reply-To: <A2F7B866-42DC-4E84-BE64-D76DAE80134C@dcn.davis.CA.us>
References: <55b49d00.4676460a.5615.ffffdc34@mx.google.com>
	<A2F7B866-42DC-4E84-BE64-D76DAE80134C@dcn.davis.CA.us>
Message-ID: <D1DCEC64.132B70%macqueen1@llnl.gov>

To which I will add:

Start up R.
At the prompt, type
  ?Control
This will show you the syntax for 'for' loops.

(is this homework?)

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/26/15, 8:19 AM, "R-help on behalf of Jeff Newmiller"
<r-help-bounces at r-project.org on behalf of jdnewmil at dcn.davis.ca.us> wrote:

>sum(v*w)
>
>There are no "column vectors" in R... there are vectors (that have no
>"direction"), and there are data frames that might only have one column,
>and matrices that might have many rows but only one column, and a piece
>of matrix or data frame is often converted to a vector when indexing is
>used to extract a column or row (e.g. mat[ , 1 ]).
>
>It may feel too dense to absorb at first, but the Introduction to R
>document that comes with R actually explains all this. Try (re)reading
>that occasionally until it sinks in.
>--------------------------------------------------------------------------
>-
>Jeff Newmiller                        The     .....       .....  Go
>Live...
>DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
>Go...
>                                      Live:   OO#.. Dead: OO#..  Playing
>Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
>/Software/Embedded Controllers)               .OO#.       .OO#.
>rocks...1k
>--------------------------------------------------------------------------
>- 
>Sent from my phone. Please excuse my brevity.
>
>On July 26, 2015 1:38:24 AM PDT, admin.dslcomputer at gmail.com wrote:
>>Hi Everyone:
>>
>>
>>How do I correctly compute z?
>>
>>
>>
>>z = 0;
>>for i = 1:7
>>  z = z + v(i) * w(i)
>>end
>>
>>
>>If there are two column vectors v and w, each with 7 elements (i.e.,
>>they have dimensions 7x1).
>>
>>
>>Regards,
>>
>>Hal
>>
>>
>>
>>
>>
>>
>>Sent from Surface
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Tue Jul 28 17:30:42 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Tue, 28 Jul 2015 15:30:42 +0000
Subject: [R] help_ReverseGeocoding
In-Reply-To: <CAB3ensGkrg_NBJdZ=xsN4s6qExPgHPAbRaysgcLL6yXhaPDvDw@mail.gmail.com>
References: <CAB3ensGkrg_NBJdZ=xsN4s6qExPgHPAbRaysgcLL6yXhaPDvDw@mail.gmail.com>
Message-ID: <D1DCEDCC.132B77%macqueen1@llnl.gov>

My first guess, after a quick glance, is that Google only lets you do a
limited number of lookups within some period of time.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 7/27/15, 10:14 PM, "R-help on behalf of shreya ghosh"
<r-help-bounces at r-project.org on behalf of shreya.cst at gmail.com> wrote:

>Hi,
>I'm trying to do reversegeocoding on a large dataset. I'm using "RJSONIO"
>library and using Google map API to get the location of the given lat-lon
>in the dataset. After 100 or 150 successful displaying location
>information
>it is showing
> Warning message - "In readLines(con) : cannot open: HTTP status was '0
>(null)'"
>and Error : "Error in fromJSON(paste(readLines(con), collapse = "")) :
>  error in evaluating the argument 'content' in selecting a method for
>function 'fromJSON': Error in readLines(con) : cannot open the connection"
>
>Please help me to solve the issue.
>
>location function is as follows :
>
>location<-function(latlng){
> latlngStr <-  gsub(' ','%20', paste(latlng, collapse=","))
>  library("RJSONIO") #Load Library
>  #Open Connection
>  connectStr <- paste('
>http://maps.google.com/maps/api/geocode/json?sensor=false&latlng=',latlngS
>tr,
>sep="")
>  con <- url(connectStr)
>  data.json <- fromJSON(paste(readLines(con), collapse=""))
>  close(con)
>
>  data.json <- unlist(data.json)
>  if(data.json["status"]=="OK")
>    address <- data.json["results.formatted_address"]
>  print (address)
>}
>
>I'm using R version 3.2.1 and Ubuntu 14.10 OS.
>
>Thank you.
>
>
>
>-- 
>
>Shreya Ghosh
>
>*9007448845*
>
>-- The mind is not a vessel to be filled, but a fire to be kindled
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bob at rudis.net  Tue Jul 28 18:08:31 2015
From: bob at rudis.net (boB Rudis)
Date: Tue, 28 Jul 2015 12:08:31 -0400
Subject: [R] help_ReverseGeocoding
In-Reply-To: <D1DCEDCC.132B77%macqueen1@llnl.gov>
References: <CAB3ensGkrg_NBJdZ=xsN4s6qExPgHPAbRaysgcLL6yXhaPDvDw@mail.gmail.com>
	<D1DCEDCC.132B77%macqueen1@llnl.gov>
Message-ID: <CAJ4QxaOE2Ddz9dw5XEqO1FXJmxaYGZYrwExMPOwJO5COf09y+g@mail.gmail.com>

You should use ggmap::revgeocode (it calls google's api) and google
will rate-limit you. There are also packages to use HERE maps
geo/revgeo lookups
http://blog.corynissen.com/2014/10/making-r-package-to-use-here-geocode-api.html
and the geocode package has GNfindNearestAddress, so tons of options
to choose from.

On Tue, Jul 28, 2015 at 11:30 AM, MacQueen, Don <macqueen1 at llnl.gov> wrote:
> My first guess, after a quick glance, is that Google only lets you do a
> limited number of lookups within some period of time.
>
> -Don
>
> --
> Don MacQueen
>
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
>
>
>
>
>
> On 7/27/15, 10:14 PM, "R-help on behalf of shreya ghosh"
> <r-help-bounces at r-project.org on behalf of shreya.cst at gmail.com> wrote:
>
>>Hi,
>>I'm trying to do reversegeocoding on a large dataset. I'm using "RJSONIO"
>>library and using Google map API to get the location of the given lat-lon
>>in the dataset. After 100 or 150 successful displaying location
>>information
>>it is showing
>> Warning message - "In readLines(con) : cannot open: HTTP status was '0
>>(null)'"
>>and Error : "Error in fromJSON(paste(readLines(con), collapse = "")) :
>>  error in evaluating the argument 'content' in selecting a method for
>>function 'fromJSON': Error in readLines(con) : cannot open the connection"
>>
>>Please help me to solve the issue.
>>
>>location function is as follows :
>>
>>location<-function(latlng){
>> latlngStr <-  gsub(' ','%20', paste(latlng, collapse=","))
>>  library("RJSONIO") #Load Library
>>  #Open Connection
>>  connectStr <- paste('
>>http://maps.google.com/maps/api/geocode/json?sensor=false&latlng=',latlngS
>>tr,
>>sep="")
>>  con <- url(connectStr)
>>  data.json <- fromJSON(paste(readLines(con), collapse=""))
>>  close(con)
>>
>>  data.json <- unlist(data.json)
>>  if(data.json["status"]=="OK")
>>    address <- data.json["results.formatted_address"]
>>  print (address)
>>}
>>
>>I'm using R version 3.2.1 and Ubuntu 14.10 OS.
>>
>>Thank you.
>>
>>
>>
>>--
>>
>>Shreya Ghosh
>>
>>*9007448845*
>>
>>-- The mind is not a vessel to be filled, but a fire to be kindled
>>
>>       [[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide
>>http://www.R-project.org/posting-guide.html
>>and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From John.Szumiloski at bms.com  Tue Jul 28 18:58:19 2015
From: John.Szumiloski at bms.com (Szumiloski, John)
Date: Tue, 28 Jul 2015 16:58:19 +0000
Subject: [R] ggplot ternary plot: two quirks
Message-ID: <759b75d09bef4582a8a34b5108061181@CO2PR26MB0011.067d.mgd.msft.net>

Dear useRs,

I am using the ggtern package to generate ternary plots in ggplot2.  I am making a diagram demonstrating how to interpret a ternary plot and am using ggtern::ggtern.multi to generate it.  I intend to have three panels, one for the scale and grid for each component.  However, is not working.  Here it is:


require(ggplot2)
require( ggtern)

local({

d <- data.frame(X=0, Y=0, Z=1)  # dummy data
brks <- seq(0,5)/5              # axis breaks/ticks
minbrk <- seq(0.1,0.9,0.2)      # minor axis breaks


p1 <- ggplot() + theme_bw(base_size=16) + theme_noarrows() +
      scale_T_continuous(name="Cpnt 1", breaks=brks, minor_breaks=minbrk, labels=brks) +
      scale_R_continuous(name=      "", breaks=   0, minor_breaks=     0, labels="") +
      scale_L_continuous(name=      "", breaks=   0, minor_breaks=     0, labels="") +
      coord_tern() + geom_point(data=d, aes(X, Y, Z), size=0)

p2 <- ggplot() + theme_bw(base_size=16) + theme_noarrows() +
      scale_T_continuous(name=      "", breaks=   0, minor_breaks=     0, labels="") +
      scale_R_continuous(name=      "", breaks=   0, minor_breaks=     0, labels="") +
      scale_L_continuous(name="Cpnt 2", breaks=brks, minor_breaks=minbrk, labels=brks) +
      coord_tern() + geom_point(data=d, aes(X, Y, Z), size=0)

p3 <- ggplot() + theme_bw(base_size=16) + theme_noarrows() +
      scale_T_continuous(name=      "", breaks=   0, minor_breaks=     0, labels="") +
      scale_R_continuous(name="Cpnt 3", breaks=brks, minor_breaks=minbrk, labels=brks) +
      scale_L_continuous(name=      "", breaks=   0, minor_breaks=     0, labels="") +
      coord_tern() + geom_point(data=d, aes(X, Y, Z), size=0)

ggtern.multi(p1, p2, p3, cols=3)

}) # end local

This code does generate three panels, but only the first is rendered as I intended.  The latter two have NAs as the axis/scale labels, and the whole code throws the warning:

Warning messages:
1: In `[<-.factor`(`*tmp*`, ri, value = c(0, 0.2, 0.4, 0.6, 0.8, 1)) :
        invalid factor level, NA generated
2: In `[<-.factor`(`*tmp*`, ri, value = c(0, 0.2, 0.4, 0.6, 0.8, 1)) :
  invalid factor level, NA generated

In addition, if I put the dummy data and the scale breaks objects in the global environment and run each of the three component plots separately, only p1 works.  p2 and p3 fail in the same way so the combining is not likely the culprit.


----------------------------

I have discovered a feature of the scale_<TLR>_continuous call which may be related.  When I am making truncated ternary plots (ranges a subset of [0, 1]), I can successfully customize axes in the following way:


  ggplot() + theme_bw(base_size=18) +
      scale_T_continuous(name="Cmpt 1", breaks=seq(0.6,0.7,0.02), minor_breaks=seq(0.61,0.71,0.01),
                         labels=seq(0.6,0.7,0.02)) +
      scale_R_continuous(name="Cmpt 2", breaks=seq(0.1,0.3,0.02), minor_breaks=seq(0.11,0.31,0.01),
                         labels=seq(0.1,0.3,0.02)) +
      scale_L_continuous(name="Cmpt  3", breaks=seq(0.1,0.3,0.02), minor_breaks=seq(0.11,0.31,0.01),
                         labels=seq(0.1,0.3,0.02)) +
      coord_tern(Tlim=c(0.595,0.690), Llim=c(0.160,0.255), Rlim=c(0.150,0.245)) +
      geom_point(data=data.frame(X=1/6, Y=2/3, Z=1/6), aes(X, Y, Z))

Note I have duplicated the breaks argument for the labels argument, to make the labels be raw fractions, not percent.  I wish to unify the formatting; this plots the endpoints only to one decimal place and I wanted two consistently.  So I try wrapping format() around only the labels argument of the scale_T_continuous layer:

  ggplot() + theme_bw(base_size=18) +
      scale_T_continuous(name="Cmpt 1", breaks=seq(0.6,0.7,0.02), minor_breaks=seq(0.61,0.71,0.01),

                         labels=format(seq(0.6,0.7,0.02), nsmall=2)) +  #####   <-----  here

      scale_R_continuous(name="Cmpt 2", breaks=seq(0.1,0.3,0.02), minor_breaks=seq(0.11,0.31,0.01),
                         labels=seq(0.1,0.3,0.02)) +
      scale_L_continuous(name="Cmpt 3", breaks=seq(0.1,0.3,0.02), minor_breaks=seq(0.11,0.31,0.01),
                         labels=seq(0.1,0.3,0.02)) +
      coord_tern(Tlim=c(0.595,0.690), Llim=c(0.160,0.255), Rlim=c(0.150,0.245)) +
      geom_point(data=data.frame(X=1/6, Y=2/3, Z=1/6), aes(X, Y, Z))

This leads to the same (double) warning as above, and the R and L axes/scales have only NAs for labels.  Interestingly the T axis labels are formatted as intended.

Any help or direction would be appreciated.

Thanks, John
John Szumiloski, Ph.D.
Principal Scientist, Statistician
Analytical and Bioanalytical Development
NBR105-1-1411

Bristol-Myers Squibb
P.O. Box 191
1 Squibb Drive
New Brunswick, NJ
08903-0191

(732) 227-7167


________________________________
 This message (including any attachments) may contain co...{{dropped:8}}


From cecilia.larrosa10 at imperial.ac.uk  Tue Jul 28 17:36:07 2015
From: cecilia.larrosa10 at imperial.ac.uk (SisoL)
Date: Tue, 28 Jul 2015 08:36:07 -0700 (PDT)
Subject: [R] Varying name of output tables from looped process of list
 of spdf objects
In-Reply-To: <CBB43074-8829-4DAD-81EA-32318DB760A2@kit.edu>
References: <7556269B-84FA-4C11-83C6-BA30A090127A@imperial.ac.uk>
	<5D4B294D-54AA-4FED-92BE-35D7D724D23F@imperial.ac.uk>
	<CA+8X3fUVN11_2uA_u-BNaYvrACzYUFiDR4TJVqsjBfx_enEkHw@mail.gmail.com>
	<8A18FABE-B192-402A-9101-F3311F74A0A1@kit.edu>
	<1438014822138-4710408.post@n4.nabble.com>
	<CBB43074-8829-4DAD-81EA-32318DB760A2@kit.edu>
Message-ID: <1438097767328-4710491.post@n4.nabble.com>

Hi,

Thank you for the replies. I tried Peter's suggestions and this worked:

###Read forest shape files
setwd("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/split_bf_fnp/")
shps<- dir(getwd(), "*.shp")
shps <- gsub('.{4}$', '', shps)
for (shp in shps) assign(shp, readOGR(".",layer=shp))

###Create list
afiles <- ls(pattern= "a_")

### Create Conefor Input Tables
for (ifile in 1:length(afiles))
{
  fnp<- mget(afiles[ifile])[[1]]
  distance.matrix<- gDistance(fnp, spgeom2= NULL, byid=T);
  row.names(distance.matrix) <- paste(1:nrow(distance.matrix), sep="");
  colnames(distance.matrix)<- paste(1:ncol(distance.matrix), sep="");
  dists.melt <-
melt(distance.matrix)[melt(upper.tri(distance.matrix))$value,];
  outfile <-
file.path("/Users/sisolarrosa/Documents/PhD/R_work/AF/IIC/conefor_inputs/",
paste0("distances_", afiles[ifile], ".txt"));
  write.table(dists.melt, outfile,row.names=FALSE, col.names=FALSE);
}



--
View this message in context: http://r.789695.n4.nabble.com/Varying-name-of-output-tables-from-looped-process-of-list-of-spdf-objects-tp4710369p4710491.html
Sent from the R help mailing list archive at Nabble.com.


From picnic101 at gmail.com  Tue Jul 28 18:41:50 2015
From: picnic101 at gmail.com (=?UTF-8?B?6rmA7IOB7ZmY?=)
Date: Wed, 29 Jul 2015 01:41:50 +0900
Subject: [R] Reading data with two rows of variable names using read.zoo
Message-ID: <CAAUm5VM6jgjn+X_=G5ytKUNquLU81RrrM_kRD6fqHYwOvO8Ktg@mail.gmail.com>

Dear R gurus.

I have a data file which has two rows of variable names.
And the time index has a little unusual format. I have no idea
how to handle two names and awkward indexing for the quarters.

Lines <- "
Index; UK; UK; JP; JP
Index; a1; a2; a1; a2
2009 2/4;2;4;3;2
2009 3/4;5;2;1;4
2009 4/4;7;1;1;6
2010 1/4;3;3;5;2
2010 2/4;5;1;2;1
"
(a snippet from a big data containing a1, a2, ..., a10 of many countries)

I want to sum a1 and a2 for UK, JP and obtain a zoo object like this:

            A    B
2009 Q2     6    5
2009 Q3     7    5
2009 Q4     8    7
2010 Q1     6    7
2010 Q2     6    3

This looks quite challenging. Thanks for your time.

	[[alternative HTML version deleted]]


From NordlDJ at dshs.wa.gov  Wed Jul 29 02:16:09 2015
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Wed, 29 Jul 2015 00:16:09 +0000
Subject: [R] Reading data with two rows of variable names using read.zoo
In-Reply-To: <CAAUm5VM6jgjn+X_=G5ytKUNquLU81RrrM_kRD6fqHYwOvO8Ktg@mail.gmail.com>
References: <CAAUm5VM6jgjn+X_=G5ytKUNquLU81RrrM_kRD6fqHYwOvO8Ktg@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662ED7053F@WAXMXOLYMB025.WAX.wa.lcl>

Not a guru, but this isn't that hard.  The following works with your sample data.  It shouldn't be too difficult to modify for your full file.

library(zoo)
df <- read.table('path_to_your_data', sep=';', skip=2, as.is=TRUE)
str(df)
substr(df$V1,5,5) <- '-'
df$V1 <- as.yearqtr(substr(df$V1,1,6))
df$A <- rowSums(df[,c(2,4)])
df$B <- rowSums(df[,c(3,5)])

want <- as.zoo(df[,-c(2:5)])
want

Hope this is helpful,

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of ???
Sent: Tuesday, July 28, 2015 9:42 AM
To: r-help at r-project.org
Subject: [R] Reading data with two rows of variable names using read.zoo

Dear R gurus.

I have a data file which has two rows of variable names.
And the time index has a little unusual format. I have no idea how to handle two names and awkward indexing for the quarters.

Lines <- "
Index; UK; UK; JP; JP
Index; a1; a2; a1; a2
2009 2/4;2;4;3;2
2009 3/4;5;2;1;4
2009 4/4;7;1;1;6
2010 1/4;3;3;5;2
2010 2/4;5;1;2;1
"
(a snippet from a big data containing a1, a2, ..., a10 of many countries)

I want to sum a1 and a2 for UK, JP and obtain a zoo object like this:

            A    B
2009 Q2     6    5
2009 Q3     7    5
2009 Q4     8    7
2010 Q1     6    7
2010 Q2     6    3

This looks quite challenging. Thanks for your time.

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dulcalma at bigpond.com  Wed Jul 29 04:48:59 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Wed, 29 Jul 2015 12:48:59 +1000
Subject: [R] Opposite color in R
In-Reply-To: <55B73BA8.7080402@utu.fi>
References: <55B73BA8.7080402@utu.fi>
Message-ID: <000f01d0c9a9$1fa3f840$5eebe8c0$@bigpond.com>

Hi

I plotted the 'spectrum' and it looked a little small - spectrum colours: red orange yellow green blue indigo violet.
I suppose you could go to infinite lengths to split it up but is this an improvement?

I have not gone into the "depths" of complimentary colours

library(colorspace)

ColorsRYB=rbind(colorRamp(c("purple","violet"))((0:4)/4)[1:4,],
                colorRamp(c("violet","blue"))((0:4)/4)[1:4,],
                colorRamp(c("blue","green"))((0:4)/4)[1:4,],
                colorRamp(c("green","yellow"))((0:4)/4)[1:4,],
                colorRamp(c("yellow","orange"))((0:4)/4)[1:4,],
                colorRamp(c("orange","red"))((0:4)/4)[1:4,],
                colorRamp(c("red","purple"))((0:4)/4)[1:4,])

LenCol=length(ColorsRYB[,1])

ColorsRYBhex=rep(0, LenCol)
for(i in 1: LenCol)
{
ColorsRYBhex[i]=rgb(ColorsRYB[i,1]/255,ColorsRYB[i,2]/255,ColorsRYB[i,3]/255)
}

pie(rep(1, LenCol), col = ColorsRYBhex)

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au




-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Atte Tenkanen
Sent: Tuesday, 28 July 2015 18:22
To: r-help at r-project.org
Subject: [R] Opposite color in R

It seems that there is no implementation for the "traditional artist's 
color circle" in R. However I'm searching for such a wheel, because my 
program needs it.

As said, the description of complementary/opposite-function in package 
"colortools" is misleading since, for example

opposite("green") produces violet, not red, but the description of 
complementary-function says

"Complementary or opposite color scheme is formed by colors that are 
opposite each other on the color wheel (example: red and green)."

So, there must be just a lapse in the text.

I "constrained" such kind of a color wheel, which is enough near of what 
I need:

library(colorspace)

ColorsRYB=rbind(colorRamp(c("red", 
"violet"))((0:4)/4)[1:4,],colorRamp(c("violet", 
"blue"))((0:4)/4)[1:4,],colorRamp(c("blue", 
"green"))((0:4)/4)[1:4,],colorRamp(c("green", 
"yellow"))((0:4)/4)[1:4,],colorRamp(c("yellow", 
"orange"))((0:4)/4)[1:4,],colorRamp(c("orange", "red"))((0:4)/4)[1:4,])

LenCol=length(ColorsRYB[,1])

ColorsRYBhex=rep(0, LenCol)
for(i in 1: LenCol)
{
ColorsRYBhex[i]=rgb(ColorsRYB[i,1]/255,ColorsRYB[i,2]/255,ColorsRYB[i,3]/255)
}

pie(rep(1, 24), col = ColorsRYBhex)

Atte T.


28.7.2015, 2.23, Steve Taylor kirjoitti:
> I wonder if the hcl colour space is useful?  Varying hue while keeping chroma and luminosity constant should give varying colours of perceptually the same "colourness" and brightness.
>
> ?hcl
> pie(rep(1,12),col=hcl((1:12)*30,c=70),border=NA)
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Atte Tenkanen
> Sent: Sunday, 26 July 2015 7:50a
> To: r-help at r-project.org
> Subject: [R] Opposite color in R
>
> Hi,
>
> I have tried to find a way to find opposite or complementary colors in R.
>
> I would like to form a color circle with R like this one:
> http://nobetty.net/dandls/colorwheel/complementary_colors.jpg
>
> If you just make a basic color wheel in R, the colors do not form
> complementary color circle:
>
> palette(rainbow(24))
> Colors=palette()
> pie(rep(1, 24), col = Colors)
>
> There is a package ?colortools? where you can find function opposite(),
> but it doesn?t work as is said. I tried
>
> library(colortools)
> opposite("violet") and got green instead of yellow and
>
> opposite("blue") and got yellow instead of orange.
>
> Do you know any solutions?
>
> Atte Tenkanen
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jjthaden at flash.net  Wed Jul 29 00:00:46 2015
From: jjthaden at flash.net (John Thaden)
Date: Tue, 28 Jul 2015 22:00:46 +0000 (UTC)
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>
References: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>
Message-ID: <108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>

Adam,??? The method you propose gives a different result than the prior methods for these example vectors
X <- c("ab", "cd", "ef")
patt <- c("b", "cd", "a")
repl <- c("B", "CD", "A")

Old method 1

mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
gives
? b?? cd??? a 
"aB" "CD" "ef"

Old method 2 

sub2 <- function(pattern, replacement, x) {
? ? len <- length(x)
? ? if (length(pattern) == 1) 
? ? ? ? pattern <- rep(pattern, len)
? ? if (length(replacement) == 1) 
? ? ? ? replacement <- rep(replacement, len)
? ? FUN <- function(i, ...) {
? ? ? ? sub(pattern[i], replacement[i], x[i], fixed = TRUE)
? ? }
? ? idx <- 1:length(x)
? ? sapply(idx, FUN) ? ?
}
sub2(patt, repl, X)
 gives
[1] "aB" "CD" "ef"

Your method (I gave it the unique name "sub3")
?sub3 <- function(pattern, replacement, x) {?? len ?? <- length(x)? y ? ? ?<- character(length=len)? patlen <- length(pattern)? replen <- length(replacement)? if(patlen != replen) stop('Error: Pattern and replacement length do not match')? for(i in 1:replen) {? ? y[which(x==pattern[i])] <- replacement[i]? }? return(y)}sub3(patt, repl, X)
gives[1] ""?? "CD" ""

Granted, whatever it does, it does it faster
#Old method 1
system.time(for(i in 1:50000)
mapply(function(p,r,x) sub(p,r,x, fixed = TRUE),p=patt,r=repl,x=X))
?? user? system elapsed 
?? 2.53??? 0.00??? 2.52 
?
#Old method 2
system.time(for(i in 1:50000)sub2(patt, repl, X))?? user? system elapsed 
?? 2.32??? 0.00??? 2.32 
?
#Your proposed method
system.time(for(i in 1:50000) sub3(patt, repl, X))
?? user? system elapsed 
?? 1.02??? 0.00??? 1.01
 but would it still be faster if it actually solved the same problem?

-John Thaden
 



     On Monday, July 27, 2015 11:40 PM, Adam Erickson <adam.michael.erickson at gmail.com> wrote:
   
I know this is an old thread, but I wrote a simple FOR loop with vectorized pattern replacement that is much faster than either of those (it can also accept outputs differing in length from the patterns):
? sub2 ?<- function(pattern, replacement, x) {?? ? len ? <- length(x)? ? y ? ? ?<- character(length=len)? ? patlen <- length(pattern)? ? replen <- length(replacement)? ? if(patlen != replen) stop('Error: Pattern and replacement length do not match')? ? for(i in 1:replen) {? ? ? y[which(x==pattern[i])] <- replacement[i]? ? }? ? return(y)? }
system.time(test <- sub2(patt, repl, XX))? ?user ?system elapsed?? ? ? 0 ? ? ? 0 ? ? ? 0?
Cheers,
Adam
On Wednesday, October 8, 2008 at 9:38:01 PM UTC-7, john wrote:
Hello Christos,
? To my surprise, vectorization actually hurt processing speed!#Example
X <- c("ab", "cd", "ef")
patt <- c("b", "cd", "a")
repl <- c("B", "CD", "A")sub2 <- function(pattern, replacement, x) {
? ? len <- length(x)
? ? if (length(pattern) == 1) 
? ? ? ? pattern <- rep(pattern, len)
? ? if (length(replacement) == 1) 
? ? ? ? replacement <- rep(replacement, len)
? ? FUN <- function(i, ...) {
? ? ? ? sub(pattern[i], replacement[i], x[i], fixed = TRUE)
? ? }
? ? idx <- 1:length(x)
? ? sapply(idx, FUN) ? ?
}
?
system.time( ?for(i in 1:10000) ?sub2(patt, repl, X) ?)
? ?user ?system elapsed 
? ?1.18 ? ?0.07 ? ?1.26 system.time( ?for(i in 1:10000) ?mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X) ?)
? ?user ?system elapsed 
? ?1.42 ? ?0.05 ? ?1.47 
?
So much for avoiding loops.
John Thaden======= At 2008-10-07, 14:58:10 Christos wrote: =======>John,
>Try the following:
>
> mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
> ? b ? cd ? ?a 
>"aB" "CD" "ef" ?
>
>-Christos>> -----My Original Message-----
>> R pattern-matching and replacement functions are
>> vectorized: they can operate on vectors of targets.
>> However, they can only use one pattern and replacement.
>> Here is code to apply a different pattern and replacement for 
>> every target. ?My question: can it be done better?
>> 
>> sub2 <- function(pattern, replacement, x) {
>> ? ? len <- length(x)
>> ? ? if (length(pattern) == 1) 
>> ? ? ? ? pattern <- rep(pattern, len)
>> ? ? if (length(replacement) == 1) 
>> ? ? ? ? replacement <- rep(replacement, len)
>> ? ? FUN <- function(i, ...) {
>> ? ? ? ? sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>> ? ? }
>> ? ? idx <- 1:length(x)
>> ? ? sapply(idx, FUN) ? ?
>> }
>> 
>> #Example
>> X <- c("ab", "cd", "ef")
>> patt <- c("b", "cd", "a")
>> repl <- c("B", "CD", "A")
>> sub2(patt, repl, X)
>> 
>> -John______________________________________________
R-h... at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.



  
	[[alternative HTML version deleted]]


From baccts at hotmail.com  Tue Jul 28 22:25:32 2015
From: baccts at hotmail.com (baccts)
Date: Tue, 28 Jul 2015 13:25:32 -0700 (PDT)
Subject: [R] indices of mismatch element in two vector with missing values
Message-ID: <1438115132091-4710497.post@n4.nabble.com>

How would you return the index where two vectors differs if they may contain
missing (NA) values?
For example:
test1 <- c("1","2",NA);
test2 <- c("1","2","3");
which(test1!=test2) does not return 3!

Thanks in advance.






--
View this message in context: http://r.789695.n4.nabble.com/indices-of-mismatch-element-in-two-vector-with-missing-values-tp4710497.html
Sent from the R help mailing list archive at Nabble.com.


From adam.michael.erickson at gmail.com  Wed Jul 29 02:12:14 2015
From: adam.michael.erickson at gmail.com (Adam Erickson)
Date: Tue, 28 Jul 2015 17:12:14 -0700
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>
References: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>
	<108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAHEemWYH4t6MkjZ29pw7yy2WU5XN6bASink2sgmjosV9NxubbQ@mail.gmail.com>

Hi John,

The version I wrote performs vectorized full string matching and
replacement with some error checking and flexible inputs. I think there are
a lot of good reasons for using this method where possible (e.g., speed and
reduced complexity). Duly noted that it is different from the original
question, which I only skimmed. The previous versions you listed are both
actually faster than the function for this in stringr:

str_replace(X,patt,repl)
[1] "aB" "CD" "ef"

system.time(for(i in 1:50000) str_replace(X,patt,repl))
   user  system elapsed
   5.51    0.00    5.79

However, it seems unrealistic that the vectors would be perfectly ordered
in this way for most applications. The previous listed code is faster than
other approaches because there are far fewer permutations and only the
first character is checked. Perhaps that was the intention? I find this
case to be rare. For data.tables, I prefer the := and like() function,
which uses grepl().

Cheers,

Adam

On Tue, Jul 28, 2015 at 3:00 PM, John Thaden <jjthaden at flash.net> wrote:

> Adam,
>     The method you propose gives a different result than the prior methods
> for these example vectors
>
> X <- c("ab", "cd", "ef")
> patt <- c("b", "cd", "a")
> repl <- c("B", "CD", "A")
>
> Old method 1
>
> mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
>
> gives
>
>
> *  b   cd    a "aB" "CD" "ef"*
>
> Old method 2
>
> sub2 <- function(pattern, replacement, x) {
>     len <- length(x)
>     if (length(pattern) == 1)
>         pattern <- rep(pattern, len)
>     if (length(replacement) == 1)
>         replacement <- rep(replacement, len)
>     FUN <- function(i, ...) {
>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>     }
>     idx <- 1:length(x)
>     sapply(idx, FUN)
> }
> sub2(patt, repl, X)
>
> gives
>
> *[1] "aB" "CD" "ef"*
>
> Your method (I gave it the unique name "sub3")
>
> sub3 <- function(pattern, replacement, x) {
>   len    <- length(x)
>   y      <- character(length=len)
>   patlen <- length(pattern)
>   replen <- length(replacement)
>   if(patlen != replen) stop('Error: Pattern and replacement length do not
> match')
>   for(i in 1:replen) {
>     y[which(x==pattern[i])] <- replacement[i]
>   }
>   return(y)
> }
> sub3(patt, repl, X)
>
> gives
>
> *[1] ""   "CD" ""*
>
> Granted, whatever it does, it does it faster
>
> #Old method 1
> system.time(for(i in 1:50000)
> mapply(function(p,r,x) sub(p,r,x, fixed = TRUE),p=patt,r=repl,x=X))
>
> *user  system elapsed    2.53    0.00    2.52 *
>
> #Old method 2
> system.time(for(i in 1:50000)
> sub2(patt, repl, X))
>
> *user  system elapsed    2.32    0.00    2.32 *
>
> #Your proposed method
> system.time(for(i in 1:50000) sub3(patt, repl, X))
>
> *user  system elapsed *
> *   1.02    0.00    1.01*
>
> but would it still be faster if it actually solved the same problem?
>
> -John Thaden
>
>
>
>
>   On Monday, July 27, 2015 11:40 PM, Adam Erickson <
> adam.michael.erickson at gmail.com> wrote:
>
> I know this is an old thread, but I wrote a simple FOR loop with
> vectorized pattern replacement that is much faster than either of those (it
> can also accept outputs differing in length from the patterns):
>
>   sub2  <- function(pattern, replacement, x) {
>     len   <- length(x)
>     y      <- character(length=len)
>     patlen <- length(pattern)
>     replen <- length(replacement)
>     if(patlen != replen) stop('Error: Pattern and replacement length do
> not match')
>     for(i in 1:replen) {
>       y[which(x==pattern[i])] <- replacement[i]
>     }
>     return(y)
>   }
>
> system.time(test <- sub2(patt, repl, XX))
>    user  system elapsed
>       0       0       0
>
> Cheers,
>
> Adam
>
> On Wednesday, October 8, 2008 at 9:38:01 PM UTC-7, john wrote:
>
> Hello Christos,
>   To my surprise, vectorization actually hurt processing speed!
> #Example
> X <- c("ab", "cd", "ef")
> patt <- c("b", "cd", "a")
> repl <- c("B", "CD", "A")
> sub2 <- function(pattern, replacement, x) {
>     len <- length(x)
>     if (length(pattern) == 1)
>         pattern <- rep(pattern, len)
>     if (length(replacement) == 1)
>         replacement <- rep(replacement, len)
>     FUN <- function(i, ...) {
>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>     }
>     idx <- 1:length(x)
>     sapply(idx, FUN)
> }
>
> system.time(  for(i in 1:10000)  sub2(patt, repl, X)  )
>    user  system elapsed
>    1.18    0.07    1.26
> system.time(  for(i in 1:10000)  mapply(function(p, r, x) sub(p, r, x,
> fixed = TRUE), p=patt, r=repl, x=X)  )
>    user  system elapsed
>    1.42    0.05    1.47
>
> So much for avoiding loops.
> John Thaden
> ======= At 2008-10-07, 14:58:10 Christos wrote: =======
> >John,
> >Try the following:
> >
> > mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
> >   b   cd    a
> >"aB" "CD" "ef"
> >
> >-Christos
> >> -----My Original Message-----
> >> R pattern-matching and replacement functions are
> >> vectorized: they can operate on vectors of targets.
> >> However, they can only use one pattern and replacement.
> >> Here is code to apply a different pattern and replacement for
> >> every target.  My question: can it be done better?
> >>
> >> sub2 <- function(pattern, replacement, x) {
> >>     len <- length(x)
> >>     if (length(pattern) == 1)
> >>         pattern <- rep(pattern, len)
> >>     if (length(replacement) == 1)
> >>         replacement <- rep(replacement, len)
> >>     FUN <- function(i, ...) {
> >>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
> >>     }
> >>     idx <- 1:length(x)
> >>     sapply(idx, FUN)
> >> }
> >>
> >> #Example
> >> X <- c("ab", "cd", "ef")
> >> patt <- c("b", "cd", "a")
> >> repl <- c("B", "CD", "A")
> >> sub2(patt, repl, X)
> >>
> >> -John
> ______________________________________________
> R-h... at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>

	[[alternative HTML version deleted]]


From lisagrace7 at hotmail.com  Wed Jul 29 04:11:11 2015
From: lisagrace7 at hotmail.com (lisagrace7)
Date: Tue, 28 Jul 2015 19:11:11 -0700 (PDT)
Subject: [R] npreg: average partial effects and customised partial
 regression plots
Message-ID: <1438135871568-4710500.post@n4.nabble.com>

Hi, 

I?ve created a simple fixed effects version of Li and Racine?s Kernel
Regression with Mixed Data Types (npreg function from np package) by
including an unordered firm id variable and ordered year variable as extra
covariates.   See below for the setup:

******
bandwidth <- npregbw(formula = log.PPE ~  factor(tax.policy) + tax.rate +
GDP.growth + log.firm.turnover + MNE.country.count, factor(subsidiary_id),
ordered(year)
                   data = Norway, 
                   regtype = "ll", 
                   bwmethod = "cv.aic",
                   ckertype = "gaussian",
                   ckerorder = 2,
                   ukertype = "aitchisonaitken",
                   okertype = "wangvanryzin" )

model.np <- npreg(bws = bandwidth, gradients = TRUE, residuals = TRUE)

plot(model.np,
     plot.errors.method = "bootstrap", 
     plot.errors.boot.num = 25,
     plot.errors.boot.method = "inid")
******

I understand that nonparametric regressions allow for nonlinear
relationships for all covariates and interactions between all covariates.  I
have calculated the gradients in the npreg function but would like advice
about how to use them.

I have several related questions:

1.	How do I calculate the average partial effect of the binary tax policy
variable over all subsidiaries in the sample? That is, I would like to
calculate the: 
    a.	 average investment level of each subsidiary when the policy variable
= 0 (holding other covariates at their median for that subsidiary) and 
    b.	 average investment level of each subsidiary when the policy variable
= 1 (holding other covariates at their median for that subsidiary) and then
    c.	 take the difference. 
    d.	 And then I would like to calculate the average of this difference
over all subsidiaries.

2.	Similarly, I am interested in the interactions between the policy
variable and other covariates.  For example, was the policy effect bigger
for larger subsidiaries or for multinationals with a presence in many other
countries?

Basically, I would like to know how to produce the type of output that one
can get from the 'margins' command in STATA.

3.	Finally, is there a way to customise the partial regression plots that
result from the plot command after using npreg?  Eg.  How do I choose to
hold the policy variable at 0 and at 1 so that I can compare two versions of
the plots? 

Many thanks, 
Lisa


Masters Student,
University of T?bingen.







--
View this message in context: http://r.789695.n4.nabble.com/npreg-average-partial-effects-and-customised-partial-regression-plots-tp4710500.html
Sent from the R help mailing list archive at Nabble.com.


From fariszaidi at mardi.gov.my  Wed Jul 29 05:38:59 2015
From: fariszaidi at mardi.gov.my (Faris Zaidi Bin Mohd Nor)
Date: Wed, 29 Jul 2015 03:38:59 +0000
Subject: [R] Inquiry: R library for bioinformatics development process
Message-ID: <1FD06892F57ACC438DD0E131EDBC79924DD9A319@PUJSVREXCH11.1GOVUC.GOV.MY>

Hi,
I would like to know about the R library. Is there any library for bioinformatic development software purpose?

Sekian, terima kasih.
"PEMIMPIN AGRO-TEKNOLOGI"
"JIHAD MEMERANGI ORANG TENGAH"

Saya yang menurut perintah,
................................................
Faris Zaidi Bin Mohd Nor
Pen. Pegawai Penyelidik (Nutrigenomik)
Pejabat Ketua Pengarah
Ibu Pejabat MARDI, Serdang
Email: fariszaidi at mardi.gov.my

	[[alternative HTML version deleted]]


From Peter.Alspach at plantandfood.co.nz  Wed Jul 29 06:58:17 2015
From: Peter.Alspach at plantandfood.co.nz (Peter Alspach)
Date: Wed, 29 Jul 2015 16:58:17 +1200
Subject: [R] indices of mismatch element in two vector with missing
 values
In-Reply-To: <1438115132091-4710497.post@n4.nabble.com>
References: <1438115132091-4710497.post@n4.nabble.com>
Message-ID: <E41B375B7520DE4A8C60781AC60B75450672242F53@AKLEXM01.PFR.CO.NZ>

One way ....

seq(test1)[-which(test1==test2)]

but I imagine there are better ones .....

Peter Alspach

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of baccts
Sent: Wednesday, 29 July 2015 8:26 a.m.
To: r-help at r-project.org
Subject: [R] indices of mismatch element in two vector with missing values

How would you return the index where two vectors differs if they may contain missing (NA) values?
For example:
test1 <- c("1","2",NA);
test2 <- c("1","2","3");
which(test1!=test2) does not return 3!

Thanks in advance.






--
View this message in context: http://r.789695.n4.nabble.com/indices-of-mismatch-element-in-two-vector-with-missing-values-tp4710497.html
Sent from the R help mailing list archive at Nabble.com.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
The contents of this e-mail are confidential and may be ...{{dropped:14}}


From johannes at huesing.name  Wed Jul 29 07:46:28 2015
From: johannes at huesing.name (Johannes Huesing)
Date: Wed, 29 Jul 2015 07:46:28 +0200
Subject: [R] Inquiry: R library for bioinformatics development process
In-Reply-To: <1FD06892F57ACC438DD0E131EDBC79924DD9A319@PUJSVREXCH11.1GOVUC.GOV.MY>
References: <1FD06892F57ACC438DD0E131EDBC79924DD9A319@PUJSVREXCH11.1GOVUC.GOV.MY>
Message-ID: <20150729054628.GA24464@huesing.name>



Faris Zaidi Bin Mohd Nor <fariszaidi at mardi.gov.my> [Wed, Jul 29, 2015 at 05:38:59AM CEST]:
>Hi,
>I would like to know about the R library. Is there any library for bioinformatic development software purpose?

Have you looked at the Bioconductor project?

-- 
Johannes H?sing               There is something fascinating about science. 
                               One gets such wholesale returns of conjecture 
mailto:johannes at huesing.name  from such a trifling investment of fact.                
http://derwisch.wikidot.com         (Mark Twain, "Life on the Mississippi")


From hpages at fredhutch.org  Wed Jul 29 07:51:35 2015
From: hpages at fredhutch.org (=?windows-1252?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 28 Jul 2015 22:51:35 -0700
Subject: [R] indices of mismatch element in two vector with missing
	values
In-Reply-To: <1438115132091-4710497.post@n4.nabble.com>
References: <1438115132091-4710497.post@n4.nabble.com>
Message-ID: <55B869E7.7010009@fredhutch.org>

Hi,

On 07/28/2015 01:25 PM, baccts wrote:
> How would you return the index where two vectors differs if they may contain
> missing (NA) values?
> For example:
> test1 <- c("1","2",NA);
> test2 <- c("1","2","3");
> which(test1!=test2) does not return 3!

which(test1 != test2 | is.na(test1) != is.na(test2))

Cheers,
H.

>
> Thanks in advance.
>
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/indices-of-mismatch-element-in-two-vector-with-missing-values-tp4710497.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hpages at fredhutch.org  Wed Jul 29 08:07:14 2015
From: hpages at fredhutch.org (=?windows-1252?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 28 Jul 2015 23:07:14 -0700
Subject: [R] indices of mismatch element in two vector with missing
	values
In-Reply-To: <E41B375B7520DE4A8C60781AC60B75450672242F53@AKLEXM01.PFR.CO.NZ>
References: <1438115132091-4710497.post@n4.nabble.com>
	<E41B375B7520DE4A8C60781AC60B75450672242F53@AKLEXM01.PFR.CO.NZ>
Message-ID: <55B86D92.4030708@fredhutch.org>

On 07/28/2015 09:58 PM, Peter Alspach wrote:
> One way ....
>
> seq(test1)[-which(test1==test2)]

One question is whether 2 NAs should be considered to match or not.
The OP doesn't tell but I guess he wants them to match:

test1 <- c("1", "2",  NA, "4", NA, "6")
test2 <- c("1", "2", "3",  NA, NA, "66")

seq(test1)[-which(test1==test2)]
# [1] 3 4 5 6

5 should probably not be there!

H.

>
> but I imagine there are better ones .....
>
> Peter Alspach
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of baccts
> Sent: Wednesday, 29 July 2015 8:26 a.m.
> To: r-help at r-project.org
> Subject: [R] indices of mismatch element in two vector with missing values
>
> How would you return the index where two vectors differs if they may contain missing (NA) values?
> For example:
> test1 <- c("1","2",NA);
> test2 <- c("1","2","3");
> which(test1!=test2) does not return 3!
>
> Thanks in advance.
>
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/indices-of-mismatch-element-in-two-vector-with-missing-values-tp4710497.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> The contents of this e-mail are confidential and may be ...{{dropped:14}}
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From skim3 at naver.com  Wed Jul 29 08:57:48 2015
From: skim3 at naver.com (SW Kim)
Date: Tue, 28 Jul 2015 23:57:48 -0700 (PDT)
Subject: [R] Reading data with two rows of variable names using read.zoo
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA27662ED7053F@WAXMXOLYMB025.WAX.wa.lcl>
References: <CAAUm5VM6jgjn+X_=G5ytKUNquLU81RrrM_kRD6fqHYwOvO8Ktg@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA27662ED7053F@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <1438153067999-4710510.post@n4.nabble.com>

Thanks, Dan.

Your codes work fine. But I have tens of countries UK, JP, BR, US...,
each of which has ten columns a1, a2, ..., a10 of data. So a little more
automation is needed.

I have been trying to make a list of each country's data and use sapply
thing
to get 
               UK   JP
2009 Q2     6    5 
2009 Q3     7    5 
2009 Q4     8    7 
2010 Q1     6    7 
2010 Q2     6    3 

But for me, it was not easy as it looks... 
Thank you in advance!



--
View this message in context: http://r.789695.n4.nabble.com/Reading-data-with-two-rows-of-variable-names-using-read-zoo-tp4710496p4710510.html
Sent from the R help mailing list archive at Nabble.com.


From maechler at stat.math.ethz.ch  Wed Jul 29 11:03:03 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 29 Jul 2015 11:03:03 +0200
Subject: [R] indices of mismatch element in two vector with
	missing	values
In-Reply-To: <55B86D92.4030708@fredhutch.org>
References: <1438115132091-4710497.post@n4.nabble.com>
	<E41B375B7520DE4A8C60781AC60B75450672242F53@AKLEXM01.PFR.CO.NZ>
	<55B86D92.4030708@fredhutch.org>
Message-ID: <21944.38599.987763.588086@stat.math.ethz.ch>

>>>>> Herv? Pag?s <hpages at fredhutch.org>
>>>>>     on Tue, 28 Jul 2015 23:07:14 -0700 writes:

    > On 07/28/2015 09:58 PM, Peter Alspach wrote:
    >> One way ....
    >> 
    >> seq(test1)[-which(test1==test2)]

    > One question is whether 2 NAs should be considered to match or not.
    > The OP doesn't tell but I guess he wants them to match:

    > test1 <- c("1", "2",  NA, "4", NA, "6")
    > test2 <- c("1", "2", "3",  NA, NA, "66")

    > seq(test1)[-which(test1==test2)]
    > # [1] 3 4 5 6

    > 5 should probably not be there!

    >> but I imagine there are better ones .....
    >> 

Yes, indeed, as logical indexing is
considerably safer than "negative indexing with which(.)" :

PLEASE, everyone, do remember:

 %%>===================================================================<%%
 %%>  There is one ___big__ disadvantage to  [ - which(<logical>) ] :  <%%
 %%>  It entirely __fails__ when the logical vector is all FALSE       <%%
 %%>===================================================================<%%


  ## Example (remember, we want the indices of *differing* entries):
  ## ------- Here, all entries differ, so we want to get  1:8 :

   x <- c(NA, 1:7)
   y <- c(11:17, NA)

   ## now this
   seq(x)[ - which(x == y) ]  ## gives not what you expect

   ## But logical indexing always works:
	    x == y & is.na(x) == is.na(y)
   seq(x)[!(x == y & is.na(x) == is.na(y))]

With output :

> x <- c(NA, 1:7)
> y <- c(11:17, NA)
> ## now this
> seq(x)[ - which(x == y) ]  ## gives not what you expect
integer(0)

> ## But logical indexing always works:
>          x == y & is.na(x) == is.na(y)
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
> seq(x)[!(x == y & is.na(x) == is.na(y))]
[1] 1 2 3 4 5 6 7 8
> 

Martin Maechler
ETH Zurich


    >> -----Original Message-----
    >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of baccts
    >> Sent: Wednesday, 29 July 2015 8:26 a.m.
    >> To: r-help at r-project.org
    >> Subject: [R] indices of mismatch element in two vector with missing values
    >> 
    >> How would you return the index where two vectors differs if they may contain missing (NA) values?
    >> For example:
    >> test1 <- c("1","2",NA);
    >> test2 <- c("1","2","3");
    >> which(test1!=test2) does not return 3!
    >> 
    >> Thanks in advance.
    >> 
    >>


From pdalgd at gmail.com  Wed Jul 29 11:30:06 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 29 Jul 2015 11:30:06 +0200
Subject: [R] Element-by-element division
In-Reply-To: <CAM_vjumcmZSeFvqYcew=WoJVmY90Rzzpjg495APaAyMUzEH76A@mail.gmail.com>
References: <CAKTtY6S5jE6Dt8GMBe7zw1ihdyttOwvRwkXT+W1-Sn9w3TXVxQ@mail.gmail.com>
	<CAM_vjunk9ObntqZ2D+23xVp9c0_mB7YU-9YjYTsR1DXXO8MBoA@mail.gmail.com>
	<55B6D6B1.9060502@gmail.com>
	<CAM_vju=gHAEn_yBD1wt95+ATtszX+tHaBob6kTE-w64mbDCKHw@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6ACF89@mb02.ads.tamu.edu>
	<CAM_vjumcmZSeFvqYcew=WoJVmY90Rzzpjg495APaAyMUzEH76A@mail.gmail.com>
Message-ID: <FC733B07-5D7E-4C6E-8995-0068B6B4B969@gmail.com>


> On 28 Jul 2015, at 15:53 , Sarah Goslee <sarah.goslee at gmail.com> wrote:
> 
> Sure, there are lots of ways to do everything in R. But mixing in
> apply muddles the issue, since apply() and sweep() use different logic
> to determine MARGIN.

Actually, apply() and sweep() were designed together and use exactly the SAME logic to determine the margin. E.g., to sweep out means according to the last two dimensions of an array, you do

> m <- array(1:24, c(4,3,2))
> (mm <- apply(m, c(2,3), mean))
     [,1] [,2]
[1,]  2.5 14.5
[2,]  6.5 18.5
[3,] 10.5 22.5
> sweep(m, c(2,3), mm, "-")
, , 1

     [,1] [,2] [,3]
[1,] -1.5 -1.5 -1.5
[2,] -0.5 -0.5 -0.5
[3,]  0.5  0.5  0.5
[4,]  1.5  1.5  1.5

, , 2

     [,1] [,2] [,3]
[1,] -1.5 -1.5 -1.5
[2,] -0.5 -0.5 -0.5
[3,]  0.5  0.5  0.5
[4,]  1.5  1.5  1.5


The trouble comes when people miss the point and start using apply() in ways it wasn't designed for... 

-pd

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From j.para.fernandez at hotmail.com  Wed Jul 29 12:37:57 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Wed, 29 Jul 2015 03:37:57 -0700 (PDT)
Subject: [R] How to read CSV from web?
In-Reply-To: <1438140099663-4710502.post@n4.nabble.com>
References: <1438140099663-4710502.post@n4.nabble.com>
Message-ID: <1438166277331-4710513.post@n4.nabble.com>

data<-read.csv("https://raw.githubusercontent.com/sjkiss/Survey/master/mlogit.out.csv",header=T,sep=",")



--
View this message in context: http://r.789695.n4.nabble.com/How-to-read-CSV-from-web-tp4710502p4710513.html
Sent from the R help mailing list archive at Nabble.com.


From j.para.fernandez at hotmail.com  Wed Jul 29 13:00:18 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Wed, 29 Jul 2015 04:00:18 -0700 (PDT)
Subject: [R] Stop tkbind
In-Reply-To: <1438088436481-4710476.post@n4.nabble.com>
References: <1438088436481-4710476.post@n4.nabble.com>
Message-ID: <1438167618426-4710514.post@n4.nabble.com>

Not any ideas?

Thanks!



--
View this message in context: http://r.789695.n4.nabble.com/Stop-tkbind-tp4710476p4710514.html
Sent from the R help mailing list archive at Nabble.com.


From ntfredo at gmail.com  Wed Jul 29 13:15:58 2015
From: ntfredo at gmail.com (Frederic Ntirenganya)
Date: Wed, 29 Jul 2015 14:15:58 +0300
Subject: [R] Removing display of R row names from list.
Message-ID: <CAGh51gTWfNkiegqme3LB6Y=Jv+jfK-_LS_Nrx7rm9E-iHVb1Yw@mail.gmail.com>

 Dear All,

Is there a way to not include the row names when creating the list? Thanks!

Regards,
Frederic.

Frederic Ntirenganya
Maseno University,
African Maths Initiative,
Kenya.
Mobile:(+254)718492836
Email: fredo at aims.ac.za
https://sites.google.com/a/aims.ac.za/fredo/

	[[alternative HTML version deleted]]


From j.para.fernandez at hotmail.com  Wed Jul 29 13:57:20 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Wed, 29 Jul 2015 04:57:20 -0700 (PDT)
Subject: [R] Stop tkbind
In-Reply-To: <1438167618426-4710514.post@n4.nabble.com>
References: <1438088436481-4710476.post@n4.nabble.com>
	<1438167618426-4710514.post@n4.nabble.com>
Message-ID: <1438171040283-4710517.post@n4.nabble.com>

The solution is to create a new tkbind function that has not argument

tkbind(img,"<button-1>","")




--
View this message in context: http://r.789695.n4.nabble.com/Stop-tkbind-tp4710476p4710517.html
Sent from the R help mailing list archive at Nabble.com.


From jrkrideau at inbox.com  Wed Jul 29 15:40:27 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 29 Jul 2015 05:40:27 -0800
Subject: [R] VIF threshold implying multicollinearity
In-Reply-To: <CAE=6FXaZ1asX9XimTsKX_dCqV9kuFOzmzm2fnrvKSO9YQQ0mHA@mail.gmail.com>
References: <55b5a1ab.1030808@auckland.ac.nz>
	<639deaa621d.000001fbjrkrideau@inbox.com>
	<cakyn3id0azk+g-ywwyf9b8he02mm=cmop6yqzrtnhy1k-h1npa@mail.gmail.com>
	<cae=6fxazrek-mnumdlqgxlxgpbhcbld-dzyfrthjsapku8wrzq@mail.gmail.com>
Message-ID: <7D83584C5A4.0000104Ejrkrideau@inbox.com>

Quite, but apparently not a boisterous one? 

John Kane
Kingston ON Canada

-----Original Message-----
From: cflynch at ncsu.edu
Sent: Mon, 27 Jul 2015 14:35:06 -0400
To: jrkrideau at inbox.com
Subject: Re: [R] VIF threshold implying multicollinearity

No actually it is a quiet good paper! :)

On Mon, Jul 27, 2015 at 8:14 AM, John Kane <jrkrideau at inbox.com> wrote:

	+1
 I, originally,? read it as a stringent criticism of the first paper.

 John Kane
 Kingston ON Canada

 > -----Original Message-----
 > From: r.turner at auckland.ac.nz
 > Sent: Mon, 27 Jul 2015 15:12:43 +1200
 > To: cflynch at ncsu.edu
 > Subject: Re: [R] VIF threshold implying multicollinearity
 >
 >
 > On 27/07/15 13:36, Collin Lynch wrote:
 >
 >> The following sources discuss the issues generally and may be a goof
 >> pointer to the literature ...
 >
 > <SNIP>
 >
 > I think that the foregoing merits fortune status! :-)
 >
 > cheers,
 >
 > Rolf
 >
 > --
 > Technical Editor ANZJS
 > Department of Statistics
 > University of Auckland
 > Phone: +64-9-373-7599 ext. 88276
 >
 > ______________________________________________
 > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
 > https://stat.ethz.ch/mailman/listinfo/r-help [https://stat.ethz.ch/mailman/listinfo/r-help]
 > PLEASE do read the posting guide
 > http://www.R-project.org/posting-guide.html [http://www.R-project.org/posting-guide.html]
 > and provide commented, minimal, self-contained, reproducible code.

 ____________________________________________________________
 FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
 Visit http://www.inbox.com/photosharing [http://www.inbox.com/photosharing] to find out more!

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From jrkrideau at inbox.com  Wed Jul 29 15:50:41 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 29 Jul 2015 05:50:41 -0800
Subject: [R] R load error
In-Reply-To: <73551C12E91468488D61D86CDD49C930026D1C265CE5@fcrrex.fcrr.net>
Message-ID: <7D9A3626ABE.0000106Djrkrideau@inbox.com>

Try restarting R-Studio. I have found that every once in a while it seems to do something squirrelly but I have never isolated the problem enough to do a report.

Otherwise,perhaps run R in a terminal and see if it will load the data from there to  check if the file is actually okay.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: ypetscher at fcrr.org
> Sent: Mon, 27 Jul 2015 15:03:52 -0400
> To: r-help at r-project.org
> Subject: [R] R load error
> 
> Greetings - I'm using RStudio and recently updated both it and R. When
> loadings up, I'm now receiving the following error:
> 
> Error: ReadItem: unknown type 63, perhaps written by later version of R
> 
> I've tried using rm(list=ls())   rm(list=ls(all.names=TRUE)) and detach()
> but nothing works in R. Within RStudio it just continues to try and load
> and I'm unable to interrupt or restart. Any ideas would be greatly
> appreciated.
> 
> Yaacov Petscher, Ph.D.
> Associate Director, Florida Center for Reading Research
> Senior Research Associate, Regional Educational Laboratory-Southeast
> (REL-SE) at Florida State University
> 2010 Levy Ave
> Suite 100
> Tallahassee, Florida, 32310
> 850-645-8963 (p)
> 850-644-9085 (f)
> http://www.fcrr.org/for-researchers/petscher2.asp
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jrkrideau at inbox.com  Wed Jul 29 16:12:22 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 29 Jul 2015 06:12:22 -0800
Subject: [R] Removing display of R row names from list.
In-Reply-To: <CAGh51gTWfNkiegqme3LB6Y=Jv+jfK-_LS_Nrx7rm9E-iHVb1Yw@mail.gmail.com>
Message-ID: <7DCAABCE14E.000010ABjrkrideau@inbox.com>

HI Frederic,
Can you supply a small example of the problem?

John Kane
Kingston ON Canada


> -----Original Message-----
> From: ntfredo at gmail.com
> Sent: Wed, 29 Jul 2015 14:15:58 +0300
> To: r-help at r-project.org
> Subject: [R] Removing display of R row names from list.
> 
>  Dear All,
> 
> Is there a way to not include the row names when creating the list?
> Thanks!
> 
> Regards,
> Frederic.
> 
> Frederic Ntirenganya
> Maseno University,
> African Maths Initiative,
> Kenya.
> Mobile:(+254)718492836
> Email: fredo at aims.ac.za
> https://sites.google.com/a/aims.ac.za/fredo/
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jrkrideau at inbox.com  Wed Jul 29 16:40:31 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 29 Jul 2015 06:40:31 -0800
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438030410589-4710431.post@n4.nabble.com>
Message-ID: <7E099629DD3.0000110Ajrkrideau@inbox.com>

First http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and http://adv-r.had.co.nz/Reproducibility.html

Second. It is very annoying to have posts come in from Nabble. Very few R-help readers use it and the total context of some post to R-help is usually lost. If possible could you post directly to R-help.

We have no idea of what your problem is (see above) but here is a very simple example of a stacked barchart using ggplot2. And since I am already complaining I have included a side-by-side version of the barchart as well. I think that stacked barcharts are not a good idea unless obfuscation is the desired outcome. It is too hard to compare quantities with no common baseline.

Good luck.

dat1  <-  structure(list(dates = structure(c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L), .Label = c("1", 
"2", "3", "4", "5"), class = "factor"), revs = c(40, 7, 40, 20, 
35, 20, 15, 20, 15, 20, 15, 15, 35, 20, 20, 7, 7, 20, 7, 35), 
    typ....rep.LETTERS.1.2...10. = structure(c(1L, 2L, 1L, 2L, 
    1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 
    2L), .Label = c("A", "B"), class = "factor")), .Names = c("dates", 
"revs", "typ....rep.LETTERS.1.2...10."), row.names = c(NA, -20L
), class = "data.frame")

#Stacked barchart
ggplot(dat1, aes(dates, revs, fill = typ)) + geom_bar(stat = "identity")


#Grouped or dodged barchart (I don't think these are the real names)
ggplot(dat1, aes(dates, revs, fill = typ)) + geom_bar(stat = "identity", position="dodge")


John Kane
Kingston ON Canada


> -----Original Message-----
> From: hussain at touchofmodern.com
> Sent: Mon, 27 Jul 2015 13:53:30 -0700 (PDT)
> To: r-help at r-project.org
> Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
> 
> I am trying to use the ggplot2 to build a stacked bar chart for daily
> Revenue
> by category. The chart would look have date on the x-axis, and revenue on
> the y axis. The fill would be the categories themselves. I have searched
> a
> great deal and have been unable to find exactly how to do this.
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From shreya.cst at gmail.com  Wed Jul 29 16:47:37 2015
From: shreya.cst at gmail.com (shreya ghosh)
Date: Wed, 29 Jul 2015 20:17:37 +0530
Subject: [R] help_ReverseGeocoding
In-Reply-To: <CAJ4QxaOE2Ddz9dw5XEqO1FXJmxaYGZYrwExMPOwJO5COf09y+g@mail.gmail.com>
References: <CAB3ensGkrg_NBJdZ=xsN4s6qExPgHPAbRaysgcLL6yXhaPDvDw@mail.gmail.com>
	<D1DCEDCC.132B77%macqueen1@llnl.gov>
	<CAJ4QxaOE2Ddz9dw5XEqO1FXJmxaYGZYrwExMPOwJO5COf09y+g@mail.gmail.com>
Message-ID: <CAB3ensEEDYq3gXvZyh3W+Ts35YjA=jsD-FJf2s+6eGss9zTUDg@mail.gmail.com>

Thank you for your suggestion. Is there any open source api that won't
rate-limit it?



On Tue, Jul 28, 2015 at 9:38 PM, boB Rudis <bob at rudis.net> wrote:

> You should use ggmap::revgeocode (it calls google's api) and google
> will rate-limit you. There are also packages to use HERE maps
> geo/revgeo lookups
>
> http://blog.corynissen.com/2014/10/making-r-package-to-use-here-geocode-api.html
> and the geocode package has GNfindNearestAddress, so tons of options
> to choose from.
>
> On Tue, Jul 28, 2015 at 11:30 AM, MacQueen, Don <macqueen1 at llnl.gov>
> wrote:
> > My first guess, after a quick glance, is that Google only lets you do a
> > limited number of lookups within some period of time.
> >
> > -Don
> >
> > --
> > Don MacQueen
> >
> > Lawrence Livermore National Laboratory
> > 7000 East Ave., L-627
> > Livermore, CA 94550
> > 925-423-1062
> >
> >
> >
> >
> >
> > On 7/27/15, 10:14 PM, "R-help on behalf of shreya ghosh"
> > <r-help-bounces at r-project.org on behalf of shreya.cst at gmail.com> wrote:
> >
> >>Hi,
> >>I'm trying to do reversegeocoding on a large dataset. I'm using "RJSONIO"
> >>library and using Google map API to get the location of the given lat-lon
> >>in the dataset. After 100 or 150 successful displaying location
> >>information
> >>it is showing
> >> Warning message - "In readLines(con) : cannot open: HTTP status was '0
> >>(null)'"
> >>and Error : "Error in fromJSON(paste(readLines(con), collapse = "")) :
> >>  error in evaluating the argument 'content' in selecting a method for
> >>function 'fromJSON': Error in readLines(con) : cannot open the
> connection"
> >>
> >>Please help me to solve the issue.
> >>
> >>location function is as follows :
> >>
> >>location<-function(latlng){
> >> latlngStr <-  gsub(' ','%20', paste(latlng, collapse=","))
> >>  library("RJSONIO") #Load Library
> >>  #Open Connection
> >>  connectStr <- paste('
> >>http://maps.google.com/maps/api/geocode/json?sensor=false&latlng=
> ',latlngS
> >>tr,
> >>sep="")
> >>  con <- url(connectStr)
> >>  data.json <- fromJSON(paste(readLines(con), collapse=""))
> >>  close(con)
> >>
> >>  data.json <- unlist(data.json)
> >>  if(data.json["status"]=="OK")
> >>    address <- data.json["results.formatted_address"]
> >>  print (address)
> >>}
> >>
> >>I'm using R version 3.2.1 and Ubuntu 14.10 OS.
> >>
> >>Thank you.
> >>
> >>
> >>
> >>--
> >>
> >>Shreya Ghosh
> >>
> >>*9007448845*
> >>
> >>-- The mind is not a vessel to be filled, but a fire to be kindled
> >>
> >>       [[alternative HTML version deleted]]
> >>
> >>______________________________________________
> >>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide
> >>http://www.R-project.org/posting-guide.html
> >>and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 

Shreya Ghosh

*9007448845*

-- The mind is not a vessel to be filled, but a fire to be kindled

	[[alternative HTML version deleted]]


From j.para.fernandez at hotmail.com  Wed Jul 29 17:00:45 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Wed, 29 Jul 2015 08:00:45 -0700 (PDT)
Subject: [R] Imbalanced random forest
Message-ID: <1438182045051-4710524.post@n4.nabble.com>

?How can i set up a study with random forest where the response is highly
imbalanced?



-----

Guided Tours Basque Country 

Guided tours in the three capitals of the Basque Country: Bilbao, Vitoria-Gasteiz and San Sebastian, as well as in their provinces. Available languages.

Travel planners for groups and design of tourist routes across the Basque Country.
--
View this message in context: http://r.789695.n4.nabble.com/Imbalanced-random-forest-tp4710524.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Wed Jul 29 17:02:45 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 29 Jul 2015 08:02:45 -0700
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>
References: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>
	<108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbQ7uE4_VGdmCmwavwmD8sfNFpz+KtY-DFg1KHMjUQEXaw@mail.gmail.com>

There is confusion here. apply() family functions are **NOT**
vectorization -- they ARE loops (at the interpreter level), just done
in "functionalized" form. Please read background material (John
Chambers's books, MASS, or numerous others) to improve your
understanding and avoid posting erroneous comments.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Jul 28, 2015 at 3:00 PM, John Thaden <jjthaden at flash.net> wrote:
> Adam,    The method you propose gives a different result than the prior methods for these example vectors
> X <- c("ab", "cd", "ef")
> patt <- c("b", "cd", "a")
> repl <- c("B", "CD", "A")
>
> Old method 1
>
> mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
> gives
>   b   cd    a
> "aB" "CD" "ef"
>
> Old method 2
>
> sub2 <- function(pattern, replacement, x) {
>     len <- length(x)
>     if (length(pattern) == 1)
>         pattern <- rep(pattern, len)
>     if (length(replacement) == 1)
>         replacement <- rep(replacement, len)
>     FUN <- function(i, ...) {
>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>     }
>     idx <- 1:length(x)
>     sapply(idx, FUN)
> }
> sub2(patt, repl, X)
>  gives
> [1] "aB" "CD" "ef"
>
> Your method (I gave it the unique name "sub3")
>  sub3 <- function(pattern, replacement, x) {   len    <- length(x)  y      <- character(length=len)  patlen <- length(pattern)  replen <- length(replacement)  if(patlen != replen) stop('Error: Pattern and replacement length do not match')  for(i in 1:replen) {    y[which(x==pattern[i])] <- replacement[i]  }  return(y)}sub3(patt, repl, X)
> gives[1] ""   "CD" ""
>
> Granted, whatever it does, it does it faster
> #Old method 1
> system.time(for(i in 1:50000)
> mapply(function(p,r,x) sub(p,r,x, fixed = TRUE),p=patt,r=repl,x=X))
>    user  system elapsed
>    2.53    0.00    2.52
>
> #Old method 2
> system.time(for(i in 1:50000)sub2(patt, repl, X))   user  system elapsed
>    2.32    0.00    2.32
>
> #Your proposed method
> system.time(for(i in 1:50000) sub3(patt, repl, X))
>    user  system elapsed
>    1.02    0.00    1.01
>  but would it still be faster if it actually solved the same problem?
>
> -John Thaden
>
>
>
>
>      On Monday, July 27, 2015 11:40 PM, Adam Erickson <adam.michael.erickson at gmail.com> wrote:
>
> I know this is an old thread, but I wrote a simple FOR loop with vectorized pattern replacement that is much faster than either of those (it can also accept outputs differing in length from the patterns):
>   sub2  <- function(pattern, replacement, x) {     len   <- length(x)    y      <- character(length=len)    patlen <- length(pattern)    replen <- length(replacement)    if(patlen != replen) stop('Error: Pattern and replacement length do not match')    for(i in 1:replen) {      y[which(x==pattern[i])] <- replacement[i]    }    return(y)  }
> system.time(test <- sub2(patt, repl, XX))   user  system elapsed       0       0       0
> Cheers,
> Adam
> On Wednesday, October 8, 2008 at 9:38:01 PM UTC-7, john wrote:
> Hello Christos,
>   To my surprise, vectorization actually hurt processing speed!#Example
> X <- c("ab", "cd", "ef")
> patt <- c("b", "cd", "a")
> repl <- c("B", "CD", "A")sub2 <- function(pattern, replacement, x) {
>     len <- length(x)
>     if (length(pattern) == 1)
>         pattern <- rep(pattern, len)
>     if (length(replacement) == 1)
>         replacement <- rep(replacement, len)
>     FUN <- function(i, ...) {
>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>     }
>     idx <- 1:length(x)
>     sapply(idx, FUN)
> }
>
> system.time(  for(i in 1:10000)  sub2(patt, repl, X)  )
>    user  system elapsed
>    1.18    0.07    1.26 system.time(  for(i in 1:10000)  mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)  )
>    user  system elapsed
>    1.42    0.05    1.47
>
> So much for avoiding loops.
> John Thaden======= At 2008-10-07, 14:58:10 Christos wrote: =======>John,
>>Try the following:
>>
>> mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X)
>>   b   cd    a
>>"aB" "CD" "ef"
>>
>>-Christos>> -----My Original Message-----
>>> R pattern-matching and replacement functions are
>>> vectorized: they can operate on vectors of targets.
>>> However, they can only use one pattern and replacement.
>>> Here is code to apply a different pattern and replacement for
>>> every target.  My question: can it be done better?
>>>
>>> sub2 <- function(pattern, replacement, x) {
>>>     len <- length(x)
>>>     if (length(pattern) == 1)
>>>         pattern <- rep(pattern, len)
>>>     if (length(replacement) == 1)
>>>         replacement <- rep(replacement, len)
>>>     FUN <- function(i, ...) {
>>>         sub(pattern[i], replacement[i], x[i], fixed = TRUE)
>>>     }
>>>     idx <- 1:length(x)
>>>     sapply(idx, FUN)
>>> }
>>>
>>> #Example
>>> X <- c("ab", "cd", "ef")
>>> patt <- c("b", "cd", "a")
>>> repl <- c("B", "CD", "A")
>>> sub2(patt, repl, X)
>>>
>>> -John______________________________________________
> R-h... at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ntfredo at gmail.com  Wed Jul 29 17:11:36 2015
From: ntfredo at gmail.com (Frederic Ntirenganya)
Date: Wed, 29 Jul 2015 18:11:36 +0300
Subject: [R] Removing display of R row names from list.
In-Reply-To: <7DCAABCE14E.000010ABjrkrideau@inbox.com>
References: <CAGh51gTWfNkiegqme3LB6Y=Jv+jfK-_LS_Nrx7rm9E-iHVb1Yw@mail.gmail.com>
	<7DCAABCE14E.000010ABjrkrideau@inbox.com>
Message-ID: <CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>

Here is a small example. it is not the real data I am working on but this
can help

#Chick weight example

names(ChickWeight) <- tolower(names(ChickWeight))
chick_m <- melt(ChickWeight, id=2:4, na.rm=TRUE)
class(chick_m)

test = function(data){
  i = 1
  table = list()
  table [[i]] <- dcast(chick_m, time ~ variable, mean)
  i = i + 1

  print(table)

}
test(chick_m )


Frederic Ntirenganya
Maseno University,
African Maths Initiative,
Kenya.
Mobile:(+254)718492836
Email: fredo at aims.ac.za
https://sites.google.com/a/aims.ac.za/fredo/

On Wed, Jul 29, 2015 at 5:12 PM, John Kane <jrkrideau at inbox.com> wrote:

> HI Frederic,
> Can you supply a small example of the problem?
>
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: ntfredo at gmail.com
> > Sent: Wed, 29 Jul 2015 14:15:58 +0300
> > To: r-help at r-project.org
> > Subject: [R] Removing display of R row names from list.
> >
> >  Dear All,
> >
> > Is there a way to not include the row names when creating the list?
> > Thanks!
> >
> > Regards,
> > Frederic.
> >
> > Frederic Ntirenganya
> > Maseno University,
> > African Maths Initiative,
> > Kenya.
> > Mobile:(+254)718492836
> > Email: fredo at aims.ac.za
> > https://sites.google.com/a/aims.ac.za/fredo/
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> Check it out at http://www.inbox.com/earth
>
>
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Jul 29 17:28:47 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 29 Jul 2015 08:28:47 -0700
Subject: [R] Removing display of R row names from list.
In-Reply-To: <CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>
References: <CAGh51gTWfNkiegqme3LB6Y=Jv+jfK-_LS_Nrx7rm9E-iHVb1Yw@mail.gmail.com>
	<7DCAABCE14E.000010ABjrkrideau@inbox.com>
	<CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>
Message-ID: <CAGxFJbT5remzj8X53nxMtmiPfey8JECHFzBK5c-SNH6PiCBK7g@mail.gmail.com>

1. Please post in plain text, not HTML.

2. Please do your homework. Data frames (the class of the result of
dcast) **always** have row names. ?data.frame  for details.

3. Read up on S3 methods if you have not done so already (any good R
tutorial will tell you about them, including the "Intro to R" tutorial
that ships with R). Then see the "row.names" argument of
?print.data.frame  .


Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Jul 29, 2015 at 8:11 AM, Frederic Ntirenganya <ntfredo at gmail.com> wrote:
> Here is a small example. it is not the real data I am working on but this
> can help
>
> #Chick weight example
>
> names(ChickWeight) <- tolower(names(ChickWeight))
> chick_m <- melt(ChickWeight, id=2:4, na.rm=TRUE)
> class(chick_m)
>
> test = function(data){
>   i = 1
>   table = list()
>   table [[i]] <- dcast(chick_m, time ~ variable, mean)
>   i = i + 1
>
>   print(table)
>
> }
> test(chick_m )
>
>
> Frederic Ntirenganya
> Maseno University,
> African Maths Initiative,
> Kenya.
> Mobile:(+254)718492836
> Email: fredo at aims.ac.za
> https://sites.google.com/a/aims.ac.za/fredo/
>
> On Wed, Jul 29, 2015 at 5:12 PM, John Kane <jrkrideau at inbox.com> wrote:
>
>> HI Frederic,
>> Can you supply a small example of the problem?
>>
>> John Kane
>> Kingston ON Canada
>>
>>
>> > -----Original Message-----
>> > From: ntfredo at gmail.com
>> > Sent: Wed, 29 Jul 2015 14:15:58 +0300
>> > To: r-help at r-project.org
>> > Subject: [R] Removing display of R row names from list.
>> >
>> >  Dear All,
>> >
>> > Is there a way to not include the row names when creating the list?
>> > Thanks!
>> >
>> > Regards,
>> > Frederic.
>> >
>> > Frederic Ntirenganya
>> > Maseno University,
>> > African Maths Initiative,
>> > Kenya.
>> > Mobile:(+254)718492836
>> > Email: fredo at aims.ac.za
>> > https://sites.google.com/a/aims.ac.za/fredo/
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ____________________________________________________________
>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>> Check it out at http://www.inbox.com/earth
>>
>>
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Wed Jul 29 17:32:53 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 29 Jul 2015 07:32:53 -0800
Subject: [R] Removing display of R row names from list.
In-Reply-To: <CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>
References: <cagh51gtwfnkiegqme3lb6y=jv+jfk-_ls_nrx7rm9e-ihvb1yw@mail.gmail.com>
	<7dcaabce14e.000010abjrkrideau@inbox.com>
Message-ID: <7E7EA688B68.000011BCjrkrideau@inbox.com>

Beats me.  You can print a single data frame with
print(dat1, rownames = FALSE) but it is not clear to me how to do it within a function.  

I am sure someone who actually know what they are doing will be along in a moment. Sorry not to have been of more help.

John Kane
Kingston ON Canada

-----Original Message-----
From: ntfredo at gmail.com
Sent: Wed, 29 Jul 2015 18:11:36 +0300
To: jrkrideau at inbox.com
Subject: Re: [R] Removing display of R row names from list.

Here is a small example. it is not the real data I am working on but this can help

#Chick weight example

names(ChickWeight) <- tolower(names(ChickWeight))
chick_m <- melt(ChickWeight, id=2:4, na.rm=TRUE)
class(chick_m)

test = function(data){
? i = 1
? table = list()? 
? table [[i]] <- dcast(chick_m, time ~ variable, mean)
? i = i + 1

? print(table)

}
test(chick_m )

Frederic Ntirenganya
Maseno University,

African Maths Initiative,

Kenya.

Mobile:(+254)718492836

Email:?fredo at aims.ac.za

https://sites.google.com/a/aims.ac.za/fredo/ [https://sites.google.com/a/aims.ac.za/fredo/]

On Wed, Jul 29, 2015 at 5:12 PM, John Kane <jrkrideau at inbox.com> wrote:

	HI Frederic,
 Can you supply a small example of the problem?

 John Kane
 Kingston ON Canada

 > -----Original Message-----
 > From: ntfredo at gmail.com
 > Sent: Wed, 29 Jul 2015 14:15:58 +0300
 > To: r-help at r-project.org
 > Subject: [R] Removing display of R row names from list.
 >
 >? Dear All,
 >
 > Is there a way to not include the row names when creating the list?
 > Thanks!
 >
 > Regards,
 > Frederic.
 >
 > Frederic Ntirenganya
 > Maseno University,
 > African Maths Initiative,
 > Kenya.
 > Mobile:(+254)718492836
 > Email: fredo at aims.ac.za
 > https://sites.google.com/a/aims.ac.za/fredo/ [https://sites.google.com/a/aims.ac.za/fredo/]
 >
 >? ? ? ?[[alternative HTML version deleted]]
 >
 > ______________________________________________
 > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
 > https://stat.ethz.ch/mailman/listinfo/r-help [https://stat.ethz.ch/mailman/listinfo/r-help]
 > PLEASE do read the posting guide
 > http://www.R-project.org/posting-guide.html [http://www.R-project.org/posting-guide.html]
 > and provide commented, minimal, self-contained, reproducible code.

 ____________________________________________________________
 FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
 Check it out at http://www.inbox.com/earth [http://www.inbox.com/earth]

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From mxkuhn at gmail.com  Wed Jul 29 18:01:05 2015
From: mxkuhn at gmail.com (Max Kuhn)
Date: Wed, 29 Jul 2015 12:01:05 -0400
Subject: [R] Imbalanced random forest
In-Reply-To: <1438182045051-4710524.post@n4.nabble.com>
References: <1438182045051-4710524.post@n4.nabble.com>
Message-ID: <CAJ9CoWkY3XHZp4pvi-cwst3v46AzUeTK97TtiJfrMB=rT5v5XA@mail.gmail.com>

This might help:

http://bit.ly/1MUP0Lj

On Wed, Jul 29, 2015 at 11:00 AM, jpara3 <j.para.fernandez at hotmail.com>
wrote:

> ?How can i set up a study with random forest where the response is highly
> imbalanced?
>
>
>
> -----
>
> Guided Tours Basque Country
>
> Guided tours in the three capitals of the Basque Country: Bilbao,
> Vitoria-Gasteiz and San Sebastian, as well as in their provinces. Available
> languages.
>
> Travel planners for groups and design of tourist routes across the Basque
> Country.
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Imbalanced-random-forest-tp4710524.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Jul 29 18:04:11 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 29 Jul 2015 09:04:11 -0700
Subject: [R] Removing display of R row names from list.
In-Reply-To: <7E7EA688B68.000011BCjrkrideau@inbox.com>
References: <cagh51gtwfnkiegqme3lb6y=jv+jfk-_ls_nrx7rm9e-ihvb1yw@mail.gmail.com>
	<7dcaabce14e.000010abjrkrideau@inbox.com>
	<CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>
	<7E7EA688B68.000011BCjrkrideau@inbox.com>
Message-ID: <CAGxFJbQtLsQ8X4k9a5YmX9rFgR6+9zvgGrLXU+2shTfom__5qQ@mail.gmail.com>

Is this what you mean?

z <- data.frame(a=1:3,b=letters[1:3],row.names=letters[1:3])
zlist <- list(one=z,too =z)

for(i in 1:2){print(zlist[[i]],row.names=FALSE); cat("\n")}

 a b
 1 a
 2 b
 3 c

 a b
 1 a
 2 b
 3 c

... which could obviously be within a function.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Jul 29, 2015 at 8:32 AM, John Kane <jrkrideau at inbox.com> wrote:
> Beats me.  You can print a single data frame with
> print(dat1, rownames = FALSE) but it is not clear to me how to do it within a function.
>
> I am sure someone who actually know what they are doing will be along in a moment. Sorry not to have been of more help.
>
> John Kane
> Kingston ON Canada
>
> -----Original Message-----
> From: ntfredo at gmail.com
> Sent: Wed, 29 Jul 2015 18:11:36 +0300
> To: jrkrideau at inbox.com
> Subject: Re: [R] Removing display of R row names from list.
>
> Here is a small example. it is not the real data I am working on but this can help
>
> #Chick weight example
>
> names(ChickWeight) <- tolower(names(ChickWeight))
> chick_m <- melt(ChickWeight, id=2:4, na.rm=TRUE)
> class(chick_m)
>
> test = function(data){
>   i = 1
>   table = list()
>   table [[i]] <- dcast(chick_m, time ~ variable, mean)
>   i = i + 1
>
>   print(table)
>
> }
> test(chick_m )
>
> Frederic Ntirenganya
> Maseno University,
>
> African Maths Initiative,
>
> Kenya.
>
> Mobile:(+254)718492836
>
> Email: fredo at aims.ac.za
>
> https://sites.google.com/a/aims.ac.za/fredo/ [https://sites.google.com/a/aims.ac.za/fredo/]
>
> On Wed, Jul 29, 2015 at 5:12 PM, John Kane <jrkrideau at inbox.com> wrote:
>
>         HI Frederic,
>  Can you supply a small example of the problem?
>
>  John Kane
>  Kingston ON Canada
>
>  > -----Original Message-----
>  > From: ntfredo at gmail.com
>  > Sent: Wed, 29 Jul 2015 14:15:58 +0300
>  > To: r-help at r-project.org
>  > Subject: [R] Removing display of R row names from list.
>  >
>  >  Dear All,
>  >
>  > Is there a way to not include the row names when creating the list?
>  > Thanks!
>  >
>  > Regards,
>  > Frederic.
>  >
>  > Frederic Ntirenganya
>  > Maseno University,
>  > African Maths Initiative,
>  > Kenya.
>  > Mobile:(+254)718492836
>  > Email: fredo at aims.ac.za
>  > https://sites.google.com/a/aims.ac.za/fredo/ [https://sites.google.com/a/aims.ac.za/fredo/]
>  >
>  >       [[alternative HTML version deleted]]
>  >
>  > ______________________________________________
>  > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>  > https://stat.ethz.ch/mailman/listinfo/r-help [https://stat.ethz.ch/mailman/listinfo/r-help]
>  > PLEASE do read the posting guide
>  > http://www.R-project.org/posting-guide.html [http://www.R-project.org/posting-guide.html]
>  > and provide commented, minimal, self-contained, reproducible code.
>
>  ____________________________________________________________
>  FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>  Check it out at http://www.inbox.com/earth [http://www.inbox.com/earth]
>
> ____________________________________________________________
> FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
> Visit http://www.inbox.com/photosharing to find out more!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From 538280 at gmail.com  Wed Jul 29 18:37:28 2015
From: 538280 at gmail.com (Greg Snow)
Date: Wed, 29 Jul 2015 10:37:28 -0600
Subject: [R] How to simulate informative censoring in a Cox PH model?
In-Reply-To: <CABrUXPVi2=GnEg14xZQ19XjJOnuW=61NVZc+Vkbfo2HDN08L8g@mail.gmail.com>
References: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>
	<CAFEqCdzk8cSwed-VdBrob1bzLQtx7rknK9UkNEU_bwt8auoD7A@mail.gmail.com>
	<CABrUXPVi2=GnEg14xZQ19XjJOnuW=61NVZc+Vkbfo2HDN08L8g@mail.gmail.com>
Message-ID: <CAFEqCdxb9_Lv-_DoYVM26F=wOW_cJnn-FRtYubQhkbL8gWpjbw@mail.gmail.com>

As models become more complex it becomes harder to distinguish
different parts and their effects.  Even for a straight forward linear
regression model if X1 and X2 are correlated with each other then it
becomes difficult to distinguish between the effects of X1^2, X2^2,
and X1*X2.  In your case the informative censoring and model
misspecification are becoming hard to distinguish (and it could be
argued that having informative censoring is really just a form of
model misspecification).  So I don't think so much that you are doing
things wrong, just that you are learning that the models are complex.

Another approach to simulation that you could try is to simulate the
event time and censoring time using copulas (and therefore they can be
correlated to give informative censoring, but without relying on a
term that you could have included in the model) then consider the
event censored if the censoring time is shorter.

On Fri, Jul 24, 2015 at 10:14 AM, Daniel Meddings <dpmeddings at gmail.com> wrote:
> Hi Greg
>
> Many thanks for taking the time to respond to my query. You are right about
> pointing out the distinction between what variables are and are not included
> in the event times process and in the censoring process. I clearly forgot
> this important aspect. I amended my code to do as you advise and now I am
> indeed getting biased estimates when using the informatively censored
> responses. The problem is now that the estimates from the independently
> censored responses are the same - i.e. they are just as biased. Thus the
> bias seems to be due entirely to model mis-specification and not the
> informative censoring.
>
>
> To give a concrete example I simulate event times T_i and censoring times
> C_i from the following models;
>
>
> T_i~ Weibull(lambda_t(x),v_t),    lambda_t(x)=lambda_t*exp( beta_t_0 +
> (beta_t_1*Treat_i) + (beta_t_2*Z_i) + (beta_t_3*Treat_i*Z_i)  )
>
> C_i~ Weibull(lambda_c(x),v_c),    lambda_c(x)=lambda_c*exp( beta_c_0 +
> (beta_c_1*Treat_i) + (beta_c_2*Z_i) + (beta_c_3*Treat_i*Z_i)  )
>
> D_i~Weibull(lambda_d(x),v_D), lambda_d(x)=lamda_d*exp( beta_d_0)
>
> where ;
>
> beta_t_0 = 1,  beta_t_1 = -1,   beta_t_2 = 1,  beta_t_3 = -2,   lambda_t=0.5
>
> beta_c_0 = 0.2,  beta_c_1 = -2,   beta_c_2 = 2,  beta_c_3 = -2,
> lambda_c=0.5
>
> beta_d_0 = -0.7,  lambda_d=0.1
>
> When I fit the cox model to both the informatively censored responses and
> the independent censored responses I include only the Treatment covariate in
> the model.
>
> I simulate Treatment from a Bernoulli distribution with p=0.5 and Z_i from a
> beta distribution so that Z ranges from 0 to 1 (I like to think of Z as a
> "poor" prognosis probability where Z_i=1 means a subject is 100% certain to
> have a poor prognosis and Z_i=0 means zero chance). These parameter choices
> give approximately 27% and 25% censoring for the informatively censored
> responses (using C_i) and the independent censored responses (using D_i)
> respectively. I use N=2000 subjects and 2000 simulation replications.
>
> The above simulation I get estimates of beta_t_2 of -1.526 and -1.537 for
> the informatively censored responses and the independent censored responses
> respectively.
>
> Furthermore when I fit a cox model to the full responses (no censoring at
> all) I get an estimate of beta_t_2 of -1.542. This represents the best that
> can possibly be done given that Z and Treat*Z are not in the model. Clearly
> censoring is not making much of a difference here - model mis-specification
> dominates.
>
> I still must be doing something wrong but I cannot figure this one out.
>
> Thanks
>
> Dan
>
>
>
> On Thu, Jul 23, 2015 at 12:33 AM, Greg Snow <538280 at gmail.com> wrote:
>>
>> I think that the Cox model still works well when the only information
>> in the censoring is conditional on variables in the model.  What you
>> describe could be called non-informative conditional on x.
>>
>> To really see the difference you need informative censoring that
>> depends on something not included in the model.  One option would be
>> to use copulas to generate dependent data and then transform the
>> values using your Weibul.  Or you could generate your event times and
>> censoring times based on x1 and x2, but then only include x1 in the
>> model.
>>
>> On Wed, Jul 22, 2015 at 2:20 AM, Daniel Meddings <dpmeddings at gmail.com>
>> wrote:
>> > I wish to simulate event times where the censoring is informative, and
>> > to
>> > compare parameter estimator quality from a Cox PH model with estimates
>> > obtained from event times generated with non-informative censoring.
>> > However
>> > I am struggling to do this, and I conclude rather than a technical flaw
>> > in
>> > my code I instead do not understand what is meant by informative and
>> > un-informative censoring.
>> >
>> > My approach is to simulate an event time T dependent on a vector of
>> > covariates x having hazard function h(t|x)=lambda*exp(beta'*x)v*t^{v-1}.
>> > This corresponds to T~ Weibull(lambda(x),v), where the scale parameter
>> > lambda(x)=lambda*exp(beta'*x) depends on x and the shape parameter v is
>> > fixed. I have N subjects where T_{i}~ Weibull(lambda(x_{i}),v_{T}),
>> > lambda(x_{i})=lambda_{T}*exp(beta_{T}'*x_{i}), for i=1,...,N. Here I
>> > assume
>> > the regression coefficients are p-dimensional.
>> >
>> > I generate informative censoring times C_i~ Weibull(lambda(x_i),v_C),
>> > lambda(x_i)=lambda_C*exp(beta_C'*x_i) and compute Y_inf_i=min(T_i,C_i)
>> > and
>> > a censored flag delta_inf_i=1 if Y_inf_i <= C_i (an observed event), and
>> > delta_inf_i=0 if Y_inf_i > C_i (informatively censored: event not
>> > observed). I am convinced this is informative censoring because as long
>> > as
>> > beta_T~=0 and beta_C~=0 then for each subject the data generating
>> > process
>> > for T and C both depend on x.
>> >
>> > In contrast I generate non-informative censoring times
>> > D_i~Weibull(lambda_D*exp(beta_D),v_D), and compute Y_ninf_i=min(T_i,D_i)
>> > and a censored flag delta_ninf_i=1 if Y_ninf_i <= D_i (an observed
>> > event),
>> > and delta_ninf_i=0 if Y_ninf_i > D_i (non-informatively censored: event
>> > not
>> > observed). Here beta_D is a scalar. I "scale" the simulation by choosing
>> > the lambda_T, lambda_C and lambda_D parameters such that on average
>> > T_i<C_i
>> > and T_i<D_i to achieve X% of censored subjects for both Y_inf_i and
>> > Y_ninf_i.
>> >
>> > The problem is that even for say 30% censoring (which I think is high),
>> > the
>> > Cox PH parameter estimates using both Y_inf and Y_ninf are unbiased when
>> > I
>> > expected the estimates using Y_inf to be biased, and I think I see why:
>> > however different beta_C is from beta_T, a censored subject can
>> > presumably
>> > influence the estimation of beta_T only by affecting the set of subjects
>> > at
>> > risk at any time t, but this does not change the fact that every single
>> > Y_inf_i with delta_inf_i=1 will have been generated using beta_T only.
>> > Thus
>> > I do not see how my simulation can possibly produce biased estimates for
>> > beta_T using Y_inf.
>> >
>> > But then what is informative censoring if not based on this approach?
>> >
>> > Any help would be greatly appreciated.
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>>
>>
>> --
>> Gregory (Greg) L. Snow Ph.D.
>> 538280 at gmail.com
>
>



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From attenka at utu.fi  Wed Jul 29 11:30:26 2015
From: attenka at utu.fi (Atte Tenkanen)
Date: Wed, 29 Jul 2015 12:30:26 +0300
Subject: [R] Opposite color in R
Message-ID: <55B89D32.5000003@utu.fi>

Hi,

Nope. My point is that color pairs red-green, yellow-violet and blue-orange are physically on the opposite sides of the wheel.
I'm not 100% satisfied with this circle, because three of greens and reds are so near each other. But this is the best solution thus far :-)
The number of colors, 24, is also fixed in the application.

Atte

> Hi
>
> I plotted the 'spectrum' and it looked a little small - spectrum colours: red orange yellow green blue indigo violet.
> I suppose you could go to infinite lengths to split it up but is this an improvement?
>
> I have not gone into the "depths" of complimentary colours
>
> library(colorspace)
>
> ColorsRYB=rbind(colorRamp(c("purple","violet"))((0:4)/4)[1:4,],
>                  colorRamp(c("violet","blue"))((0:4)/4)[1:4,],
>                  colorRamp(c("blue","green"))((0:4)/4)[1:4,],
>                  colorRamp(c("green","yellow"))((0:4)/4)[1:4,],
>                  colorRamp(c("yellow","orange"))((0:4)/4)[1:4,],
>                  colorRamp(c("orange","red"))((0:4)/4)[1:4,],
>                  colorRamp(c("red","purple"))((0:4)/4)[1:4,])
>
> LenCol=length(ColorsRYB[,1])
>
> ColorsRYBhex=rep(0, LenCol)
> for(i in 1: LenCol)
> {
> ColorsRYBhex[i]=rgb(ColorsRYB[i,1]/255,ColorsRYB[i,2]/255,ColorsRYB[i,3]/255)
> }
>
> pie(rep(1, LenCol), col = ColorsRYBhex)
>
> Regards
>
> Duncan
>
> Duncan Mackay
> Department of Agronomy and Soil Science
> University of New England
> Armidale NSW 2351
> Email: home:mackay at northnet.com.au  <https://stat.ethz.ch/mailman/listinfo/r-help>
>
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org  <https://stat.ethz.ch/mailman/listinfo/r-help>] On Behalf Of Atte Tenkanen
> Sent: Tuesday, 28 July 2015 18:22
> To:r-help at r-project.org  <https://stat.ethz.ch/mailman/listinfo/r-help>
> Subject: [R] Opposite color in R
>
> It seems that there is no implementation for the "traditional artist's
> color circle" in R. However I'm searching for such a wheel, because my
> program needs it.
>
> As said, the description of complementary/opposite-function in package
> "colortools" is misleading since, for example
>
> opposite("green") produces violet, not red, but the description of
> complementary-function says
>
> "Complementary or opposite color scheme is formed by colors that are
> opposite each other on the color wheel (example: red and green)."
>
> So, there must be just a lapse in the text.
>
> I "constrained" such kind of a color wheel, which is enough near of what
> I need:
>
> library(colorspace)
>
> ColorsRYB=rbind(colorRamp(c("red",
> "violet"))((0:4)/4)[1:4,],colorRamp(c("violet",
> "blue"))((0:4)/4)[1:4,],colorRamp(c("blue",
> "green"))((0:4)/4)[1:4,],colorRamp(c("green",
> "yellow"))((0:4)/4)[1:4,],colorRamp(c("yellow",
> "orange"))((0:4)/4)[1:4,],colorRamp(c("orange", "red"))((0:4)/4)[1:4,])
>
> LenCol=length(ColorsRYB[,1])
>
> ColorsRYBhex=rep(0, LenCol)
> for(i in 1: LenCol)
> {
> ColorsRYBhex[i]=rgb(ColorsRYB[i,1]/255,ColorsRYB[i,2]/255,ColorsRYB[i,3]/255)
> }
>
> pie(rep(1, 24), col = ColorsRYBhex)
>
> Atte T.
>
>
> 28.7.2015, 2.23, Steve Taylor kirjoitti:
> >/  I wonder if the hcl colour space is useful?  Varying hue while keeping chroma and luminosity constant should give varying colours of perceptually the same "colourness" and brightness.
> />/
> />/  ?hcl
> />/  pie(rep(1,12),col=hcl((1:12)*30,c=70),border=NA)
> />/
> />/
> />/  -----Original Message-----
> />/  From: R-help [mailto:r-help-bounces at r-project.org  <https://stat.ethz.ch/mailman/listinfo/r-help>] On Behalf Of Atte Tenkanen
> />/  Sent: Sunday, 26 July 2015 7:50a
> />/  To:r-help at r-project.org  <https://stat.ethz.ch/mailman/listinfo/r-help>
> />/  Subject: [R] Opposite color in R
> />/
> />/  Hi,
> />/
> />/  I have tried to find a way to find opposite or complementary colors in R.
> />/
> />/  I would like to form a color circle with R like this one:
> />/  http://nobetty.net/dandls/colorwheel/complementary_colors.jpg
> />/
> />/  If you just make a basic color wheel in R, the colors do not form
> />/  complementary color circle:
> />/
> />/  palette(rainbow(24))
> />/  Colors=palette()
> />/  pie(rep(1, 24), col = Colors)
> />/
> />/  There is a package ?colortools? where you can find function opposite(),
> />/  but it doesn?t work as is said. I tried
> />/
> />/  library(colortools)
> />/  opposite("violet") and got green instead of yellow and
> />/
> />/  opposite("blue") and got yellow instead of orange.
> />/
> />/  Do you know any solutions?
> />/
> />/  Atte Tenkanen
> />


	[[alternative HTML version deleted]]


From baccts at hotmail.com  Wed Jul 29 15:19:49 2015
From: baccts at hotmail.com (baccts)
Date: Wed, 29 Jul 2015 06:19:49 -0700 (PDT)
Subject: [R] indices of mismatch element in two vector with
	missing	values
In-Reply-To: <21944.38599.987763.588086@stat.math.ethz.ch>
References: <1438115132091-4710497.post@n4.nabble.com>
	<E41B375B7520DE4A8C60781AC60B75450672242F53@AKLEXM01.PFR.CO.NZ>
	<55B86D92.4030708@fredhutch.org>
	<21944.38599.987763.588086@stat.math.ethz.ch>
Message-ID: <1438175989495-4710518.post@n4.nabble.com>

Thank you everyone for taking the time to reply.

Thanks Martin and Herv? for your solutions.
Now I just need to remember to check for NA match.



--
View this message in context: http://r.789695.n4.nabble.com/indices-of-mismatch-element-in-two-vector-with-missing-values-tp4710497p4710518.html
Sent from the R help mailing list archive at Nabble.com.


From magifranquesa at gmail.com  Wed Jul 29 18:29:24 2015
From: magifranquesa at gmail.com (Magi Franquesa)
Date: Wed, 29 Jul 2015 18:29:24 +0200
Subject: [R] PythonInR. Python script in R with parameters required.
 Download satellite images from NASA
Message-ID: <CAJn2PbX8ahfRui_5d1O=XUNOur8A6fZROY68gu2u9dPWV3A-mg@mail.gmail.com>

Hello,

I'm trying to execute a python script within R (3.2.1 x 64) with the
PythonInR package. I would like to download an order of satellite images
from Nasa using a python script (
http://landsat.usgs.gov/documents/espa_bulk_downloader_v1.0.0.zip) but I
have no success. I first run the pyExecfile command with the *feedparser.py*
script and then the *download_espa_order.py* giving the required parameters
(my mail acount and the order number), here is the code:

setwd("C:/Python27")
install.packages("PythonInR")
library(PythonInR)
pyConnect(pythonExePath="C:/Python27/python.exe")
pyIsConnected()
# autodetectPython("C:/Python27/python.exe")

pyExecfile("C:/Landsat/feedparser.py")
pyExecfile("C:/Landsat/download_espa_order.py" -e "magifranquesa at gmail.com"
-o "magifranquesa at gmail.com-07222015-120911" -d "C:/Landsat/ESPA")

and I get this error:

Error: unexpected string constant in
"pyExecfile("C:/Landsat/download_espa_order.py" -e
"magifranquesa at gmail.com""

The code "C:/Landsat/download_espa_order.py" -e
"magifranquesa at gmail.com" -o "magifranquesa at gmail.com-07222015-120911"
-d "C:/Landsat/ESPA" runs ok when I use it within
system console.

I appreciate if someone could help me to solve this problem.

Thank you

	[[alternative HTML version deleted]]


From hussain at touchofmodern.com  Wed Jul 29 20:23:54 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Wed, 29 Jul 2015 11:23:54 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <7E099629DD3.0000110Ajrkrideau@inbox.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
Message-ID: <1438194234637-4710540.post@n4.nabble.com>

Hi John, 

Great thanks for the examples! I am not sure what you are referring to when
you say a post from Nabble - I posted this directly on R-help. I actually
never heard of Nabble.

Stacked Bar charts are great for when you have a lot of moving parts and
need to be able to zero in on one of them (e.g. Advertising Placements).
Unfortunately, I can't post the data since it is confidential. 



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710540.html
Sent from the R help mailing list archive at Nabble.com.


From jdnewmil at dcn.davis.CA.us  Wed Jul 29 20:32:58 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 29 Jul 2015 14:32:58 -0400
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438194234637-4710540.post@n4.nabble.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
Message-ID: <02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>

Then why do your messages contain the Nabble  footer:

View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710540.html
Sent from the R help mailing list archive at Nabble.com.

I think you are confused. You should be looking at

https://stat.ethz.ch/mailman/listinfo/r-help

and using your email client to send emails rather than posting on a web page.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 29, 2015 2:23:54 PM EDT, Hidden Markov Model <hussain at touchofmodern.com> wrote:
>Hi John, 
>
>Great thanks for the examples! I am not sure what you are referring to
>when
>you say a post from Nabble - I posted this directly on R-help. I
>actually
>never heard of Nabble.
>
>Stacked Bar charts are great for when you have a lot of moving parts
>and
>need to be able to zero in on one of them (e.g. Advertising
>Placements).
>Unfortunately, I can't post the data since it is confidential. 
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710540.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From hussain at touchofmodern.com  Wed Jul 29 20:37:56 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Wed, 29 Jul 2015 11:37:56 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
Message-ID: <1438195076846-4710543.post@n4.nabble.com>

No, I'm not confused. I just posted on R help website. I don't know how to
use the email client to do anything which you are speaking of. If you would
like the posts to be made in an alternative way, then you will need to
provide clear direction. Sending someone to a help page which doesn't
reference what you are speaking about isn't actually helpful. 



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710543.html
Sent from the R help mailing list archive at Nabble.com.


From jdnewmil at dcn.davis.CA.us  Wed Jul 29 21:49:47 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Wed, 29 Jul 2015 15:49:47 -0400
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438195076846-4710543.post@n4.nabble.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
Message-ID: <2C0705F7-2E43-41A6-B032-2C5E28574316@dcn.davis.CA.us>

Since there IS NO R-help website that you can post on, you are definitely being confused by the Nabble website. This is a MAILING list that you interact with by sending emails to r-help at r-project.org, not a website forum. The trouble with Nabble is that it does confuse users, and breaks the continuity of emails that people who use it as it was intended to be used depend on.

For example, as a Nabble user you may not even be seeing the following footer that the rest of us live by:

PLEASE do read the posting guide
http://www.R-project.org/posting-guide.html and provide commented, minimal, self-contained, reproducible code.

A more accurate web rendition of R-help than Nabble presents may be seen here:

https://stat.ethz.ch/pipermail/r-help/

which is mentioned at the link that you complained did not help you.

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 29, 2015 2:37:56 PM EDT, Hidden Markov Model <hussain at touchofmodern.com> wrote:
>No, I'm not confused. I just posted on R help website. I don't know how
>to
>use the email client to do anything which you are speaking of. If you
>would
>like the posts to be made in an alternative way, then you will need to
>provide clear direction. Sending someone to a help page which doesn't
>reference what you are speaking about isn't actually helpful. 
>
>
>
>--
>View this message in context:
>http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710543.html
>Sent from the R help mailing list archive at Nabble.com.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From bhh at xs4all.nl  Wed Jul 29 21:51:44 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 29 Jul 2015 21:51:44 +0200
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438195076846-4710543.post@n4.nabble.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
Message-ID: <BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>


> On 29-07-2015, at 20:37, Hidden Markov Model <hussain at touchofmodern.com> wrote:
> 
> No, I'm not confused. I just posted on R help website.

The R help website is  NOT on Nabble. You posted from Nabble as can be seen from your message.
It can also be seen in what the internet browser displays. It is what we see in our email application.

Jeff gave you the correct advice.
Use your email client. Don?t use Nabble and your internet browser.
See the section Using R help on https://stat.ethz.ch/mailman/listinfo/r-help

Berend

> I don't know how to
> use the email client to do anything which you are speaking of. If you would
> like the posts to be made in an alternative way, then you will need to
> provide clear direction. Sending someone to a help page which doesn't
> reference what you are speaking about isn't actually helpful. 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710543.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hussain at touchofmodern.com  Wed Jul 29 22:02:25 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Wed, 29 Jul 2015 13:02:25 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
Message-ID: <1438200145867-4710549.post@n4.nabble.com>

Alright, I think I understand what you guys are talking about. It is still
not clear the relationship between R-help and Nabble since you guys haven't
actually answered that. 

Again, the best way to provide proper advice is to actually quote to portion
of the website you are sending people to, i.e., "To post a message to all
the list members, send email to r-help at r-project.org." 

But this still doesn't address the issue of why that is necessary in the
first place. Indeed, when a post is made the user sees the following
message, which seems to indicate that the method of posting doesn't actually
matter: 

"This forum is an archive/gateway which will forward your post to the
r-help at r-project.org mailing list".

Cheers! 





--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710549.html
Sent from the R help mailing list archive at Nabble.com.


From farnoosh_81 at yahoo.com  Wed Jul 29 20:45:44 2015
From: farnoosh_81 at yahoo.com (farnoosh sheikhi)
Date: Wed, 29 Jul 2015 18:45:44 +0000 (UTC)
Subject: [R] Mixed Date Formats
Message-ID: <121728020.5157918.1438195544846.JavaMail.yahoo@mail.yahoo.com>

?Hi Arun,
Hope all is well with you. I have a data with a column for date.The date format is mixed. There are date values with Month/Day/Year format and values with Day/Month/Year format.I don't know how to unify it.I really appreciate your help.Thanks.



	[[alternative HTML version deleted]]


From Stella.Xu at hli.ubc.ca  Wed Jul 29 19:14:38 2015
From: Stella.Xu at hli.ubc.ca (Stella Xu)
Date: Wed, 29 Jul 2015 10:14:38 -0700
Subject: [R] Rearrange Data Frame
Message-ID: <55B8A78E0200002100052076@mail.hli.ubc.ca>


My question is about how to select and rearrange the data to a new data
frame
Here is an example:
Samples  counts  time 
A                 10           3
A                 12           4
A                 11           3
B                 12           4
B                 10           5
C                 11           2
C                 13           3
Say, if I want to make a new table that only look at ?counts? as
below:
A        B       C
10     12     11
12     10     13
11
How can I do this in R?
Thank you!
.


From sarah.goslee at gmail.com  Wed Jul 29 22:48:43 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 29 Jul 2015 16:48:43 -0400
Subject: [R] Rearrange Data Frame
In-Reply-To: <55B8A78E0200002100052076@mail.hli.ubc.ca>
References: <55B8A78E0200002100052076@mail.hli.ubc.ca>
Message-ID: <CAM_vjume19DP=2KxHf-ej5rekvs26maJDZzyFbBV9C=5KOEYeQ@mail.gmail.com>

Hi Stella,

On Wed, Jul 29, 2015 at 1:14 PM, Stella Xu <Stella.Xu at hli.ubc.ca> wrote:
>
> My question is about how to select and rearrange the data to a new data
> frame
> Here is an example:
> Samples  counts  time
> A                 10           3
> A                 12           4
> A                 11           3
> B                 12           4
> B                 10           5
> C                 11           2
> C                 13           3
> Say, if I want to make a new table that only look at ?counts? as
> below:
> A        B       C
> 10     12     11
> 12     10     13
> 11
> How can I do this in R?
> Thank you!

Your example data doesn't use time at all, and contains a duplicate
pair of A,?,3 - what do you want to have happen there? How should
duplicates be handled? How should the ordering of values work? If
instead that should be a 5, here's something that is almost what you
want (but I find more useful):

x <- structure(list(Samples = c("A", "A", "A", "B", "B", "C", "C"),
    counts = c(10L, 12L, 11L, 12L, 10L, 11L, 13L), time = c(3L,
    4L, 5L, 4L, 5L, 2L, 3L)), .Names = c("Samples", "counts",
"time"), class = "data.frame", row.names = c(NA, -7L))

library(reshape2)
dcast(x, time ~ Samples, value.var="counts", sum)
  time  A  B  C
1    2  0  0 11
2    3 10  0 13
3    4 12 12  0
4    5 11 10  0

If you want the results "scooted up", I think there was recently a
discussion on this list on doing so.

Sarah
-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Wed Jul 29 22:50:36 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 29 Jul 2015 16:50:36 -0400
Subject: [R] Mixed Date Formats
In-Reply-To: <121728020.5157918.1438195544846.JavaMail.yahoo@mail.yahoo.com>
References: <121728020.5157918.1438195544846.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAM_vjunGHxxpQVFAp9jWMc1UwKDYzcsd=xbfyvRO4BP_SYhtRQ@mail.gmail.com>

On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
<r-help at r-project.org> wrote:
>  Hi Arun,
> Hope all is well with you. I have a data with a column for date.The date format is mixed. There are date values with Month/Day/Year format and values with Day/Month/Year format.I don't know how to unify it.I really appreciate your help.Thanks.

You sent this to the R-help list, not just to Arun, so I'm assuming
this is an R question. The best way to get help is to provide a sample
of your data using dput() and to clearly specify what you would like
as the result - "unify" is a bit vague. paste(x, collapse="") could be
considered unification, after all.

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


From sarah.goslee at gmail.com  Wed Jul 29 23:27:24 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 29 Jul 2015 17:27:24 -0400
Subject: [R] Mixed Date Formats
In-Reply-To: <1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>
References: <CAM_vjunGHxxpQVFAp9jWMc1UwKDYzcsd=xbfyvRO4BP_SYhtRQ@mail.gmail.com>
	<1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAM_vjumq0F7E_Zp=tXa9TAHt4+M_wn0-gSsPEB8xzdtV3_aLyQ@mail.gmail.com>

I'm assuming you actually want the date column to be character, not factor:
SampleData <- structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,
1375L, 2223L, 3423L), date = c("19-Dec-11", "07-Dec-11", "06/05/11",
"05/12/11", "31/12/2011", "10/19/2011", "01/22/2011")), .Names = c("id",
"value", "date"), row.names = c(NA, -7L), class = "data.frame")

  id value       date
1  1  5813  19-Dec-11
2  2  8706  07-Dec-11
3  3  4049   06/05/11
4  4  5877   05/12/11
5  5  1375 31/12/2011
6  6  2223 10/19/2011
7  7  3423 01/22/2011

Given this assemblage of dates, there is a lot of ambiguity. The first
two are clear, but is the third in May or June? Is the fourth May or
December?

I had hoped that the number of digits in the year provided a clue, but
#5 is clearly dd/mm/yyyy while #7 is mm/dd/yyyy

Or actually, #3 could be yy/mm/dd too.

I don't see a way to programmatically solve your problem, because
there is no clear way to parse all of these dates because of the
ambiguity.

You could get some of them by checking for values outside the bounds
of legitimate month numbers, or just try it and discard the values
that give NA results:
> as.Date("01/22/2011", "%m/%d/%Y")
[1] "2011-01-22"
> as.Date("01/22/2011", "%d/%m/%Y")
[1] NA

But you need more information to figure out the rest.

Sarah

On Wed, Jul 29, 2015 at 5:15 PM, farnoosh sheikhi <farnoosh_81 at yahoo.com> wrote:
> Hi Sarah,
>
> Thanks for getting back to me.
> Here is an example of my data:
> SampleData <- structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,
>                                              1375L, 2223L, 3423L), date =
> structure(c(4L, 3L, 2L, 1L, 7L,
>
> 6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",
>
> "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"
>
> ), class = "factor")), .Names = c("id", "value", "date"), row.names = c(NA,
>
> -7L), class = "data.frame")
> SampleData
>
> Thanks for your help:).
>
>
>
>
>
>
> On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
>
>
> On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
>
> <r-help at r-project.org> wrote:
>>  Hi Arun,
>> Hope all is well with you. I have a data with a column for date.The date
>> format is mixed. There are date values with Month/Day/Year format and values
>> with Day/Month/Year format.I don't know how to unify it.I really appreciate
>> your help.Thanks.
>
>
> You sent this to the R-help list, not just to Arun, so I'm assuming
> this is an R question. The best way to get help is to provide a sample
> of your data using dput() and to clearly specify what you would like
> as the result - "unify" is a bit vague. paste(x, collapse="") could be
> considered unification, after all.
>
> Sarah


From dwinsemius at comcast.net  Wed Jul 29 23:40:47 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 29 Jul 2015 14:40:47 -0700
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438200145867-4710549.post@n4.nabble.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
Message-ID: <8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>


On Jul 29, 2015, at 1:02 PM, Hidden Markov Model wrote:

> Alright, I think I understand what you guys are talking about. It is still
> not clear the relationship between R-help and Nabble since you guys haven't
> actually answered that. 

Nabble's relationship to Rhelp? Nabble is an ongoing annoyance to the regular users and to the list moderators. It's web display format deceives naive users (who usually fail to read the Posting Guide as they were directed)  into thinking everyone sees what they see, and so they often fail to maintain the context of the discussion. Many regular contributors simply ignore the content from Nabble-originated postings.

> Again, the best way to provide proper advice is to actually quote to portion
> of the website you are sending people to, i.e., "To post a message to all
> the list members, send email to r-help at r-project.org." 
> 
> But this still doesn't address the issue of why that is necessary in the
> first place. Indeed, when a post is made the user sees the following
> message, which seems to indicate that the method of posting doesn't actually
> matter: 
> 
> "This forum is an archive/gateway which will forward your post to the
> r-help at r-project.org mailing list".

Nabble lies. It is not an archive. I periodically get notices telling me that my old posts are going to be dropped. The real archive is: https://stat.ethz.ch/pipermail/r-help/

Nabble does forward messages to Rhelp and if a message passes the spam filters and the human moderators, it gets posted. Nabble then strips off the footers from the returned messages.

If you want to complain about Nabble not living up to your expectations, you should contact them. We did not ask them to mirror Rhelp.

You are expected to know how to control your own mail-client. Expecting us to educate you on the basics of computer use is unreasonable.

> View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710549.html
> 

R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

-- 
David Winsemius
Alameda, CA, USA


From farnoosh_81 at yahoo.com  Wed Jul 29 23:15:45 2015
From: farnoosh_81 at yahoo.com (farnoosh sheikhi)
Date: Wed, 29 Jul 2015 21:15:45 +0000 (UTC)
Subject: [R] Mixed Date Formats
In-Reply-To: <CAM_vjunGHxxpQVFAp9jWMc1UwKDYzcsd=xbfyvRO4BP_SYhtRQ@mail.gmail.com>
References: <CAM_vjunGHxxpQVFAp9jWMc1UwKDYzcsd=xbfyvRO4BP_SYhtRQ@mail.gmail.com>
Message-ID: <1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>

Hi Sarah,
Thanks for getting back to me.Here is an example of my data:SampleData <- structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?1375L, 2223L, 3423L), date = structure(c(4L, 3L, 2L, 1L, 7L,?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ), class = "factor")), .Names = c("id", "value", "date"), row.names = c(NA,?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? -7L), class = "data.frame")SampleData
Thanks for your help:).
?
 


     On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
   

 On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
<r-help at r-project.org> wrote:
>? Hi Arun,
> Hope all is well with you. I have a data with a column for date.The date format is mixed. There are date values with Month/Day/Year format and values with Day/Month/Year format.I don't know how to unify it.I really appreciate your help.Thanks.

You sent this to the R-help list, not just to Arun, so I'm assuming
this is an R question. The best way to get help is to provide a sample
of your data using dput() and to clearly specify what you would like
as the result - "unify" is a bit vague. paste(x, collapse="") could be
considered unification, after all.

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


  
	[[alternative HTML version deleted]]


From adam.michael.erickson at gmail.com  Wed Jul 29 23:42:23 2015
From: adam.michael.erickson at gmail.com (Adam Erickson)
Date: Wed, 29 Jul 2015 14:42:23 -0700 (PDT)
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <CAGxFJbQ7uE4_VGdmCmwavwmD8sfNFpz+KtY-DFg1KHMjUQEXaw@mail.gmail.com>
References: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>
	<108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQ7uE4_VGdmCmwavwmD8sfNFpz+KtY-DFg1KHMjUQEXaw@mail.gmail.com>
Message-ID: <08b66b82-a0a4-49b7-b7cd-bda951fb8f29@googlegroups.com>

Further refining the vectorized (within a loop) exact string match 
function, I get times below 0.9 seconds while maintaining error checking. 
This is accomplished by removing which() and replacing 1:length() with 
seq_along().

sub2 <- function(pattern, replacement, x) {
   len    <- length(x)
   y      <- character(length=len)
   patlen <- length(pattern)
   replen <- length(replacement)
   if(patlen != replen) stop('Error: Pattern and replacement length do not 
match')
   for(i in seq_along(pattern)) {
     y[x==pattern[i]] <- replacement[i]
   }
   return(y)
 }

system.time(for(i in 1:50000) sub2(patt, repl, X))
   user  system elapsed 
   0.86    0.00    0.86 

Since the ordered vectors are perfectly aligned, might as well do an exact 
string match. Hence, I think this is not off-topic.

Cheers,

Adam

On Wednesday, July 29, 2015 at 8:15:52 AM UTC-7, Bert Gunter wrote:
>
> There is confusion here. apply() family functions are **NOT** 
> vectorization -- they ARE loops (at the interpreter level), just done 
> in "functionalized" form. Please read background material (John 
> Chambers's books, MASS, or numerous others) to improve your 
> understanding and avoid posting erroneous comments. 
>
> Cheers, 
> Bert 
>
>
> Bert Gunter 
>
> "Data is not information. Information is not knowledge. And knowledge 
> is certainly not wisdom." 
>    -- Clifford Stoll 
>
>
> On Tue, Jul 28, 2015 at 3:00 PM, John Thaden <jjth... at flash.net 
> <javascript:>> wrote: 
> > Adam,    The method you propose gives a different result than the prior 
> methods for these example vectors 
> > X <- c("ab", "cd", "ef") 
> > patt <- c("b", "cd", "a") 
> > repl <- c("B", "CD", "A") 
> > 
> > Old method 1 
> > 
> > mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, 
> x=X) 
> > gives 
> >   b   cd    a 
> > "aB" "CD" "ef" 
> > 
> > Old method 2 
> > 
> > sub2 <- function(pattern, replacement, x) { 
> >     len <- length(x) 
> >     if (length(pattern) == 1) 
> >         pattern <- rep(pattern, len) 
> >     if (length(replacement) == 1) 
> >         replacement <- rep(replacement, len) 
> >     FUN <- function(i, ...) { 
> >         sub(pattern[i], replacement[i], x[i], fixed = TRUE) 
> >     } 
> >     idx <- 1:length(x) 
> >     sapply(idx, FUN) 
> > } 
> > sub2(patt, repl, X) 
> >  gives 
> > [1] "aB" "CD" "ef" 
> > 
> > Your method (I gave it the unique name "sub3") 
> >  sub3 <- function(pattern, replacement, x) {   len    <- length(x)  y   
>    <- character(length=len)  patlen <- length(pattern)  replen <- 
> length(replacement)  if(patlen != replen) stop('Error: Pattern and 
> replacement length do not match')  for(i in 1:replen) {   
>  y[which(x==pattern[i])] <- replacement[i]  }  return(y)}sub3(patt, repl, 
> X) 
> > gives[1] ""   "CD" "" 
> > 
> > Granted, whatever it does, it does it faster 
> > #Old method 1 
> > system.time(for(i in 1:50000) 
> > mapply(function(p,r,x) sub(p,r,x, fixed = TRUE),p=patt,r=repl,x=X)) 
> >    user  system elapsed 
> >    2.53    0.00    2.52 
> > 
> > #Old method 2 
> > system.time(for(i in 1:50000)sub2(patt, repl, X))   user  system elapsed 
> >    2.32    0.00    2.32 
> > 
> > #Your proposed method 
> > system.time(for(i in 1:50000) sub3(patt, repl, X)) 
> >    user  system elapsed 
> >    1.02    0.00    1.01 
> >  but would it still be faster if it actually solved the same problem? 
> > 
> > -John Thaden 
> > 
> > 
> > 
> > 
> >      On Monday, July 27, 2015 11:40 PM, Adam Erickson <
> adam.micha... at gmail.com <javascript:>> wrote: 
> > 
> > I know this is an old thread, but I wrote a simple FOR loop with 
> vectorized pattern replacement that is much faster than either of those (it 
> can also accept outputs differing in length from the patterns): 
> >   sub2  <- function(pattern, replacement, x) {     len   <- length(x)   
>  y      <- character(length=len)    patlen <- length(pattern)    replen <- 
> length(replacement)    if(patlen != replen) stop('Error: Pattern and 
> replacement length do not match')    for(i in 1:replen) {     
>  y[which(x==pattern[i])] <- replacement[i]    }    return(y)  } 
> > system.time(test <- sub2(patt, repl, XX))   user  system elapsed       0 
>       0       0 
> > Cheers, 
> > Adam 
> > On Wednesday, October 8, 2008 at 9:38:01 PM UTC-7, john wrote: 
> > Hello Christos, 
> >   To my surprise, vectorization actually hurt processing speed!#Example 
> > X <- c("ab", "cd", "ef") 
> > patt <- c("b", "cd", "a") 
> > repl <- c("B", "CD", "A")sub2 <- function(pattern, replacement, x) { 
> >     len <- length(x) 
> >     if (length(pattern) == 1) 
> >         pattern <- rep(pattern, len) 
> >     if (length(replacement) == 1) 
> >         replacement <- rep(replacement, len) 
> >     FUN <- function(i, ...) { 
> >         sub(pattern[i], replacement[i], x[i], fixed = TRUE) 
> >     } 
> >     idx <- 1:length(x) 
> >     sapply(idx, FUN) 
> > } 
> > 
> > system.time(  for(i in 1:10000)  sub2(patt, repl, X)  ) 
> >    user  system elapsed 
> >    1.18    0.07    1.26 system.time(  for(i in 1:10000) 
>  mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X) 
>  ) 
> >    user  system elapsed 
> >    1.42    0.05    1.47 
> > 
> > So much for avoiding loops. 
> > John Thaden======= At 2008-10-07, 14:58:10 Christos wrote: =======>John, 
> >>Try the following: 
> >> 
> >> mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, 
> x=X) 
> >>   b   cd    a 
> >>"aB" "CD" "ef" 
> >> 
> >>-Christos>> -----My Original Message----- 
> >>> R pattern-matching and replacement functions are 
> >>> vectorized: they can operate on vectors of targets. 
> >>> However, they can only use one pattern and replacement. 
> >>> Here is code to apply a different pattern and replacement for 
> >>> every target.  My question: can it be done better? 
> >>> 
> >>> sub2 <- function(pattern, replacement, x) { 
> >>>     len <- length(x) 
> >>>     if (length(pattern) == 1) 
> >>>         pattern <- rep(pattern, len) 
> >>>     if (length(replacement) == 1) 
> >>>         replacement <- rep(replacement, len) 
> >>>     FUN <- function(i, ...) { 
> >>>         sub(pattern[i], replacement[i], x[i], fixed = TRUE) 
> >>>     } 
> >>>     idx <- 1:length(x) 
> >>>     sapply(idx, FUN) 
> >>> } 
> >>> 
> >>> #Example 
> >>> X <- c("ab", "cd", "ef") 
> >>> patt <- c("b", "cd", "a") 
> >>> repl <- c("B", "CD", "A") 
> >>> sub2(patt, repl, X) 
> >>> 
> >>> -John______________________________________________ 
> > R-h... at r-project.org mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
> > 
> > 
> > 
> > 
> >         [[alternative HTML version deleted]] 
> > 
> > ______________________________________________ 
> > R-h... at r-project.org <javascript:> mailing list -- To UNSUBSCRIBE and 
> more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
>
> ______________________________________________ 
> R-h... at r-project.org <javascript:> mailing list -- To UNSUBSCRIBE and 
> more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 
>

From hussain at touchofmodern.com  Thu Jul 30 00:43:33 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Wed, 29 Jul 2015 15:43:33 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
	<8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
Message-ID: <1438209813434-4710560.post@n4.nabble.com>

Alright thanks for clarifying. That's all a bit esoteric. Quite different
from "basics off computer use".  None of the documentation anyone mentioned
actually contains any of this. If Nabble is so bad, then why does anyone use
it? It doesn't make sense. You should probably write a wiki going forward so
you won't get upset every-time somebody asks a reasonable question. 



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710560.html
Sent from the R help mailing list archive at Nabble.com.


From bgunter.4567 at gmail.com  Thu Jul 30 01:46:17 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 29 Jul 2015 16:46:17 -0700
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438209813434-4710560.post@n4.nabble.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
	<8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
	<1438209813434-4710560.post@n4.nabble.com>
Message-ID: <CAGxFJbTP4oFnftnVXg3su=Lzw6dWh+qkwYpeXt7JRCf32wbdTw@mail.gmail.com>

Below.

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Jul 29, 2015 at 3:43 PM, Hidden Markov Model
<hussain at touchofmodern.com> wrote:
> Alright thanks for clarifying. That's all a bit esoteric. Quite different
> from "basics off computer use".  None of the documentation anyone mentioned
> actually contains any of this. If Nabble is so bad, then why does anyone use
> it? It doesn't make sense.

**You should probably write a wiki going forward so
 you won't get upset every-time somebody asks a reasonable question.**

Fortune nomination!

(I don't actually agree, but I thought it was amusing)


>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710560.html
> Sent from the R help mailing list archive at Nabble.com.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hussain at touchofmodern.com  Thu Jul 30 01:49:34 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Wed, 29 Jul 2015 16:49:34 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <CAGxFJbTP4oFnftnVXg3su=Lzw6dWh+qkwYpeXt7JRCf32wbdTw@mail.gmail.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
	<8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
	<1438209813434-4710560.post@n4.nabble.com>
	<CAGxFJbTP4oFnftnVXg3su=Lzw6dWh+qkwYpeXt7JRCf32wbdTw@mail.gmail.com>
Message-ID: <1438213774262-4710563.post@n4.nabble.com>

Alright kids, I think we should close this topic since we appear to be
getting further and further off-topic. 



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710563.html
Sent from the R help mailing list archive at Nabble.com.


From dulcalma at bigpond.com  Thu Jul 30 04:12:34 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Thu, 30 Jul 2015 12:12:34 +1000
Subject: [R] Mixed Date Formats
In-Reply-To: <1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>
References: <CAM_vjunGHxxpQVFAp9jWMc1UwKDYzcsd=xbfyvRO4BP_SYhtRQ@mail.gmail.com>
	<1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <000901d0ca6d$334a2e90$99de8bb0$@bigpond.com>

Hi

I wonder if it is easier to convert the dates to character format and then reformat using gsub or the like

str(SampleData)
SampleData$date <- as.character(SampleData$date)
str(SampleData)

as.Date(
ifelse(nchar(SampleData[,"date"]) == 9, as.Date(SampleData[,"date"], format = "%d-%b-%y"),
       ifelse(nchar(SampleData[,"date"]) == 8, as.Date(SampleData[,"date"], format = "%d/%m/%y"),
              ifelse(as.numeric(substr(SampleData[,"date"],1,2)) > 12,
                      as.Date(SampleData[,"date"], format =  "%d/%m/%Y"),
                      as.Date(SampleData[,"date"], format =  "%m/%d/%Y")) )), origin = as.Date("1970-01-01"))

Beware of the American format in months jan feb mar oct nov -- will need more conditions to be imposed

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of farnoosh sheikhi via R-help
Sent: Thursday, 30 July 2015 07:16
To: Sarah Goslee
Cc: R. Help
Subject: Re: [R] Mixed Date Formats

Hi Sarah,
Thanks for getting back to me.Here is an example of my data:SampleData <- structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,                                              1375L, 2223L, 3423L), date = structure(c(4L, 3L, 2L, 1L, 7L,                                                                                       6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",                                                                                                           "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"                                                                                      ), class = "factor")), .Names = c("id", "value", "date"), row.names = c(NA,                                                                                                                                                               -7L), class = "data.frame")SampleData
Thanks for your help:).
 
 


     On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
   

 On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
<r-help at r-project.org> wrote:
>  Hi Arun,
> Hope all is well with you. I have a data with a column for date.The date format is mixed. There are date values with Month/Day/Year format and values with Day/Month/Year format.I don't know how to unify it.I really appreciate your help.Thanks.

You sent this to the R-help list, not just to Arun, so I'm assuming
this is an R question. The best way to get help is to provide a sample
of your data using dput() and to clearly specify what you would like
as the result - "unify" is a bit vague. paste(x, collapse="") could be
considered unification, after all.

Sarah

-- 
Sarah Goslee
http://www.functionaldiversity.org


  
	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jonsleepy at gmail.com  Thu Jul 30 04:37:56 2015
From: jonsleepy at gmail.com (Jon BR)
Date: Wed, 29 Jul 2015 22:37:56 -0400
Subject: [R] dplyr help
Message-ID: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>

Hello,
    I've recently discovered the helpful dplyr package.  I'm using the
'aggregate' function as such:


bevs <- data.frame(cbind(name = c("Bill", "Mary"), drink = c("coffee",
"tea", "cocoa", "water"), cost = seq(1:8), sex = c("male","female")));
bevs$cost <- seq(1:8)

> bevs
  name  drink cost    sex
1 Bill coffee    1   male
2 Mary    tea    2 female
3 Bill  cocoa    3   male
4 Mary  water    4 female
5 Bill coffee    5   male
6 Mary    tea    6 female
7 Bill  cocoa    7   male
8 Mary  water    8 female
>

> aggregate(cost ~ name + drink, data = bevs, sum)
  name  drink cost
1 Bill  cocoa   10
2 Bill coffee    6
3 Mary    tea    8
4 Mary  water   12

My issue is that I would like to keep a column for 'sex', for which there
is a 1:1 mapping with 'name', such that every time 'Bill' appears, it is
always 'male'.

Does anyone know of a way to accomplish this, with or without dplyr?  The
ideal command(s) would produce this:

  name  drink cost sex
1 Bill  cocoa   10   male
2 Bill coffee    6   male
3 Mary    tea    8   female
4 Mary  water   12   female

I would be thankful for any suggestion!

Thanks,
Jonathan

	[[alternative HTML version deleted]]


From glennmschultz at me.com  Thu Jul 30 03:35:24 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Thu, 30 Jul 2015 01:35:24 +0000 (GMT)
Subject: [R] Vignette using knitr, devtools, and R markdown
Message-ID: <ecfcdf79-e83b-4760-92c8-dcc2c00db647@me.com>

Hi All,
I am writing a Vignette with Knitr using devtools ... actually quite awesome. ?However, I would like to create a table of contents. ?I have read the knitr and r markdown instructions on TOC and I can create a TOC in a markdown document but I cannot translate the command to a vignette - maybe doing something wrong. ?Does anyone know if I can create a TOC in a vignette using r-markdown and devtools?

Glenn ?

From jonsleepy at gmail.com  Thu Jul 30 05:11:51 2015
From: jonsleepy at gmail.com (Jon BR)
Date: Wed, 29 Jul 2015 23:11:51 -0400
Subject: [R] dplyr help
In-Reply-To: <CAN7h_v3EYvP9u+kPFZr8A6_S06ip+prJXLYU=KZX_5_yJB3QTw@mail.gmail.com>
References: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>
	<CAN7h_v3EYvP9u+kPFZr8A6_S06ip+prJXLYU=KZX_5_yJB3QTw@mail.gmail.com>
Message-ID: <CA+d7zeT4H_iDNJR=FpObtH-rRFUiVw6BOjR600zNyqiixq=5bA@mail.gmail.com>

Hi Brian,
    Thanks for the suggestion, although the command is throwing an error as
such:

> bevs %>% group_by(name, sex, drink) %>% summarise( cost = sum(cost)) %>%
select(name, drink, cost, sex)
Error: unexpected input in "bevs %>% group_by(name, sex, drink) %>%
summarise( "

Your syntax is new to me so I'm not immediately clear on how to fix it; any
idea how?

Thanks again,
Jonathan


On Wed, Jul 29, 2015 at 11:07 PM, Brian Kreeger <brian.kreeger at gmail.com>
wrote:

> ?dplyr solution:
>
> bevs %>% group_by(name, sex, drink) %>% summarise(?cost = sum(cost)) %>%
> select(name, drink, cost, sex)
>
> The last select statement puts the output in the column order you wanted
> in your result.
>
> I hope this helps.
>
> Brian
>
>
>
> On Wed, Jul 29, 2015 at 9:37 PM, Jon BR <jonsleepy at gmail.com> wrote:
>
>> Hello,
>>     I've recently discovered the helpful dplyr package.  I'm using the
>> 'aggregate' function as such:
>>
>>
>> bevs <- data.frame(cbind(name = c("Bill", "Mary"), drink = c("coffee",
>> "tea", "cocoa", "water"), cost = seq(1:8), sex = c("male","female")));
>> bevs$cost <- seq(1:8)
>>
>> > bevs
>>   name  drink cost    sex
>> 1 Bill coffee    1   male
>> 2 Mary    tea    2 female
>> 3 Bill  cocoa    3   male
>> 4 Mary  water    4 female
>> 5 Bill coffee    5   male
>> 6 Mary    tea    6 female
>> 7 Bill  cocoa    7   male
>> 8 Mary  water    8 female
>> >
>>
>> > aggregate(cost ~ name + drink, data = bevs, sum)
>>   name  drink cost
>> 1 Bill  cocoa   10
>> 2 Bill coffee    6
>> 3 Mary    tea    8
>> 4 Mary  water   12
>>
>> My issue is that I would like to keep a column for 'sex', for which there
>> is a 1:1 mapping with 'name', such that every time 'Bill' appears, it is
>> always 'male'.
>>
>> Does anyone know of a way to accomplish this, with or without dplyr?  The
>> ideal command(s) would produce this:
>>
>>   name  drink cost sex
>> 1 Bill  cocoa   10   male
>> 2 Bill coffee    6   male
>> 3 Mary    tea    8   female
>> 4 Mary  water   12   female
>>
>> I would be thankful for any suggestion!
>>
>> Thanks,
>> Jonathan
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Jul 30 05:14:03 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 29 Jul 2015 20:14:03 -0700
Subject: [R] dplyr help
In-Reply-To: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>
References: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>
Message-ID: <0E000CB1-C771-4DC5-B19B-B5C823FC3AE5@comcast.net>


On Jul 29, 2015, at 7:37 PM, Jon BR wrote:

> Hello,
>    I've recently discovered the helpful dplyr package.  I'm using the
> 'aggregate' function as such: 

The `aggregate` function is part of base-R:

> bevs <- data.frame(cbind(name = c("Bill", "Mary"), drink = c("coffee",
> "tea", "cocoa", "water"), cost = seq(1:8), sex = c("male","female")));
> bevs$cost <- seq(1:8)
> 
>> bevs
>  name  drink cost    sex
> 1 Bill coffee    1   male
> 2 Mary    tea    2 female
> 3 Bill  cocoa    3   male
> 4 Mary  water    4 female
> 5 Bill coffee    5   male
> 6 Mary    tea    6 female
> 7 Bill  cocoa    7   male
> 8 Mary  water    8 female
>> 
> 
>> aggregate(cost ~ name + drink, data = bevs, sum)
>  name  drink cost
> 1 Bill  cocoa   10
> 2 Bill coffee    6
> 3 Mary    tea    8
> 4 Mary  water   12
> 
> My issue is that I would like to keep a column for 'sex', for which there
> is a 1:1 mapping with 'name', such that every time 'Bill' appears, it is
> always 'male'.
> 
> Does anyone know of a way to accomplish this, with or without dplyr?

As pointed out you have not yet demonstrated any dplyr functions.

> The
> ideal command(s) would produce this:
> 
>  name  drink cost sex
> 1 Bill  cocoa   10   male
> 2 Bill coffee    6   male
> 3 Mary    tea    8   female
> 4 Mary  water   12   female

Doesn't this (glaringly obvious?) approach succeed?

> aggregate(cost ~ name + drink+sex, data = bevs, sum)
  name  drink    sex cost
1 Mary    tea female    8
2 Mary  water female   12
3 Bill  cocoa   male   10
4 Bill coffee   male    6
> 


> 
> I would be thankful for any suggestion!
> 
> Thanks,
> Jonathan
> 
> 	[[alternative HTML version deleted]]
> 
> 

Please learn to post in plain text.

-- 

David Winsemius
Alameda, CA, USA


From jonsleepy at gmail.com  Thu Jul 30 05:16:17 2015
From: jonsleepy at gmail.com (Jon BR)
Date: Wed, 29 Jul 2015 23:16:17 -0400
Subject: [R] dplyr help
In-Reply-To: <0E000CB1-C771-4DC5-B19B-B5C823FC3AE5@comcast.net>
References: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>
	<0E000CB1-C771-4DC5-B19B-B5C823FC3AE5@comcast.net>
Message-ID: <CA+d7zeTPLD45ocmf9QifG-kBX63OzacuVsMUf_sjkkopVnKO5w@mail.gmail.com>

David,
    I do appreciate your help, if not the dose of contempt.  I hope you
feel OK.

Thanks for the tips,
-Jonathan

On Wed, Jul 29, 2015 at 11:14 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On Jul 29, 2015, at 7:37 PM, Jon BR wrote:
>
> > Hello,
> >    I've recently discovered the helpful dplyr package.  I'm using the
> > 'aggregate' function as such:
>
> The `aggregate` function is part of base-R:
>
> > bevs <- data.frame(cbind(name = c("Bill", "Mary"), drink = c("coffee",
> > "tea", "cocoa", "water"), cost = seq(1:8), sex = c("male","female")));
> > bevs$cost <- seq(1:8)
> >
> >> bevs
> >  name  drink cost    sex
> > 1 Bill coffee    1   male
> > 2 Mary    tea    2 female
> > 3 Bill  cocoa    3   male
> > 4 Mary  water    4 female
> > 5 Bill coffee    5   male
> > 6 Mary    tea    6 female
> > 7 Bill  cocoa    7   male
> > 8 Mary  water    8 female
> >>
> >
> >> aggregate(cost ~ name + drink, data = bevs, sum)
> >  name  drink cost
> > 1 Bill  cocoa   10
> > 2 Bill coffee    6
> > 3 Mary    tea    8
> > 4 Mary  water   12
> >
> > My issue is that I would like to keep a column for 'sex', for which there
> > is a 1:1 mapping with 'name', such that every time 'Bill' appears, it is
> > always 'male'.
> >
> > Does anyone know of a way to accomplish this, with or without dplyr?
>
> As pointed out you have not yet demonstrated any dplyr functions.
>
> > The
> > ideal command(s) would produce this:
> >
> >  name  drink cost sex
> > 1 Bill  cocoa   10   male
> > 2 Bill coffee    6   male
> > 3 Mary    tea    8   female
> > 4 Mary  water   12   female
>
> Doesn't this (glaringly obvious?) approach succeed?
>
> > aggregate(cost ~ name + drink+sex, data = bevs, sum)
>   name  drink    sex cost
> 1 Mary    tea female    8
> 2 Mary  water female   12
> 3 Bill  cocoa   male   10
> 4 Bill coffee   male    6
> >
>
>
> >
> > I would be thankful for any suggestion!
> >
> > Thanks,
> > Jonathan
> >
> >       [[alternative HTML version deleted]]
> >
> >
>
> Please learn to post in plain text.
>
> --
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From brian.kreeger at gmail.com  Thu Jul 30 05:07:11 2015
From: brian.kreeger at gmail.com (Brian Kreeger)
Date: Wed, 29 Jul 2015 22:07:11 -0500
Subject: [R] dplyr help
In-Reply-To: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>
References: <CA+d7zeRUW1CV4O7Rr1mL80u=9A-MpN7YHZBZXmW75piki3WXjg@mail.gmail.com>
Message-ID: <CAN7h_v3EYvP9u+kPFZr8A6_S06ip+prJXLYU=KZX_5_yJB3QTw@mail.gmail.com>

?dplyr solution:

bevs %>% group_by(name, sex, drink) %>% summarise(?cost = sum(cost)) %>%
select(name, drink, cost, sex)

The last select statement puts the output in the column order you wanted in
your result.

I hope this helps.

Brian



On Wed, Jul 29, 2015 at 9:37 PM, Jon BR <jonsleepy at gmail.com> wrote:

> Hello,
>     I've recently discovered the helpful dplyr package.  I'm using the
> 'aggregate' function as such:
>
>
> bevs <- data.frame(cbind(name = c("Bill", "Mary"), drink = c("coffee",
> "tea", "cocoa", "water"), cost = seq(1:8), sex = c("male","female")));
> bevs$cost <- seq(1:8)
>
> > bevs
>   name  drink cost    sex
> 1 Bill coffee    1   male
> 2 Mary    tea    2 female
> 3 Bill  cocoa    3   male
> 4 Mary  water    4 female
> 5 Bill coffee    5   male
> 6 Mary    tea    6 female
> 7 Bill  cocoa    7   male
> 8 Mary  water    8 female
> >
>
> > aggregate(cost ~ name + drink, data = bevs, sum)
>   name  drink cost
> 1 Bill  cocoa   10
> 2 Bill coffee    6
> 3 Mary    tea    8
> 4 Mary  water   12
>
> My issue is that I would like to keep a column for 'sex', for which there
> is a 1:1 mapping with 'name', such that every time 'Bill' appears, it is
> always 'male'.
>
> Does anyone know of a way to accomplish this, with or without dplyr?  The
> ideal command(s) would produce this:
>
>   name  drink cost sex
> 1 Bill  cocoa   10   male
> 2 Bill coffee    6   male
> 3 Mary    tea    8   female
> 4 Mary  water   12   female
>
> I would be thankful for any suggestion!
>
> Thanks,
> Jonathan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From anshuk.p at motivitylabs.com  Thu Jul 30 07:23:49 2015
From: anshuk.p at motivitylabs.com (Anshuk Pal Chaudhuri)
Date: Thu, 30 Jul 2015 05:23:49 +0000
Subject: [R] R Parse HTML tabular data and apply NLP
Message-ID: <HKXPR02MB0632723F638A04AC781460D6F08B0@HKXPR02MB0632.apcprd02.prod.outlook.com>

Hi All,

I have quite a few files which is having HTML tabular data. All the files have have different format, numerous nested tables and different information and the table structure is completely different. The only common thing in these files is that they are in tables.

I was able to read the table using the readHTMLTable function. e.g one file has 23 tables, able to put all data one data frame. Obviously, the read function is not able to interpret the header obviously (which is also it not supposed to), hence creating creating variables like V1, V2..

Now when I have got all the text into a dataframe (the data is scattered in different columns), how do I interpret the text using machine learning to train that this text (sentence,word..)means this, or this text means this. Basically, automatic categorization of the all the text in the dataframe.

I was reading about RTextTools (http://www.rtexttools.com/), well in that case it has be told that this value is for this text and hence further...

Any help would be appreciated.

Regards,
Anshuk Pal Chaudhuri


	[[alternative HTML version deleted]]


From amelia_marsh08 at yahoo.com  Thu Jul 30 08:56:46 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Thu, 30 Jul 2015 06:56:46 +0000 (UTC)
Subject: [R] Bootstrap library
Message-ID: <387756433.3477108.1438239406192.JavaMail.yahoo@mail.yahoo.com>

Hello!

I have following data and I am trying to apply bootstrapping. My data and code is as follows-

amounts = c(829.53,4000,6000,1000,1063904,102400,22000,4000,4200,2000,10000,400,459006,7276,4000,100,4000,10000,613803.36, 3825,1000,5000,4000,3000,84500,200,2000,68000,97400,6267.8,49500,27000,2100,10489.92,2200,2000,2000,1000,1900,6000,5600,100,4000,14300,100,94100,1200,7000,2000,3000,1100,6900,1000,18500,6000,2000,4000,8400,11200,1000,15100,23300,4000,13100,4500,200,2000,50000,3900,3200,2000,2000,67000,2000,500,2000,1000,1900,10400,1900,2000,3200,6500,10000,2900,1000,14300,1000,2700,1500,12000,40000,25000,2800,5000,15000,4000,1000,21000,15000,16000,54000,1500,19200,2000,2000,1000,39000,5000,1100,18000,10000,3500,1000,10000,5000,14000,1800,4000,1000,300,4000,1000,100,1000,4400,2000,2000,12000,200,100,1000,1000,2000,1600,2000,4000,14000,4000,13500,1000,200,200,1000,18000,23000,41400,60000,500,3000,21000,6900,14600,1900,4000,4500,1000,2000,2000,1000,4100,2000,1000,2000,8000,3000,1500,2000,2000,3500,2000,2000,1000,3800,30000,55000,500,1000,1000,2000,62400,2000,3000,200,2000,3500,2000,2000,500,3000,4500,1000,10000,2000,3000,3600,1000,2000,2000,5000,23000,2000,1900,2000,60000,2000,60000,20000,2000,2000,4600,1000,2000,1000,18000,6000,62000,68000,26800,50000,45900,16900,21500,2000,22700,2000,2000,32000,10000,5000,138000,159700,13000,2000,17619,2000,1000,4000,2000,1500,4000,20000,158900,74100,6000,24900,60000,500,1000,40000,10000,50000,800,4000,4900,6500,5000,400,500,3000,32300,24000,300,11500,2000,5000,1000,500,5000,5500,17450,56800,2000,1000,21400,22000,60000,3000,7500,3000,1000,1000,2000,1500,83700,2000,4000,170005,70000,6700,1500,3500,2000,10563.97,1500,25000,2000,2000,2267.57,1100,3100,2000,3500,10000,2000,6000,1500,200,20000,4000,46400,296900,150000,3700,7500,20000,48500,3500,12000,2500,4000,8500,1000,14500,1000,11000,2000,2000,120000,20000,7600,3000,2000,8000,1600,40000,2000,5000,34187.67,279100,9900,31300,814000,43500,5100,49500,4500,6262.38,100,10400,2400,1500,5000,2500,15000,40000,32500,41100,358600,109600,514300,258200,225900,402700,274300,75000,1000,56000,10000,4100,1000,15000,100,40000,7900,5000,105000,15100,2000,1100,2900,1500,600,500,1300,100,5000,5000,10000,10100,7000,40000,10500,5000,9500,1000,15200,2000,10000,10000,100,7800,3500,189900,58000,345000,151700,11000,6000,7000,15700,6000,3000,5000,10000,2000,1000,36000,1000,500,8000,9000,6000,2000,26500,6000,5000,97200,2000,5100,17000,2500,25500,24000,5400,90000,41500,6200,7500,5000,7000,41000,25000,1500,40000,5000,10000,21500,100,32000,32500,70000,500,66400,21000,5000,5000,12600,3000,6200,38900,10000,1000,60000,41100,1200,31300,2500,58000,4100,58000,42500) 

library(bootstrap) 

per = 0.999 

theta <- function(amounts){ 
OpVaR_sample <- quantile(sort(amounts), probs = per) 
} 
(results <- bootstrap(amounts,10,theta)) 

(OPVaR <- mean(results$thetastar)) 
 
# ______________________________________________________________________________________

MY QUERY

(1) How do I find out what was the sample size selected during bootstrapping the original data of 472 observations?

(2) I have taken 10 samples (for illustration purpose). Is there any way to find out the observations constituting each sample?

Here, as per the above code, I am getting the direct results w.r.t. the samples.

Thanking in advance.


Regards

Amelia


From j.para.fernandez at hotmail.com  Thu Jul 30 09:33:17 2015
From: j.para.fernandez at hotmail.com (jpara3)
Date: Thu, 30 Jul 2015 00:33:17 -0700 (PDT)
Subject: [R] Imbalanced random forest
In-Reply-To: <CAJ9CoWkY3XHZp4pvi-cwst3v46AzUeTK97TtiJfrMB=rT5v5XA@mail.gmail.com>
References: <1438182045051-4710524.post@n4.nabble.com>
	<CAJ9CoWkY3XHZp4pvi-cwst3v46AzUeTK97TtiJfrMB=rT5v5XA@mail.gmail.com>
Message-ID: <1438241597550-4710573.post@n4.nabble.com>

Thanks for the info!



-----

Guided Tours Basque Country 

Guided tours in the three capitals of the Basque Country: Bilbao, Vitoria-Gasteiz and San Sebastian, as well as in their provinces. Available languages.

Travel planners for groups and design of tourist routes across the Basque Country.
--
View this message in context: http://r.789695.n4.nabble.com/Imbalanced-random-forest-tp4710524p4710573.html
Sent from the R help mailing list archive at Nabble.com.


From peter.humburg at gmail.com  Thu Jul 30 09:53:20 2015
From: peter.humburg at gmail.com (Peter Humburg)
Date: Thu, 30 Jul 2015 07:53:20 +0000
Subject: [R] Vignette using knitr, devtools, and R markdown
In-Reply-To: <ecfcdf79-e83b-4760-92c8-dcc2c00db647@me.com>
References: <ecfcdf79-e83b-4760-92c8-dcc2c00db647@me.com>
Message-ID: <CABy5_1_wzrje8cYDR=MmuKNof9kSBy=b-8tkF7e3Hq5Pryud2Q@mail.gmail.com>

Hi Glenn,

You should be able to add "toc: true" as an option to the yaml meta data
block at the beginning of your vignette. If that isn't working for you you
may have to give some more detail on what exactly you are doing and how it
is failing.

Peter

On Thu, 30 Jul 2015 at 04:08 Glenn Schultz <glennmschultz at me.com> wrote:

> Hi All,
> I am writing a Vignette with Knitr using devtools ... actually quite
> awesome.  However, I would like to create a table of contents.  I have read
> the knitr and r markdown instructions on TOC and I can create a TOC in a
> markdown document but I cannot translate the command to a vignette - maybe
> doing something wrong.  Does anyone know if I can create a TOC in a
> vignette using r-markdown and devtools?
>
> Glenn
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ntfredo at gmail.com  Thu Jul 30 10:41:24 2015
From: ntfredo at gmail.com (Frederic Ntirenganya)
Date: Thu, 30 Jul 2015 11:41:24 +0300
Subject: [R] Removing display of R row names from list.
In-Reply-To: <CAGxFJbQtLsQ8X4k9a5YmX9rFgR6+9zvgGrLXU+2shTfom__5qQ@mail.gmail.com>
References: <cagh51gtwfnkiegqme3lb6y=jv+jfk-_ls_nrx7rm9e-ihvb1yw@mail.gmail.com>
	<7dcaabce14e.000010abjrkrideau@inbox.com>
	<CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>
	<7E7EA688B68.000011BCjrkrideau@inbox.com>
	<CAGxFJbQtLsQ8X4k9a5YmX9rFgR6+9zvgGrLXU+2shTfom__5qQ@mail.gmail.com>
Message-ID: <CAGh51gRuh5+R7=C8_sC3919eXjEUQk-E4yaNUUL=L1+bfTyWkw@mail.gmail.com>

I want to remove the rownames after incrementation. Thanks

Frederic Ntirenganya
Maseno University,
African Maths Initiative,
Kenya.
Mobile:(+254)718492836
Email: fredo at aims.ac.za
https://sites.google.com/a/aims.ac.za/fredo/

On Wed, Jul 29, 2015 at 7:04 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Is this what you mean?
>
> z <- data.frame(a=1:3,b=letters[1:3],row.names=letters[1:3])
> zlist <- list(one=z,too =z)
>
> for(i in 1:2){print(zlist[[i]],row.names=FALSE); cat("\n")}
>
>  a b
>  1 a
>  2 b
>  3 c
>
>  a b
>  1 a
>  2 b
>  3 c
>
> ... which could obviously be within a function.
>
> Cheers,
> Bert
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Wed, Jul 29, 2015 at 8:32 AM, John Kane <jrkrideau at inbox.com> wrote:
> > Beats me.  You can print a single data frame with
> > print(dat1, rownames = FALSE) but it is not clear to me how to do it
> within a function.
> >
> > I am sure someone who actually know what they are doing will be along in
> a moment. Sorry not to have been of more help.
> >
> > John Kane
> > Kingston ON Canada
> >
> > -----Original Message-----
> > From: ntfredo at gmail.com
> > Sent: Wed, 29 Jul 2015 18:11:36 +0300
> > To: jrkrideau at inbox.com
> > Subject: Re: [R] Removing display of R row names from list.
> >
> > Here is a small example. it is not the real data I am working on but
> this can help
> >
> > #Chick weight example
> >
> > names(ChickWeight) <- tolower(names(ChickWeight))
> > chick_m <- melt(ChickWeight, id=2:4, na.rm=TRUE)
> > class(chick_m)
> >
> > test = function(data){
> >   i = 1
> >   table = list()
> >   table [[i]] <- dcast(chick_m, time ~ variable, mean)
> >   i = i + 1
> >
> >   print(table)
> >
> > }
> > test(chick_m )
> >
> > Frederic Ntirenganya
> > Maseno University,
> >
> > African Maths Initiative,
> >
> > Kenya.
> >
> > Mobile:(+254)718492836
> >
> > Email: fredo at aims.ac.za
> >
> > https://sites.google.com/a/aims.ac.za/fredo/ [
> https://sites.google.com/a/aims.ac.za/fredo/]
> >
> > On Wed, Jul 29, 2015 at 5:12 PM, John Kane <jrkrideau at inbox.com> wrote:
> >
> >         HI Frederic,
> >  Can you supply a small example of the problem?
> >
> >  John Kane
> >  Kingston ON Canada
> >
> >  > -----Original Message-----
> >  > From: ntfredo at gmail.com
> >  > Sent: Wed, 29 Jul 2015 14:15:58 +0300
> >  > To: r-help at r-project.org
> >  > Subject: [R] Removing display of R row names from list.
> >  >
> >  >  Dear All,
> >  >
> >  > Is there a way to not include the row names when creating the list?
> >  > Thanks!
> >  >
> >  > Regards,
> >  > Frederic.
> >  >
> >  > Frederic Ntirenganya
> >  > Maseno University,
> >  > African Maths Initiative,
> >  > Kenya.
> >  > Mobile:(+254)718492836
> >  > Email: fredo at aims.ac.za
> >  > https://sites.google.com/a/aims.ac.za/fredo/ [
> https://sites.google.com/a/aims.ac.za/fredo/]
> >  >
> >  >       [[alternative HTML version deleted]]
> >  >
> >  > ______________________________________________
> >  > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >  > https://stat.ethz.ch/mailman/listinfo/r-help [
> https://stat.ethz.ch/mailman/listinfo/r-help]
> >  > PLEASE do read the posting guide
> >  > http://www.R-project.org/posting-guide.html [
> http://www.R-project.org/posting-guide.html]
> >  > and provide commented, minimal, self-contained, reproducible code.
> >
> >  ____________________________________________________________
> >  FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> >  Check it out at http://www.inbox.com/earth [http://www.inbox.com/earth]
> >
> > ____________________________________________________________
> > FREE ONLINE PHOTOSHARING - Share your photos online with your friends
> and family!
> > Visit http://www.inbox.com/photosharing to find out more!
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Thu Jul 30 10:55:22 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 30 Jul 2015 18:55:22 +1000
Subject: [R] Rearrange Data Frame
In-Reply-To: <CAM_vjume19DP=2KxHf-ej5rekvs26maJDZzyFbBV9C=5KOEYeQ@mail.gmail.com>
References: <55B8A78E0200002100052076@mail.hli.ubc.ca>
	<CAM_vjume19DP=2KxHf-ej5rekvs26maJDZzyFbBV9C=5KOEYeQ@mail.gmail.com>
Message-ID: <CA+8X3fWB24uAb=AJypRZgmNMWR8wB0Gm4QgKXujUq9WCwgYW5w@mail.gmail.com>

Hi Stella,
I think Sarah is correct in asking if that is what you really want,
but you can get a list similar to what you asked for like this:

sample_names<-unique(sx.df$Samples)
sx.lst<-list()
for(sn in 1:length(sample_names))
 sx.lst[[sn]]<-sx.df$counts[sx.df$Samples==sample_names[sn]]
names(sx.lst)<-sample_names

If you really want a data frame, you will have to coerce the number of
values in each element of the list to the same length:

maxlen<-max(unlist(lapply(sx.lst,length)))
for(i in 1:length(sx.lst)) {
 to_fill<-maxlen-length(sx.lst[[i]])
 if(to_fill > 0) sx.lst[[i]]<-c(sx.lst[[i]],rep(NA,to_fill))
}
sx2.df<-as.data.frame(sx.lst)

Jim


On Thu, Jul 30, 2015 at 6:48 AM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Hi Stella,
>
> On Wed, Jul 29, 2015 at 1:14 PM, Stella Xu <Stella.Xu at hli.ubc.ca> wrote:
>>
>> My question is about how to select and rearrange the data to a new data
>> frame
>> Here is an example:
>> Samples  counts  time
>> A                 10           3
>> A                 12           4
>> A                 11           3
>> B                 12           4
>> B                 10           5
>> C                 11           2
>> C                 13           3
>> Say, if I want to make a new table that only look at ?counts? as
>> below:
>> A        B       C
>> 10     12     11
>> 12     10     13
>> 11
>> How can I do this in R?
>> Thank you!
>
> Your example data doesn't use time at all, and contains a duplicate
> pair of A,?,3 - what do you want to have happen there? How should
> duplicates be handled? How should the ordering of values work? If
> instead that should be a 5, here's something that is almost what you
> want (but I find more useful):
>
> x <- structure(list(Samples = c("A", "A", "A", "B", "B", "C", "C"),
>     counts = c(10L, 12L, 11L, 12L, 10L, 11L, 13L), time = c(3L,
>     4L, 5L, 4L, 5L, 2L, 3L)), .Names = c("Samples", "counts",
> "time"), class = "data.frame", row.names = c(NA, -7L))
>
> library(reshape2)
> dcast(x, time ~ Samples, value.var="counts", sum)
>   time  A  B  C
> 1    2  0  0 11
> 2    3 10  0 13
> 3    4 12 12  0
> 4    5 11 10  0
>
> If you want the results "scooted up", I think there was recently a
> discussion on this list on doing so.
>
> Sarah
> --
> Sarah Goslee
> http://www.functionaldiversity.org
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Thu Jul 30 11:23:27 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 30 Jul 2015 09:23:27 +0000
Subject: [R] Removing display of R row names from list.
In-Reply-To: <CAGh51gRuh5+R7=C8_sC3919eXjEUQk-E4yaNUUL=L1+bfTyWkw@mail.gmail.com>
References: <cagh51gtwfnkiegqme3lb6y=jv+jfk-_ls_nrx7rm9e-ihvb1yw@mail.gmail.com>
	<7dcaabce14e.000010abjrkrideau@inbox.com>
	<CAGh51gRZkuQa5eiu8xwua-Ggux7sxJKAQW=ez3=4H__evfFV4A@mail.gmail.com>
	<7E7EA688B68.000011BCjrkrideau@inbox.com>
	<CAGxFJbQtLsQ8X4k9a5YmX9rFgR6+9zvgGrLXU+2shTfom__5qQ@mail.gmail.com>
	<CAGh51gRuh5+R7=C8_sC3919eXjEUQk-E4yaNUUL=L1+bfTyWkw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C38407@SRVEXCHMBX.precheza.cz>

Hi

AFAIK you can not **remove** row names from data frame. You can suppress their printingas other adviced or you can rename row names.

> lev
  animals animalYears ind
1    bird           1   2
2     cat           1   2
3     dog           1   2
4    bird           2   2
5     cat           2   2
6     dog           2   2
> rownames(lev)<-11:16
> lev
   animals animalYears ind
11    bird           1   2
12     cat           1   2
13     dog           1   2
14    bird           2   2
15     cat           2   2
16     dog           2   2
>

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Frederic Ntirenganya
> Sent: Thursday, July 30, 2015 10:41 AM
> To: Bert Gunter
> Cc: r-help at r-project.org
> Subject: Re: [R] Removing display of R row names from list.
>
> I want to remove the rownames after incrementation. Thanks
>
> Frederic Ntirenganya
> Maseno University,
> African Maths Initiative,
> Kenya.
> Mobile:(+254)718492836
> Email: fredo at aims.ac.za
> https://sites.google.com/a/aims.ac.za/fredo/
>
> On Wed, Jul 29, 2015 at 7:04 PM, Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
> > Is this what you mean?
> >
> > z <- data.frame(a=1:3,b=letters[1:3],row.names=letters[1:3])
> > zlist <- list(one=z,too =z)
> >
> > for(i in 1:2){print(zlist[[i]],row.names=FALSE); cat("\n")}
> >
> >  a b
> >  1 a
> >  2 b
> >  3 c
> >
> >  a b
> >  1 a
> >  2 b
> >  3 c
> >
> > ... which could obviously be within a function.
> >
> > Cheers,
> > Bert
> > Bert Gunter
> >
> > "Data is not information. Information is not knowledge. And knowledge
> > is certainly not wisdom."
> >    -- Clifford Stoll
> >
> >
> > On Wed, Jul 29, 2015 at 8:32 AM, John Kane <jrkrideau at inbox.com>
> wrote:
> > > Beats me.  You can print a single data frame with print(dat1,
> > > rownames = FALSE) but it is not clear to me how to do it
> > within a function.
> > >
> > > I am sure someone who actually know what they are doing will be
> > > along in
> > a moment. Sorry not to have been of more help.
> > >
> > > John Kane
> > > Kingston ON Canada
> > >
> > > -----Original Message-----
> > > From: ntfredo at gmail.com
> > > Sent: Wed, 29 Jul 2015 18:11:36 +0300
> > > To: jrkrideau at inbox.com
> > > Subject: Re: [R] Removing display of R row names from list.
> > >
> > > Here is a small example. it is not the real data I am working on
> but
> > this can help
> > >
> > > #Chick weight example
> > >
> > > names(ChickWeight) <- tolower(names(ChickWeight)) chick_m <-
> > > melt(ChickWeight, id=2:4, na.rm=TRUE)
> > > class(chick_m)
> > >
> > > test = function(data){
> > >   i = 1
> > >   table = list()
> > >   table [[i]] <- dcast(chick_m, time ~ variable, mean)
> > >   i = i + 1
> > >
> > >   print(table)
> > >
> > > }
> > > test(chick_m )
> > >
> > > Frederic Ntirenganya
> > > Maseno University,
> > >
> > > African Maths Initiative,
> > >
> > > Kenya.
> > >
> > > Mobile:(+254)718492836
> > >
> > > Email: fredo at aims.ac.za
> > >
> > > https://sites.google.com/a/aims.ac.za/fredo/ [
> > https://sites.google.com/a/aims.ac.za/fredo/]
> > >
> > > On Wed, Jul 29, 2015 at 5:12 PM, John Kane <jrkrideau at inbox.com>
> wrote:
> > >
> > >         HI Frederic,
> > >  Can you supply a small example of the problem?
> > >
> > >  John Kane
> > >  Kingston ON Canada
> > >
> > >  > -----Original Message-----
> > >  > From: ntfredo at gmail.com
> > >  > Sent: Wed, 29 Jul 2015 14:15:58 +0300  > To: r-help at r-
> project.org
> > > > Subject: [R] Removing display of R row names from list.
> > >  >
> > >  >  Dear All,
> > >  >
> > >  > Is there a way to not include the row names when creating the
> list?
> > >  > Thanks!
> > >  >
> > >  > Regards,
> > >  > Frederic.
> > >  >
> > >  > Frederic Ntirenganya
> > >  > Maseno University,
> > >  > African Maths Initiative,
> > >  > Kenya.
> > >  > Mobile:(+254)718492836
> > >  > Email: fredo at aims.ac.za
> > >  > https://sites.google.com/a/aims.ac.za/fredo/ [
> > https://sites.google.com/a/aims.ac.za/fredo/]
> > >  >
> > >  >       [[alternative HTML version deleted]]
> > >  >
> > >  > ______________________________________________
> > >  > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help [
> > https://stat.ethz.ch/mailman/listinfo/r-help]
> > >  > PLEASE do read the posting guide
> > >  > http://www.R-project.org/posting-guide.html [
> > http://www.R-project.org/posting-guide.html]
> > >  > and provide commented, minimal, self-contained, reproducible
> code.
> > >
> > >  ____________________________________________________________
> > >  FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> > >  Check it out at http://www.inbox.com/earth
> > > [http://www.inbox.com/earth]
> > >
> > > ____________________________________________________________
> > > FREE ONLINE PHOTOSHARING - Share your photos online with your
> > > friends
> > and family!
> > > Visit http://www.inbox.com/photosharing to find out more!
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From Martin.Spindler at gmx.de  Thu Jul 30 14:26:34 2015
From: Martin.Spindler at gmx.de (Martin Spindler)
Date: Thu, 30 Jul 2015 14:26:34 +0200
Subject: [R] R parallel - slow speed
Message-ID: <trinity-69cc511b-6769-419d-a083-618f32f1d575-1438259194833@3capp-gmx-bs03>

Dear all,

I am trying to parallelize the function npnewpar given below. When I am comparing an application of "apply" with "parApply" the parallelized version seems to be much slower (cf output below). Therefore I would like to ask how the function could be parallelized more efficient. (With increasing sample size the difference becomes smaller, but I was wondering about this big differences and how it could be improved.)

Thank you very much for help in advance!

Best,

Martin


library(microbenchmark)
library(doParallel)

n <- 500
y <- rnorm(n)
Xc <- rnorm(n)
Xd <- sample(c(0,1), replace=TRUE)
Weights <- diag(n)
n1 <- 50
Xeval <- cbind(rnorm(n1), sample(c(0,1), n1, replace=TRUE))


detectCores()
cl <- makeCluster(4)
registerDoParallel(cl)
microbenchmark(apply(Xeval, 1, npnewpar, y=y, Xc=Xc, Xd = Xd, Weights=Weights, h=0.5),  parApply(cl, Xeval, 1, npnewpar, y=y, Xc=Xc, Xd = Xd, Weights=Weights, h=0.5), times=100)
stopCluster(cl)


Unit: milliseconds
                                                                                       expr       min        lq      mean    median
        apply(Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights = Weights,      h = 0.5)  4.674914  4.726463  5.455323  4.771016
 parApply(cl, Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights = Weights,      h = 0.5) 34.168250 35.434829 56.553296 39.438899
        uq       max neval
  4.843324  57.01519   100
 49.777265 347.77887   100














npnewpar <- function(y, Xc, Xd, Weights, h, xeval) {
  xc <- xeval[1]
  xd <- xeval[2]
  l <- function(x,X) {
    w <-  Weights[x,X]
    return(w)
  }
  u <- (Xc-xc)/h
  #K <- kernel(u)
  K <- dnorm(u)
  L <- l(xd,Xd)
  nom <- sum(y*K*L)
  denom <- sum(K*L)
  ghat <- nom/denom
  return(ghat)
}


From mylisttech at gmail.com  Thu Jul 30 14:41:26 2015
From: mylisttech at gmail.com (My List)
Date: Thu, 30 Jul 2015 18:11:26 +0530
Subject: [R] R and AWS
Message-ID: <CAFpdVnxFRyX5VhzinqzKEbNKtRQRD0bsNCfFjr+BHz9jzJdMyA@mail.gmail.com>

Hello All,

I wanted to know if there is a quick tutorial which I could be pointed to,
for the understanding of setting R and R studio on Amazon web services.

Thanks in Advance,
Harmeet

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.CA.us  Thu Jul 30 14:56:53 2015
From: jdnewmil at dcn.davis.CA.us (Jeff Newmiller)
Date: Thu, 30 Jul 2015 08:56:53 -0400
Subject: [R] R parallel - slow speed
In-Reply-To: <trinity-69cc511b-6769-419d-a083-618f32f1d575-1438259194833@3capp-gmx-bs03>
References: <trinity-69cc511b-6769-419d-a083-618f32f1d575-1438259194833@3capp-gmx-bs03>
Message-ID: <B40A052D-07AB-4D27-A07B-A78005C06328@dcn.davis.CA.us>

Parallelizing comes at a price... and there is no guarantee that you can afford it. Vectorizing your algorithms is often a better approach. Microbenchmarking  is usually overkill for evaluating parallelizing.

You assume 4 cores... but many CPUs have 2 cores and use hyperthreading to make each core look like two.

The operating system can make a difference also... Windows processes are more expensive to start and communicate between than *nix processes are. In particular, Windows seems to require duplicated RAM pages while *nix can share process RAM (at least until they are written to) so you end up needing more memory and disk paging of virtual memory becomes more likely.
---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                      Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
--------------------------------------------------------------------------- 
Sent from my phone. Please excuse my brevity.

On July 30, 2015 8:26:34 AM EDT, Martin Spindler <Martin.Spindler at gmx.de> wrote:
>Dear all,
>
>I am trying to parallelize the function npnewpar given below. When I am
>comparing an application of "apply" with "parApply" the parallelized
>version seems to be much slower (cf output below). Therefore I would
>like to ask how the function could be parallelized more efficient.
>(With increasing sample size the difference becomes smaller, but I was
>wondering about this big differences and how it could be improved.)
>
>Thank you very much for help in advance!
>
>Best,
>
>Martin
>
>
>library(microbenchmark)
>library(doParallel)
>
>n <- 500
>y <- rnorm(n)
>Xc <- rnorm(n)
>Xd <- sample(c(0,1), replace=TRUE)
>Weights <- diag(n)
>n1 <- 50
>Xeval <- cbind(rnorm(n1), sample(c(0,1), n1, replace=TRUE))
>
>
>detectCores()
>cl <- makeCluster(4)
>registerDoParallel(cl)
>microbenchmark(apply(Xeval, 1, npnewpar, y=y, Xc=Xc, Xd = Xd,
>Weights=Weights, h=0.5),  parApply(cl, Xeval, 1, npnewpar, y=y, Xc=Xc,
>Xd = Xd, Weights=Weights, h=0.5), times=100)
>stopCluster(cl)
>
>
>Unit: milliseconds
>                           expr       min        lq      mean    median
>apply(Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights = Weights,  
>   h = 0.5)  4.674914  4.726463  5.455323  4.771016
>parApply(cl, Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights =
>Weights,      h = 0.5) 34.168250 35.434829 56.553296 39.438899
>        uq       max neval
>  4.843324  57.01519   100
> 49.777265 347.77887   100
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>npnewpar <- function(y, Xc, Xd, Weights, h, xeval) {
>  xc <- xeval[1]
>  xd <- xeval[2]
>  l <- function(x,X) {
>    w <-  Weights[x,X]
>    return(w)
>  }
>  u <- (Xc-xc)/h
>  #K <- kernel(u)
>  K <- dnorm(u)
>  L <- l(xd,Xd)
>  nom <- sum(y*K*L)
>  denom <- sum(K*L)
>  ghat <- nom/denom
>  return(ghat)
>}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Thu Jul 30 15:14:30 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 30 Jul 2015 05:14:30 -0800
Subject: [R] Mixed Date Formats
In-Reply-To: <1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>
References: <cam_vjunghxxpqvfap9jwmc1uwkdyzcsd=xbfyvro4bp_syhtrq@mail.gmail.com>
Message-ID: <89DBF91A6CA.000009DFjrkrideau@inbox.com>

This does not look good. But not too bad

Can we assume that the original data is in D-M-Y in all cases. The values at 3 & 4 in the sample data are ambigous, in that someone may be using the American dating system of M-D-Y.  Given the rest of the data it seems unlikely but possible.

Otherwise it looks possible, but probably not for me since I'm lousy at things like grep, to sort the data set into three parts based on the last three characters in the Year part of the date, convert and recombine.

SampleData
  id value       date
  1  5813  19-Dec-11
  2  8706  07-Dec-11
  3  4049   06/05/11
  4  5877   05/12/11
  5  1375 31/12/2011
  6  2223 10/19/2011
  7  3423 01/22/2011

John Kane
Kingston ON Canada


> -----Original Message-----
> From: r-help at r-project.org
> Sent: Wed, 29 Jul 2015 21:15:45 +0000 (UTC)
> To: sarah.goslee at gmail.com
> Subject: Re: [R] Mixed Date Formats
> 
> Hi Sarah,
> Thanks for getting back to me.Here is an example of my data:SampleData <-
> structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,
> 1375L, 2223L, 3423L), date = structure(c(4L, 3L, 2L, 1L, 7L,
> 6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",
> "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"
> ), class = "factor")), .Names = c("id", "value", "date"), row.names =
> c(NA,
> -7L), class = "data.frame")SampleData
> Thanks for your help:).
> 
> 
> 
> 
>      On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee
> <sarah.goslee at gmail.com> wrote:
> 
> 
>  On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
> <r-help at r-project.org> wrote:
> >? Hi Arun,
>> Hope all is well with you. I have a data with a column for date.The date
>> format is mixed. There are date values with Month/Day/Year format and
>> values with Day/Month/Year format.I don't know how to unify it.I really
>> appreciate your help.Thanks.
> 
> You sent this to the R-help list, not just to Arun, so I'm assuming
> this is an R question. The best way to get help is to provide a sample
> of your data using dput() and to clearly specify what you would like
> as the result - "unify" is a bit vague. paste(x, collapse="") could be
> considered unification, after all.
> 
> Sarah
> 
> --
> Sarah Goslee
> http://www.functionaldiversity.org
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From sarah.goslee at gmail.com  Thu Jul 30 15:19:55 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 30 Jul 2015 09:19:55 -0400
Subject: [R] Mixed Date Formats
In-Reply-To: <89DBF91A6CA.000009DFjrkrideau@inbox.com>
References: <cam_vjunghxxpqvfap9jwmc1uwkdyzcsd=xbfyvro4bp_syhtrq@mail.gmail.com>
	<1448534226.5250488.1438204545318.JavaMail.yahoo@mail.yahoo.com>
	<89DBF91A6CA.000009DFjrkrideau@inbox.com>
Message-ID: <CAM_vju=9xMcv3rfkXE39qzZYS8T05KoavCL95-Wsq+OdTsBjQw@mail.gmail.com>

On Thu, Jul 30, 2015 at 9:14 AM, John Kane <jrkrideau at inbox.com> wrote:
> This does not look good. But not too bad
>
> Can we assume that the original data is in D-M-Y in all cases. The values at 3 & 4 in the sample data are ambigous, in that someone may be using the American dating system of M-D-Y.  Given the rest of the data it seems unlikely but possible.

Take a look at 5 and 7:
>   5  1375 31/12/2011 # dd/mm/yyyy
>   7  3423 01/22/2011 # mm/dd/yyyy

They're unambiguously different.

> Otherwise it looks possible, but probably not for me since I'm lousy at things like grep, to sort the data set into three parts based on the last three characters in the Year part of the date, convert and recombine.

It would be easy if month and day were consistent, regardless of year format.


> SampleData
>   id value       date
>   1  5813  19-Dec-11
>   2  8706  07-Dec-11
>   3  4049   06/05/11
>   4  5877   05/12/11
>   5  1375 31/12/2011
>   6  2223 10/19/2011
>   7  3423 01/22/2011
>
> John Kane
> Kingston ON Canada
>
>
>> -----Original Message-----
>> From: r-help at r-project.org
>> Sent: Wed, 29 Jul 2015 21:15:45 +0000 (UTC)
>> To: sarah.goslee at gmail.com
>> Subject: Re: [R] Mixed Date Formats
>>
>> Hi Sarah,
>> Thanks for getting back to me.Here is an example of my data:SampleData <-
>> structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,
>> 1375L, 2223L, 3423L), date = structure(c(4L, 3L, 2L, 1L, 7L,
>> 6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",
>> "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"
>> ), class = "factor")), .Names = c("id", "value", "date"), row.names =
>> c(NA,
>> -7L), class = "data.frame")SampleData
>> Thanks for your help:).
>>
>>
>>
>>
>>      On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee
>> <sarah.goslee at gmail.com> wrote:
>>
>>
>>  On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
>> <r-help at r-project.org> wrote:
>> >  Hi Arun,
>>> Hope all is well with you. I have a data with a column for date.The date
>>> format is mixed. There are date values with Month/Day/Year format and
>>> values with Day/Month/Year format.I don't know how to unify it.I really
>>> appreciate your help.Thanks.
>>
>> You sent this to the R-help list, not just to Arun, so I'm assuming
>> this is an R question. The best way to get help is to provide a sample
>> of your data using dput() and to clearly specify what you would like
>> as the result - "unify" is a bit vague. paste(x, collapse="") could be
>> considered unification, after all.
>>
>> Sarah
>>
>> --
>> Sarah Goslee
>> http://www.functionaldiversity.org
>>
>>
>



-- 
Sarah Goslee
http://www.functionaldiversity.org


From jholtman at gmail.com  Thu Jul 30 15:28:03 2015
From: jholtman at gmail.com (jim holtman)
Date: Thu, 30 Jul 2015 09:28:03 -0400
Subject: [R] R parallel - slow speed
In-Reply-To: <B40A052D-07AB-4D27-A07B-A78005C06328@dcn.davis.CA.us>
References: <trinity-69cc511b-6769-419d-a083-618f32f1d575-1438259194833@3capp-gmx-bs03>
	<B40A052D-07AB-4D27-A07B-A78005C06328@dcn.davis.CA.us>
Message-ID: <CAAxdm-7rL2WdN=chA=14bQL+wMXhxtmnYaJ5KXOGcucbcfkfWQ@mail.gmail.com>

I ran a test on my Windows box with 4 CPUs.  THere were 4 RScript processes
started in response to the request for a cluster of 4.  Each of these ran
for an elapsed time of around 23 seconds, making the median time around 0.2
seconds for 100 iterations as reported by microbenchmark.  The 'apply' only
takes about 0.003 seconds for a single iteration - again what
microbenchmark is reporting.

The 4 RScript processes each use about 3 CPU seconds in the 23 seconds of
elapsed time, most of that is probably the communication and startup time
for the processes and reporting results.

So as was pointed out previous there is overhead is running in parallel.
You probably have to have at least several seconds of heavy computation for
a iteration to make trying to parallelize something.  You should also
investigate exactly what is happening on your system so that you can
account for the time being spent.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Jul 30, 2015 at 8:56 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Parallelizing comes at a price... and there is no guarantee that you can
> afford it. Vectorizing your algorithms is often a better approach.
> Microbenchmarking  is usually overkill for evaluating parallelizing.
>
> You assume 4 cores... but many CPUs have 2 cores and use hyperthreading to
> make each core look like two.
>
> The operating system can make a difference also... Windows processes are
> more expensive to start and communicate between than *nix processes are. In
> particular, Windows seems to require duplicated RAM pages while *nix can
> share process RAM (at least until they are written to) so you end up
> needing more memory and disk paging of virtual memory becomes more likely.
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                       Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> Sent from my phone. Please excuse my brevity.
>
> On July 30, 2015 8:26:34 AM EDT, Martin Spindler <Martin.Spindler at gmx.de>
> wrote:
> >Dear all,
> >
> >I am trying to parallelize the function npnewpar given below. When I am
> >comparing an application of "apply" with "parApply" the parallelized
> >version seems to be much slower (cf output below). Therefore I would
> >like to ask how the function could be parallelized more efficient.
> >(With increasing sample size the difference becomes smaller, but I was
> >wondering about this big differences and how it could be improved.)
> >
> >Thank you very much for help in advance!
> >
> >Best,
> >
> >Martin
> >
> >
> >library(microbenchmark)
> >library(doParallel)
> >
> >n <- 500
> >y <- rnorm(n)
> >Xc <- rnorm(n)
> >Xd <- sample(c(0,1), replace=TRUE)
> >Weights <- diag(n)
> >n1 <- 50
> >Xeval <- cbind(rnorm(n1), sample(c(0,1), n1, replace=TRUE))
> >
> >
> >detectCores()
> >cl <- makeCluster(4)
> >registerDoParallel(cl)
> >microbenchmark(apply(Xeval, 1, npnewpar, y=y, Xc=Xc, Xd = Xd,
> >Weights=Weights, h=0.5),  parApply(cl, Xeval, 1, npnewpar, y=y, Xc=Xc,
> >Xd = Xd, Weights=Weights, h=0.5), times=100)
> >stopCluster(cl)
> >
> >
> >Unit: milliseconds
> >                           expr       min        lq      mean    median
> >apply(Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights = Weights,
> >   h = 0.5)  4.674914  4.726463  5.455323  4.771016
> >parApply(cl, Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights =
> >Weights,      h = 0.5) 34.168250 35.434829 56.553296 39.438899
> >        uq       max neval
> >  4.843324  57.01519   100
> > 49.777265 347.77887   100
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >npnewpar <- function(y, Xc, Xd, Weights, h, xeval) {
> >  xc <- xeval[1]
> >  xd <- xeval[2]
> >  l <- function(x,X) {
> >    w <-  Weights[x,X]
> >    return(w)
> >  }
> >  u <- (Xc-xc)/h
> >  #K <- kernel(u)
> >  K <- dnorm(u)
> >  L <- l(xd,Xd)
> >  nom <- sum(y*K*L)
> >  denom <- sum(K*L)
> >  ghat <- nom/denom
> >  return(ghat)
> >}
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Thu Jul 30 15:39:38 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 30 Jul 2015 05:39:38 -0800
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438209813434-4710560.post@n4.nabble.com>
References: <7e099629dd3.0000110ajrkrideau@inbox.com>
	<8a0310da-7828-40bd-8299-aad575eb7588@comcast.net>
	<1438195076846-4710543.post@n4.nabble.com>
	<1438030410589-4710431.post@n4.nabble.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02bb98d1-ca17-493e-b7be-c2284225b5dc@dcn.davis.ca.us>
	<1438200145867-4710549.post@n4.nabble.com>
	<bcbd1e6d-347e-4fab-ab19-689e05d5c366@xs4all.nl>
Message-ID: <8A142396B2F.00000A18jrkrideau@inbox.com>

Hi Markov
I have no idea of what nabble is or why it exists but the real problem with it is that it looks like (is?) a form where questions and responses are threaded so that it is obvious what the on-going conversation is. 

So a nabble user will respond with something like :
Great but why xx1 is not a postive number? 

Unfortunately there no context.  Perhaps in the mailing list their origiinal question appeared 35 or 40 emails ago. In some cases, probably many the R-help readers have  decided the OP is not interested any more and had deleted the rest of the correspondence. Therefore they have no clue what the nabble user is blathering on about and is usually not inclined to go to nabble read possibly 4-10 posts etc.  

There are basically two ways of replying in a mailing list depending on personal preference and on exact needs.  Typically one top-posts if the answer is relatively self-contained. See my response.

The other way is in-line. This is used normally when the responder wants to deal with individual points in the code or make various specific comments.

In both cases the entire context of the original problem is maintained. Judidcious editing is okay but the main issues are always included in the post so any reader can have a quick look and understand what is happening.

Nabble posters seem never to provide context because they think they are in a forum. In many cases R-help users just are not willing to waste time doing what the original poster should have done

To properly participate in R-help you should go to the R home page and subscribe to the R-help mailing list there.

>ou should probably write a wiki going forward so you won't get 
>upset every-time somebody asks a reasonable question.

Feel free to do so. 
One thing to note about the R-help list. It can be incredibly helpful but is not terribly polite or, in some cases particularly tolerant of what is perceived as un-list behaviours and so people do get flames, sometimes deservedly and sometimes unjustly. Just hope you don't get Ripleyied.:)

I'll have to think about your use of a barchart. I think you are the first person who has ever come up with an explanation of why one would use a barchart that may make sense. Can you give me an example?

Well come to the R-help list (down with nabble)



John Kane
Kingston ON Canada


> -----Original Message-----
> From: hussain at touchofmodern.com
> Sent: Wed, 29 Jul 2015 15:43:33 -0700 (PDT)
> To: r-help at r-project.org
> Subject: Re: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
> 
> Alright thanks for clarifying. That's all a bit esoteric. Quite different
> from "basics off computer use".  None of the documentation anyone
> mentioned
> actually contains any of this. If Nabble is so bad, then why does anyone
> use
> it? It doesn't make sense. You should probably write a wiki going forward
> so
> you won't get upset every-time somebody asks a reasonable question.
> 
> 
> 
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710560.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From mylisttech at gmail.com  Thu Jul 30 16:00:16 2015
From: mylisttech at gmail.com (MyList)
Date: Thu, 30 Jul 2015 19:30:16 +0530
Subject: [R] Need help
Message-ID: <CB74D208-BAD2-41C7-AE30-AE39F9353146@gmail.com>

Dear All,

What are inner and outer matrix multiplication. Kindly guide me where to look for this Wikipedia was not much help.

Thanks in Advance,
Harmeet

From jrkrideau at inbox.com  Thu Jul 30 16:07:53 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 30 Jul 2015 06:07:53 -0800
Subject: [R] Mixed Date Formats
In-Reply-To: <CAM_vju=9xMcv3rfkXE39qzZYS8T05KoavCL95-Wsq+OdTsBjQw@mail.gmail.com>
References: <cam_vjunghxxpqvfap9jwmc1uwkdyzcsd=xbfyvro4bp_syhtrq@mail.gmail.com>
	<89dbf91a6ca.000009dfjrkrideau@inbox.com>
	<1448534226.5250488.1438204545318.javamail.yahoo@mail.yahoo.com>
Message-ID: <8A534C9FFEF.00000A6Djrkrideau@inbox.com>

That's what I get for reading without a cup of tea beside me. I looked at 5 and my eye just slide over the last entries. 


I change my assessment : this looks bad. (Tea at hand).

So, presumably Farnoosh may not be able to guarantee the formats for 3 & 4 either unless they are unambiguously dated.


John Kane
Kingston ON Canada


> -----Original Message-----
> From: sarah.goslee at gmail.com
> Sent: Thu, 30 Jul 2015 09:19:55 -0400
> To: jrkrideau at inbox.com
> Subject: Re: [R] Mixed Date Formats
> 
> On Thu, Jul 30, 2015 at 9:14 AM, John Kane <jrkrideau at inbox.com> wrote:
>> This does not look good. But not too bad
>> 
>> Can we assume that the original data is in D-M-Y in all cases. The
>> values at 3 & 4 in the sample data are ambigous, in that someone may be
>> using the American dating system of M-D-Y.  Given the rest of the data
>> it seems unlikely but possible.
> 
> Take a look at 5 and 7:
>>   5  1375 31/12/2011 # dd/mm/yyyy
>>   7  3423 01/22/2011 # mm/dd/yyyy
> 
> They're unambiguously different.
> 
>> Otherwise it looks possible, but probably not for me since I'm lousy at
>> things like grep, to sort the data set into three parts based on the
>> last three characters in the Year part of the date, convert and
>> recombine.
> 
> It would be easy if month and day were consistent, regardless of year
> format.
> 
> 
>> SampleData
>>   id value       date
>>   1  5813  19-Dec-11
>>   2  8706  07-Dec-11
>>   3  4049   06/05/11
>>   4  5877   05/12/11
>>   5  1375 31/12/2011
>>   6  2223 10/19/2011
>>   7  3423 01/22/2011
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: r-help at r-project.org
>>> Sent: Wed, 29 Jul 2015 21:15:45 +0000 (UTC)
>>> To: sarah.goslee at gmail.com
>>> Subject: Re: [R] Mixed Date Formats
>>> 
>>> Hi Sarah,
>>> Thanks for getting back to me.Here is an example of my data:SampleData
>>> <-
>>> structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,
>>> 1375L, 2223L, 3423L), date = structure(c(4L, 3L, 2L, 1L, 7L,
>>> 6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",
>>> "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"
>>> ), class = "factor")), .Names = c("id", "value", "date"), row.names =
>>> c(NA,
>>> -7L), class = "data.frame")SampleData
>>> Thanks for your help:).
>>> 
>>> 
>>> 
>>> 
>>>      On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee
>>> <sarah.goslee at gmail.com> wrote:
>>> 
>>> 
>>>  On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
>>> <r-help at r-project.org> wrote:
>>>>  Hi Arun,
>>>> Hope all is well with you. I have a data with a column for date.The
>>>> date
>>>> format is mixed. There are date values with Month/Day/Year format and
>>>> values with Day/Month/Year format.I don't know how to unify it.I
>>>> really
>>>> appreciate your help.Thanks.
>>> 
>>> You sent this to the R-help list, not just to Arun, so I'm assuming
>>> this is an R question. The best way to get help is to provide a sample
>>> of your data using dput() and to clearly specify what you would like
>>> as the result - "unify" is a bit vague. paste(x, collapse="") could be
>>> considered unification, after all.
>>> 
>>> Sarah
>>> 
>>> --
>>> Sarah Goslee
>>> http://www.functionaldiversity.org
>>> 
>>> 
>> 
> 
> 
> 
> --
> Sarah Goslee
> http://www.functionaldiversity.org

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From pdalgd at gmail.com  Thu Jul 30 16:22:15 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 30 Jul 2015 16:22:15 +0200
Subject: [R] Need help
In-Reply-To: <CB74D208-BAD2-41C7-AE30-AE39F9353146@gmail.com>
References: <CB74D208-BAD2-41C7-AE30-AE39F9353146@gmail.com>
Message-ID: <2D62843B-DF0B-47B9-B897-5A13D4B08CE1@gmail.com>


> On 30 Jul 2015, at 16:00 , MyList <mylisttech at gmail.com> wrote:
> 
> Dear All,
> 
> What are inner and outer matrix multiplication. Kindly guide me where to look for this Wikipedia was not much help.

I'd need some context to be sure, but some use tr(AB) as the inner product in a space of square matrices and the kronecker product (see its help page) is sometimes called an outer product.

-pd

> 
> Thanks in Advance,
> Harmeet
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jrkrideau at inbox.com  Thu Jul 30 17:23:55 2015
From: jrkrideau at inbox.com (John Kane)
Date: Thu, 30 Jul 2015 07:23:55 -0800
Subject: [R] ggplot2 - geom_text() with date as x-axis
Message-ID: <8AFD4242805.00000B93jrkrideau@inbox.com>

I am trying to annotate a graph using geom_text() and I seem to be misunderstanding how to use a date in the co-ordinates---or, at least, I think that is the problem. Code is below.

Can anyone give me a suggestion of where I am going wrong?

Thanks,
John

John Kane
Kingston ON Canada
###===========================================
ibrary(ggplot2)
library(lubridate)
ins  <-  structure(list(td = structure(c(1437804720, 1437824100, 1437836220, 
1437851580, 1437863460, 1437878640, 1437890640, 1437904800, 1437918240, 
1437926100, 1437941340, 1437951240), tzone = "UTC", class = c("POSIXct", 
"POSIXt")), glucose = c(328L, 390L, 358L, 387L, 440L, 328L, 365L, 
450L, 467L, 477L, 408L, 457L), dose = c(NA, 0.5, NA, NA, 0.5, 
NA, NA, 0.5, NA, NA, NA, 0.5)), .Names = c("td", "glucose", "dose"
), row.names = c(NA, -12L), class = "data.frame")

anon  <- na.omit(ins)  # extract shots 

texdat =  ymd_hm("2015-07-26 20:09")

glucose  <-  ggplot(ins, aes(td, glucose)) + geom_point(colour = "red") + geom_line(colour = "blue")
p1  <-  glucose + annotate("text", x = texdat, y = 500, label = anon[ ,3 ], cex = 3) 
p1

# Now the problem
 p2  <-  p1 +  geom_text(x = texdat, y = 400 , size = 2,  label= "Glucose")
p2
###=============================================

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From farnoosh_81 at yahoo.com  Thu Jul 30 17:23:10 2015
From: farnoosh_81 at yahoo.com (farnoosh sheikhi)
Date: Thu, 30 Jul 2015 15:23:10 +0000 (UTC)
Subject: [R] Mixed Date Formats
In-Reply-To: <8A534C9FFEF.00000A6Djrkrideau@inbox.com>
References: <8A534C9FFEF.00000A6Djrkrideau@inbox.com>
Message-ID: <1373861330.5776024.1438269790639.JavaMail.yahoo@mail.yahoo.com>

Thank you all for your help.I came to this conclusion that data needs to be verified by its source.?Thanks again.?
 


     On Thursday, July 30, 2015 7:07 AM, John Kane <jrkrideau at inbox.com> wrote:
   

 That's what I get for reading without a cup of tea beside me. I looked at 5 and my eye just slide over the last entries. 


I change my assessment : this looks bad. (Tea at hand).

So, presumably Farnoosh may not be able to guarantee the formats for 3 & 4 either unless they are unambiguously dated.


John Kane
Kingston ON Canada


> -----Original Message-----
> From: sarah.goslee at gmail.com
> Sent: Thu, 30 Jul 2015 09:19:55 -0400
> To: jrkrideau at inbox.com
> Subject: Re: [R] Mixed Date Formats
> 
> On Thu, Jul 30, 2015 at 9:14 AM, John Kane <jrkrideau at inbox.com> wrote:
>> This does not look good. But not too bad
>> 
>> Can we assume that the original data is in D-M-Y in all cases. The
>> values at 3 & 4 in the sample data are ambigous, in that someone may be
>> using the American dating system of M-D-Y.? Given the rest of the data
>> it seems unlikely but possible.
> 
> Take a look at 5 and 7:
>>? 5? 1375 31/12/2011 # dd/mm/yyyy
>>? 7? 3423 01/22/2011 # mm/dd/yyyy
> 
> They're unambiguously different.
> 
>> Otherwise it looks possible, but probably not for me since I'm lousy at
>> things like grep, to sort the data set into three parts based on the
>> last three characters in the Year part of the date, convert and
>> recombine.
> 
> It would be easy if month and day were consistent, regardless of year
> format.
> 
> 
>> SampleData
>>? id value? ? ? date
>>? 1? 5813? 19-Dec-11
>>? 2? 8706? 07-Dec-11
>>? 3? 4049? 06/05/11
>>? 4? 5877? 05/12/11
>>? 5? 1375 31/12/2011
>>? 6? 2223 10/19/2011
>>? 7? 3423 01/22/2011
>> 
>> John Kane
>> Kingston ON Canada
>> 
>> 
>>> -----Original Message-----
>>> From: r-help at r-project.org
>>> Sent: Wed, 29 Jul 2015 21:15:45 +0000 (UTC)
>>> To: sarah.goslee at gmail.com
>>> Subject: Re: [R] Mixed Date Formats
>>> 
>>> Hi Sarah,
>>> Thanks for getting back to me.Here is an example of my data:SampleData
>>> <-
>>> structure(list(id = 1:7, value = c(5813L, 8706L, 4049L, 5877L,
>>> 1375L, 2223L, 3423L), date = structure(c(4L, 3L, 2L, 1L, 7L,
>>> 6L, 5L), .Label = c("05/12/11", "06/05/11", "07-Dec-11",
>>> "19-Dec-11", "01/22/2011", "10/19/2011", "31/12/2011"
>>> ), class = "factor")), .Names = c("id", "value", "date"), row.names =
>>> c(NA,
>>> -7L), class = "data.frame")SampleData
>>> Thanks for your help:).
>>> 
>>> 
>>> 
>>> 
>>>? ? ? On Wednesday, July 29, 2015 1:50 PM, Sarah Goslee
>>> <sarah.goslee at gmail.com> wrote:
>>> 
>>> 
>>>? On Wed, Jul 29, 2015 at 2:45 PM, farnoosh sheikhi via R-help
>>> <r-help at r-project.org> wrote:
>>>>? Hi Arun,
>>>> Hope all is well with you. I have a data with a column for date.The
>>>> date
>>>> format is mixed. There are date values with Month/Day/Year format and
>>>> values with Day/Month/Year format.I don't know how to unify it.I
>>>> really
>>>> appreciate your help.Thanks.
>>> 
>>> You sent this to the R-help list, not just to Arun, so I'm assuming
>>> this is an R question. The best way to get help is to provide a sample
>>> of your data using dput() and to clearly specify what you would like
>>> as the result - "unify" is a bit vague. paste(x, collapse="") could be
>>> considered unification, after all.
>>> 
>>> Sarah
>>> 
>>> --
>>> Sarah Goslee
>>> http://www.functionaldiversity.org
>>> 
>>> 
>> 
> 
> 
> 
> --
> Sarah Goslee
> http://www.functionaldiversity.org

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
[[elided Yahoo spam]]



  
	[[alternative HTML version deleted]]


From Sebastien.Bihorel at cognigencorp.com  Thu Jul 30 17:37:55 2015
From: Sebastien.Bihorel at cognigencorp.com (sbihorel)
Date: Thu, 30 Jul 2015 11:37:55 -0400
Subject: [R] Using latticeExtra as.layer function with different number of
 plot panels
Message-ID: <55BA44D3.3080605@cognigencorp.com>

Hi,

When the as.layer function is used to overaly 2 lattice plots, there 
seems to be an assumption that the data used in both plots will generate 
the same number of panels (and, I believe, in the same order). In case 
the data used in the plot within the as.layer call is incomplete , data 
may be plotted on the "wrong" panel, and data seem to get re-used on the 
last panel(s). See what happens in the example code below when the 
records with state.region=="South" are dropped...

Is there a trick to overlay panel based upon the conditioning variable 
value rather than the panel order?

require(lattice)
require(latticeExtra)
state2 <- state <- data.frame(state.x77,state.region)
state2$Income <- sample(state2$Income)
state3 <- state2[which(state2$state.region!="South"),]
foo <- xyplot(Income~Population|state.region,data=state,main='foo')
foo

bar <- update(foo,main='bar') + 
as.layer(xyplot(Income~Population|state.region,data=state2,col='red'))
bar

bar2 <- update(foo,main='bar2') + 
as.layer(xyplot(Income~Population|state.region,data=state3,col='red'))
bar2

Thank you

Sebastien

PS: I know that I could get what I want by setting the Income variable 
to NA for records with state.region=="South" instead of dropping them... 
but this is not the point of my example. I am just trying to illustrate 
what happens when as.layer is used for plotting data with inconsistent 
dimensions.


From h.wickham at gmail.com  Thu Jul 30 17:55:27 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Thu, 30 Jul 2015 10:55:27 -0500
Subject: [R] ggplot2 - geom_text() with date as x-axis
In-Reply-To: <8AFD4242805.00000B93jrkrideau@inbox.com>
References: <8AFD4242805.00000B93jrkrideau@inbox.com>
Message-ID: <CABdHhvHHOrBEvdqT80v1rOCmdRfwzPC4n2gKzkztRrjox69NUw@mail.gmail.com>

I'm a bit confused what you're trying to accomplish - the mix of
annotate() and geom_text() is confusing. The following code works for
me, and I think might be what you want:

ggplot(ins, aes(td, glucose)) +
  geom_point(colour = "red") +
  geom_line(colour = "blue") +
  annotate("text", x = texdat, y = 500, label = "Glucose", cex = 3)

Hadley

On Thu, Jul 30, 2015 at 10:23 AM, John Kane <jrkrideau at inbox.com> wrote:
> I am trying to annotate a graph using geom_text() and I seem to be misunderstanding how to use a date in the co-ordinates---or, at least, I think that is the problem. Code is below.
>
> Can anyone give me a suggestion of where I am going wrong?
>
> Thanks,
> John
>
> John Kane
> Kingston ON Canada
> ###===========================================
> ibrary(ggplot2)
> library(lubridate)
> ins  <-  structure(list(td = structure(c(1437804720, 1437824100, 1437836220,
> 1437851580, 1437863460, 1437878640, 1437890640, 1437904800, 1437918240,
> 1437926100, 1437941340, 1437951240), tzone = "UTC", class = c("POSIXct",
> "POSIXt")), glucose = c(328L, 390L, 358L, 387L, 440L, 328L, 365L,
> 450L, 467L, 477L, 408L, 457L), dose = c(NA, 0.5, NA, NA, 0.5,
> NA, NA, 0.5, NA, NA, NA, 0.5)), .Names = c("td", "glucose", "dose"
> ), row.names = c(NA, -12L), class = "data.frame")
>
> anon  <- na.omit(ins)  # extract shots
>
> texdat =  ymd_hm("2015-07-26 20:09")
>
> glucose  <-  ggplot(ins, aes(td, glucose)) + geom_point(colour = "red") + geom_line(colour = "blue")
> p1  <-  glucose + annotate("text", x = texdat, y = 500, label = anon[ ,3 ], cex = 3)
> p1
>
> # Now the problem
>  p2  <-  p1 +  geom_text(x = texdat, y = 400 , size = 2,  label= "Glucose")
> p2
> ###=============================================
>
> ____________________________________________________________
> FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
http://had.co.nz/


From hussain at touchofmodern.com  Thu Jul 30 19:12:42 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Thu, 30 Jul 2015 10:12:42 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <8A142396B2F.00000A18jrkrideau@inbox.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
	<8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
	<1438209813434-4710560.post@n4.nabble.com>
	<8A142396B2F.00000A18jrkrideau@inbox.com>
Message-ID: <1438276362369-4710595.post@n4.nabble.com>

Hi John, 

Certainly, that makes sense. Thank you for the clear explanation. I am
subscribed to the email list, it is how I knew you replied. A stacked bar
chart is great for managing and analyzing multi-ad placement Profit and
seeing the time series of the individual charts. Tableau makes it very easy
to use, since you can highlight a specific section and gray out the others
to zero in on issues. 



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710595.html
Sent from the R help mailing list archive at Nabble.com.


From pdalgd at gmail.com  Thu Jul 30 20:43:52 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 30 Jul 2015 20:43:52 +0200
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <1438276362369-4710595.post@n4.nabble.com>
References: <1438030410589-4710431.post@n4.nabble.com>
	<7E099629DD3.0000110Ajrkrideau@inbox.com>
	<1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
	<8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
	<1438209813434-4710560.post@n4.nabble.com>
	<8A142396B2F.00000A18jrkrideau@inbox.com>
	<1438276362369-4710595.post@n4.nabble.com>
Message-ID: <78036930-7DD2-470F-9107-8C510A687C0B@gmail.com>


You still don't get it, do you? 

First, the list is not "John". 

Second, and more importantly, those of us with threading mail programs (or "group by conversation" in Apple-speak) can fairly easily find Johns letter if we haven't deleted it yet. Those without threading will have to look back through the last 5 hours of mail to find it, which is probably not unbearable, but imagine what happens when people do the same thing with posts from 2012. 

And no, it is NOT reasonable to force readers to go to Nabble to find out what the h*ck you are on about!

There has been talk about making the Nabble gateways read-only. Hopefully, that will happen sooner rather than later.

-pd
 

> On 30 Jul 2015, at 19:12 , Hidden Markov Model <hussain at touchofmodern.com> wrote:
> 
> Hi John, 
> 
> Certainly, that makes sense. Thank you for the clear explanation. I am
> subscribed to the email list, it is how I knew you replied. 
....
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710595.html
> Sent from the R help mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From hussain at touchofmodern.com  Thu Jul 30 20:49:36 2015
From: hussain at touchofmodern.com (Hidden Markov Model)
Date: Thu, 30 Jul 2015 11:49:36 -0700 (PDT)
Subject: [R] Daily Category Revenue-Stacked Bar Chart in ggplot2
In-Reply-To: <78036930-7DD2-470F-9107-8C510A687C0B@gmail.com>
References: <1438194234637-4710540.post@n4.nabble.com>
	<02BB98D1-CA17-493E-B7BE-C2284225B5DC@dcn.davis.CA.us>
	<1438195076846-4710543.post@n4.nabble.com>
	<BCBD1E6D-347E-4FAB-AB19-689E05D5C366@xs4all.nl>
	<1438200145867-4710549.post@n4.nabble.com>
	<8A0310DA-7828-40BD-8299-AAD575EB7588@comcast.net>
	<1438209813434-4710560.post@n4.nabble.com>
	<8A142396B2F.00000A18jrkrideau@inbox.com>
	<1438276362369-4710595.post@n4.nabble.com>
	<78036930-7DD2-470F-9107-8C510A687C0B@gmail.com>
Message-ID: <1438282176970-4710598.post@n4.nabble.com>

Thanks Mom! 

"You still don't get it, do you? 

First, the list is not "John". 

Second, and more importantly, those of us with threading mail programs (or
"group by conversation" in Apple-speak) can fairly easily find Johns letter
if we haven't deleted it yet. Those without threading will have to look back
through the last 5 hours of mail to find it, which is probably not
unbearable, but imagine what happens when people do the same thing with
posts from 2012. 

And no, it is NOT reasonable to force readers to go to Nabble to find out
what the h*ck you are on about! 

There has been talk about making the Nabble gateways read-only. Hopefully,
that will happen sooner rather than later. 

-pd"



--
View this message in context: http://r.789695.n4.nabble.com/Daily-Category-Revenue-Stacked-Bar-Chart-in-ggplot2-tp4710431p4710598.html
Sent from the R help mailing list archive at Nabble.com.


From dwinsemius at comcast.net  Thu Jul 30 21:51:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 30 Jul 2015 12:51:53 -0700
Subject: [R] Using latticeExtra as.layer function with different number
	of plot panels
In-Reply-To: <55BA44D3.3080605@cognigencorp.com>
References: <55BA44D3.3080605@cognigencorp.com>
Message-ID: <461A0D41-22E8-4141-BDB9-C6A340A0F0F9@comcast.net>


On Jul 30, 2015, at 8:37 AM, sbihorel wrote:

> Hi,
> 
> When the as.layer function is used to overaly 2 lattice plots, there seems to be an assumption that the data used in both plots will generate the same number of panels (and, I believe, in the same order). In case the data used in the plot within the as.layer call is incomplete , data may be plotted on the "wrong" panel, and data seem to get re-used on the last panel(s). See what happens in the example code below when the records with state.region=="South" are dropped...
> 
> Is there a trick to overlay panel based upon the conditioning variable value rather than the panel order?
> 
> require(lattice)
> require(latticeExtra)
> state2 <- state <- data.frame(state.x77,state.region)
> state2$Income <- sample(state2$Income)
> state3 <- state2[which(state2$state.region!="South"),]
> foo <- xyplot(Income~Population|state.region,data=state,main='foo')
> foo
> 
> bar <- update(foo,main='bar') + as.layer(xyplot(Income~Population|state.region,data=state2,col='red'))
> bar
> 
> bar2 <- update(foo,main='bar2') + as.layer(xyplot(Income~Population|state.region,data=state3,col='red'))
> bar2

I don't know if this works using the `+.lattice` function but it is possible to selectively update panels using `trellis.focus`

-- 
David.
> 
> Thank you
> 
> Sebastien
> 
> PS: I know that I could get what I want by setting the Income variable to NA for records with state.region=="South" instead of dropping them... but this is not the point of my example. I am just trying to illustrate what happens when as.layer is used for plotting data with inconsistent dimensions.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dpmeddings at gmail.com  Thu Jul 30 22:02:21 2015
From: dpmeddings at gmail.com (Daniel Meddings)
Date: Thu, 30 Jul 2015 21:02:21 +0100
Subject: [R] How to simulate informative censoring in a Cox PH model?
In-Reply-To: <CAFEqCdxb9_Lv-_DoYVM26F=wOW_cJnn-FRtYubQhkbL8gWpjbw@mail.gmail.com>
References: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>
	<CAFEqCdzk8cSwed-VdBrob1bzLQtx7rknK9UkNEU_bwt8auoD7A@mail.gmail.com>
	<CABrUXPVi2=GnEg14xZQ19XjJOnuW=61NVZc+Vkbfo2HDN08L8g@mail.gmail.com>
	<CAFEqCdxb9_Lv-_DoYVM26F=wOW_cJnn-FRtYubQhkbL8gWpjbw@mail.gmail.com>
Message-ID: <CABrUXPWCVc_vJYqPBCkmL-30KQKAP_Hp+=2K5M0SeARU5eq+=w@mail.gmail.com>

Thanks Greg once more for taking the time to reply. I certainly agree that
this is not a simple set-up, although it is realistic I think. In short you
are correct about model mis-specification being the key to producing more
biased estimates under informative than under non-informative censoring. After
looking again at my code and trying various things I realize that the key
factor that leads to the informative and non-informative censoring data
giving rise to the same biased estimates is how I generate my Z_i variable,
and also the magnitude of the Z_i coefficient in both of the event and
informative censoring models.

In the example I gave I generated Z_i (I think of this as a "poor
prognosis" variable) from a beta distribution so that it ranged from 0-1.
The biased estimates for "beta_t_1" (I think of this as the effect of a
treatment on survival) were approximately 1.56 when the true value was -1.
What I forgot to mention was that estimating a cox model with 1,000,000
subjects to the full data (i.e. no censoring at all) arguably gives the
best treatment effect estimate possible given that the effects of Z_i and
Z_i*Treat_i are not in the model. This "best possible" estimate turns out
to be 1.55 - i.e. the example I gave just so happens to be such that even
with 25-27% censoring, the estimates obtained are almost the best that can
be attained.

My guess is that the informative censoring does not bias the estimate more
than non-informative censoring because the only variable not accounted for
in the model is Z_i which does not have a large enough effect "beta_t_2",
and/or "beta_c_2", or perhaps because Z_i only has a narrow range which
does not permit the current "beta_t_2" value to do any damage?

To investigate the "beta_t_2", and/or "beta_c_2" issue I changed "beta_c_2"
from 2 to 7 and "beta_c_0" from 0.2 to -1.2, and "beta_d_0" from -0.7 to
-0.4 to keep the censoring %'s equal at about 30%. This time the "best
possible" estimate of "beta_t_1" was -1.53 which was similar to that
obtained previously. The informative censoring gave an estimate for
"beta_t_1" of -1.49 whereas the non-informative censoring gave -1.53 - this
time the non-informative censoring attains the "best possible" but the
non-informative censoring does not.



I then instead changed "beta_t_2" from 1 to 7 and "beta_c_0" from 0.2 to 2,
and "beta_d_0" from -0.7 to -1.9 again to keep the censoring %'s equal at
about 30%. This time the "best possible" estimate of "beta_t_1" was -0.999
which is pretty much equal to the true value of -1. The informative
censoring gave an estimate for "beta_t_1" of -1.09 whereas the
non-informative censoring gave -0.87 ? surprisingly this time the
informative censoring is slightly closer to the ?best possible? than the
non-informative censoring.



To investigate the Z_i issue I generated it from a normal distribution with
mean 1 and variance 1. I changed "beta_c_0 " from 0.2 to -0.5 to keep the
censoring levels equal (this time about 30% for both). This time the "best
possible" estimate was -1.98 which was further from -1 than previous
examples. The informative censoring gave an estimate for "beta_t_1" of
-1.81 whereas the non-informative censoring gave -1.84. So again the
informative censoring gives an estimate closer to the "best possible" when
compared with the informative censoring, but this time it does not attain
the "best possible".

In conclusion it is clear to me that a stronger Z_i effect in the censoring
model causes the informative censoring to be worse than the non-informative
one (as expected), but a stronger Z_i effect in the event model does not
have this effect and even causes the independent censoring to be worse ?
this in general may not hold but I nonetheless see it here. I am wondering
if this is because altering the treatment effect in the event model also
affects the independent censoring process and so it ?muddies the waters?
whereas altering the treatment effect in the informative censoring model
obviously confines the changes to just the informative censoring process.
For a fixed treatment effect size in both the event and informative
censoring models the effect of Z_i having a wider range than is possible
under the beta distribution also appears to produce informative censoring
that is worse than the non-informative one. This makes sense I think
because the Z_i-response relationship must be more informative?



Thanks for your suggestion of copulas ? I have not come across these. Is
this similar to assuming a event model for censored subjects (this is
unobserved) ? i.e. if the event model is different conditional on censoring
then if we could observe the events beyond censoring then clearly the
parameter estimates would be different compared to those obtained when
modelling only non-censored times?



On Wed, Jul 29, 2015 at 5:37 PM, Greg Snow <538280 at gmail.com> wrote:

> As models become more complex it becomes harder to distinguish
> different parts and their effects.  Even for a straight forward linear
> regression model if X1 and X2 are correlated with each other then it
> becomes difficult to distinguish between the effects of X1^2, X2^2,
> and X1*X2.  In your case the informative censoring and model
> misspecification are becoming hard to distinguish (and it could be
> argued that having informative censoring is really just a form of
> model misspecification).  So I don't think so much that you are doing
> things wrong, just that you are learning that the models are complex.
>
> Another approach to simulation that you could try is to simulate the
> event time and censoring time using copulas (and therefore they can be
> correlated to give informative censoring, but without relying on a
> term that you could have included in the model) then consider the
> event censored if the censoring time is shorter.
>
> On Fri, Jul 24, 2015 at 10:14 AM, Daniel Meddings <dpmeddings at gmail.com>
> wrote:
> > Hi Greg
> >
> > Many thanks for taking the time to respond to my query. You are right
> about
> > pointing out the distinction between what variables are and are not
> included
> > in the event times process and in the censoring process. I clearly forgot
> > this important aspect. I amended my code to do as you advise and now I am
> > indeed getting biased estimates when using the informatively censored
> > responses. The problem is now that the estimates from the independently
> > censored responses are the same - i.e. they are just as biased. Thus the
> > bias seems to be due entirely to model mis-specification and not the
> > informative censoring.
> >
> >
> > To give a concrete example I simulate event times T_i and censoring times
> > C_i from the following models;
> >
> >
> > T_i~ Weibull(lambda_t(x),v_t),    lambda_t(x)=lambda_t*exp( beta_t_0 +
> > (beta_t_1*Treat_i) + (beta_t_2*Z_i) + (beta_t_3*Treat_i*Z_i)  )
> >
> > C_i~ Weibull(lambda_c(x),v_c),    lambda_c(x)=lambda_c*exp( beta_c_0 +
> > (beta_c_1*Treat_i) + (beta_c_2*Z_i) + (beta_c_3*Treat_i*Z_i)  )
> >
> > D_i~Weibull(lambda_d(x),v_D), lambda_d(x)=lamda_d*exp( beta_d_0)
> >
> > where ;
> >
> > beta_t_0 = 1,  beta_t_1 = -1,   beta_t_2 = 1,  beta_t_3 = -2,
>  lambda_t=0.5
> >
> > beta_c_0 = 0.2,  beta_c_1 = -2,   beta_c_2 = 2,  beta_c_3 = -2,
> > lambda_c=0.5
> >
> > beta_d_0 = -0.7,  lambda_d=0.1
> >
> > When I fit the cox model to both the informatively censored responses and
> > the independent censored responses I include only the Treatment
> covariate in
> > the model.
> >
> > I simulate Treatment from a Bernoulli distribution with p=0.5 and Z_i
> from a
> > beta distribution so that Z ranges from 0 to 1 (I like to think of Z as a
> > "poor" prognosis probability where Z_i=1 means a subject is 100% certain
> to
> > have a poor prognosis and Z_i=0 means zero chance). These parameter
> choices
> > give approximately 27% and 25% censoring for the informatively censored
> > responses (using C_i) and the independent censored responses (using D_i)
> > respectively. I use N=2000 subjects and 2000 simulation replications.
> >
> > The above simulation I get estimates of beta_t_2 of -1.526 and -1.537 for
> > the informatively censored responses and the independent censored
> responses
> > respectively.
> >
> > Furthermore when I fit a cox model to the full responses (no censoring at
> > all) I get an estimate of beta_t_2 of -1.542. This represents the best
> that
> > can possibly be done given that Z and Treat*Z are not in the model.
> Clearly
> > censoring is not making much of a difference here - model
> mis-specification
> > dominates.
> >
> > I still must be doing something wrong but I cannot figure this one out.
> >
> > Thanks
> >
> > Dan
> >
> >
> >
> > On Thu, Jul 23, 2015 at 12:33 AM, Greg Snow <538280 at gmail.com> wrote:
> >>
> >> I think that the Cox model still works well when the only information
> >> in the censoring is conditional on variables in the model.  What you
> >> describe could be called non-informative conditional on x.
> >>
> >> To really see the difference you need informative censoring that
> >> depends on something not included in the model.  One option would be
> >> to use copulas to generate dependent data and then transform the
> >> values using your Weibul.  Or you could generate your event times and
> >> censoring times based on x1 and x2, but then only include x1 in the
> >> model.
> >>
> >> On Wed, Jul 22, 2015 at 2:20 AM, Daniel Meddings <dpmeddings at gmail.com>
> >> wrote:
> >> > I wish to simulate event times where the censoring is informative, and
> >> > to
> >> > compare parameter estimator quality from a Cox PH model with estimates
> >> > obtained from event times generated with non-informative censoring.
> >> > However
> >> > I am struggling to do this, and I conclude rather than a technical
> flaw
> >> > in
> >> > my code I instead do not understand what is meant by informative and
> >> > un-informative censoring.
> >> >
> >> > My approach is to simulate an event time T dependent on a vector of
> >> > covariates x having hazard function
> h(t|x)=lambda*exp(beta'*x)v*t^{v-1}.
> >> > This corresponds to T~ Weibull(lambda(x),v), where the scale parameter
> >> > lambda(x)=lambda*exp(beta'*x) depends on x and the shape parameter v
> is
> >> > fixed. I have N subjects where T_{i}~ Weibull(lambda(x_{i}),v_{T}),
> >> > lambda(x_{i})=lambda_{T}*exp(beta_{T}'*x_{i}), for i=1,...,N. Here I
> >> > assume
> >> > the regression coefficients are p-dimensional.
> >> >
> >> > I generate informative censoring times C_i~ Weibull(lambda(x_i),v_C),
> >> > lambda(x_i)=lambda_C*exp(beta_C'*x_i) and compute Y_inf_i=min(T_i,C_i)
> >> > and
> >> > a censored flag delta_inf_i=1 if Y_inf_i <= C_i (an observed event),
> and
> >> > delta_inf_i=0 if Y_inf_i > C_i (informatively censored: event not
> >> > observed). I am convinced this is informative censoring because as
> long
> >> > as
> >> > beta_T~=0 and beta_C~=0 then for each subject the data generating
> >> > process
> >> > for T and C both depend on x.
> >> >
> >> > In contrast I generate non-informative censoring times
> >> > D_i~Weibull(lambda_D*exp(beta_D),v_D), and compute
> Y_ninf_i=min(T_i,D_i)
> >> > and a censored flag delta_ninf_i=1 if Y_ninf_i <= D_i (an observed
> >> > event),
> >> > and delta_ninf_i=0 if Y_ninf_i > D_i (non-informatively censored:
> event
> >> > not
> >> > observed). Here beta_D is a scalar. I "scale" the simulation by
> choosing
> >> > the lambda_T, lambda_C and lambda_D parameters such that on average
> >> > T_i<C_i
> >> > and T_i<D_i to achieve X% of censored subjects for both Y_inf_i and
> >> > Y_ninf_i.
> >> >
> >> > The problem is that even for say 30% censoring (which I think is
> high),
> >> > the
> >> > Cox PH parameter estimates using both Y_inf and Y_ninf are unbiased
> when
> >> > I
> >> > expected the estimates using Y_inf to be biased, and I think I see
> why:
> >> > however different beta_C is from beta_T, a censored subject can
> >> > presumably
> >> > influence the estimation of beta_T only by affecting the set of
> subjects
> >> > at
> >> > risk at any time t, but this does not change the fact that every
> single
> >> > Y_inf_i with delta_inf_i=1 will have been generated using beta_T only.
> >> > Thus
> >> > I do not see how my simulation can possibly produce biased estimates
> for
> >> > beta_T using Y_inf.
> >> >
> >> > But then what is informative censoring if not based on this approach?
> >> >
> >> > Any help would be greatly appreciated.
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >>
> >>
> >>
> >> --
> >> Gregory (Greg) L. Snow Ph.D.
> >> 538280 at gmail.com
> >
> >
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>

	[[alternative HTML version deleted]]


From evan.kransdorf at gmail.com  Thu Jul 30 22:31:45 2015
From: evan.kransdorf at gmail.com (Evan Kransdorf)
Date: Thu, 30 Jul 2015 13:31:45 -0700
Subject: [R] Shiny help with verbatimTextOutput
Message-ID: <CAKZWb7fCVP+t031CYSpamzsm36NFzS3XiCXbeRqLu_k+pub0XQ@mail.gmail.com>

Hello,

I am working with Shiny and want to use it to show user-entered input from
a check box.  However, when using verbatimTextOutput with no options
selected in the check box I get "character(0)" as the output. Is there any
way to make this so it displays nothing instead?
Thanks

#Begin R code
#ui.R

checkboxGroupInput("string", label = h5("test"), choices = list("opt1" = 1,
"opt2" = 2)),

h5("User input goes here"),verbatimTextOutput("o"),

#server.R

output$o <- renderPrint({ paste(string) })

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Jul 30 22:38:59 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 30 Jul 2015 13:38:59 -0700
Subject: [R] Shiny help with verbatimTextOutput
In-Reply-To: <CAKZWb7fCVP+t031CYSpamzsm36NFzS3XiCXbeRqLu_k+pub0XQ@mail.gmail.com>
References: <CAKZWb7fCVP+t031CYSpamzsm36NFzS3XiCXbeRqLu_k+pub0XQ@mail.gmail.com>
Message-ID: <CAGxFJbS17k5RMYH-6dh14VgVLVuZMx=qeyR=qZg0eLpCwkzLYg@mail.gmail.com>

Shiny is not R. It is an RStudio product, which is separate from R. So
I think you need to post on RStudio's support forum, not here.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Jul 30, 2015 at 1:31 PM, Evan Kransdorf
<evan.kransdorf at gmail.com> wrote:
> Hello,
>
> I am working with Shiny and want to use it to show user-entered input from
> a check box.  However, when using verbatimTextOutput with no options
> selected in the check box I get "character(0)" as the output. Is there any
> way to make this so it displays nothing instead?
> Thanks
>
> #Begin R code
> #ui.R
>
> checkboxGroupInput("string", label = h5("test"), choices = list("opt1" = 1,
> "opt2" = 2)),
>
> h5("User input goes here"),verbatimTextOutput("o"),
>
> #server.R
>
> output$o <- renderPrint({ paste(string) })
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From anshuk.p at motivitylabs.com  Thu Jul 30 17:45:31 2015
From: anshuk.p at motivitylabs.com (Anshuk Pal Chaudhuri)
Date: Thu, 30 Jul 2015 15:45:31 +0000
Subject: [R] Itinerary Ticket Parser
Message-ID: <HKXPR02MB063203DD0984613352B7A4CCF08B0@HKXPR02MB0632.apcprd02.prod.outlook.com>

Dear All,

I have seeing a lot of apis', (like worldmate or new product called sift from easilydo) which is used for parsing email and different itinerary tickets. Is there any packages in R which does that?

Regards,
Anshuk Pal Chaudhuri


	[[alternative HTML version deleted]]


From fanjianling at gmail.com  Thu Jul 30 21:51:22 2015
From: fanjianling at gmail.com (Jianling Fan)
Date: Thu, 30 Jul 2015 13:51:22 -0600
Subject: [R] About nls.
Message-ID: <CAJ7mryJzSkM6NpP2J5dO3wZZ1--VEO-w4CZsrujFKWFyEs29jQ@mail.gmail.com>

Hello,

I am trying to do a nls regression with R.  but I always get a error
as "Error in numericDeriv(form[[3L]], names(ind), env) :  Missing
value or an infinity produced when evaluating the model".  I googled
it and found someone said it is because of the improper start value. I
tried many times but can not solve it. Does anyone can help me?

thanks a lot !

my code is:

fit1<-nls2(lnd~log(1/(den1/R1+den2+den3+den4+den5/R5)-1)/c+log(d50),
              start=c(R1=0.9, R5=23, c=-1.1, d50=10), data=SWrt)

data (SWrt) is:

  Depth      lnd   den1 den2 den3 den4   den5
1     20 2.995732 0.4190 0.00 0.00 0.00   0.00
2     40 3.688879 0.6725 0.00 0.00 0.00   0.00
3     60 4.094345 0.8780 0.00 0.00 0.00   0.00
4     80 4.382027 0.9660 0.00 0.00 0.00   0.00
5    100 4.605170 0.9990 0.00 0.00 0.00   0.00
6     15 2.708050 0.0000 0.38 0.00 0.00   0.00
7     30 3.401197 0.0000 0.48 0.00 0.00   0.00
8     45 3.806662 0.0000 0.85 0.00 0.00   0.00
9     60 4.094345 0.0000 0.88 0.00 0.00   0.00
10   120 4.787492 0.0000 1.00 0.00 0.00   0.00
11    15 2.708050 0.0000 0.00 0.48 0.00   0.00
12    30 3.401197 0.0000 0.00 0.89 0.00   0.00
13    45 3.806662 0.0000 0.00 0.98 0.00   0.00
14    60 4.094345 0.0000 0.00 0.99 0.00   0.00
15   120 4.787492 0.0000 0.00 1.00 0.00   0.00
16    15 2.708050 0.0000 0.00 0.00 0.20   0.00
17    30 3.401197 0.0000 0.00 0.00 0.39   0.00
18    45 3.806662 0.0000 0.00 0.00 0.69   0.00
19    60 4.094345 0.0000 0.00 0.00 0.99   0.00
20   120 4.787492 0.0000 0.00 0.00 1.00   0.00
21    10 2.302585 0.0000 0.00 0.00 0.00 139.50
22    30 3.401197 0.0000 0.00 0.00 0.00 227.85
23    50 3.912023 0.0000 0.00 0.00 0.00 306.90
24    70 4.248495 0.0000 0.00 0.00 0.00 381.30
25    90 4.499810 0.0000 0.00 0.00 0.00 432.45
26   110 4.700480 0.0000 0.00 0.00 0.00 455.70


From luca.gaglia at gmail.com  Thu Jul 30 21:56:22 2015
From: luca.gaglia at gmail.com (Luca Gagliardone)
Date: Thu, 30 Jul 2015 21:56:22 +0200
Subject: [R] Problem with package plm
Message-ID: <CAJW7ayNtnMi4DULj2sNKkJnUgXEmc4CtcdRHG7wh3JT_jZwFbw@mail.gmail.com>

Hi!

I am trying yo run a regression using the plm package.
I tried in two different ways, that should give the same result, but
eventually do not; and i cannot understand the reason.

- The first way:
Accellerator.model <- read.delim("~/Desktop/Tesi/Accellerator model.txt")
data <- Accellerator.model
Panel <- plm.data(Accellerator.model,index=c("ID","Year"))
y <- data$Net.Cap.Inv
p  <- data$Oil.Price
rev <- data$Revenue
ebi <- data$EBITDA
tas <- data$Tot.Assets
fas <- data$Non.current
tas1 <- data$tas1
rev1 <- data$rev1
rev2 <- data$rev2
fas1 <- data$fas1
cap1 <- data$cap1
# FIXED 1
fixed <- plm(y~fas1+rev+rev1+p,data=pdata,model="within")
summary(fixed)
Oneway (individual) effect Within Model

Call:
plm(formula = y ~ fas1 + rev + rev1 + p, data = pdata, model = "within")

Unbalanced Panel: n=19, T=6-19, N=282

Residuals :
    Min.  1st Qu.   Median  3rd Qu.     Max.
-91.6000  -8.4600   0.0711   7.1000 110.0000

Coefficients :
       Estimate Std. Error t-value  Pr(>|t|)
fas1 -0.0231717  0.0096980 -2.3893  0.017595 *
rev   0.0922257  0.0035990 25.6251 < 2.2e-16 ***
rev1  0.0127055  0.0058961  2.1549  0.032093 *
p     0.1587968  0.0579552  2.7400  0.006571 **
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Total Sum of Squares:    804760
Residual Sum of Squares: 178290
R-Squared      :  0.77845
      Adj. R-Squared :  0.71496
F-statistic: 227.513 on 4 and 259 DF, p-value: < 2.22e-16
>


- The second way:
Accellerator.model <- read.delim("~/Desktop/Tesi/Accellerator model.txt")
data <- plm.data(Accellerator.model,index=c("ID","Year"))
data$y <- data$Net.Cap.Inv
data$p  <- data$Oil.Price
data$rev <- data$Revenue
data$ebi <- data$EBITDA
data$tas <- data$Tot.Assets
data$fas <- data$Non.current
Panel <- data
fixed <- plm(y~fas1+rev+rev1+p,data=Panel,model="within")
summary(fixed)
Oneway (individual) effect Within Model

Call:
plm(formula = y ~ fas1 + rev + rev1 + p, data = Panel, model = "within")

Unbalanced Panel: n=19, T=9-19, N=296

Residuals :
   Min. 1st Qu.  Median 3rd Qu.    Max.
-75.200  -3.710  -0.604   3.150  80.000

Coefficients :
      Estimate Std. Error t-value  Pr(>|t|)
fas1 0.0892306  0.0066033 13.5131 < 2.2e-16 ***
rev  0.0120747  0.0111825  1.0798    0.2812
rev1 0.0419186  0.0095914  4.3704 1.764e-05 ***
p    0.0078837  0.0375021  0.2102    0.8337
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Total Sum of Squares:    774610
Residual Sum of Squares: 85942
R-Squared      :  0.88905
      Adj. R-Squared :  0.81997
F-statistic: 546.893 on 4 and 273 DF, p-value: < 2.22e-16

As you can see, the results are totally different, and I have no
explanation for that!
Thanks for the help

Regards.
Luca.

	[[alternative HTML version deleted]]


From adam.michael.erickson at gmail.com  Fri Jul 31 01:11:07 2015
From: adam.michael.erickson at gmail.com (Adam Erickson)
Date: Thu, 30 Jul 2015 16:11:07 -0700 (PDT)
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <08b66b82-a0a4-49b7-b7cd-bda951fb8f29@googlegroups.com>
References: <c70d3e6e-1154-46d2-a02d-6f0a809deebb@googlegroups.com>
	<108888426.2209755.1438120846276.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQ7uE4_VGdmCmwavwmD8sfNFpz+KtY-DFg1KHMjUQEXaw@mail.gmail.com>
	<08b66b82-a0a4-49b7-b7cd-bda951fb8f29@googlegroups.com>
Message-ID: <e484d1d6-e29b-48ba-8123-9188841aebec@googlegroups.com>

Here is a Rcpp version for exact character matching (for example) written 
in C++ that is substantially faster. Hence, I think this is the way to go 
where loops may be unavoidable. However, the input vector length has to 
match the length of the pattern and replacement vectors, as your original 
code did. That can be changed though.

#include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
CharacterVector subCPP(CharacterVector pattern, CharacterVector 
replacement, CharacterVector x) {
  int len = x.size();
  CharacterVector y(len);
  int patlen = pattern.size();
  int replen = replacement.size();
  if (patlen != replen)
    Rcout<<"Error: Pattern and replacement length do not match";
  for(int i = 0; i < patlen; ++i) {
    if (*(char*)x[i] == *(char*)pattern[i])
      y[x[i] == pattern[i]] = replacement[i];
  }
  return y;
}

""   "CD" ""  

system.time(for(i in 1:50000) subCPP(patt, repl, X))
   user  system elapsed 
   0.16    0.00    0.16 

Cheers,

Adam

On Wednesday, July 29, 2015 at 2:42:23 PM UTC-7, Adam Erickson wrote:
>
> Further refining the vectorized (within a loop) exact string match 
> function, I get times below 0.9 seconds while maintaining error checking. 
> This is accomplished by removing which() and replacing 1:length() with 
> seq_along().
>
> sub2 <- function(pattern, replacement, x) {
>    len    <- length(x)
>    y      <- character(length=len)
>    patlen <- length(pattern)
>    replen <- length(replacement)
>    if(patlen != replen) stop('Error: Pattern and replacement length do not 
> match')
>    for(i in seq_along(pattern)) {
>      y[x==pattern[i]] <- replacement[i]
>    }
>    return(y)
>  }
>
> system.time(for(i in 1:50000) sub2(patt, repl, X))
>    user  system elapsed 
>    0.86    0.00    0.86 
>
> Since the ordered vectors are perfectly aligned, might as well do an exact 
> string match. Hence, I think this is not off-topic.
>
> Cheers,
>
> Adam
>
> On Wednesday, July 29, 2015 at 8:15:52 AM UTC-7, Bert Gunter wrote:
>>
>> There is confusion here. apply() family functions are **NOT** 
>> vectorization -- they ARE loops (at the interpreter level), just done 
>> in "functionalized" form. Please read background material (John 
>> Chambers's books, MASS, or numerous others) to improve your 
>> understanding and avoid posting erroneous comments. 
>>
>> Cheers, 
>> Bert 
>>
>>
>> Bert Gunter 
>>
>> "Data is not information. Information is not knowledge. And knowledge 
>> is certainly not wisdom." 
>>    -- Clifford Stoll 
>>
>>
>> On Tue, Jul 28, 2015 at 3:00 PM, John Thaden <jjth... at flash.net> wrote: 
>> > Adam,    The method you propose gives a different result than the prior 
>> methods for these example vectors 
>> > X <- c("ab", "cd", "ef") 
>> > patt <- c("b", "cd", "a") 
>> > repl <- c("B", "CD", "A") 
>> > 
>> > Old method 1 
>> > 
>> > mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, 
>> x=X) 
>> > gives 
>> >   b   cd    a 
>> > "aB" "CD" "ef" 
>> > 
>> > Old method 2 
>> > 
>> > sub2 <- function(pattern, replacement, x) { 
>> >     len <- length(x) 
>> >     if (length(pattern) == 1) 
>> >         pattern <- rep(pattern, len) 
>> >     if (length(replacement) == 1) 
>> >         replacement <- rep(replacement, len) 
>> >     FUN <- function(i, ...) { 
>> >         sub(pattern[i], replacement[i], x[i], fixed = TRUE) 
>> >     } 
>> >     idx <- 1:length(x) 
>> >     sapply(idx, FUN) 
>> > } 
>> > sub2(patt, repl, X) 
>> >  gives 
>> > [1] "aB" "CD" "ef" 
>> > 
>> > Your method (I gave it the unique name "sub3") 
>> >  sub3 <- function(pattern, replacement, x) {   len    <- length(x)  y   
>>    <- character(length=len)  patlen <- length(pattern)  replen <- 
>> length(replacement)  if(patlen != replen) stop('Error: Pattern and 
>> replacement length do not match')  for(i in 1:replen) {   
>>  y[which(x==pattern[i])] <- replacement[i]  }  return(y)}sub3(patt, repl, 
>> X) 
>> > gives[1] ""   "CD" "" 
>> > 
>> > Granted, whatever it does, it does it faster 
>> > #Old method 1 
>> > system.time(for(i in 1:50000) 
>> > mapply(function(p,r,x) sub(p,r,x, fixed = TRUE),p=patt,r=repl,x=X)) 
>> >    user  system elapsed 
>> >    2.53    0.00    2.52 
>> > 
>> > #Old method 2 
>> > system.time(for(i in 1:50000)sub2(patt, repl, X))   user  system 
>> elapsed 
>> >    2.32    0.00    2.32 
>> > 
>> > #Your proposed method 
>> > system.time(for(i in 1:50000) sub3(patt, repl, X)) 
>> >    user  system elapsed 
>> >    1.02    0.00    1.01 
>> >  but would it still be faster if it actually solved the same problem? 
>> > 
>> > -John Thaden 
>> > 
>> > 
>> > 
>> > 
>> >      On Monday, July 27, 2015 11:40 PM, Adam Erickson <
>> adam.micha... at gmail.com> wrote: 
>> > 
>> > I know this is an old thread, but I wrote a simple FOR loop with 
>> vectorized pattern replacement that is much faster than either of those (it 
>> can also accept outputs differing in length from the patterns): 
>> >   sub2  <- function(pattern, replacement, x) {     len   <- length(x)   
>>  y      <- character(length=len)    patlen <- length(pattern)    replen <- 
>> length(replacement)    if(patlen != replen) stop('Error: Pattern and 
>> replacement length do not match')    for(i in 1:replen) {     
>>  y[which(x==pattern[i])] <- replacement[i]    }    return(y)  } 
>> > system.time(test <- sub2(patt, repl, XX))   user  system elapsed       
>> 0       0       0 
>> > Cheers, 
>> > Adam 
>> > On Wednesday, October 8, 2008 at 9:38:01 PM UTC-7, john wrote: 
>> > Hello Christos, 
>> >   To my surprise, vectorization actually hurt processing speed!#Example 
>> > X <- c("ab", "cd", "ef") 
>> > patt <- c("b", "cd", "a") 
>> > repl <- c("B", "CD", "A")sub2 <- function(pattern, replacement, x) { 
>> >     len <- length(x) 
>> >     if (length(pattern) == 1) 
>> >         pattern <- rep(pattern, len) 
>> >     if (length(replacement) == 1) 
>> >         replacement <- rep(replacement, len) 
>> >     FUN <- function(i, ...) { 
>> >         sub(pattern[i], replacement[i], x[i], fixed = TRUE) 
>> >     } 
>> >     idx <- 1:length(x) 
>> >     sapply(idx, FUN) 
>> > } 
>> > 
>> > system.time(  for(i in 1:10000)  sub2(patt, repl, X)  ) 
>> >    user  system elapsed 
>> >    1.18    0.07    1.26 system.time(  for(i in 1:10000) 
>>  mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, x=X) 
>>  ) 
>> >    user  system elapsed 
>> >    1.42    0.05    1.47 
>> > 
>> > So much for avoiding loops. 
>> > John Thaden======= At 2008-10-07, 14:58:10 Christos wrote: 
>> =======>John, 
>> >>Try the following: 
>> >> 
>> >> mapply(function(p, r, x) sub(p, r, x, fixed = TRUE), p=patt, r=repl, 
>> x=X) 
>> >>   b   cd    a 
>> >>"aB" "CD" "ef" 
>> >> 
>> >>-Christos>> -----My Original Message----- 
>> >>> R pattern-matching and replacement functions are 
>> >>> vectorized: they can operate on vectors of targets. 
>> >>> However, they can only use one pattern and replacement. 
>> >>> Here is code to apply a different pattern and replacement for 
>> >>> every target.  My question: can it be done better? 
>> >>> 
>> >>> sub2 <- function(pattern, replacement, x) { 
>> >>>     len <- length(x) 
>> >>>     if (length(pattern) == 1) 
>> >>>         pattern <- rep(pattern, len) 
>> >>>     if (length(replacement) == 1) 
>> >>>         replacement <- rep(replacement, len) 
>> >>>     FUN <- function(i, ...) { 
>> >>>         sub(pattern[i], replacement[i], x[i], fixed = TRUE) 
>> >>>     } 
>> >>>     idx <- 1:length(x) 
>> >>>     sapply(idx, FUN) 
>> >>> } 
>> >>> 
>> >>> #Example 
>> >>> X <- c("ab", "cd", "ef") 
>> >>> patt <- c("b", "cd", "a") 
>> >>> repl <- c("B", "CD", "A") 
>> >>> sub2(patt, repl, X) 
>> >>> 
>> >>> -John______________________________________________ 
>> > R-h... at r-project.org mailing list 
>> > https://stat.ethz.ch/mailman/listinfo/r-help 
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html 
>> > and provide commented, minimal, self-contained, reproducible code. 
>> > 
>> > 
>> > 
>> > 
>> >         [[alternative HTML version deleted]] 
>> > 
>> > ______________________________________________ 
>> > R-h... at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> > https://stat.ethz.ch/mailman/listinfo/r-help 
>> > PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html 
>> > and provide commented, minimal, self-contained, reproducible code. 
>>
>> ______________________________________________ 
>> R-h... at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html 
>> and provide commented, minimal, self-contained, reproducible code. 
>>
>

From aprilgracesmith at gmail.com  Fri Jul 31 02:20:30 2015
From: aprilgracesmith at gmail.com (April Smith)
Date: Thu, 30 Jul 2015 17:20:30 -0700
Subject: [R] Looping help
Message-ID: <CAKcEf9C6CiwVFAOKd1G8xxRM=biOQYtrcEZ0fzNRME8_Rwqa7w@mail.gmail.com>

I have never looped before and know I need to.  I am unsure how to proceed:


   - Action I need done: d(Data[1,2:399], q=0, boot=TRUE,
   boot.arg=list(num.iter=1000))
   - I need this to happen to all rows, I need All[1,2:399] to increase to
   All[2:399], etc.
   - But I also need the results from q increasing from 0 to 0.25, 0.5, 1,
   2, 4,8,16,32,64 before the loop moves on to the next row.
   - For each iteration I will receive two values: D and st.err.  I need
   this put into a matrix


I feel like this should be pretty simple to learn, but I have never looped
before.

I am hoping to get more of a tutorial on how to write loop code, then to
just be given the loop code.

Thanks,
April

	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Fri Jul 31 05:53:16 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Fri, 31 Jul 2015 13:53:16 +1000
Subject: [R] Using latticeExtra as.layer function with different number
	of plot panels
In-Reply-To: <55BA44D3.3080605@cognigencorp.com>
References: <55BA44D3.3080605@cognigencorp.com>
Message-ID: <000601d0cb44$6f5dd760$4e198620$@bigpond.com>


David has replied

But I am wondering if the lattice function make.groups is what you  want.
As you seem to want extra panels.
For the 3 graphs I get the 4 same panels NC W NE S and coloured points are
added after foo but bar2 has more red points added
bar1 and bar2 have the same colour for pch

I'm not upto date with some of the latticeExtra functions so cannot really
comment otherwise.

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au
  
-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of sbihorel
Sent: Friday, 31 July 2015 01:38
To: r-help at r-project.org
Subject: [R] Using latticeExtra as.layer function with different number of
plot panels

Hi,

When the as.layer function is used to overaly 2 lattice plots, there 
seems to be an assumption that the data used in both plots will generate 
the same number of panels (and, I believe, in the same order). In case 
the data used in the plot within the as.layer call is incomplete , data 
may be plotted on the "wrong" panel, and data seem to get re-used on the 
last panel(s). See what happens in the example code below when the 
records with state.region=="South" are dropped...

Is there a trick to overlay panel based upon the conditioning variable 
value rather than the panel order?

require(lattice)
require(latticeExtra)
state2 <- state <- data.frame(state.x77,state.region)
state2$Income <- sample(state2$Income)
state3 <- state2[which(state2$state.region!="South"),]
foo <- xyplot(Income~Population|state.region,data=state,main='foo')
foo

bar <- update(foo,main='bar') + 
as.layer(xyplot(Income~Population|state.region,data=state2,col='red'))
bar

bar2 <- update(foo,main='bar2') + 
as.layer(xyplot(Income~Population|state.region,data=state3,col='red'))
bar2

Thank you

Sebastien

PS: I know that I could get what I want by setting the Income variable 
to NA for records with state.region=="South" instead of dropping them... 
but this is not the point of my example. I am just trying to illustrate 
what happens when as.layer is used for plotting data with inconsistent 
dimensions.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From jjthaden at flash.net  Fri Jul 31 06:09:17 2015
From: jjthaden at flash.net (John Thaden)
Date: Thu, 30 Jul 2015 21:09:17 -0700
Subject: [R] vectorized sub, gsub, grep, etc.
In-Reply-To: <e484d1d6-e29b-48ba-8123-9188841aebec@googlegroups.com>
Message-ID: <1438315757.49661.YahooMailAndroidMobile@web185401.mail.gq1.yahoo.com>

Can you show what is its solution for the original sample data? Why that discrepancy for you original sub2() function?


	[[alternative HTML version deleted]]


From prabir111 at gmail.com  Fri Jul 31 06:46:00 2015
From: prabir111 at gmail.com (prabir das)
Date: Fri, 31 Jul 2015 10:16:00 +0530
Subject: [R] dataframe for netcdf data
Message-ID: <CAN_n9hZ7KnYYHm84hR1-bu0TMVZYuT_FftPL9ODBJJmNJib3KQ@mail.gmail.com>

I am trying to analyse time-series .netcdf (3D lat,long and time domain)
climate data. I want to apply the SPEI package (calculation of standardized
precipitation evapotranspiration index) on it. But unable to arrange my data
in the required data frame. As I am a beginner in R, it will be very much
helpful if someone provide me the details of the code to be written before
executing the package.

The details of SPEI proggrame is as follows:

spei(data, scale, kernel = list(type = 'rectangular', shift = 0),
distribution = 'log-Logistic', fit = 'ub-pwm', na.rm = FALSE,
ref.start=NULL, ref.end=NULL, x=FALSE, params=NULL, ...)


Thanks in advance.

Prabir

	[[alternative HTML version deleted]]


From mcooganj at gmail.com  Fri Jul 31 07:48:27 2015
From: mcooganj at gmail.com (Matthew Johnson)
Date: Fri, 31 Jul 2015 15:48:27 +1000
Subject: [R] setting up R -- VM Fusion, WIndows7
Message-ID: <CABCh89ovRNB+EQ5iZpN4wG4gfoxqOMiZkjD-LB+HqFpqBjAFWA@mail.gmail.com>

Hi,

As i need R to speak to Bloomberg (and big only runs on windows), i'm
running windows 7 via VM Fusion on my mac.

I think i am having permission problems, as i cannot use install.packages,
and cannot change .libPaths via either a .Rprofile, or Profile.site.

I've posted more detail in this super-user question --
http://superuser.com/questions/948083/how-to-set-environment-variables-in-vm-fusion-windows-7

Throwing it over to this list as well, as I've spent about half the time i
had allowed for my project on (not getting) set up.

I realise this is a very niche problem - hoping that someone else has had a
similar problem, and can offer pointers.

best

mj

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Fri Jul 31 08:38:35 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 31 Jul 2015 06:38:35 +0000
Subject: [R] Looping help
In-Reply-To: <CAKcEf9C6CiwVFAOKd1G8xxRM=biOQYtrcEZ0fzNRME8_Rwqa7w@mail.gmail.com>
References: <CAKcEf9C6CiwVFAOKd1G8xxRM=biOQYtrcEZ0fzNRME8_Rwqa7w@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C385E9@SRVEXCHMBX.precheza.cz>

Hi

Your question is a bit cloudy. Simple loop can be realised to populate lists


res<-vector(100, "list")
for (i in 1:100) {

lll <- do something based on i value

res[[i]] <- put lll in ith place of the list
}

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of April
> Smith
> Sent: Friday, July 31, 2015 2:21 AM
> To: r-help at r-project.org
> Subject: [R] Looping help
>
> I have never looped before and know I need to.  I am unsure how to
> proceed:
>
>
>    - Action I need done: d(Data[1,2:399], q=0, boot=TRUE,
>    boot.arg=list(num.iter=1000))
>    - I need this to happen to all rows, I need All[1,2:399] to increase
> to
>    All[2:399], etc.
>    - But I also need the results from q increasing from 0 to 0.25, 0.5,
> 1,
>    2, 4,8,16,32,64 before the loop moves on to the next row.
>    - For each iteration I will receive two values: D and st.err.  I
> need
>    this put into a matrix
>
>
> I feel like this should be pretty simple to learn, but I have never
> looped before.
>
> I am hoping to get more of a tutorial on how to write loop code, then
> to just be given the loop code.
>
> Thanks,
> April
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From anshuk.p at motivitylabs.com  Fri Jul 31 08:37:40 2015
From: anshuk.p at motivitylabs.com (Anshuk Pal Chaudhuri)
Date: Fri, 31 Jul 2015 06:37:40 +0000
Subject: [R] Itinerary Ticket Parser
In-Reply-To: <HKXPR02MB063203DD0984613352B7A4CCF08B0@HKXPR02MB0632.apcprd02.prod.outlook.com>
References: <HKXPR02MB063203DD0984613352B7A4CCF08B0@HKXPR02MB0632.apcprd02.prod.outlook.com>
Message-ID: <HKXPR02MB063280823197B291E117E5B6F08A0@HKXPR02MB0632.apcprd02.prod.outlook.com>

Dear All, 

Was trying to study similar kind of this and found out a post on one of stack overflow site: http://stackoverflow.com/questions/8438903/open-source-projects-for-email-scrubbing-generating-structured-data-from-unstruc 

I guess this is not really answered yet. But wanted to check the all R-enthusiasts, if something has been done or not. 

Regards,
Anshuk Pal Chaudhuri

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Anshuk Pal Chaudhuri
Sent: 30 July 2015 21:16
To: r-help at r-project.org
Subject: [R] Itinerary Ticket Parser

Dear All,

I have seeing a lot of apis', (like worldmate or new product called sift from easilydo) which is used for parsing email and different itinerary tickets. Is there any packages in R which does that?

Regards,
Anshuk Pal Chaudhuri


	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Fri Jul 31 08:54:50 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 31 Jul 2015 06:54:50 +0000
Subject: [R] About nls.
In-Reply-To: <CAJ7mryJzSkM6NpP2J5dO3wZZ1--VEO-w4CZsrujFKWFyEs29jQ@mail.gmail.com>
References: <CAJ7mryJzSkM6NpP2J5dO3wZZ1--VEO-w4CZsrujFKWFyEs29jQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C38602@SRVEXCHMBX.precheza.cz>

Hi

I am not an expert but the problem seems to me that

(den1/R1+den2+den3+den4+den5/R5)-1

gives you sometimes value 0 and sometimes negative. In these cases the value of log(1/result) is NA or Inf and nls can not handle this.

I do not search vhere is nls2 from so I used nls and removed -1 form your formula, which resulted to some final values.

> fit1<-nls(lnd~log(1/(den1/R1+den2+den3+den4+den5/R5))/c+log(d50),
+ start=c(R1=0.9, R5=23, c=-1.1, d50=10), data=test)
>
> coef(fit)
         A          B
 6.9720965 -0.0272203
> coef(fit1)
         R1          R5           c         d50
  0.9622249 416.1272498  -0.7178156  73.6017161
>
Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Jianling Fan
> Sent: Thursday, July 30, 2015 9:51 PM
> To: r-help at r-project.org
> Subject: [R] About nls.
>
> Hello,
>
> I am trying to do a nls regression with R.  but I always get a error as
> "Error in numericDeriv(form[[3L]], names(ind), env) :  Missing value or
> an infinity produced when evaluating the model".  I googled it and
> found someone said it is because of the improper start value. I tried
> many times but can not solve it. Does anyone can help me?
>
> thanks a lot !
>
> my code is:
>
> fit1<-nls2(lnd~log(1/(den1/R1+den2+den3+den4+den5/R5)-1)/c+log(d50),
>               start=c(R1=0.9, R5=23, c=-1.1, d50=10), data=SWrt)
>
> data (SWrt) is:
>
>   Depth      lnd   den1 den2 den3 den4   den5
> 1     20 2.995732 0.4190 0.00 0.00 0.00   0.00
> 2     40 3.688879 0.6725 0.00 0.00 0.00   0.00
> 3     60 4.094345 0.8780 0.00 0.00 0.00   0.00
> 4     80 4.382027 0.9660 0.00 0.00 0.00   0.00
> 5    100 4.605170 0.9990 0.00 0.00 0.00   0.00
> 6     15 2.708050 0.0000 0.38 0.00 0.00   0.00
> 7     30 3.401197 0.0000 0.48 0.00 0.00   0.00
> 8     45 3.806662 0.0000 0.85 0.00 0.00   0.00
> 9     60 4.094345 0.0000 0.88 0.00 0.00   0.00
> 10   120 4.787492 0.0000 1.00 0.00 0.00   0.00
> 11    15 2.708050 0.0000 0.00 0.48 0.00   0.00
> 12    30 3.401197 0.0000 0.00 0.89 0.00   0.00
> 13    45 3.806662 0.0000 0.00 0.98 0.00   0.00
> 14    60 4.094345 0.0000 0.00 0.99 0.00   0.00
> 15   120 4.787492 0.0000 0.00 1.00 0.00   0.00
> 16    15 2.708050 0.0000 0.00 0.00 0.20   0.00
> 17    30 3.401197 0.0000 0.00 0.00 0.39   0.00
> 18    45 3.806662 0.0000 0.00 0.00 0.69   0.00
> 19    60 4.094345 0.0000 0.00 0.00 0.99   0.00
> 20   120 4.787492 0.0000 0.00 0.00 1.00   0.00
> 21    10 2.302585 0.0000 0.00 0.00 0.00 139.50
> 22    30 3.401197 0.0000 0.00 0.00 0.00 227.85
> 23    50 3.912023 0.0000 0.00 0.00 0.00 306.90
> 24    70 4.248495 0.0000 0.00 0.00 0.00 381.30
> 25    90 4.499810 0.0000 0.00 0.00 0.00 432.45
> 26   110 4.700480 0.0000 0.00 0.00 0.00 455.70
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From Martin.Spindler at gmx.de  Fri Jul 31 09:39:13 2015
From: Martin.Spindler at gmx.de (Martin Spindler)
Date: Fri, 31 Jul 2015 09:39:13 +0200
Subject: [R] R parallel - slow speed
Message-ID: <trinity-9111a6f1-b33b-4b61-b456-fbb1a15bd1cb-1438328353777@3capp-gmx-bs60>

Thank you very much for your help.
?
I tried it under Unix and then the parallel version was faster than under Windows (but still slower than the non parall version). This is an important point to keep in mind. Thanks for this.
?
Best,
?
Martin

?
?

Gesendet:?Donnerstag, 30. Juli 2015 um 14:56 Uhr
Von:?"Jeff Newmiller" <jdnewmil at dcn.davis.CA.us>
An:?"Martin Spindler" <Martin.Spindler at gmx.de>, "r-help at r-project.org" <r-help at r-project.org>
Betreff:?Re: [R] R parallel - slow speed
Parallelizing comes at a price... and there is no guarantee that you can afford it. Vectorizing your algorithms is often a better approach. Microbenchmarking is usually overkill for evaluating parallelizing.

You assume 4 cores... but many CPUs have 2 cores and use hyperthreading to make each core look like two.

The operating system can make a difference also... Windows processes are more expensive to start and communicate between than *nix processes are. In particular, Windows seems to require duplicated RAM pages while *nix can share process RAM (at least until they are written to) so you end up needing more memory and disk paging of virtual memory becomes more likely.
---------------------------------------------------------------------------
Jeff Newmiller The ..... ..... Go Live...
DCN:<jdnewmil at dcn.davis.ca.us> Basics: ##.#. ##.#. Live Go...
Live: OO#.. Dead: OO#.. Playing
Research Engineer (Solar/Batteries O.O#. #.O#. with
/Software/Embedded Controllers) .OO#. .OO#. rocks...1k
---------------------------------------------------------------------------
Sent from my phone. Please excuse my brevity.

On July 30, 2015 8:26:34 AM EDT, Martin Spindler <Martin.Spindler at gmx.de> wrote:
>Dear all,
>
>I am trying to parallelize the function npnewpar given below. When I am
>comparing an application of "apply" with "parApply" the parallelized
>version seems to be much slower (cf output below). Therefore I would
>like to ask how the function could be parallelized more efficient.
>(With increasing sample size the difference becomes smaller, but I was
>wondering about this big differences and how it could be improved.)
>
>Thank you very much for help in advance!
>
>Best,
>
>Martin
>
>
>library(microbenchmark)
>library(doParallel)
>
>n <- 500
>y <- rnorm(n)
>Xc <- rnorm(n)
>Xd <- sample(c(0,1), replace=TRUE)
>Weights <- diag(n)
>n1 <- 50
>Xeval <- cbind(rnorm(n1), sample(c(0,1), n1, replace=TRUE))
>
>
>detectCores()
>cl <- makeCluster(4)
>registerDoParallel(cl)
>microbenchmark(apply(Xeval, 1, npnewpar, y=y, Xc=Xc, Xd = Xd,
>Weights=Weights, h=0.5), parApply(cl, Xeval, 1, npnewpar, y=y, Xc=Xc,
>Xd = Xd, Weights=Weights, h=0.5), times=100)
>stopCluster(cl)
>
>
>Unit: milliseconds
> expr min lq mean median
>apply(Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights = Weights,
> h = 0.5) 4.674914 4.726463 5.455323 4.771016
>parApply(cl, Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights =
>Weights, h = 0.5) 34.168250 35.434829 56.553296 39.438899
> uq max neval
> 4.843324 57.01519 100
> 49.777265 347.77887 100
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>npnewpar <- function(y, Xc, Xd, Weights, h, xeval) {
> xc <- xeval[1]
> xd <- xeval[2]
> l <- function(x,X) {
> w <- Weights[x,X]
> return(w)
> }
> u <- (Xc-xc)/h
> #K <- kernel(u)
> K <- dnorm(u)
> L <- l(xd,Xd)
> nom <- sum(y*K*L)
> denom <- sum(K*L)
> ghat <- nom/denom
> return(ghat)
>}
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
>and provide commented, minimal, self-contained, reproducible code.
?


From Martin.Spindler at gmx.de  Fri Jul 31 09:43:06 2015
From: Martin.Spindler at gmx.de (Martin Spindler)
Date: Fri, 31 Jul 2015 09:43:06 +0200
Subject: [R] R parallel - slow speed
In-Reply-To: <CAAxdm-7rL2WdN=chA=14bQL+wMXhxtmnYaJ5KXOGcucbcfkfWQ@mail.gmail.com>
References: <trinity-69cc511b-6769-419d-a083-618f32f1d575-1438259194833@3capp-gmx-bs03>
	<B40A052D-07AB-4D27-A07B-A78005C06328@dcn.davis.CA.us>,
	<CAAxdm-7rL2WdN=chA=14bQL+wMXhxtmnYaJ5KXOGcucbcfkfWQ@mail.gmail.com>
Message-ID: <trinity-54f2c174-6dc0-4a1b-93a4-d3dfc7d2e4e7-1438328585888@3capp-gmx-bs60>

Thank you very much to you both for your help.

I knew that parallelizing has some additional "overhead" costs, but I was surprised be the order of magnitude (it was 10 times slower.) Therefore I thought I made some mistake or that there is a more clever way to do it.

Best,

Martin
?
?

Gesendet:?Donnerstag, 30. Juli 2015 um 15:28 Uhr
Von:?"jim holtman" <jholtman at gmail.com>
An:?"Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
Cc:?"Martin Spindler" <Martin.Spindler at gmx.de>, "r-help at r-project.org" <r-help at r-project.org>
Betreff:?Re: [R] R parallel - slow speed

I ran a test on my Windows box with 4 CPUs.? THere were 4 RScript processes started in response to the request for a cluster of 4.? Each of these ran for an elapsed time of around 23 seconds, making the median time around 0.2 seconds for 100 iterations as reported by microbenchmark.? The 'apply' only takes about 0.003 seconds for a single iteration - again what microbenchmark is reporting.
?
The 4 RScript processes each use about 3 CPU seconds in the 23 seconds of elapsed time, most of that is probably the communication and startup time for the processes and reporting results.
?
So as was pointed out previous there is overhead is running in parallel.? You probably have to have at least several seconds of heavy computation for a iteration to make trying to parallelize something.? You should also investigate exactly what is happening on your system so that you can account for the time being spent.
?

Jim Holtman
Data Munger Guru
?
What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.?
On Thu, Jul 30, 2015 at 8:56 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:Parallelizing comes at a price... and there is no guarantee that you can afford it. Vectorizing your algorithms is often a better approach. Microbenchmarking? is usually overkill for evaluating parallelizing.

You assume 4 cores... but many CPUs have 2 cores and use hyperthreading to make each core look like two.

The operating system can make a difference also... Windows processes are more expensive to start and communicate between than *nix processes are. In particular, Windows seems to require duplicated RAM pages while *nix can share process RAM (at least until they are written to) so you end up needing more memory and disk paging of virtual memory becomes more likely.
---------------------------------------------------------------------------
Jeff Newmiller? ? ? ? ? ? ? ? ? ? ? ? The? ? ?.....? ? ? ?.....? Go Live...
DCN:<jdnewmil at dcn.davis.ca.us[jdnewmil at dcn.davis.ca.us]>? ? ? ? Basics: ##.#.? ? ? ?##.#.? Live Go...
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Live:? ?OO#.. Dead: OO#..? Playing
Research Engineer (Solar/Batteries? ? ? ? ? ? O.O#.? ? ? ?#.O#.? with
/Software/Embedded Controllers)? ? ? ? ? ? ? ?.OO#.? ? ? ?.OO#.? rocks...1k
---------------------------------------------------------------------------
Sent from my phone. Please excuse my brevity.

On July 30, 2015 8:26:34 AM EDT, Martin Spindler <Martin.Spindler at gmx.de[Martin.Spindler at gmx.de]> wrote:
>Dear all,
>
>I am trying to parallelize the function npnewpar given below. When I am
>comparing an application of "apply" with "parApply" the parallelized
>version seems to be much slower (cf output below). Therefore I would
>like to ask how the function could be parallelized more efficient.
>(With increasing sample size the difference becomes smaller, but I was
>wondering about this big differences and how it could be improved.)
>
>Thank you very much for help in advance!
>
>Best,
>
>Martin
>
>
>library(microbenchmark)
>library(doParallel)
>
>n <- 500
>y <- rnorm(n)
>Xc <- rnorm(n)
>Xd <- sample(c(0,1), replace=TRUE)
>Weights <- diag(n)
>n1 <- 50
>Xeval <- cbind(rnorm(n1), sample(c(0,1), n1, replace=TRUE))
>
>
>detectCores()
>cl <- makeCluster(4)
>registerDoParallel(cl)
>microbenchmark(apply(Xeval, 1, npnewpar, y=y, Xc=Xc, Xd = Xd,
>Weights=Weights, h=0.5),? parApply(cl, Xeval, 1, npnewpar, y=y, Xc=Xc,
>Xd = Xd, Weights=Weights, h=0.5), times=100)
>stopCluster(cl)
>
>
>Unit: milliseconds
>? ? ? ? ? ? ? ? ? ? ? ? ? ?expr? ? ? ?min? ? ? ? lq? ? ? mean? ? median
>apply(Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights = Weights,
>? ?h = 0.5)? 4.674914? 4.726463? 5.455323? 4.771016
>parApply(cl, Xeval, 1, npnewpar, y = y, Xc = Xc, Xd = Xd, Weights =
>Weights,? ? ? h = 0.5) 34.168250 35.434829 56.553296 39.438899
>? ? ? ? uq? ? ? ?max neval
>? 4.843324? 57.01519? ?100
> 49.777265 347.77887? ?100
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>npnewpar <- function(y, Xc, Xd, Weights, h, xeval) {
>? xc <- xeval[1]
>? xd <- xeval[2]
>? l <- function(x,X) {
>? ? w <-? Weights[x,X]
>? ? return(w)
>? }
>? u <- (Xc-xc)/h
>? #K <- kernel(u)
>? K <- dnorm(u)
>? L <- l(xd,Xd)
>? nom <- sum(y*K*L)
>? denom <- sum(K*L)
>? ghat <- nom/denom
>? return(ghat)
>}
>
>______________________________________________
>R-help at r-project.org[R-help at r-project.org] mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help[https://stat.ethz.ch/mailman/listinfo/r-help]
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
>and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org[R-help at r-project.org] mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help[https://stat.ethz.ch/mailman/listinfo/r-help]
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html[http://www.R-project.org/posting-guide.html]
and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Jul 31 13:23:13 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 31 Jul 2015 21:23:13 +1000
Subject: [R] Looping help
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF8802C385E9@SRVEXCHMBX.precheza.cz>
References: <CAKcEf9C6CiwVFAOKd1G8xxRM=biOQYtrcEZ0fzNRME8_Rwqa7w@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF8802C385E9@SRVEXCHMBX.precheza.cz>
Message-ID: <CA+8X3fV+Xffkmkfw1j7kTdcOD0Z5VB5eqjwOwK2tKefunmOaiw@mail.gmail.com>

Hi April,
You need nested loops for something like this

qs<- c(0,0.25,0.5,1,2,4,8,16,32,64)
nrows<-dim(Data)[1]
nqs<-length(qs)
D.mat<-SE.mat<-matrix(NA,nrow=nrows,ncol=nqs)
for(row in 1:nrows) {
 for(qval in 1:nqs) {
  # perform your calculation and set D.mat[row,qval] and
SE.mat[row,qval] to the return values
 }
}

Jim

> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of April
> Smith
> Sent: Friday, July 31, 2015 2:21 AM
> To: r-help at r-project.org
> Subject: [R] Looping help
>
> I have never looped before and know I need to.  I am unsure how to
> proceed:
>
>
>    - Action I need done: d(Data[1,2:399], q=0, boot=TRUE,
>    boot.arg=list(num.iter=1000))
>    - I need this to happen to all rows, I need All[1,2:399] to increase
> to
>    All[2:399], etc.
>    - But I also need the results from q increasing from 0 to 0.25, 0.5,
> 1,
>    2, 4,8,16,32,64 before the loop moves on to the next row.
>    - For each iteration I will receive two values: D and st.err.  I
> need
>    this put into a matrix
>
>
> I feel like this should be pretty simple to learn, but I have never
> looped before.
>
> I am hoping to get more of a tutorial on how to write loop code, then
> to just be given the loop code.
>
> Thanks,
> April
>
>       [[alternative HTML version deleted]]


From Martin.Spindler at gmx.de  Fri Jul 31 14:39:00 2015
From: Martin.Spindler at gmx.de (Martin Spindler)
Date: Fri, 31 Jul 2015 14:39:00 +0200
Subject: [R] R parallel / foreach - aggregation of results
Message-ID: <trinity-0f1e8231-2016-4c21-b2a6-c52882ecf380-1438346340329@3capp-gmx-bs44>

Dear all,

when I am running the code attached below, it seems that no results are returned, only the predefined NAs. What mistake do I make?
Any comments and help is highly appreciated.

Thanks and best,

Martin


Simpar3 <- function(n1) {
  L2distance <- matrix(NA, ncol=n1, nrow=n1)
  data <- rnorm(n1)
  diag(L2distance)=0
  cl <- makeCluster(4)
  registerDoParallel(cl)
  foreach(j=1:n1)  %dopar% {
    library(np)
    datj <- data[j]
    for(k in j:n1) {
      L2distance[j,k] <- k*datj
    }
  }
  stopCluster(cl)
  return(L2distance)
}

Res <- Simpar3(100)


From jon.skoien at jrc.ec.europa.eu  Fri Jul 31 15:20:42 2015
From: jon.skoien at jrc.ec.europa.eu (Jon Skoien)
Date: Fri, 31 Jul 2015 15:20:42 +0200
Subject: [R] R parallel / foreach - aggregation of results
In-Reply-To: <trinity-0f1e8231-2016-4c21-b2a6-c52882ecf380-1438346340329@3capp-gmx-bs44>
References: <trinity-0f1e8231-2016-4c21-b2a6-c52882ecf380-1438346340329@3capp-gmx-bs44>
Message-ID: <55BB762A.3040501@jrc.ec.europa.eu>

Martin,

I think the main problem is that you are trying to assign your results 
to the result matrix inside the foreach loop. Parallel functions in R 
are generally not good at updating parts of matrices from the different 
workers in this way. Instead, using e.g. foreach, each loop of the 
foreach-call has to return a vector which can be cbind-ed to a result 
matrix. Something like:

L2distance = foreach(j=1:n1, .combine = cbind)  %dopar% {
     res = rep(NA, 10)
     for (k in j:n1) res[k] = k*data[j]
     res
}
L2distance

I am not sure what the np-library is, but you should consider putting it 
in a clusterExport-call after creating the cluster.

Best wishes,
Jon


On 7/31/2015 2:39 PM, Martin Spindler wrote:
> Dear all,
>
> when I am running the code attached below, it seems that no results are returned, only the predefined NAs. What mistake do I make?
> Any comments and help is highly appreciated.
>
> Thanks and best,
>
> Martin
>
>
> Simpar3 <- function(n1) {
>    L2distance <- matrix(NA, ncol=n1, nrow=n1)
>    data <- rnorm(n1)
>    diag(L2distance)=0
>    cl <- makeCluster(4)
>    registerDoParallel(cl)
>    foreach(j=1:n1)  %dopar% {
>      library(np)
>      datj <- data[j]
>      for(k in j:n1) {
>        L2distance[j,k] <- k*datj
>      }
>    }
>    stopCluster(cl)
>    return(L2distance)
> }
>
> Res <- Simpar3(100)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Jon Olav Sk?ien
Joint Research Centre - European Commission
Institute for Environment and Sustainability (IES)
Climate Risk Management Unit

Via Fermi 2749, TP 100-01,  I-21027 Ispra (VA), ITALY

jon.skoien at jrc.ec.europa.eu
Tel:  +39 0332 789205

Disclaimer: Views expressed in this email are those of the individual 
and do not necessarily represent official views of the European Commission.


From bbiashvili at yahoo.com  Fri Jul 31 15:38:32 2015
From: bbiashvili at yahoo.com (Beka Biashvili)
Date: Fri, 31 Jul 2015 13:38:32 +0000 (UTC)
Subject: [R] question about implementation of the R
Message-ID: <1135764151.6431418.1438349912050.JavaMail.yahoo@mail.yahoo.com>

Dear Sir/MadamPlease provide me with the information how to implement R, what is a steps of implementation and obligations, difficulties and etc.
thank you in advance?Mr. Beka Biashvili
Quality Management System


ISO 9001:2008 Lead Auditor 





18 Lortkipanidze street, Tbilisi, Georgia.


mob: +995 599 414547 ??

	[[alternative HTML version deleted]]


From rootswalk at yahoo.co.uk  Fri Jul 31 12:55:21 2015
From: rootswalk at yahoo.co.uk (tom walk)
Date: Fri, 31 Jul 2015 10:55:21 +0000 (UTC)
Subject: [R] trojan with R download
Message-ID: <1549893275.6266208.1438340121309.JavaMail.yahoo@mail.yahoo.com>




I am working in China for a month and needed to download an earlier version of R in order to use Deseq2 and its requirements. The download got to the last few seconds and hung up. A trojan was found. It could be coincidence that it happened when I was downloading R, or perhaps a man in the middle added a little something. Anyway, I thought you might be interested. You might want to check on this source and others from this server.


https://mirrors.ustc.edu.cn/CRAN/bin/windows/base/old/3.0.3/R-3.0.3-win.exe

From marc_schwartz at me.com  Fri Jul 31 17:46:42 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 31 Jul 2015 10:46:42 -0500
Subject: [R] question about implementation of the R
In-Reply-To: <1135764151.6431418.1438349912050.JavaMail.yahoo@mail.yahoo.com>
References: <1135764151.6431418.1438349912050.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <7293F382-DBF8-4C75-9A9F-68502C9C24AB@me.com>


> On Jul 31, 2015, at 8:38 AM, Beka Biashvili via R-help <r-help at r-project.org> wrote:
> 
> Dear Sir/MadamPlease provide me with the information how to implement R, what is a steps of implementation and obligations, difficulties and etc.
> thank you in advance Mr. Beka Biashvili
> Quality Management System


I would start with the R Installation and Administration Manual here:

  https://cran.r-project.org/manuals.html

and then other manuals as may be apropos for your needs.

If you need some documentation on R?s SDLC or perhaps any clinical trial related regulatory guidance:

  https://www.r-project.org/certification.html


Regards,

Marc Schwartz


From marc_schwartz at me.com  Fri Jul 31 18:01:05 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 31 Jul 2015 11:01:05 -0500
Subject: [R] trojan with R download
In-Reply-To: <1549893275.6266208.1438340121309.JavaMail.yahoo@mail.yahoo.com>
References: <1549893275.6266208.1438340121309.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <AEFB38F5-405E-4E54-8A20-567EFC9C6F36@me.com>


> On Jul 31, 2015, at 5:55 AM, tom walk <rootswalk at yahoo.co.uk> wrote:
> 
> 
> 
> 
> I am working in China for a month and needed to download an earlier version of R in order to use Deseq2 and its requirements. The download got to the last few seconds and hung up. A trojan was found. It could be coincidence that it happened when I was downloading R, or perhaps a man in the middle added a little something. Anyway, I thought you might be interested. You might want to check on this source and others from this server.
> 
> 
> https://mirrors.ustc.edu.cn/CRAN/bin/windows/base/old/3.0.3/R-3.0.3-win.exe


These things are typically false positives due to overly aggressive filtering.

I downloaded the above file from the same server:

$ md5 R-3.0.3-win.exe
MD5 (R-3.0.3-win.exe) = 446db51e5c188ed2dccbd44dfa5f4aa9

The official MD5 value from the main CRAN server at:

  https://cran.r-project.org/bin/windows/base/old/3.0.3/md5sum.txt

is:

  446db51e5c188ed2dccbd44dfa5f4aa9 *R-3.0.3-win.exe

So unless that hash value was compromised centrally...which if that is the case, it has been long enough that mirrors would probably reflect that as well.

Presuming you can get to a different server, try it to see what happens.

Regards,

Marc Schwartz


From 538280 at gmail.com  Fri Jul 31 18:06:12 2015
From: 538280 at gmail.com (Greg Snow)
Date: Fri, 31 Jul 2015 10:06:12 -0600
Subject: [R] How to simulate informative censoring in a Cox PH model?
In-Reply-To: <CABrUXPWCVc_vJYqPBCkmL-30KQKAP_Hp+=2K5M0SeARU5eq+=w@mail.gmail.com>
References: <CABrUXPWBEe_TovEoTK+wY6UWeQtNTM_j1WnOic5wG1qAJi1seg@mail.gmail.com>
	<CAFEqCdzk8cSwed-VdBrob1bzLQtx7rknK9UkNEU_bwt8auoD7A@mail.gmail.com>
	<CABrUXPVi2=GnEg14xZQ19XjJOnuW=61NVZc+Vkbfo2HDN08L8g@mail.gmail.com>
	<CAFEqCdxb9_Lv-_DoYVM26F=wOW_cJnn-FRtYubQhkbL8gWpjbw@mail.gmail.com>
	<CABrUXPWCVc_vJYqPBCkmL-30KQKAP_Hp+=2K5M0SeARU5eq+=w@mail.gmail.com>
Message-ID: <CAFEqCdyVFU7z-Eiwrnq3oQ2TeZZif5rRGi2S-T_6eaaQsnM1Vg@mail.gmail.com>

Daniel,

Basically just responding to your last paragraph (the others are
interesting, but I think that you are learning as much as anyone and I
don't currently have any other suggestions).

I am not an expert on copulas, so this is a basic understanding, you
should learn more about them if you choose to use them.  The main idea
of a copula is that it is a bivariate or multivariate distribution
where all the variables have uniform marginal distributions but the
variables are not independent from each other.  How I would suggest
using them is to choose a copula and generate random points from a
bivariate copula, then put those (uniform) values into the inverse pdf
function for the Weibull (or other distribution), one of which is the
event time, the other the censoring time.  This will give you times
that (marginally) come from the distributions of interest, but are not
independent (so would be considered informative censoring).  Repeat
this with different levels of relationship in the copula to see how
much difference it makes in your simulations.

On Thu, Jul 30, 2015 at 2:02 PM, Daniel Meddings <dpmeddings at gmail.com> wrote:
> Thanks Greg once more for taking the time to reply. I certainly agree that
> this is not a simple set-up, although it is realistic I think. In short you
> are correct about model mis-specification being the key to producing more
> biased estimates under informative than under non-informative censoring.
> After looking again at my code and trying various things I realize that the
> key factor that leads to the informative and non-informative censoring data
> giving rise to the same biased estimates is how I generate my Z_i variable,
> and also the magnitude of the Z_i coefficient in both of the event and
> informative censoring models.
>
> In the example I gave I generated Z_i (I think of this as a "poor prognosis"
> variable) from a beta distribution so that it ranged from 0-1. The biased
> estimates for "beta_t_1" (I think of this as the effect of a treatment on
> survival) were approximately 1.56 when the true value was -1. What I forgot
> to mention was that estimating a cox model with 1,000,000 subjects to the
> full data (i.e. no censoring at all) arguably gives the best treatment
> effect estimate possible given that the effects of Z_i and Z_i*Treat_i are
> not in the model. This "best possible" estimate turns out to be 1.55 - i.e.
> the example I gave just so happens to be such that even with 25-27%
> censoring, the estimates obtained are almost the best that can be attained.
>
> My guess is that the informative censoring does not bias the estimate more
> than non-informative censoring because the only variable not accounted for
> in the model is Z_i which does not have a large enough effect "beta_t_2",
> and/or "beta_c_2", or perhaps because Z_i only has a narrow range which does
> not permit the current "beta_t_2" value to do any damage?
>
> To investigate the "beta_t_2", and/or "beta_c_2" issue I changed "beta_c_2"
> from 2 to 7 and "beta_c_0" from 0.2 to -1.2, and "beta_d_0" from -0.7 to
> -0.4 to keep the censoring %'s equal at about 30%. This time the "best
> possible" estimate of "beta_t_1" was -1.53 which was similar to that
> obtained previously. The informative censoring gave an estimate for
> "beta_t_1" of -1.49 whereas the non-informative censoring gave -1.53 - this
> time the non-informative censoring attains the "best possible" but the
> non-informative censoring does not.
>
>
>
> I then instead changed "beta_t_2" from 1 to 7 and "beta_c_0" from 0.2 to 2,
> and "beta_d_0" from -0.7 to -1.9 again to keep the censoring %'s equal at
> about 30%. This time the "best possible" estimate of "beta_t_1" was -0.999
> which is pretty much equal to the true value of -1. The informative
> censoring gave an estimate for "beta_t_1" of -1.09 whereas the
> non-informative censoring gave -0.87 ? surprisingly this time the
> informative censoring is slightly closer to the ?best possible? than the
> non-informative censoring.
>
>
>
> To investigate the Z_i issue I generated it from a normal distribution with
> mean 1 and variance 1. I changed "beta_c_0 " from 0.2 to -0.5 to keep the
> censoring levels equal (this time about 30% for both). This time the "best
> possible" estimate was -1.98 which was further from -1 than previous
> examples. The informative censoring gave an estimate for "beta_t_1" of -1.81
> whereas the non-informative censoring gave -1.84. So again the informative
> censoring gives an estimate closer to the "best possible" when compared with
> the informative censoring, but this time it does not attain the "best
> possible".
>
> In conclusion it is clear to me that a stronger Z_i effect in the censoring
> model causes the informative censoring to be worse than the non-informative
> one (as expected), but a stronger Z_i effect in the event model does not
> have this effect and even causes the independent censoring to be worse ?
> this in general may not hold but I nonetheless see it here. I am wondering
> if this is because altering the treatment effect in the event model also
> affects the independent censoring process and so it ?muddies the waters?
> whereas altering the treatment effect in the informative censoring model
> obviously confines the changes to just the informative censoring process.
> For a fixed treatment effect size in both the event and informative
> censoring models the effect of Z_i having a wider range than is possible
> under the beta distribution also appears to produce informative censoring
> that is worse than the non-informative one. This makes sense I think because
> the Z_i-response relationship must be more informative?
>
>
>
> Thanks for your suggestion of copulas ? I have not come across these. Is
> this similar to assuming a event model for censored subjects (this is
> unobserved) ? i.e. if the event model is different conditional on censoring
> then if we could observe the events beyond censoring then clearly the
> parameter estimates would be different compared to those obtained when
> modelling only non-censored times?
>
>
>
>
> On Wed, Jul 29, 2015 at 5:37 PM, Greg Snow <538280 at gmail.com> wrote:
>>
>> As models become more complex it becomes harder to distinguish
>> different parts and their effects.  Even for a straight forward linear
>> regression model if X1 and X2 are correlated with each other then it
>> becomes difficult to distinguish between the effects of X1^2, X2^2,
>> and X1*X2.  In your case the informative censoring and model
>> misspecification are becoming hard to distinguish (and it could be
>> argued that having informative censoring is really just a form of
>> model misspecification).  So I don't think so much that you are doing
>> things wrong, just that you are learning that the models are complex.
>>
>> Another approach to simulation that you could try is to simulate the
>> event time and censoring time using copulas (and therefore they can be
>> correlated to give informative censoring, but without relying on a
>> term that you could have included in the model) then consider the
>> event censored if the censoring time is shorter.
>>
>> On Fri, Jul 24, 2015 at 10:14 AM, Daniel Meddings <dpmeddings at gmail.com>
>> wrote:
>> > Hi Greg
>> >
>> > Many thanks for taking the time to respond to my query. You are right
>> > about
>> > pointing out the distinction between what variables are and are not
>> > included
>> > in the event times process and in the censoring process. I clearly
>> > forgot
>> > this important aspect. I amended my code to do as you advise and now I
>> > am
>> > indeed getting biased estimates when using the informatively censored
>> > responses. The problem is now that the estimates from the independently
>> > censored responses are the same - i.e. they are just as biased. Thus the
>> > bias seems to be due entirely to model mis-specification and not the
>> > informative censoring.
>> >
>> >
>> > To give a concrete example I simulate event times T_i and censoring
>> > times
>> > C_i from the following models;
>> >
>> >
>> > T_i~ Weibull(lambda_t(x),v_t),    lambda_t(x)=lambda_t*exp( beta_t_0 +
>> > (beta_t_1*Treat_i) + (beta_t_2*Z_i) + (beta_t_3*Treat_i*Z_i)  )
>> >
>> > C_i~ Weibull(lambda_c(x),v_c),    lambda_c(x)=lambda_c*exp( beta_c_0 +
>> > (beta_c_1*Treat_i) + (beta_c_2*Z_i) + (beta_c_3*Treat_i*Z_i)  )
>> >
>> > D_i~Weibull(lambda_d(x),v_D), lambda_d(x)=lamda_d*exp( beta_d_0)
>> >
>> > where ;
>> >
>> > beta_t_0 = 1,  beta_t_1 = -1,   beta_t_2 = 1,  beta_t_3 = -2,
>> > lambda_t=0.5
>> >
>> > beta_c_0 = 0.2,  beta_c_1 = -2,   beta_c_2 = 2,  beta_c_3 = -2,
>> > lambda_c=0.5
>> >
>> > beta_d_0 = -0.7,  lambda_d=0.1
>> >
>> > When I fit the cox model to both the informatively censored responses
>> > and
>> > the independent censored responses I include only the Treatment
>> > covariate in
>> > the model.
>> >
>> > I simulate Treatment from a Bernoulli distribution with p=0.5 and Z_i
>> > from a
>> > beta distribution so that Z ranges from 0 to 1 (I like to think of Z as
>> > a
>> > "poor" prognosis probability where Z_i=1 means a subject is 100% certain
>> > to
>> > have a poor prognosis and Z_i=0 means zero chance). These parameter
>> > choices
>> > give approximately 27% and 25% censoring for the informatively censored
>> > responses (using C_i) and the independent censored responses (using D_i)
>> > respectively. I use N=2000 subjects and 2000 simulation replications.
>> >
>> > The above simulation I get estimates of beta_t_2 of -1.526 and -1.537
>> > for
>> > the informatively censored responses and the independent censored
>> > responses
>> > respectively.
>> >
>> > Furthermore when I fit a cox model to the full responses (no censoring
>> > at
>> > all) I get an estimate of beta_t_2 of -1.542. This represents the best
>> > that
>> > can possibly be done given that Z and Treat*Z are not in the model.
>> > Clearly
>> > censoring is not making much of a difference here - model
>> > mis-specification
>> > dominates.
>> >
>> > I still must be doing something wrong but I cannot figure this one out.
>> >
>> > Thanks
>> >
>> > Dan
>> >
>> >
>> >
>> > On Thu, Jul 23, 2015 at 12:33 AM, Greg Snow <538280 at gmail.com> wrote:
>> >>
>> >> I think that the Cox model still works well when the only information
>> >> in the censoring is conditional on variables in the model.  What you
>> >> describe could be called non-informative conditional on x.
>> >>
>> >> To really see the difference you need informative censoring that
>> >> depends on something not included in the model.  One option would be
>> >> to use copulas to generate dependent data and then transform the
>> >> values using your Weibul.  Or you could generate your event times and
>> >> censoring times based on x1 and x2, but then only include x1 in the
>> >> model.
>> >>
>> >> On Wed, Jul 22, 2015 at 2:20 AM, Daniel Meddings <dpmeddings at gmail.com>
>> >> wrote:
>> >> > I wish to simulate event times where the censoring is informative,
>> >> > and
>> >> > to
>> >> > compare parameter estimator quality from a Cox PH model with
>> >> > estimates
>> >> > obtained from event times generated with non-informative censoring.
>> >> > However
>> >> > I am struggling to do this, and I conclude rather than a technical
>> >> > flaw
>> >> > in
>> >> > my code I instead do not understand what is meant by informative and
>> >> > un-informative censoring.
>> >> >
>> >> > My approach is to simulate an event time T dependent on a vector of
>> >> > covariates x having hazard function
>> >> > h(t|x)=lambda*exp(beta'*x)v*t^{v-1}.
>> >> > This corresponds to T~ Weibull(lambda(x),v), where the scale
>> >> > parameter
>> >> > lambda(x)=lambda*exp(beta'*x) depends on x and the shape parameter v
>> >> > is
>> >> > fixed. I have N subjects where T_{i}~ Weibull(lambda(x_{i}),v_{T}),
>> >> > lambda(x_{i})=lambda_{T}*exp(beta_{T}'*x_{i}), for i=1,...,N. Here I
>> >> > assume
>> >> > the regression coefficients are p-dimensional.
>> >> >
>> >> > I generate informative censoring times C_i~ Weibull(lambda(x_i),v_C),
>> >> > lambda(x_i)=lambda_C*exp(beta_C'*x_i) and compute
>> >> > Y_inf_i=min(T_i,C_i)
>> >> > and
>> >> > a censored flag delta_inf_i=1 if Y_inf_i <= C_i (an observed event),
>> >> > and
>> >> > delta_inf_i=0 if Y_inf_i > C_i (informatively censored: event not
>> >> > observed). I am convinced this is informative censoring because as
>> >> > long
>> >> > as
>> >> > beta_T~=0 and beta_C~=0 then for each subject the data generating
>> >> > process
>> >> > for T and C both depend on x.
>> >> >
>> >> > In contrast I generate non-informative censoring times
>> >> > D_i~Weibull(lambda_D*exp(beta_D),v_D), and compute
>> >> > Y_ninf_i=min(T_i,D_i)
>> >> > and a censored flag delta_ninf_i=1 if Y_ninf_i <= D_i (an observed
>> >> > event),
>> >> > and delta_ninf_i=0 if Y_ninf_i > D_i (non-informatively censored:
>> >> > event
>> >> > not
>> >> > observed). Here beta_D is a scalar. I "scale" the simulation by
>> >> > choosing
>> >> > the lambda_T, lambda_C and lambda_D parameters such that on average
>> >> > T_i<C_i
>> >> > and T_i<D_i to achieve X% of censored subjects for both Y_inf_i and
>> >> > Y_ninf_i.
>> >> >
>> >> > The problem is that even for say 30% censoring (which I think is
>> >> > high),
>> >> > the
>> >> > Cox PH parameter estimates using both Y_inf and Y_ninf are unbiased
>> >> > when
>> >> > I
>> >> > expected the estimates using Y_inf to be biased, and I think I see
>> >> > why:
>> >> > however different beta_C is from beta_T, a censored subject can
>> >> > presumably
>> >> > influence the estimation of beta_T only by affecting the set of
>> >> > subjects
>> >> > at
>> >> > risk at any time t, but this does not change the fact that every
>> >> > single
>> >> > Y_inf_i with delta_inf_i=1 will have been generated using beta_T
>> >> > only.
>> >> > Thus
>> >> > I do not see how my simulation can possibly produce biased estimates
>> >> > for
>> >> > beta_T using Y_inf.
>> >> >
>> >> > But then what is informative censoring if not based on this approach?
>> >> >
>> >> > Any help would be greatly appreciated.
>> >> >
>> >> >         [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide
>> >> > http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible code.
>> >>
>> >>
>> >>
>> >> --
>> >> Gregory (Greg) L. Snow Ph.D.
>> >> 538280 at gmail.com
>> >
>> >
>>
>>
>>
>> --
>> Gregory (Greg) L. Snow Ph.D.
>> 538280 at gmail.com
>
>



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From jholtman at gmail.com  Fri Jul 31 18:22:51 2015
From: jholtman at gmail.com (jim holtman)
Date: Fri, 31 Jul 2015 12:22:51 -0400
Subject: [R] R parallel / foreach - aggregation of results
In-Reply-To: <trinity-0f1e8231-2016-4c21-b2a6-c52882ecf380-1438346340329@3capp-gmx-bs44>
References: <trinity-0f1e8231-2016-4c21-b2a6-c52882ecf380-1438346340329@3capp-gmx-bs44>
Message-ID: <CAAxdm-7d80ZptONLfdvpfQQqY7TvrgYzgaA4xsgZD6t94TD5HA@mail.gmail.com>

Try this chance to actually return values:


library(doParallel)
Simpar3 <- function(n1) {
   L2distance <- matrix(NA, ncol=n1, nrow=n1)
   data <- rnorm(n1)
   diag(L2distance)=0
   cl <- makeCluster(4)
   registerDoParallel(cl)
   x <- foreach(j=1:n1)  %dopar% {
     library(np)
     datj <- data[j]
     for(k in j:n1) {
       L2distance[j,k] <- k*datj
     }
     L2distance  # return the value
   }
   stopCluster(cl)
   return(x)
 }
 Res <- Simpar3(100)


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Fri, Jul 31, 2015 at 8:39 AM, Martin Spindler <Martin.Spindler at gmx.de>
wrote:

> Dear all,
>
> when I am running the code attached below, it seems that no results are
> returned, only the predefined NAs. What mistake do I make?
> Any comments and help is highly appreciated.
>
> Thanks and best,
>
> Martin
>
>
> Simpar3 <- function(n1) {
>   L2distance <- matrix(NA, ncol=n1, nrow=n1)
>   data <- rnorm(n1)
>   diag(L2distance)=0
>   cl <- makeCluster(4)
>   registerDoParallel(cl)
>   foreach(j=1:n1)  %dopar% {
>     library(np)
>     datj <- data[j]
>     for(k in j:n1) {
>       L2distance[j,k] <- k*datj
>     }
>   }
>   stopCluster(cl)
>   return(L2distance)
> }
>
> Res <- Simpar3(100)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Sebastien.Bihorel at cognigencorp.com  Fri Jul 31 21:26:49 2015
From: Sebastien.Bihorel at cognigencorp.com (sbihorel)
Date: Fri, 31 Jul 2015 15:26:49 -0400
Subject: [R] Using latticeExtra as.layer function with different number
 of plot panels
In-Reply-To: <461A0D41-22E8-4141-BDB9-C6A340A0F0F9@comcast.net>
References: <55BA44D3.3080605@cognigencorp.com>
	<461A0D41-22E8-4141-BDB9-C6A340A0F0F9@comcast.net>
Message-ID: <55BBCBF9.5030503@cognigencorp.com>

Thanks David,

I was hoping for something a little bit more generic and less 
case-by-case basis.

Sebastien

On 7/30/2015 3:51 PM, David Winsemius wrote:
> On Jul 30, 2015, at 8:37 AM, sbihorel wrote:
>
>> Hi,
>>
>> When the as.layer function is used to overaly 2 lattice plots, there seems to be an assumption that the data used in both plots will generate the same number of panels (and, I believe, in the same order). In case the data used in the plot within the as.layer call is incomplete , data may be plotted on the "wrong" panel, and data seem to get re-used on the last panel(s). See what happens in the example code below when the records with state.region=="South" are dropped...
>>
>> Is there a trick to overlay panel based upon the conditioning variable value rather than the panel order?
>>
>> require(lattice)
>> require(latticeExtra)
>> state2 <- state <- data.frame(state.x77,state.region)
>> state2$Income <- sample(state2$Income)
>> state3 <- state2[which(state2$state.region!="South"),]
>> foo <- xyplot(Income~Population|state.region,data=state,main='foo')
>> foo
>>
>> bar <- update(foo,main='bar') + as.layer(xyplot(Income~Population|state.region,data=state2,col='red'))
>> bar
>>
>> bar2 <- update(foo,main='bar2') + as.layer(xyplot(Income~Population|state.region,data=state3,col='red'))
>> bar2
> I don't know if this works using the `+.lattice` function but it is possible to selectively update panels using `trellis.focus`
>


From adma89 at gmail.com  Fri Jul 31 20:49:51 2015
From: adma89 at gmail.com (Adam Jauregui)
Date: Fri, 31 Jul 2015 11:49:51 -0700
Subject: [R] Exclude 2014 data from mean
Message-ID: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>

Hello R-help,

I am trying to compute the mean of a quarterback's career fantasy football
stats, but I wish to exclude his 2014 stats from the mean, as that will be
the test data for the model I am trying to build for my academic undergrad
research.

The code for figuring out the mean of his Yds for every career Game 1 was
simple:


*mean(brady.t$Yds[brady.t$G. == 1])*
How can I make an "if-then" statement though so that his 2014 stats are
excluded? Or is there an easier way besides "if-then?"

Thank you,

AKJ

	[[alternative HTML version deleted]]


From backus at whimsy.med.utah.edu  Fri Jul 31 21:22:44 2015
From: backus at whimsy.med.utah.edu (Steven Backus)
Date: Fri, 31 Jul 2015 13:22:44 -0600 (MDT)
Subject: [R] x11() hangs in 3.2.1
Message-ID: <201507311922.t6VJMiOl010572@whimsy.med.utah.edu>

I'm on RHEL 6.6, R version 3.2.1 Patched (2015-07-30 r68761) --
"World-Famous Astronaut".  Issuing the x11() command hangs R and
does not complete.  A window is partially drawn then freezes.
Does anyone know of a solution?

Thanks,
  Steve
-- 
Steven J. Backus                        Computer Systems Manager
University of Utah                      E-Mail:  steven.backus at utah.edu
Genetic Epidemiology                    Alternate:  backus at math.utah.edu
391 Chipeta Way -- Suite D              Office:  801.587.9308
Salt Lake City, UT 84108-1266           http://www.math.utah.edu/~backus


From marc_schwartz at me.com  Fri Jul 31 22:33:49 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 31 Jul 2015 15:33:49 -0500
Subject: [R] x11() hangs in 3.2.1
In-Reply-To: <201507311922.t6VJMiOl010572@whimsy.med.utah.edu>
References: <201507311922.t6VJMiOl010572@whimsy.med.utah.edu>
Message-ID: <02D24771-4BA5-4825-9B35-88A800A77739@me.com>


> On Jul 31, 2015, at 2:22 PM, Steven Backus <backus at whimsy.med.utah.edu> wrote:
> 
> I'm on RHEL 6.6, R version 3.2.1 Patched (2015-07-30 r68761) --
> "World-Famous Astronaut".  Issuing the x11() command hangs R and
> does not complete.  A window is partially drawn then freezes.
> Does anyone know of a solution?
> 
> Thanks,
>  Steve


First, just an FYI, that this would be better posted to R-SIG-Fedora:

  https://stat.ethz.ch/mailman/listinfo/r-sig-fedora


Can you run:

  capabilities()

in a terminal session and see what it shows for X11:

> capabilities()
       jpeg         png        tiff       tcltk         X11        aqua 
       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 
   http/ftp     sockets      libxml        fifo      cledit       iconv 
       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 
        NLS     profmem       cairo         ICU long.double     libcurl 
       TRUE        TRUE        TRUE        TRUE        TRUE        TRUE 


The above is on a Mac, just for clarity.

Did you install R via local compilation or via RPMS from the EPEL? I have not looked to see if RPMS are available for the patched version.

Presuming local compilation, I would check your configure and build logs for warnings/errors. It is possible that you are missing an X11 header or lib someplace.


Regards,

Marc Schwartz


From sarah.goslee at gmail.com  Fri Jul 31 22:38:09 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Fri, 31 Jul 2015 16:38:09 -0400
Subject: [R] Exclude 2014 data from mean
In-Reply-To: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>
References: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>
Message-ID: <CAM_vjunD01J2rpe9L_XJ3gxQu6CpRs5wK=Vb1idf-=nGubr4yA@mail.gmail.com>

Hi Adam,

Possibly subset() or & would be helpful. Or even aggregate(),
depending on your ultimate goal.

Without a reproducible example that includes some sample data provided
using dput() (fake is fine), the code you used, and some clear idea of
what output you expect, it's impossible to figure out how to help you.
Here are some suggestions for creating a good reproducible example:
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example

Sarah


On Fri, Jul 31, 2015 at 2:49 PM, Adam Jauregui <adma89 at gmail.com> wrote:
> Hello R-help,
>
> I am trying to compute the mean of a quarterback's career fantasy football
> stats, but I wish to exclude his 2014 stats from the mean, as that will be
> the test data for the model I am trying to build for my academic undergrad
> research.
>
> The code for figuring out the mean of his Yds for every career Game 1 was
> simple:
>
>
> *mean(brady.t$Yds[brady.t$G. == 1])*
> How can I make an "if-then" statement though so that his 2014 stats are
> excluded? Or is there an easier way besides "if-then?"
>
> Thank you,
>
> AKJ
>
>         [[alternative HTML version deleted]]
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From bgunter.4567 at gmail.com  Fri Jul 31 22:43:49 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 31 Jul 2015 13:43:49 -0700
Subject: [R] Exclude 2014 data from mean
In-Reply-To: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>
References: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>
Message-ID: <CAGxFJbQDXGx+XV=K3K=DUrd2Ks1cwdb-Zr4sWW6mJUSF4RKp3A@mail.gmail.com>

This is very basic. Have you made **any** effort to learn R -- e.g. by
going through an R tutorial? If not, please do this before posting
further. This will save you -- and foks on this list, probably -- a
lot of grief in the long (or even short) run.

Also, if/when you do post further, post in plain text, not HTML, as
requested by the posting guide below.

Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Jul 31, 2015 at 11:49 AM, Adam Jauregui <adma89 at gmail.com> wrote:
> Hello R-help,
>
> I am trying to compute the mean of a quarterback's career fantasy football
> stats, but I wish to exclude his 2014 stats from the mean, as that will be
> the test data for the model I am trying to build for my academic undergrad
> research.
>
> The code for figuring out the mean of his Yds for every career Game 1 was
> simple:
>
>
> *mean(brady.t$Yds[brady.t$G. == 1])*
> How can I make an "if-then" statement though so that his 2014 stats are
> excluded? Or is there an easier way besides "if-then?"
>
> Thank you,
>
> AKJ
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Fri Jul 31 22:43:39 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 31 Jul 2015 15:43:39 -0500
Subject: [R] Exclude 2014 data from mean
In-Reply-To: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>
References: <CALtvmmiGu6pO9NDVbtTDggpFx_3EKFCLT4OzPO8+HupWV+GuNw@mail.gmail.com>
Message-ID: <D9E58E66-189E-4F32-B832-473084AE6A25@me.com>


> On Jul 31, 2015, at 1:49 PM, Adam Jauregui <adma89 at gmail.com> wrote:
> 
> Hello R-help,
> 
> I am trying to compute the mean of a quarterback's career fantasy football
> stats, but I wish to exclude his 2014 stats from the mean, as that will be
> the test data for the model I am trying to build for my academic undergrad
> research.
> 
> The code for figuring out the mean of his Yds for every career Game 1 was
> simple:
> 
> 
> *mean(brady.t$Yds[brady.t$G. == 1])*
> How can I make an "if-then" statement though so that his 2014 stats are
> excluded? Or is there an easier way besides "if-then?"
> 
> Thank you,
> 
> AKJ


It would be helpful to have a sample of data to know the structure, but take a look at:

  ?subset

for examples of more complicated logic for subsetting data frames. 

You might be able to use something along the lines of:

  mean(subset(brady.t, (G. == 1) & (Year != 2014), select = Yds)[[1]])

Basically, subset() is returning a data frame where Year does not equal 2014 and G. is equal to 1. The select argument is only returning the Yds column, which would otherwise be a list, so the [[1]] only returns a vector, which is passed to mean().

Regards,

Marc Schwartz


From rshepard at appl-ecosys.com  Fri Jul 31 22:56:16 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 31 Jul 2015 13:56:16 -0700 (PDT)
Subject: [R] x11() hangs in 3.2.1
In-Reply-To: <02D24771-4BA5-4825-9B35-88A800A77739@me.com>
References: <201507311922.t6VJMiOl010572@whimsy.med.utah.edu>
	<02D24771-4BA5-4825-9B35-88A800A77739@me.com>
Message-ID: <alpine.LNX.2.11.1507311354420.10973@localhost>

On Fri, 31 Jul 2015, Marc Schwartz wrote:

> Can you run:
>  capabilities()

   FWIW, on Slackware-14.1 I've had no issues with X11().

> capabilities()
        jpeg         png        tiff       tcltk         X11        aqua
        TRUE        TRUE        TRUE        TRUE        TRUE       FALSE
    http/ftp     sockets      libxml        fifo      cledit       iconv
        TRUE        TRUE        TRUE        TRUE        TRUE        TRUE
         NLS     profmem       cairo         ICU long.double     libcurl
        TRUE       FALSE        TRUE        TRUE        TRUE        TRUE

   Built from the SlackBuilds.org script I've been using.

Rich


