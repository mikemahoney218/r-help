From phii m@iii@g oii phiiipsmith@c@  Fri Nov  1 00:41:22 2019
From: phii m@iii@g oii phiiipsmith@c@ (phii m@iii@g oii phiiipsmith@c@)
Date: Thu, 31 Oct 2019 19:41:22 -0400
Subject: [R] 
 " Error in firstnonmiss:lastnonmiss : argument of length 0 "
In-Reply-To: <CAF8bMcZouT=0uynhDDRSQgppygNi86JKTbC515=ZkkjmSh+q6Q@mail.gmail.com>
References: <ba212904dba13d3b9ec51c2acee40482@philipsmith.ca>
 <c81d293a-764c-3d65-e625-2b0c89ef53a5@sapo.pt>
 <ed86cb587a9728244d73d1c7dcccad0a@philipsmith.ca>
 <CAF8bMcZ4OeCqHtz4+J=WbQJEbsNx8P9bKJdVT3fbWt3fwqusRA@mail.gmail.com>
 <cf6fddcc20027206a0afca921ccb5059@philipsmith.ca>
 <CAF8bMcZouT=0uynhDDRSQgppygNi86JKTbC515=ZkkjmSh+q6Q@mail.gmail.com>
Message-ID: <ad6df0458daef091e6875b96c4e1b945@philipsmith.ca>

I finally got it to work, drawing on your advice William. I extracted 
the time vector (REF_DATE) from the data frame (vseries) so the latter 
was numeric only, and I converted the resulting data frame to a matrix. 
I also converted the dts data frame to a matrix. Now it works properly. 
Your help is much appreciated.

Philip


On 2019-10-30 21:55, William Dunlap wrote:
> The length(x)>dt2 requirement is not quite right - it is only required
> for one branch of your if statement.  Figure out the assumptions for
> each branch of the code and put stopifnot calls in each branch.  Or
> leave them out and debug later.
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com [1]
> 
> On Wed, Oct 30, 2019 at 5:13 PM <phil at philipsmith.ca> wrote:
> 
>> Thanks for your suggestions. I have tried them all, with no success.
>> 
>> William's looked quite promising. I put in the
>> stopifnot(is.numeric(x), NCOL(x)==1, length(x)>dt2)
>> statement and it detected the problem:
>> length(x) > dt2 is not TRUE
>> Then I changed all the references to vseries to the [[j]]
>> notation:
>> vseries[,j] became vseries[[j]] and
>> vseries$REF_DATE became vseries[[1]]
>> But I still got the same error statement from stopifnot:
>> length(x) > dt2 is not TRUE
>> 
>> I am quite at a loss on this one. Any other suggestions? Maybe I
>> should
>> just take a completely different approach of some kind?
>> 
>> Philip
>> 
>> On 2019-10-30 12:58, William Dunlap wrote:
>>> Your EXTEND() function appears to expect that its 'x' argument
>> will be
>>> a numeric vector, but you pass it a one-column tibble.  Hence
>>> length(x) is 1 and things go downhill from there.
>>> 
>>> I like to start such functions with a long stopifnot() statement
>> that
>>> does a quick check of inputs.  E.g.,
>>> 
>>> stopifnot(is.numeric(x), NCOL(x)==1, length(x)>dt2)
>>> 
>>> Use vseries1[,j,drop=TRUE] or vseries1[[j]] to extract a single
>> column
>>> from the tibble 'vseries1'.  vseries1[,1] will return a one-column
>>> tibble.
>>> 
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com [1] [2]
>>> 
>>> On Wed, Oct 30, 2019 at 5:25 AM <phil at philipsmith.ca> wrote:
>>> 
>>>> Thanks for the suggestion Rui, but no, this will not remove the
>>>> error.
>>>> In fact, if I drop the second term entirely as in:
>>>> 
>>>> if ( (!is.na [2] [1](dts[1,j-1])) ) {
>>>> 
>>>> then I still get the error. I have been unable to find a
>>>> work-around.
>>>> 
>>>> Philip
>>>> 
>>>> On 2019-10-30 05:17, Rui Barradas wrote:
>>>>> Hello,
>>>>> 
>>>>> Is this as simple as
>>>>> 
>>>>> 
>>>>> if ( (!is.na [2] [1](dts[1,j-1])) & (!is.na [2] [1](dts[3,j-1]))
>> ) {
>>>>> 
>>>>> 
>>>>> (change the logical operator from '|' to '&')?
>>>>> The result vseries1 still has some NA's at the end of some of
>> its
>>>>> series.
>>>>> 
>>>>> Hope this helps,
>>>>> 
>>>>> Rui Barradas
>>>>> 
>>>>> ?s 02:05 de 30/10/19, phil at philipsmith.ca escreveu:
>>>>>> I am having a problem that generates the error message: " Error
>>>> in
>>>>>> firstnonmiss:lastnonmiss : argument of length 0 ". There is an
>>>> article
>>>>>> on this in stackoverflow, but I have been unable to understand
>> it
>>>> well
>>>>>> enough to solve my problem. Essentially, I have a data frame
>> with
>>>> 41
>>>>>> indicator series and some of the series have missing values
>> (NAs)
>>>> at
>>>>>> the beginning and/or the end. I want to fill in the missing
>>>> values
>>>>>> using ARIMA models, via the forecast() function. It works when
>> I
>>>> use
>>>>>> my EXTEND function on a single series, but it fails when I try
>> to
>>>> loop
>>>>>> through all 41 series. Here is a reproducible example. Thanks
>> for
>>>> any
>>>>>> advice.
>>>>>> 
>>>>>> # Reproducible example
>>>>>> # " Error in firstnonmiss:lastnonmiss : argument of length 0 "
>>>>>> # See also stackoverflow:
>>>>>> 
>>>> 
>>> 
>> 
> https://stackoverflow.com/questions/27350636/r-argument-is-of-length-zero-in-if-statement
>>>> 
>>>>>> library(forecast)
>>>>>> library(lubridate)
>>>>>> vseries <- dget("vseries.txt") # data frame containing REF_DATE
>>>> and 41
>>>>>> "indicator" vectors
>>>>>> dts <- dget("dts.txt") # data frame recording where NAs are in
>>>> vseries
>>>>>> - they will be replaced
>>>>>> testcase1 <- dget("testcase1.txt") # a vector for use in
>> testing
>>>>>> # Function to fill in missing values (NAs) using ARIMA
>> forecasts
>>>> and
>>>>>> backcasts
>>>>>> EXTEND <-
>>>> function(x,REF_DATE,dt1,dt2,dt3,dt4,first_date,last_date) {
>>>>>> if (!is.na [2] [1](dt1)) {
>>>>>> bfct = dt2-dt1+1 # number of months to backcast
>>>>>> revx <- ts(rev(x),frequency=12)
>>>>>> revx <- revx[1:(length(revx)-dt2)]
>>>>>> fc <- forecast(auto.arima(revx),bfct)
>>>>>> revx1 <- c(revx,fc$mean) # extend with forecasts (in
>>>> fc$mean)
>>>>>> m <- month(as.POSIXlt(first_date,format="%Y-%m-%d"))
>>>>>> y <- year(as.POSIXlt(first_date,format="%Y-%m-%d"))
>>>>>> x <- as.numeric(ts(rev(revx1),start=c(y,m),frequency=12))
>>>>>> }
>>>>>> if (!is.na [2] [1](dt3)) {
>>>>>> ffct <- dt4-dt3+1 # number of months to forecast
>>>>>> x <- x[1:(dt3-1)]
>>>>>> fc <- forecast(auto.arima(x),ffct)
>>>>>> x <- c(x,fc$mean)
>>>>>> }
>>>>>> return(x)
>>>>>> }
>>>>>> # Test EXTEND function with a single vector - apparently it
>> works
>>>>>> (testcase1_extended <-
>>>> EXTEND(testcase1,vseries$REF_DATE,1,3,222,223,
>>>>>> 
>> vseries$REF_DATE[1],vseries$REF_DATE[length(vseries$REF_DATE)]))
>>>>>> View(cbind(testcase1,testcase1_extended))
>>>>>> # Now use EXTEND to fill in the missing values in all 41
>>>> indicators in
>>>>>> vseries
>>>>>> # Data frame dts has NAs where no extension is required,
>> integer
>>>>>> values showing where to start and
>>>>>> # where to end when extensions are required. Store extended
>>>> series in
>>>>>> vseries1.
>>>>>> vseries1 <- vseries
>>>>>> for (j in 2:(ncol(vseries))) { # j starts at 2 because column 1
>>>> in
>>>>>> vseries is REF_DATE
>>>>>> if ( (!is.na [2] [1](dts[1,j-1])) | (!is.na [2]
>> [1](dts[3,j-1])) ) {
>>>>>> vseries1[,j] <-
>>>>>> 
>>>> 
>>> 
>> 
> EXTEND(vseries[,j],vseries$REF_DATE,dts[1,j-1],dts[2,j-1],dts[3,j-1],dts[4,j-1],
>>>> 
>>>>>> vseries$REF_DATE[1],vseries$REF_DATE[length(vseries$REF_DATE)])
>>>>>> }
>>>>>> }
>>>>>> View(vseries)
>>>>>> View(vseries1)
>>>>>> 
>>>>>> dput() output for vseries:
>>>>>> 
>>>>>> structure(list(REF_DATE = c("2001-01-01", "2001-02-01",
>>>> "2001-03-01",
>>>>>> "2001-04-01", "2001-05-01", "2001-06-01", "2001-07-01",
>>>> "2001-08-01",
>>>>>> "2001-09-01", "2001-10-01", "2001-11-01", "2001-12-01",
>>>> "2002-01-01",
>>>>>> "2002-02-01", "2002-03-01", "2002-04-01", "2002-05-01",
>>>> "2002-06-01",
>>>>>> "2002-07-01", "2002-08-01", "2002-09-01", "2002-10-01",
>>>> "2002-11-01",
>>>>>> "2002-12-01", "2003-01-01", "2003-02-01", "2003-03-01",
>>>> "2003-04-01",
>>>>>> "2003-05-01", "2003-06-01", "2003-07-01", "2003-08-01",
>>>> "2003-09-01",
>>>>>> "2003-10-01", "2003-11-01", "2003-12-01", "2004-01-01",
>>>> "2004-02-01",
>>>>>> "2004-03-01", "2004-04-01", "2004-05-01", "2004-06-01",
>>>> "2004-07-01",
>>>>>> "2004-08-01", "2004-09-01", "2004-10-01", "2004-11-01",
>>>> "2004-12-01",
>>>>>> "2005-01-01", "2005-02-01", "2005-03-01", "2005-04-01",
>>>> "2005-05-01",
>>>>>> "2005-06-01", "2005-07-01", "2005-08-01", "2005-09-01",
>>>> "2005-10-01",
>>>>>> "2005-11-01", "2005-12-01", "2006-01-01", "2006-02-01",
>>>> "2006-03-01",
>>>>>> "2006-04-01", "2006-05-01", "2006-06-01", "2006-07-01",
>>>> "2006-08-01",
>>>>>> "2006-09-01", "2006-10-01", "2006-11-01", "2006-12-01",
>>>> "2007-01-01",
>>>>>> "2007-02-01", "2007-03-01", "2007-04-01", "2007-05-01",
>>>> "2007-06-01",
>>>>>> "2007-07-01", "2007-08-01", "2007-09-01", "2007-10-01",
>>>> "2007-11-01",
>>>>>> "2007-12-01", "2008-01-01", "2008-02-01", "2008-03-01",
>>>> "2008-04-01",
>>>>>> "2008-05-01", "2008-06-01", "2008-07-01", "2008-08-01",
>>>> "2008-09-01",
>>>>>> "2008-10-01", "2008-11-01", "2008-12-01", "2009-01-01",
>>>> "2009-02-01",
>>>>>> "2009-03-01", "2009-04-01", "2009-05-01", "2009-06-01",
>>>> "2009-07-01",
>>>>>> "2009-08-01", "2009-09-01", "2009-10-01", "2009-11-01",
>>>> "2009-12-01",
>>>>>> "2010-01-01", "2010-02-01", "2010-03-01", "2010-04-01",
>>>> "2010-05-01",
>>>>>> "2010-06-01", "2010-07-01", "2010-08-01", "2010-09-01",
>>>> "2010-10-01",
>>>>>> "2010-11-01", "2010-12-01", "2011-01-01", "2011-02-01",
>>>> "2011-03-01",
>>>>>> "2011-04-01", "2011-05-01", "2011-06-01", "2011-07-01",
>>>> "2011-08-01",
>>>>>> "2011-09-01", "2011-10-01", "2011-11-01", "2011-12-01",
>>>> "2012-01-01",
>>>>>> "2012-02-01", "2012-03-01", "2012-04-01", "2012-05-01",
>>>> "2012-06-01",
>>>>>> "2012-07-01", "2012-08-01", "2012-09-01", "2012-10-01",
>>>> "2012-11-01",
>>>>>> "2012-12-01", "2013-01-01", "2013-02-01", "2013-03-01",
>>>> "2013-04-01",
>>>>>> "2013-05-01", "2013-06-01", "2013-07-01", "2013-08-01",
>>>> "2013-09-01",
>>>>>> "2013-10-01", "2013-11-01", "2013-12-01", "2014-01-01",
>>>> "2014-02-01",
>>>>>> "2014-03-01", "2014-04-01", "2014-05-01", "2014-06-01",
>>>> "2014-07-01",
>>>>>> "2014-08-01", "2014-09-01", "2014-10-01", "2014-11-01",
>>>> "2014-12-01",
>>>>>> "2015-01-01", "2015-02-01", "2015-03-01", "2015-04-01",
>>>> "2015-05-01",
>>>>>> "2015-06-01", "2015-07-01", "2015-08-01", "2015-09-01",
>>>> "2015-10-01",
>>>>>> "2015-11-01", "2015-12-01", "2016-01-01", "2016-02-01",
>>>> "2016-03-01",
>>>>>> "2016-04-01", "2016-05-01", "2016-06-01", "2016-07-01",
>>>> "2016-08-01",
>>>>>> "2016-09-01", "2016-10-01", "2016-11-01", "2016-12-01",
>>>> "2017-01-01",
>>>>>> "2017-02-01", "2017-03-01", "2017-04-01", "2017-05-01",
>>>> "2017-06-01",
>>>>>> "2017-07-01", "2017-08-01", "2017-09-01", "2017-10-01",
>>>> "2017-11-01",
>>>>>> "2017-12-01", "2018-01-01", "2018-02-01", "2018-03-01",
>>>> "2018-04-01",
>>>>>> "2018-05-01", "2018-06-01", "2018-07-01", "2018-08-01",
>>>> "2018-09-01",
>>>>>> "2018-10-01", "2018-11-01", "2018-12-01", "2019-01-01",
>>>> "2019-02-01",
>>>>>> "2019-03-01", "2019-04-01", "2019-05-01", "2019-06-01",
>>>> "2019-07-01"
>>>>>> ), v1001816015 = c(17262087, 17014344, 20247169, 19144987,
>>>> 19671015,
>>>>>> 19189905, 14739759, 18235434, 16623028, 19087839, 18792227,
>>>> 15362864,
>>>>>> 16782525, 17523910, 19390559, 19783530, 20283113, 19318846,
>>>> 16124593,
>>>>>> 19250140, 18889900, 20916009, 19349004, 17140428, 17531326,
>>>> 17559123,
>>>>>> 19401870, 18748298, 18885310, 17982314, 14912147, 15835801,
>>>> 17557392,
>>>>>> 18431507, 17296519, 16050211, 15215468, 16939038, 19917657,
>>>> 19027956,
>>>>>> 19932323, 19921644, 16315362, 18862032, 19250137, 18862111,
>>>> 18665646,
>>>>>> 17710225, 16863860, 17594959, 20442499, 19721391, 20471368,
>>>> 20466651,
>>>>>> 16013264, 19545483, 19572010, 19837635, 19794191, 18282403,
>>>> 17890774,
>>>>>> 17694365, 21286043, 19339429, 20363518, 20802697, 16690141,
>>>> 20060353,
>>>>>> 19217265, 20687952, 20886199, 18925660, 19052279, 19444846,
>>>> 23024759,
>>>>>> 20960798, 21284786, 20601708, 17882780, 20226914, 19443713,
>>>> 21213020,
>>>>>> 19984134, 17515960, 17735035, 18639461, 19708496, 20218572,
>>>> 20881111,
>>>>>> 20967845, 20246946, 20205727, 20951905, 23040393, 21107926,
>>>> 18556973,
>>>>>> 15757698, 16235071, 18220863, 17502183, 16206875, 16664295,
>>>> 16443231,
>>>>>> 17309836, 18065805, 18425679, 18383119, 17930175, 16093108,
>>>> 17853071,
>>>>>> 20468225, 18944075, 19949187, 20940064, 18679964, 20517783,
>>>> 21107272,
>>>>>> 20935541, 20476625, 19111285, 18591785, 18648376, 22572654,
>>>> 20878866,
>>>>>> 21334626, 22356452, 19273175, 22959478, 22082938, 22830337,
>>>> 22337136,
>>>>>> 21205679, 20255195, 21009917, 23627267, 21570319, 23091288,
>>>> 23252411,
>>>>>> 21004135, 22349893, 20779032, 22622782, 22804069, 19504385,
>>>> 20109262,
>>>>>> 20720256, 22697357, 23405692, 23330003, 22617671, 21538128,
>>>> 23130117,
>>>>>> 22943596, 24431094, 23449486, 21320812, 20897790, 21679882,
>>>> 25197042,
>>>>>> 24941550, 25904330, 24960421, 23649466, 24701113, 25754289,
>>>> 27899355,
>>>>>> 25196278, 24819686, 23767512, 24169080, 28917366, 27068182,
>>>> 27040475,
>>>>>> 28384836, 26443178, 27799087, 28229589, 28740172, 27763129,
>>>> 27108195,
>>>>>> 25800631, 27411304, 29036170, 27999387, 27759231, 29219186,
>>>> 25857724,
>>>>>> 29127423, 28085258, 28804390, 28364963, 26162758, 26152772,
>>>> 26468833,
>>>>>> 31455559, 29035964, 31486857, 32048318, 26815735, 29868265,
>>>> 28179157,
>>>>>> 28333370, 29749364, 27253403, 25959199, 26900190, 31813939,
>>>> 29778379,
>>>>>> 31070923, 31387782, 28663047, 30679026, 29362553, 31046824,
>>>> 29820781,
>>>>>> 28263730, 28062692, 28162744, 32752923, 30838600, 32865423,
>>>> 30947662,
>>>>>> 29590180), v1001816392 = c(14810380, 14200859, 16756119,
>>>> 15217622,
>>>>>> 16152695, 15734143, 11943953, 15182087, 14144821, 15327643,
>>>> 15666826,
>>>>>> 12873678, 14082760, 15086691, 15833563, 15883991, 16045785,
>>>> 15250640,
>>>>>> 12445691, 15776653, 15481482, 16487072, 15489524, 12938416,
>>>> 13984446,
>>>>>> 14161079, 15530205, 14786496, 14588586, 13934508, 11387684,
>>>> 12738958,
>>>>>> 14653137, 15079013, 13731146, 13103385, 12270059, 14238311,
>>>> 16669218,
>>>>>> 15938279, 16007457, 17036074, 12355149, 15533794, 15448684,
>>>> 15377076,
>>>>>> 14589920, 13589694, 13549648, 14624327, 15706302, 15239530,
>>>> 15633935,
>>>>>> 15974885, 11803449, 15736297, 15611744, 16105812, 15964696,
>>>> 14282888,
>>>>>> 14526761, 14449653, 16434235, 14597318, 15435231, 15712547,
>>>> 11808763,
>>>>>> 14862181, 14503627, 15295540, 15261211, 14585813, 15410585,
>>>> 14739285,
>>>>>> 17582347, 16171120, 15931155, 15565709, 12619998, 14547113,
>>>> 13908728,
>>>>>> 14778066, 14423148, 11701775, 12670956, 13600996, 14284120,
>>>> 14157825,
>>>>>> 13998465, 14543121, 12774383, 13773610, 14353232, 14770313,
>>>> 13084751,
>>>>>> 11637597, 9588706, 10497657, 11879518, 10586090, 9367990,
>>>> 9888218,
>>>>>> 9223044, 9922735, 11365570, 11658665, 11111213, 10508361,
>>>> 10292744,
>>>>>> 10992491, 13210908, 11847469, 12344814, 12772222, 10359995,
>>>> 12464327,
>>>>>> 12473841, 13137908, 13140871, 12928499, 12412546, 11481365,
>>>> 14066907,
>>>>>> 12082611, 13050270, 12875573, 11009100, 13447116, 13903540,
>>>> 13928150,
>>>>>> 13858803, 13349287, 13017205, 13136581, 14629295, 13175174,
>>>> 14259088,
>>>>>> 14721292, 11943999, 13855276, 13219889, 14406322, 13976428,
>>>> 12348507,
>>>>>> 12795753, 12884766, 14475286, 14122036, 14061445, 14563402,
>>>> 12114754,
>>>>>> 13444547, 13827458, 14397852, 14401695, 13108404, 12684810,
>>>> 13122736,
>>>>>> 15857365, 14149173, 15132604, 15077002, 13674898, 14475121,
>>>> 16407589,
>>>>>> 16552843, 15367514, 15446201, 14885482, 13981302, 17023925,
>>>> 15389386,
>>>>>> 15181518, 17365931, 15884142, 16708039, 17481246, 17751529,
>>>> 17312508,
>>>>>> 17964853, 17635476, 17403932, 18117391, 16708266, 16449636,
>>>> 17043771,
>>>>>> 14607070, 17536121, 17562680, 17898917, 17882877, 16194251,
>>>> 16292556,
>>>>>> 15904704, 18550339, 16665634, 18563263, 17857082, 13863082,
>>>> 16254963,
>>>>>> 15516553, 16359112, 17351737, 16080636, 16136849, 15070366,
>>>> 17819011,
>>>>>> 16691318, 17603308, 18075717, 15809284, 16585137, 16279853,
>>>> 18388536,
>>>>>> 18103660, 15593270, 17027143, 15580767, 17959460, 17242596,
>>>> 19056997,
>>>>>> 17565595, 15977646), v107792891 = c(72.1, 72.1, 72.1, 72.1,
>> 72.1,
>>>>>> 79, 79, 79, 79, 79, 79, 79, 79, 79, 81.5, 81.5, 73.8, 82.5,
>> 113,
>>>>>> 122.3, 139.6, 99.3, 99.9, 109.6, 113.4, 146.5, 141.8, 112.6,
>>>>>> 91.8, 92.2, 88.8, 99.9, 98.6, 109.3, 87.7, 93.8, 122.4, 103,
>>>>>> 97.9, 95.7, 99.9, 98.5, 96.3, 93.4, 100.7, 99.5, 103.9, 102.5,
>>>>>> 111.9, 100, 113.5, 115.1, 104.6, 125.7, 138, 155.1, 160, 135.3,
>>>>>> 110.7, 139, 106.1, 100.3, 97.7, 92, 94.9, 92.4, 99.9, 102.8,
>>>>>> 78.4, 84.4, 96.3, 84.4, 89.8, 107.7, 102.9, 92, 83.8, 92.2,
>> 91.2,
>>>>>> 103.6, 91.7, 96.2, 92.9, 96.9, 80.9, 94.6, 98.9, 91.1, 74.9,
>>>>>> 105.3, 105.3, 90.8, 93.5, 87.4, 95, 90.2, 98, 90.6, 69.3, 56.5,
>>>>>> 67.5, 62.4, 57, 66.6, 58.2, 68.9, 65.7, 75.5, 78.2, 76, 67.1,
>>>>>> 70.5, 81.1, 82.5, 104.7, 95.3, 79.3, 74.5, 77.6, 81.9, 78.1,
>>>>>> 79, 76.4, 74, 70.5, 79.3, 80.8, 78.8, 78.8, 80.4, 79.5, 79.2,
>>>>>> 76.7, 77.8, 79.2, 80.2, 81.8, 81.3, 85.8, 73.2, 78, 80.7, 84.1,
>>>>>> 77.2, 84.1, 85.1, 82.9, 87.9, 87.1, 96.1, 92.2, 87.3, 94.1,
>> 86.9,
>>>>>> 92, 102.1, 109.1, 121.8, 111.6, 81.7, 88.4, 101.2, 94.3, 94.9,
>>>>>> 95.4, 94.3, 104.2, 103.2, 99.5, 117.3, 94.6, 101.6, 108.3, 108,
>>>>>> 112.6, 112.5, 112.9, 103.1, 107, 111.3, 111.7, 114.7, 107.3,
>>>>>> 113.2, 115.5, 122.4, 116.1, 123.3, 108.2, 113, 123, 119.5,
>> 107.4,
>>>>>> 112.4, 109.1, 98.9, 106.8, 115.2, 115.3, 118.2, 118.1, 109.6,
>>>>>> 109.3, 114.2, 116.5, 100.5, 106.8, 115.9, 101, 121.3, 112.6,
>>>>>> 109.7, 109.3, 106.2, 116.7, 109, 109.3, 113.9, 107.7, 107,
>> 110.3,
>>>>>> 111.9, 127.2), v107792892 = c(59.1, 59.1, 59.1, 59.1, 59.1,
>> 64.3,
>>>>>> 64.3, 64.3, 64.3, 64.3, 64.3, 64.3, 64.3, 64.3, 66.3, 66.3,
>> 60.7,
>>>>>> 67.1, 89.2, 95.6, 107.4, 80.4, 81, 83, 84.5, 97, 95.3, 85.3,
>>>>>> 77.3, 77.5, 76.1, 80.9, 80.4, 84.8, 76.3, 79, 91.3, 84, 81.9,
>>>>>> 86.4, 88.4, 87.8, 86.8, 85.5, 88.9, 88.3, 90.3, 89.7, 93.8,
>> 88.6,
>>>>>> 94.8, 98.3, 93.6, 103.2, 108.5, 115.2, 117.2, 108.1, 98.2, 111,
>>>>>> 97.9, 94.4, 93.9, 91.1, 96.4, 94.8, 98.7, 100.2, 88.1, 91.5,
>>>>>> 96.6, 90.6, 79.7, 82.7, 87.3, 81.9, 77.4, 81.4, 80.9, 86.5,
>> 81.4,
>>>>>> 83.4, 80.8, 82.6, 77, 83.7, 85.7, 82.1, 73.4, 88.9, 88.9, 82.6,
>>>>>> 83.9, 81, 87.1, 84.2, 88, 84.5, 74.2, 66.8, 74.6, 71.7, 68.4,
>>>>>> 74.5, 69.6, 76.3, 74.6, 80.5, 82.3, 81.1, 76.2, 78.2, 89.9,
>> 90.7,
>>>>>> 106.8, 101.8, 92.9, 89.9, 91.4, 94.1, 91.9, 92.4, 90.8, 89.3,
>>>>>> 88.2, 95.5, 97.5, 92.5, 92.5, 94.1, 93.7, 93, 90.9, 92.7, 94.8,
>>>>>> 98.1, 95.4, 94.6, 97.9, 85.4, 90.7, 93.7, 96.2, 90.1, 94.8,
>> 98.2,
>>>>>> 95.1, 101.1, 101.4, 109.9, 106.4, 99.5, 110.9, 101.7, 106.6,
>>>>>> 113.9, 108.9, 111.1, 101.4, 54, 88.8, 110.3, 97.3, 97.3, 106.4,
>>>>>> 97.1, 120.7, 106.7, 99.2, 125.4, 83.6, 99.7, 119.9, 118.6,
>> 121.1,
>>>>>> 121.3, 123.1, 104.7, 104.2, 131.4, 114.4, 127.4, 113.1, 135,
>>>>>> 128.4, 142, 116.1, 129.1, 107.3, 122.8, 143.7, 134.9, 106.7,
>>>>>> 133.5, 121.5, 100.4, 121.2, 132.9, 128.5, 136.9, 149.2, 121.8,
>>>>>> 131.6, 117.3, 127.2, 107.3, 110.5, 129.5, 114.6, 147.1, 119.2,
>>>>>> 114.1, 118, 110.5, 141.5, 121.7, 105.2, 126.1, 114.2, 106.7,
>>>>>> 135.2, 130.5, 150.4), v111955490 = c(59, 59.1, 59.4, 59.5,
>> 59.6,
>>>>>> 59.8, 59.9, 60, 60.1, 60.1, 60.3, 60.3, 60.6, 61, 61.1, 61.4,
>>>>>> 61.8, 61.9, 62, 62.4, 62.4, 62.9, 63.1, 63.2, 63.3, 63.6, 63.7,
>>>>>> 63.9, 64.4, 64.5, 64.8, 65.2, 65.4, 65.6, 66.2, 66.3, 66.4,
>> 66.7,
>>>>>> 66.9, 67.5, 68.2, 68.8, 68.8, 69.2, 69.3, 69.4, 69.7, 69.9, 70,
>>>>>> 70.2, 70.5, 70.8, 71.2, 71.8, 71.9, 72, 72.2, 72.5, 72.6, 72.6,
>>>>>> 73.1, 73.3, 73.4, 73.6, 73.8, 74.1, 74.4, 74.8, 74.9, 74.8,
>> 75.1,
>>>>>> 75.1, 75.2, 75.3, 75.4, 75.4, 75.7, 76.1, 76.2, 76.5, 76.7,
>> 76.7,
>>>>>> 77.2, 77.3, 78, 78.4, 78.6, 78.7, 78.7, 79, 79, 79.1, 79.1,
>> 79.1,
>>>>>> 79, 79.1, 79.1, 79.1, 78.9, 78.5, 78.3, 78.3, 78.4, 78.5, 78.9,
>>>>>> 79.1, 79.4, 79.9, 80.2, 80, 80.2, 80.3, 80.7, 80.9, 80.8, 81,
>>>>>> 81, 81.2, 81.5, 81.6, 81.8, 82.2, 82.3, 82.8, 83.3, 83.8, 84,
>>>>>> 84.2, 84.4, 84.7, 85.3, 85.4, 85.5, 85.9, 86.3, 86.5, 86.9,
>> 87.2,
>>>>>> 87.3, 87.5, 87.8, 88, 88.2, 88.4, 88.5, 88.5, 88.6, 88.7, 88.8,
>>>>>> 88.8, 89, 89.1, 89.1, 89.2, 89.3, 89.4, 89.5, 89.8, 89.8, 90.2,
>>>>>> 90.2, 90.4, 90.4, 90.7, 90.9, 91, 91.1, 91.2, 91.2, 91.4, 91.6,
>>>>>> 91.8, 92.1, 92.6, 92.8, 93.3, 93.4, 93.8, 93.9, 94.1, 94.2,
>> 94.6,
>>>>>> 94.8, 95.3, 96.6, 97, 97.8, 98.2, 98.5, 99.5, 99.9, 100, 100.3,
>>>>>> 101.1, 101.4, 103, 103.8, 103.9, 104.1, 104.2, 104.3, 104.4,
>>>>>> 104.5, 104.5, 104.6, 104.3, 104.3, 104.1, 104.1, 104.2, 104.3,
>>>>>> 104.4, 104.4, 104.5, 104.6, 104.6, 104.5, 104.5, 104.6, 104.6,
>>>>>> 104.6, 104.7, 104.5), v113397631 = c(13666403, 13254077,
>>>> 15407812,
>>>>>> 14247486, 15080786, 14660425, 11002204, 14202820, 13271658,
>>>> 14415174,
>>>>>> 14508659, 11955671, 13146722, 14074199, 14791521, 14924080,
>>>> 15044874,
>>>>>> 14018888, 11578184, 14895437, 14496000, 15378756, 14347316,
>>>> 11919494,
>>>>>> 12767068, 13001096, 14151808, 13591747, 13337681, 12781326,
>>>> 10377252,
>>>>>> 11837321, 13499209, 13699497, 12521117, 11856721, 11166102,
>>>> 12992845,
>>>>>> 15124559, 14541534, 14645934, 15470071, 11120381, 14214588,
>>>> 13924365,
>>>>>> 13823149, 12964700, 12076947, 12095824, 13216048, 14025754,
>>>> 13550102,
>>>>>> 13914087, 14115722, 10160966, 14024121, 14022788, 14334153,
>>>> 14102556,
>>>>>> 12565384, 12839303, 12721934, 14366235, 12974063, 13389033,
>>>> 13670648,
>>>>>> 10141903, 12812065, 12299642, 13098258, 12851798, 12327714,
>>>> 12847367,
>>>>>> 12591336, 14825763, 13341600, 13237020, 12836974, 10203377,
>>>> 12244381,
>>>>>> 11765368, 12504709, 11956004, 9762716, 10401787, 11322046,
>>>> 11770050,
>>>>>> 11765753, 11284183, 11974342, 10123526, 11289637, 11723217,
>>>> 12097708,
>>>>>> 10555661, 9420181, 7523279, 8481494, 9242771, 8598074, 7202570,
>>>>>> 7866207, 7324976, 8190094, 9088589, 9446953, 8763624, 8523617,
>>>>>> 8047988, 8741224, 10166223, 9667012, 9914833, 10698557,
>> 8007402,
>>>>>> 9979283, 10042726, 10032883, 9896590, 9780364, 9299971,
>> 8878504,
>>>>>> 11119562, 9746562, 10190889, 10039663, 8115021, 10083267,
>>>> 10621244,
>>>>>> 10939833, 10470646, 10082230, 10164563, 10122394, 11107394,
>>>> 10523656,
>>>>>> 11128673, 11656285, 8974980, 10923049, 10334113, 11053800,
>>>> 11353041,
>>>>>> 9446652, 9983629, 9791384, 11053091, 11219875, 11272210,
>>>> 11132326,
>>>>>> 9306880, 10917075, 10818430, 11628396, 11478765, 10251218,
>>>> 10184002,
>>>>>> 10281970, 12121307, 11862960, 12308111, 12096729, 10731262,
>>>> 11590206,
>>>>>> 12816366, 12961856, 11982666, 12058824, 11517780, 11085487,
>>>> 13436617,
>>>>>> 12553043, 12335153, 14275307, 12679415, 13737743, 14456727,
>>>> 14243928,
>>>>>> 14099287, 14174328, 14110113, 14426606, 14658088, 13508307,
>>>> 13826213,
>>>>>> 14372422, 11711022, 14076134, 13858027, 14416062, 14337693,
>>>> 12754175,
>>>>>> 13196514, 13101829, 14765626, 13632730, 14826133, 14505383,
>>>> 10931400,
>>>>>> 13195201, 12111306, 13091406, 14152405, 12544611, 12451609,
>>>> 12366614,
>>>>>> 13998036, 13374490, 13857587, 14176873, 12006484, 13497534,
>>>> 12786433,
>>>>>> 14622182, 13953516, 12187672, 12676560, 12261957, 14051828,
>>>> 13274018,
>>>>>> 14558725, 14100590, 12336372), v113399963 = c(12383492,
>> 12489535,
>>>>>> 14782841, 13968637, 14630933, 14467257, 10134331, 13173103,
>>>> 12173082,
>>>>>> 13608236, 13678969, 11195227, 12282398, 12965165, 14472495,
>>>> 14636326,
>>>>>> 14869906, 14265366, 10942242, 14014202, 13616642, 15069966,
>>>> 13877119,
>>>>>> 11980443, 12667065, 12919482, 14273228, 13699916, 13856189,
>>>> 13126548,
>>>>>> 9946994, 11150401, 12469921, 12936259, 12054563, 11110027,
>>>> 10695234,
>>>>>> 12210008, 14226547, 13555826, 13976396, 14022407, 10640447,
>>>> 12927490,
>>>>>> 13216051, 12728488, 12215523, 11892094, 11408471, 11992608,
>>>> 13925071,
>>>>>> 13514739, 14058000, 13934254, 9995077, 13074057, 12947771,
>>>> 13135370,
>>>>>> 12911642, 12064102, 11668352, 12008392, 14377904, 13014856,
>>>> 13492799,
>>>>>> 13909599, 10372128, 13203866, 12301356, 12880238, 13158430,
>>>> 12127888,
>>>>>> 11990355, 12589435, 15029864, 13510353, 14063373, 13387185,
>>>> 10548301,
>>>>>> 12513166, 12253214, 13093732, 12154649, 10724007, 10974227,
>>>> 11593154,
>>>>>> 12167544, 12576271, 13252139, 13282088, 11986385, 12388363,
>>>> 12622830,
>>>>>> 13808547, 12412223, 10498437, 8868838, 9756446, 10918198,
>>>> 10608854,
>>>>>> 9635848, 10008018, 9262002, 10205239, 10773340, 10943384,
>>>> 10612403,
>>>>>> 10447653, 9446266, 10744663, 12072595, 11437008, 12055765,
>>>> 12335679,
>>>>>> 10375837, 11786080, 11836877, 11717685, 11050898, 10512867,
>>>> 10507066,
>>>>>> 10651956, 13301571, 11834961, 12194410, 12513463, 10330839,
>>>> 12542127,
>>>>>> 12113245, 12524548, 11821431, 11259600, 11364736, 11684179,
>>>> 13279826,
>>>>>> 12466423, 13651294, 13655882, 11400445, 12646589, 11701238,
>>>> 12657427,
>>>>>> 12432006, 10561389, 11303161, 11657370, 13166161, 13479612,
>>>> 13452761,
>>>>>> 13182454, 11606259, 13265594, 12954057, 13697290, 12957655,
>>>> 11701079,
>>>>>> 11521999, 12453497, 14751017, 14372295, 15060344, 14724363,
>>>> 13341724,
>>>>>> 14051091, 14306668, 15496758, 13943134, 13813528, 13107368,
>>>> 13650455,
>>>>>> 16239726, 15416259, 15520089, 16101026, 14490154, 15577891,
>>>> 15646361,
>>>>>> 15773300, 15292659, 15452107, 14554541, 15530759, 16579288,
>>>> 16527074,
>>>>>> 16377782, 17185264, 14518363, 16503262, 15763025, 15855797,
>>>> 15281552,
>>>>>> 14373491, 14894646, 15106333, 17847651, 16239885, 18225339,
>>>> 18182041,
>>>>>> 14161870, 16504649, 15462683, 15413283, 15693526, 14506023,
>>>> 14000735,
>>>>>> 15234901, 17994510, 16652601, 17250143, 17357528, 14854995,
>>>> 16636582,
>>>>>> 15856325, 16470266, 15502332, 14715205, 14804754, 15773203,
>>>> 17997663,
>>>>>> 16940919, 18145629, 17048409, 15038038), v121422275 = c(NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, 1294427, 1085157, 1659103, 1227780,
>>>> 1264742,
>>>>>> 1233227, 1369915, 1241682, 1195597, 1337652, 1463474, 1747482,
>>>>>> 1677063, 1710162, 1408062, 1367046, 1325700, 1691519, 1490239,
>>>>>> 1513014, 1619474, 1475569, 1128444, 975554, 1238609, 1397151,
>>>>>> 1318789, 1574134, 1671827, 1418491, 1334309, 1502402, 1396184,
>>>>>> 1534954, 1495128, 1385045, 1564246, 1572126, 1414339, 1390512,
>>>>>> 1517317, 1674464, 1804056, 1390981, 1661250, 1521750, 1785414,
>>>>>> 1689831, 1657511, 1471717, 1730868, 1914114, 1655118, 1787784,
>>>>>> 1978644, 2061312, 1716886, 1775567, 1695326, 1731401, 1476016,
>>>>>> 1747452, 1912495, 2113177, 1881425, 1849288, 1847819, 2037590,
>>>>>> 2329039, 2215335, 2117342, 2440624, 2144344, 2149473, 2245874,
>>>>>> 1776567, 2249560, 2091031, 1900474, 2027541, 1842369, 1956289,
>>>>>> 2164626, 1810228, 2118096, 2076786, 1799082, 1985239, 2297525,
>>>>>> 2217866, 2262269, 1938379, 1946678, 2282724, 1933329, 1934100,
>>>>>> 2210996, 2019742, 1970774, 1922537, 2175532, 2191166, 2270389
>>>>>> ), v121429235 = c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 936232,
>>>> 1380845,
>>>>>> 1096564, 804996, 947642, 966950, 1016699, 918866, 849556,
>>>> 1053348,
>>>>>> 986012, 946487, 950837, 940777, 1201431, 1056492, 869576,
>>>> 1029796,
>>>>>> 791533, 1342575, 768689, 1129197, 888039, 1196702, 844473,
>>>> 836908,
>>>>>> 1000086, 948109, 955454, 905751, 1334532, 795432, 939875,
>>>> 1055059,
>>>>>> 1162267, 887897, 1054106, 1321348, 926182, 747960, 994037,
>>>> 934344,
>>>>>> 931091, 768161, 1336244, 814041, 993226, 915337, 878574,
>> 962984,
>>>>>> 856037, 1349725, 1125033, 1145050, 735484, 1139687, 914030,
>>>> 959260,
>>>>>> 880369, 1003863, 1100186, 1106088, 943878, 908737, 921558,
>>>> 760214,
>>>>>> 1109277, 1149700, 881935, 854219, 1141033, 1046805, 990984,
>>>> 942851,
>>>>>> 1237111, 1260890, 1246517, 1386806, 1459511, 1049323, 1265316,
>>>>>> 1526229, 1164388, 1234202, 1213186, 1219715, 1136702, 1018132,
>>>>>> 1069785, 1189086, 1051625, 1075847, 1086470, 1097881, 1067886,
>>>>>> 1154182, 1176269, 1124279, 1183524, 1411238, 1169574, 1246182,
>>>>>> 1294246), v129360 = c(968307, 959792, 1076115, 1086887,
>> 1251495,
>>>>>> 1424836, 1645926, 1684631, 940698, 828397, 856445, 841089,
>>>> 828632,
>>>>>> 840372, 950609, 979841, 1145547, 1241005, 1423829, 1520486,
>>>> 1096690,
>>>>>> 1047509, 931186, 893051, 818595, 759586, 819928, 783164,
>> 958451,
>>>>>> 1052234, 1266479, 1297035, 962400, 940103, 811110, 772748,
>>>> 665961,
>>>>>> 734449, 784144, 758539, 894121, 1055843, 1236130, 1192797,
>>>> 960006,
>>>>>> 849533, 729150, 692275, 607965, 632583, 710260, 704517, 824671,
>>>>>> 972044, 1142496, 1078026, 845734, 774906, 664769, 659470,
>> 612101,
>>>>>> 589993, 674934, 678565, 749718, 889528, 1009429, 969411,
>> 766777,
>>>>>> 662975, 606170, 602809, 512657, 475338, 563786, 560655, 668895,
>>>>>> 812314, 889427, 905246, 682748, 609636, 515527, 468840, 432613,
>>>>>> 413188, 446004, 473583, 581658, 663425, 766218, 798365, 560733,
>>>>>> 544244, 448566, 422434, 363941, 373681, 423161, 455848, 549006,
>>>>>> 512311, 643896, 655521, 523765, 473648, 395028, 395936, 370816,
>>>>>> 364373, 427710, 454885, 527961, 618566, 751011, 714393, 576052,
>>>>>> 525327, 433970, 415091, 358181, 346337, 408076, 435412, 503904,
>>>>>> 609108, 725224, 688454, 558688, 498784, 427279, 430118, 366452,
>>>>>> 377491, 419938, 445065, 528163, 634929, 690784, 707384, 529718,
>>>>>> 482768, 418217, 402933, 358317, 342634, 408017, 408543, 507661,
>>>>>> 593475, 669148, 698274, 512723, 495423, 412292, 389515, 332097,
>>>>>> 333292, 379271, 413360, 496660, 584914, 663033, 687526, 506198,
>>>>>> 494580, 384589, 408166, 351113, 324025, 385497, 420713, 517844,
>>>>>> 612642, 719301, 696045, 569477, 518096, 423544, 437956, 383651,
>>>>>> 377702, 433709, 441800, 539359, 636840, 756702, 714218, 596912,
>>>>>> 524944, 438817, 431740, 383612, 367863, 406395, 443138, 500629,
>>>>>> 608716, 704251, 689926, 543102, 463086, 397399, 394648, 331558,
>>>>>> 323805, 401966, 383701, 492991, 630428, 709143, 710910, 528833,
>>>>>> 468225, 398455, 413621, 320746, 315212, 387521, 409642, 517469,
>>>>>> 628480, 719469), v129367 = c(895221, 827924, 1053256, 1021700,
>>>>>> 1041938, 987341, 1054306, 1143262, 815543, 857805, 842812,
>>>> 796604,
>>>>>> 834925, 770257, 975730, 934709, 970982, 911664, 970902,
>> 1059151,
>>>>>> 927251, 969482, 872692, 839512, 839548, 755360, 888372, 855790,
>>>>>> 934028, 917631, 966595, 1008325, 924734, 978616, 895027,
>> 860250,
>>>>>> 796199, 816724, 993943, 973312, 971248, 927537, 964599,
>> 1023056,
>>>>>> 935330, 952958, 906419, 843985, 784260, 775884, 972637, 942266,
>>>>>> 981175, 928757, 996353, 1063537, 934522, 962724, 917215,
>> 877143,
>>>>>> 864687, 789247, 1026369, 995372, 1041521, 959846, 1014128,
>>>> 1084626,
>>>>>> 957957, 965648, 964565, 919535, 860830, 770369, 1018339,
>> 987533,
>>>>>> 1036117, 963210, 1052261, 1147884, 1009249, 1102833, 1079839,
>>>>>> 920577, 889896, 831708, 1027160, 1004141, 1053969, 978913,
>>>> 1058493,
>>>>>> 1166431, 948350, 948412, 863854, 775676, 686194, 699708,
>> 877517,
>>>>>> 855250, 891139, 751240, 867679, 1010731, 882881, 908782,
>> 899653,
>>>>>> 858629, 828679, 785364, 1057204, 1035422, 1046472, 974958,
>>>> 1091468,
>>>>>> 1170113, 1024139, 1062967, 1051213, 1006391, 899269, 852185,
>>>>>> 1117451, 1137169, 1169148, 1099355, 1195778, 1294448, 1099930,
>>>>>> 1103412, 1084880, 1064711, 951741, 951003, 1167572, 1160377,
>>>>>> 1155739, 1097731, 1212005, 1323590, 1135465, 1136159, 1123725,
>>>>>> 1072563, 988504, 923774, 1184955, 1146244, 1187120, 1118133,
>>>>>> 1219236, 1315701, 1126955, 1149551, 1106605, 1058712, 912693,
>>>>>> 861161, 1096749, 1114567, 1153588, 1094638, 1187780, 1277890,
>>>>>> 1070771, 1101986, 996189, 1007737, 846412, 739882, 983093,
>>>> 971856,
>>>>>> 1009164, 955152, 1042033, 1056119, 910781, 915331, 869120,
>>>> 845165,
>>>>>> 782995, 716476, 898016, 901224, 955598, 913648, 992280,
>> 1063029,
>>>>>> 931401, 933769, 873861, 822407, 804971, 747805, 904296, 928485,
>>>>>> 960101, 944055, 1019181, 1128435, 995324, 1002176, 955042,
>>>> 892834,
>>>>>> 857278, 807727, 1035350, 992653, 1043240, 991543, 1012867,
>>>> 1094743,
>>>>>> 956089, 1005115, 913995, 881382, 796717, 765572, 986340,
>> 958601,
>>>>>> 991910, 947639, 1028986), v1992301 = c(70840, 74340, 75970,
>>>> 72620,
>>>>>> 72590, 72320, 71220, 69980, 82940, 84910, 80230, 73010, 74010,
>>>>>> 71200, 64550, 72030, 67040, 58060, 65780, 73570, 71080, 73530,
>>>>>> 70170, 67420, 72770, 70300, 76620, 71950, 82400, 77510, 79280,
>>>>>> 73720, 75030, 71800, 75720, 68520, 72890, 73340, 65900, 68700,
>>>>>> 69190, 78770, 69700, 69720, 66280, 67950, 66450, 74310, 72100,
>>>>>> 79690, 77360, 74970, 73110, 66650, 76190, 76370, 69110, 68130,
>>>>>> 69240, 77060, 68900, 70100, 75210, 75370, 69140, 73610, 71880,
>>>>>> 70460, 81690, 75830, 76550, 64380, 77620, 70250, 73070, 69700,
>>>>>> 76760, 73740, 68430, 68590, 67880, 68000, 67850, 73290, 73330,
>>>>>> 71580, 76610, 77860, 71730, 86340, 73130, 74260, 78350, 89670,
>>>>>> 92310, 96840, 104600, 118550, 109870, 113690, 121680, 113260,
>>>>>> 82240, 88020, 85840, 81320, 84270, 79190, 75740, 72110, 73930,
>>>>>> 75840, 75900, 77350, 77450, 77940, 79730, 78970, 76830, 81130,
>>>>>> 68780, 74280, 72990, 75610, 74010, 74510, 79640, 77040, 73180,
>>>>>> 77450, 76500, 76360, 72950, 80210, 75280, 74220, 72640, 70760,
>>>>>> 72080, 74320, 75890, 73160, 73830, 70910, 77080, 71730, 71700,
>>>>>> 71440, 73600, 70030, 73910, 71780, 74220, 73980, 72180, 74590,
>>>>>> 78850, 73980, 81220, 73320, 70550, 86770, 67370, 68910, 71120,
>>>>>> 72460, 74740, 70800, 72760, 79610, 72920, 74290, 76720, 101550,
>>>>>> 60190, 70970, 70560, 70590, 71150, 74470, 71840, 70230, 69300,
>>>>>> 71960, 72530, 68620, 77810, 74260, 71340, 67720, 71970, 73330,
>>>>>> 67820, 70210, 72790, 74620, 70370, 70320, 73740, 71250, 71200,
>>>>>> 76850, 70010, 73020, 76190, 68980, 70820, 70170, 72000, 72100,
>>>>>> 73900, 75660, 73650, 71520, 73530, 71030, 72710, 75470, 76750,
>>>>>> 73720, 74040, 71920, 75250), v1996491 = c(20333104, 20404933,
>>>>>> 20429451, 20443702, 20442474, 20453171, 20494907, 20469899,
>>>> 20426712,
>>>>>> 20423088, 20431074, 20563595, 20608707, 20722376, 20729355,
>>>> 20768807,
>>>>>> 21040249, 21061442, 21072385, 21386395, 21361143, 21481726,
>>>> 21596243,
>>>>>> 21614432, 21702298, 21736376, 21736357, 21744928, 21848072,
>>>> 21846894,
>>>>>> 21947454, 21940721, 22017595, 22187732, 22226844, 22379231,
>>>> 22554639,
>>>>>> 22641673, 22744549, 22907620, 22980134, 23092919, 23181451,
>>>> 23206767,
>>>>>> 23425453, 23318773, 23348036, 23517288, 23482935, 23602818,
>>>> 23784278,
>>>>>> 23974850, 23921664, 24074395, 24223603, 24289526, 24427768,
>>>> 24503153,
>>>>>> 24606727, 24715635, 24661798, 24748394, 24847099, 24906920,
>>>> 25035239,
>>>>>> 25153096, 25225233, 25425738, 25594618, 25555772, 25803231,
>>>> 25918644,
>>>>>> 26050884, 26090730, 26304952, 26332475, 26592901, 26628543,
>>>> 26691191,
>>>>>> 26795716, 26761208, 26875873, 26972976, 26938528, 27101037,
>>>> 27180711,
>>>>>> 27241360, 27354111, 27442498, 27514060, 27466513, 27514474,
>>>> 27386049,
>>>>>> 27517692, 27313803, 27136562, 27069115, 26962612, 26955040,
>>>> 26877705,
>>>>>> 26886419, 26588008, 26677212, 26851356, 26953581, 27016460,
>>>> 27155400,
>>>>>> 27251087, 27253169, 27300593, 27342306, 27436290, 27580341,
>>>> 27686074,
>>>>>> 27860430, 27878998, 28147776, 28256507, 28354370, 28557974,
>>>> 28716843,
>>>>>> 28861177, 29057415, 28913944, 28887173, 29038232, 29065032,
>>>> 29165480,
>>>>>> 29222770, 29278705, 29304052, 29449805, 29488534, 29620041,
>>>> 29795842,
>>>>>> 29879786, 29839111, 30084067, 30096498, 30229968, 30298304,
>>>> 30287625,
>>>>>> 30389211, 30484631, 30628734, 30641277, 30932839, 30736619,
>>>> 30970797,
>>>>>> 30851107, 30920882, 31165786, 31066548, 31258395, 31403046,
>>>> 31382224,
>>>>>> 31477046, 31488824, 31587271, 31728896, 31880578, 32021255,
>>>> 32294373,
>>>>>> 32244815, 32415589, 32434552, 32436242, 32524750, 32735158,
>>>> 33033517,
>>>>>> 33172260, 33441230, 33246392, 33273322, 33847198, 33389255,
>>>> 33687475,
>>>>>> 34049329, 33935583, 33971885, 33781778, 33706550, 33873093,
>>>> 33938988,
>>>>>> 33985921, 34184776, 34057971, 34092996, 34084079, 34099494,
>>>> 34350509,
>>>>>> 34805403, 34785441, 34875075, 35152571, 35003861, 35186315,
>>>> 35518127,
>>>>>> 35361642, 35984437, 36321295, 36349916, 36944209, 36901171,
>>>> 36971724,
>>>>>> 37173621, 37152181, 37260262, 37492044, 37517038, 37471633,
>>>> 37827498,
>>>>>> 37872621, 38129314, 38213257, 38064353, 38230587, 38258308,
>>>> 38560349,
>>>>>> 38883535, 39178365, 39132811, NA), v2063945 = c(5904.9, 5875.6,
>>>>>> 5887.4, 5930.5, 5938.4, 5925, 5937.2, 5937.5, 5934.5, 5926,
>>>> 5937.3,
>>>>>> 5929, 5926.3, 5944.8, 5961.3, 5976.4, 5999, 5999.5, 6031.6,
>>>> 6075.3,
>>>>>> 6093.5, 6103, 6138.1, 6146.9, 6164.7, 6197.4, 6214, 6186,
>> 6184.7,
>>>>>> 6196, 6193, 6210, 6225.1, 6237.7, 6253.3, 6284.2, 6293.8,
>> 6308.9,
>>>>>> 6288, 6300.3, 6301.9, 6319.3, 6331.5, 6307.3, 6316.1, 6334.8,
>>>>>> 6340.7, 6341, 6328.7, 6348.3, 6347.4, 6363, 6387.7, 6385.3,
>>>> 6379.3,
>>>>>> 6389.3, 6408.1, 6421.1, 6419.8, 6395.7, 6405.6, 6391.1, 6424,
>>>>>> 6444.7, 6477.2, 6476.3, 6476.7, 6463.2, 6456, 6442.3, 6465.3,
>>>>>> 6502.4, 6499.9, 6511, 6518, 6507.3, 6521.6, 6530, 6558.8,
>> 6559.4,
>>>>>> 6578.1, 6610, 6586.2, 6569.6, 6578.8, 6626.7, 6605.8, 6623,
>>>> 6638.2,
>>>>>> 6599.7, 6596.8, 6614, 6632.7, 6650.1, 6568, 6576.2, 6480.5,
>>>> 6461.2,
>>>>>> 6458.8, 6416.4, 6385.7, 6377.6, 6407.4, 6418.1, 6438, 6458.4,
>>>>>> 6465.5, 6451.6, 6483.7, 6477.7, 6471.1, 6506.2, 6543.2, 6573,
>>>>>> 6566.9, 6566.7, 6534.3, 6553.8, 6578.9, 6602.1, 6638.1, 6618.2,
>>>>>> 6629.4, 6658.9, 6649.1, 6677.9, 6670.2, 6684.5, 6668.5, 6651.3,
>>>>>> 6668.6, 6682.6, 6669.8, 6659.6, 6692.4, 6705.4, 6672.6, 6702,
>>>>>> 6709.3, 6698.6, 6714.3, 6717.2, 6735.8, 6768.9, 6771.7, 6786,
>>>>>> 6785.5, 6795.2, 6822.6, 6827.8, 6828.2, 6845.7, 6856.1, 6849.1,
>>>>>> 6856.9, 6845.2, 6843, 6861.8, 6864.3, 6882.6, 6864.9, 6848.7,
>>>>>> 6879.7, 6881.4, 6892.3, 6930.9, 6899.4, 6879.1, 6878.7, 6898.1,
>>>>>> 6899.6, 6894, 6925.8, 6943.9, 6953.9, 6949.8, 6919.6, 6935,
>>>> 6930.3,
>>>>>> 6963.1, 6979.8, 6968.8, 6985.6, 6991, 7006.8, 7007.8, 6972.6,
>>>>>> 6991.4, 7002.6, 7024.7, 7028.3, 7033.3, 7074.1, 7077.5, 7069.9,
>>>>>> 7078.6, 7094.7, 7086, 7116.1, 7154.1, 7177.2, 7179.7, 7208.3,
>>>>>> 7208.8, 7171.6, 7187.1, 7194.9, 7212.3, 7215.8, 7246.8, 7302.5,
>>>>>> 7228.4, 7266.4, 7267.5, 7284.4, 7300.5, 7341.9, 7378.8, 7370,
>>>>>> 7417.1, 7438, 7431, 7420.3), v2063948 = c(359.8, 388.7, 394.7,
>>>>>> 383.6, 375.4, 395.2, 393.5, 410.6, 414, 419, 438.1, 453.4, 481,
>>>>>> 457.2, 466.4, 452.4, 446.7, 469.1, 449.1, 464, 472, 469.6,
>> 441.2,
>>>>>> 461.4, 453.5, 456.9, 436.9, 459.1, 467.7, 464.8, 470.3, 464.8,
>>>>>> 499.8, 467, 449.6, 448.6, 441.9, 449.4, 462.1, 456.6, 456.9,
>>>>>> 460.7, 452, 456.8, 446.4, 450.5, 467.5, 478.6, 447.4, 468.4,
>>>>>> 465.8, 459.4, 480.1, 466.5, 455.8, 454.5, 451.9, 449.9, 414,
>>>>>> 435.2, 446.7, 433.1, 424.8, 425.3, 415.6, 416.5, 457.9, 449.5,
>>>>>> 468.9, 444.1, 439.8, 426.5, 450.9, 446, 455.1, 461.4, 433,
>> 459.3,
>>>>>> 454.2, 448.3, 438.2, 426.1, 443.3, 468.2, 448.9, 435.6, 456.8,
>>>>>> 439.2, 445.8, 451.4, 457.7, 450.8, 457.1, 471.6, 518.1, 525.3,
>>>>>> 573.1, 623.1, 625, 632.7, 671.6, 677.8, 672.5, 666, 643.3,
>> 641.2,
>>>>>> 664.9, 655.6, 644.7, 646.5, 638.5, 636.1, 619.8, 593.1, 614,
>>>>>> 646.8, 645.4, 595.7, 584.8, 585, 588.5, 570.8, 583.5, 576.4,
>>>>>> 585.8, 571, 554.4, 544.6, 571.3, 575.3, 583, 546, 586.4, 556.3,
>>>>>> 555.5, 568.6, 595.9, 568.8, 575.5, 570, 576.9, 613.3, 578.4,
>>>>>> 581.7, 564, 572, 575.5, 569.2, 550, 563.2, 565, 556.1, 540.1,
>>>>>> 552.3, 543.4, 570.3, 550.2, 553.8, 545.8, 541.8, 542.1, 547.2,
>>>>>> 566.7, 539.5, 526.9, 502.8, 521.6, 518.6, 505.1, 509.9, 506.5,
>>>>>> 500.6, 483.7, 485.2, 483.3, 510.2, 515.2, 501.8, 518.3, 499.4,
>>>>>> 507, 508.6, 503.5, 531.1, 481.2, 465.8, 479, 503.5, 491.2,
>> 472.7,
>>>>>> 473.3, 483.9, 485.3, 475.5, 482.1, 445.6, 484.2, 473.7, 459.1,
>>>>>> 417.4, 423, 441.8, 421.4, 428.9, 424, 419.3, 424.3, 430.1,
>> 438.8,
>>>>>> 455, 418.3, 439.1, 448.1, 428.5, 428.5, 416.7, 447.4, 442.2,
>>>>>> 460.5, 470.1, 405.2, 427.9, 448.2), v2063949 = c(5.7, 6.2, 6.3,
>>>>>> 6.1, 5.9, 6.3, 6.2, 6.5, 6.5, 6.6, 6.9, 7.1, 7.5, 7.1, 7.3, 7,
>>>>>> 6.9, 7.3, 6.9, 7.1, 7.2, 7.1, 6.7, 7, 6.9, 6.9, 6.6, 6.9, 7,
>>>>>> 7, 7.1, 7, 7.4, 7, 6.7, 6.7, 6.6, 6.6, 6.8, 6.8, 6.8, 6.8, 6.7,
>>>>>> 6.8, 6.6, 6.6, 6.9, 7, 6.6, 6.9, 6.8, 6.7, 7, 6.8, 6.7, 6.6,
>>>>>> 6.6, 6.5, 6.1, 6.4, 6.5, 6.3, 6.2, 6.2, 6, 6, 6.6, 6.5, 6.8,
>>>>>> 6.4, 6.4, 6.2, 6.5, 6.4, 6.5, 6.6, 6.2, 6.6, 6.5, 6.4, 6.2,
>> 6.1,
>>>>>> 6.3, 6.7, 6.4, 6.2, 6.5, 6.2, 6.3, 6.4, 6.5, 6.4, 6.4, 6.6,
>> 7.3,
>>>>>> 7.4, 8.1, 8.8, 8.8, 9, 9.5, 9.6, 9.5, 9.4, 9.1, 9, 9.3, 9.2,
>>>>>> 9, 9.1, 9, 8.9, 8.7, 8.3, 8.6, 9, 9, 8.3, 8.2, 8.1, 8.1, 7.9,
>>>>>> 8.1, 8, 8.1, 7.9, 7.7, 7.5, 7.9, 8, 8, 7.6, 8.1, 7.7, 7.7, 7.8,
>>>>>> 8.2, 7.8, 7.9, 7.8, 7.9, 8.4, 7.9, 7.9, 7.7, 7.8, 7.8, 7.7,
>> 7.5,
>>>>>> 7.6, 7.6, 7.5, 7.3, 7.5, 7.3, 7.7, 7.4, 7.5, 7.4, 7.3, 7.3,
>> 7.4,
>>>>>> 7.6, 7.3, 7.1, 6.8, 7, 7, 6.8, 6.9, 6.8, 6.8, 6.5, 6.5, 6.5,
>>>>>> 6.8, 6.9, 6.7, 7, 6.7, 6.8, 6.8, 6.7, 7.1, 6.4, 6.2, 6.4, 6.7,
>>>>>> 6.6, 6.3, 6.3, 6.4, 6.4, 6.3, 6.4, 5.9, 6.4, 6.3, 6.1, 5.5,
>> 5.6,
>>>>>> 5.8, 5.5, 5.6, 5.6, 5.5, 5.6, 5.6, 5.7, 5.9, 5.4, 5.7, 5.8,
>> 5.6,
>>>>>> 5.6, 5.4, 5.7, 5.7, 5.9, 6, 5.2, 5.4, 5.7), v2063950 = c(67.3,
>>>>>> 67.2, 67.3, 67.5, 67.4, 67.3, 67.3, 67.3, 67.2, 67.1, 67.3,
>> 67.3,
>>>>>> 67.5, 67.3, 67.4, 67.3, 67.4, 67.5, 67.5, 68, 68.2, 68.2, 68.2,
>>>>>> 68.4, 68.4, 68.7, 68.6, 68.4, 68.4, 68.4, 68.3, 68.3, 68.7,
>> 68.5,
>>>>>> 68.4, 68.6, 68.5, 68.7, 68.5, 68.5, 68.4, 68.5, 68.4, 68.1, 68,
>>>>>> 68.2, 68.3, 68.4, 67.9, 68.2, 68.1, 68.1, 68.4, 68.2, 67.9,
>> 67.9,
>>>>>> 68, 68, 67.6, 67.5, 67.6, 67.3, 67.4, 67.6, 67.7, 67.6, 67.9,
>>>>>> 67.6, 67.7, 67.3, 67.4, 67.5, 67.7, 67.7, 67.8, 67.7, 67.5,
>> 67.8,
>>>>>> 67.9, 67.8, 67.8, 67.9, 67.8, 67.8, 67.7, 67.9, 67.9, 67.8,
>> 67.9,
>>>>>> 67.5, 67.5, 67.5, 67.7, 67.9, 67.5, 67.6, 67.1, 67.3, 67.3,
>> 66.9,
>>>>>> 66.9, 66.8, 66.9, 66.9, 66.8, 66.9, 67.1, 66.8, 67, 66.9, 66.7,
>>>>>> 66.9, 67, 66.9, 67, 67.2, 66.8, 66.4, 66.5, 66.7, 67, 66.6,
>> 66.8,
>>>>>> 66.9, 66.8, 66.9, 66.6, 66.5, 66.5, 66.3, 66.5, 66.2, 66.4,
>> 65.9,
>>>>>> 66.2, 66.3, 66.2, 66.1, 66.2, 66, 66.1, 66.4, 66.2, 66.4, 66.2,
>>>>>> 66.4, 66.3, 66.3, 66.3, 66.4, 66.3, 66.3, 66.2, 66.2, 66.1,
>> 66.2,
>>>>>> 65.9, 66.1, 66, 66, 65.8, 65.6, 66, 65.7, 65.7, 65.8, 65.6,
>> 65.3,
>>>>>> 65.2, 65.3, 65.3, 65.1, 65.2, 65.3, 65.3, 65.4, 65.1, 65.1,
>> 65.1,
>>>>>> 65.2, 65.4, 65.2, 65.2, 65.5, 65.1, 64.9, 64.6, 64.9, 64.8,
>> 64.8,
>>>>>> 64.8, 64.9, 65.1, 65, 64.9, 64.6, 65, 64.8, 64.8, 64.7, 64.8,
>>>>>> 64.9, 64.9, 64.9, 64.4, 64.4, 64.4, 64.5, 64.5, 64.8, 64.8,
>> 64.3,
>>>>>> 64.6, 64.3, 64.4, 64.3, 64.8, 65, 65, 65.3, 64.9, 64.9, 64.8),
>>>>>> v2063951 = c(63.5, 63, 63.1, 63.4, 63.4, 63.1, 63.1, 63,
>>>>>> 62.9, 62.7, 62.7, 62.5, 62.4, 62.5, 62.6, 62.6, 62.7, 62.6,
>>>>>> 62.8, 63.2, 63.3, 63.3, 63.6, 63.6, 63.7, 64, 64.1, 63.7,
>>>>>> 63.6, 63.6, 63.5, 63.6, 63.6, 63.7, 63.8, 64, 64, 64.1,
>>>> 63.8,
>>>>>> 63.8, 63.8, 63.8, 63.9, 63.5, 63.5, 63.7, 63.7, 63.6, 63.4,
>>>>>> 63.5, 63.4, 63.5, 63.6, 63.5, 63.4, 63.4, 63.5, 63.5, 63.5,
>>>>>> 63.2, 63.2, 63, 63.2, 63.4, 63.6, 63.5, 63.4, 63.2, 63.1,
>>>>>> 62.9, 63.1, 63.4, 63.3, 63.4, 63.4, 63.2, 63.3, 63.3, 63.5,
>>>>>> 63.4, 63.6, 63.8, 63.5, 63.3, 63.3, 63.7, 63.5, 63.6, 63.7,
>>>>>> 63.2, 63.1, 63.2, 63.3, 63.4, 62.6, 62.6, 61.7, 61.4, 61.3,
>>>>>> 60.9, 60.5, 60.4, 60.6, 60.6, 60.7, 60.8, 60.9, 60.7, 60.9,
>>>>>> 60.8, 60.7, 60.9, 61.2, 61.4, 61.3, 61.2, 60.8, 60.9, 61.1,
>>>>>> 61.3, 61.5, 61.3, 61.4, 61.6, 61.4, 61.6, 61.5, 61.5, 61.3,
>>>>>> 61, 61.1, 61.2, 61, 60.9, 61.1, 61.1, 60.8, 61, 60.9, 60.8,
>>>>>> 60.9, 60.8, 60.9, 61.2, 61.1, 61.2, 61.2, 61.2, 61.3, 61.3,
>>>>>> 61.2, 61.3, 61.3, 61.2, 61.2, 61.1, 61, 61.1, 61.1, 61.2,
>>>>>> 61, 60.8, 61, 61, 61, 61.3, 61, 60.8, 60.7, 60.8, 60.8,
>>>> 60.7,
>>>>>> 60.9, 61, 61.1, 61, 60.6, 60.7, 60.6, 60.8, 60.9, 60.8,
>>>> 60.9,
>>>>>> 60.9, 60.9, 60.9, 60.5, 60.6, 60.6, 60.7, 60.7, 60.7, 61,
>>>>>> 60.9, 60.8, 60.8, 60.9, 60.7, 60.9, 61.1, 61.2, 61.1, 61.3,
>>>>>> 61.2, 60.8, 60.9, 60.8, 60.9, 60.8, 61, 61.3, 60.6, 60.8,
>>>>>> 60.7, 60.8, 60.8, 61.1, 61.3, 61.2, 61.4, 61.5, 61.3, 61.1
>>>>>> ), v382201 = c(212164, 193583, 215866, 211263, 222871,
>>>> 215519,
>>>>>> 219942, 216251, 207703, 212013, 207781, 219596, 222977,
>>>> 202836,
>>>>>> 225014, 217040, 223269, 212081, 211644, 208999, 200885,
>>>> 207899,
>>>>>> 204091, 216573, 218997, 195583, 215113, 206360, 214884,
>>>> 208636,
>>>>>> 214072, 211055, 206974, 211607, 205837, 215020, 216294,
>>>> 201218,
>>>>>> 216097, 209029, 216690, 207829, 211911, 212123, 205356,
>>>> 211083,
>>>>>> 205076, 212205, 212779, 191520, 213090, 209484, 215975,
>>>> 205127,
>>>>>> 207085, 209844, 201884, 206749, 200246, 208719, 211380,
>>>> 190749,
>>>>>> 209346, 200994, 207431, 198322, 201983, 200869, 196045,
>>>> 199493,
>>>>>> 195149, 205332, 207126, 185698, 207850, 205701, 216965,
>>>> 209296,
>>>>>> 215644, 214061, 205121, 209924, 204004, 213241, 217848,
>>>> 203144,
>>>>>> 213848, 206368, 212813, 200362, 202502, 202339, 195491,
>>>> 201314,
>>>>>> 196093, 206104, 208701, 190879, 213025, 208090, 217292,
>>>> 210890,
>>>>>> 216526, 213092, 206553, 209884, 204042, 211314, 212389,
>>>> 192037,
>>>>>> 213815, 208826, 216756, 207533, 210091, 212435, 204595,
>>>> 209379,
>>>>>> 203827, 210218, 212451, 192470, 214095, 208717, 217913,
>>>> 212215,
>>>>>> 217087, 218783, 209940, 214059, 208998, 220829, 223171,
>>>> 208246,
>>>>>> 222458, 215499, 221694, 213553, 219458, 221757, 214203,
>>>> 221003,
>>>>>> 214380, 221215, 220019, 197689, 218045, 210866, 218954,
>>>> 209825,
>>>>>> 212145, 218399, 210138, 215187, 206605, 211504, 212024,
>>>> 192311,
>>>>>> 212260, 207136, 215097, 210584, 218528, 220739, 214149,
>>>> 218748,
>>>>>> 208968, 216712, 218399, 197318, 223819, 221927, 234903,
>>>> 228622,
>>>>>> 233660, 231277, 221779, 227918, 220888, 231634, 235272,
>>>> 220580,
>>>>>> 233970, 226381, 235613, 227998, 234353, 235796, 230542,
>>>> 239605,
>>>>>> 233645, 245124, 248935, 226969, 252118, 245888, 253940,
>>>> 242869,
>>>>>> 249518, 250305, 242427, 251666, 242906, 253645, 257586,
>>>> 236425,
>>>>>> 262768, 252595, 262910, 254396, 254417, 252449, 241404,
>>>> 246729,
>>>>>> 237361, 248692, 251752, 228933, 256286, 249336, 255692,
>>>> 247109,
>>>>>> 251855), v41691004 = c(96.1, 96.7, 97.1, 96.6, 98, 95.7,
>>>>>> 97, 98.2, 97.3, 98.3, 98.4, 98.6, 95.7, 98.4, 99.3, 99.7,
>>>>>> 99.1, 100.4, 99.8, 100.5, 99.8, 99.9, 103.6, 103.7, 101.6,
>>>>>> 104.4, 103.6, 104.1, 104.8, 104.9, 104.8, 105.2, 104.7,
>>>> 103.8,
>>>>>> 104.5, 105.7, 104.6, 105.6, 106.8, 107.1, 107, 107.1, 108.5,
>>>>>> 108.6, 109.1, 109.6, 109.8, 109, 109.3, 110.3, 111, 111.8,
>>>>>> 110.8, 109.9, 111.4, 111, 110.8, 111.4, 111, 111.6, 108.9,
>>>>>> 111.2, 110.9, 111.5, 111.7, 113.2, 112.7, 112.2, 111.1,
>>>> 112.6,
>>>>>> 112.2, 113.7, 113.5, 114.2, 113.4, 114.7, 113.4, 113.8, 114,
>>>>>> 113.9, 112.9, 114.2, 114.5, 116.7, 117.5, 120.1, 120.5, 123,
>>>>>> 125.5, 126.9, 126.9, 128.3, 127.5, 129.4, 129.9, 131, 128.9,
>>>>>> 130.4, 130.4, 132.6, 134.3, 133.2, 133.7, 133.4, 132.7,
>>>> 134.3,
>>>>>> 134.4, 137.5, 134.8, 137.1, 136.3, 136.6, 136, 135.8, 134.6,
>>>>>> 135.6, 136.1, 136.4, 136.2, 138, 136.4, 137.9, 139.7, 140.2,
>>>>>> 139.9, 140.9, 141.9, 139.8, 139.4, 140.1, 140, 142.3, 139.1,
>>>>>> 141.3, 141.3, 141.9, 139, 140.2, 140.3, 141.2, 139.8, 145.6,
>>>>>> 143, 142.1, 140.9, 145, 142.5, 143.6, 144, 143.6, 145.1,
>>>>>> 144.5, 143.9, 143.4, 142.7, 142.6, 140.9, 146, 142.6, 144.2,
>>>>>> 145.4, 143.9, 145.8, 146.2, 144.3, 143.4, 144.2, 146.8,
>>>> 141.5,
>>>>>> 147.4, 146.1, 144.8, 146.4, 144.9, 147, 146, 143.2, 142.5,
>>>>>> 141.9, 146.1, 144.8, 146.9, 143.7, 146, 151.6, 144.1, 146.8,
>>>>>> 145.6, 146.2, 146.3, 147.1, 148.1, 145, 145.1, 149.3, 148.6,
>>>>>> 147.9, 148.6, 149.9, 150.4, 148.2, 150.9, 149, 151.4, 147.8,
>>>>>> 149.5, 149.2, 149.1, 151.8, 150.9, 151.2, 150, 149.5, 152.5,
>>>>>> 152, 153.6, 149.9, 153.7, 152.4, 149.8, 151.8, 150, 152.5
>>>>>> ), v41691014 = c(103.9, 106.6, 106.1, 105.2, 107.6, 105.4,
>>>>>> 108, 106.5, 102.2, 101.4, 97.5, 105.2, 102.7, 101.5, 95.5,
>>>>>> 101.7, 99, 100.5, 101.9, 98.6, 98.2, 100.7, 97.2, 102.4,
>>>>>> 100.9, 103.6, 105.1, 104.9, 105.3, 100.9, 104.1, 104.8,
>>>> 98.1,
>>>>>> 98.3, 101.5, 101.9, 102.2, 101.8, 95.6, 97.6, 97.9, 102.3,
>>>>>> 98.7, 102.7, 96.4, 101.3, 98, 95.4, 95.6, 101.3, 100.6,
>>>> 97.5,
>>>>>> 101.2, 98.8, 96.1, 98.4, 92.6, 99.2, 96.9, 100, 99.5, 103.8,
>>>>>> 103.8, 104.1, 105.7, 106.9, 104.9, 103, 95.5, 101.5, 100.8,
>>>>>> 101.9, 104.2, 104.3, 104.5, 104.4, 105.8, 105.2, 105.5,
>>>> 105.4,
>>>>>> 98.3, 100.4, 98.3, 98, 101.7, 102.9, 110.7, 120.4, 121.3,
>>>>>> 120.1, 121.8, 121.1, 117.8, 122.4, 133.1, 134.9, 135.2,
>>>> 137.7,
>>>>>> 140, 140.8, 140.8, 140.8, 140.5, 139.9, 135.4, 138.2, 138.6,
>>>>>> 138.5, 137, 140.4, 139.6, 136.9, 136.5, 136.9, 137.1, 136,
>>>>>> 130.9, 136.1, 139, 136.5, 139.4, 141.2, 143.1, 145.1, 142.5,
>>>>>> 143.6, 142.9, 142.1, 134.4, 141.4, 139.4, 141.8, 142.4,
>>>> 143.5,
>>>>>> 143.1, 143.4, 143, 142.3, 143, 143.3, 137.5, 141.6, 141.2,
>>>>>> 142.9, 138.9, 143.3, 143.8, 140.8, 137, 138.6, 137.8, 137.5,
>>>>>> 132, 137.8, 136, 135.7, 136.2, 137.2, 139.6, 140.2, 140.2,
>>>>>> 139.9, 139.8, 140, 138.9, 139.3, 138.6, 139, 138.1, 139.1,
>>>>>> 144.6, 144.4, 143.3, 143.9, 142.9, 143.8, 143.2, 142.6,
>>>> 141.5,
>>>>>> 141.1, 145.3, 145.8, 145.2, 145.3, 140.5, 140.1, 139.7,
>>>> 137.1,
>>>>>> 134.5, 135.4, 133.6, 132.4, 132.3, 133.5, 134, 133.7, 133,
>>>>>> 133.2, 133, 132.9, 131.3, 133.5, 133.2, 132.8, 133.4, 134,
>>>>>> 134.5, 134.4, 134.5, 134.8, 133.5, 130.4, 130.5, 131.5, 131,
>>>>>> 130.4, 131.3, 130.4, 131.5, 131.5, 130.7, 131.3, 130),
>>>> v41691919
>>>>>> = c(96.5,
>>>>>> 97.1, 97.7, 98.1, 98.7, 98.7, 98.5, 98.5, 98.7, 98.4, 97.7,
>>>>>> 97.7, 97.7, 98.4, 99.5, 99.5, 99.5, 99.8, 100.5, 101.3,
>>>> 100.9,
>>>>>> 101.1, 101.4, 100.4, 101.9, 102.7, 102.8, 101.8, 102.2,
>>>> 102.3,
>>>>>> 102.5, 103, 103.2, 102.9, 103.1, 103.4, 103.4, 103.6, 104,
>>>>>> 104.1, 105, 104.8, 104.9, 104.7, 104.8, 105, 105.4, 105.3,
>>>>>> 105.1, 105.8, 106.4, 106.5, 106.6, 106.8, 106.9, 107.5,
>>>> 108.2,
>>>>>> 107.7, 107.5, 107.6, 108.2, 107.9, 108.8, 109.1, 109.5,
>>>> 109.3,
>>>>>> 109, 109.1, 108.5, 108.4, 108.6, 108.8, 108.6, 109.7, 110.8,
>>>>>> 111.1, 111.6, 111.1, 111.1, 110.9, 111, 110.9, 111.2, 111.1,
>>>>>> 110.9, 111.4, 111.7, 112.5, 113.6, 114.2, 115.1, 114.8,
>>>> 115.1,
>>>>>> 113.7, 113.5, 112.8, 112.4, 113.1, 113.7, 113.2, 114, 114.2,
>>>>>> 113.7, 113.7, 113.8, 113.9, 114.6, 114.1, 114.5, 115.1,
>>>> 115.3,
>>>>>> 115.7, 116.2, 116, 117, 117, 117.1, 117.8, 118, 117.9,
>>>> 117.8,
>>>>>> 118, 119.4, 119.9, 120.9, 120.2, 120.5, 120.6, 121.1, 121,
>>>>>> 121, 120.3, 120.6, 121.4, 122, 122.4, 122.4, 121.6, 121.4,
>>>>>> 121.8, 122, 122.2, 121.9, 121.3, 121.3, 122.8, 123.2, 122.9,
>>>>>> 123, 123.2, 123.4, 123.4, 123.5, 123.3, 123.3, 123.1, 123.3,
>>>>>> 124.6, 125.1, 125.9, 126.5, 126.9, 126.5, 126.5, 126.7,
>>>> 126.8,
>>>>>> 126.3, 125.4, 125.3, 126.2, 127.1, 126.9, 127.7, 128.2,
>>>> 128.4,
>>>>>> 128, 127.8, 127.9, 127.9, 127.5, 127.8, 128.2, 129, 129.6,
>>>>>> 130.1, 130.4, 130.3, 129.9, 130.1, 130.6, 130.2, 130, 130.8,
>>>>>> 131.2, 131.4, 132, 131.9, 132.1, 131.9, 131.8, 132.3, 132.3,
>>>>>> 132.7, 132, 133.2, 134, 134.6, 134.8, 134.9, 135.3, 136,
>>>>>> 135.9, 135.2, 135.6, 135.1, 135, 135.2, 136, 137, 137.4,
>>>>>> 138.1, 138, 138.8), v41691920 = c(95, 95.9, 96.6, 96.8,
>>>> 97.7,
>>>>>> 98.1, 98, 97.4, 96.7, 96.8, 97.4, 98, 99, 99.9, 100.3,
>>>> 100.2,
>>>>>> 100.1, 100.9, 100.6, 100.3, 99.1, 98.5, 100.2, 100.9, 102,
>>>>>> 102.1, 102, 102, 101.6, 102.1, 102.5, 101.6, 100.5, 100.2,
>>>>>> 100.9, 102.2, 102.5, 102.5, 102.6, 102.2, 103.2, 103.8,
>>>> 104.3,
>>>>>> 104, 103.3, 103.7, 104.9, 105.9, 105.4, 106, 106.4, 107.2,
>>>>>> 107.5, 107.8, 107.3, 107.3, 105.9, 105.9, 106.6, 107.6, 109,
>>>>>> 108.8, 109.2, 108.9, 109.2, 109.5, 108.9, 109, 108.2, 108.3,
>>>>>> 109.6, 109.5, 110.8, 112.9, 112.3, 112, 113, 112.5, 112.2,
>>>>>> 111.9, 110.5, 110.1, 111.2, 111.7, 112.5, 113, 112.8, 113.9,
>>>>>> 115.2, 116.1, 117, 117.1, 117.3, 117.4, 119.6, 119.7, 120.8,
>>>>>> 121.5, 121.6, 121.8, 121.6, 122.3, 122.3, 121.2, 120.2, 120,
>>>>>> 121.6, 122.3, 123, 123.3, 123.8, 123, 123.1, 123.1, 123.6,
>>>>>> 123.5, 122.5, 122.2, 122.9, 124.1, 124.9, 125.2, 127.4,
>>>> 127.1,
>>>>>> 128.5, 128.9, 129.7, 129.6, 128.4, 128.1, 129.6, 129.4,
>>>> 130.6,
>>>>>> 131, 130.5, 130.9, 131.6, 130.7, 131.5, 131.9, 130.1, 130.5,
>>>>>> 131.5, 131.5, 131.7, 133.3, 132.8, 132.9, 132.9, 133.6,
>>>> 133.5,
>>>>>> 133.5, 131.7, 132.4, 133.5, 133, 133.7, 135.1, 135.1, 135.1,
>>>>>> 136.7, 137.8, 137.5, 136.7, 136, 135.5, 137.6, 137.7, 140.2,
>>>>>> 140.3, 140.2, 140, 141.7, 141.9, 141.6, 141.4, 140, 141.3,
>>>>>> 142.4, 143.6, 145.7, 146.9, 146.1, 145.1, 144, 143.8, 144,
>>>>>> 142.4, 140.5, 140.8, 141.6, 141.5, 142.1, 142.6, 142.4, 143,
>>>>>> 143.6, 144.1, 145.1, 144.3, 142.8, 142, 143.3, 144.3, 146.3,
>>>>>> 146.6, 146, 147, 146.7, 148.3, 148.9, 148.1, 146.7, 146.3,
>>>>>> 148, 150.2, 151.6, 151.9, 151.9, 151.8, 152.7, 153.7, 153.8
>>>>>> ), v41691952 = c(97.3, 97.4, 98.9, 98.8, 99, 99.6, 99.8,
>>>>>> 99.7, 99.2, 99.9, 99.3, 99.5, 99.5, 99.6, 100.4, 99.1, 99.3,
>>>>>> 99, 99.4, 101.5, 100.9, 101.9, 101.6, 97.7, 101.5, 101.8,
>>>>>> 102.2, 100.4, 102.3, 103.1, 103, 103.5, 103.6, 103.7, 103.8,
>>>>>> 104.4, 103.9, 104.1, 104.4, 105, 105.2, 105.3, 106, 106.1,
>>>>>> 106.3, 106.7, 107, 107.1, 107.4, 107.9, 108.2, 108.2, 108.6,
>>>>>> 108.8, 109.2, 109.3, 109.6, 110.3, 110.4, 110.2, 111.7, 112,
>>>>>> 112.2, 111.7, 113, 112.9, 112.5, 112.7, 112.8, 113, 112.4,
>>>>>> 112.9, 112.9, 112.9, 113.3, 114.2, 114.2, 114.4, 114.4,
>>>> 114.7,
>>>>>> 115, 116, 115.5, 115.8, 115.6, 115.9, 116.3, 117.1, 117.5,
>>>>>> 117.6, 119.9, 120, 119.8, 120.1, 120.6, 120.6, 120.3, 120.3,
>>>>>> 120.2, 118.1, 118.3, 118.3, 117.8, 117.7, 117.7, 118.2, 118,
>>>>>> 118.2, 118.6, 118.6, 118.8, 120, 121.4, 121.4, 122.9, 122.5,
>>>>>> 122.5, 123.2, 122.8, 123, 122.4, 121.5, 122, 122.2, 123,
>>>>>> 123.1, 123.5, 123.6, 123.6, 123.8, 123.9, 124.1, 124.3,
>>>> 124.3,
>>>>>> 124.4, 124.4, 125.2, 125.3, 125.4, 125.4, 125.2, 125.5,
>>>> 125.4,
>>>>>> 125.4, 125.9, 126.1, 126.1, 126.1, 126.4, 126.6, 127.1,
>>>> 127.2,
>>>>>> 127.4, 127.4, 128.1, 128.3, 128.8, 128.9, 129.1, 131.9,
>>>> 132.4,
>>>>>> 132.3, 132.1, 132.1, 132.1, 132.3, 132.6, 132.9, 133.4,
>>>> 133.5,
>>>>>> 133.8, 133.5, 134.1, 134.4, 134.9, 135, 135.1, 135, 135.3,
>>>>>> 135.5, 136.1, 136.2, 136.5, 136.9, 137.5, 138, 138.5, 138.7,
>>>>>> 138.9, 139.5, 139.9, 140.1, 140.2, 140.2, 140.5, 140.7,
>>>> 140.3,
>>>>>> 140.6, 139.8, 140, 140.1, 140.5, 140.8, 141.1, 141.7, 141.9,
>>>>>> 141.9, 141.9, 142.1, 142.4, 142.6, 142.9, 143.2, 143.7,
>>>> 143.9,
>>>>>> 144.1, 145, 145.5, 145.8, 146.5, 146.6, 146.8, 147.3),
>>>> v41691965
>>>>>> = c(96.9,
>>>>>> 97.5, 97.8, 98.7, 98.8, 98.9, 98.7, 98.9, 100, 99.4, 98.9,
>>>>>> 98.9, 98.8, 99.5, 100.3, 100.2, 100.2, 100.5, 100.8, 100.7,
>>>>>> 100, 99.7, 99.9, 99.5, 99.5, 100.8, 100.8, 101, 101.1, 101,
>>>>>> 101.1, 101.1, 101.6, 101.7, 101.3, 101.5, 101.2, 101.5,
>>>> 101.7,
>>>>>> 102.2, 101.9, 101.4, 101.2, 101.3, 101.5, 101.3, 101.2,
>>>> 101.2,
>>>>>> 101.2, 101.3, 101.7, 101.3, 101.5, 101.6, 101.2, 101.5,
>>>> 102.2,
>>>>>> 101.9, 102.5, 102.5, 102.4, 102.5, 102.3, 102.3, 102.6,
>>>> 101.7,
>>>>>> 100.9, 101.3, 101.9, 102.2, 102.3, 102.1, 102.3, 103, 103.4,
>>>>>> 103.6, 103.3, 102.9, 103.3, 103.6, 103.9, 103.6, 103.4,
>>>> 103.2,
>>>>>> 103.5, 104.9, 104.6, 104.6, 104.6, 104.4, 104.6, 104.6,
>>>> 106.1,
>>>>>> 105.6, 106, 105.9, 105.9, 106.8, 107.1, 107.6, 107.6, 107.1,
>>>>>> 106.6, 106.3, 107.9, 107.5, 108.6, 107.6, 108.1, 109.3,
>>>> 109.2,
>>>>>> 109.2, 109.4, 108.9, 110, 109.7, 110.4, 109.7, 110.1, 110,
>>>>>> 110.1, 110.6, 111, 110.9, 111.5, 111.7, 111.8, 112.4, 113,
>>>>>> 113.1, 113.3, 113.1, 113.3, 114.2, 113.8, 114, 114.1, 113.9,
>>>>>> 113.9, 113.4, 113.5, 114.2, 114.5, 114, 114.4, 115.1, 115.6,
>>>>>> 115.6, 115.2, 115.6, 116.1, 115.5, 115.6, 115.4, 115.6,
>>>> 115.6,
>>>>>> 115.6, 116.2, 116.8, 116, 116.4, 117.6, 117.7, 119.2, 119.2,
>>>>>> 119.6, 119.6, 119.5, 119.8, 120.7, 121, 121.6, 121.7, 122,
>>>>>> 121.9, 122.2, 122.4, 122.3, 121.7, 121.1, 121.4, 122.3,
>>>> 122.6,
>>>>>> 123.1, 123.7, 124.1, 124.3, 123.4, 123.7, 124.1, 123.7,
>>>> 123.5,
>>>>>> 123.6, 124.4, 124.2, 124.8, 124.4, 124.8, 124.2, 123.5,
>>>> 123.2,
>>>>>> 124.1, 124.8, 122.3, 124.9, 126.4, 125.7, 126.2, 125.2,
>>>> 125.2,
>>>>>> 125.9, 125.7, 125.5, 126.6, 126.3, 125.9, 125.8, 125.3,
>>>> 124.9,
>>>>>> 125.3, 125.3, 125.7, 124.9), v41691980 = c(102.4, 103.2,
>>>>>> 105.3, 101, 100.4, 101.2, 101.2, 101.8, 102.7, 101.9, 100.5,
>>>>>> 99.5, 95.8, 99, 101.8, 101.8, 99, 99.1, 101, 100.9, 101.9,
>>>>>> 101.8, 100.1, 97.8, 96.6, 98.7, 99.9, 97.6, 97.7, 94.9,
>>>> 95.6,
>>>>>> 96.1, 99.2, 98.5, 99.6, 98, 98, 99.3, 100.2, 98.9, 97.7,
>>>>>> 96.8, 95.3, 96.5, 99.8, 100.1, 98.1, 95.7, 92, 97.2, 99.3,
>>>>>> 97.1, 96, 93.7, 93.2, 96.1, 99.7, 99.7, 95.8, 94.7, 92.1,
>>>>>> 92.3, 97, 93.5, 94.1, 91.8, 90.2, 92.7, 95.2, 93.9, 94.4,
>>>>>> 90.9, 90.7, 92.5, 95.4, 96.3, 94, 90.1, 92.7, 94.2, 94.7,
>>>>>> 95, 94.6, 92.2, 89.4, 92.5, 93.7, 91.6, 90.2, 90.4, 92.5,
>>>>>> 93.1, 95.5, 93.1, 93, 91.2, 90.4, 92.9, 95.2, 94.7, 92.6,
>>>>>> 89.1, 89.6, 90.6, 93.2, 93.3, 94, 88.9, 88.4, 90.2, 91.9,
>>>>>> 92.6, 92.2, 88.2, 88, 89.7, 93.1, 95, 91.5, 88.1, 86.3, 89,
>>>>>> 93.7, 93.7, 93.1, 91, 89.9, 90.6, 95.8, 95.4, 92, 88.6,
>>>> 88.3,
>>>>>> 92.6, 94.6, 93.3, 91.2, 88.6, 86.5, 86.5, 90, 92.1, 88.6,
>>>>>> 85.7, 83.8, 88.7, 93, 91.4, 91.1, 88.8, 87.6, 89.1, 89.8,
>>>>>> 90.3, 87.9, 85.4, 85.2, 89.3, 92, 93.6, 93.2, 91.7, 90.9,
>>>>>> 91.2, 94, 95.8, 93.3, 90.4, 89.3, 91, 93.7, 93.4, 92.3,
>>>> 91.5,
>>>>>> 91.7, 91.4, 93.6, 94.9, 94.2, 89.8, 88.1, 88.7, 92.9, 93.6,
>>>>>> 93.6, 92.6, 90.6, 91.8, 94.3, 95.3, 93.2, 89.7, 88.2, 89.3,
>>>>>> 91, 90.2, 90.3, 89.2, 88.9, 89.7, 90.6, 91.7, 91.6, 88.1,
>>>>>> 87.6, 88.5, 90.5, 91.7, 90.6, 89.5, 88.8, 90.1, 91.9, 93.8,
>>>>>> 92.6, 88.9, 89, 91.1, 92.3, 92.6, 92, 90.4, 90.2), v41691988
>>>> =
>>>>>> c(98.2,
>>>>>> 98.6, 97.4, 99.3, 100.4, 98.8, 97.3, 97.8, 99, 97.3, 94.9,
>>>>>> 94.4, 95.3, 95.8, 97.8, 99.8, 99.6, 99.3, 100.4, 101.3,
>>>> 101.2,
>>>>>> 101.9, 102.9, 104.5, 106.3, 107.8, 107.7, 105.6, 104.2,
>>>> 103.9,
>>>>>> 104.2, 106.9, 106.5, 105.3, 105.9, 105.9, 107.1, 106.6,
>>>> 106.8,
>>>>>> 107, 110.4, 109, 108.6, 107.2, 106.5, 107.4, 109, 108.4,
>>>>>> 109.5, 109.7, 110.6, 111, 110.5, 111.3, 111.7, 113.7, 116.5,
>>>>>> 113.7, 113.1, 113.4, 115.1, 112.5, 114.3, 117.8, 116.8,
>>>> 116.9,
>>>>>> 117.7, 117, 112.1, 111.7, 112.8, 114.2, 112.7, 115.1, 118.7,
>>>>>> 118.8, 120.3, 119.2, 118.2, 116.1, 116.6, 115.4, 117.3, 118,
>>>>>> 118, 117.1, 117.6, 120.3, 123.7, 125.8, 125.7, 123.5, 122.5,
>>>>>> 116.5, 113.1, 110.9, 109.2, 110.2, 111.2, 111.2, 114.5,
>>>> 116.7,
>>>>>> 114.9, 115.8, 114.4, 115, 117, 117, 118.8, 118.5, 119,
>>>> 119.1,
>>>>>> 119.5, 119, 119.9, 119.3, 118.9, 121.4, 124, 124.2, 125.8,
>>>>>> 125.6, 127.4, 130.2, 132.4, 129.1, 128.9, 128.9, 129.5,
>>>> 129.5,
>>>>>> 130.3, 128.1, 130.2, 130.8, 132.1, 134.1, 132.3, 129.7, 129,
>>>>>> 130.2, 130.8, 130.7, 130.1, 128.7, 130.1, 133.3, 132.3,
>>>> 131.3,
>>>>>> 131.2, 132.1, 132.3, 131.8, 131.8, 131.2, 130.8, 131.7,
>>>> 132.3,
>>>>>> 133.8, 134.1, 134.7, 134.8, 135.7, 133.9, 132.9, 131.8,
>>>> 131.4,
>>>>>> 129.2, 126.1, 123.4, 125.3, 127.3, 127.4, 128.7, 130.4,
>>>> 130.6,
>>>>>> 128.2, 125.7, 125.6, 126.5, 126.3, 125.7, 124.5, 125.6,
>>>> 128.6,
>>>>>> 130.2, 130.8, 129.5, 128.5, 128.3, 129.9, 128.8, 130.3, 134,
>>>>>> 133, 131.7, 134.3, 133.6, 132.9, 132.7, 132.8, 134.5, 134.8,
>>>>>> 136.5, 137, 138.6, 138.9, 139.3, 140.7, 140.7, 141.2, 142.7,
>>>>>> 142, 138.6, 139.2, 137.4, 138.5, 137, 138, 140.5, 142.7,
>>>>>> 143.8, 143.2, 145.2), v41691994 = c(103.8, 103.4, 100.6,
>>>>>> 110.1, 115.3, 105.7, 96.6, 98.7, 104.8, 96.7, 88.7, 85.4,
>>>>>> 88.7, 89.1, 96.7, 103, 100, 98.4, 101.4, 104, 104.4, 106.7,
>>>>>> 104.5, 103, 109.3, 116.4, 115.2, 104.5, 97, 98.6, 100.1,
>>>>>> 111.2, 109.5, 103.7, 100.5, 100.1, 107, 109.3, 110, 111.4,
>>>>>> 127.5, 120.5, 118.5, 117.9, 116.1, 122.8, 115.3, 109.1,
>>>> 114.8,
>>>>>> 117, 123.6, 128.9, 123.8, 128.3, 133.6, 142.9, 159.4, 143.5,
>>>>>> 127.3, 128.7, 138.8, 125.6, 132.5, 150.2, 145.2, 147.9,
>>>> 154.3,
>>>>>> 152.2, 124.9, 122.8, 123, 129, 120.7, 132.6, 149.5, 149.3,
>>>>>> 155.3, 151.1, 149, 141.7, 143.3, 140.5, 147.8, 148.9, 152.1,
>>>>>> 152.2, 156.6, 167.5, 182.1, 192.4, 193.9, 182, 182.2, 156.2,
>>>>>> 122.6, 110.1, 116, 120.4, 124.5, 125.1, 137.6, 145.5, 138,
>>>>>> 143.5, 139.8, 139.8, 144, 139.6, 144.6, 142.1, 147.5, 147.1,
>>>>>> 144.4, 140.9, 151, 149, 148.1, 155.2, 159.2, 162, 167.1,
>>>>>> 168.1, 177.6, 191.4, 195.8, 189.3, 189.5, 186.8, 184, 182,
>>>>>> 177.7, 173.2, 178.2, 184.1, 190.4, 196.8, 188.7, 179.8,
>>>> 182.4,
>>>>>> 187.4, 190.7, 186.1, 179.6, 176.2, 178.2, 190.6, 189.6,
>>>> 183.7,
>>>>>> 182.2, 187, 192.5, 191.3, 191, 182.7, 181.5, 186.1, 184.4,
>>>>>> 189.7, 191.6, 196.8, 197.9, 204.5, 200.6, 192.7, 189.9,
>>>> 181.1,
>>>>>> 167.9, 151, 131.8, 144.8, 152.6, 153.3, 162.9, 172.7, 175.8,
>>>>>> 163.8, 150.9, 145.7, 147.3, 144.2, 136.1, 128.3, 132.2,
>>>> 146.8,
>>>>>> 152.9, 154.8, 149.5, 146.5, 145.7, 151.6, 146.7, 152.1,
>>>> 163.9,
>>>>>> 156.6, 151.8, 168.9, 163.9, 156.8, 158.8, 161.3, 172.8,
>>>> 167.2,
>>>>>> 177.8, 175.7, 180.8, 178.6, 182.5, 194.2, 198.8, 193.7,
>>>> 194.8,
>>>>>> 192, 186.6, 179.2, 161.8, 152, 146.8, 146.3, 166.2, 184,
>>>>>> 186.9, 174.6, 183.5), v42170711 = c(1170046, 1167260,
>>>> 1172192,
>>>>>> 1173559, 1173865, 1163159, 1173230, 1168291, 1148136,
>>>> 1135730,
>>>>>> 1176671, 1184706, 1186957, 1177962, 1194820, 1192486,
>>>> 1204268,
>>>>>> 1196677, 1198166, 1192339, 1188059, 1189860, 1195524,
>>>> 1179193,
>>>>>> 1160881, 1156255, 1154137, 1151079, 1223651, 1193786,
>>>> 1187791,
>>>>>> 1185070, 1222101, 1233158, 1222271, 1205916, 1211049,
>>>> 1237429,
>>>>>> 1221806, 1288146, 1268949, 1239192, 1259979, 1259944,
>>>> 1257909,
>>>>>> 1229472, 1206627, 1245003, 1262388, 1269395, 1276432,
>>>> 1277248,
>>>>>> 1261277, 1291593, 1298114, 1305550, 1267413, 1294688,
>>>> 1295694,
>>>>>> 1317484, 1333362, 1330478, 1362024, 1326774, 1319396,
>>>> 1318673,
>>>>>> 1334044, 1346594, 1339499, 1363158, 1404445, 1434255,
>>>> 1396545,
>>>>>> 1355204, 1367796, 1352577, 1349119, 1374720, 1398253,
>>>> 1369215,
>>>>>> 1413653, 1431392, 1421479, 1419408, 1487702, 1434740,
>>>> 1434398,
>>>>>> 1472990, 1471028, 1460549, 1461166, 1482247, 1490620,
>>>> 1486005,
>>>>>> 1467463, 1444413, 1469876, 1477567, 1460040, 1446679,
>>>> 1452324,
>>>>>> 1456279, 1455826, 1472154, 1485169, 1464240, 1487456,
>>>> 1518692,
>>>>>> 1516207, 1512158, 1535860, 1535652, 1538473, 1549310,
>>>> 1517172,
>>>>>> 1511436, 1531246, 1558108, 1555668, 1566767, 1566882,
>>>> 1580492,
>>>>>> 1566209, 1594096, 1600677, 1613907, 1619388, 1607791,
>>>> 1602639,
>>>>>> 1612385, 1631814, 1643924, 1650496, 1659254, 1679545,
>>>> 1674956,
>>>>>> 1675779, 1676020, 1676843, 1679625, 1694232, 1695255,
>>>> 1720478,
>>>>>> 1741576, 1736856, 1722187, 1750359, 1728204, 1736587,
>>>> 1752139,
>>>>>> 1752173, 1770304, 1762990, 1777806, 1787814, 1766222,
>>>> 1771789,
>>>>>> 1806919, 1817679, 1826720, 1854228, 1850492, 1857490,
>>>> 1892994,
>>>>>> 1890432, 1892879, 1890258, 1911380, 1915088, 1891501,
>>>> 1922679,
>>>>>> 1947153, 1994578, 1970953, 1983867, 2014740, 2008756,
>>>> 2030234,
>>>>>> 2032886, 2059674, 2068652, 2073047, 2083031, 2136894,
>>>> 2115520,
>>>>>> 2130280, 2158663, 2134182, 2172927, 2173367, 2173553,
>>>> 2156196,
>>>>>> 2218283, 2237749, 2206090, 2227664, 2221296, 2238127,
>>>> 2265268,
>>>>>> 2262536, 2284770, 2309970, 2310161, 2307041, 2331779,
>>>> 2369833,
>>>>>> 2388535, 2335561, 2373858, 2397911, 2392511, 2405673,
>>>> 2410779,
>>>>>> 2411443, 2438862, 2483861, 2444050, 2429816, 2451814,
>>>> 2464695,
>>>>>> 2470975, 2496128, 2476598), v44174769 = c(NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, 14870199, 13850896, 13723104, 12678291,
>>>>>> 12481142, 13192192, 14390793, 13792514, 12301574, 12337737,
>>>>>> 12179895, 13722325, 14545047, 12249895, 13035258, 10927584,
>>>>>> 10791532, 11585680, 11896537, 12370088, 10788546, 10909386,
>>>>>> 11181286, 13637486, 13731878, 12072844, 12075216, 10300763,
>>>>>> 10397213, 11345202, 12392196, 12028914, 11059735, 11243073,
>>>>>> 11257174, 13342863, 13491946, 11306554, 12021618, 11138199,
>>>>>> 11643671, 11282364, 12719648, 12056468, 10814757, 10851400,
>>>>>> 10601293, 11868363, 12184798, 11191009, 11849582, 10987177,
>>>>>> 11591640, 11751264, 12496489, 11803400, 10478331, 11611753,
>>>>>> 12350431, 12881577, 13487609, 11618200, 12250783, 11826684,
>>>>>> 12102462, 12172373, 13118089, 12941911, 11778903, 12221484,
>>>>>> 12655891, 13338404, 13206706, 11502235, 12312545, 11265569,
>>>>>> 12023172, 12146589, 12418915, 12514403, 12441607, 12174062,
>>>>>> 12761836, 13383973, 13602164, 12153121, 13222284, 11645090,
>>>>>> 11870180, 11908548, 12536353, 11770646, 10308391, 9717057,
>>>>>> 11975998, 12389373, 14714456, 13612168, 13246479, 12151889,
>>>>>> 11885998, 12568410, 13693121, 13972899, 12538465, 12579381,
>>>>>> 12575806, 13673046, 13741588, 12046519, 13195851, 12099752,
>>>>>> 12305290, 12681867, 13133006, 12799308, 11905570, 12044349,
>>>>>> 12653710, 14138740, 14612989, 12188411, 12860259, 11715642,
>>>>>> 12189386, 12338008, 13697316, 13571423, 12294172, 12624440,
>>>>>> 12973386, 13561183, 14465206, 12636860, 13160251, 11960370,
>>>>>> 12456209, 12804423, 14463529), v47794 = c(2600262, 2215920,
>>>>>> 2120366, 1437092, 708746, 493068, 361536, 438401, 445833,
>>>>>> 778862, 1138557, 1636203, 2233518, 2114752, 1974646,
>>>> 1534387,
>>>>>> 998860, 559524, 447850, 393285, 402189, 651017, 1523288,
>>>>>> 2076491, 2751527, 2619063, 2246900, 1555782, 855432, 475902,
>>>>>> 452943, 420167, 414293, 850387, 1358940, 1968115, 2708192,
>>>>>> 2497621, 1948959, 1466106, 828148, 462889, 422246, 405036,
>>>>>> 425039, 687867, 1280263, 2001790, 2666491, 2470415, 2263741,
>>>>>> 1411442, 900073, 491461, 413880, 390364, 393934, 586734,
>>>>>> 1266299, 2202400, 2309033, 2164470, 2145389, 1328220,
>>>> 736964,
>>>>>> 480707, 445443, 369321, 424694, 845898, 1385128, 1748853,
>>>>>> 3283157, 3804876, 3244792, 2477267, 1569953, 1169971,
>>>> 1121247,
>>>>>> 1218442, 1182862, 1409876, 2241749, 3274756, 3494706,
>>>> 3615470,
>>>>>> 3455902, 2199773, 1647961, 1230718, 1125165, 1078839,
>>>> 1099809,
>>>>>> 1539702, 2190793, 3126922, 3886115, 3290320, 2942615,
>>>> 2110948,
>>>>>> 1405583, 1308947, 1014607, 1148503, 1066941, 1596998,
>>>> 2039112,
>>>>>> 2798847, 3446040, 3255602, 2815621, 1839592, 1426302,
>>>> 1177683,
>>>>>> 1090902, 1145557, 1140902, 1524916, 2145484, 3256107,
>>>> 3775523,
>>>>>> 3559115, 3289994, 2219521, 1774688, 1291471, 1191457,
>>>> 1170953,
>>>>>> 1159087, 1543907, 2159364, 2786142, 3447635, 3179625,
>>>> 2751435,
>>>>>> 2152765, 1596927, 1290466, 1278246, 1247608, 1253886,
>>>> 1622877,
>>>>>> 2433993, 2831938, 3565488, 3636268, 3253630, 2559152,
>>>> 1626984,
>>>>>> 1251460, 1163841, 1130486, 1190063, 1491645, 2345181,
>>>> 3346689,
>>>>>> 4165953, 3842336, 3682352, 2580288, 1731497, 1199181,
>>>> 1098817,
>>>>>> 1113400, 1079822, 1464918, 2344949, 3006111, 3847035,
>>>> 4007847,
>>>>>> 3550039, 2397284, 1457876, 1155121, 1115934, 1113953,
>>>> 1185585,
>>>>>> 1421534, 2074815, 2726464, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,
>>>>>> NA, NA, NA, NA, NA), v52300164 = c(75.656, 69.085, 78.652,
>>>>>> 76.864, 72.529, 81.702, 56.027, 83.519, 70.07, 69.387,
>>>> 77.807,
>>>>>> 74.695, 100.946, 79.579, 93.638, 78.087, 86.824, 73.918,
>>>>>> 87.194, 83.403, 76.443, 89.991, 86.48, 72.567, 72.041,
>>>> 114.235,
>>>>>> 82.83, 79.503, 73.893, 78.598, 81.738, 91.451, 86.032,
>>>> 108.011,
>>>>>> 79.145, 77.618, 65.539, 79.438, 102.972, 90.846, 85.324,
>>>>>> 83.823, 77.963, 104.5, 85.947, 95.124, 85.782, 76.143,
>>>> 57.357,
>>>>>> 71.102, 79.966, 90.391, 70.392, 99.74, 101.093, 68.519,
>>>> 70.549,
>>>>>> 64.984, 84.727, 80.825, 78.937, 78.679, 81.605, 74.906,
>>>> 74.252,
>>>>>> 84.369, 86.341, 66.781, 61.618, 74.485, 69.132, 63.522,
>>>> 74.888,
>>>>>> 52.395, 57.398, 65.234, 74.207, 65.783, 73.555, 68.3,
>>>> 82.511,
>>>>>> 71.971, 78.523, 48.721, 65.401, 72.862, 78.834, 82.79,
>>>> 72.919,
>>>>>> 92.277, 56.461, 93.681, 87.929, 89.085, 56.675, 53.501,
>>>> 43.62,
>>>>>> 37.324, 59.424, 36.481, 42.83, 50.28, 41.606, 48.622,
>>>> 57.833,
>>>>>> 72.492, 53.894, 53.125, 54.779, 70.139, 58.95, 67.712,
>>>> 69.299,
>>>>>> 56.243, 56.468, 64.636, 60.448, 46.748, 81.488, 45.477,
>>>> 50.752,
>>>>>> 66.029, 73.787, 77.554, 58.62, 75.255, 76.621, 66.846,
>>>> 66.99,
>>>>>> 75.322, 54.47, 71.371, 80.795, 71.066, 87.498, 99.376,
>>>> 76.071,
>>>>>> 70.67, 72.869, 87.986, 73.416, 68.684, 60.868, 79.346,
>>>> 47.764,
>>>>>> 68.905, 58.11, 49.672, 65.877, 59.735, 62.198, 70.928,
>>>> 60.752,
>>>>>> 71.221, 60.905, 56.839, 64.447, 61.82, 42.108, 67.44,
>>>> 65.443,
>>>>>> 55.605, 68.712, 52.171, 53.423, 52.954, 58.842, 60.084,
>>>> 61.375,
>>>>>> 41.948, 62.912, 64.89, 84.012, 57.381, 51.031, 96.148,
>>>> 90.318,
>>>>>> 75.115, 87.755, 55.083, 61.053, 76.113, 86.185, 68.21,
>>>> 72.047,
>>>>>> 88.35, 79.231, 74.712, 69.061, 84.729, 58.144, 79.135,
>>>> 100.138,
>>>>>> 84.905, 88.793, 74.315, 54.912, 78.051, 82.618, 97.755,
>>>> 77.894,
>>>>>> 59.878, 97.51, 64.46, 84.769, 105.506, 74.729, 69.223,
>>>> 54.825,
>>>>>> 104.078, 68.996, 64.279, 78.005, 91.656, 85.848, 70.439,
>>>>>> 73.273, 56.563, 61.413, 84.829, 48.976, 66.784, 69.821),
>>>>>> v52367573 = c(9464438, 9389850, 9414772, 9544913, 9572846,
>>>>>> 9584905, 9382415, 9482770, 9307864, 9492970, 9715825,
>>>> 9940764,
>>>>>> 10187105, 9817105, 9894764, 10188403, 9752106, 10139040,
>>>>>> 10087479, 10102348, 10016255, 10262020, 10237805, 10307612,
>>>>>> 10275184, 10419349, 10293724, 10289432, 10424067, 10431761,
>>>>>> 10527812, 10580315, 10595496, 10523776, 10425828, 10335721,
>>>>>> 10424135, 10732762, 10659720, 10501711, 10717818, 10609381,
>>>>>> 10645290, 10737670, 10887666, 10937108, 11132192, 10931217,
>>>>>> 11103939, 11305801, 11075886, 11028720, 11033612, 11241378,
>>>>>> 11397097, 11326044, 11304720, 11399814, 11421142, 11492582,
>>>>>> 11649949, 11524634, 11642228, 11686280, 11599854, 11634717,
>>>>>> 11884843, 11829986, 11677538, 11713733, 11776782, 11976272,
>>>>>> 11770553, 11952997, 12037744, 12132233, 12294168, 12192129,
>>>>>> 12059206, 12249964, 12079285, 12188221, 12370418, 12650935,
>>>>>> 12764803, 12552916, 12608853, 12795617, 12750516, 12876709,
>>>>>> 12952859, 12827097, 12887679, 12691419, 12219526, 11768893,
>>>>>> 11971337, 12144957, 12191313, 12116405, 12224295, 12297602,
>>>>>> 12347845, 12466555, 12569735, 12626521, 12497659, 12654901,
>>>>>> 12729332, 12664595, 13078388, 12984005, 12842562, 12990498,
>>>>>> 12993825, 12972340, 13039000, 13133983, 13479873, 13367799,
>>>>>> 13335395, 13464771, 13363329, 13442441, 13465088, 13511336,
>>>>>> 13442888, 13465232, 13516739, 13526653, 13589859, 13734948,
>>>>>> 13860252, 13684437, 13849354, 13682966, 13668438, 13655828,
>>>>>> 13580493, 13625938, 13664902, 13702841, 13755498, 13686368,
>>>>>> 13734294, 13844170, 13949851, 13820642, 14051575, 13898439,
>>>>>> 14174606, 14358699, 14312287, 14272243, 14209392, 14249460,
>>>>>> 14227780, 14452281, 14308973, 14748479, 14743171, 15010368,
>>>>>> 15069059, 15070431, 15195531, 15255816, 15183239, 15205394,
>>>>>> 14647813, 14945576, 15334757, 15374511, 15592985, 15798994,
>>>>>> 15891823, 15968627, 15951432, 15952705, 16254637, 16221005,
>>>>>> 16501660, 16685866, 16457644, 16452841, 16389104, 16572296,
>>>>>> 16682913, 16647074, 16870723, 17044004, 17229456, 17351957,
>>>>>> 17620979, 17602540, 17854654, 18075746, 17997156, 17852577,
>>>>>> 18009088, 18068445, 18089431, 18415992, 18478794, 18252180,
>>>>>> 18447029, 18476603, 18431675, 18001966, 18923864, 19072002,
>>>>>> 19036564, 19125896, 19252974, 19388570, 18913792, 18775702,
>>>>>> 18549987, 18866029, 19106589, 19318267, 19393034, 19381921,
>>>>>> 19635452), v52368043 = c(17055707, 16701281, 16786260,
>>>> 16756166,
>>>>>> 16426263, 17318876, 17415322, 17417988, 17467072, 16667843,
>>>>>> 16809604, 16838837, 17656245, 17760898, 17801874, 18105941,
>>>>>> 18478672, 17862917, 18431580, 18835012, 18673362, 18981674,
>>>>>> 19288359, 18532630, 19026893, 19180319, 18826048, 18486116,
>>>>>> 18089625, 18443518, 18602047, 16896308, 18860395, 18915620,
>>>>>> 19204737, 19105493, 19057133, 19208788, 19480899, 19674690,
>>>>>> 19944916, 20191003, 19112930, 19976985, 19988078, 19908461,
>>>>>> 19984769, 20052585, 20174329, 20282393, 19971041, 20011153,
>>>>>> 20118897, 20384291, 19394115, 20528811, 20747769, 21226830,
>>>>>> 20889921, 20835911, 21174421, 21127491, 21115706, 21262173,
>>>>>> 21259669, 20762658, 20924455, 21128838, 20983747, 20702448,
>>>>>> 20902061, 22345557, 21606059, 22455072, 22565907, 22270902,
>>>>>> 22234914, 21980827, 21847268, 21946181, 21747434, 22088155,
>>>>>> 22182948, 21347892, 21621099, 21430878, 21369303, 21760587,
>>>>>> 22460331, 22607535, 23038099, 22220849, 22449645, 21821231,
>>>>>> 21527176, 21186532, 19338429, 19597253, 19884704, 19771201,
>>>>>> 20036009, 20304254, 21454669, 21050550, 21284980, 21705066,
>>>>>> 22639281, 22690758, 22232400, 22063785, 21935778, 22036533,
>>>>>> 22056549, 22257846, 21838250, 22219803, 22420845, 22433192,
>>>>>> 22470460, 22707910, 23223292, 22951308, 23074595, 23073830,
>>>>>> 23064664, 22851575, 23617734, 23483236, 23583258, 23634482,
>>>>>> 23437658, 23930976, 23919897, 24166465, 24493107, 24053481,
>>>>>> 23681436, 24261439, 23776686, 23773759, 23555273, 23586467,
>>>>>> 24004817, 23802877, 23706412, 23500283, 23937931, 23774129,
>>>>>> 23950854, 23426495, 23813338, 24075179, 24192801, 24430720,
>>>>>> 24673059, 24276619, 24149635, 25049048, 24676187, 24825428,
>>>>>> 25678929, 25704834, 25639134, 26020843, 26609736, 26171747,
>>>>>> 25832302, 27524063, 25919116, 26045163, 26524824, 27370490,
>>>>>> 27221566, 27327466, 27786373, 27642938, 27539806, 27574631,
>>>>>> 28242316, 29454873, 29076991, 28757458, 28179964, 28507837,
>>>>>> 28785755, 29279475, 29506538, 29536098, 29652113, 29854882,
>>>>>> 29339135, 29688228, 30665282, 30467501, 30692879, 31064467,
>>>>>> 31001099, 31230693, 31505904, 31660230, 31558092, 31645834,
>>>>>> 32469843, 32170840, 32023093, 31736921, 32477853, 32189881,
>>>>>> 31808769, 32214965, 32187342, 32060514, 31894538, 32692963,
>>>>>> 32169342, 32134043, 32349968, 32960842, 32835304, 33065854,
>>>>>> 32796030, 32840886, 33870331), v54027371 = c(5006719,
>>>> 5067172,
>>>>>> 5060865, 5062576, 5083254, 5072377, 5031234, 5052979,
>>>> 5027204,
>>>>>> 5043632, 5035454, 5044842, 5063238, 5093024, 5091640,
>>>> 5102054,
>>>>>> 5133352, 5142226, 5165655, 5203037, 5186307, 5174704,
>>>> 5194163,
>>>>>> 5190310, 5232432, 5210572, 5234726, 5235080, 5254050,
>>>> 5245134,
>>>>>> 5245175, 5249094, 5235952, 5262268, 5263595, 5253813,
>>>> 5274263,
>>>>>> 5265423, 5299891, 5327379, 5316191, 5331555, 5346574,
>>>> 5307140,
>>>>>> 5337742, 5347287, 5348381, 5377210, 5374212, 5383470,
>>>> 5366697,
>>>>>> 5392882, 5388985, 5416367, 5417477, 5433010, 5448853,
>>>> 5441429,
>>>>>> 5458425, 5471520, 5489243, 5495727, 5511121, 5512216,
>>>> 5526438,
>>>>>> 5537303, 5522426, 5518461, 5536277, 5501749, 5564018,
>>>> 5570599,
>>>>>> 5584461, 5593844, 5590604, 5602700, 5608632, 5616521,
>>>> 5608951,
>>>>>> 5623657, 5616005, 5645595, 5647305, 5647451, 5693895,
>>>> 5691488,
>>>>>> 5700220, 5707445, 5725799, 5728509, 5717031, 5723853,
>>>> 5711995,
>>>>>> 5719339, 5681740, 5663311, 5643435, 5611268, 5590716,
>>>> 5570704,
>>>>>> 5557739, 5536962, 5560436, 5529977, 5551949, 5574728,
>>>> 5563493,
>>>>>> 5575590, 5577965, 5587020, 5596086, 5612563, 5609016,
>>>> 5626466,
>>>>>> 5654424, 5633372, 5661574, 5667386, 5669274, 5689694,
>>>> 5681104,
>>>>>> 5701730, 5706973, 5707289, 5692449, 5724435, 5730464,
>>>> 5731650,
>>>>>> 5750310, 5739110, 5745413, 5742657, 5748606, 5744911,
>>>> 5767140,
>>>>>> 5785313, 5808618, 5816135, 5806957, 5825860, 5814678,
>>>> 5800046,
>>>>>> 5822757, 5809772, 5808239, 5818814, 5833898, 5829771,
>>>> 5825801,
>>>>>> 5833676, 5847351, 5886288, 5872335, 5876134, 5878844,
>>>> 5883709,
>>>>>> 5891947, 5885232, 5889681, 5891564, 5921837, 5931270,
>>>> 5952834,
>>>>>> 5959543, 5957751, 5970395, 5962589, 5958983, 5996099,
>>>> 5997162,
>>>>>> 5997791, 6025616, 6046802, 6043966, 6072665, 6063075,
>>>> 6065666,
>>>>>> 6115610, 6091893, 6118988, 6119637, 6126261, 6136886,
>>>> 6169330,
>>>>>> 6172147, 6198848, 6202387, 6197097, 6228521, 6219420,
>>>> 6240999,
>>>>>> 6266000, 6254684, 6277414, 6289899, 6290412, 6302735,
>>>> 6343442,
>>>>>> 6341087, 6359345, 6383177, 6364995, 6373585, 6394318,
>>>> 6396556,
>>>>>> 6406482, 6430135, 6428086, 6458862, 6470913, 6480563,
>>>> 6509009,
>>>>>> 6514678, 6545545, 6547838, 6538390, 6566127, 6566755,
>>>> 6578311,
>>>>>> 6586796, 6596265, 6592137, 6621773), v54027372 = c(698.62,
>>>>>> 692.1, 695.87, 692.13, 690.73, 695.22, 696.41, 695.69,
>>>> 699.31,
>>>>>> 697.67, 701.01, 698.38, 698.51, 697.03, 704.24, 709.05,
>>>> 708.58,
>>>>>> 709.21, 716.46, 712.37, 715.7, 719.6, 720.31, 722.96,
>>>> 716.06,
>>>>>> 724.73, 720.79, 721.51, 722.85, 728.82, 727, 734.71, 736.25,
>>>>>> 736.27, 737.61, 737.25, 744.83, 742.92, 744.9, 746.98,
>>>> 752.13,
>>>>>> 745.86, 750.15, 750.31, 744.13, 750.81, 755.16, 759.22,
>>>> 763.57,
>>>>>> 770.53, 764.34, 770.7, 773.6, 777.54, 775.18, 780.65,
>>>> 789.71,
>>>>>> 784.48, 782.33, 782.15, 783.23, 781.14, 783.74, 782.76,
>>>> 783.53,
>>>>>> 781.87, 787.48, 790.22, 794.1, 796.77, 793.41, 806.6,
>>>> 806.36,
>>>>>> 802.21, 813.11, 814.56, 817.25, 824.93, 824.76, 821.27,
>>>> 821.08,
>>>>>> 823.78, 831.6, 828.09, 828.79, 835.7, 833.64, 835.26,
>>>> 838.09,
>>>>>> 840.33, 838.09, 840.35, 839.84, 846.35, 846.62, 836.65,
>>>> 840.76,
>>>>>> 843.8, 846.48, 847.11, 843.82, 843.64, 844.55, 848.04,
>>>> 853.68,
>>>>>> 852.59, 856.16, 864.11, 864.09, 873.24, 870.03, 872.13,
>>>> 873.52,
>>>>>> 875.68, 883.39, 888.95, 902.33, 886.78, 891.69, 893.37,
>>>> 897.82,
>>>>>> 894.44, 896.07, 890.68, 893.01, 889.93, 887.63, 889.08,
>>>> 888.93,
>>>>>> 901.95, 894.42, 897.03, 898.32, 892.12, 895.8, 904.11,
>>>> 902.56,
>>>>>> 909.48, 912.62, 909.97, 911.16, 911.32, 911.6, 913.03,
>>>> 911.93,
>>>>>> 913.79, 916.13, 913.46, 921.64, 919.21, 916.77, 924.35,
>>>> 920.2,
>>>>>> 924, 930.44, 926.46, 930.9, 933.12, 928.53, 933.05, 938.36,
>>>>>> 938.25, 945.34, 943.02, 944.16, 942.02, 939.85, 940.82,
>>>> 950.06,
>>>>>> 956.56, 962.54, 963.78, 955.99, 960.3, 972.32, 959.15,
>>>> 966.48,
>>>>>> 968.39, 967.04, 969.76, 962.76, 964.22, 969.64, 969.64,
>>>> 974.42,
>>>>>> 978.96, 975.58, 979.4, 970.39, 973.89, 976.8, 987.21, 985.7,
>>>>>> 982.39, 982.7, 987.22, 987.37, 986.88, 984.19, 992.24,
>>>> 1002.93,
>>>>>> 1000.22, 1006.93, 1008.81, 1010.78, 1015.52, 1014.01,
>>>> 1012.58,
>>>>>> 1015.28, 1019.15, 1017.74, 1025.74, 1027.27, 1031.26,
>>>> 1034.06,
>>>>>> 1031.76, 1027.91, 1028.6, 1037.3, 1040.28, 1051.1, 1038.39,
>>>>>> 1049.13), v54027376 = c(32.3, 32.2, 32.2, 32.1, 31.8, 31.6,
>>>>>> 31.5, 31.3, 31.3, 31.4, 31.3, 31.2, 31.2, 31, 31.3, 31.3,
>>>>>> 31.3, 31.3, 31.5, 31.4, 31.3, 31.4, 31.4, 31.4, 31.4, 31.5,
>>>>>> 31.2, 31.3, 31.5, 31.4, 31.4, 31.5, 31.6, 31.6, 31.5, 31.5,
>>>>>> 31.8, 31.7, 31.8, 31.8, 31.6, 31.6, 31.6, 31.5, 31.5, 31.5,
>>>>>> 31.5, 31.5, 31.3, 31.4, 31.4, 31.3, 31.4, 31.4, 31.3, 31.5,
>>>>>> 31.5, 31.4, 31.5, 31.4, 31.4, 31.4, 31.4, 31.3, 31.4, 31.2,
>>>>>> 31.3, 31, 31.1, 31, 31.2, 31.3, 31.2, 30.9, 30.9, 31.1,
>>>> 30.8,
>>>>>> 30.8, 30.7, 30.8, 30.6, 30.7, 30.8, 30.8, 30.7, 30.9, 30.6,
>>>>>> 30.7, 30.8, 31, 30.9, 30.8, 31, 31.1, 30.8, 30.1, 30.3,
>>>> 30.4,
>>>>>> 30.4, 30.3, 30.3, 30.2, 30.3, 30.4, 30.1, 30.2, 30, 30.1,
>>>>>> 30, 30.2, 30.4, 30.6, 30.3, 30.3, 30.5, 30.5, 30.4, 30.3,
>>>>>> 30.3, 30.5, 30.5, 30.5, 30.5, 30.2, 30.4, 30.3, 30.3, 30.4,
>>>>>> 30.6, 30.6, 30.7, 30.6, 30.5, 30.5, 30.5, 30.7, 30.6, 30.9,
>>>>>> 30.6, 30.6, 30.5, 30.6, 30.7, 30.2, 30.4, 30.3, 30.2, 30.2,
>>>>>> 30.5, 30, 30.2, 30.4, 30.3, 30.1, 30.1, 30.2, 30.4, 30.4,
>>>>>> 30.3, 30, 30, 30.1, 30.2, 30.3, 30, 30.3, 30.2, 30.2, 30.4,
>>>>>> 30.2, 30.5, 30.6, 30.5, 30.5, 30.9, 30.2, 30.6, 30.7, 30.4,
>>>>>> 30.8, 30.3, 30.5, 30.4, 30.5, 30.3, 30.6, 30.2, 30.1, 30.4,
>>>>>> 30, 29.8, 30.2, 30.2, 29.9, 29.7, 29.9, 29.9, 29.8, 29.7,
>>>>>> 30.2, 30, 30.3, 30.5, 30.4, 30, 30.3, 30.6, 30.2, 30.4,
>>>> 30.4,
>>>>>> 30.4, 30.2, 29.9, 29.9, 30, 29.5, 29.8, 29.8, 30, 30, 29.7,
>>>>>> 29.4, 29.7), v61172 = c(22005, 20776, 22513, 21920, 22802,
>>>>>> 22362, 23115, 22733, 21398, 22752, 22852, 23176, 22406,
>>>> 20917,
>>>>>> 21902, 21685, 22286, 22379, 22462, 24011, 21938, 23253,
>>>> 22542,
>>>>>> 23390, 22772, 21411, 23059, 21923, 22676, 22603, 22269,
>>>> 23088,
>>>>>> 21739, 22853, 23036, 22761, 24435, 22798, 23316, 22371,
>>>> 23691,
>>>>>> 25848, 26116, 26482, 24657, 25219, 23780, 25027, 23459,
>>>> 20867,
>>>>>> 21815, 21448, 22205, 22734, 21750, 21894, 21129, 22359,
>>>> 21206,
>>>>>> 23018, 21452, 20560, 23331, 22223, 24465, 22765, 23132,
>>>> 22893,
>>>>>> 22497, 24115, 23879, 24696, 23399, 21413, 24268, 23387,
>>>> 25378,
>>>>>> 24807, 23902, 26674, 24842, 27060, 25579, 27140, 25948,
>>>> 23937,
>>>>>> 27291, 26785, 26741, 28014, 26468, 30254, 28078, 31165,
>>>> 27903,
>>>>>> 30406, 28724, 25872, 26674, 28005, 28574, 28116, 27645,
>>>> 28820,
>>>>>> 26915, 29534, 29506, 29755, 25859, 25733, 27848, 27664,
>>>> 29056,
>>>>>> 28913, 27682, 28455, 28286, 28968, 27711, 31211, 26457,
>>>> 25821,
>>>>>> 29790, 28530, 32482, 31544, 32214, 33592, 30870, 34086,
>>>> 31843,
>>>>>> 33502, 32137, 30800, 31770, 30887, 31224, 34883, 31727,
>>>> 34795,
>>>>>> 33329, 37090, 35631, 36927, 36254, 34678, 35695, 34568,
>>>> 36096,
>>>>>> 35285, 36044, 36811, 34141, 35512, 34365, 34675, 34920,
>>>> 33633,
>>>>>> 33938, 35237, 35859, 34650, 34644, 35059, 34299, 35653,
>>>> 35486,
>>>>>> 35769, 36267, 35524, 36736, 36364, 36730, 36261, 37129,
>>>> 37903,
>>>>>> 37136, 38402, 38094, 39260, 39189, 38222, 39043, 38617,
>>>> 38777,
>>>>>> 38777, 38843, 39769, 40365, 40270, 40605, 40847, 40330,
>>>> 39600,
>>>>>> 40629, 40509, 41056, 40898, 40745, 41350, 41735, 41405,
>>>> 41141,
>>>>>> 41662, 40998, 39582, 41728, 42189, 43148, 42427, 42908,
>>>> 44375,
>>>>>> 43402, 44144, 43871, 44696, 44803, 43998, 44419, 45686,
>>>> 45193,
>>>>>> 44981, 46250), v66449943 = c(88.1, 86.1, 86.5, 84.6, 85.7,
>>>>>> 85.8, 88.9, 92.4, 90.8, 90.7, 90.5, 91.6, 91.5, 90.8, 91.2,
>>>>>> 92.4, 93.4, 91.3, 100.4, 100.8, 96.6, 101.2, 101.1, 96.7,
>>>>>> 100.2, 100.1, 98.5, 100.4, 98.7, 97.4, 92.6, 95.2, 93.1,
>>>>>> 96.5, 95.9, 96.1, 97.5, 99.1, 104.1, 105.8, 103, 101.9,
>>>> 95.7,
>>>>>> 94.8, 96.7, 95.1, 89.1, 89.1, 86.4, 87.3, 87.6, 86.3, 88.9,
>>>>>> 93, 91.8, 95.2, 91.6, 85.2, 81.1, 89.4, 88.1, 86.7, 87.7,
>>>>>> 88.4, 89.9, 90.5, 90.2, 91.9, 88.1, 86.3, 91.6, 90.9, 101.2,
>>>>>> 101.8, 100.7, 96.8, 92.3, 92.4, 100.2, 103.2, 103.1, 106.6,
>>>>>> 104.3, 102.1, 112.8, 113.5, 115.6, 117, 109.8, 111.8, 108.9,
>>>>>> 112.9, 112, 117.6, 114.3, 109.5, 111.9, 112.3, 111.8, 113.7,
>>>>>> 115.9, 119.9, 117.1, 113.6, 108.6, 105.7, 104.1, 119.6,
>>>> 115.4,
>>>>>> 114.4, 110.6, 109.1, 113.3, 111.8, 111.8, 112.3, 110.8,
>>>> 110.8,
>>>>>> 113.2, 118.3, 121.9, 123.6, 124.4, 126.2, 121.1, 120.4,
>>>> 124.2,
>>>>>> 129.5, 127.2, 124.9, 126.9, 123.5, 125.4, 127.3, 128.3,
>>>> 126.3,
>>>>>> 121, 121.8, 140.2, 142.1, 142.7, 138.8, 133.9, 135.4, 138.3,
>>>>>> 142, 141.6, 141.3, 141.5, 142.7, 127.8, 123.3, 124.6, 126.1,
>>>>>> 117.6, 119.1, 119.1, 122.9, 127, 130.2, 131.5, 133.5, 130.2,
>>>>>> 132.8, 127.7, 116.6, 113.5, 118.9, 122.2, 125.1, 126.2,
>>>> 126.7,
>>>>>> 128, 128.1, 129.3, 129.7, 129, 121.8, 122.4, 127.6, 128.5,
>>>>>> 130.3, 131.4, 131.7, 132.6, 135.2, 128, 124.9, 127.1, 130.2,
>>>>>> 131.3, 132.8, 133.3, 134.6, 136.8, 135.7, 136, 136, 137.9,
>>>>>> 135.8, 133.7, 129.9, 128.5, 128.5, 129.5, 131, 135.8, 136.3,
>>>>>> 136.6, 136, 137.2, 137.9, 136, 133.2, 133.4, 134.1, 134.2,
>>>>>> 135.5, 135.6, 136.1, 135.1, 136.8, NA), v66449950 = c(94.3,
>>>>>> 100.4, 103.6, 103.3, 106.1, 107.5, 105.3, 103.4, 99, 98.8,
>>>>>> 98.3, 96.9, 97.5, 102.1, 100.2, 93.9, 95.3, 93.4, 94.9, 95,
>>>>>> 89.6, 93, 93.9, 96.3, 95.7, 101.5, 98.7, 100.9, 99.8, 94.2,
>>>>>> 86.3, 87.9, 88, 89.7, 90.3, 89.3, 89.4, 91.8, 93.5, 92.7,
>>>>>> 96, 97, 96.8, 95.5, 95.1, 92.4, 91.5, 94.9, 96.6, 102.1,
>>>>>> 97.1, 97.9, 98, 94.8, 96.6, 95.8, 97.2, 99.4, 97.8, 97.8,
>>>>>> 97, 96.8, 96.7, 97.7, 98.3, 98.7, 100, 99.1, 100.6, 96.8,
>>>>>> 97.4, 97.8, 96.9, 102.8, 104.2, 103.4, 106.5, 104.1, 102.3,
>>>>>> 102.3, 98, 95.6, 92.3, 95, 92.6, 97.3, 96.2, 94.6, 103.8,
>>>>>> 103.7, 104.4, 110, 109, 106.5, 107.2, 107.9, 105, 107,
>>>> 107.5,
>>>>>> 106, 107.8, 102.7, 102, 98.4, 98.7, 96.7, 99.2, 101.7,
>>>> 102.7,
>>>>>> 104.8, 105, 104.7, 107.7, 105.7, 105.3, 106.7, 108.2, 105.3,
>>>>>> 104.1, 107.5, 110, 112.8, 115.2, 118.5, 116.6, 115.9, 116.4,
>>>>>> 119.1, 118.7, 122.6, 121.7, 122.3, 120.5, 121.5, 118.2,
>>>> 117.2,
>>>>>> 115.5, 118.3, 116.3, 116.5, 112.3, 117.1, 120.4, 120.3,
>>>> 117.4,
>>>>>> 120.7, 118.9, 117.8, 121.6, 123.6, 123.8, 125, 124.6, 124.8,
>>>>>> 123.8, 124.1, 124, 128, 136.2, 141.6, 137.5, 138.2, 142,
>>>>>> 140.3, 134.6, 140.3, 140.2, 139.4, 136.9, 137, 134.8, 136.4,
>>>>>> 136.2, 135.3, 132.9, 133.8, 134.8, 131.8, 130.5, 125.7,
>>>> 128.4,
>>>>>> 130.2, 129.9, 124, 127.5, 128.9, 127.1, 123.8, 123.8, 121.1,
>>>>>> 118.8, 122.4, 124.3, 128.3, 130.7, 128.2, 131.7, 133.4,
>>>> 132.6,
>>>>>> 128.4, 121, 121.2, 122.5, 121.7, 123.5, 122.7, 122, 118.6,
>>>>>> 120.6, 124, 124.9, 121.4, 118, 124, 123.3, 120, 122.5,
>>>> 121.1,
>>>>>> 122.3, 129.8, 131.4, 127.8, NA), v806086 = c(24241837,
>>>> 23571783,
>>>>>> 24044065, 24286278, 24628066, 24118928, 23617076, 23636633,
>>>>>> 23428219, 22676196, 23634652, 23129728, 23460762, 24470816,
>>>>>> 24710021, 25047202, 24605685, 24889031, 25374882, 25327868,
>>>>>> 25109773, 25083430, 25307028, 24264047, 25218362, 25026529,
>>>>>> 25123228, 24770185, 24471279, 23997307, 25100446, 22964008,
>>>>>> 24847797, 24677891, 24855666, 24657136, 24257410, 24671333,
>>>>>> 25042084, 25294153, 25594879, 25710661, 25392113, 25762642,
>>>>>> 25495968, 25594992, 25046371, 24990942, 26049772, 25720141,
>>>>>> 24908463, 24706488, 25213097, 24845513, 24526632, 25604879,
>>>>>> 25607606, 25775948, 24928252, 25720454, 25230304, 24969882,
>>>>>> 24588168, 24950843, 24342432, 24727389, 24207745, 24653546,
>>>>>> 24098105, 23526031, 24359382, 25982179, 24579627, 24404161,
>>>>>> 25000895, 24807287, 24362175, 23689711, 24144763, 23438032,
>>>>>> 23439725, 23516262, 23790262, 21814921, 21887345, 22899655,
>>>>>> 22230193, 22324816, 22934833, 23503414, 23954111, 23283045,
>>>>>> 22841589, 22386978, 21842554, 19295160, 17419392, 18364375,
>>>>>> 17905440, 18108957, 16916442, 17026042, 18751034, 18321591,
>>>>>> 19048289, 18784796, 18924599, 19475767, 19573589, 19850904,
>>>>>> 19804795, 20157495, 20328658, 20361279, 20061299, 20395027,
>>>>>> 20298913, 20737329, 19902428, 20480090, 21614065, 20987752,
>>>>>> 20921846, 20374783, 20233473, 20215808, 21174832, 21660830,
>>>>>> 21640023, 22114244, 21850029, 22827161, 23011928, 21728327,
>>>>>> 22257123, 22937471, 22827906, 22875554, 22170195, 22516726,
>>>>>> 22303145, 21675434, 22247002, 21567688, 21860739, 22317023,
>>>>>> 22207928, 22332500, 22343634, 22272300, 22054266, 22139544,
>>>>>> 22476830, 22475093, 22870618, 22550632, 21973040, 22959343,
>>>>>> 23266389, 23188580, 23768769, 23473685, 24108092, 23017700,
>>>>>> 23922409, 23820985, 23428273, 23928861, 23165872, 22611064,
>>>>>> 23647886, 23309658, 23349645, 23502513, 24672977, 24826679,
>>>>>> 24264370, 24314700, 24691695, 24783596, 25965007, 24936109,
>>>>>> 24148638, 24551950, 24096124, 24428453, 24656116, 24898703,
>>>>>> 25091096, 25097671, 24932606, 25523713, 25877824, 25493931,
>>>>>> 25584427, 25434925, 26270738, 25655031, 24154450, 24916780,
>>>>>> 24698894, 23950081, 25547953, 25979949, 25162501, 26286322,
>>>>>> 26227143, 26106405, 26205893, 26432292, 26910272, 26326292,
>>>>>> 26552306, 26781714, 26338911, 26176024, 26466019, 26026198,
>>>>>> 26637112, 26198157, 26778716, 26846047, 26409309)),
>>>> row.names =
>>>>>> c(NA,
>>>>>> -223L), class = c("tbl_df", "tbl", "data.frame"))
>>>>>> 
>>>>>> dput() output for dts:
>>>>>> 
>>>>>> 
>>>>>> structure(list(X1 = c(NA_real_, NA_real_, NA_real_, NA_real_),
>>>>>> X2 = c(NA_real_, NA_real_, NA_real_, NA_real_), X3 =
>>>> c(NA_real_,
>>>>>> NA_real_, NA_real_, NA_real_), X4 = c(NA_real_, NA_real_,
>>>>>> NA_real_, NA_real_), X5 = c(NA_real_, NA_real_, NA_real_,
>>>>>> NA_real_), X6 = c(NA_real_, NA_real_, NA_real_, NA_real_),
>>>>>> X7 = c(NA_real_, NA_real_, NA_real_, NA_real_), X8 = c(1,
>>>>>> 120, NA, NA), X9 = c(1, 120, NA, NA), X10 = c(NA_real_,
>>>> NA_real_,
>>>>>> NA_real_, NA_real_), X11 = c(NA_real_, NA_real_, NA_real_,
>>>>>> NA_real_), X12 = c(NA_real_, NA_real_, NA_real_, NA_real_
>>>>>> ), X13 = c(NA, NA, 223, 223), X14 = c(NA_real_, NA_real_,
>>>>>> NA_real_, NA_real_), X15 = c(NA_real_, NA_real_, NA_real_,
>>>>>> NA_real_), X16 = c(NA_real_, NA_real_, NA_real_, NA_real_
>>>>>> ), X17 = c(NA_real_, NA_real_, NA_real_, NA_real_), X18 =
>>>>>> c(NA_real_,
>>>>>> NA_real_, NA_real_, NA_real_), X19 = c(NA_real_, NA_real_,
>>>>>> NA_real_, NA_real_), X20 = c(NA_real_, NA_real_, NA_real_,
>>>>>> NA_real_), X21 = c(NA_real_, NA_real_, NA_real_, NA_real_
>>>>>> ), X22 = c(NA_real_, NA_real_, NA_real_, NA_real_), X23 =
>>>>>> c(NA_real_,
>>>>>> NA_real_, NA_real_, NA_real_), X24 = c(NA_real_, NA_real_,
>>>>>> NA_real_, NA_real_), X25 = c(NA_real_, NA_real_, NA_real_,
>>>>>> NA_real_), X26 = c(NA_real_, NA_real_, NA_real_, NA_real_
>>>>>> ), X27 = c(NA_real_, NA_real_, NA_real_, NA_real_), X28 =
>>>>>> c(NA_real_,
>>>>>> NA_real_, NA_real_, NA_real_), X29 = c(NA_real_, NA_real_,
>>>>>> NA_real_, NA_real_), X30 = c(1, 84, NA, NA), X31 = c(NA,
>>>>>> NA, 181, 223), X32 = c(NA_real_, NA_real_, NA_real_,
>>>> NA_real_
>>>>>> ), X33 = c(NA_real_, NA_real_, NA_real_, NA_real_), X34 =
>>>>>> c(NA_real_,
>>>>>> NA_real_, NA_real_, NA_real_), X35 = c(NA_real_, NA_real_,
>>>>>> NA_real_, NA_real_), X36 = c(NA_real_, NA_real_, NA_real_,
>>>>>> NA_real_), X37 = c(NA_real_, NA_real_, NA_real_, NA_real_
>>>>>> ), X38 = c(NA_real_, NA_real_, NA_real_, NA_real_), X39 =
>>>> c(NA,
>>>>>> NA, 223, 223), X40 = c(NA, NA, 223, 223), X41 = c(NA_real_,
>>>>>> NA_real_, NA_real_, NA_real_)), row.names = c(NA, -4L),
>>>> class =
>>>>>> "data.frame")
>>>>>> 
>>>>>> dput() output for testcase1:
>>>>>> 
>>>>>> c(NA, NA, NA, 84.6, 85.7, 85.8, 88.9, 92.4, 90.8, 90.7, 90.5,
>>>>>> 91.6, 91.5, 90.8, 91.2, 92.4, 93.4, 91.3, 100.4, 100.8, 96.6,
>>>>>> 101.2, 101.1, 96.7, 100.2, 100.1, 98.5, 100.4, 98.7, 97.4,
>> 92.6,
>>>>>> 95.2, 93.1, 96.5, 95.9, 96.1, 97.5, 99.1, 104.1, 105.8, 103,
>>>>>> 101.9, 95.7, 94.8, 96.7, 95.1, 89.1, 89.1, 86.4, 87.3, 87.6,
>>>>>> 86.3, 88.9, 93, 91.8, 95.2, 91.6, 85.2, 81.1, 89.4, 88.1, 86.7,
>>>>>> 87.7, 88.4, 89.9, 90.5, 90.2, 91.9, 88.1, 86.3, 91.6, 90.9,
>>>> 101.2,
>>>>>> 101.8, 100.7, 96.8, 92.3, 92.4, 100.2, 103.2, 103.1, 106.6,
>>>> 104.3,
>>>>>> 102.1, 112.8, 113.5, 115.6, 117, 109.8, 111.8, 108.9, 112.9,
>>>>>> 112, 117.6, 114.3, 109.5, 111.9, 112.3, 111.8, 113.7, 115.9,
>>>>>> 119.9, 117.1, 113.6, 108.6, 105.7, 104.1, 119.6, 115.4, 114.4,
>>>>>> 110.6, 109.1, 113.3, 111.8, 111.8, 112.3, 110.8, 110.8, 113.2,
>>>>>> 118.3, 121.9, 123.6, 124.4, 126.2, 121.1, 120.4, 124.2, 129.5,
>>>>>> 127.2, 124.9, 126.9, 123.5, 125.4, 127.3, 128.3, 126.3, 121,
>>>>>> 121.8, 140.2, 142.1, 142.7, 138.8, 133.9, 135.4, 138.3, 142,
>>>>>> 141.6, 141.3, 141.5, 142.7, 127.8, 123.3, 124.6, 126.1, 117.6,
>>>>>> 119.1, 119.1, 122.9, 127, 130.2, 131.5, 133.5, 130.2, 132.8,
>>>>>> 127.7, 116.6, 113.5, 118.9, 122.2, 125.1, 126.2, 126.7, 128,
>>>>>> 128.1, 129.3, 129.7, 129, 121.8, 122.4, 127.6, 128.5, 130.3,
>>>>>> 131.4, 131.7, 132.6, 135.2, 128, 124.9, 127.1, 130.2, 131.3,
>>>>>> 132.8, 133.3, 134.6, 136.8, 135.7, 136, 136, 137.9, 135.8,
>> 133.7,
>>>>>> 129.9, 128.5, 128.5, 129.5, 131, 135.8, 136.3, 136.6, 136,
>> 137.2,
>>>>>> 137.9, 136, 133.2, 133.4, 134.1, 134.2, 135.5, 135.6, 136.1,
>>>>>> 135.1, NA, NA)
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible
>>>> code.
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible
>> code.
>>> 
>>> 
>>> Links:
>>> ------
>>> [1] http://is.na
>>> [2] http://tibco.com
> 
> 
> Links:
> ------
> [1] http://tibco.com
> [2] http://is.na


From @dm|n @end|ng |rom vm|um@com  Fri Nov  1 00:01:34 2019
From: @dm|n @end|ng |rom vm|um@com (Vmlum )
Date: Fri, 01 Nov 2019 00:01:34 +0100
Subject: [R] Sample Order
Message-ID: <0.0.1.96F.1D5904787D0323A.0@slot0.vmlum.com>

GoodDay,


We have an urgent requirement as per attached samples & specifications. Kindly quote your price and inquiry at the earliest .Also confirm the Terms & Condition included, Delivery schedule for supply.



Looking forward to your valuable  confirmation.



sincerely yours


-- 
Thanks

Arif Jenkins

Manager



From j@vedbtk111 @end|ng |rom gm@||@com  Fri Nov  1 19:27:35 2019
From: j@vedbtk111 @end|ng |rom gm@||@com (javed khan)
Date: Fri, 1 Nov 2019 19:27:35 +0100
Subject: [R] error in train function
Message-ID: <CAJhui+tg3FuLwL4z5nDT6L4gUxyGTe4X8YFMdchUcrTfspQCHA@mail.gmail.com>

Hi

I receive the following error, where is the problem?

Error in train(Effort ~ ., data = d, method = "lpSVM", trControl =
fitControl,  :
  unused arguments (data = d, method = "lpSVM", trControl = fitControl,
verbose = FALSE, metric = "ROC")

The code is here


fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                              classProbs = TRUE,
                        summaryFunction = twoClassSummary)

myGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
Fit3 <- train(Effort ~ ., data = d,
                 method = "lpSVM",
                 trControl = fitControl,
                 verbose = FALSE, metric = "ROC")

	[[alternative HTML version deleted]]


From t@ub|@ @end|ng |rom |mgprec|@|on@com  Sat Nov  2 00:14:14 2019
From: t@ub|@ @end|ng |rom |mgprec|@|on@com (Thomas Subia)
Date: Fri, 1 Nov 2019 23:14:14 +0000
Subject: [R] Help for pdf conversion
In-Reply-To: <CA+8X3fWZp4WZWCexzWAC69GCFdaAruiQf7_r0xdkAi3=Dqx6Ng@mail.gmail.com>
References: <CH2PR17MB3749B7793E74F35B4DD7B198B8630@CH2PR17MB3749.namprd17.prod.outlook.com>
 <CA+8X3fWZp4WZWCexzWAC69GCFdaAruiQf7_r0xdkAi3=Dqx6Ng@mail.gmail.com>
Message-ID: <CH2PR17MB3749D811F1CD7530D373D6ABB8620@CH2PR17MB3749.namprd17.prod.outlook.com>

Jim,

That works well! 
Thanks again for your help!

Thomas Subia

-----Original Message-----
From: Jim Lemon <drjimlemon at gmail.com> 
Sent: Wednesday, October 30, 2019 11:14 PM
To: Thomas Subia <tsubia at imgprecision.com>
Cc: r-help at r-project.org
Subject: Re: [R] Help for pdf conversion

Hi Thomas,
Perhaps you should be doing something like writeLines(txt[1],...) or just:

sink("10619.txt")
cat(txt[1])
sink()

Jim

On Thu, Oct 31, 2019 at 4:48 PM Thomas Subia <tsubia at imgprecision.com> wrote:
>
> Colleagues,
>
> I'm trying to convert a pdf to a text file with the following code.
>
> # pdf to excel
> library(pdftools) # pdf to excel library # set working directory
> setwd("C:/Users")
> # input pdf
> txt <- pdf_text("C:/Users/10619.pdf")
> cat(txt[1])
> write.table(cat(txt[1]),file="10619.txt",sep= "\t",row.names 
> =TRUE,col.names =FALSE)
>
> When I examine the contents of cat(txt[1]) on the console, everything I need is displayed in the format I need.
>
> However when I execute write.table(cat(txt[1]),file="10619.txt",sep= "\t",row.names =TRUE,col.names =FALSE) and examine the output, my output does not match cat(txt[1]).
> I suspect that sep= "\t",row.names =TRUE,col.names =FALSE) might be the error.
>
> How can one output the contents of cat(txt[1]) and retain its format?
>
> Thomas Subia
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From reichm@@j m@iii@g oii sbcgiob@i@@et  Sun Nov  3 03:25:09 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Sat, 2 Nov 2019 21:25:09 -0500
Subject: [R] transforming  dates
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
Message-ID: <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>

R-Help Forum

 

I have a data set that contains a date field but the dates are in two
formats

 

11/7/2016            dd/mm/yyyy

14-07-16               dd-mm-yy

 

How would I go about correcting this problem. Should I separate the dates,
format them , and then recombine?

 

Sincerely

 

Jeff Reichman

(314) 457-1966

 


	[[alternative HTML version deleted]]


From reichm@@j m@iii@g oii sbcgiob@i@@et  Sun Nov  3 03:34:46 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Sat, 2 Nov 2019 21:34:46 -0500
Subject: [R] Recall: transforming  dates
References: <!&!GAAAAAAAAABWob9e2JwrSqMGgmGX/wghwoAAABgAAAAAAAAAVqG/XticK0qjBoJhl/8IIUSRVAAAAAAAEAAAACxGcfCh78JKmlUTyt4088wUAAAAdHJhbnNmb3JtaW5nICBkYXRlcwA=.ref@sbcglobal.net>
Message-ID: <!&!GAAAAAAAAABWob9e2JwrSqMGgmGX/wghwoAAABgAAAAAAAAAVqG/XticK0qjBoJhl/8IIUSRVAAAAAAAEAAAACxGcfCh78JKmlUTyt4088wUAAAAdHJhbnNmb3JtaW5nICBkYXRlcwA=@sbcglobal.net>

reichmanj at sbcglobal.net would like to recall the message, "transforming
dates".

From reichm@@j m@iii@g oii sbcgiob@i@@et  Sun Nov  3 03:40:50 2019
From: reichm@@j m@iii@g oii sbcgiob@i@@et (reichm@@j m@iii@g oii sbcgiob@i@@et)
Date: Sat, 2 Nov 2019 21:40:50 -0500
Subject: [R] transforming  dates
References: <!&!GAAAAAAAAABWob9e2JwrSqMGgmGX/wghwoAAABgAAAAAAAAAVqG/XticK0qjBoJhl/8IIUSRVAAAAAAAEAAAACxGcfCh78JKmlUTyt4088wUAAAAdHJhbnNmb3JtaW5nICBkYXRlcwA=.ref@sbcglobal.net>
Message-ID: <!&!GAAAAAAAAABWob9e2JwrSqMGgmGX/wghwoAAABgAAAAAAAAAVqG/XticK0qjBoJhl/8IIUSRVAAAAAAAEAAAACxGcfCh78JKmlUTyt4088wUAAAAdHJhbnNmb3JtaW5nICBkYXRlcwA=@sbcglobal.net>

R-Help Forum

 

I have a data set that contains a date field but the dates are in two
formats

 

11/7/2016            dd/mm/yyyy

14-07-16               dd-mm-yy

 

How would I go about correcting this problem. 

 

I'm thinking something like ..

 

for (i in 1:length(myDat$date)){ 

                    if ((myDat$date [i]) = <some sort of format> { 

  <format one way, <else format another way> 

  } 

}

 

Or something like this 

 

 

Sincerely

 

Jeff Reichman

(314) 457-1966

 


	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Nov  3 03:56:25 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sat, 2 Nov 2019 19:56:25 -0700
Subject: [R] transforming dates
In-Reply-To: <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
Message-ID: <CAGxFJbSrHT=YH0ya3uS+vzOBKVrsodR-gGNOcTKHdRTG3=iPHA@mail.gmail.com>

Well, one way to do it is via regex's -- no splitting and recombining
needed.
Note: This will convert a factor into a character vector.

> z <- c("11/7/2016", "14-07-16")
> z <- gsub("-([[:digit:]]{2})-([[:digit:]]{2})", "/\\1/20\\2",z) ## /\ is
/ and \
> z
[1] "11/7/2016"  "14/07/2016"

I leave it to you as an exercise to either convert 7 to 07 or vice-versa if
you want to do this.
Note, if you have spaces sprinkled inconsistently around your separators,
you'll have to work a bit harder with your regex.

Cheers,
Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Nov 2, 2019 at 7:25 PM <reichmanj at sbcglobal.net> wrote:

> R-Help Forum
>
>
>
> I have a data set that contains a date field but the dates are in two
> formats
>
>
>
> 11/7/2016            dd/mm/yyyy
>
> 14-07-16               dd-mm-yy
>
>
>
> How would I go about correcting this problem. Should I separate the dates,
> format them , and then recombine?
>
>
>
> Sincerely
>
>
>
> Jeff Reichman
>
> (314) 457-1966
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Nov  3 08:14:58 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 3 Nov 2019 07:14:58 +0000
Subject: [R] transforming dates
In-Reply-To: <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
Message-ID: <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>

Hello,

I believe the simplest is to use package lubridate. Its functions try 
several formats until either one is right or none fits the data.

x <- c('11/7/2016', '14-07-16')
lubridate::dmy(x)
#[1] "2016-07-11" "2016-07-14"


The order dmy must be the same for all vector elements, if not

y <- c('11/7/2016', '14-07-16', '2016/7/11')
lubridate::dmy(y)
#[1] "2016-07-11" "2016-07-14" NA
#Warning message:
# 1 failed to parse.


Hope this helps,

Rui Barradas

?s 02:25 de 03/11/19, reichmanj at sbcglobal.net escreveu:
> R-Help Forum
> 
>   
> 
> I have a data set that contains a date field but the dates are in two
> formats
> 
>   
> 
> 11/7/2016            dd/mm/yyyy
> 
> 14-07-16               dd-mm-yy
> 
>   
> 
> How would I go about correcting this problem. Should I separate the dates,
> format them , and then recombine?
> 
>   
> 
> Sincerely
> 
>   
> 
> Jeff Reichman
> 
> (314) 457-1966
> 
>   
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Nov  3 17:03:43 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 3 Nov 2019 08:03:43 -0800
Subject: [R] error in train function
In-Reply-To: <CAJhui+tg3FuLwL4z5nDT6L4gUxyGTe4X8YFMdchUcrTfspQCHA@mail.gmail.com>
References: <CAJhui+tg3FuLwL4z5nDT6L4gUxyGTe4X8YFMdchUcrTfspQCHA@mail.gmail.com>
Message-ID: <310641a5-1778-d2be-f7ec-b2c93a429a05@comcast.net>


On 11/1/19 11:27 AM, javed khan wrote:
> Hi
>
> I receive the following error, where is the problem?
>
> Error in train(Effort ~ ., data = d, method = "lpSVM", trControl =
> fitControl,  :
>    unused arguments (data = d, method = "lpSVM", trControl = fitControl,
> verbose = FALSE, metric = "ROC")
>
> The code is here


No library calls, so we will need to guess what packages are being assumed.

>
>
> fitControl <- trainControl(method = "repeatedcv",
>                             number = 10,
>                             repeats = 10,
>                                classProbs = TRUE,
>                          summaryFunction = twoClassSummary)
>
> myGrid <-  expand.grid(interaction.depth = c(1, 5, 9),
>                          n.trees = (1:30)*50,
>                          shrinkage = 0.1,
>                          n.minobsinnode = 20)
> Fit3 <- train(Effort ~ ., data = d,
>                   method = "lpSVM",


This appears to be calling the caret::train.recipe function. I cannot 
find a "lpSVM" method. Is it possible that this is simply a misspelling? 
Or have you failed to create a method with that name?


In the tutorial: 
https://topepo.github.io/caret/using-your-own-model-in-train.html the 
package author creates a list with that name in the section entitled 
"Model Components".

|lpSVM <-list(type = "Classification", library = "kernlab", loop = NULL) |

My further guess: you trying to run code fragments from some source 
without fully executing parts of a multi-step process?


-- 

David.

>                   trControl = fitControl,
>                   verbose = FALSE, metric = "ROC")
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ||nu@@|@chen @end|ng |rom gm@||@com  Sun Nov  3 19:12:49 2019
From: ||nu@@|@chen @end|ng |rom gm@||@com (Linus Chen)
Date: Sun, 3 Nov 2019 19:12:49 +0100
Subject: [R] Another Real Basic Question
In-Reply-To: <CAKZQJMDSQMfJzPFEFxq-FE5f3dgE4Fn_vRXRyOGCbB=Vj478aQ@mail.gmail.com>
References: <ED891BAC0E9446778B265ADED0785132@OWNERPC>
 <CAKZQJMDSQMfJzPFEFxq-FE5f3dgE4Fn_vRXRyOGCbB=Vj478aQ@mail.gmail.com>
Message-ID: <CAPm+3sAfYPNXd=LpGF6bNzwzB4bhB_PKjr=Y-+u6R+2Ft+9ADQ@mail.gmail.com>

A title like "Real basic question" is not quite informative...

On Fri, 18 Oct 2019 at 17:45, John Kane <jrkrideau at gmail.com> wrote:
>
> Can you open in a text editor?
>
> On Wed, 16 Oct 2019 at 23:11, Phillip Heinrich <herd_dog at cox.net> wrote:
> >
> > In the Source window of RStudio (upper left) I save my code (File/Save) but can not reload it.  There is a file labeled (RECode.R) but neither File/Open file or File/Recent Files gets me anywhere.
> >
> > Any ideas what I?m doing wrong.
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> John Kane
> Kingston ON Canada
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Nov  3 20:51:23 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 3 Nov 2019 11:51:23 -0800
Subject: [R] transforming dates
In-Reply-To: <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
 <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
Message-ID: <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>

Rui is right -- lubridate functionality and robustness is better -- but
just for fun, here is a simple function, poorly named reformat(), that
splits up the date formats, cleans them up and standardizes them a bit, and
spits them back out with a sep character of your choice (your original
split and recombine suggestion). Lubridate probably does something similar
but more sophisticated, but maybe it's worthwhile to see how one can do it
using basic functionality. This only requires a few short lines of code.

reformat <- function(z, sep = "-"){
   z <- gsub(" ","",z) ## remove blanks
   ## break up dates into 3 component pieces and convert to matrix
   z <- matrix(unlist(strsplit(z, "-|/")), nrow = 3)
   ## add "0" in front of single digit in dd and mm
   ## add "20" in front  of "yy"
   for(i in 1:2) z[i, ] <- gsub("\\<([[:digit:]])\\>","0\\1",z[i, ])
   z[3, ] <- sub("\\<([[:digit:]]{2})\\>","20\\1",z[3, ])
   ## combine back into single string separated by sep
   paste(z[1, ],z[2, ],z[3, ], sep = sep)
}

## Testit
> z <- c(" 1 / 22 /2015"," 1 -5 -15","11/7/2016", "14-07-16")

> reformat(z)
[1] "01-22-2015" "01-05-2015" "11-07-2016" "14-07-2016"

> reformat(z,"/")
[1] "01/22/2015" "01/05/2015" "11/07/2016" "14/07/2016"

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Nov 3, 2019 at 12:15 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> I believe the simplest is to use package lubridate. Its functions try
> several formats until either one is right or none fits the data.
>
> x <- c('11/7/2016', '14-07-16')
> lubridate::dmy(x)
> #[1] "2016-07-11" "2016-07-14"
>
>
> The order dmy must be the same for all vector elements, if not
>
> y <- c('11/7/2016', '14-07-16', '2016/7/11')
> lubridate::dmy(y)
> #[1] "2016-07-11" "2016-07-14" NA
> #Warning message:
> # 1 failed to parse.
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 02:25 de 03/11/19, reichmanj at sbcglobal.net escreveu:
> > R-Help Forum
> >
> >
> >
> > I have a data set that contains a date field but the dates are in two
> > formats
> >
> >
> >
> > 11/7/2016            dd/mm/yyyy
> >
> > 14-07-16               dd-mm-yy
> >
> >
> >
> > How would I go about correcting this problem. Should I separate the
> dates,
> > format them , and then recombine?
> >
> >
> >
> > Sincerely
> >
> >
> >
> > Jeff Reichman
> >
> > (314) 457-1966
> >
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Nov  3 21:22:22 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 3 Nov 2019 12:22:22 -0800
Subject: [R] transforming dates
In-Reply-To: <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
 <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
 <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>
Message-ID: <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>


On 11/3/19 11:51 AM, Bert Gunter wrote:
> Rui is right -- lubridate functionality and robustness is better -- but
> just for fun, here is a simple function, poorly named reformat(), that
> splits up the date formats, cleans them up and standardizes them a bit, and
> spits them back out with a sep character of your choice (your original
> split and recombine suggestion). Lubridate probably does something similar
> but more sophisticated, but maybe it's worthwhile to see how one can do it
> using basic functionality. This only requires a few short lines of code.

If one wants to investigate existing efforts at automatic date _and_ 
time reformatting, then do not forget Dirk's anytime package:


https://cran.r-project.org/web/packages/anytime/index.html


-- 

David.

>
> reformat <- function(z, sep = "-"){
>     z <- gsub(" ","",z) ## remove blanks
>     ## break up dates into 3 component pieces and convert to matrix
>     z <- matrix(unlist(strsplit(z, "-|/")), nrow = 3)
>     ## add "0" in front of single digit in dd and mm
>     ## add "20" in front  of "yy"
>     for(i in 1:2) z[i, ] <- gsub("\\<([[:digit:]])\\>","0\\1",z[i, ])
>     z[3, ] <- sub("\\<([[:digit:]]{2})\\>","20\\1",z[3, ])
>     ## combine back into single string separated by sep
>     paste(z[1, ],z[2, ],z[3, ], sep = sep)
> }
>
> ## Testit
>> z <- c(" 1 / 22 /2015"," 1 -5 -15","11/7/2016", "14-07-16")
>> reformat(z)
> [1] "01-22-2015" "01-05-2015" "11-07-2016" "14-07-2016"
>
>> reformat(z,"/")
> [1] "01/22/2015" "01/05/2015" "11/07/2016" "14/07/2016"
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Sun, Nov 3, 2019 at 12:15 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
>> Hello,
>>
>> I believe the simplest is to use package lubridate. Its functions try
>> several formats until either one is right or none fits the data.
>>
>> x <- c('11/7/2016', '14-07-16')
>> lubridate::dmy(x)
>> #[1] "2016-07-11" "2016-07-14"
>>
>>
>> The order dmy must be the same for all vector elements, if not
>>
>> y <- c('11/7/2016', '14-07-16', '2016/7/11')
>> lubridate::dmy(y)
>> #[1] "2016-07-11" "2016-07-14" NA
>> #Warning message:
>> # 1 failed to parse.
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 02:25 de 03/11/19, reichmanj at sbcglobal.net escreveu:
>>> R-Help Forum
>>>
>>>
>>>
>>> I have a data set that contains a date field but the dates are in two
>>> formats
>>>
>>>
>>>
>>> 11/7/2016            dd/mm/yyyy
>>>
>>> 14-07-16               dd-mm-yy
>>>
>>>
>>>
>>> How would I go about correcting this problem. Should I separate the
>> dates,
>>> format them , and then recombine?
>>>
>>>
>>>
>>> Sincerely
>>>
>>>
>>>
>>> Jeff Reichman
>>>
>>> (314) 457-1966
>>>
>>>
>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Sun Nov  3 22:49:35 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 3 Nov 2019 13:49:35 -0800
Subject: [R] transforming dates
In-Reply-To: <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
 <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
 <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>
 <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>
Message-ID: <CAGxFJbQ2O=V6-P1DFOZ1krxFYdZFy5isEtutJgaP3G1Hs4adEQ@mail.gmail.com>

Yes, indeed.

Thanks, David.

Cheers,
Bert

On Sun, Nov 3, 2019 at 12:22 PM David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On 11/3/19 11:51 AM, Bert Gunter wrote:
> > Rui is right -- lubridate functionality and robustness is better -- but
> > just for fun, here is a simple function, poorly named reformat(), that
> > splits up the date formats, cleans them up and standardizes them a bit,
> and
> > spits them back out with a sep character of your choice (your original
> > split and recombine suggestion). Lubridate probably does something
> similar
> > but more sophisticated, but maybe it's worthwhile to see how one can do
> it
> > using basic functionality. This only requires a few short lines of code.
>
> If one wants to investigate existing efforts at automatic date _and_
> time reformatting, then do not forget Dirk's anytime package:
>
>
> https://cran.r-project.org/web/packages/anytime/index.html
>
>
> --
>
> David.
>
> >
> > reformat <- function(z, sep = "-"){
> >     z <- gsub(" ","",z) ## remove blanks
> >     ## break up dates into 3 component pieces and convert to matrix
> >     z <- matrix(unlist(strsplit(z, "-|/")), nrow = 3)
> >     ## add "0" in front of single digit in dd and mm
> >     ## add "20" in front  of "yy"
> >     for(i in 1:2) z[i, ] <- gsub("\\<([[:digit:]])\\>","0\\1",z[i, ])
> >     z[3, ] <- sub("\\<([[:digit:]]{2})\\>","20\\1",z[3, ])
> >     ## combine back into single string separated by sep
> >     paste(z[1, ],z[2, ],z[3, ], sep = sep)
> > }
> >
> > ## Testit
> >> z <- c(" 1 / 22 /2015"," 1 -5 -15","11/7/2016", "14-07-16")
> >> reformat(z)
> > [1] "01-22-2015" "01-05-2015" "11-07-2016" "14-07-2016"
> >
> >> reformat(z,"/")
> > [1] "01/22/2015" "01/05/2015" "11/07/2016" "14/07/2016"
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along
> and
> > sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Sun, Nov 3, 2019 at 12:15 AM Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
> >
> >> Hello,
> >>
> >> I believe the simplest is to use package lubridate. Its functions try
> >> several formats until either one is right or none fits the data.
> >>
> >> x <- c('11/7/2016', '14-07-16')
> >> lubridate::dmy(x)
> >> #[1] "2016-07-11" "2016-07-14"
> >>
> >>
> >> The order dmy must be the same for all vector elements, if not
> >>
> >> y <- c('11/7/2016', '14-07-16', '2016/7/11')
> >> lubridate::dmy(y)
> >> #[1] "2016-07-11" "2016-07-14" NA
> >> #Warning message:
> >> # 1 failed to parse.
> >>
> >>
> >> Hope this helps,
> >>
> >> Rui Barradas
> >>
> >> ?s 02:25 de 03/11/19, reichmanj at sbcglobal.net escreveu:
> >>> R-Help Forum
> >>>
> >>>
> >>>
> >>> I have a data set that contains a date field but the dates are in two
> >>> formats
> >>>
> >>>
> >>>
> >>> 11/7/2016            dd/mm/yyyy
> >>>
> >>> 14-07-16               dd-mm-yy
> >>>
> >>>
> >>>
> >>> How would I go about correcting this problem. Should I separate the
> >> dates,
> >>> format them , and then recombine?
> >>>
> >>>
> >>>
> >>> Sincerely
> >>>
> >>>
> >>>
> >>> Jeff Reichman
> >>>
> >>> (314) 457-1966
> >>>
> >>>
> >>>
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Mon Nov  4 00:04:45 2019
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Mon, 4 Nov 2019 00:04:45 +0100
Subject: [R] transforming dates
In-Reply-To: <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
 <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
 <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>
 <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>
Message-ID: <9ABD5422-25D1-4F4A-955F-60B80A25A54A@gmail.com>



> On 3 Nov 2019, at 21:22 , David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> On 11/3/19 11:51 AM, Bert Gunter wrote:
    =======

Hey, that's my birthday! Err, no it isn't... ;-)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Mon Nov  4 00:33:34 2019
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sun, 3 Nov 2019 17:33:34 -0600
Subject: [R] transforming dates
In-Reply-To: <9ABD5422-25D1-4F4A-955F-60B80A25A54A@gmail.com>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
 <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
 <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>
 <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>
 <9ABD5422-25D1-4F4A-955F-60B80A25A54A@gmail.com>
Message-ID: <91149c4a-9822-d344-e143-940507a24f7c@effectivedefense.org>



On 2019-11-03 17:04, Peter Dalgaard wrote:
>
>> On 3 Nov 2019, at 21:22 , David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>
>> On 11/3/19 11:51 AM, Bert Gunter wrote:
> =======
>
> Hey, that's my birthday! Err, no it isn't... ;-)
>

 ????? Is that November 11 of 2019 or March 19 of 2011 or 11 March 2019?


 ????? The English still use stones as a unit of mass, and most of the 
US still steadfastly refuses to seriously consider metrication or? ISO 
8601.? I know an architect in the US, who has worked on several 
different projects every year for the past 40 years only one of which 
has been in metric units.


 ?????? Binary, octal or hex is superior to decimal, except for the fact 
that most humans have 10 digits on hands and feet.? And decimal is 
vastly superior to arithmetic in mixed bases, e.g., adding miles, rods, 
yards, feet, inches, and 64ths.


 ????? Spencer Graves


From @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org  Mon Nov  4 00:37:08 2019
From: @pencer@gr@ve@ @end|ng |rom e||ect|vede|en@e@org (Spencer Graves)
Date: Sun, 3 Nov 2019 17:37:08 -0600
Subject: [R] transforming dates
In-Reply-To: <9ABD5422-25D1-4F4A-955F-60B80A25A54A@gmail.com>
References: <000001d591ed$eae94350$c0bbc9f0$.ref@sbcglobal.net>
 <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
 <f572508c-6a5f-8cd4-e5cf-5df47c6a62f3@sapo.pt>
 <CAGxFJbTv64r0NbiUdPzbPW0CuHKk5QAPjWbX6+JZ=sJ8YnmUOQ@mail.gmail.com>
 <7f3b64ff-e0d8-5b9b-ac1d-c3e2576d5be3@comcast.net>
 <9ABD5422-25D1-4F4A-955F-60B80A25A54A@gmail.com>
Message-ID: <49b1e6af-9e10-3b6a-6dda-28cf7ada837a@effectivedefense.org>



On 2019-11-03 17:04, Peter Dalgaard wrote:
>
>> On 3 Nov 2019, at 21:22 , David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>
>> On 11/3/19 11:51 AM, Bert Gunter wrote:
> =======
>
> Hey, that's my birthday! Err, no it isn't... ;-)
>

 ????? Is that November 3 of 2019 or March 19 of 2011 or 11 March 2019?? 
[please excuse the typo in the earlier response]


 ????? The English still use stones as a unit of mass, and most of the 
US still steadfastly refuses to seriously consider metrication or? ISO 
8601.? I know an architect in the US, who has worked on several 
different projects every year for the past 40 years only one of which 
has been in metric units.


 ?????? Binary, octal or hex is superior to decimal, except for the fact 
that most humans have 10 digits on hands and feet.? And decimal is 
vastly superior to arithmetic in mixed bases, e.g., adding miles, rods, 
yards, feet, inches, and 64ths.


 ????? Spencer Graves


From @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t  Mon Nov  4 12:20:23 2019
From: @te|@no@@o||@ @end|ng |rom reg|one@m@rche@|t (Stefano Sofia)
Date: Mon, 4 Nov 2019 11:20:23 +0000
Subject: [R] transforming  dates
Message-ID: <8B435C9568170B469AE31E8891E8CC4F8092B66E@ESINO.regionemarche.intra>

Hello.
I am not sure if this hint might be useful, but I share it anyway: what about using the as.POSIXct command?

as.POSIXct(mydate, format="%d-%m-%Y) or as.POSIXct(mydate, format="%d/%m/%Y)

Hope this helps
Stefano

         (oo)
--oOO--( )--OOo----------------
Stefano Sofia PhD
Civil Protection - Marche Region
Meteo Section
Snow Section
Via del Colle Ameno 5
60126 Torrette di Ancona, Ancona
Uff: 071 806 7743
E-mail: stefano.sofia at regione.marche.it
---Oo---------oO----------------

Date: Sat, 2 Nov 2019 21:25:09 -0500
From: <reichmanj at sbcglobal.net>
To: <r-help at r-project.org>
Subject: [R] transforming  dates
Message-ID: <000001d591ed$eae94350$c0bbc9f0$@sbcglobal.net>
Content-Type: text/plain; charset="utf-8"

R-Help Forum



I have a data set that contains a date field but the dates are in two
formats



11/7/2016            dd/mm/yyyy

14-07-16               dd-mm-yy



How would I go about correcting this problem. Should I separate the dates,
format them , and then recombine?



Sincerely



Jeff Reichman

(314) 457-1966




        [[alternative HTML version deleted]]




________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

--
Questo messaggio  stato analizzato da Libra ESVA ed  risultato non infetto.
This message was scanned by Libra ESVA and is believed to be clean.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Mon Nov  4 14:31:39 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Mon, 4 Nov 2019 14:31:39 +0100
Subject: [R] Order axis by number of entries in lattice plot
Message-ID: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>

Dear all,
I am plotting some values with lattice barchart: the y-axis is
automatically ordered alphabetically; is it possible to order the
entries by number, so that the 'larger' histograms would be at the top
of the plot?
This is a working example

```
library(lattice)
Family = c("Adenoviridae", "Baculoviridae",  "Herpesviridae",   "Mimiviridae",
"Myoviridae", "Pandoraviridae",  "Phycodnaviridae", "Podoviridae",
"Polydnaviridae",  "Retroviridae", "Siphoviridae",    "Unassigned")
Normal = c(7, 15, 24,  8, 65, 24, 17, 16,  8, 15, 49 , 9)
Tumour =c(  17,  75,  94,  14, 242,  28,  41,  69,  12,  11, 305,  51)
Metastasis =c(41,  66,  95,   3, 173,  22,  33, 101,  12,  12, 552,  57)
df = data.frame(Family, Normal, Tumour, Metastasis, stringsAsFactors = FALSE)
COLS = c("darkolivegreen3", "brown3", "darkorchid3")
barchart(Family ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
         xlim=c(1,1000),
         main = "Alphabetical order",
         xlab = expression(bold("Number of species")),
         ylab = expression(bold("Families")),
         auto.key = list(space = "top", columns=3),
         par.settings = list(superpose.polygon = list(col = COLS)))
```


-- 
Best regards,
Luigi


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Nov  4 15:25:05 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 4 Nov 2019 09:25:05 -0500
Subject: [R] Order axis by number of entries in lattice plot
In-Reply-To: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>
References: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>
Message-ID: <a2bca676-711c-bbc9-2c3f-50204a237909@gmail.com>

On 04/11/2019 8:31 a.m., Luigi Marongiu wrote:
> Dear all,
> I am plotting some values with lattice barchart: the y-axis is
> automatically ordered alphabetically; is it possible to order the
> entries by number, so that the 'larger' histograms would be at the top
> of the plot?
> This is a working example
> 
> ```
> library(lattice)
> Family = c("Adenoviridae", "Baculoviridae",  "Herpesviridae",   "Mimiviridae",
> "Myoviridae", "Pandoraviridae",  "Phycodnaviridae", "Podoviridae",
> "Polydnaviridae",  "Retroviridae", "Siphoviridae",    "Unassigned")
> Normal = c(7, 15, 24,  8, 65, 24, 17, 16,  8, 15, 49 , 9)
> Tumour =c(  17,  75,  94,  14, 242,  28,  41,  69,  12,  11, 305,  51)
> Metastasis =c(41,  66,  95,   3, 173,  22,  33, 101,  12,  12, 552,  57)
> df = data.frame(Family, Normal, Tumour, Metastasis, stringsAsFactors = FALSE)
> COLS = c("darkolivegreen3", "brown3", "darkorchid3")
> barchart(Family ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
>           xlim=c(1,1000),
>           main = "Alphabetical order",
>           xlab = expression(bold("Number of species")),
>           ylab = expression(bold("Families")),
>           auto.key = list(space = "top", columns=3),
>           par.settings = list(superpose.polygon = list(col = COLS)))
> ```
> 
> 

You could do it by using an ordered factor.  For example,

o <- order(Normal + Tumour + Metastasis)
df$Ordered <- ordered(Family, levels = Family[o])
barchart(Ordered ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
          xlim=c(1,1000),
          main = "Ordered by total",
          xlab = expression(bold("Number of species")),
          ylab = expression(bold("Families")),
          auto.key = list(space = "top", columns=3),
          par.settings = list(superpose.polygon = list(col = COLS)))

Duncan Murdoch


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Nov  4 15:32:40 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 4 Nov 2019 09:32:40 -0500
Subject: [R] Order axis by number of entries in lattice plot
In-Reply-To: <a2bca676-711c-bbc9-2c3f-50204a237909@gmail.com>
References: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>
 <a2bca676-711c-bbc9-2c3f-50204a237909@gmail.com>
Message-ID: <11929ba6-2adf-9421-2bb1-d4412d8af696@gmail.com>

On 04/11/2019 9:25 a.m., Duncan Murdoch wrote:
> On 04/11/2019 8:31 a.m., Luigi Marongiu wrote:
>> Dear all,
>> I am plotting some values with lattice barchart: the y-axis is
>> automatically ordered alphabetically; is it possible to order the
>> entries by number, so that the 'larger' histograms would be at the top
>> of the plot?
>> This is a working example
>>
>> ```
>> library(lattice)
>> Family = c("Adenoviridae", "Baculoviridae",  "Herpesviridae",   "Mimiviridae",
>> "Myoviridae", "Pandoraviridae",  "Phycodnaviridae", "Podoviridae",
>> "Polydnaviridae",  "Retroviridae", "Siphoviridae",    "Unassigned")
>> Normal = c(7, 15, 24,  8, 65, 24, 17, 16,  8, 15, 49 , 9)
>> Tumour =c(  17,  75,  94,  14, 242,  28,  41,  69,  12,  11, 305,  51)
>> Metastasis =c(41,  66,  95,   3, 173,  22,  33, 101,  12,  12, 552,  57)
>> df = data.frame(Family, Normal, Tumour, Metastasis, stringsAsFactors = FALSE)
>> COLS = c("darkolivegreen3", "brown3", "darkorchid3")
>> barchart(Family ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
>>            xlim=c(1,1000),
>>            main = "Alphabetical order",
>>            xlab = expression(bold("Number of species")),
>>            ylab = expression(bold("Families")),
>>            auto.key = list(space = "top", columns=3),
>>            par.settings = list(superpose.polygon = list(col = COLS)))
>> ```
>>
>>
> 
> You could do it by using an ordered factor.  For example,
> 
> o <- order(Normal + Tumour + Metastasis)
> df$Ordered <- ordered(Family, levels = Family[o])
> barchart(Ordered ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
>            xlim=c(1,1000),
>            main = "Ordered by total",
>            xlab = expression(bold("Number of species")),
>            ylab = expression(bold("Families")),
>            auto.key = list(space = "top", columns=3),
>            par.settings = list(superpose.polygon = list(col = COLS)))
> 
> Duncan Murdoch
> 

Sorry, I meant to add:  you don't need to use an ordered factor for the 
plot.  A regular factor with the levels in the order you want is fine, 
i.e. you could use

df$Ordered <- factor(Family, levels = Family[o])

However, there are some other operations (e.g. comparison using ">") 
that do require the ordered factor.

Duncan Murdoch


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Sun Nov  3 18:30:28 2019
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Sun, 3 Nov 2019 18:30:28 +0100
Subject: [R] Train and testing in caret package
Message-ID: <CA+nrPnu+pe3QY=BMJsM1Z7wuM887ANDbsaL9jVp9ptUvuWogtQ@mail.gmail.com>

Hello to all. I am new to R language, just read few tutorials.

I have a question, we use trainControl and train function for k fold cross
validation and we don't need to predict the data separately using predict
function for test data.

But how when we have a separate test set? How can we use the trainControl
function, what to write in the method instead of repeated cv, how to
predict data on test set?
Thanks for your understanding

	[[alternative HTML version deleted]]


From m@rco@be@o48 @end|ng |rom gm@||@com  Mon Nov  4 18:32:54 2019
From: m@rco@be@o48 @end|ng |rom gm@||@com (Marco Besozzi)
Date: Mon, 4 Nov 2019 18:32:54 +0100
Subject: [R] clusplot() and fviz_cluster() functions
Message-ID: <CAO=Vm8pKWzW466h1=+z=jR6=hYAYn9QjR+=79KHLBSJj+x3g5A@mail.gmail.com>

Can't understand why with clusplot() and fviz_cluster() functions I obtain
two x-axis mirror graphs. That's the code...


*mydata <- USArrests*













*z <- scale(mydata)## clustering MacQueen (k-means )#fit <- kmeans(z, 4,
algorithm = c("MacQueen"), nstart=50)#library(cluster)windows()clusplot(z,
fit$cluster, labels=3, lines=0, cex=0.6, col.txt="black",
col.p="black")#library(factoextra)windows()fviz_cluster(fit, data=z)#*

Thanks if someone has a suggestion!

---------------------------------------------------------------
Marco Besozzi, MD
---------------------------------------------------------------

	[[alternative HTML version deleted]]


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Mon Nov  4 18:50:42 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Mon, 4 Nov 2019 12:50:42 -0500
Subject: [R] Train and testing in caret package
In-Reply-To: <CA+nrPnu+pe3QY=BMJsM1Z7wuM887ANDbsaL9jVp9ptUvuWogtQ@mail.gmail.com>
References: <CA+nrPnu+pe3QY=BMJsM1Z7wuM887ANDbsaL9jVp9ptUvuWogtQ@mail.gmail.com>
Message-ID: <CAJc=yOH_rBhxQ=6bwgtL8Gbax_09a3Lbu4bmC47zjvC_H1g1Kw@mail.gmail.com>

This isn't an R language question, but there are many tutorials,
blogs, and videos on the web that address this sort of thing, in
addition to StackExchange.

On Mon, Nov 4, 2019 at 11:58 AM Neha gupta <neha.bologna90 at gmail.com> wrote:
>
> Hello to all. I am new to R language, just read few tutorials.
>
> I have a question, we use trainControl and train function for k fold cross
> validation and we don't need to predict the data separately using predict
> function for test data.
>
> But how when we have a separate test set? How can we use the trainControl
> function, what to write in the method instead of repeated cv, how to
> predict data on test set?
> Thanks for your understanding
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rmh @end|ng |rom temp|e@edu  Mon Nov  4 18:56:57 2019
From: rmh @end|ng |rom temp|e@edu (Richard M. Heiberger)
Date: Mon, 4 Nov 2019 12:56:57 -0500
Subject: [R] Order axis by number of entries in lattice plot
In-Reply-To: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>
References: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>
Message-ID: <CAGx1TMCW2WU0dvz7xT0+RSvJ7fT+16WCw+n2+OWp0teptUU7gg@mail.gmail.com>

## The likert function would work well for this example.
## Continuing from your example

## install.packages("HH") ## if necessary
library(HH)

likert(Family ~ Normal+Tumour+Metastasis, data = df,
       main = "likert, data-order, ReferenceZero=0\nDuplicates your example",
       ReferenceZero=0,
       as.table=FALSE,
       xlab = expression(bold("Number of species")),
       ylab = expression(bold("Families")),
       auto.key = list(space = "top", columns=3),
       col = COLS)

likert(Family ~ Normal+Tumour+Metastasis, data = df,
       main = "likert, positive.order=TRUE, ReferenceZero=0\nThis is
what you asked for",
       positive.order=TRUE,
       ReferenceZero=0,
       xlab = expression(bold("Number of species")),
       ylab = expression(bold("Families")),
       auto.key = list(space = "top", columns=3),
       col = COLS)

likert(Family ~ Normal+Tumour+Metastasis, data = df,
       main = "likert, positive.order=TRUE, ReferenceZero=1.5\nThis
puts Normal on left and not-Normal on right",
       positive.order=TRUE,
       ReferenceZero=1.5,
       xlab = expression(bold("Number of species")),
       ylab = expression(bold("Families")),
       auto.key = list(space = "top", columns=3),
       col = COLS)

## For information on the likert function
?likert

## For more examples
demo("likert-paper", package="HH")

## for the paper, open
http://www.jstatsoft.org/v57/i05/
## and click on
##       Download PDF

On Mon, Nov 4, 2019 at 8:32 AM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>
> Dear all,
> I am plotting some values with lattice barchart: the y-axis is
> automatically ordered alphabetically; is it possible to order the
> entries by number, so that the 'larger' histograms would be at the top
> of the plot?
> This is a working example
>
> ```
> library(lattice)
> Family = c("Adenoviridae", "Baculoviridae",  "Herpesviridae",   "Mimiviridae",
> "Myoviridae", "Pandoraviridae",  "Phycodnaviridae", "Podoviridae",
> "Polydnaviridae",  "Retroviridae", "Siphoviridae",    "Unassigned")
> Normal = c(7, 15, 24,  8, 65, 24, 17, 16,  8, 15, 49 , 9)
> Tumour =c(  17,  75,  94,  14, 242,  28,  41,  69,  12,  11, 305,  51)
> Metastasis =c(41,  66,  95,   3, 173,  22,  33, 101,  12,  12, 552,  57)
> df = data.frame(Family, Normal, Tumour, Metastasis, stringsAsFactors = FALSE)
> COLS = c("darkolivegreen3", "brown3", "darkorchid3")
> barchart(Family ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
>          xlim=c(1,1000),
>          main = "Alphabetical order",
>          xlab = expression(bold("Number of species")),
>          ylab = expression(bold("Families")),
>          auto.key = list(space = "top", columns=3),
>          par.settings = list(superpose.polygon = list(col = COLS)))
> ```
>
>
> --
> Best regards,
> Luigi
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @@r@h@go@|ee @end|ng |rom gm@||@com  Mon Nov  4 19:04:33 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Mon, 4 Nov 2019 13:04:33 -0500
Subject: [R] clusplot() and fviz_cluster() functions
In-Reply-To: <CAO=Vm8pKWzW466h1=+z=jR6=hYAYn9QjR+=79KHLBSJj+x3g5A@mail.gmail.com>
References: <CAO=Vm8pKWzW466h1=+z=jR6=hYAYn9QjR+=79KHLBSJj+x3g5A@mail.gmail.com>
Message-ID: <CAM_vjumsDjRYMKAHa6TsqSmSnLvGC4J3-AVzBm8xDW0J0j6d0Q@mail.gmail.com>

The orientation of a principal components axis is entirely arbitrary,
and can vary with implementation. It doesn't matter at all.

Your code was hugely mangled by posting in HTML - see below.

Sarah

On Mon, Nov 4, 2019 at 12:33 PM Marco Besozzi <marco.beso48 at gmail.com> wrote:
>
> Can't understand why with clusplot() and fviz_cluster() functions I obtain
> two x-axis mirror graphs. That's the code...
>
>
> *mydata <- USArrests*
>
>
>
>
>
>
>
>
>
>
>
>
>
> *z <- scale(mydata)## clustering MacQueen (k-means )#fit <- kmeans(z, 4,
> algorithm = c("MacQueen"), nstart=50)#library(cluster)windows()clusplot(z,
> fit$cluster, labels=3, lines=0, cex=0.6, col.txt="black",
> col.p="black")#library(factoextra)windows()fviz_cluster(fit, data=z)#*
>
> Thanks if someone has a suggestion!
>
> ---------------------------------------------------------------
> Marco Besozzi, MD
> ---------------------------------------------------------------
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From |mr@nd@mk@r222 @end|ng |rom gm@||@com  Mon Nov  4 18:30:00 2019
From: |mr@nd@mk@r222 @end|ng |rom gm@||@com (imran damkar)
Date: Mon, 4 Nov 2019 23:00:00 +0530
Subject: [R] (no subject)
Message-ID: <CA+ysrC6H51NWdxrUNY1ntNAW_fuwadMOmiLsNDLeEEdYw_5JcA@mail.gmail.com>

Hi,
I would like to know what is the difference between function oneway_test
and independence_test of coin package.
You will highly appreciated
Regards

	[[alternative HTML version deleted]]


From kev|n@thorpe @end|ng |rom utoronto@c@  Mon Nov  4 20:07:26 2019
From: kev|n@thorpe @end|ng |rom utoronto@c@ (Kevin Thorpe)
Date: Mon, 4 Nov 2019 19:07:26 +0000
Subject: [R] (no subject)
In-Reply-To: <CA+ysrC6H51NWdxrUNY1ntNAW_fuwadMOmiLsNDLeEEdYw_5JcA@mail.gmail.com>
References: <CA+ysrC6H51NWdxrUNY1ntNAW_fuwadMOmiLsNDLeEEdYw_5JcA@mail.gmail.com>
Message-ID: <63E83839-2F86-4E8F-AAA0-959CE48F7D01@utoronto.ca>

Have you read the help pages of the two functions? That is where I would start.

-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
 

?On 2019-11-04, 1:47 PM, "R-help on behalf of imran damkar" <r-help-bounces at r-project.org on behalf of imrandamkar222 at gmail.com> wrote:

    Hi,
    I would like to know what is the difference between function oneway_test
    and independence_test of coin package.
    You will highly appreciated
    Regards
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    


From dw|n@em|u@ @end|ng |rom comc@@t@net  Mon Nov  4 20:22:29 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Mon, 4 Nov 2019 11:22:29 -0800
Subject: [R] 
 coin::oneway_test and independence_test ...was Re: (no subject)
In-Reply-To: <CA+ysrC6H51NWdxrUNY1ntNAW_fuwadMOmiLsNDLeEEdYw_5JcA@mail.gmail.com>
References: <CA+ysrC6H51NWdxrUNY1ntNAW_fuwadMOmiLsNDLeEEdYw_5JcA@mail.gmail.com>
Message-ID: <f98b44a3-8a5b-b017-b36e-64519e1038b5@comcast.net>

You _should_ read the Posting Guide. Informative subjects are 
emphatically requested, and rhelp is a plain text mailing list

On 11/4/19 9:30 AM, imran damkar wrote:
> Hi,
> I would like to know what is the difference between function oneway_test
> and independence_test of coin package.


It's not so much that they are "different", but rather that one is 
(much) more general than the other. If you look at the code, which one 
should always do in such questions, you immediately see that 
`oneway_test` calls `independence_test` after some setup of parameters.? 
You should also read ?oneway_test and ?independence_test where this is 
also clearly explained. The `oneway_test` is actually a method to do 
some of the tests that you would encounter in a traditional introductory 
statistics course in the section on "nonparametric tests". So 
`oneway_test` is just a restricted type of the more general 
"independence test" suite of methods.

methods(oneway_test)
[1] oneway_test.formula* oneway_test.IndependenceProblem*

# since the default method is not exposed, use `getAnywhere`


getAnywhere(oneway_test.IndependenceProblem)

# omitting parameter setup code

object <- do.call("independence_test", c(list(object = object),
 ??????? args))

if (is_ordered_x(object at statistic))
 ??????? object at method <- "Linear-by-Linear Association Test"
 ??? else if (twosamp) {
 ??????? object at method <- "Two-Sample Fisher-Pitman Permutation Test"
 ??????? object at nullvalue <- 0
 ??? }
 ?? else object at method <- "K-Sample Fisher-Pitman Permutation Test"

object

#------------------

-- 

David.

> You will highly appreciated
> Regards
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From v@|kremk @end|ng |rom gm@||@com  Tue Nov  5 04:33:10 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Mon, 4 Nov 2019 21:33:10 -0600
Subject: [R] File conca.
Message-ID: <CAJOiR6abaaKwdhYrahij3TgRKgEbm=ttrONMZzSyD-yVitDNcg@mail.gmail.com>

Hi All,

I have data files in several folders and want combine all  these files
in one file.  In each folder  there are several files  and these
files have the same structure but different names.   First, in each
folder  I want to concatenate(rbind) all files in to one file. While I
am  reading each files and concatenating (rbind) all files, I want to
added  the folder name as one variable  in each row. I am reading the
folder names  from a file and for demonstration I am using only two
folders  as shown below.
Data\week1             # folder name 1
           WT13.csv
           WT26.csv           ...
           WT10.csv
Data\week2            #folder name 2
           WT02.csv
           WT12.csv

Below please find  my attempt,

folders=c("week1","week2")
for(i in folders){
  path=paste("\data\"", i , sep = "")
  setwd(path)
  Flist = list.files(path,pattern = "^WT")
  dataA =  lapply(Flist, function(x)read.csv(x, header=T))
  Alldata = do.call("rbind", dataA)     # combine all files
  Alldata$foldername=i                  # adding the folder name
}
The above works for  for one folder but how can I do it for more than
one folders?

Thank you in advance,


From petr@p|k@| @end|ng |rom prechez@@cz  Tue Nov  5 09:13:19 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Tue, 5 Nov 2019 08:13:19 +0000
Subject: [R] File conca.
In-Reply-To: <CAJOiR6abaaKwdhYrahij3TgRKgEbm=ttrONMZzSyD-yVitDNcg@mail.gmail.com>
References: <CAJOiR6abaaKwdhYrahij3TgRKgEbm=ttrONMZzSyD-yVitDNcg@mail.gmail.com>
Message-ID: <144d81c7129241f1940641fcf8225f73@SRVEXCHCM1301.precheza.cz>

Hi

Help with such operations is rather tricky as only you know exact structrure
of your folders.

see some hints in line

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> Sent: Tuesday, November 5, 2019 4:33 AM
> To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> Subject: [R] File conca.
> 
> Hi All,
> 
> I have data files in several folders and want combine all  these files in
one
> file.  In each folder  there are several files  and these
> files have the same structure but different names.   First, in each
> folder  I want to concatenate(rbind) all files in to one file. While I am
> reading each files and concatenating (rbind) all files, I want to added
the
> folder name as one variable  in each row. I am reading the folder names
> from a file and for demonstration I am using only two folders  as shown
> below.
> Data\week1             # folder name 1
>            WT13.csv
>            WT26.csv           ...
>            WT10.csv
> Data\week2            #folder name 2
>            WT02.csv
>            WT12.csv
> 
> Below please find  my attempt,
> 
> folders=c("week1","week2")
> for(i in folders){
>   path=paste("\data\"", i , sep = "")
>   setwd(path)

you should use 
wd <- setwd(path)

which keeps the original directory for subsequent use

>   Flist = list.files(path,pattern = "^WT")
>   dataA =  lapply(Flist, function(x)read.csv(x, header=T))
>   Alldata = do.call("rbind", dataA)     # combine all files
>   Alldata$foldername=i                  # adding the folder name
> 

now you can do

setwd(wd)

to return to original directory
}

> The above works for  for one folder but how can I do it for more than one
> folders?

You also need to decide if you want all data from all folders in one object
called Alldata or if you want several Alldata objects, one for each folder.

In second case you could use list structure for Alldata. In the first case
you could store data from each folder in some temporary object and use rbind
directly.

something like

temp <- do.call("rbind", dataA)
temp$foldername <- i

Alldata <- temp
in the first cycle
and
Alldata <- rbind(Alldata, temp)
in second and all others.

Or you could initiate first Alldata manually and use only
Alldata <- rbind(Alldata, temp)

in your loop.

Cheers
Petr

> 
> Thank you in advance,
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Nov  5 09:29:45 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 05 Nov 2019 00:29:45 -0800
Subject: [R] File conca.
In-Reply-To: <144d81c7129241f1940641fcf8225f73@SRVEXCHCM1301.precheza.cz>
References: <CAJOiR6abaaKwdhYrahij3TgRKgEbm=ttrONMZzSyD-yVitDNcg@mail.gmail.com>
 <144d81c7129241f1940641fcf8225f73@SRVEXCHCM1301.precheza.cz>
Message-ID: <9501E9F3-6FAA-44D2-8E1B-4E6389F82D34@dcn.davis.ca.us>

I recommend not using setwd unless you have to (e.g at the beginning of a script run by cron or another task scheduler). It is much simpler to build paths to directories and files using file.path.

On November 5, 2019 12:13:19 AM PST, PIKAL Petr <petr.pikal at precheza.cz> wrote:
>Hi
>
>Help with such operations is rather tricky as only you know exact
>structrure
>of your folders.
>
>see some hints in line
>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
>> Sent: Tuesday, November 5, 2019 4:33 AM
>> To: r-help at R-project.org (r-help at r-project.org)
><r-help at r-project.org>
>> Subject: [R] File conca.
>> 
>> Hi All,
>> 
>> I have data files in several folders and want combine all  these
>files in
>one
>> file.  In each folder  there are several files  and these
>> files have the same structure but different names.   First, in each
>> folder  I want to concatenate(rbind) all files in to one file. While
>I am
>> reading each files and concatenating (rbind) all files, I want to
>added
>the
>> folder name as one variable  in each row. I am reading the folder
>names
>> from a file and for demonstration I am using only two folders  as
>shown
>> below.
>> Data\week1             # folder name 1
>>            WT13.csv
>>            WT26.csv           ...
>>            WT10.csv
>> Data\week2            #folder name 2
>>            WT02.csv
>>            WT12.csv
>> 
>> Below please find  my attempt,
>> 
>> folders=c("week1","week2")
>> for(i in folders){
>>   path=paste("\data\"", i , sep = "")
>>   setwd(path)
>
>you should use 
>wd <- setwd(path)
>
>which keeps the original directory for subsequent use
>
>>   Flist = list.files(path,pattern = "^WT")
>>   dataA =  lapply(Flist, function(x)read.csv(x, header=T))
>>   Alldata = do.call("rbind", dataA)     # combine all files
>>   Alldata$foldername=i                  # adding the folder name
>> 
>
>now you can do
>
>setwd(wd)
>
>to return to original directory
>}
>
>> The above works for  for one folder but how can I do it for more than
>one
>> folders?
>
>You also need to decide if you want all data from all folders in one
>object
>called Alldata or if you want several Alldata objects, one for each
>folder.
>
>In second case you could use list structure for Alldata. In the first
>case
>you could store data from each folder in some temporary object and use
>rbind
>directly.
>
>something like
>
>temp <- do.call("rbind", dataA)
>temp$foldername <- i
>
>Alldata <- temp
>in the first cycle
>and
>Alldata <- rbind(Alldata, temp)
>in second and all others.
>
>Or you could initiate first Alldata manually and use only
>Alldata <- rbind(Alldata, temp)
>
>in your loop.
>
>Cheers
>Petr
>
>> 
>> Thank you in advance,
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From m@rong|u@|u|g| @end|ng |rom gm@||@com  Tue Nov  5 11:10:19 2019
From: m@rong|u@|u|g| @end|ng |rom gm@||@com (Luigi Marongiu)
Date: Tue, 5 Nov 2019 11:10:19 +0100
Subject: [R] Order axis by number of entries in lattice plot
In-Reply-To: <CAGx1TMCW2WU0dvz7xT0+RSvJ7fT+16WCw+n2+OWp0teptUU7gg@mail.gmail.com>
References: <CAMk+s2ShRbwQChaR7ch1DB7kfMQcPrpABdF-QV16DvjDAgAaRg@mail.gmail.com>
 <CAGx1TMCW2WU0dvz7xT0+RSvJ7fT+16WCw+n2+OWp0teptUU7gg@mail.gmail.com>
Message-ID: <CAMk+s2SzRAn=wFWwtnHULzd0vAGWDzBcCeo+QvDptkX1e0uQSw@mail.gmail.com>

Thank you. the factor approach worked all right. Thank you for Likert
also: I already used it but here I wanted to run with lattice only.
Best regards

On Mon, Nov 4, 2019 at 6:57 PM Richard M. Heiberger <rmh at temple.edu> wrote:
>
> ## The likert function would work well for this example.
> ## Continuing from your example
>
> ## install.packages("HH") ## if necessary
> library(HH)
>
> likert(Family ~ Normal+Tumour+Metastasis, data = df,
>        main = "likert, data-order, ReferenceZero=0\nDuplicates your example",
>        ReferenceZero=0,
>        as.table=FALSE,
>        xlab = expression(bold("Number of species")),
>        ylab = expression(bold("Families")),
>        auto.key = list(space = "top", columns=3),
>        col = COLS)
>
> likert(Family ~ Normal+Tumour+Metastasis, data = df,
>        main = "likert, positive.order=TRUE, ReferenceZero=0\nThis is
> what you asked for",
>        positive.order=TRUE,
>        ReferenceZero=0,
>        xlab = expression(bold("Number of species")),
>        ylab = expression(bold("Families")),
>        auto.key = list(space = "top", columns=3),
>        col = COLS)
>
> likert(Family ~ Normal+Tumour+Metastasis, data = df,
>        main = "likert, positive.order=TRUE, ReferenceZero=1.5\nThis
> puts Normal on left and not-Normal on right",
>        positive.order=TRUE,
>        ReferenceZero=1.5,
>        xlab = expression(bold("Number of species")),
>        ylab = expression(bold("Families")),
>        auto.key = list(space = "top", columns=3),
>        col = COLS)
>
> ## For information on the likert function
> ?likert
>
> ## For more examples
> demo("likert-paper", package="HH")
>
> ## for the paper, open
> http://www.jstatsoft.org/v57/i05/
> ## and click on
> ##       Download PDF
>
> On Mon, Nov 4, 2019 at 8:32 AM Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> >
> > Dear all,
> > I am plotting some values with lattice barchart: the y-axis is
> > automatically ordered alphabetically; is it possible to order the
> > entries by number, so that the 'larger' histograms would be at the top
> > of the plot?
> > This is a working example
> >
> > ```
> > library(lattice)
> > Family = c("Adenoviridae", "Baculoviridae",  "Herpesviridae",   "Mimiviridae",
> > "Myoviridae", "Pandoraviridae",  "Phycodnaviridae", "Podoviridae",
> > "Polydnaviridae",  "Retroviridae", "Siphoviridae",    "Unassigned")
> > Normal = c(7, 15, 24,  8, 65, 24, 17, 16,  8, 15, 49 , 9)
> > Tumour =c(  17,  75,  94,  14, 242,  28,  41,  69,  12,  11, 305,  51)
> > Metastasis =c(41,  66,  95,   3, 173,  22,  33, 101,  12,  12, 552,  57)
> > df = data.frame(Family, Normal, Tumour, Metastasis, stringsAsFactors = FALSE)
> > COLS = c("darkolivegreen3", "brown3", "darkorchid3")
> > barchart(Family ~ Normal+Tumour+Metastasis, data = df, stack = TRUE,
> >          xlim=c(1,1000),
> >          main = "Alphabetical order",
> >          xlab = expression(bold("Number of species")),
> >          ylab = expression(bold("Families")),
> >          auto.key = list(space = "top", columns=3),
> >          par.settings = list(superpose.polygon = list(col = COLS)))
> > ```
> >
> >
> > --
> > Best regards,
> > Luigi
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.



-- 
Best regards,
Luigi


From ch@ndeep@v|rd| @end|ng |rom gm@||@com  Tue Nov  5 04:33:54 2019
From: ch@ndeep@v|rd| @end|ng |rom gm@||@com (Chandeep Kaur)
Date: Tue, 5 Nov 2019 09:03:54 +0530
Subject: [R] Help needed for one question (Urgent)
Message-ID: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>

Dear Team,

Could you please help me with the below question? How can I get the desired
output?

Produce the following sequence using only rep(), seq() and potentially
other functions/operators. You must not use c() nor explicit loops

?xa? ?xb? ?xc? ?ya? ?yb? ?zc?

Thanks & Regards,

Chandeep Kaur

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Tue Nov  5 11:52:34 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Tue, 5 Nov 2019 21:52:34 +1100
Subject: [R] Help needed for one question (Urgent)
In-Reply-To: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
References: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
Message-ID: <CA+8X3fWyhLiUc-fS2ro257h3pvd_kTk+isDSR3-hSVxs3VB7oQ@mail.gmail.com>

Homework Chandeep, homework.

Jim

On Tue, Nov 5, 2019 at 9:40 PM Chandeep Kaur <chandeep.virdi at gmail.com> wrote:
>
> Dear Team,
>
> Could you please help me with the below question? How can I get the desired
> output?
>
> Produce the following sequence using only rep(), seq() and potentially
> other functions/operators. You must not use c() nor explicit loops
>
> ?xa? ?xb? ?xc? ?ya? ?yb? ?zc?
>
> Thanks & Regards,
>
> Chandeep Kaur
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From |mr@nd@mk@r222 @end|ng |rom gm@||@com  Tue Nov  5 13:23:20 2019
From: |mr@nd@mk@r222 @end|ng |rom gm@||@com (imran damkar)
Date: Tue, 5 Nov 2019 17:53:20 +0530
Subject: [R] (no subject)
Message-ID: <CA+ysrC5xeGY3osE-jv=FtbZUrJkrEcvxMMYLUtBwnXPsgYJWZQ@mail.gmail.com>

Greetings,
I have collected the data based on 5 point likert scale(very low, low,
neutral, high,very high) on the factor considered by individual before
making investment decision. There are five factors (D.Vs) Influencing
investment decisions. I am interested in knowing whether there is
significant difference in preference level of male and female(IVs) for the
above said factors.Number of males are 252 and females 98. I have applied
Permutational Manova in adonis function of R, since data are ordinal in
nature, and result is non significant. I have read in literature that
permutation test is non parametric in nature. Does for the permutation test
equality of variance is highly important. Also the computation of mean, S.
D and variance are discouraged by many scholars for ordinal data.
Pls through some light of permutation test and it?s assumption and
violation of it. And if in case PERMANAVO is not suitable for my data then
please suggest appropriate alternative.
Shall be highly obliged.
Thanks

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Nov  5 15:36:41 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 05 Nov 2019 06:36:41 -0800
Subject: [R] Help needed for one question (Urgent)
In-Reply-To: <CA+8X3fWyhLiUc-fS2ro257h3pvd_kTk+isDSR3-hSVxs3VB7oQ@mail.gmail.com>
References: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
 <CA+8X3fWyhLiUc-fS2ro257h3pvd_kTk+isDSR3-hSVxs3VB7oQ@mail.gmail.com>
Message-ID: <94BCD50C-3211-44EB-BE72-165419FDB4FF@dcn.davis.ca.us>

In other words... read the Posting Guide.

On November 5, 2019 2:52:34 AM PST, Jim Lemon <drjimlemon at gmail.com> wrote:
>Homework Chandeep, homework.
>
>Jim
>
>On Tue, Nov 5, 2019 at 9:40 PM Chandeep Kaur <chandeep.virdi at gmail.com>
>wrote:
>>
>> Dear Team,
>>
>> Could you please help me with the below question? How can I get the
>desired
>> output?
>>
>> Produce the following sequence using only rep(), seq() and
>potentially
>> other functions/operators. You must not use c() nor explicit loops
>>
>> ?xa? ?xb? ?xc? ?ya? ?yb? ?zc?
>>
>> Thanks & Regards,
>>
>> Chandeep Kaur
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From r@oknz @end|ng |rom gm@||@com  Tue Nov  5 16:27:28 2019
From: r@oknz @end|ng |rom gm@||@com (Richard O'Keefe)
Date: Wed, 6 Nov 2019 04:27:28 +1300
Subject: [R] Help needed for one question (Urgent)
In-Reply-To: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
References: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
Message-ID: <CABcYAdJZAXmSXrX8NNi2RPzY_ohtUA6zo-F8YgUM-ROgFyqTBg@mail.gmail.com>

This looks vaguely like something from exercism.
Let's approach it logically.
 xa xb xc ya yb zc
We see two patterns here:
A:  x x x y y z
B: a b c a b c
If only we had these two character vectors, we could use
 paste(A, B, sep = "")
to get the desired result.  So now we have reduced the
problem to two simpler subproblems.  We have been given
a clue that rep() might be useful.
A: rep(c("x", "y", "z"), c(1, 2, 3))
B: rep(c("a", "b", "c"), 3)
But you were told not to use c().  So now we have three
simpler subsubproblems:
C: "x" "y" "z"
D: 3 2 1
E: "a" "b" "c"
You were given another hint.  seq().  That builds a vector of numbers.
Reading ?seq will give you
D: seq(from = 3, to = 1, by = -1)
or using ":" syntax,
D: 3:1

What about C and E?  This needs two more pieces of knowledge:
- the variable letters,whose value is c("a","b",...,"y","z")
- how vector indexing works in R.
E: letters[1:3]
C: letters[24:26]
So now we can put all the pieces together:
paste(rep(letters[24:26], 3:1), rep(letters[1:3], 2), sep = "")

You were given
 - seq
 - rep
as hints.  You were expected to look up string handling in R
and find things like paste(), substr(), and nchar().

What about the variable 'letters'?
Well, you were expected to know or find out about substr.
You were certainly expected to know about "vectorising".
So you would naturally try substr("abc", 1:3, 1:3).
And that would not work.
So you would be expected to read the documentation:
?substr
And then you would find that substr() *doesn't* do what
you expect, but substring() *does*.  So
C: substring("xyz", 1:3, 1:3)
E: substring("abc", 1:3, 1:3)

This is not really an exercise in R programming.
In real R programming you *don't* avoid arbitrary aspects of the
language and library, but use whatever is appropriate.
So what *is* this exercise about?

(1) It is an exercise in working backwards.  (See the classic book
"How to Solve It" by Polya.)  You know what you must construct,
you have been given some directions about what to use.  It's
about saying "well, I could *finish* this task by doing this action,
so what would I have to set up for that?"  In this case, the key
step for me was seeing xa xb xc ya yb yc as (x,x,x,y,y,z)++(a,b,c,a,b,c).
The mention of rep had me *looking* for repetitions like that.

(2) It is an exercise in using the R documentation to figure out how to
use rep and seq and what is available for splitting and pasting strings.

There is of course no unique answer to this.
substring("xaxbxcyaybzc", seq(from=1,to=11,by=2), seq(from=2,to=12,by=2))
is another solution.  You didn't say you *had* to use rep.

It's not the answer that matters for an exercise like this.
It's how you get there.





On Tue, 5 Nov 2019 at 23:40, Chandeep Kaur <chandeep.virdi at gmail.com> wrote:
>
> Dear Team,
>
> Could you please help me with the below question? How can I get the desired
> output?
>
> Produce the following sequence using only rep(), seq() and potentially
> other functions/operators. You must not use c() nor explicit loops
>
> ?xa? ?xb? ?xc? ?ya? ?yb? ?zc?
>
> Thanks & Regards,
>
> Chandeep Kaur
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cry@n @end|ng |rom b|ngh@mton@edu  Tue Nov  5 16:39:27 2019
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W Ryan)
Date: Tue, 5 Nov 2019 10:39:27 -0500
Subject: [R] getting summary statistics easily with dplyr
Message-ID: <CAM+rpYk6Bk9Wy=82ymw8uexOx5-bUzD9wwW7PECxjvVG9KzdAA@mail.gmail.com>

I'm trying to modernize my way of thinking, and my coding, into the
dplyr/tidyverse way of doing things.

To get basic summary statistics on a variable in a dataframe, with the
output also being a dataframe. I previously would do something like this,
using other packages:

library(doBy)
doBy.output <- summaryBy(mpg ~ am, data = mtcars, FUN = fivenum)
str(doBy.output)   ## yes, it's a dataframe
## which I would then incorporate into my report via Sweave and latex
latex(doBy.output, file = "")

## Or this:

library(mosaic)
mosaic.output <- favstats(mpg ~ am, data = mtcars)
str(mosaic.output)  ## yes, it's a dataframe
latex(mosaic.output, file = "")


## What would be the "dplyr way" of doing this?  I know I could specify
each summary statistic individually:

library(dplyr)
dplyr.output <- mtcars %>% group_by(am) %>% summarise(min = min(mpg),
     p25 = quantile(mpg, prob = 0.25),
     p50 = median(mpg),
     p75 = quantile(mpg, prob = 0.75),
     max = max(mpg) )
str(dplyr.output)  ## yes, it's a dataframe
latex(dplyr.output, file = "")

## Is there a way to use a single function like fivenum instead of
specifying each desired summary statistic?  dplyr summarise() wants a
result of length 1, not 5

dplyr.output.2 <- mtcars %>% group_by(am) %>% summarise(fivenum(mpg) )

group_map or group_modify seem like they might do the job, but I could
use some guidance on the syntax.


Thanks.

--Chris Ryan

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov  5 17:16:05 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 5 Nov 2019 10:16:05 -0600
Subject: [R] How to merge 3 data frames by rownames?
Message-ID: <CAF9-5jNOiTKiRd584_-3nm5L3gN8d4-dDSPPhF87P50ieo9DEw@mail.gmail.com>

Hi,

I have 3 data frames like this:

> head(s11)
                      B_NoD
Ebfrl.7uOZfnjp_E7k 7.583709
ueQUrXd5FH554RlhZc 5.177791
0Uu3XrB6Bd14qoNeuc 4.680306
0t7nhVLii6tSAxtLhc 4.565023
fSUyR.vR7Xu0iR4nUU 2.885992
0Tm7hdRJxd9zoevPlA 2.866847
> head(s22)
                     B_DwoC
Ebfrl.7uOZfnjp_E7k 7.583709
ueQUrXd5FH554RlhZc 5.177791
0Uu3XrB6Bd14qoNeuc 4.680306
0t7nhVLii6tSAxtLhc 4.565023
fSUyR.vR7Xu0iR4nUU 2.885992
0Tm7hdRJxd9zoevPlA 2.866847
> head(s33)
                      B_DwC
Ebfrl.7uOZfnjp_E7k 7.583709
ueQUrXd5FH554RlhZc 5.177791
0Uu3XrB6Bd14qoNeuc 4.680306
0t7nhVLii6tSAxtLhc 4.565023
fSUyR.vR7Xu0iR4nUU 2.885992
0Tm7hdRJxd9zoevPlA 2.866847

I tried merging them using:

rn <- rownames(s11)
l <- list(s11, s22, s33)
allF <- l[[1]]
for(i in 2:length(l)) {
  dat <- merge(allF, l[[i]],  by= "row.names", all.x= F, all.y= F) [,-1]
  rownames(allF) <- rn
}

but my allF has only one column in results, while it would have all 3
from all 3 data frames:

> head(allF)
                      B_NoD
Ebfrl.7uOZfnjp_E7k 7.583709
ueQUrXd5FH554RlhZc 5.177791
0Uu3XrB6Bd14qoNeuc 4.680306
0t7nhVLii6tSAxtLhc 4.565023
fSUyR.vR7Xu0iR4nUU 2.885992
0Tm7hdRJxd9zoevPlA 2.866847

Please advise,
Ana


From cry@n @end|ng |rom b|ngh@mton@edu  Tue Nov  5 17:40:45 2019
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W Ryan)
Date: Tue, 5 Nov 2019 11:40:45 -0500
Subject: [R] how to place a rug on only the x-axis in a scatterplot with
 lattice
Message-ID: <CAM+rpY=6BEQ5ss44PSbCfTAoqL8jh1BGKwCcNKRaQ_u-yg5Aag@mail.gmail.com>

The following produces a scatterplot with rugs on both the vertical and
horizontal axes.

library(dplyr)
library(stringr)
library(lattice)
library(latticeExtra)
## .....
xyplot(scheduleInterval ~ calledForApptDate, data = dd.2, xlab = "Date
patient called for appointment", ylab = "Days in the future that patient
was scheduled",
panel = function(...) {
panel.xyplot(..., col = "red")
panel.smoother(..., span = 0.9, se = FALSE)
panel.rug(...)
})

I'd like a rug to appear only on the horizontal axis.  None of the
following seem to be the correct syntax:

panel.rug(..., y = NULL)
panel.rug(..., y = FALSE)
panel.rug(x)
panel.rug(x = ...)

This does the job:

xyplot(scheduleInterval ~ calledForApptDate, data = dd.2, xlab = "Date
patient called for appointment", ylab = "Days in the future that patient
was scheduled",
panel = function(...) {
panel.xyplot(..., col = "red")
panel.smoother(..., span = 0.9, se = FALSE)
panel.rug(x = dd.2$calledForApptDate)
})

but seems inadvisable. Shouldn't I be making use of ... for passing
arguments through to the panel.rug() function?  Specifying a variable in a
dataframe by name isn't generalizable.

Thanks.

--Chris Ryan

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Tue Nov  5 17:50:09 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 5 Nov 2019 18:50:09 +0200
Subject: [R] How to merge 3 data frames by rownames?
In-Reply-To: <CAF9-5jNOiTKiRd584_-3nm5L3gN8d4-dDSPPhF87P50ieo9DEw@mail.gmail.com>
References: <CAF9-5jNOiTKiRd584_-3nm5L3gN8d4-dDSPPhF87P50ieo9DEw@mail.gmail.com>
Message-ID: <CAGgJW76Wd4C31zZshU85T77Fo3ZXUTs7fqE2Nmr5hGwXPbij9g@mail.gmail.com>

I think your code is a bit buggy. Try this

for(i in 2:length(l)) {
  allF <- merge(allF, l[[i]],  by= "row.names", all.x= F, all.y= F)
  rownames(allF) <- allF$Row.names
  allF <- allF[,-1]
}

HTH,
Eric


On Tue, Nov 5, 2019 at 6:16 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hi,
>
> I have 3 data frames like this:
>
> > head(s11)
>                       B_NoD
> Ebfrl.7uOZfnjp_E7k 7.583709
> ueQUrXd5FH554RlhZc 5.177791
> 0Uu3XrB6Bd14qoNeuc 4.680306
> 0t7nhVLii6tSAxtLhc 4.565023
> fSUyR.vR7Xu0iR4nUU 2.885992
> 0Tm7hdRJxd9zoevPlA 2.866847
> > head(s22)
>                      B_DwoC
> Ebfrl.7uOZfnjp_E7k 7.583709
> ueQUrXd5FH554RlhZc 5.177791
> 0Uu3XrB6Bd14qoNeuc 4.680306
> 0t7nhVLii6tSAxtLhc 4.565023
> fSUyR.vR7Xu0iR4nUU 2.885992
> 0Tm7hdRJxd9zoevPlA 2.866847
> > head(s33)
>                       B_DwC
> Ebfrl.7uOZfnjp_E7k 7.583709
> ueQUrXd5FH554RlhZc 5.177791
> 0Uu3XrB6Bd14qoNeuc 4.680306
> 0t7nhVLii6tSAxtLhc 4.565023
> fSUyR.vR7Xu0iR4nUU 2.885992
> 0Tm7hdRJxd9zoevPlA 2.866847
>
> I tried merging them using:
>
> rn <- rownames(s11)
> l <- list(s11, s22, s33)
> allF <- l[[1]]
> for(i in 2:length(l)) {
>   dat <- merge(allF, l[[i]],  by= "row.names", all.x= F, all.y= F) [,-1]
>   rownames(allF) <- rn
> }
>
> but my allF has only one column in results, while it would have all 3
> from all 3 data frames:
>
> > head(allF)
>                       B_NoD
> Ebfrl.7uOZfnjp_E7k 7.583709
> ueQUrXd5FH554RlhZc 5.177791
> 0Uu3XrB6Bd14qoNeuc 4.680306
> 0t7nhVLii6tSAxtLhc 4.565023
> fSUyR.vR7Xu0iR4nUU 2.885992
> 0Tm7hdRJxd9zoevPlA 2.866847
>
> Please advise,
> Ana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov  5 18:02:20 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 5 Nov 2019 11:02:20 -0600
Subject: [R] how to get higher precision p value output
Message-ID: <CAF9-5jPtD4g4g-ucP64HHoAgWiL2zFKQUuxA4gpVVizQSDmm6A@mail.gmail.com>

Hi,

I am running this function:

library(psych)
corr.test.col.1to3 <- corr.test(allF[1:3], method = "spearman", use =
"complete.obs")
names(corr.test.col.1to3)
corr.test.col.1to3$p

and my result looks like this:

> corr.test.col.1to3$p
           B_NoD    B_DwoC B_DwC
B_NoD  0.0000000 0.0000000     1
B_DwoC 0.0000000 0.0000000     1
B_DwC  0.6501836 0.6501836     0

Does anyone know how to get higher precision for those p values
instead of 0.0000000?

I tried:
corr.test.col.1to3 <- corr.test(allF[1:3], method = "spearman", use =
"complete.obs",minlength=20)

but it didn't change anything

if I do:
> str(corr.test.col.1to3)
List of 11
 $ r     : num [1:3, 1:3] 1 1 0.0139 1 1 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
 $ n     : num 1068
 $ t     : num [1:3, 1:3] Inf Inf 0.454 Inf Inf ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
 $ p     : num [1:3, 1:3] 0 0 0.65 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
 $ se    : num [1:3, 1:3] 0 0 0.0306 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
  .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
 $ sef   : num 0.0306
 $ adjust: chr "holm"
 $ sym   : logi TRUE
 $ ci    :'data.frame':    3 obs. of  4 variables:
  ..$ lower: num [1:3] NaN -0.0461 -0.0461
  ..$ r    : num [1:3] 1 0.0139 0.0139
  ..$ upper: num [1:3] NaN 0.0738 0.0738
  ..$ p    : num [1:3] 0 0.65 0.65
 $ ci.adj:'data.frame':    3 obs. of  2 variables:
  ..$ lower.adj: num [1:3] NaN -0.0461 -0.0547
  ..$ upper.adj: num [1:3] NaN 0.0738 0.0824
 $ Call  : language corr.test(x = allF[1:3], use = "complete.obs",
method = "spearman")
 - attr(*, "class")= chr [1:2] "psych" "corr.test"


From w@g@t@||@j@me@ @end|ng |rom gm@||@com  Tue Nov  5 13:35:09 2019
From: w@g@t@||@j@me@ @end|ng |rom gm@||@com (James Wagstaff)
Date: Tue, 5 Nov 2019 12:35:09 +0000
Subject: [R] Global curve fitting/shared parameters with nls() alternatives
Message-ID: <CAKCTGt+MPECxaguw2beeZhp3SVF=PaA63FBFkGq8f1s2bOgTeg@mail.gmail.com>

Hello
I am trying to determine least-squares estimates of the parameters of a
nonlinear model, where I expect some parameters to remain constant across
experiments, and for others to vary. I believe this is typically referred
to as global curve fitting, or the presence of shared/nested parameters.
The "[]" syntax in the stats::nls() function is an extremely convenient
solution (
https://r.789695.n4.nabble.com/How-to-do-global-curve-fitting-in-R-td4712052.html),
but in my case I seem to need the Levenberg-Marquardt/Marquardt solvers
such as nlsr::nlxb() and minpack.lm::nlsLM. I can not find any
examples/documentation explaining a similar syntax for these tools. Is
anyone aware of a nls-like tool with this functionality, or an alternative
approach?
Best wishes
James Wagstaff

	[[alternative HTML version deleted]]


From ch@ndeep@v|rd| @end|ng |rom gm@||@com  Tue Nov  5 16:30:08 2019
From: ch@ndeep@v|rd| @end|ng |rom gm@||@com (Chandeep Kaur)
Date: Tue, 5 Nov 2019 21:00:08 +0530
Subject: [R] Help needed for one question (Urgent)
In-Reply-To: <CABcYAdJZAXmSXrX8NNi2RPzY_ohtUA6zo-F8YgUM-ROgFyqTBg@mail.gmail.com>
References: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
 <CABcYAdJZAXmSXrX8NNi2RPzY_ohtUA6zo-F8YgUM-ROgFyqTBg@mail.gmail.com>
Message-ID: <CA+qsUL0f+kokiy_a7quOhYNvEh73HDkL=iV=1-Y+Y8nPWwsZ6w@mail.gmail.com>

Dear All,

Thanks for all the support and help and I think I was able to solve my
problem.

Thanks a ton.

Best Regards,
Chandeep Kaur

On Tue, 5 Nov 2019, 8:57 pm Richard O'Keefe, <raoknz at gmail.com> wrote:

> This looks vaguely like something from exercism.
> Let's approach it logically.
>  xa xb xc ya yb zc
> We see two patterns here:
> A:  x x x y y z
> B: a b c a b c
> If only we had these two character vectors, we could use
>  paste(A, B, sep = "")
> to get the desired result.  So now we have reduced the
> problem to two simpler subproblems.  We have been given
> a clue that rep() might be useful.
> A: rep(c("x", "y", "z"), c(1, 2, 3))
> B: rep(c("a", "b", "c"), 3)
> But you were told not to use c().  So now we have three
> simpler subsubproblems:
> C: "x" "y" "z"
> D: 3 2 1
> E: "a" "b" "c"
> You were given another hint.  seq().  That builds a vector of numbers.
> Reading ?seq will give you
> D: seq(from = 3, to = 1, by = -1)
> or using ":" syntax,
> D: 3:1
>
> What about C and E?  This needs two more pieces of knowledge:
> - the variable letters,whose value is c("a","b",...,"y","z")
> - how vector indexing works in R.
> E: letters[1:3]
> C: letters[24:26]
> So now we can put all the pieces together:
> paste(rep(letters[24:26], 3:1), rep(letters[1:3], 2), sep = "")
>
> You were given
>  - seq
>  - rep
> as hints.  You were expected to look up string handling in R
> and find things like paste(), substr(), and nchar().
>
> What about the variable 'letters'?
> Well, you were expected to know or find out about substr.
> You were certainly expected to know about "vectorising".
> So you would naturally try substr("abc", 1:3, 1:3).
> And that would not work.
> So you would be expected to read the documentation:
> ?substr
> And then you would find that substr() *doesn't* do what
> you expect, but substring() *does*.  So
> C: substring("xyz", 1:3, 1:3)
> E: substring("abc", 1:3, 1:3)
>
> This is not really an exercise in R programming.
> In real R programming you *don't* avoid arbitrary aspects of the
> language and library, but use whatever is appropriate.
> So what *is* this exercise about?
>
> (1) It is an exercise in working backwards.  (See the classic book
> "How to Solve It" by Polya.)  You know what you must construct,
> you have been given some directions about what to use.  It's
> about saying "well, I could *finish* this task by doing this action,
> so what would I have to set up for that?"  In this case, the key
> step for me was seeing xa xb xc ya yb yc as (x,x,x,y,y,z)++(a,b,c,a,b,c).
> The mention of rep had me *looking* for repetitions like that.
>
> (2) It is an exercise in using the R documentation to figure out how to
> use rep and seq and what is available for splitting and pasting strings.
>
> There is of course no unique answer to this.
> substring("xaxbxcyaybzc", seq(from=1,to=11,by=2), seq(from=2,to=12,by=2))
> is another solution.  You didn't say you *had* to use rep.
>
> It's not the answer that matters for an exercise like this.
> It's how you get there.
>
>
>
>
>
> On Tue, 5 Nov 2019 at 23:40, Chandeep Kaur <chandeep.virdi at gmail.com>
> wrote:
> >
> > Dear Team,
> >
> > Could you please help me with the below question? How can I get the
> desired
> > output?
> >
> > Produce the following sequence using only rep(), seq() and potentially
> > other functions/operators. You must not use c() nor explicit loops
> >
> > ?xa? ?xb? ?xc? ?ya? ?yb? ?zc?
> >
> > Thanks & Regards,
> >
> > Chandeep Kaur
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From py_berr@rd @end|ng |rom gmx@|r  Tue Nov  5 17:02:59 2019
From: py_berr@rd @end|ng |rom gmx@|r (PY Berrard)
Date: Tue, 5 Nov 2019 17:02:59 +0100
Subject: [R] [R-pkgs] New package 'funprog' : data manipulation with
 high-order functions
Message-ID: <trinity-cfe6d357-b000-4baa-be54-3e572a938b50-1572969779507@3c-app-mailcom-bs11>

DeaR users,

A new package called 'funprog' is now on CRAN : https://CRAN.R-project.org/package=funprog

'funprog' contains high-order functions to manipulate data, given one or more auxiliary functions.
Functions are inspired by other pure functional programming languages (Haskell mainly).

The package also provides built-in function operators for creating compact anonymous functions, as well as the possibility to use the purrr package syntax.

If you are interested, you will find more details and examples here : https://gitlab.com/py_b/funprog#readme

Best regards,

Pierre-Yves B.

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From er|cjberger @end|ng |rom gm@||@com  Tue Nov  5 18:13:15 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Tue, 5 Nov 2019 19:13:15 +0200
Subject: [R] how to get higher precision p value output
In-Reply-To: <CAF9-5jPtD4g4g-ucP64HHoAgWiL2zFKQUuxA4gpVVizQSDmm6A@mail.gmail.com>
References: <CAF9-5jPtD4g4g-ucP64HHoAgWiL2zFKQUuxA4gpVVizQSDmm6A@mail.gmail.com>
Message-ID: <CAGgJW76KwekPP0y_YNss8JiMFCJRak3buKRfA=KdNaAYecsOEg@mail.gmail.com>

> set.seed(1)
> m <- matrix(rnorm(500),ncol=2)
> cor(m)
#             [,1]       [,2]
#  [1,] 1.00000000 0.04060113
#  [2,] 0.04060113 1.00000000

> options(digits=12)
> cor(m)
#                 [,1]            [,2]
# [1,] 1.0000000000000 0.0406011304584
# [2,] 0.0406011304584 1.0000000000000

HTH,
Eric


On Tue, Nov 5, 2019 at 7:02 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hi,
>
> I am running this function:
>
> library(psych)
> corr.test.col.1to3 <- corr.test(allF[1:3], method = "spearman", use =
> "complete.obs")
> names(corr.test.col.1to3)
> corr.test.col.1to3$p
>
> and my result looks like this:
>
> > corr.test.col.1to3$p
>            B_NoD    B_DwoC B_DwC
> B_NoD  0.0000000 0.0000000     1
> B_DwoC 0.0000000 0.0000000     1
> B_DwC  0.6501836 0.6501836     0
>
> Does anyone know how to get higher precision for those p values
> instead of 0.0000000?
>
> I tried:
> corr.test.col.1to3 <- corr.test(allF[1:3], method = "spearman", use =
> "complete.obs",minlength=20)
>
> but it didn't change anything
>
> if I do:
> > str(corr.test.col.1to3)
> List of 11
>  $ r     : num [1:3, 1:3] 1 1 0.0139 1 1 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>  $ n     : num 1068
>  $ t     : num [1:3, 1:3] Inf Inf 0.454 Inf Inf ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>  $ p     : num [1:3, 1:3] 0 0 0.65 0 0 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>  $ se    : num [1:3, 1:3] 0 0 0.0306 0 0 ...
>   ..- attr(*, "dimnames")=List of 2
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>   .. ..$ : chr [1:3] "B_NoD" "B_DwoC" "B_DwC"
>  $ sef   : num 0.0306
>  $ adjust: chr "holm"
>  $ sym   : logi TRUE
>  $ ci    :'data.frame':    3 obs. of  4 variables:
>   ..$ lower: num [1:3] NaN -0.0461 -0.0461
>   ..$ r    : num [1:3] 1 0.0139 0.0139
>   ..$ upper: num [1:3] NaN 0.0738 0.0738
>   ..$ p    : num [1:3] 0 0.65 0.65
>  $ ci.adj:'data.frame':    3 obs. of  2 variables:
>   ..$ lower.adj: num [1:3] NaN -0.0461 -0.0547
>   ..$ upper.adj: num [1:3] NaN 0.0738 0.0824
>  $ Call  : language corr.test(x = allF[1:3], use = "complete.obs",
> method = "spearman")
>  - attr(*, "class")= chr [1:2] "psych" "corr.test"
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Nov  5 18:17:10 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 5 Nov 2019 09:17:10 -0800
Subject: [R] (no subject)
In-Reply-To: <CA+ysrC5xeGY3osE-jv=FtbZUrJkrEcvxMMYLUtBwnXPsgYJWZQ@mail.gmail.com>
References: <CA+ysrC5xeGY3osE-jv=FtbZUrJkrEcvxMMYLUtBwnXPsgYJWZQ@mail.gmail.com>
Message-ID: <d896c2ce-79bb-3d8d-da19-08895b7a0bc5@comcast.net>

This question is off-topic for rhelp despite your use of an R package 
because it is a request for advice for statistical issues, rather than 
about R coding. You should read the Posting guide where you are advised 
of this concern. You are also asked to post in plain text and include an 
informative subject line. There are other venues for statistical advice 
such as stats.stackexchange.com and there is also the possibility that 
the package author or maintainer might respond to a polite email.


-- 

David.

On 11/5/19 4:23 AM, imran damkar wrote:
> Greetings,
> I have collected the data based on 5 point likert scale(very low, low,
> neutral, high,very high) on the factor considered by individual before
> making investment decision. There are five factors (D.Vs) Influencing
> investment decisions. I am interested in knowing whether there is
> significant difference in preference level of male and female(IVs) for the
> above said factors.Number of males are 252 and females 98. I have applied
> Permutational Manova in adonis function of R, since data are ordinal in
> nature, and result is non significant. I have read in literature that
> permutation test is non parametric in nature. Does for the permutation test
> equality of variance is highly important. Also the computation of mean, S.
> D and variance are discouraged by many scholars for ordinal data.
> Pls through some light of permutation test and it?s assumption and
> violation of it. And if in case PERMANAVO is not suitable for my data then
> please suggest appropriate alternative.
> Shall be highly obliged.
> Thanks
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Nov  5 18:28:15 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 Nov 2019 09:28:15 -0800
Subject: [R] how to place a rug on only the x-axis in a scatterplot with
 lattice
In-Reply-To: <CAM+rpY=6BEQ5ss44PSbCfTAoqL8jh1BGKwCcNKRaQ_u-yg5Aag@mail.gmail.com>
References: <CAM+rpY=6BEQ5ss44PSbCfTAoqL8jh1BGKwCcNKRaQ_u-yg5Aag@mail.gmail.com>
Message-ID: <CAGxFJbQNZiSgSGBTEBZNsE_OwcBUDXyXe27a0WA2puE+727kaw@mail.gmail.com>

Here's how you pass an argument down to the panel function.

foo <- runif(30,0,5)
y <- rnorm(30, mean = 10)
xyplot(y~foo,
       panel = function(x,...) {
          panel.xyplot(x,..., col = "red")
          panel.rug(x, col="black")
       })

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 5, 2019 at 8:41 AM Christopher W Ryan <cryan at binghamton.edu>
wrote:

> The following produces a scatterplot with rugs on both the vertical and
> horizontal axes.
>
> library(dplyr)
> library(stringr)
> library(lattice)
> library(latticeExtra)
> ## .....
> xyplot(scheduleInterval ~ calledForApptDate, data = dd.2, xlab = "Date
> patient called for appointment", ylab = "Days in the future that patient
> was scheduled",
> panel = function(...) {
> panel.xyplot(..., col = "red")
> panel.smoother(..., span = 0.9, se = FALSE)
> panel.rug(...)
> })
>
> I'd like a rug to appear only on the horizontal axis.  None of the
> following seem to be the correct syntax:
>
> panel.rug(..., y = NULL)
> panel.rug(..., y = FALSE)
> panel.rug(x)
> panel.rug(x = ...)
>
> This does the job:
>
> xyplot(scheduleInterval ~ calledForApptDate, data = dd.2, xlab = "Date
> patient called for appointment", ylab = "Days in the future that patient
> was scheduled",
> panel = function(...) {
> panel.xyplot(..., col = "red")
> panel.smoother(..., span = 0.9, se = FALSE)
> panel.rug(x = dd.2$calledForApptDate)
> })
>
> but seems inadvisable. Shouldn't I be making use of ... for passing
> arguments through to the panel.rug() function?  Specifying a variable in a
> dataframe by name isn't generalizable.
>
> Thanks.
>
> --Chris Ryan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Nov  5 21:27:48 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 Nov 2019 12:27:48 -0800
Subject: [R] 
 Global curve fitting/shared parameters with nls() alternatives
In-Reply-To: <CAKCTGt+MPECxaguw2beeZhp3SVF=PaA63FBFkGq8f1s2bOgTeg@mail.gmail.com>
References: <CAKCTGt+MPECxaguw2beeZhp3SVF=PaA63FBFkGq8f1s2bOgTeg@mail.gmail.com>
Message-ID: <CAGxFJbQwWmv_rw9TdziNBqFb9bqArskp11PEWpiEZhShmWPo0A@mail.gmail.com>

A simplified example of what you wish to do might help to clarify here.

Here's my guess. Feel free to dismiss if I'm off base.

Suppose your model is:
y = exp(a*x) + b

and you wish the b to be constant but the a to vary across expts. Then can
you not combine the data from both into single x, y vectors, add a variable
expt that takes the value 1 for expt1 and 2 for expt 2 and fit the single
model:

y = (expt ==1)*(exp(a1*x) + b)   +  (expt == 2)* (exp(a2*x) + b)

This would obtain separate estimates of a1 and a2 but a single estimate of
b .

There are probably better ways to do this, but I've done hardly any
nonlinear model fitting (so warning!) and can only offer this brute force
approach; so wait for someone to suggest something better before trying it.

Cheers,
Bert


On Tue, Nov 5, 2019 at 9:12 AM James Wagstaff <wagstaff.james at gmail.com>
wrote:

> Hello
> I am trying to determine least-squares estimates of the parameters of a
> nonlinear model, where I expect some parameters to remain constant across
> experiments, and for others to vary. I believe this is typically referred
> to as global curve fitting, or the presence of shared/nested parameters.
> The "[]" syntax in the stats::nls() function is an extremely convenient
> solution (
>
> https://r.789695.n4.nabble.com/How-to-do-global-curve-fitting-in-R-td4712052.html
> ),
> but in my case I seem to need the Levenberg-Marquardt/Marquardt solvers
> such as nlsr::nlxb() and minpack.lm::nlsLM. I can not find any
> examples/documentation explaining a similar syntax for these tools. Is
> anyone aware of a nls-like tool with this functionality, or an alternative
> approach?
> Best wishes
> James Wagstaff
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From r@turner @end|ng |rom @uck|@nd@@c@nz  Tue Nov  5 22:51:45 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Wed, 6 Nov 2019 10:51:45 +1300
Subject: [R] [FORGED] Re:  Help needed for one question (Urgent)
In-Reply-To: <CABcYAdJZAXmSXrX8NNi2RPzY_ohtUA6zo-F8YgUM-ROgFyqTBg@mail.gmail.com>
References: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
 <CABcYAdJZAXmSXrX8NNi2RPzY_ohtUA6zo-F8YgUM-ROgFyqTBg@mail.gmail.com>
Message-ID: <2e5cd137-20e2-6893-4f36-ff14e87c2925@auckland.ac.nz>


Richard:  I know that you mean well, but *please* don't do people's 
homework for them!!!  (They are *cheating* by asking R-help to do their 
homework.)

cheers,

Rolf Turner

On 6/11/19 4:27 AM, Richard O'Keefe wrote:
> This looks vaguely like something from exercism.
> Let's approach it logically.
>   xa xb xc ya yb zc
> We see two patterns here:
> A:  x x x y y z
> B: a b c a b c
> If only we had these two character vectors, we could use
>   paste(A, B, sep = "")
> to get the desired result.  So now we have reduced the
> problem to two simpler subproblems.  We have been given
> a clue that rep() might be useful.
> A: rep(c("x", "y", "z"), c(1, 2, 3))
> B: rep(c("a", "b", "c"), 3)
> But you were told not to use c().  So now we have three
> simpler subsubproblems:
> C: "x" "y" "z"
> D: 3 2 1
> E: "a" "b" "c"
> You were given another hint.  seq().  That builds a vector of numbers.
> Reading ?seq will give you
> D: seq(from = 3, to = 1, by = -1)
> or using ":" syntax,
> D: 3:1
> 
> What about C and E?  This needs two more pieces of knowledge:
> - the variable letters,whose value is c("a","b",...,"y","z")
> - how vector indexing works in R.
> E: letters[1:3]
> C: letters[24:26]
> So now we can put all the pieces together:
> paste(rep(letters[24:26], 3:1), rep(letters[1:3], 2), sep = "")
> 
> You were given
>   - seq
>   - rep
> as hints.  You were expected to look up string handling in R
> and find things like paste(), substr(), and nchar().
> 
> What about the variable 'letters'?
> Well, you were expected to know or find out about substr.
> You were certainly expected to know about "vectorising".
> So you would naturally try substr("abc", 1:3, 1:3).
> And that would not work.
> So you would be expected to read the documentation:
> ?substr
> And then you would find that substr() *doesn't* do what
> you expect, but substring() *does*.  So
> C: substring("xyz", 1:3, 1:3)
> E: substring("abc", 1:3, 1:3)
> 
> This is not really an exercise in R programming.
> In real R programming you *don't* avoid arbitrary aspects of the
> language and library, but use whatever is appropriate.
> So what *is* this exercise about?
> 
> (1) It is an exercise in working backwards.  (See the classic book
> "How to Solve It" by Polya.)  You know what you must construct,
> you have been given some directions about what to use.  It's
> about saying "well, I could *finish* this task by doing this action,
> so what would I have to set up for that?"  In this case, the key
> step for me was seeing xa xb xc ya yb yc as (x,x,x,y,y,z)++(a,b,c,a,b,c).
> The mention of rep had me *looking* for repetitions like that.
> 
> (2) It is an exercise in using the R documentation to figure out how to
> use rep and seq and what is available for splitting and pasting strings.
> 
> There is of course no unique answer to this.
> substring("xaxbxcyaybzc", seq(from=1,to=11,by=2), seq(from=2,to=12,by=2))
> is another solution.  You didn't say you *had* to use rep.
> 
> It's not the answer that matters for an exercise like this.
> It's how you get there.
> 
> 
> 
> 
> 
> On Tue, 5 Nov 2019 at 23:40, Chandeep Kaur <chandeep.virdi at gmail.com> wrote:
>>
>> Dear Team,
>>
>> Could you please help me with the below question? How can I get the desired
>> output?
>>
>> Produce the following sequence using only rep(), seq() and potentially
>> other functions/operators. You must not use c() nor explicit loops
>>
>> ?xa? ?xb? ?xc? ?ya? ?yb? ?zc?
>>
>> Thanks & Regards,
>>
>> Chandeep Kaur


From bgunter@4567 @end|ng |rom gm@||@com  Tue Nov  5 23:41:38 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 5 Nov 2019 14:41:38 -0800
Subject: [R] how to place a rug on only the x-axis in a scatterplot with
 lattice
In-Reply-To: <CAGxFJbTPMbORj0zZU59XW8=qdUOAufw18977grZeaDBaysEt+g@mail.gmail.com>
References: <CAM+rpY=6BEQ5ss44PSbCfTAoqL8jh1BGKwCcNKRaQ_u-yg5Aag@mail.gmail.com>
 <CAGxFJbQNZiSgSGBTEBZNsE_OwcBUDXyXe27a0WA2puE+727kaw@mail.gmail.com>
 <CAM+rpY=zu+GW+2=OviL75HZCTx+uzWe4wXKQVfUhaRMZEM9Fug@mail.gmail.com>
 <CAGxFJbTPMbORj0zZU59XW8=qdUOAufw18977grZeaDBaysEt+g@mail.gmail.com>
Message-ID: <CAGxFJbSP=eEyBKHRgNnb+E2_vWva4TwmqPwtk4i6wond2RE=EQ@mail.gmail.com>

For the record, I left out a key word in my prior "explanation", which I
have corrected below. I also needed to clarify something, as my original
wording is confusing. Sorry about that.

Bert


On Tue, Nov 5, 2019 at 11:09 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Lattice functions pass down their **unrecognized** arguments to the panel
> function. Once you know this, argument handling is controlled by R's usual
> rules, especially with regard to the "..." argument. Hence, you may wish to
> review tutorials on argument passing in function calls in R.
>
> But briefly, the following may be informative:
>
> xyplot(y ~ foo,
>    panel = function(...){## x and y in ... arguments
>      panel.xyplot(...)
>      panel.rug(..., col = "black")
> })
>
> will pass down all the **unrecognized** arguments in the xyplot call
> (there are none here) **plus** the x and y arguments obtained from the
> formula method. Thus panel.rug(...) will get *both* x =  and y = arguments
> and will accordingly put rugs on BOTH axes, as you saw.
>
> To prevent this, you only want to pass down the x argument, not y. Here
> are several ways to do this (check them!):
>
> ## pass down x in ... but pass y explicitly and set it to NULL in
> panel.rug call
> xyplot(y ~ foo,
>        panel = function(y,...) { ## x is in ... arguments
>           panel.xyplot(y,..., col = "red")
>           panel.rug(y = NULL,..., col="black")
>        })
>
> ## explicitly omit y from the panel.rug call (same as above):
> xyplot(y ~ foo,
>        panel = function(y,...) { ## x is in ... arguments
>           panel.xyplot(y,..., col = "red")
>           panel.rug(..., col="black")## y omitted
>        })
>
> ## only pass down x explicitly and omit y
> xyplot(y ~ foo,
>        panel = function(x,...) {  ## y is in ... arguments
>           panel.xyplot(x,..., col = "red")
>           panel.rug(x, col="black") ## only x argument is passed
>        })
>
> Cheers,
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Nov 5, 2019 at 10:04 AM Christopher W Ryan <cryan at binghamton.edu>
> wrote:
>
>> Thanks Bert. So my lesson here is that I have to "feed" "x" to all my
>> panel functions "upstream" from my panel.rug()?
>>
>> --Chris Ryan
>>
>> On Tue, Nov 5, 2019 at 12:28 PM Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>>> Here's how you pass an argument down to the panel function.
>>>
>>> foo <- runif(30,0,5)
>>> y <- rnorm(30, mean = 10)
>>> xyplot(y~foo,
>>>        panel = function(x,...) {
>>>           panel.xyplot(x,..., col = "red")
>>>           panel.rug(x, col="black")
>>>        })
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> Bert Gunter
>>>
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>>
>>> On Tue, Nov 5, 2019 at 8:41 AM Christopher W Ryan <cryan at binghamton.edu>
>>> wrote:
>>>
>>>> The following produces a scatterplot with rugs on both the vertical and
>>>> horizontal axes.
>>>>
>>>> library(dplyr)
>>>> library(stringr)
>>>> library(lattice)
>>>> library(latticeExtra)
>>>> ## .....
>>>> xyplot(scheduleInterval ~ calledForApptDate, data = dd.2, xlab = "Date
>>>> patient called for appointment", ylab = "Days in the future that patient
>>>> was scheduled",
>>>> panel = function(...) {
>>>> panel.xyplot(..., col = "red")
>>>> panel.smoother(..., span = 0.9, se = FALSE)
>>>> panel.rug(...)
>>>> })
>>>>
>>>> I'd like a rug to appear only on the horizontal axis.  None of the
>>>> following seem to be the correct syntax:
>>>>
>>>> panel.rug(..., y = NULL)
>>>> panel.rug(..., y = FALSE)
>>>> panel.rug(x)
>>>> panel.rug(x = ...)
>>>>
>>>> This does the job:
>>>>
>>>> xyplot(scheduleInterval ~ calledForApptDate, data = dd.2, xlab = "Date
>>>> patient called for appointment", ylab = "Days in the future that patient
>>>> was scheduled",
>>>> panel = function(...) {
>>>> panel.xyplot(..., col = "red")
>>>> panel.smoother(..., span = 0.9, se = FALSE)
>>>> panel.rug(x = dd.2$calledForApptDate)
>>>> })
>>>>
>>>> but seems inadvisable. Shouldn't I be making use of ... for passing
>>>> arguments through to the panel.rug() function?  Specifying a variable
>>>> in a
>>>> dataframe by name isn't generalizable.
>>>>
>>>> Thanks.
>>>>
>>>> --Chris Ryan
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>

	[[alternative HTML version deleted]]


From @zwj|08 @end|ng |rom gm@||@com  Wed Nov  6 02:21:04 2019
From: @zwj|08 @end|ng |rom gm@||@com (Wang Jiefei)
Date: Tue, 5 Nov 2019 20:21:04 -0500
Subject: [R] [FORGED] Re: Help needed for one question (Urgent)
In-Reply-To: <2e5cd137-20e2-6893-4f36-ff14e87c2925@auckland.ac.nz>
References: <CA+qsUL33-D2qv7HNaa1hV0tkmM5AEOXFQOqJxHfYy+6z6QahqQ@mail.gmail.com>
 <CABcYAdJZAXmSXrX8NNi2RPzY_ohtUA6zo-F8YgUM-ROgFyqTBg@mail.gmail.com>
 <2e5cd137-20e2-6893-4f36-ff14e87c2925@auckland.ac.nz>
Message-ID: <CAGiFhPNnUZik=XYgAmTnDyMFZPCm+9PzSRsU40RFwdagXLgQtg@mail.gmail.com>

Agree, especially there is an "Urgent" on the title. He must be too
"urgent" to think about your answer. I will wonder if your effort will be
in vain.

Best,
Jiefei



On Tue, Nov 5, 2019 at 4:52 PM Rolf Turner <r.turner at auckland.ac.nz> wrote:

>
> Richard:  I know that you mean well, but *please* don't do people's
> homework for them!!!  (They are *cheating* by asking R-help to do their
> homework.)
>
> cheers,
>
> Rolf Turner
>
> On 6/11/19 4:27 AM, Richard O'Keefe wrote:
> > This looks vaguely like something from exercism.
> > Let's approach it logically.
> >   xa xb xc ya yb zc
> > We see two patterns here:
> > A:  x x x y y z
> > B: a b c a b c
> > If only we had these two character vectors, we could use
> >   paste(A, B, sep = "")
> > to get the desired result.  So now we have reduced the
> > problem to two simpler subproblems.  We have been given
> > a clue that rep() might be useful.
> > A: rep(c("x", "y", "z"), c(1, 2, 3))
> > B: rep(c("a", "b", "c"), 3)
> > But you were told not to use c().  So now we have three
> > simpler subsubproblems:
> > C: "x" "y" "z"
> > D: 3 2 1
> > E: "a" "b" "c"
> > You were given another hint.  seq().  That builds a vector of numbers.
> > Reading ?seq will give you
> > D: seq(from = 3, to = 1, by = -1)
> > or using ":" syntax,
> > D: 3:1
> >
> > What about C and E?  This needs two more pieces of knowledge:
> > - the variable letters,whose value is c("a","b",...,"y","z")
> > - how vector indexing works in R.
> > E: letters[1:3]
> > C: letters[24:26]
> > So now we can put all the pieces together:
> > paste(rep(letters[24:26], 3:1), rep(letters[1:3], 2), sep = "")
> >
> > You were given
> >   - seq
> >   - rep
> > as hints.  You were expected to look up string handling in R
> > and find things like paste(), substr(), and nchar().
> >
> > What about the variable 'letters'?
> > Well, you were expected to know or find out about substr.
> > You were certainly expected to know about "vectorising".
> > So you would naturally try substr("abc", 1:3, 1:3).
> > And that would not work.
> > So you would be expected to read the documentation:
> > ?substr
> > And then you would find that substr() *doesn't* do what
> > you expect, but substring() *does*.  So
> > C: substring("xyz", 1:3, 1:3)
> > E: substring("abc", 1:3, 1:3)
> >
> > This is not really an exercise in R programming.
> > In real R programming you *don't* avoid arbitrary aspects of the
> > language and library, but use whatever is appropriate.
> > So what *is* this exercise about?
> >
> > (1) It is an exercise in working backwards.  (See the classic book
> > "How to Solve It" by Polya.)  You know what you must construct,
> > you have been given some directions about what to use.  It's
> > about saying "well, I could *finish* this task by doing this action,
> > so what would I have to set up for that?"  In this case, the key
> > step for me was seeing xa xb xc ya yb yc as (x,x,x,y,y,z)++(a,b,c,a,b,c).
> > The mention of rep had me *looking* for repetitions like that.
> >
> > (2) It is an exercise in using the R documentation to figure out how to
> > use rep and seq and what is available for splitting and pasting strings.
> >
> > There is of course no unique answer to this.
> > substring("xaxbxcyaybzc", seq(from=1,to=11,by=2), seq(from=2,to=12,by=2))
> > is another solution.  You didn't say you *had* to use rep.
> >
> > It's not the answer that matters for an exercise like this.
> > It's how you get there.
> >
> >
> >
> >
> >
> > On Tue, 5 Nov 2019 at 23:40, Chandeep Kaur <chandeep.virdi at gmail.com>
> wrote:
> >>
> >> Dear Team,
> >>
> >> Could you please help me with the below question? How can I get the
> desired
> >> output?
> >>
> >> Produce the following sequence using only rep(), seq() and potentially
> >> other functions/operators. You must not use c() nor explicit loops
> >>
> >> ?xa? ?xb? ?xc? ?ya? ?yb? ?zc?
> >>
> >> Thanks & Regards,
> >>
> >> Chandeep Kaur
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From v@|kremk @end|ng |rom gm@||@com  Wed Nov  6 03:24:15 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Tue, 5 Nov 2019 20:24:15 -0600
Subject: [R] File conca.
In-Reply-To: <144d81c7129241f1940641fcf8225f73@SRVEXCHCM1301.precheza.cz>
References: <CAJOiR6abaaKwdhYrahij3TgRKgEbm=ttrONMZzSyD-yVitDNcg@mail.gmail.com>
 <144d81c7129241f1940641fcf8225f73@SRVEXCHCM1301.precheza.cz>
Message-ID: <CAJOiR6bEARbT4SB9T3oNYxvs=UFZ=+VVSfYA9gz-WMxF7=Yn5g@mail.gmail.com>

Thank you Petr and Jeff fro your suggestions.

I made some improvement but  still need some tweaking.  I could not
get correctly the folders names added to each row. Only the last
forename was added.
table(Alldata$oldername) resulted
   week2
    25500

Please see the complete,

####################################################
folders=c("week1","week2")
for(i in folders){
  path=paste("\data\"", i , sep = "")
  wd <-  setwd(path)
  Flist = list.files(path,pattern = "^WT")
  dataA =  lapply(Flist, function(x)read.csv(x, header=T))
  setwd(wd)
  temp = do.call("rbind", Alldata)
  temp$foldername <- i
  Alldata <- temp
  Alldata <- rbind(Alldata, temp)
}
#######################################################
Any suggestion please?


On Tue, Nov 5, 2019 at 2:13 AM PIKAL Petr <petr.pikal at precheza.cz> wrote:
>
> Hi
>
> Help with such operations is rather tricky as only you know exact structrure
> of your folders.
>
> see some hints in line
>
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> > Sent: Tuesday, November 5, 2019 4:33 AM
> > To: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> > Subject: [R] File conca.
> >
> > Hi All,
> >
> > I have data files in several folders and want combine all  these files in
> one
> > file.  In each folder  there are several files  and these
> > files have the same structure but different names.   First, in each
> > folder  I want to concatenate(rbind) all files in to one file. While I am
> > reading each files and concatenating (rbind) all files, I want to added
> the
> > folder name as one variable  in each row. I am reading the folder names
> > from a file and for demonstration I am using only two folders  as shown
> > below.
> > Data\week1             # folder name 1
> >            WT13.csv
> >            WT26.csv           ...
> >            WT10.csv
> > Data\week2            #folder name 2
> >            WT02.csv
> >            WT12.csv
> >
> > Below please find  my attempt,
> >
> > folders=c("week1","week2")
> > for(i in folders){
> >   path=paste("\data\"", i , sep = "")
> >   setwd(path)
>
> you should use
> wd <- setwd(path)
>
> which keeps the original directory for subsequent use
>
> >   Flist = list.files(path,pattern = "^WT")
> >   dataA =  lapply(Flist, function(x)read.csv(x, header=T))
> >   Alldata = do.call("rbind", dataA)     # combine all files
> >   Alldata$foldername=i                  # adding the folder name
> >
>
> now you can do
>
> setwd(wd)
>
> to return to original directory
> }
>
> > The above works for  for one folder but how can I do it for more than one
> > folders?
>
> You also need to decide if you want all data from all folders in one object
> called Alldata or if you want several Alldata objects, one for each folder.
>
> In second case you could use list structure for Alldata. In the first case
> you could store data from each folder in some temporary object and use rbind
> directly.
>
> something like
>
> temp <- do.call("rbind", dataA)
> temp$foldername <- i
>
> Alldata <- temp
> in the first cycle
> and
> Alldata <- rbind(Alldata, temp)
> in second and all others.
>
> Or you could initiate first Alldata manually and use only
> Alldata <- rbind(Alldata, temp)
>
> in your loop.
>
> Cheers
> Petr
>
> >
> > Thank you in advance,
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From petr@p|k@| @end|ng |rom prechez@@cz  Wed Nov  6 08:50:56 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 6 Nov 2019 07:50:56 +0000
Subject: [R] File conca.
In-Reply-To: <CAJOiR6bEARbT4SB9T3oNYxvs=UFZ=+VVSfYA9gz-WMxF7=Yn5g@mail.gmail.com>
References: <CAJOiR6abaaKwdhYrahij3TgRKgEbm=ttrONMZzSyD-yVitDNcg@mail.gmail.com>
 <144d81c7129241f1940641fcf8225f73@SRVEXCHCM1301.precheza.cz>
 <CAJOiR6bEARbT4SB9T3oNYxvs=UFZ=+VVSfYA9gz-WMxF7=Yn5g@mail.gmail.com>
Message-ID: <0e0aeb883a9c4f6e8ebab26d7a4dc6ab@SRVEXCHCM1301.precheza.cz>

Hi

in line

> -----Original Message-----
> From: Val <valkremk at gmail.com>
> Sent: Wednesday, November 6, 2019 3:24 AM
> To: PIKAL Petr <petr.pikal at precheza.cz>
> Cc: r-help at R-project.org (r-help at r-project.org) <r-help at r-project.org>
> Subject: Re: [R] File conca.
>
> Thank you Petr and Jeff fro your suggestions.
>
> I made some improvement but  still need some tweaking.  I could not get
> correctly the folders names added to each row. Only the last forename was
> added.
> table(Alldata$oldername) resulted
>    week2
>     25500
>
> Please see the complete,
>
> ####################################################
> folders=c("week1","week2")
> for(i in folders){
>   path=paste("\data\"", i , sep = "")
>   wd <-  setwd(path)
>   Flist = list.files(path,pattern = "^WT")
>   dataA =  lapply(Flist, function(x)read.csv(x, header=T))
>   setwd(wd)
>   temp = do.call("rbind", Alldata)


Shouldn't it be
temp = do.call("rbind", dataA)


This is problematic piece

>   temp$foldername <- i # this seems to be OK

But these in each cycle put recent temp in Alldata and adds temp again by 
rbinding.

>   Alldata <- temp
>   Alldata <- rbind(Alldata, temp)

I understand from your description that you want all data from all files in 
one Alldata object.

You could either read the files from first folder and put them into Alldata 
**before** your cycle.
Alldata <- temp
After you declare Alldata in such way, you could use only

Alldata <- rbind(Alldata, temp)

in your cycle to add data from other folders.

Or you could use some incremental variable to check if it is the first run.

something like

k <- 0

for(i in folders){...
k <- k+1
....
if (k==1)    Alldata <- temp else Alldata <- rbind(Alldata, temp)
...
}

Cheers
Petr

> }
> #######################################################
> Any suggestion please?
>
>
> On Tue, Nov 5, 2019 at 2:13 AM PIKAL Petr <petr.pikal at precheza.cz> wrote:
> >
> > Hi
> >
> > Help with such operations is rather tricky as only you know exact
> > structrure of your folders.
> >
> > see some hints in line
> >
> > > -----Original Message-----
> > > From: R-help <r-help-bounces at r-project.org> On Behalf Of Val
> > > Sent: Tuesday, November 5, 2019 4:33 AM
> > > To: r-help at R-project.org (r-help at r-project.org)
> > > <r-help at r-project.org>
> > > Subject: [R] File conca.
> > >
> > > Hi All,
> > >
> > > I have data files in several folders and want combine all  these
> > > files in
> > one
> > > file.  In each folder  there are several files  and these
> > > files have the same structure but different names.   First, in each
> > > folder  I want to concatenate(rbind) all files in to one file. While
> > > I am reading each files and concatenating (rbind) all files, I want
> > > to added
> > the
> > > folder name as one variable  in each row. I am reading the folder
> > > names from a file and for demonstration I am using only two folders
> > > as shown below.
> > > Data\week1             # folder name 1
> > >            WT13.csv
> > >            WT26.csv           ...
> > >            WT10.csv
> > > Data\week2            #folder name 2
> > >            WT02.csv
> > >            WT12.csv
> > >
> > > Below please find  my attempt,
> > >
> > > folders=c("week1","week2")
> > > for(i in folders){
> > >   path=paste("\data\"", i , sep = "")
> > >   setwd(path)
> >
> > you should use
> > wd <- setwd(path)
> >
> > which keeps the original directory for subsequent use
> >
> > >   Flist = list.files(path,pattern = "^WT")
> > >   dataA =  lapply(Flist, function(x)read.csv(x, header=T))
> > >   Alldata = do.call("rbind", dataA)     # combine all files
> > >   Alldata$foldername=i                  # adding the folder name
> > >
> >
> > now you can do
> >
> > setwd(wd)
> >
> > to return to original directory
> > }
> >
> > > The above works for  for one folder but how can I do it for more
> > > than one folders?
> >
> > You also need to decide if you want all data from all folders in one
> > object called Alldata or if you want several Alldata objects, one for each
> folder.
> >
> > In second case you could use list structure for Alldata. In the first
> > case you could store data from each folder in some temporary object
> > and use rbind directly.
> >
> > something like
> >
> > temp <- do.call("rbind", dataA)
> > temp$foldername <- i
> >
> > Alldata <- temp
> > in the first cycle
> > and
> > Alldata <- rbind(Alldata, temp)
> > in second and all others.
> >
> > Or you could initiate first Alldata manually and use only Alldata <-
> > rbind(Alldata, temp)
> >
> > in your loop.
> >
> > Cheers
> > Petr
> >
> > >
> > > Thank you in advance,
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-
> > > guide.html and provide commented, minimal, self-contained,
> > > reproducible code.

From m@k@hho||y @end|ng |rom gm@||@com  Wed Nov  6 22:57:12 2019
From: m@k@hho||y @end|ng |rom gm@||@com (greg holly)
Date: Wed, 6 Nov 2019 15:57:12 -0600
Subject: [R] problem in WRS2
Message-ID: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>

I got the following error message after running  t2way(y ~ Grup*Time, data
= cp)
  Error in x[[grp[i]]] :
  attempt to select less than one element in get1index

a part of the data is given below. Your help is highly appreciated.

Greg

> head(cp)
  Birey Grup Time         y
1     1    1  Cp1 0.7916386
2     1    1  Cp3 1.7463777
3     1    1  Cp7 1.2008390
4     1    1 Cp14 0.6311380
5     1    1 Cp21 2.1563557
6     1    1 Cp28 1.2008390

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Nov  7 01:06:20 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 6 Nov 2019 16:06:20 -0800
Subject: [R] problem in WRS2
In-Reply-To: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
Message-ID: <CAGxFJbQkWKhH4e0-EP4XP_eoUcRGqqj7dcU5fELLwgwGsaYr6Q@mail.gmail.com>

This might be impossible to answer without all the data. You might wish to
contact the package maintainer for a question like this.

Cheers,
Bert



On Wed, Nov 6, 2019 at 1:58 PM greg holly <mak.hholly at gmail.com> wrote:

> I got the following error message after running  t2way(y ~ Grup*Time, data
> = cp)
>   Error in x[[grp[i]]] :
>   attempt to select less than one element in get1index
>
> a part of the data is given below. Your help is highly appreciated.
>
> Greg
>
> > head(cp)
>   Birey Grup Time         y
> 1     1    1  Cp1 0.7916386
> 2     1    1  Cp3 1.7463777
> 3     1    1  Cp7 1.2008390
> 4     1    1 Cp14 0.6311380
> 5     1    1 Cp21 2.1563557
> 6     1    1 Cp28 1.2008390
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Nov  7 01:30:38 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 6 Nov 2019 18:30:38 -0600
Subject: [R] How to interpret Mendelian randomization results?
Message-ID: <CAF9-5jPNSGCRbR0iZc+k4Oz0qxzV3OUs81E8e2HMw5B5AHvJgA@mail.gmail.com>

Hello,

I did Mendelian randomization using this software:
https://cran.r-project.org/web/packages/MendelianRandomization/vignettes/Vignette_MR.pdf

library(MendelianRandomization)
f=read.table("246LDout272Biobank_Retina.txt", header=T)
> head(f)
          rs exposure.beta exposure.se outcome.beta outcome.se
1  rs1029830      0.723525  0.03026430  0.066715400  0.0359278
2  rs1029832      0.723785  0.03029603  0.064105600  0.0359021
3 rs11078374     -0.411789  0.04189295 -0.000376929  0.0406439
4 rs11078382      0.882549  0.14275799 -0.197074000  0.1247720
5  rs1124961     -0.333763  0.05589377 -0.075468600  0.0576012
6  rs1135237     -0.316831  0.05552530 -0.074086200  0.0573111

MRInputObject <- mr_input(bx = f$exposure.beta,bxse = f$exposure.se,by
= f$outcome.beta,byse = f$outcome.se)
EggerObject <- mr_egger(MRInputObject,robust = FALSE,penalized =
FALSE,correl = FALSE,distribution = "normal",alpha = 0.05)

MR-Egger method
(variants uncorrelated, random-effect model)

Number of Variants =  246

------------------------------------------------------------------
  Method     Estimate Std Error  95% CI       p-value
  MR-Egger    0.115     0.016  0.084, 0.146   3.28e-13
 (intercept) -0.010     0.008 -0.025, 0.006   0.226
------------------------------------------------------------------
Residual Standard Error :  1.106
Heterogeneity test statistic = 298.4508 on 244 degrees of freedom,
(p-value = 0.0099)
 I^2_GX statistic: 97.2%

Let's say that my exposure is called Retina and my outcome is called
Biobank. Can someone please help me interpret these results in terms
of 'horizontal pleiotropy' as mentioned here:
https://www.ncbi.nlm.nih.gov/pubmed/29771313


Thanks
Ana


From drj|m|emon @end|ng |rom gm@||@com  Thu Nov  7 02:42:01 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 7 Nov 2019 12:42:01 +1100
Subject: [R] problem in WRS2
In-Reply-To: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
Message-ID: <CA+8X3fWY+D7y_ismHX56g_QOL9ZMXD8JuW_Y+u1xozrcHB2ZOw@mail.gmail.com>

Hi Greg,
I tried this:

cp<-read.table(text="Birey Grup Time y
 1    1  Cp1 0.7916386
 1    1  Cp3 1.7463777
 1    1  Cp7 1.2008390
 1    1 Cp14 0.6311380
 1    1 Cp21 2.1563557
 1    1 Cp28 1.2008390",
 header=TRUE)
library(prettyR)
xtab(Grup~Time,cp)
Crosstabulation of Grup by Time
       Time
Grup     Cp1      Cp14     Cp21     Cp28     Cp3      Cp7
1              1       1       1       1       1       1       6
          16.67   16.67   16.67   16.67   16.67   16.67       -
            100     100     100     100     100     100     100

              1       1       1       1       1       1       6
          16.67   16.67   16.67   16.67   16.67   16.67  100.00

Maybe doing the same on your entire data set will reveal something.

Jim

On Thu, Nov 7, 2019 at 8:58 AM greg holly <mak.hholly at gmail.com> wrote:
>
> I got the following error message after running  t2way(y ~ Grup*Time, data
> = cp)
>   Error in x[[grp[i]]] :
>   attempt to select less than one element in get1index
>
> a part of the data is given below. Your help is highly appreciated.
>
> Greg
>
> > head(cp)
>   Birey Grup Time         y
> 1     1    1  Cp1 0.7916386
> 2     1    1  Cp3 1.7463777
> 3     1    1  Cp7 1.2008390
> 4     1    1 Cp14 0.6311380
> 5     1    1 Cp21 2.1563557
> 6     1    1 Cp28 1.2008390
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dw|n@em|u@ @end|ng |rom comc@@t@net  Thu Nov  7 02:46:20 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Wed, 6 Nov 2019 17:46:20 -0800
Subject: [R] problem in WRS2
In-Reply-To: <CA+8X3fWY+D7y_ismHX56g_QOL9ZMXD8JuW_Y+u1xozrcHB2ZOw@mail.gmail.com>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
 <CA+8X3fWY+D7y_ismHX56g_QOL9ZMXD8JuW_Y+u1xozrcHB2ZOw@mail.gmail.com>
Message-ID: <4EAF168E-16AB-4EE1-B223-01785E09052E@comcast.net>

If a crosstabs on the two factors has any zero counts it might explain. 

? 
David

Sent from my iPhone

> On Nov 6, 2019, at 5:42 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> 
> Hi Greg,
> I tried this:
> 
> cp<-read.table(text="Birey Grup Time y
> 1    1  Cp1 0.7916386
> 1    1  Cp3 1.7463777
> 1    1  Cp7 1.2008390
> 1    1 Cp14 0.6311380
> 1    1 Cp21 2.1563557
> 1    1 Cp28 1.2008390",
> header=TRUE)
> library(prettyR)
> xtab(Grup~Time,cp)
> Crosstabulation of Grup by Time
>       Time
> Grup     Cp1      Cp14     Cp21     Cp28     Cp3      Cp7
> 1              1       1       1       1       1       1       6
>          16.67   16.67   16.67   16.67   16.67   16.67       -
>            100     100     100     100     100     100     100
> 
>              1       1       1       1       1       1       6
>          16.67   16.67   16.67   16.67   16.67   16.67  100.00
> 
> Maybe doing the same on your entire data set will reveal something.
> 
> Jim
> 
>> On Thu, Nov 7, 2019 at 8:58 AM greg holly <mak.hholly at gmail.com> wrote:
>> 
>> I got the following error message after running  t2way(y ~ Grup*Time, data
>> = cp)
>>  Error in x[[grp[i]]] :
>>  attempt to select less than one element in get1index
>> 
>> a part of the data is given below. Your help is highly appreciated.
>> 
>> Greg
>> 
>>> head(cp)
>>  Birey Grup Time         y
>> 1     1    1  Cp1 0.7916386
>> 2     1    1  Cp3 1.7463777
>> 3     1    1  Cp7 1.2008390
>> 4     1    1 Cp14 0.6311380
>> 5     1    1 Cp21 2.1563557
>> 6     1    1 Cp28 1.2008390
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g@@@uu| @end|ng |rom gm@||@com  Thu Nov  7 03:23:06 2019
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Thu, 7 Nov 2019 11:23:06 +0900
Subject: [R] unordered y axis
Message-ID: <CAHXS41wgs5a8_7_9b94s2sPk8PJ1OxEd9=OqhHc_P1KPicTSqg@mail.gmail.com>

Dear R-Help,

I have 35 data that is month when the annual minima happened. So I want to
plot those data but the order of y axis is not from 1 to 12, but let say
start from 9,10,11,12,1,..8. The reason to do this is when 12 (Dec) meet 1
(Jan) in the following year the graph is not quite good (for me).


year<-c(1981:2015)
month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2, 1,
1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
axis(1, at=1986:2015,cex.axis=.8)


Any lead or comment is appreciate.

Best, Ani

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Thu Nov  7 04:15:31 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 7 Nov 2019 14:15:31 +1100
Subject: [R] unordered y axis
In-Reply-To: <CAHXS41wgs5a8_7_9b94s2sPk8PJ1OxEd9=OqhHc_P1KPicTSqg@mail.gmail.com>
References: <CAHXS41wgs5a8_7_9b94s2sPk8PJ1OxEd9=OqhHc_P1KPicTSqg@mail.gmail.com>
Message-ID: <CA+8X3fWpQv959ExVmpuX2vhVSaT7GeO9S3H9cCuy0ghE3sS8pA@mail.gmail.com>

Hi Ani,
There are a number of ways to modify this sort of plot. Here is one:

x11(width=7,height=5)
par(cex.axis=.8)
plot(month~year,xaxt="n", type="l", ylab="Month", xlab="Year",
 main="Month of occurrence in year")
axis(1,at=seq(1981,2014,3))
library(plotrix)
boxed.labels(year,month,month.abb[month],border=NA,cex=0.7)

Jim

On Thu, Nov 7, 2019 at 1:24 PM ani jaya <gaaauul at gmail.com> wrote:
>
> Dear R-Help,
>
> I have 35 data that is month when the annual minima happened. So I want to
> plot those data but the order of y axis is not from 1 to 12, but let say
> start from 9,10,11,12,1,..8. The reason to do this is when 12 (Dec) meet 1
> (Jan) in the following year the graph is not quite good (for me).
>
>
> year<-c(1981:2015)
> month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2, 1,
> 1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
> plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
> axis(1, at=1986:2015,cex.axis=.8)
>
>
> Any lead or comment is appreciate.
>
> Best, Ani
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From g@@@uu| @end|ng |rom gm@||@com  Thu Nov  7 05:34:53 2019
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Thu, 7 Nov 2019 13:34:53 +0900
Subject: [R] unordered y axis
In-Reply-To: <CA+8X3fWpQv959ExVmpuX2vhVSaT7GeO9S3H9cCuy0ghE3sS8pA@mail.gmail.com>
References: <CAHXS41wgs5a8_7_9b94s2sPk8PJ1OxEd9=OqhHc_P1KPicTSqg@mail.gmail.com>
 <CA+8X3fWpQv959ExVmpuX2vhVSaT7GeO9S3H9cCuy0ghE3sS8pA@mail.gmail.com>
Message-ID: <CAHXS41xHfnjLgzhRV4Pahkx2+-+x9qSoiYQBmSBsJFvA9o4rvw@mail.gmail.com>

Dear Jim,

Thank you very much for nice suggestion and figure there. But what I need
is the y axis start from let say 7 (July) and end at 6 (June).
In those sense, I can said clearly that the occurrence, minima in this
case, is fall during winter season, because Jan and Dec close to each other.
Actually everything is good, but I want to 'kill' my curiosity.

year<-c(1981:2015)
month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2, 1,
1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
axis(1, at=1981:2015,cex.axis=.8)

Best,
Ani


On Thu, Nov 7, 2019 at 12:15 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Ani,
> There are a number of ways to modify this sort of plot. Here is one:
>
> x11(width=7,height=5)
> par(cex.axis=.8)
> plot(month~year,xaxt="n", type="l", ylab="Month", xlab="Year",
>  main="Month of occurrence in year")
> axis(1,at=seq(1981,2014,3))
> library(plotrix)
> boxed.labels(year,month,month.abb[month],border=NA,cex=0.7)
>
> Jim
>
> On Thu, Nov 7, 2019 at 1:24 PM ani jaya <gaaauul at gmail.com> wrote:
> >
> > Dear R-Help,
> >
> > I have 35 data that is month when the annual minima happened. So I want
> to
> > plot those data but the order of y axis is not from 1 to 12, but let say
> > start from 9,10,11,12,1,..8. The reason to do this is when 12 (Dec) meet
> 1
> > (Jan) in the following year the graph is not quite good (for me).
> >
> >
> > year<-c(1981:2015)
> > month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2, 1,
> > 1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
> > plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
> > axis(1, at=1986:2015,cex.axis=.8)
> >
> >
> > Any lead or comment is appreciate.
> >
> > Best, Ani
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Thu Nov  7 06:08:53 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Thu, 7 Nov 2019 16:08:53 +1100
Subject: [R] unordered y axis
In-Reply-To: <CAHXS41xHfnjLgzhRV4Pahkx2+-+x9qSoiYQBmSBsJFvA9o4rvw@mail.gmail.com>
References: <CAHXS41wgs5a8_7_9b94s2sPk8PJ1OxEd9=OqhHc_P1KPicTSqg@mail.gmail.com>
 <CA+8X3fWpQv959ExVmpuX2vhVSaT7GeO9S3H9cCuy0ghE3sS8pA@mail.gmail.com>
 <CAHXS41xHfnjLgzhRV4Pahkx2+-+x9qSoiYQBmSBsJFvA9o4rvw@mail.gmail.com>
Message-ID: <CA+8X3fUYO5=J1vR+2TEJzZ4a8+P2gawiRMSHTP3dE=sHjg5UGg@mail.gmail.com>

That's not too hard:

x11(width=7,height=5)
par(cex.axis=.8)
fin_month<-month
fin_month[fin_month>6]<-fin_month[fin_month>6]-12
fin_month<-fin_month+6
plot(fin_month~year,axes=FALSE,type="l", ylab="Month", xlab="Year",
 main="Month of occurrence in year")
axis(1,at=seq(1981,2014,3))
fin_months<-month.abb[c(7:12,1:6)]
axis(2,at=1:12,labels=month.abb[c(7:12,1:6)])
library(plotrix)
boxed.labels(year,fin_month,month.abb[month],border=NA,cex=0.7)

Jim

On Thu, Nov 7, 2019 at 3:35 PM ani jaya <gaaauul at gmail.com> wrote:
>
> Dear Jim,
>
> Thank you very much for nice suggestion and figure there. But what I need is the y axis start from let say 7 (July) and end at 6 (June).
> In those sense, I can said clearly that the occurrence, minima in this case, is fall during winter season, because Jan and Dec close to each other.
> Actually everything is good, but I want to 'kill' my curiosity.
>
> year<-c(1981:2015)
> month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2, 1, 1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
> plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
> axis(1, at=1981:2015,cex.axis=.8)
>
> Best,
> Ani
>
>
> On Thu, Nov 7, 2019 at 12:15 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>>
>> Hi Ani,
>> There are a number of ways to modify this sort of plot. Here is one:
>>
>> x11(width=7,height=5)
>> par(cex.axis=.8)
>> plot(month~year,xaxt="n", type="l", ylab="Month", xlab="Year",
>>  main="Month of occurrence in year")
>> axis(1,at=seq(1981,2014,3))
>> library(plotrix)
>> boxed.labels(year,month,month.abb[month],border=NA,cex=0.7)
>>
>> Jim
>>
>> On Thu, Nov 7, 2019 at 1:24 PM ani jaya <gaaauul at gmail.com> wrote:
>> >
>> > Dear R-Help,
>> >
>> > I have 35 data that is month when the annual minima happened. So I want to
>> > plot those data but the order of y axis is not from 1 to 12, but let say
>> > start from 9,10,11,12,1,..8. The reason to do this is when 12 (Dec) meet 1
>> > (Jan) in the following year the graph is not quite good (for me).
>> >
>> >
>> > year<-c(1981:2015)
>> > month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2, 1,
>> > 1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
>> > plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
>> > axis(1, at=1986:2015,cex.axis=.8)
>> >
>> >
>> > Any lead or comment is appreciate.
>> >
>> > Best, Ani
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From g@@@uu| @end|ng |rom gm@||@com  Thu Nov  7 10:00:37 2019
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Thu, 7 Nov 2019 18:00:37 +0900
Subject: [R] unordered y axis
In-Reply-To: <CA+8X3fUYO5=J1vR+2TEJzZ4a8+P2gawiRMSHTP3dE=sHjg5UGg@mail.gmail.com>
References: <CAHXS41wgs5a8_7_9b94s2sPk8PJ1OxEd9=OqhHc_P1KPicTSqg@mail.gmail.com>
 <CA+8X3fWpQv959ExVmpuX2vhVSaT7GeO9S3H9cCuy0ghE3sS8pA@mail.gmail.com>
 <CAHXS41xHfnjLgzhRV4Pahkx2+-+x9qSoiYQBmSBsJFvA9o4rvw@mail.gmail.com>
 <CA+8X3fUYO5=J1vR+2TEJzZ4a8+P2gawiRMSHTP3dE=sHjg5UGg@mail.gmail.com>
Message-ID: <CAHXS41wBaNd1kyEPfGi2Sb6zynqmScT2d2KTdvF66mosLHwYLg@mail.gmail.com>

Thank you very much, Jim. You help a lot!

On Thu, Nov 7, 2019 at 2:09 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> That's not too hard:
>
> x11(width=7,height=5)
> par(cex.axis=.8)
> fin_month<-month
> fin_month[fin_month>6]<-fin_month[fin_month>6]-12
> fin_month<-fin_month+6
> plot(fin_month~year,axes=FALSE,type="l", ylab="Month", xlab="Year",
>  main="Month of occurrence in year")
> axis(1,at=seq(1981,2014,3))
> fin_months<-month.abb[c(7:12,1:6)]
> axis(2,at=1:12,labels=month.abb[c(7:12,1:6)])
> library(plotrix)
> boxed.labels(year,fin_month,month.abb[month],border=NA,cex=0.7)
>
> Jim
>
> On Thu, Nov 7, 2019 at 3:35 PM ani jaya <gaaauul at gmail.com> wrote:
> >
> > Dear Jim,
> >
> > Thank you very much for nice suggestion and figure there. But what I
> need is the y axis start from let say 7 (July) and end at 6 (June).
> > In those sense, I can said clearly that the occurrence, minima in this
> case, is fall during winter season, because Jan and Dec close to each other.
> > Actually everything is good, but I want to 'kill' my curiosity.
> >
> > year<-c(1981:2015)
> > month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6, 2,
> 1, 1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
> > plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
> > axis(1, at=1981:2015,cex.axis=.8)
> >
> > Best,
> > Ani
> >
> >
> > On Thu, Nov 7, 2019 at 12:15 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >>
> >> Hi Ani,
> >> There are a number of ways to modify this sort of plot. Here is one:
> >>
> >> x11(width=7,height=5)
> >> par(cex.axis=.8)
> >> plot(month~year,xaxt="n", type="l", ylab="Month", xlab="Year",
> >>  main="Month of occurrence in year")
> >> axis(1,at=seq(1981,2014,3))
> >> library(plotrix)
> >> boxed.labels(year,month,month.abb[month],border=NA,cex=0.7)
> >>
> >> Jim
> >>
> >> On Thu, Nov 7, 2019 at 1:24 PM ani jaya <gaaauul at gmail.com> wrote:
> >> >
> >> > Dear R-Help,
> >> >
> >> > I have 35 data that is month when the annual minima happened. So I
> want to
> >> > plot those data but the order of y axis is not from 1 to 12, but let
> say
> >> > start from 9,10,11,12,1,..8. The reason to do this is when 12 (Dec)
> meet 1
> >> > (Jan) in the following year the graph is not quite good (for me).
> >> >
> >> >
> >> > year<-c(1981:2015)
> >> > month<-c(3, 1, 12, 11, 2, 1, 12, 1, 2, 1, 12, 3, 2, 12, 2, 7, 2, 6,
> 2, 1,
> >> > 1, 12, 3, 12, 3, 12, 2, 2, 9, 2, 1, 4, 12, 3, 4)
> >> > plot(month~year,xaxt="n", type="b", ylab="Month", xlab="Year")
> >> > axis(1, at=1986:2015,cex.axis=.8)
> >> >
> >> >
> >> > Any lead or comment is appreciate.
> >> >
> >> > Best, Ani
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From b@un1 @end|ng |rom @tudent@@tow@on@edu  Thu Nov  7 02:27:42 2019
From: b@un1 @end|ng |rom @tudent@@tow@on@edu (Baojun Sun)
Date: Wed, 6 Nov 2019 20:27:42 -0500
Subject: [R] [help]Format excel spreadsheet using R?
Message-ID: <CAPkQrW+gZXEu0arF65eafCeAXpXwGxOk1E6ujK13vuSjmWXcmw@mail.gmail.com>

I have a Excel spreadsheet with two kinds of elements: these include
non-integers and integers. I want to select all numbers with decimal points
i.e. 3.1415 (non-integers). I don't want to highlight cells that are whole
numbers like 314. How do I do this custom selection? My goal is to make all
non-integers fixed to 3 decimal places and keep all whole numbers as they
are.

I've tried conditional formatting.

Is there any way I can do this in a programming language like R's dplyr?


Thanks.

	[[alternative HTML version deleted]]


From he|d@@@d|@ @end|ng |rom gm@||@com  Thu Nov  7 08:46:32 2019
From: he|d@@@d|@ @end|ng |rom gm@||@com (Sadia Seddiqi)
Date: Thu, 7 Nov 2019 12:16:32 +0430
Subject: [R] help about codes from probability
Message-ID: <CADavyCinpxOqRH79cEnQq0XyM+1ZpOy86iQJwQqfA7xzo7ZG1Q@mail.gmail.com>

Could you help me how I can solve run this code in R-labUnions

1-You are picking flowers from a garden. The garden contains flowers of
varying colors, including flowers that are completely blue, flowers that
are completely pink, and also flowers that are both colours. The
probability of picking a flower that is at least partly blue is 0.4 and the
probability of picking a flower that is at least partly pink is 0.2. Let's
think about some probabilities!

# Calculate the probability of picking a flower that is blue, pink or both

2-
Conditional Probability I

Enter plants into your console to see information about whether 90 plants
live indoors or outdoors, and whether they are a flower, a succulent or a
tree. The values in this table represent probabilities, calculated from
frequencies.

Let's use these probability values to find some conditional probabilities.
To do this you need to use the formula P(A|B)=P(A+B)/P(B)P(A|B)=P(A+B)/P(B).

# What is the probability a plant lives indoors?

# What is the probability a plant is a flower, given that we know it lives
indoors?

# What is the probability a plant lives indoors, given that we know it is a
succulent?

-------------------------------------------------------------------------

*Sadia seddiqi*


*Health Care Financing Officer *


*Health Economics & Financing Directorate (HEFD) Ministry of Public Health
(MoPH), Kabul-Afghanistan*

* Email: hefd.sadia at gmail.com <hefd.sadia at gmail.com>
<+93%2078%20692%207270>*

	[[alternative HTML version deleted]]


From terry@therne@u @end|ng |rom gm@||@com  Wed Nov  6 13:15:43 2019
From: terry@therne@u @end|ng |rom gm@||@com (Terry Therneau)
Date: Wed, 6 Nov 2019 06:15:43 -0600
Subject: [R] [R-pkgs] survival3.1
Message-ID: <CAGzH7UfN-qWE1N0=Mm1i-uVCEc24_dyL0qi70UMkjcHxFTGt=A@mail.gmail.com>

Version 3.x of the survival package is now available from CRAN.  This is a
major update whose primary goal is to make the analysis of multi-state
survival models as easy to do as simple Kaplan-Meier and Cox proportional
hazard fits. The primary changes are

    -- in Surv(time1, time2, status) the status variable can now be a
factor that specifies which state a subject has transitioned into.  From a
user's point of view this is the single key change.

    -- survfit() will then create multi-state Pr(state | time) curves,
i.e., the Aalen-Johansen estimate.  (Kaplan-Meier and cumulative incidence
are special cases of the AJ).  Printout and plotting are unchanged.

    -- coxph() will then create multi-state fits.  There is an easy
formula-like syntax for specifying shared coefficients or baseline hazards.

    -- all use standard data, i.e. the (time1, time2, status) format that
is common for time-dependent covariates; there is no need to create special
intermediate data sets.  (An id variable is necessary to specify which rows
go with whom.)

    -- robust variance estimates available throughout

Many other CRAN packages depend on survival, which drove a second goal of
"don't break it".  Part of the testing involved running the test suits of
all 679 reverse dependencies under the new version.

Terry Therneau
therneau at mayo.edu

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From cry@n @end|ng |rom b|ngh@mton@edu  Thu Nov  7 15:44:48 2019
From: cry@n @end|ng |rom b|ngh@mton@edu (Christopher W. Ryan)
Date: Thu, 07 Nov 2019 09:44:48 -0500
Subject: [R] help about codes from probability
In-Reply-To: <CADavyCinpxOqRH79cEnQq0XyM+1ZpOy86iQJwQqfA7xzo7ZG1Q@mail.gmail.com>
References: <CADavyCinpxOqRH79cEnQq0XyM+1ZpOy86iQJwQqfA7xzo7ZG1Q@mail.gmail.com>
Message-ID: <47A57D94-D824-4737-9A49-E18FB38C7B9C@binghamton.edu>

Is this homework for a class? If so, it would be better to ask your professor for guidance. R-help List frowns on homework questions.

Chris Ryan


On November 7, 2019 2:46:32 AM EST, Sadia Seddiqi <hefd.sadia at gmail.com> wrote:
>Could you help me how I can solve run this code in R-labUnions
>
>1-You are picking flowers from a garden. The garden contains flowers of
>varying colors, including flowers that are completely blue, flowers
>that
>are completely pink, and also flowers that are both colours. The
>probability of picking a flower that is at least partly blue is 0.4 and
>the
>probability of picking a flower that is at least partly pink is 0.2.
>Let's
>think about some probabilities!
>
># Calculate the probability of picking a flower that is blue, pink or
>both
>
>2-
>Conditional Probability I
>
>Enter plants into your console to see information about whether 90
>plants
>live indoors or outdoors, and whether they are a flower, a succulent or
>a
>tree. The values in this table represent probabilities, calculated from
>frequencies.
>
>Let's use these probability values to find some conditional
>probabilities.
>To do this you need to use the formula
>P(A|B)=P(A+B)/P(B)P(A|B)=P(A+B)/P(B).
>
># What is the probability a plant lives indoors?
>
># What is the probability a plant is a flower, given that we know it
>lives
>indoors?
>
># What is the probability a plant lives indoors, given that we know it
>is a
>succulent?
>
>-------------------------------------------------------------------------
>
>*Sadia seddiqi*
>
>
>*Health Care Financing Officer *
>
>
>*Health Economics & Financing Directorate (HEFD) Ministry of Public
>Health
>(MoPH), Kabul-Afghanistan*
>
>* Email: hefd.sadia at gmail.com <hefd.sadia at gmail.com>
><+93%2078%20692%207270>*
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.
	[[alternative HTML version deleted]]


From m@k@hho||y @end|ng |rom gm@||@com  Thu Nov  7 16:48:05 2019
From: m@k@hho||y @end|ng |rom gm@||@com (greg holly)
Date: Thu, 7 Nov 2019 09:48:05 -0600
Subject: [R] problem in WRS2
In-Reply-To: <4EAF168E-16AB-4EE1-B223-01785E09052E@comcast.net>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
 <CA+8X3fWY+D7y_ismHX56g_QOL9ZMXD8JuW_Y+u1xozrcHB2ZOw@mail.gmail.com>
 <4EAF168E-16AB-4EE1-B223-01785E09052E@comcast.net>
Message-ID: <CAM9Qe4jXeTtcPkRH1BvxtqoDLThz4FVN0a-B-0ejrvfZ2PmMyQ@mail.gmail.com>

Hi David, Jim and Bert;

Thanks so much. Your responses are much appreciated. Here is the results
when I create a cross-tabulation from xtab(Grup~Time,cp) for the whole
data. It seems to me there is no problem. I am wondering why still I
have the
 Error in x[[grp[i]]] :
  attempt to select less than one element in get1index

ERROR when I am running the t2way(y ~ Grup*Time, data = cp)  using WRS2
package. Yours advice are highly appreciated.

Kind regards,
Greg


        Time
Grup     Cp0      Cp1      Cp14     Cp21     Cp28     Cp3      Cp7
1             25      25      25      25      25      25      25     175
           14.29   14.29   14.29   14.29   14.29   14.29   14.29       -
            62.5    62.5    62.5    62.5    62.5    62.5    62.5    62.5

2             15      15      15      15      15      15      15     105
           14.29   14.29   14.29   14.29   14.29   14.29   14.29       -
            37.5    37.5    37.5    37.5    37.5    37.5    37.5    37.5

              40      40      40      40      40      40      40     280
           14.29   14.29   14.29   14.29   14.29   14.29   14.29  100.00





On Wed, Nov 6, 2019 at 7:46 PM David Winsemius <dwinsemius at comcast.net>
wrote:

> If a crosstabs on the two factors has any zero counts it might explain.
>
> ?
> David
>
> Sent from my iPhone
>
> > On Nov 6, 2019, at 5:42 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > Hi Greg,
> > I tried this:
> >
> > cp<-read.table(text="Birey Grup Time y
> > 1    1  Cp1 0.7916386
> > 1    1  Cp3 1.7463777
> > 1    1  Cp7 1.2008390
> > 1    1 Cp14 0.6311380
> > 1    1 Cp21 2.1563557
> > 1    1 Cp28 1.2008390",
> > header=TRUE)
> > library(prettyR)
> > xtab(Grup~Time,cp)
> > Crosstabulation of Grup by Time
> >       Time
> > Grup     Cp1      Cp14     Cp21     Cp28     Cp3      Cp7
> > 1              1       1       1       1       1       1       6
> >          16.67   16.67   16.67   16.67   16.67   16.67       -
> >            100     100     100     100     100     100     100
> >
> >              1       1       1       1       1       1       6
> >          16.67   16.67   16.67   16.67   16.67   16.67  100.00
> >
> > Maybe doing the same on your entire data set will reveal something.
> >
> > Jim
> >
> >> On Thu, Nov 7, 2019 at 8:58 AM greg holly <mak.hholly at gmail.com> wrote:
> >>
> >> I got the following error message after running  t2way(y ~ Grup*Time,
> data
> >> = cp)
> >>  Error in x[[grp[i]]] :
> >>  attempt to select less than one element in get1index
> >>
> >> a part of the data is given below. Your help is highly appreciated.
> >>
> >> Greg
> >>
> >>> head(cp)
> >>  Birey Grup Time         y
> >> 1     1    1  Cp1 0.7916386
> >> 2     1    1  Cp3 1.7463777
> >> 3     1    1  Cp7 1.2008390
> >> 4     1    1 Cp14 0.6311380
> >> 5     1    1 Cp21 2.1563557
> >> 6     1    1 Cp28 1.2008390
> >>
> >>        [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Nov  7 18:21:40 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 7 Nov 2019 09:21:40 -0800
Subject: [R] problem in WRS2
In-Reply-To: <CAM9Qe4jXeTtcPkRH1BvxtqoDLThz4FVN0a-B-0ejrvfZ2PmMyQ@mail.gmail.com>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
 <CA+8X3fWY+D7y_ismHX56g_QOL9ZMXD8JuW_Y+u1xozrcHB2ZOw@mail.gmail.com>
 <4EAF168E-16AB-4EE1-B223-01785E09052E@comcast.net>
 <CAM9Qe4jXeTtcPkRH1BvxtqoDLThz4FVN0a-B-0ejrvfZ2PmMyQ@mail.gmail.com>
Message-ID: <CAGxFJbQvAW1QpfCZfi5c1EQbxZZ_b5M=V63iA7svzBmjLZZfZQ@mail.gmail.com>

As I said previously, you probably should contact the maintainer. But a
(wild??) guess might be that robust/resistant procedures -- which is what
you are using -- can downweight data values to 0 weight, effectively
removing them.This might lead to effectively empty cells in a cross
tabulation that would then yield your error, despite the crosstab on raw
data showing all cells with data.

Again, note my weasel words ("might","could", etc.), so the above could be
complete nonsense. Which is why I suggested you contact the maintainer.

-- Bert

On Thu, Nov 7, 2019 at 7:50 AM greg holly <mak.hholly at gmail.com> wrote:

> Hi David, Jim and Bert;
>
> Thanks so much. Your responses are much appreciated. Here is the results
> when I create a cross-tabulation from xtab(Grup~Time,cp) for the whole
> data. It seems to me there is no problem. I am wondering why still I
> have the
>  Error in x[[grp[i]]] :
>   attempt to select less than one element in get1index
>
> ERROR when I am running the t2way(y ~ Grup*Time, data = cp)  using WRS2
> package. Yours advice are highly appreciated.
>
> Kind regards,
> Greg
>
>
>         Time
> Grup     Cp0      Cp1      Cp14     Cp21     Cp28     Cp3      Cp7
> 1             25      25      25      25      25      25      25     175
>            14.29   14.29   14.29   14.29   14.29   14.29   14.29       -
>             62.5    62.5    62.5    62.5    62.5    62.5    62.5    62.5
>
> 2             15      15      15      15      15      15      15     105
>            14.29   14.29   14.29   14.29   14.29   14.29   14.29       -
>             37.5    37.5    37.5    37.5    37.5    37.5    37.5    37.5
>
>               40      40      40      40      40      40      40     280
>            14.29   14.29   14.29   14.29   14.29   14.29   14.29  100.00
>
>
>
>
>
> On Wed, Nov 6, 2019 at 7:46 PM David Winsemius <dwinsemius at comcast.net>
> wrote:
>
> > If a crosstabs on the two factors has any zero counts it might explain.
> >
> > ?
> > David
> >
> > Sent from my iPhone
> >
> > > On Nov 6, 2019, at 5:42 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> > >
> > > Hi Greg,
> > > I tried this:
> > >
> > > cp<-read.table(text="Birey Grup Time y
> > > 1    1  Cp1 0.7916386
> > > 1    1  Cp3 1.7463777
> > > 1    1  Cp7 1.2008390
> > > 1    1 Cp14 0.6311380
> > > 1    1 Cp21 2.1563557
> > > 1    1 Cp28 1.2008390",
> > > header=TRUE)
> > > library(prettyR)
> > > xtab(Grup~Time,cp)
> > > Crosstabulation of Grup by Time
> > >       Time
> > > Grup     Cp1      Cp14     Cp21     Cp28     Cp3      Cp7
> > > 1              1       1       1       1       1       1       6
> > >          16.67   16.67   16.67   16.67   16.67   16.67       -
> > >            100     100     100     100     100     100     100
> > >
> > >              1       1       1       1       1       1       6
> > >          16.67   16.67   16.67   16.67   16.67   16.67  100.00
> > >
> > > Maybe doing the same on your entire data set will reveal something.
> > >
> > > Jim
> > >
> > >> On Thu, Nov 7, 2019 at 8:58 AM greg holly <mak.hholly at gmail.com>
> wrote:
> > >>
> > >> I got the following error message after running  t2way(y ~ Grup*Time,
> > data
> > >> = cp)
> > >>  Error in x[[grp[i]]] :
> > >>  attempt to select less than one element in get1index
> > >>
> > >> a part of the data is given below. Your help is highly appreciated.
> > >>
> > >> Greg
> > >>
> > >>> head(cp)
> > >>  Birey Grup Time         y
> > >> 1     1    1  Cp1 0.7916386
> > >> 2     1    1  Cp3 1.7463777
> > >> 3     1    1  Cp7 1.2008390
> > >> 4     1    1 Cp14 0.6311380
> > >> 5     1    1 Cp21 2.1563557
> > >> 6     1    1 Cp28 1.2008390
> > >>
> > >>        [[alternative HTML version deleted]]
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdun|@p @end|ng |rom t|bco@com  Thu Nov  7 19:02:22 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 7 Nov 2019 10:02:22 -0800
Subject: [R] problem in WRS2
In-Reply-To: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
Message-ID: <CAF8bMca=-9Cm6MOhp0wTKmHA1sqbVm381fx+qt4EP_4AAzwOWg@mail.gmail.com>

You can get this error if one of the explanatory variables is not a
factor.  E.g.
> WRS2::t2way(y ~ x1 * x2, data =
expand.grid(y=11:12,x1=letters[11:13],x2=21:24))
Error in x[[grp[i]]] :
  attempt to select less than one element in get1index

The immediate cause is that t2way uses 1:p instead of seq_len(p) when p may
be zero, but I suspect that the code should be converting some things to
factors so p cannot be zero.  Take this up with the package's maintainer.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Nov 6, 2019 at 1:58 PM greg holly <mak.hholly at gmail.com> wrote:

> I got the following error message after running  t2way(y ~ Grup*Time, data
> = cp)
>   Error in x[[grp[i]]] :
>   attempt to select less than one element in get1index
>
> a part of the data is given below. Your help is highly appreciated.
>
> Greg
>
> > head(cp)
>   Birey Grup Time         y
> 1     1    1  Cp1 0.7916386
> 2     1    1  Cp3 1.7463777
> 3     1    1  Cp7 1.2008390
> 4     1    1 Cp14 0.6311380
> 5     1    1 Cp21 2.1563557
> 6     1    1 Cp28 1.2008390
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From m@k@hho||y @end|ng |rom gm@||@com  Thu Nov  7 21:23:37 2019
From: m@k@hho||y @end|ng |rom gm@||@com (greg holly)
Date: Thu, 7 Nov 2019 14:23:37 -0600
Subject: [R] problem in WRS2
In-Reply-To: <CAF8bMca=-9Cm6MOhp0wTKmHA1sqbVm381fx+qt4EP_4AAzwOWg@mail.gmail.com>
References: <CAM9Qe4j2aFzU2K3Zni29bTNDeiwLsQpxo7djGSxMY_NCemMKWQ@mail.gmail.com>
 <CAF8bMca=-9Cm6MOhp0wTKmHA1sqbVm381fx+qt4EP_4AAzwOWg@mail.gmail.com>
Message-ID: <CAM9Qe4haNWmrkpf1DRpBbBaGevuNw3LyKie6=-e_WSgH7WtmeA@mail.gmail.com>

Thanks, William. After converting Grup into the factor, I got the results.
Thanks, everyone to spend time to give their help.

Regards,
Greg

On Thu, Nov 7, 2019 at 12:02 PM William Dunlap <wdunlap at tibco.com> wrote:

> You can get this error if one of the explanatory variables is not a
> factor.  E.g.
> > WRS2::t2way(y ~ x1 * x2, data =
> expand.grid(y=11:12,x1=letters[11:13],x2=21:24))
> Error in x[[grp[i]]] :
>   attempt to select less than one element in get1index
>
> The immediate cause is that t2way uses 1:p instead of seq_len(p) when p
> may be zero, but I suspect that the code should be converting some things
> to factors so p cannot be zero.  Take this up with the package's maintainer.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, Nov 6, 2019 at 1:58 PM greg holly <mak.hholly at gmail.com> wrote:
>
>> I got the following error message after running  t2way(y ~ Grup*Time, data
>> = cp)
>>   Error in x[[grp[i]]] :
>>   attempt to select less than one element in get1index
>>
>> a part of the data is given below. Your help is highly appreciated.
>>
>> Greg
>>
>> > head(cp)
>>   Birey Grup Time         y
>> 1     1    1  Cp1 0.7916386
>> 2     1    1  Cp3 1.7463777
>> 3     1    1  Cp7 1.2008390
>> 4     1    1 Cp14 0.6311380
>> 5     1    1 Cp21 2.1563557
>> 6     1    1 Cp28 1.2008390
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Fri Nov  8 00:24:16 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 8 Nov 2019 10:24:16 +1100
Subject: [R] [help]Format excel spreadsheet using R?
In-Reply-To: <CAPkQrW+gZXEu0arF65eafCeAXpXwGxOk1E6ujK13vuSjmWXcmw@mail.gmail.com>
References: <CAPkQrW+gZXEu0arF65eafCeAXpXwGxOk1E6ujK13vuSjmWXcmw@mail.gmail.com>
Message-ID: <CA+8X3fW_=937xQPZY0CDQO6+S2nr3TochFHeopXmZ8mYgCgVCA@mail.gmail.com>

Hi Baojun,
You probably need the right condition:

x<-c(1,2,3.1415926535,4)
> nonint<-floor(x)!=x
> nonint
[1] FALSE FALSE  TRUE FALSE
> x[nonint]<-round(x[nonint],3)
> x
[1] 1.000 2.000 3.142 4.000

Jim

On Fri, Nov 8, 2019 at 12:54 AM Baojun Sun <bsun1 at students.towson.edu> wrote:
>
> I have a Excel spreadsheet with two kinds of elements: these include
> non-integers and integers. I want to select all numbers with decimal points
> i.e. 3.1415 (non-integers). I don't want to highlight cells that are whole
> numbers like 314. How do I do this custom selection? My goal is to make all
> non-integers fixed to 3 decimal places and keep all whole numbers as they
> are.
>
> I've tried conditional formatting.
>
> Is there any way I can do this in a programming language like R's dplyr?
>
>
> Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From w@g@t@||@j@me@ @end|ng |rom gm@||@com  Fri Nov  8 14:20:41 2019
From: w@g@t@||@j@me@ @end|ng |rom gm@||@com (James Wagstaff)
Date: Fri, 8 Nov 2019 13:20:41 +0000
Subject: [R] 
 Global curve fitting/shared parameters with nls() alternatives
In-Reply-To: <CAGxFJbQwWmv_rw9TdziNBqFb9bqArskp11PEWpiEZhShmWPo0A@mail.gmail.com>
References: <CAKCTGt+MPECxaguw2beeZhp3SVF=PaA63FBFkGq8f1s2bOgTeg@mail.gmail.com>
 <CAGxFJbQwWmv_rw9TdziNBqFb9bqArskp11PEWpiEZhShmWPo0A@mail.gmail.com>
Message-ID: <CAKCTGtKNjGVATsWT3gXQ6L6qNKy=8JOQRGU2SPkNs0ZMw8EZgA@mail.gmail.com>

Dear Bert
Thanks for getting back to me. Yes that is exactly the sort of problem I am
trying to solve. I am aware of the option of hard coding the experimental
groups as you suggested, but was hoping for an easy out of the box approach
as I have many groups!
Thanks
James

On Tue, 5 Nov 2019 at 20:28, Bert Gunter <bgunter.4567 at gmail.com> wrote:

> A simplified example of what you wish to do might help to clarify here.
>
> Here's my guess. Feel free to dismiss if I'm off base.
>
> Suppose your model is:
> y = exp(a*x) + b
>
> and you wish the b to be constant but the a to vary across expts. Then can
> you not combine the data from both into single x, y vectors, add a variable
> expt that takes the value 1 for expt1 and 2 for expt 2 and fit the single
> model:
>
> y = (expt ==1)*(exp(a1*x) + b)   +  (expt == 2)* (exp(a2*x) + b)
>
> This would obtain separate estimates of a1 and a2 but a single estimate of
> b .
>
> There are probably better ways to do this, but I've done hardly any
> nonlinear model fitting (so warning!) and can only offer this brute force
> approach; so wait for someone to suggest something better before trying it.
>
> Cheers,
> Bert
>
>
> On Tue, Nov 5, 2019 at 9:12 AM James Wagstaff <wagstaff.james at gmail.com>
> wrote:
>
>> Hello
>> I am trying to determine least-squares estimates of the parameters of a
>> nonlinear model, where I expect some parameters to remain constant across
>> experiments, and for others to vary. I believe this is typically referred
>> to as global curve fitting, or the presence of shared/nested parameters.
>> The "[]" syntax in the stats::nls() function is an extremely convenient
>> solution (
>>
>> https://r.789695.n4.nabble.com/How-to-do-global-curve-fitting-in-R-td4712052.html
>> ),
>> but in my case I seem to need the Levenberg-Marquardt/Marquardt solvers
>> such as nlsr::nlxb() and minpack.lm::nlsLM. I can not find any
>> examples/documentation explaining a similar syntax for these tools. Is
>> anyone aware of a nls-like tool with this functionality, or an alternative
>> approach?
>> Best wishes
>> James Wagstaff
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

-- 
James Wagstaff

+447910113349

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Nov  8 15:09:12 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 Nov 2019 15:09:12 +0100
Subject: [R] 
 Global curve fitting/shared parameters with nls() alternatives
In-Reply-To: <CAKCTGtKNjGVATsWT3gXQ6L6qNKy=8JOQRGU2SPkNs0ZMw8EZgA@mail.gmail.com>
References: <CAKCTGt+MPECxaguw2beeZhp3SVF=PaA63FBFkGq8f1s2bOgTeg@mail.gmail.com>
 <CAGxFJbQwWmv_rw9TdziNBqFb9bqArskp11PEWpiEZhShmWPo0A@mail.gmail.com>
 <CAKCTGtKNjGVATsWT3gXQ6L6qNKy=8JOQRGU2SPkNs0ZMw8EZgA@mail.gmail.com>
Message-ID: <24005.30472.922392.694774@stat.math.ethz.ch>

>>>>> James Wagstaff 
>>>>>     on Fri, 8 Nov 2019 13:20:41 +0000 writes:

    > Dear Bert Thanks for getting back to me. Yes that is
    > exactly the sort of problem I am trying to solve. I am
    > aware of the option of hard coding the experimental groups
    > as you suggested, but was hoping for an easy out of the
    > box approach as I have many groups!  Thanks James

If I understand correctly,

nlme :: nlsList()  is exactly what you want.

No need to install anything, as 'nlme' is among the formally
'Recommended' packages and hence is part of every
(non-handicapped) R  installation.

Best,
Martin Maechler
ETH Zurich and R Core Team



    > On Tue, 5 Nov 2019 at 20:28, Bert Gunter
    > <bgunter.4567 at gmail.com> wrote:

    >> A simplified example of what you wish to do might help to
    >> clarify here.
    >> 
    >> Here's my guess. Feel free to dismiss if I'm off base.
    >> 
    >> Suppose your model is: y = exp(a*x) + b
    >> 
    >> and you wish the b to be constant but the a to vary
    >> across expts. Then can you not combine the data from both
    >> into single x, y vectors, add a variable expt that takes
    >> the value 1 for expt1 and 2 for expt 2 and fit the single
    >> model:
    >> 
    >> y = (expt ==1)*(exp(a1*x) + b) + (expt == 2)* (exp(a2*x)
    >> + b)
    >> 
    >> This would obtain separate estimates of a1 and a2 but a
    >> single estimate of b .
    >> 
    >> There are probably better ways to do this, but I've done
    >> hardly any nonlinear model fitting (so warning!) and can
    >> only offer this brute force approach; so wait for someone
    >> to suggest something better before trying it.
    >> 
    >> Cheers, Bert
    >> 
    >> 
    >> On Tue, Nov 5, 2019 at 9:12 AM James Wagstaff
    >> <wagstaff.james at gmail.com> wrote:
    >> 
    >>> Hello I am trying to determine least-squares estimates
    >>> of the parameters of a nonlinear model, where I expect
    >>> some parameters to remain constant across experiments,
    >>> and for others to vary. I believe this is typically
    >>> referred to as global curve fitting, or the presence of
    >>> shared/nested parameters.  The "[]" syntax in the
    >>> stats::nls() function is an extremely convenient
    >>> solution (
    >>> 
    >>> https://r.789695.n4.nabble.com/How-to-do-global-curve-fitting-in-R-td4712052.html
    >>> ), but in my case I seem to need the
    >>> Levenberg-Marquardt/Marquardt solvers such as
    >>> nlsr::nlxb() and minpack.lm::nlsLM. I can not find any
    >>> examples/documentation explaining a similar syntax for
    >>> these tools. Is anyone aware of a nls-like tool with
    >>> this functionality, or an alternative approach?  Best
    >>> wishes James Wagstaff
    >>> 
    >>> [[alternative HTML version deleted]]
    >>> 
    >>> ______________________________________________
    >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    >>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
    >>> PLEASE do read the posting guide
    >>> http://www.R-project.org/posting-guide.html and provide
    >>> commented, minimal, self-contained, reproducible code.
    >>> 
    >> 

    > -- 
    > James Wagstaff

    > +447910113349

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-help at r-project.org mailing list -- To UNSUBSCRIBE and
    > more, see https://stat.ethz.ch/mailman/listinfo/r-help
    > PLEASE do read the posting guide
    > http://www.R-project.org/posting-guide.html and provide
    > commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov  8 15:38:56 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 8 Nov 2019 08:38:56 -0600
Subject: [R] how to find number of unique rows for combination of r columns
Message-ID: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>

Hello,

I have a data frame like this:

> head(dt,20)
     chr    pos         gene_id pval_nominal  pval_ret       wl      wr
 1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
 2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
 3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
 4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
 5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
 6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
 7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
 8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
 9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838

it is very big,
> dim(dt)
[1] 73719122        8

To count number of unique rows for all 3 columns: chr, pos and gene_id
I could just join those 3 columns and than count. But how would I find
unique number of rows for these 4 columns without joining them?

Thanks
Ana


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Fri Nov  8 15:50:59 2019
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Fri, 8 Nov 2019 15:50:59 +0100
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
Message-ID: <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>

Hi, Ana,

doesn't

udt <- unique(dt[c("chr", "pos", "gene_id")])
nrow(udt)

get close to what you want?

  Hth  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

Am 08.11.2019 um 15:38 schrieb Ana Marija:
> Hello,
> 
> I have a data frame like this:
> 
>> head(dt,20)
>       chr    pos         gene_id pval_nominal  pval_ret       wl      wr
>   1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
>   2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
>   3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
>   4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
>   5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
>   6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
>   7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
>   8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
>   9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
> 
> it is very big,
>> dim(dt)
> [1] 73719122        8
> 
> To count number of unique rows for all 3 columns: chr, pos and gene_id
> I could just join those 3 columns and than count. But how would I find
> unique number of rows for these 4 columns without joining them?
> 
> Thanks
> Ana
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov  8 16:02:35 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 8 Nov 2019 09:02:35 -0600
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
Message-ID: <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>

I tried it but I got this error:
> udt <- unique(dt[c("chr", "pos", "gene_id")])
Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
  When i is a data.table (or character vector), the columns to join by
must be specified using 'on=' argument (see ?data.table), by keying x
(i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
column names between x and i (i.e., a natural join). Keyed joins might
have further speed benefits on very large data due to x being sorted
in RAM.

On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
<gerrit.eichner at math.uni-giessen.de> wrote:
>
> Hi, Ana,
>
> doesn't
>
> udt <- unique(dt[c("chr", "pos", "gene_id")])
> nrow(udt)
>
> get close to what you want?
>
>   Hth  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> http://www.uni-giessen.de/eichner
> ---------------------------------------------------------------------
>
> Am 08.11.2019 um 15:38 schrieb Ana Marija:
> > Hello,
> >
> > I have a data frame like this:
> >
> >> head(dt,20)
> >       chr    pos         gene_id pval_nominal  pval_ret       wl      wr
> >   1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
> >   2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
> >   3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
> >   4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
> >   5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
> >   6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
> >   7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
> >   8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
> >   9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
> > 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
> >
> > it is very big,
> >> dim(dt)
> > [1] 73719122        8
> >
> > To count number of unique rows for all 3 columns: chr, pos and gene_id
> > I could just join those 3 columns and than count. But how would I find
> > unique number of rows for these 4 columns without joining them?
> >
> > Thanks
> > Ana
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de  Fri Nov  8 16:19:41 2019
From: gerr|t@e|chner @end|ng |rom m@th@un|-g|e@@en@de (Gerrit Eichner)
Date: Fri, 8 Nov 2019 16:19:41 +0100
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
Message-ID: <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>

It seems as if dt is not a (base R) data frame but a
data table. I assume, you will have to transform dt
into a data frame (maybe with as.data.frame) to be
able to apply unique in the suggested way. However,
I am not familiar with data tables. Perhaps somebody
else can provide a more profound guess.

  Regards  --  Gerrit

---------------------------------------------------------------------
Dr. Gerrit Eichner                   Mathematical Institute, Room 212
gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
http://www.uni-giessen.de/eichner
---------------------------------------------------------------------

Am 08.11.2019 um 16:02 schrieb Ana Marija:
> I tried it but I got this error:
>> udt <- unique(dt[c("chr", "pos", "gene_id")])
> Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
>    When i is a data.table (or character vector), the columns to join by
> must be specified using 'on=' argument (see ?data.table), by keying x
> (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
> column names between x and i (i.e., a natural join). Keyed joins might
> have further speed benefits on very large data due to x being sorted
> in RAM.
> 
> On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
> <gerrit.eichner at math.uni-giessen.de> wrote:
>>
>> Hi, Ana,
>>
>> doesn't
>>
>> udt <- unique(dt[c("chr", "pos", "gene_id")])
>> nrow(udt)
>>
>> get close to what you want?
>>
>>    Hth  --  Gerrit
>>
>> ---------------------------------------------------------------------
>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>> http://www.uni-giessen.de/eichner
>> ---------------------------------------------------------------------
>>
>> Am 08.11.2019 um 15:38 schrieb Ana Marija:
>>> Hello,
>>>
>>> I have a data frame like this:
>>>
>>>> head(dt,20)
>>>        chr    pos         gene_id pval_nominal  pval_ret       wl      wr
>>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
>>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
>>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
>>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
>>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
>>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
>>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
>>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
>>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
>>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
>>>
>>> it is very big,
>>>> dim(dt)
>>> [1] 73719122        8
>>>
>>> To count number of unique rows for all 3 columns: chr, pos and gene_id
>>> I could just join those 3 columns and than count. But how would I find
>>> unique number of rows for these 4 columns without joining them?
>>>
>>> Thanks
>>> Ana
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov  8 16:30:57 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 8 Nov 2019 09:30:57 -0600
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
Message-ID: <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>

Thank you so much! Converting it to data frame resolved the issue!

On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
<gerrit.eichner at math.uni-giessen.de> wrote:
>
> It seems as if dt is not a (base R) data frame but a
> data table. I assume, you will have to transform dt
> into a data frame (maybe with as.data.frame) to be
> able to apply unique in the suggested way. However,
> I am not familiar with data tables. Perhaps somebody
> else can provide a more profound guess.
>
>   Regards  --  Gerrit
>
> ---------------------------------------------------------------------
> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> http://www.uni-giessen.de/eichner
> ---------------------------------------------------------------------
>
> Am 08.11.2019 um 16:02 schrieb Ana Marija:
> > I tried it but I got this error:
> >> udt <- unique(dt[c("chr", "pos", "gene_id")])
> > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
> >    When i is a data.table (or character vector), the columns to join by
> > must be specified using 'on=' argument (see ?data.table), by keying x
> > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
> > column names between x and i (i.e., a natural join). Keyed joins might
> > have further speed benefits on very large data due to x being sorted
> > in RAM.
> >
> > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
> > <gerrit.eichner at math.uni-giessen.de> wrote:
> >>
> >> Hi, Ana,
> >>
> >> doesn't
> >>
> >> udt <- unique(dt[c("chr", "pos", "gene_id")])
> >> nrow(udt)
> >>
> >> get close to what you want?
> >>
> >>    Hth  --  Gerrit
> >>
> >> ---------------------------------------------------------------------
> >> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >> http://www.uni-giessen.de/eichner
> >> ---------------------------------------------------------------------
> >>
> >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
> >>> Hello,
> >>>
> >>> I have a data frame like this:
> >>>
> >>>> head(dt,20)
> >>>        chr    pos         gene_id pval_nominal  pval_ret       wl      wr
> >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
> >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
> >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
> >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
> >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
> >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
> >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
> >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
> >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
> >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
> >>>
> >>> it is very big,
> >>>> dim(dt)
> >>> [1] 73719122        8
> >>>
> >>> To count number of unique rows for all 3 columns: chr, pos and gene_id
> >>> I could just join those 3 columns and than count. But how would I find
> >>> unique number of rows for these 4 columns without joining them?
> >>>
> >>> Thanks
> >>> Ana
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov  8 16:32:46 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 8 Nov 2019 09:32:46 -0600
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
Message-ID: <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>

would you know how would I extract from my original data frame, just
these unique rows?
because this gives me only those 3 columns, and I want all columns
from the original data frame

> head(udt)
   chr   pos         gene_id
1 chr1 54490 ENSG00000227232
2 chr1 58814 ENSG00000227232
3 chr1 60351 ENSG00000227232
4 chr1 61920 ENSG00000227232
5 chr1 63671 ENSG00000227232
6 chr1 64931 ENSG00000227232

> head(dt)
    chr   pos         gene_id pval_nominal pval_ret       wl      wr      META
1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838 0.7475480
2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838 0.6031214
3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838 0.6907182
4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838 0.4032200
5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838 0.7482519
6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838 0.5974800

On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Thank you so much! Converting it to data frame resolved the issue!
>
> On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
> <gerrit.eichner at math.uni-giessen.de> wrote:
> >
> > It seems as if dt is not a (base R) data frame but a
> > data table. I assume, you will have to transform dt
> > into a data frame (maybe with as.data.frame) to be
> > able to apply unique in the suggested way. However,
> > I am not familiar with data tables. Perhaps somebody
> > else can provide a more profound guess.
> >
> >   Regards  --  Gerrit
> >
> > ---------------------------------------------------------------------
> > Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> > gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> > Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> > http://www.uni-giessen.de/eichner
> > ---------------------------------------------------------------------
> >
> > Am 08.11.2019 um 16:02 schrieb Ana Marija:
> > > I tried it but I got this error:
> > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
> > > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
> > >    When i is a data.table (or character vector), the columns to join by
> > > must be specified using 'on=' argument (see ?data.table), by keying x
> > > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
> > > column names between x and i (i.e., a natural join). Keyed joins might
> > > have further speed benefits on very large data due to x being sorted
> > > in RAM.
> > >
> > > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
> > > <gerrit.eichner at math.uni-giessen.de> wrote:
> > >>
> > >> Hi, Ana,
> > >>
> > >> doesn't
> > >>
> > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
> > >> nrow(udt)
> > >>
> > >> get close to what you want?
> > >>
> > >>    Hth  --  Gerrit
> > >>
> > >> ---------------------------------------------------------------------
> > >> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> > >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> > >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> > >> http://www.uni-giessen.de/eichner
> > >> ---------------------------------------------------------------------
> > >>
> > >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
> > >>> Hello,
> > >>>
> > >>> I have a data frame like this:
> > >>>
> > >>>> head(dt,20)
> > >>>        chr    pos         gene_id pval_nominal  pval_ret       wl      wr
> > >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
> > >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
> > >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
> > >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
> > >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
> > >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
> > >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
> > >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
> > >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
> > >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
> > >>>
> > >>> it is very big,
> > >>>> dim(dt)
> > >>> [1] 73719122        8
> > >>>
> > >>> To count number of unique rows for all 3 columns: chr, pos and gene_id
> > >>> I could just join those 3 columns and than count. But how would I find
> > >>> unique number of rows for these 4 columns without joining them?
> > >>>
> > >>> Thanks
> > >>> Ana
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>>
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.


From bor|@@@te|pe @end|ng |rom utoronto@c@  Fri Nov  8 16:49:48 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Fri, 8 Nov 2019 15:49:48 +0000
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
Message-ID: <5D6AEC54-04FA-4032-8C76-B0FEBB890AF6@utoronto.ca>

Are you trying to eliminate duplicated rows from your dataframe? Because that would be better achieved with duplicated().


B.




> On 2019-11-08, at 10:32, Ana Marija <sokovic.anamarija at gmail.com> wrote:
> 
> would you know how would I extract from my original data frame, just
> these unique rows?
> because this gives me only those 3 columns, and I want all columns
> from the original data frame
> 
>> head(udt)
>   chr   pos         gene_id
> 1 chr1 54490 ENSG00000227232
> 2 chr1 58814 ENSG00000227232
> 3 chr1 60351 ENSG00000227232
> 4 chr1 61920 ENSG00000227232
> 5 chr1 63671 ENSG00000227232
> 6 chr1 64931 ENSG00000227232
> 
>> head(dt)
>    chr   pos         gene_id pval_nominal pval_ret       wl      wr      META
> 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838 0.7475480
> 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838 0.6031214
> 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838 0.6907182
> 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838 0.4032200
> 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838 0.7482519
> 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838 0.5974800
> 
> On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>> 
>> Thank you so much! Converting it to data frame resolved the issue!
>> 
>> On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
>> <gerrit.eichner at math.uni-giessen.de> wrote:
>>> 
>>> It seems as if dt is not a (base R) data frame but a
>>> data table. I assume, you will have to transform dt
>>> into a data frame (maybe with as.data.frame) to be
>>> able to apply unique in the suggested way. However,
>>> I am not familiar with data tables. Perhaps somebody
>>> else can provide a more profound guess.
>>> 
>>>  Regards  --  Gerrit
>>> 
>>> ---------------------------------------------------------------------
>>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>>> http://www.uni-giessen.de/eichner
>>> ---------------------------------------------------------------------
>>> 
>>> Am 08.11.2019 um 16:02 schrieb Ana Marija:
>>>> I tried it but I got this error:
>>>>> udt <- unique(dt[c("chr", "pos", "gene_id")])
>>>> Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
>>>>   When i is a data.table (or character vector), the columns to join by
>>>> must be specified using 'on=' argument (see ?data.table), by keying x
>>>> (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
>>>> column names between x and i (i.e., a natural join). Keyed joins might
>>>> have further speed benefits on very large data due to x being sorted
>>>> in RAM.
>>>> 
>>>> On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
>>>> <gerrit.eichner at math.uni-giessen.de> wrote:
>>>>> 
>>>>> Hi, Ana,
>>>>> 
>>>>> doesn't
>>>>> 
>>>>> udt <- unique(dt[c("chr", "pos", "gene_id")])
>>>>> nrow(udt)
>>>>> 
>>>>> get close to what you want?
>>>>> 
>>>>>   Hth  --  Gerrit
>>>>> 
>>>>> ---------------------------------------------------------------------
>>>>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>>>>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>>>>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>>>>> http://www.uni-giessen.de/eichner
>>>>> ---------------------------------------------------------------------
>>>>> 
>>>>> Am 08.11.2019 um 15:38 schrieb Ana Marija:
>>>>>> Hello,
>>>>>> 
>>>>>> I have a data frame like this:
>>>>>> 
>>>>>>> head(dt,20)
>>>>>>       chr    pos         gene_id pval_nominal  pval_ret       wl      wr
>>>>>>   1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
>>>>>>   2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
>>>>>>   3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
>>>>>>   4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
>>>>>>   5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
>>>>>>   6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
>>>>>>   7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
>>>>>>   8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
>>>>>>   9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
>>>>>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
>>>>>> 
>>>>>> it is very big,
>>>>>>> dim(dt)
>>>>>> [1] 73719122        8
>>>>>> 
>>>>>> To count number of unique rows for all 3 columns: chr, pos and gene_id
>>>>>> I could just join those 3 columns and than count. But how would I find
>>>>>> unique number of rows for these 4 columns without joining them?
>>>>>> 
>>>>>> Thanks
>>>>>> Ana
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>> 
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov  8 16:59:29 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 8 Nov 2019 07:59:29 -0800
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
Message-ID: <CAGxFJbTM0TMkeAn=2_rwB+oX=pt18-Cti1nzCGDmPZjHs1R4pg@mail.gmail.com>

Sorry, but you ask basic questions.You really need to spend some more time
with an R tutorial or two. This list is not meant to replace your own
learning efforts.

You also do not seem to be reading the docs carefully. Under ?unique, it
links ?duplicated and tells you that it gives indices of duplicated rows of
a data frame. These then can be used by subscripting to remove those rows
from the data frame. Here is a reproducible example:

df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
df[-duplicated(df[,2:3]), ]  ## Note the - sign

If you prefer, the "Tidyverse" world has what are purported to be more
user-friendly versions of such data handling functionality that you can use
instead.


Bert

On Fri, Nov 8, 2019 at 7:38 AM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> would you know how would I extract from my original data frame, just
> these unique rows?
> because this gives me only those 3 columns, and I want all columns
> from the original data frame
>
> > head(udt)
>    chr   pos         gene_id
> 1 chr1 54490 ENSG00000227232
> 2 chr1 58814 ENSG00000227232
> 3 chr1 60351 ENSG00000227232
> 4 chr1 61920 ENSG00000227232
> 5 chr1 63671 ENSG00000227232
> 6 chr1 64931 ENSG00000227232
>
> > head(dt)
>     chr   pos         gene_id pval_nominal pval_ret       wl      wr
> META
> 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838
> 0.7475480
> 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838
> 0.6031214
> 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838
> 0.6907182
> 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838
> 0.4032200
> 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838
> 0.7482519
> 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838
> 0.5974800
>
> On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > Thank you so much! Converting it to data frame resolved the issue!
> >
> > On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
> > <gerrit.eichner at math.uni-giessen.de> wrote:
> > >
> > > It seems as if dt is not a (base R) data frame but a
> > > data table. I assume, you will have to transform dt
> > > into a data frame (maybe with as.data.frame) to be
> > > able to apply unique in the suggested way. However,
> > > I am not familiar with data tables. Perhaps somebody
> > > else can provide a more profound guess.
> > >
> > >   Regards  --  Gerrit
> > >
> > > ---------------------------------------------------------------------
> > > Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> > > gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> > > Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> > > http://www.uni-giessen.de/eichner
> > > ---------------------------------------------------------------------
> > >
> > > Am 08.11.2019 um 16:02 schrieb Ana Marija:
> > > > I tried it but I got this error:
> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
> > > > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
> > > >    When i is a data.table (or character vector), the columns to join
> by
> > > > must be specified using 'on=' argument (see ?data.table), by keying x
> > > > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
> > > > column names between x and i (i.e., a natural join). Keyed joins
> might
> > > > have further speed benefits on very large data due to x being sorted
> > > > in RAM.
> > > >
> > > > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
> > > > <gerrit.eichner at math.uni-giessen.de> wrote:
> > > >>
> > > >> Hi, Ana,
> > > >>
> > > >> doesn't
> > > >>
> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
> > > >> nrow(udt)
> > > >>
> > > >> get close to what you want?
> > > >>
> > > >>    Hth  --  Gerrit
> > > >>
> > > >>
> ---------------------------------------------------------------------
> > > >> Dr. Gerrit Eichner                   Mathematical Institute, Room
> 212
> > > >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University
> Giessen
> > > >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen,
> Germany
> > > >> http://www.uni-giessen.de/eichner
> > > >>
> ---------------------------------------------------------------------
> > > >>
> > > >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
> > > >>> Hello,
> > > >>>
> > > >>> I have a data frame like this:
> > > >>>
> > > >>>> head(dt,20)
> > > >>>        chr    pos         gene_id pval_nominal  pval_ret       wl
>     wr
> > > >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278
> 21.2838
> > > >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278
> 21.2838
> > > >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278
> 21.2838
> > > >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278
> 21.2838
> > > >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278
> 21.2838
> > > >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278
> 21.2838
> > > >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278
> 21.2838
> > > >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278
> 21.2838
> > > >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278
> 21.2838
> > > >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278
> 21.2838
> > > >>>
> > > >>> it is very big,
> > > >>>> dim(dt)
> > > >>> [1] 73719122        8
> > > >>>
> > > >>> To count number of unique rows for all 3 columns: chr, pos and
> gene_id
> > > >>> I could just join those 3 columns and than count. But how would I
> find
> > > >>> unique number of rows for these 4 columns without joining them?
> > > >>>
> > > >>> Thanks
> > > >>> Ana
> > > >>>
> > > >>> ______________________________________________
> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > >>> and provide commented, minimal, self-contained, reproducible code.
> > > >>>
> > > >>
> > > >> ______________________________________________
> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov  8 17:30:23 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 8 Nov 2019 10:30:23 -0600
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <5D6AEC54-04FA-4032-8C76-B0FEBB890AF6@utoronto.ca>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
 <5D6AEC54-04FA-4032-8C76-B0FEBB890AF6@utoronto.ca>
Message-ID: <CAF9-5jPonj6xvLMoyuxPRgOQceDQh76ZXrPdHtBzEnBRMECNEA@mail.gmail.com>

I am trying to first identify how many duplicate rows are there determined
by the unique values in the first 3 columns. Now I know that is about 20000
rows which are non unique. But I would like to extract all 8 columns for
those non unique rows and see what is going on with META value I have in
them.

About duplicated() function I know as well as about unique

On Fri, 8 Nov 2019 at 10:08, Boris Steipe <boris.steipe at utoronto.ca> wrote:

> Are you trying to eliminate duplicated rows from your dataframe? Because
> that would be better achieved with duplicated().
>
>
> B.
>
>
>
>
> > On 2019-11-08, at 10:32, Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > would you know how would I extract from my original data frame, just
> > these unique rows?
> > because this gives me only those 3 columns, and I want all columns
> > from the original data frame
> >
> >> head(udt)
> >   chr   pos         gene_id
> > 1 chr1 54490 ENSG00000227232
> > 2 chr1 58814 ENSG00000227232
> > 3 chr1 60351 ENSG00000227232
> > 4 chr1 61920 ENSG00000227232
> > 5 chr1 63671 ENSG00000227232
> > 6 chr1 64931 ENSG00000227232
> >
> >> head(dt)
> >    chr   pos         gene_id pval_nominal pval_ret       wl      wr
> META
> > 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838
> 0.7475480
> > 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838
> 0.6031214
> > 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838
> 0.6907182
> > 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838
> 0.4032200
> > 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838
> 0.7482519
> > 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838
> 0.5974800
> >
> > On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >>
> >> Thank you so much! Converting it to data frame resolved the issue!
> >>
> >> On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
> >> <gerrit.eichner at math.uni-giessen.de> wrote:
> >>>
> >>> It seems as if dt is not a (base R) data frame but a
> >>> data table. I assume, you will have to transform dt
> >>> into a data frame (maybe with as.data.frame) to be
> >>> able to apply unique in the suggested way. However,
> >>> I am not familiar with data tables. Perhaps somebody
> >>> else can provide a more profound guess.
> >>>
> >>>  Regards  --  Gerrit
> >>>
> >>> ---------------------------------------------------------------------
> >>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> >>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >>> http://www.uni-giessen.de/eichner
> >>> ---------------------------------------------------------------------
> >>>
> >>> Am 08.11.2019 um 16:02 schrieb Ana Marija:
> >>>> I tried it but I got this error:
> >>>>> udt <- unique(dt[c("chr", "pos", "gene_id")])
> >>>> Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
> >>>>   When i is a data.table (or character vector), the columns to join by
> >>>> must be specified using 'on=' argument (see ?data.table), by keying x
> >>>> (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
> >>>> column names between x and i (i.e., a natural join). Keyed joins might
> >>>> have further speed benefits on very large data due to x being sorted
> >>>> in RAM.
> >>>>
> >>>> On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
> >>>> <gerrit.eichner at math.uni-giessen.de> wrote:
> >>>>>
> >>>>> Hi, Ana,
> >>>>>
> >>>>> doesn't
> >>>>>
> >>>>> udt <- unique(dt[c("chr", "pos", "gene_id")])
> >>>>> nrow(udt)
> >>>>>
> >>>>> get close to what you want?
> >>>>>
> >>>>>   Hth  --  Gerrit
> >>>>>
> >>>>> ---------------------------------------------------------------------
> >>>>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> >>>>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University
> Giessen
> >>>>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >>>>> http://www.uni-giessen.de/eichner
> >>>>> ---------------------------------------------------------------------
> >>>>>
> >>>>> Am 08.11.2019 um 15:38 schrieb Ana Marija:
> >>>>>> Hello,
> >>>>>>
> >>>>>> I have a data frame like this:
> >>>>>>
> >>>>>>> head(dt,20)
> >>>>>>       chr    pos         gene_id pval_nominal  pval_ret       wl
>   wr
> >>>>>>   1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278
> 21.2838
> >>>>>>   2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278
> 21.2838
> >>>>>>   3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278
> 21.2838
> >>>>>>   4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278
> 21.2838
> >>>>>>   5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278
> 21.2838
> >>>>>>   6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278
> 21.2838
> >>>>>>   7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278
> 21.2838
> >>>>>>   8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278
> 21.2838
> >>>>>>   9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278
> 21.2838
> >>>>>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278
> 21.2838
> >>>>>>
> >>>>>> it is very big,
> >>>>>>> dim(dt)
> >>>>>> [1] 73719122        8
> >>>>>>
> >>>>>> To count number of unique rows for all 3 columns: chr, pos and
> gene_id
> >>>>>> I could just join those 3 columns and than count. But how would I
> find
> >>>>>> unique number of rows for these 4 columns without joining them?
> >>>>>>
> >>>>>> Thanks
> >>>>>> Ana
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bor|@@@te|pe @end|ng |rom utoronto@c@  Fri Nov  8 17:49:44 2019
From: bor|@@@te|pe @end|ng |rom utoronto@c@ (Boris Steipe)
Date: Fri, 8 Nov 2019 16:49:44 +0000
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jPonj6xvLMoyuxPRgOQceDQh76ZXrPdHtBzEnBRMECNEA@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
 <5D6AEC54-04FA-4032-8C76-B0FEBB890AF6@utoronto.ca>
 <CAF9-5jPonj6xvLMoyuxPRgOQceDQh76ZXrPdHtBzEnBRMECNEA@mail.gmail.com>
Message-ID: <B1BE5D61-C7B1-4C28-96EE-1250CE796472@utoronto.ca>

Good. Duplicated returns a boolean index vector that you can use to extract the non-unique rows. 

B.



> On 2019-11-08, at 11:30, Ana Marija <sokovic.anamarija at gmail.com> wrote:
> 
> I am trying to first identify how many duplicate rows are there determined by the unique values in the first 3 columns. Now I know that is about 20000 rows which are non unique. But I would like to extract all 8 columns for those non unique rows and see what is going on with META value I have in them. 
> 
> About duplicated() function I know as well as about unique
> 
> On Fri, 8 Nov 2019 at 10:08, Boris Steipe <boris.steipe at utoronto.ca> wrote:
> Are you trying to eliminate duplicated rows from your dataframe? Because that would be better achieved with duplicated().
> 
> 
> B.
> 
> 
> 
> 
> > On 2019-11-08, at 10:32, Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > 
> > would you know how would I extract from my original data frame, just
> > these unique rows?
> > because this gives me only those 3 columns, and I want all columns
> > from the original data frame
> > 
> >> head(udt)
> >   chr   pos         gene_id
> > 1 chr1 54490 ENSG00000227232
> > 2 chr1 58814 ENSG00000227232
> > 3 chr1 60351 ENSG00000227232
> > 4 chr1 61920 ENSG00000227232
> > 5 chr1 63671 ENSG00000227232
> > 6 chr1 64931 ENSG00000227232
> > 
> >> head(dt)
> >    chr   pos         gene_id pval_nominal pval_ret       wl      wr      META
> > 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838 0.7475480
> > 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838 0.6031214
> > 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838 0.6907182
> > 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838 0.4032200
> > 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838 0.7482519
> > 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838 0.5974800
> > 
> > On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >> 
> >> Thank you so much! Converting it to data frame resolved the issue!
> >> 
> >> On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
> >> <gerrit.eichner at math.uni-giessen.de> wrote:
> >>> 
> >>> It seems as if dt is not a (base R) data frame but a
> >>> data table. I assume, you will have to transform dt
> >>> into a data frame (maybe with as.data.frame) to be
> >>> able to apply unique in the suggested way. However,
> >>> I am not familiar with data tables. Perhaps somebody
> >>> else can provide a more profound guess.
> >>> 
> >>>  Regards  --  Gerrit
> >>> 
> >>> ---------------------------------------------------------------------
> >>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> >>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >>> http://www.uni-giessen.de/eichner
> >>> ---------------------------------------------------------------------
> >>> 
> >>> Am 08.11.2019 um 16:02 schrieb Ana Marija:
> >>>> I tried it but I got this error:
> >>>>> udt <- unique(dt[c("chr", "pos", "gene_id")])
> >>>> Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
> >>>>   When i is a data.table (or character vector), the columns to join by
> >>>> must be specified using 'on=' argument (see ?data.table), by keying x
> >>>> (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
> >>>> column names between x and i (i.e., a natural join). Keyed joins might
> >>>> have further speed benefits on very large data due to x being sorted
> >>>> in RAM.
> >>>> 
> >>>> On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
> >>>> <gerrit.eichner at math.uni-giessen.de> wrote:
> >>>>> 
> >>>>> Hi, Ana,
> >>>>> 
> >>>>> doesn't
> >>>>> 
> >>>>> udt <- unique(dt[c("chr", "pos", "gene_id")])
> >>>>> nrow(udt)
> >>>>> 
> >>>>> get close to what you want?
> >>>>> 
> >>>>>   Hth  --  Gerrit
> >>>>> 
> >>>>> ---------------------------------------------------------------------
> >>>>> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
> >>>>> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
> >>>>> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
> >>>>> http://www.uni-giessen.de/eichner
> >>>>> ---------------------------------------------------------------------
> >>>>> 
> >>>>> Am 08.11.2019 um 15:38 schrieb Ana Marija:
> >>>>>> Hello,
> >>>>>> 
> >>>>>> I have a data frame like this:
> >>>>>> 
> >>>>>>> head(dt,20)
> >>>>>>       chr    pos         gene_id pval_nominal  pval_ret       wl      wr
> >>>>>>   1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
> >>>>>>   2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
> >>>>>>   3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
> >>>>>>   4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
> >>>>>>   5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
> >>>>>>   6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
> >>>>>>   7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
> >>>>>>   8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
> >>>>>>   9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
> >>>>>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
> >>>>>> 
> >>>>>> it is very big,
> >>>>>>> dim(dt)
> >>>>>> [1] 73719122        8
> >>>>>> 
> >>>>>> To count number of unique rows for all 3 columns: chr, pos and gene_id
> >>>>>> I could just join those 3 columns and than count. But how would I find
> >>>>>> unique number of rows for these 4 columns without joining them?
> >>>>>> 
> >>>>>> Thanks
> >>>>>> Ana
> >>>>>> 
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>> 
> >>>>> 
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov  8 18:39:55 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Fri, 8 Nov 2019 09:39:55 -0800
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAGxFJbTM0TMkeAn=2_rwB+oX=pt18-Cti1nzCGDmPZjHs1R4pg@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
 <CAGxFJbTM0TMkeAn=2_rwB+oX=pt18-Cti1nzCGDmPZjHs1R4pg@mail.gmail.com>
Message-ID: <CAGxFJbQck6pFvi=n63qVMen_qYvZKCk+yAFwUNopH3ZwVv0agw@mail.gmail.com>

Correction:
df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
df[!duplicated(df[,2:3]), ]  ## Note the ! sign

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Nov 8, 2019 at 7:59 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Sorry, but you ask basic questions.You really need to spend some more time
> with an R tutorial or two. This list is not meant to replace your own
> learning efforts.
>
> You also do not seem to be reading the docs carefully. Under ?unique, it
> links ?duplicated and tells you that it gives indices of duplicated rows of
> a data frame. These then can be used by subscripting to remove those rows
> from the data frame. Here is a reproducible example:
>
> df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
> df[-duplicated(df[,2:3]), ]  ## Note the - sign
>
> If you prefer, the "Tidyverse" world has what are purported to be more
> user-friendly versions of such data handling functionality that you can use
> instead.
>
>
> Bert
>
> On Fri, Nov 8, 2019 at 7:38 AM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
>
>> would you know how would I extract from my original data frame, just
>> these unique rows?
>> because this gives me only those 3 columns, and I want all columns
>> from the original data frame
>>
>> > head(udt)
>>    chr   pos         gene_id
>> 1 chr1 54490 ENSG00000227232
>> 2 chr1 58814 ENSG00000227232
>> 3 chr1 60351 ENSG00000227232
>> 4 chr1 61920 ENSG00000227232
>> 5 chr1 63671 ENSG00000227232
>> 6 chr1 64931 ENSG00000227232
>>
>> > head(dt)
>>     chr   pos         gene_id pval_nominal pval_ret       wl      wr
>> META
>> 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838
>> 0.7475480
>> 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838
>> 0.6031214
>> 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838
>> 0.6907182
>> 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838
>> 0.4032200
>> 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838
>> 0.7482519
>> 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838
>> 0.5974800
>>
>> On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com>
>> wrote:
>> >
>> > Thank you so much! Converting it to data frame resolved the issue!
>> >
>> > On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
>> > <gerrit.eichner at math.uni-giessen.de> wrote:
>> > >
>> > > It seems as if dt is not a (base R) data frame but a
>> > > data table. I assume, you will have to transform dt
>> > > into a data frame (maybe with as.data.frame) to be
>> > > able to apply unique in the suggested way. However,
>> > > I am not familiar with data tables. Perhaps somebody
>> > > else can provide a more profound guess.
>> > >
>> > >   Regards  --  Gerrit
>> > >
>> > > ---------------------------------------------------------------------
>> > > Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>> > > gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>> > > Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>> > > http://www.uni-giessen.de/eichner
>> > > ---------------------------------------------------------------------
>> > >
>> > > Am 08.11.2019 um 16:02 schrieb Ana Marija:
>> > > > I tried it but I got this error:
>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
>> > > > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
>> > > >    When i is a data.table (or character vector), the columns to
>> join by
>> > > > must be specified using 'on=' argument (see ?data.table), by keying
>> x
>> > > > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
>> > > > column names between x and i (i.e., a natural join). Keyed joins
>> might
>> > > > have further speed benefits on very large data due to x being sorted
>> > > > in RAM.
>> > > >
>> > > > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
>> > > > <gerrit.eichner at math.uni-giessen.de> wrote:
>> > > >>
>> > > >> Hi, Ana,
>> > > >>
>> > > >> doesn't
>> > > >>
>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
>> > > >> nrow(udt)
>> > > >>
>> > > >> get close to what you want?
>> > > >>
>> > > >>    Hth  --  Gerrit
>> > > >>
>> > > >>
>> ---------------------------------------------------------------------
>> > > >> Dr. Gerrit Eichner                   Mathematical Institute, Room
>> 212
>> > > >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University
>> Giessen
>> > > >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen,
>> Germany
>> > > >> http://www.uni-giessen.de/eichner
>> > > >>
>> ---------------------------------------------------------------------
>> > > >>
>> > > >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
>> > > >>> Hello,
>> > > >>>
>> > > >>> I have a data frame like this:
>> > > >>>
>> > > >>>> head(dt,20)
>> > > >>>        chr    pos         gene_id pval_nominal  pval_ret
>>  wl      wr
>> > > >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278
>> 21.2838
>> > > >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278
>> 21.2838
>> > > >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278
>> 21.2838
>> > > >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278
>> 21.2838
>> > > >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278
>> 21.2838
>> > > >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278
>> 21.2838
>> > > >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278
>> 21.2838
>> > > >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278
>> 21.2838
>> > > >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278
>> 21.2838
>> > > >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278
>> 21.2838
>> > > >>>
>> > > >>> it is very big,
>> > > >>>> dim(dt)
>> > > >>> [1] 73719122        8
>> > > >>>
>> > > >>> To count number of unique rows for all 3 columns: chr, pos and
>> gene_id
>> > > >>> I could just join those 3 columns and than count. But how would I
>> find
>> > > >>> unique number of rows for these 4 columns without joining them?
>> > > >>>
>> > > >>> Thanks
>> > > >>> Ana
>> > > >>>
>> > > >>> ______________________________________________
>> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > > >>> and provide commented, minimal, self-contained, reproducible code.
>> > > >>>
>> > > >>
>> > > >> ______________________________________________
>> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > > >> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From j@vedbtk111 @end|ng |rom gm@||@com  Fri Nov  8 18:56:02 2019
From: j@vedbtk111 @end|ng |rom gm@||@com (javed khan)
Date: Fri, 8 Nov 2019 18:56:02 +0100
Subject: [R] About separate train and test data
Message-ID: <CAJhui+tH+M7TKToiypFM_vUc67MTqqPHFBrGncHUSdYb00ER=g@mail.gmail.com>

Hi

For instance, we have separate train and test data files (not want to do k
fold), so we will not use the function trainControl? In that case if we
have to tune the parameters, do we need to specify search =grid in the
train function?

My second question is how we can measure MCC classification measure? Is it
same like metric=Roc in the train function?

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov  8 19:29:46 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 8 Nov 2019 12:29:46 -0600
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAGxFJbQck6pFvi=n63qVMen_qYvZKCk+yAFwUNopH3ZwVv0agw@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
 <CAGxFJbTM0TMkeAn=2_rwB+oX=pt18-Cti1nzCGDmPZjHs1R4pg@mail.gmail.com>
 <CAGxFJbQck6pFvi=n63qVMen_qYvZKCk+yAFwUNopH3ZwVv0agw@mail.gmail.com>
Message-ID: <CAF9-5jOoCYa7kKG6qXPkYc857cvRXLx=kfjTSbgje_tCw7vKSQ@mail.gmail.com>

Thank you so much!!!

On Fri, Nov 8, 2019 at 11:40 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Correction:
> df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
> df[!duplicated(df[,2:3]), ]  ## Note the ! sign
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Fri, Nov 8, 2019 at 7:59 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Sorry, but you ask basic questions.You really need to spend some more time with an R tutorial or two. This list is not meant to replace your own learning efforts.
>>
>> You also do not seem to be reading the docs carefully. Under ?unique, it links ?duplicated and tells you that it gives indices of duplicated rows of a data frame. These then can be used by subscripting to remove those rows from the data frame. Here is a reproducible example:
>>
>> df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
>> df[-duplicated(df[,2:3]), ]  ## Note the - sign
>>
>> If you prefer, the "Tidyverse" world has what are purported to be more user-friendly versions of such data handling functionality that you can use instead.
>>
>>
>> Bert
>>
>> On Fri, Nov 8, 2019 at 7:38 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>
>>> would you know how would I extract from my original data frame, just
>>> these unique rows?
>>> because this gives me only those 3 columns, and I want all columns
>>> from the original data frame
>>>
>>> > head(udt)
>>>    chr   pos         gene_id
>>> 1 chr1 54490 ENSG00000227232
>>> 2 chr1 58814 ENSG00000227232
>>> 3 chr1 60351 ENSG00000227232
>>> 4 chr1 61920 ENSG00000227232
>>> 5 chr1 63671 ENSG00000227232
>>> 6 chr1 64931 ENSG00000227232
>>>
>>> > head(dt)
>>>     chr   pos         gene_id pval_nominal pval_ret       wl      wr      META
>>> 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838 0.7475480
>>> 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838 0.6031214
>>> 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838 0.6907182
>>> 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838 0.4032200
>>> 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838 0.7482519
>>> 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838 0.5974800
>>>
>>> On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>> >
>>> > Thank you so much! Converting it to data frame resolved the issue!
>>> >
>>> > On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
>>> > <gerrit.eichner at math.uni-giessen.de> wrote:
>>> > >
>>> > > It seems as if dt is not a (base R) data frame but a
>>> > > data table. I assume, you will have to transform dt
>>> > > into a data frame (maybe with as.data.frame) to be
>>> > > able to apply unique in the suggested way. However,
>>> > > I am not familiar with data tables. Perhaps somebody
>>> > > else can provide a more profound guess.
>>> > >
>>> > >   Regards  --  Gerrit
>>> > >
>>> > > ---------------------------------------------------------------------
>>> > > Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>>> > > gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>>> > > Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>>> > > http://www.uni-giessen.de/eichner
>>> > > ---------------------------------------------------------------------
>>> > >
>>> > > Am 08.11.2019 um 16:02 schrieb Ana Marija:
>>> > > > I tried it but I got this error:
>>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
>>> > > > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
>>> > > >    When i is a data.table (or character vector), the columns to join by
>>> > > > must be specified using 'on=' argument (see ?data.table), by keying x
>>> > > > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
>>> > > > column names between x and i (i.e., a natural join). Keyed joins might
>>> > > > have further speed benefits on very large data due to x being sorted
>>> > > > in RAM.
>>> > > >
>>> > > > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
>>> > > > <gerrit.eichner at math.uni-giessen.de> wrote:
>>> > > >>
>>> > > >> Hi, Ana,
>>> > > >>
>>> > > >> doesn't
>>> > > >>
>>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
>>> > > >> nrow(udt)
>>> > > >>
>>> > > >> get close to what you want?
>>> > > >>
>>> > > >>    Hth  --  Gerrit
>>> > > >>
>>> > > >> ---------------------------------------------------------------------
>>> > > >> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>>> > > >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>>> > > >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>>> > > >> http://www.uni-giessen.de/eichner
>>> > > >> ---------------------------------------------------------------------
>>> > > >>
>>> > > >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
>>> > > >>> Hello,
>>> > > >>>
>>> > > >>> I have a data frame like this:
>>> > > >>>
>>> > > >>>> head(dt,20)
>>> > > >>>        chr    pos         gene_id pval_nominal  pval_ret       wl      wr
>>> > > >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
>>> > > >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
>>> > > >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
>>> > > >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
>>> > > >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
>>> > > >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
>>> > > >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
>>> > > >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
>>> > > >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
>>> > > >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
>>> > > >>>
>>> > > >>> it is very big,
>>> > > >>>> dim(dt)
>>> > > >>> [1] 73719122        8
>>> > > >>>
>>> > > >>> To count number of unique rows for all 3 columns: chr, pos and gene_id
>>> > > >>> I could just join those 3 columns and than count. But how would I find
>>> > > >>> unique number of rows for these 4 columns without joining them?
>>> > > >>>
>>> > > >>> Thanks
>>> > > >>> Ana
>>> > > >>>
>>> > > >>> ______________________________________________
>>> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> > > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> > > >>> and provide commented, minimal, self-contained, reproducible code.
>>> > > >>>
>>> > > >>
>>> > > >> ______________________________________________
>>> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
>>> > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> > > >> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.


From mtmorg@n@b|oc @end|ng |rom gm@||@com  Fri Nov  8 22:33:35 2019
From: mtmorg@n@b|oc @end|ng |rom gm@||@com (Martin Morgan)
Date: Fri, 8 Nov 2019 21:33:35 +0000
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <CAF9-5jOoCYa7kKG6qXPkYc857cvRXLx=kfjTSbgje_tCw7vKSQ@mail.gmail.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
 <CAGxFJbTM0TMkeAn=2_rwB+oX=pt18-Cti1nzCGDmPZjHs1R4pg@mail.gmail.com>
 <CAGxFJbQck6pFvi=n63qVMen_qYvZKCk+yAFwUNopH3ZwVv0agw@mail.gmail.com>
 <CAF9-5jOoCYa7kKG6qXPkYc857cvRXLx=kfjTSbgje_tCw7vKSQ@mail.gmail.com>
Message-ID: <DM6PR04MB4235CA6C7270076725BFFF66F97B0@DM6PR04MB4235.namprd04.prod.outlook.com>

With this example

> df = data.frame(a = c(1, 1, 2, 2), b = c(1, 1, 2, 3), value = 1:4)
> df
  a b value
1 1 1     1
2 1 1     2
3 2 2     3
4 2 3     4

The approach to drop duplicates in the first and second columns has as a consequence the arbitrary choice of 'value' for the duplicate entries -- why chose a value of '1' rather than '2' (or the average of 1 and 2, or a list containing all possible values, or...) for the rows duplicated in columns a and b?

> df[!duplicated(df[,1:2]),]
  a b value
1 1 1     1
3 2 2     3
4 2 3     4

In base R one might

> aggregate(value ~ a + b, df, mean)
  a b value
1 1 1   1.5
2 2 2   3.0
3 2 3   4.0
> aggregate(value ~ a + b, df, list)
  a b value
1 1 1  1, 2
2 2 2     3
3 2 3     4

but handling several value-like columns would be hard(?)

Using library(dplyr), I have

> group_by(df, a, b) %>% summarize(mean_value = mean(value))
# A tibble: 3 x 3
# Groups:   a [2]
      a     b mean_value
  <dbl> <dbl>      <dbl>
1     1     1        1.5
2     2     2        3
3     2     3        4

or

> group_by(df, a, b) %>% summarize(values = list(value))
# A tibble: 3 x 3
# Groups:   a [2]
      a     b values
  <dbl> <dbl> <list>
1     1     1 <int [2]>
2     2     2 <int [1]>
3     2     3 <int [1]>

summarizing multiple columns with dplyr

> df$v1 = 1:4
> df$v2 = 4:1                                                                   
>  group_by(df, a, b) %>% summarize(v1_mean = mean(v1), v2_median = median(v2))
# A tibble: 3 x 4
# Groups:   a [2]
      a     b v1_mean v2_median
  <dbl> <dbl>   <dbl>     <dbl>
1     1     1     1.5       3.5
2     2     2     3         2
3     2     3     4         1

I do not know how performant this would be with data of your size.

Martin Morgan

?On 11/8/19, 1:39 PM, "R-help on behalf of Ana Marija" <r-help-bounces at r-project.org on behalf of sokovic.anamarija at gmail.com> wrote:

    Thank you so much!!!
    
    On Fri, Nov 8, 2019 at 11:40 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
    >
    > Correction:
    > df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
    > df[!duplicated(df[,2:3]), ]  ## Note the ! sign
    >
    > Bert Gunter
    >
    > "The trouble with having an open mind is that people keep coming along and sticking things into it."
    > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
    >
    >
    > On Fri, Nov 8, 2019 at 7:59 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
    >>
    >> Sorry, but you ask basic questions.You really need to spend some more time with an R tutorial or two. This list is not meant to replace your own learning efforts.
    >>
    >> You also do not seem to be reading the docs carefully. Under ?unique, it links ?duplicated and tells you that it gives indices of duplicated rows of a data frame. These then can be used by subscripting to remove those rows from the data frame. Here is a reproducible example:
    >>
    >> df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
    >> df[-duplicated(df[,2:3]), ]  ## Note the - sign
    >>
    >> If you prefer, the "Tidyverse" world has what are purported to be more user-friendly versions of such data handling functionality that you can use instead.
    >>
    >>
    >> Bert
    >>
    >> On Fri, Nov 8, 2019 at 7:38 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
    >>>
    >>> would you know how would I extract from my original data frame, just
    >>> these unique rows?
    >>> because this gives me only those 3 columns, and I want all columns
    >>> from the original data frame
    >>>
    >>> > head(udt)
    >>>    chr   pos         gene_id
    >>> 1 chr1 54490 ENSG00000227232
    >>> 2 chr1 58814 ENSG00000227232
    >>> 3 chr1 60351 ENSG00000227232
    >>> 4 chr1 61920 ENSG00000227232
    >>> 5 chr1 63671 ENSG00000227232
    >>> 6 chr1 64931 ENSG00000227232
    >>>
    >>> > head(dt)
    >>>     chr   pos         gene_id pval_nominal pval_ret       wl      wr      META
    >>> 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838 0.7475480
    >>> 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838 0.6031214
    >>> 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838 0.6907182
    >>> 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838 0.4032200
    >>> 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838 0.7482519
    >>> 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838 0.5974800
    >>>
    >>> On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
    >>> >
    >>> > Thank you so much! Converting it to data frame resolved the issue!
    >>> >
    >>> > On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
    >>> > <gerrit.eichner at math.uni-giessen.de> wrote:
    >>> > >
    >>> > > It seems as if dt is not a (base R) data frame but a
    >>> > > data table. I assume, you will have to transform dt
    >>> > > into a data frame (maybe with as.data.frame) to be
    >>> > > able to apply unique in the suggested way. However,
    >>> > > I am not familiar with data tables. Perhaps somebody
    >>> > > else can provide a more profound guess.
    >>> > >
    >>> > >   Regards  --  Gerrit
    >>> > >
    >>> > > ---------------------------------------------------------------------
    >>> > > Dr. Gerrit Eichner                   Mathematical Institute, Room 212
    >>> > > gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
    >>> > > Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
    >>> > > http://www.uni-giessen.de/eichner
    >>> > > ---------------------------------------------------------------------
    >>> > >
    >>> > > Am 08.11.2019 um 16:02 schrieb Ana Marija:
    >>> > > > I tried it but I got this error:
    >>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
    >>> > > > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
    >>> > > >    When i is a data.table (or character vector), the columns to join by
    >>> > > > must be specified using 'on=' argument (see ?data.table), by keying x
    >>> > > > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
    >>> > > > column names between x and i (i.e., a natural join). Keyed joins might
    >>> > > > have further speed benefits on very large data due to x being sorted
    >>> > > > in RAM.
    >>> > > >
    >>> > > > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
    >>> > > > <gerrit.eichner at math.uni-giessen.de> wrote:
    >>> > > >>
    >>> > > >> Hi, Ana,
    >>> > > >>
    >>> > > >> doesn't
    >>> > > >>
    >>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
    >>> > > >> nrow(udt)
    >>> > > >>
    >>> > > >> get close to what you want?
    >>> > > >>
    >>> > > >>    Hth  --  Gerrit
    >>> > > >>
    >>> > > >> ---------------------------------------------------------------------
    >>> > > >> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
    >>> > > >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
    >>> > > >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
    >>> > > >> http://www.uni-giessen.de/eichner
    >>> > > >> ---------------------------------------------------------------------
    >>> > > >>
    >>> > > >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
    >>> > > >>> Hello,
    >>> > > >>>
    >>> > > >>> I have a data frame like this:
    >>> > > >>>
    >>> > > >>>> head(dt,20)
    >>> > > >>>        chr    pos         gene_id pval_nominal  pval_ret       wl      wr
    >>> > > >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
    >>> > > >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
    >>> > > >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
    >>> > > >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
    >>> > > >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
    >>> > > >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
    >>> > > >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
    >>> > > >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
    >>> > > >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
    >>> > > >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
    >>> > > >>>
    >>> > > >>> it is very big,
    >>> > > >>>> dim(dt)
    >>> > > >>> [1] 73719122        8
    >>> > > >>>
    >>> > > >>> To count number of unique rows for all 3 columns: chr, pos and gene_id
    >>> > > >>> I could just join those 3 columns and than count. But how would I find
    >>> > > >>> unique number of rows for these 4 columns without joining them?
    >>> > > >>>
    >>> > > >>> Thanks
    >>> > > >>> Ana
    >>> > > >>>
    >>> > > >>> ______________________________________________
    >>> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >>> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
    >>> > > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >>> > > >>> and provide commented, minimal, self-contained, reproducible code.
    >>> > > >>>
    >>> > > >>
    >>> > > >> ______________________________________________
    >>> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >>> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
    >>> > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >>> > > >> and provide commented, minimal, self-contained, reproducible code.
    >>>
    >>> ______________________________________________
    >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    >>> https://stat.ethz.ch/mailman/listinfo/r-help
    >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    >>> and provide commented, minimal, self-contained, reproducible code.
    
    ______________________________________________
    R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
    https://stat.ethz.ch/mailman/listinfo/r-help
    PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
    and provide commented, minimal, self-contained, reproducible code.
    

From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Nov  9 00:02:17 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Fri, 8 Nov 2019 23:02:17 +0000
Subject: [R] 
 how to find number of unique rows for combination of r columns
In-Reply-To: <DM6PR04MB4235CA6C7270076725BFFF66F97B0@DM6PR04MB4235.namprd04.prod.outlook.com>
References: <CAF9-5jO1SwbD4ohKfg52G_GNen6r0WhjyHde7UOfkUskoDsbog@mail.gmail.com>
 <70dcffc4-8ee9-e0d5-201f-4d559030ca1a@math.uni-giessen.de>
 <CAF9-5jOx-p4DYH0=1A_AoLh=VsEsP9ZOkNc2DWDAxMMaoPnqjA@mail.gmail.com>
 <0568d609-4d8f-2d8a-399b-209b62b81561@math.uni-giessen.de>
 <CAF9-5jPUMPkGUM6q56OGOEWQnBA-SJVvG587pU+oWOWnps8sHw@mail.gmail.com>
 <CAF9-5jPsA70ThxPouObWYH9tqV_xALP5wwgPTgsmmRDrxUyk3g@mail.gmail.com>
 <CAGxFJbTM0TMkeAn=2_rwB+oX=pt18-Cti1nzCGDmPZjHs1R4pg@mail.gmail.com>
 <CAGxFJbQck6pFvi=n63qVMen_qYvZKCk+yAFwUNopH3ZwVv0agw@mail.gmail.com>
 <CAF9-5jOoCYa7kKG6qXPkYc857cvRXLx=kfjTSbgje_tCw7vKSQ@mail.gmail.com>
 <DM6PR04MB4235CA6C7270076725BFFF66F97B0@DM6PR04MB4235.namprd04.prod.outlook.com>
Message-ID: <6ee26aad-7c1e-0281-22d7-50d0330840a1@sapo.pt>

Hello,

If performance is important, and with 73M rows it probably is, take a 
look at this StackOverflow post.

[1] https://stackoverflow.com/a/36058634/8245406


Hope this helps,

Rui Barradas

?s 21:33 de 08/11/19, Martin Morgan escreveu:
> With this example
> 
>> df = data.frame(a = c(1, 1, 2, 2), b = c(1, 1, 2, 3), value = 1:4)
>> df
>    a b value
> 1 1 1     1
> 2 1 1     2
> 3 2 2     3
> 4 2 3     4
> 
> The approach to drop duplicates in the first and second columns has as a consequence the arbitrary choice of 'value' for the duplicate entries -- why chose a value of '1' rather than '2' (or the average of 1 and 2, or a list containing all possible values, or...) for the rows duplicated in columns a and b?
> 
>> df[!duplicated(df[,1:2]),]
>    a b value
> 1 1 1     1
> 3 2 2     3
> 4 2 3     4
> 
> In base R one might
> 
>> aggregate(value ~ a + b, df, mean)
>    a b value
> 1 1 1   1.5
> 2 2 2   3.0
> 3 2 3   4.0
>> aggregate(value ~ a + b, df, list)
>    a b value
> 1 1 1  1, 2
> 2 2 2     3
> 3 2 3     4
> 
> but handling several value-like columns would be hard(?)
> 
> Using library(dplyr), I have
> 
>> group_by(df, a, b) %>% summarize(mean_value = mean(value))
> # A tibble: 3 x 3
> # Groups:   a [2]
>        a     b mean_value
>    <dbl> <dbl>      <dbl>
> 1     1     1        1.5
> 2     2     2        3
> 3     2     3        4
> 
> or
> 
>> group_by(df, a, b) %>% summarize(values = list(value))
> # A tibble: 3 x 3
> # Groups:   a [2]
>        a     b values
>    <dbl> <dbl> <list>
> 1     1     1 <int [2]>
> 2     2     2 <int [1]>
> 3     2     3 <int [1]>
> 
> summarizing multiple columns with dplyr
> 
>> df$v1 = 1:4
>> df$v2 = 4:1
>>   group_by(df, a, b) %>% summarize(v1_mean = mean(v1), v2_median = median(v2))
> # A tibble: 3 x 4
> # Groups:   a [2]
>        a     b v1_mean v2_median
>    <dbl> <dbl>   <dbl>     <dbl>
> 1     1     1     1.5       3.5
> 2     2     2     3         2
> 3     2     3     4         1
> 
> I do not know how performant this would be with data of your size.
> 
> Martin Morgan
> 
> ?On 11/8/19, 1:39 PM, "R-help on behalf of Ana Marija" <r-help-bounces at r-project.org on behalf of sokovic.anamarija at gmail.com> wrote:
> 
>      Thank you so much!!!
>      
>      On Fri, Nov 8, 2019 at 11:40 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>      >
>      > Correction:
>      > df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
>      > df[!duplicated(df[,2:3]), ]  ## Note the ! sign
>      >
>      > Bert Gunter
>      >
>      > "The trouble with having an open mind is that people keep coming along and sticking things into it."
>      > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>      >
>      >
>      > On Fri, Nov 8, 2019 at 7:59 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>      >>
>      >> Sorry, but you ask basic questions.You really need to spend some more time with an R tutorial or two. This list is not meant to replace your own learning efforts.
>      >>
>      >> You also do not seem to be reading the docs carefully. Under ?unique, it links ?duplicated and tells you that it gives indices of duplicated rows of a data frame. These then can be used by subscripting to remove those rows from the data frame. Here is a reproducible example:
>      >>
>      >> df <- data.frame(a = 1:3, b = letters[c(1,1,2)], d = LETTERS[c(1,1,2)])
>      >> df[-duplicated(df[,2:3]), ]  ## Note the - sign
>      >>
>      >> If you prefer, the "Tidyverse" world has what are purported to be more user-friendly versions of such data handling functionality that you can use instead.
>      >>
>      >>
>      >> Bert
>      >>
>      >> On Fri, Nov 8, 2019 at 7:38 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>      >>>
>      >>> would you know how would I extract from my original data frame, just
>      >>> these unique rows?
>      >>> because this gives me only those 3 columns, and I want all columns
>      >>> from the original data frame
>      >>>
>      >>> > head(udt)
>      >>>    chr   pos         gene_id
>      >>> 1 chr1 54490 ENSG00000227232
>      >>> 2 chr1 58814 ENSG00000227232
>      >>> 3 chr1 60351 ENSG00000227232
>      >>> 4 chr1 61920 ENSG00000227232
>      >>> 5 chr1 63671 ENSG00000227232
>      >>> 6 chr1 64931 ENSG00000227232
>      >>>
>      >>> > head(dt)
>      >>>     chr   pos         gene_id pval_nominal pval_ret       wl      wr      META
>      >>> 1: chr1 54490 ENSG00000227232     0.608495 0.783778 31.62278 21.2838 0.7475480
>      >>> 2: chr1 58814 ENSG00000227232     0.295211 0.897582 31.62278 21.2838 0.6031214
>      >>> 3: chr1 60351 ENSG00000227232     0.439788 0.867959 31.62278 21.2838 0.6907182
>      >>> 4: chr1 61920 ENSG00000227232     0.319528 0.601809 31.62278 21.2838 0.4032200
>      >>> 5: chr1 63671 ENSG00000227232     0.237739 0.988039 31.62278 21.2838 0.7482519
>      >>> 6: chr1 64931 ENSG00000227232     0.276679 0.907037 31.62278 21.2838 0.5974800
>      >>>
>      >>> On Fri, Nov 8, 2019 at 9:30 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>      >>> >
>      >>> > Thank you so much! Converting it to data frame resolved the issue!
>      >>> >
>      >>> > On Fri, Nov 8, 2019 at 9:19 AM Gerrit Eichner
>      >>> > <gerrit.eichner at math.uni-giessen.de> wrote:
>      >>> > >
>      >>> > > It seems as if dt is not a (base R) data frame but a
>      >>> > > data table. I assume, you will have to transform dt
>      >>> > > into a data frame (maybe with as.data.frame) to be
>      >>> > > able to apply unique in the suggested way. However,
>      >>> > > I am not familiar with data tables. Perhaps somebody
>      >>> > > else can provide a more profound guess.
>      >>> > >
>      >>> > >   Regards  --  Gerrit
>      >>> > >
>      >>> > > ---------------------------------------------------------------------
>      >>> > > Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>      >>> > > gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>      >>> > > Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>      >>> > > http://www.uni-giessen.de/eichner
>      >>> > > ---------------------------------------------------------------------
>      >>> > >
>      >>> > > Am 08.11.2019 um 16:02 schrieb Ana Marija:
>      >>> > > > I tried it but I got this error:
>      >>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
>      >>> > > > Error in `[.data.table`(dt, c("chr", "pos", "gene_id")) :
>      >>> > > >    When i is a data.table (or character vector), the columns to join by
>      >>> > > > must be specified using 'on=' argument (see ?data.table), by keying x
>      >>> > > > (i.e. sorted, and, marked as sorted, see ?setkey), or by sharing
>      >>> > > > column names between x and i (i.e., a natural join). Keyed joins might
>      >>> > > > have further speed benefits on very large data due to x being sorted
>      >>> > > > in RAM.
>      >>> > > >
>      >>> > > > On Fri, Nov 8, 2019 at 8:58 AM Gerrit Eichner
>      >>> > > > <gerrit.eichner at math.uni-giessen.de> wrote:
>      >>> > > >>
>      >>> > > >> Hi, Ana,
>      >>> > > >>
>      >>> > > >> doesn't
>      >>> > > >>
>      >>> > > >> udt <- unique(dt[c("chr", "pos", "gene_id")])
>      >>> > > >> nrow(udt)
>      >>> > > >>
>      >>> > > >> get close to what you want?
>      >>> > > >>
>      >>> > > >>    Hth  --  Gerrit
>      >>> > > >>
>      >>> > > >> ---------------------------------------------------------------------
>      >>> > > >> Dr. Gerrit Eichner                   Mathematical Institute, Room 212
>      >>> > > >> gerrit.eichner at math.uni-giessen.de   Justus-Liebig-University Giessen
>      >>> > > >> Tel: +49-(0)641-99-32104          Arndtstr. 2, 35392 Giessen, Germany
>      >>> > > >> http://www.uni-giessen.de/eichner
>      >>> > > >> ---------------------------------------------------------------------
>      >>> > > >>
>      >>> > > >> Am 08.11.2019 um 15:38 schrieb Ana Marija:
>      >>> > > >>> Hello,
>      >>> > > >>>
>      >>> > > >>> I have a data frame like this:
>      >>> > > >>>
>      >>> > > >>>> head(dt,20)
>      >>> > > >>>        chr    pos         gene_id pval_nominal  pval_ret       wl      wr
>      >>> > > >>>    1: chr1  54490 ENSG00000227232    0.6084950 0.7837780 31.62278 21.2838
>      >>> > > >>>    2: chr1  58814 ENSG00000227232    0.2952110 0.8975820 31.62278 21.2838
>      >>> > > >>>    3: chr1  60351 ENSG00000227232    0.4397880 0.8679590 31.62278 21.2838
>      >>> > > >>>    4: chr1  61920 ENSG00000227232    0.3195280 0.6018090 31.62278 21.2838
>      >>> > > >>>    5: chr1  63671 ENSG00000227232    0.2377390 0.9880390 31.62278 21.2838
>      >>> > > >>>    6: chr1  64931 ENSG00000227232    0.2766790 0.9070370 31.62278 21.2838
>      >>> > > >>>    7: chr1  81587 ENSG00000227232    0.6057930 0.6167630 31.62278 21.2838
>      >>> > > >>>    8: chr1 115746 ENSG00000227232    0.4078770 0.7799110 31.62278 21.2838
>      >>> > > >>>    9: chr1 135203 ENSG00000227232    0.4078770 0.9299130 31.62278 21.2838
>      >>> > > >>> 10: chr1 138593 ENSG00000227232    0.8464560 0.5696060 31.62278 21.2838
>      >>> > > >>>
>      >>> > > >>> it is very big,
>      >>> > > >>>> dim(dt)
>      >>> > > >>> [1] 73719122        8
>      >>> > > >>>
>      >>> > > >>> To count number of unique rows for all 3 columns: chr, pos and gene_id
>      >>> > > >>> I could just join those 3 columns and than count. But how would I find
>      >>> > > >>> unique number of rows for these 4 columns without joining them?
>      >>> > > >>>
>      >>> > > >>> Thanks
>      >>> > > >>> Ana
>      >>> > > >>>
>      >>> > > >>> ______________________________________________
>      >>> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      >>> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>      >>> > > >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>      >>> > > >>> and provide commented, minimal, self-contained, reproducible code.
>      >>> > > >>>
>      >>> > > >>
>      >>> > > >> ______________________________________________
>      >>> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      >>> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
>      >>> > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>      >>> > > >> and provide commented, minimal, self-contained, reproducible code.
>      >>>
>      >>> ______________________________________________
>      >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      >>> https://stat.ethz.ch/mailman/listinfo/r-help
>      >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>      >>> and provide commented, minimal, self-contained, reproducible code.
>      
>      ______________________________________________
>      R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>      https://stat.ethz.ch/mailman/listinfo/r-help
>      PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>      and provide commented, minimal, self-contained, reproducible code.
>      
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ||@her @end|ng |rom p|e@@th@n@com  Sat Nov  9 17:51:43 2019
From: ||@her @end|ng |rom p|e@@th@n@com (Dennis Fisher)
Date: Sat, 9 Nov 2019 08:51:43 -0800
Subject: [R] Identifying presence of Java
Message-ID: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>

R 3.6.3
OSX and Windows

Colleagues

I want to identify if Java is installed on a particular computer.

When I execute
	Java -version
in a terminal (OSX), but not in R, there are two outcomes:
	
Java installed yields:
	java version "13.0.1" 2019-10-15 
	Java(TM) SE Runtime Environment (build 13.0.1+9) 
	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing) 

Java not installed yields:
	No Java runtime installed

I assume that there are comparable outputs in Windows.

I would like to capture this output in R using the system command, then search for ?No Java runtime installed? )or the correponding text in Windows).

I execute something like:
	CAPTURE	<- system("Java -version", intern=TRUE, ignore.stderr=FALSE, ignore.stdout=FALSE)	
(with various permutations of TRUE/FALSE for the options) but I cannot capture what is displayed on the console.

I have also tried ?system2? with various TRUE/FALSE permutations without success.

Any clever ideas?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Sat Nov  9 18:28:34 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Sat, 09 Nov 2019 09:28:34 -0800
Subject: [R] Identifying presence of Java
In-Reply-To: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
Message-ID: <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>

I am mystified by your description.

A) If java is not installed, the operating system or system shell will be the source of any error associated with attempting to invoke it. That means the error message could be anything, but I find it quite surprising that the message emitted by the OS would mention "runtime". In any event, look for the output you can rely on from java for positive identification rather than looking for any consistent pattern in the error messages for negative confirmation.

B) Many OS command shells are case-sensitive... asking for the "Java" program is in that case different than asking for the "java" program.

On November 9, 2019 8:51:43 AM PST, Dennis Fisher <fisher at plessthan.com> wrote:
>R 3.6.3
>OSX and Windows
>
>Colleagues
>
>I want to identify if Java is installed on a particular computer.
>
>When I execute
>	Java -version
>in a terminal (OSX), but not in R, there are two outcomes:
>	
>Java installed yields:
>	java version "13.0.1" 2019-10-15 
>	Java(TM) SE Runtime Environment (build 13.0.1+9) 
>	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode,
>sharing) 
>
>Java not installed yields:
>	No Java runtime installed
>
>I assume that there are comparable outputs in Windows.
>
>I would like to capture this output in R using the system command, then
>search for ?No Java runtime installed? )or the correponding text in
>Windows).
>
>I execute something like:
>	CAPTURE	<- system("Java -version", intern=TRUE, ignore.stderr=FALSE,
>ignore.stdout=FALSE)	
>(with various permutations of TRUE/FALSE for the options) but I cannot
>capture what is displayed on the console.
>
>I have also tried ?system2? with various TRUE/FALSE permutations
>without success.
>
>Any clever ideas?
>
>Dennis
>
>Dennis Fisher MD
>P < (The "P Less Than" Company)
>Phone / Fax: 1-866-PLessThan (1-866-753-7784)
>www.PLessThan.com
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ||@her @end|ng |rom p|e@@th@n@com  Sat Nov  9 18:33:30 2019
From: ||@her @end|ng |rom p|e@@th@n@com (Dennis Fisher)
Date: Sat, 9 Nov 2019 09:33:30 -0800
Subject: [R] Identifying presence of Java
In-Reply-To: <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>
Message-ID: <74A10226-0649-42B0-9EB6-2E88D6BC2A7D@plessthan.com>

Jeff

A.  I can certainly look for the output from Java ? but that was not the point ? nothing was captured to CAPTURE with either scenarios.
B.  I tried changing case ? that did not solve the problem.

The issue remains ? when I execute the system command, the text output that I presented below is not being displayed.  
Is there a way to capture that output?  If so, I can certainly figure out how to parse it.

Dennis


Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com <http://www.plessthan.com/>




> On Nov 9, 2019, at 9:28 AM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
> 
> I am mystified by your description.
> 
> A) If java is not installed, the operating system or system shell will be the source of any error associated with attempting to invoke it. That means the error message could be anything, but I find it quite surprising that the message emitted by the OS would mention "runtime". In any event, look for the output you can rely on from java for positive identification rather than looking for any consistent pattern in the error messages for negative confirmation.
> 
> B) Many OS command shells are case-sensitive... asking for the "Java" program is in that case different than asking for the "java" program.
> 
> On November 9, 2019 8:51:43 AM PST, Dennis Fisher <fisher at plessthan.com> wrote:
>> R 3.6.3
>> OSX and Windows
>> 
>> Colleagues
>> 
>> I want to identify if Java is installed on a particular computer.
>> 
>> When I execute
>> 	Java -version
>> in a terminal (OSX), but not in R, there are two outcomes:
>> 	
>> Java installed yields:
>> 	java version "13.0.1" 2019-10-15 
>> 	Java(TM) SE Runtime Environment (build 13.0.1+9) 
>> 	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode,
>> sharing) 
>> 
>> Java not installed yields:
>> 	No Java runtime installed
>> 
>> I assume that there are comparable outputs in Windows.
>> 
>> I would like to capture this output in R using the system command, then
>> search for ?No Java runtime installed? )or the correponding text in
>> Windows).
>> 
>> I execute something like:
>> 	CAPTURE	<- system("Java -version", intern=TRUE, ignore.stderr=FALSE,
>> ignore.stdout=FALSE)	
>> (with various permutations of TRUE/FALSE for the options) but I cannot
>> capture what is displayed on the console.
>> 
>> I have also tried ?system2? with various TRUE/FALSE permutations
>> without success.
>> 
>> Any clever ideas?
>> 
>> Dennis
>> 
>> Dennis Fisher MD
>> P < (The "P Less Than" Company)
>> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
>> www.PLessThan.com
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> -- 
> Sent from my phone. Please excuse my brevity.


	[[alternative HTML version deleted]]


From edd @end|ng |rom deb|@n@org  Sat Nov  9 18:35:27 2019
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 9 Nov 2019 11:35:27 -0600
Subject: [R] Identifying presence of Java
In-Reply-To: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
Message-ID: <24006.63711.211807.560039@rob.eddelbuettel.com>


Dennis,

R does that for you already as it needs to know it for rJava too.

On my (Linux) box:

  edd at rob:~$ R CMD config JAVA
  /usr/lib/jvm/default-java/bin/java
  edd at rob:~$ R CMD config JAVA_HOME
  /usr/lib/jvm/default-java
  edd at rob:~$ 

You could also do the equivalent of `which` or `type -p` from R:

  R> Sys.which("javac")
             javac 
  "/usr/bin/javac" 
  R> Sys.which("javacdoesnotexist")
  javacdoesnotexist 
               "" 
  R> 

Hope this helps, Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Nov  9 18:37:54 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 Nov 2019 17:37:54 +0000
Subject: [R] Identifying presence of Java
In-Reply-To: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
Message-ID: <a74495c5-f644-400d-48e5-cfb06ceb925b@sapo.pt>

Hello,

Here are two ways.

1. system2 returns the return value of the shell command so if Java is 
installed, it should return 0. In the first call it's 'java' (lowercase 
j), in the second 'Java' (uppercase J).

java <- system2('java', '-version')
java
#[1] 0

Java <- system2('Java', '-version')
Java
#[1] 127


2.
Another way is to redirect the shell output to file. In this case I'm 
redirecting to a temp file. Then, readLines from that file.

tmpfile <- tempfile()
if(!dir.exists(dirname(tmpfile))) dir.create(dirname(tmpfile))

system2('java', '-version', '>', tmpfile)
readLines(tmpfile)

unlink(tmpfile)  # final cleanup


Hope this helps,

Rui Barradas


?s 16:51 de 09/11/19, Dennis Fisher escreveu:
> R 3.6.3
> OSX and Windows
> 
> Colleagues
> 
> I want to identify if Java is installed on a particular computer.
> 
> When I execute
> 	Java -version
> in a terminal (OSX), but not in R, there are two outcomes:
> 	
> Java installed yields:
> 	java version "13.0.1" 2019-10-15
> 	Java(TM) SE Runtime Environment (build 13.0.1+9)
> 	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)
> 
> Java not installed yields:
> 	No Java runtime installed
> 
> I assume that there are comparable outputs in Windows.
> 
> I would like to capture this output in R using the system command, then search for ?No Java runtime installed? )or the correponding text in Windows).
> 
> I execute something like:
> 	CAPTURE	<- system("Java -version", intern=TRUE, ignore.stderr=FALSE, ignore.stdout=FALSE)	
> (with various permutations of TRUE/FALSE for the options) but I cannot capture what is displayed on the console.
> 
> I have also tried ?system2? with various TRUE/FALSE permutations without success.
> 
> Any clever ideas?
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Nov  9 18:46:54 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 9 Nov 2019 12:46:54 -0500
Subject: [R] Identifying presence of Java
In-Reply-To: <74A10226-0649-42B0-9EB6-2E88D6BC2A7D@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>
 <74A10226-0649-42B0-9EB6-2E88D6BC2A7D@plessthan.com>
Message-ID: <0dd37518-3ab3-3b83-f905-2dae97f9640e@gmail.com>

On 09/11/2019 12:33 p.m., Dennis Fisher wrote:
> Jeff
> 
> A.  I can certainly look for the output from Java ? but that was not the point ? nothing was captured to CAPTURE with either scenarios.
> B.  I tried changing case ? that did not solve the problem.
> 
> The issue remains ? when I execute the system command, the text output that I presented below is not being displayed.
> Is there a way to capture that output?  If so, I can certainly figure out how to parse it.

These seem to work for me:

  system("Java -version 2>&1", intern = TRUE)

  system2("Java","-version", stdout = TRUE, stderr = TRUE)

These are based on reading the help pages, which sometimes helps.

Duncan Murdoch


From ccberry @end|ng |rom uc@d@edu  Sat Nov  9 18:56:14 2019
From: ccberry @end|ng |rom uc@d@edu (Berry, Charles)
Date: Sat, 9 Nov 2019 17:56:14 +0000
Subject: [R] Identifying presence of Java
In-Reply-To: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
Message-ID: <A4803707-B1D4-41E3-9DD8-402ECC38B5D3@ucsd.edu>



> On Nov 9, 2019, at 8:51 AM, Dennis Fisher <fisher at plessthan.com> wrote:
> 
> R 3.6.3
> OSX and Windows
> 
> Colleagues
> 
> I want to identify if Java is installed on a particular computer.
> 


[...]

> I execute something like:
> 	CAPTURE	<- system("Java -version", intern=TRUE, ignore.stderr=FALSE, ignore.stdout=FALSE)	
> (with various permutations of TRUE/FALSE for the options) but I cannot capture what is displayed on the console.
> 
> I have also tried ?system2? with various TRUE/FALSE permutations without success.
> 
> Any clever ideas?
> 

?Sys.which

HTH,

CCB


From ||@her @end|ng |rom p|e@@th@n@com  Sat Nov  9 19:53:36 2019
From: ||@her @end|ng |rom p|e@@th@n@com (Dennis Fisher)
Date: Sat, 9 Nov 2019 10:53:36 -0800
Subject: [R] Identifying presence of Java
In-Reply-To: <24006.63711.211807.560039@rob.eddelbuettel.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <24006.63711.211807.560039@rob.eddelbuettel.com>
Message-ID: <1D7426B2-4BBF-44C7-ADB5-DEE7FF0C21EB@plessthan.com>

Dirk

I am now more confused (and I appreciate your help in sorting this out).  

I executed
	require(?rJava?)
and received the following error:

> Error: package or namespace load failed for ?rJava?:
>  .onLoad failed in loadNamespace() for 'rJava', details:
>   call: dyn.load(file, DLLpath = DLLpath, ...)
>   error: unable to load shared object '/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so':
>   dlopen(/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so, 6): Library not loaded: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/Home/lib/server/libjvm.dylib
>   Referenced from: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so
>   Reason: image not found
> 
Java is definitely installed on this machine ? from a terminal:
	PET513> Java -version
	java version "13.0.1" 2019-10-15
	Java(TM) SE Runtime Environment (build 13.0.1+9)
	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)

It appears that the problem relates to version # ? I have 13.03.1 installed whereas R is looking for 11.0.1

Any idea how to fix this?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com




> On Nov 9, 2019, at 9:35 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
> 
> 
> Dennis,
> 
> R does that for you already as it needs to know it for rJava too.
> 
> On my (Linux) box:
> 
>  edd at rob:~$ R CMD config JAVA
>  /usr/lib/jvm/default-java/bin/java
>  edd at rob:~$ R CMD config JAVA_HOME
>  /usr/lib/jvm/default-java
>  edd at rob:~$ 
> 
> You could also do the equivalent of `which` or `type -p` from R:
> 
>  R> Sys.which("javac")
>             javac 
>  "/usr/bin/javac" 
>  R> Sys.which("javacdoesnotexist")
>  javacdoesnotexist 
>               "" 
>  R> 
> 
> Hope this helps, Dirk
> 
> -- 
> http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From @purd|e@@ @end|ng |rom gm@||@com  Sat Nov  9 22:24:34 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 10 Nov 2019 10:24:34 +1300
Subject: [R] Identifying presence of Java
In-Reply-To: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
Message-ID: <CAB8pepwkp0ajEXre_YtVZOxf50azsx3CSweZWjc+2w-Xd+2GDg@mail.gmail.com>

> I would like to capture this output in R using the system command, then search for ?No Java runtime installed? )or the correponding text in Windows).
>
> I execute something like:
>         CAPTURE <- system("Java -version", intern=TRUE, ignore.stderr=FALSE, ignore.stdout=FALSE)
> (with various permutations of TRUE/FALSE for the options) but I cannot capture what is displayed on the console.

That's fascinating...
I'm offsite, I'll try it tomorrow...

Two questions:
(1) What is the value of CAPTURE (or does it hang)?
(2) Can you run other java commands ("java MyClass", "javac MyClass.java", etc)?

Another thing, did you try Duncan's suggestion?
If that doesn't work, another possibility is to pipe the output, to a
command line app that echos the text.


From @purd|e@@ @end|ng |rom gm@||@com  Sat Nov  9 23:06:15 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Sun, 10 Nov 2019 11:06:15 +1300
Subject: [R] Identifying presence of Java
In-Reply-To: <CAB8pepwkp0ajEXre_YtVZOxf50azsx3CSweZWjc+2w-Xd+2GDg@mail.gmail.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <CAB8pepwkp0ajEXre_YtVZOxf50azsx3CSweZWjc+2w-Xd+2GDg@mail.gmail.com>
Message-ID: <CAB8pepzrK3kz8phfsjoNtAg+3k4efG0hgH9AkmNt08+Nf3Csxg@mail.gmail.com>

> (2) Can you run other java commands ("java MyClass", "javac MyClass.java", etc)?

Sorry, I meant to say, can you capture the output from other jdk commands?


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Nov  9 23:21:55 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 9 Nov 2019 22:21:55 +0000
Subject: [R] Identifying presence of Java
In-Reply-To: <1D7426B2-4BBF-44C7-ADB5-DEE7FF0C21EB@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <24006.63711.211807.560039@rob.eddelbuettel.com>
 <1D7426B2-4BBF-44C7-ADB5-DEE7FF0C21EB@plessthan.com>
Message-ID: <022a423d-e814-06b6-f299-9d14419631d7@sapo.pt>

Hello,

This error can be a 32 bits vs 64 bits thing. R and java must have the 
same architecture. If you have 64b Java installed, don't run 32b R. And 
vice-versa.

Hope this helps,

Rui Barradas

?s 18:53 de 09/11/19, Dennis Fisher escreveu:
> Dirk
> 
> I am now more confused (and I appreciate your help in sorting this out).
> 
> I executed
> 	require(?rJava?)
> and received the following error:
> 
>> Error: package or namespace load failed for ?rJava?:
>>   .onLoad failed in loadNamespace() for 'rJava', details:
>>    call: dyn.load(file, DLLpath = DLLpath, ...)
>>    error: unable to load shared object '/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so':
>>    dlopen(/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so, 6): Library not loaded: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/Home/lib/server/libjvm.dylib
>>    Referenced from: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so
>>    Reason: image not found
>>
> Java is definitely installed on this machine ? from a terminal:
> 	PET513> Java -version
> 	java version "13.0.1" 2019-10-15
> 	Java(TM) SE Runtime Environment (build 13.0.1+9)
> 	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)
> 
> It appears that the problem relates to version # ? I have 13.03.1 installed whereas R is looking for 11.0.1
> 
> Any idea how to fix this?
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com
> 
> 
> 
> 
>> On Nov 9, 2019, at 9:35 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>>
>>
>> Dennis,
>>
>> R does that for you already as it needs to know it for rJava too.
>>
>> On my (Linux) box:
>>
>>   edd at rob:~$ R CMD config JAVA
>>   /usr/lib/jvm/default-java/bin/java
>>   edd at rob:~$ R CMD config JAVA_HOME
>>   /usr/lib/jvm/default-java
>>   edd at rob:~$
>>
>> You could also do the equivalent of `which` or `type -p` from R:
>>
>>   R> Sys.which("javac")
>>              javac
>>   "/usr/bin/javac"
>>   R> Sys.which("javacdoesnotexist")
>>   javacdoesnotexist
>>                ""
>>   R>
>>
>> Hope this helps, Dirk
>>
>> -- 
>> http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From edd @end|ng |rom deb|@n@org  Sun Nov 10 00:20:13 2019
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 9 Nov 2019 17:20:13 -0600
Subject: [R] Identifying presence of Java
In-Reply-To: <1D7426B2-4BBF-44C7-ADB5-DEE7FF0C21EB@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <24006.63711.211807.560039@rob.eddelbuettel.com>
 <1D7426B2-4BBF-44C7-ADB5-DEE7FF0C21EB@plessthan.com>
Message-ID: <24007.18861.472653.990946@rob.eddelbuettel.com>


Dennis,

On 9 November 2019 at 10:53, Dennis Fisher wrote:
| Dirk
| 
| I am now more confused (and I appreciate your help in sorting this out).  
| 
| I executed
| 	require(?rJava?)
| and received the following error:
| 
| > Error: package or namespace load failed for ?rJava?:
| >  .onLoad failed in loadNamespace() for 'rJava', details:
| >   call: dyn.load(file, DLLpath = DLLpath, ...)
| >   error: unable to load shared object '/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so':
| >   dlopen(/Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so, 6): Library not loaded: /Library/Java/JavaVirtualMachines/jdk-11.0.1.jdk/Contents/Home/lib/server/libjvm.dylib
| >   Referenced from: /Library/Frameworks/R.framework/Versions/3.6/Resources/library/rJava/libs/rJava.so
| >   Reason: image not found
| > 
| Java is definitely installed on this machine ? from a terminal:
| 	PET513> Java -version
| 	java version "13.0.1" 2019-10-15
| 	Java(TM) SE Runtime Environment (build 13.0.1+9)
| 	Java HotSpot(TM) 64-Bit Server VM (build 13.0.1+9, mixed mode, sharing)
| 
| It appears that the problem relates to version # ? I have 13.03.1 installed whereas R is looking for 11.0.1
| 
| Any idea how to fix this?

I have seen this happen when the Java settings changed from a different
version which can happen.  The common fix is to

   R CMD javareconf

to updated R's settings about Java. (This step also happens when you rebuild
R; but rebuilding R is typically not required; we just want updated Java
settings).

If that is done, and your Java installation is otherwise sufficient then

   R CMD INSTALL rJava_*tar.gz

(or equally from within R)  should do the trick.

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From @purd|e@@ @end|ng |rom gm@||@com  Sun Nov 10 20:49:19 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Mon, 11 Nov 2019 08:49:19 +1300
Subject: [R] Identifying presence of Java
In-Reply-To: <0dd37518-3ab3-3b83-f905-2dae97f9640e@gmail.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>
 <74A10226-0649-42B0-9EB6-2E88D6BC2A7D@plessthan.com>
 <0dd37518-3ab3-3b83-f905-2dae97f9640e@gmail.com>
Message-ID: <CAB8pepyWbXsaPMk_jPA8yhpZHRnxur=QhMu=d5FOyHv2XoC4zg@mail.gmail.com>

> These seem to work for me:
>   system("Java -version 2>&1", intern = TRUE)
>
>   system2("Java","-version", stdout = TRUE, stderr = TRUE)
>

Hi Dennis

I tried your example and Duncan's examples.
In your example, I get zero.
The second of Duncan's examples worked for me, but not the first.

I've got a suspicion that java is writing in unicode, but R is
expecting another format, however, I could be completely wrong on
that.

The following also worked for me:
CAPTURE = shell ("Java -version 2>&1", intern=TRUE)

Abs


From ||@her @end|ng |rom p|e@@th@n@com  Sun Nov 10 21:26:23 2019
From: ||@her @end|ng |rom p|e@@th@n@com (Dennis Fisher)
Date: Sun, 10 Nov 2019 12:26:23 -0800
Subject: [R] Identifying presence of Java
In-Reply-To: <CAB8pepyWbXsaPMk_jPA8yhpZHRnxur=QhMu=d5FOyHv2XoC4zg@mail.gmail.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>
 <74A10226-0649-42B0-9EB6-2E88D6BC2A7D@plessthan.com>
 <0dd37518-3ab3-3b83-f905-2dae97f9640e@gmail.com>
 <CAB8pepyWbXsaPMk_jPA8yhpZHRnxur=QhMu=d5FOyHv2XoC4zg@mail.gmail.com>
Message-ID: <A24C8E1C-6E2A-4237-A588-569923BC53D7@plessthan.com>

Abby

I assume that your OS in Windows ? shell is Windows only.  Fortunately, it appears that the changing ?shell? to ?system? works in OS X.
So, now I have a solution for both OSs.  

This issue was a prelude to a larger issue that I will address in a separate email.

Thanks to everyone for their suggestions.

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com <http://www.plessthan.com/>




> On Nov 10, 2019, at 11:49 AM, Abby Spurdle <spurdle.a at gmail.com> wrote:
> 
>  shell ("Java -version 2>&1", intern=TRUE)


	[[alternative HTML version deleted]]


From @|mon@urb@nek @end|ng |rom R-project@org  Mon Nov 11 06:33:38 2019
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Mon, 11 Nov 2019 00:33:38 -0500
Subject: [R] Identifying presence of Java
In-Reply-To: <A24C8E1C-6E2A-4237-A588-569923BC53D7@plessthan.com>
References: <381D52CE-748D-4626-A01B-100F9C84B605@plessthan.com>
 <220150D3-39BF-4051-B81E-FC95B840119B@dcn.davis.ca.us>
 <74A10226-0649-42B0-9EB6-2E88D6BC2A7D@plessthan.com>
 <0dd37518-3ab3-3b83-f905-2dae97f9640e@gmail.com>
 <CAB8pepyWbXsaPMk_jPA8yhpZHRnxur=QhMu=d5FOyHv2XoC4zg@mail.gmail.com>
 <A24C8E1C-6E2A-4237-A588-569923BC53D7@plessthan.com>
Message-ID: <ECE6FCD3-219C-494F-BFD0-EBF2A0ED6D85@R-project.org>

FWIW you cannot assume that java will be on the PATH - especially on Windows it typically is not. That?s why rJava uses registry on Windows to find the Java location. For other platforms there is
R CMD config JAVA

Cheers,
Simon


> On Nov 10, 2019, at 3:26 PM, Dennis Fisher <fisher at plessthan.com> wrote:
> 
> Abby
> 
> I assume that your OS in Windows ? shell is Windows only.  Fortunately, it appears that the changing ?shell? to ?system? works in OS X.
> So, now I have a solution for both OSs.  
> 
> This issue was a prelude to a larger issue that I will address in a separate email.
> 
> Thanks to everyone for their suggestions.
> 
> Dennis
> 
> Dennis Fisher MD
> P < (The "P Less Than" Company)
> Phone / Fax: 1-866-PLessThan (1-866-753-7784)
> www.PLessThan.com <http://www.plessthan.com/>
> 
> 
> 
> 
>> On Nov 10, 2019, at 11:49 AM, Abby Spurdle <spurdle.a at gmail.com> wrote:
>> 
>> shell ("Java -version 2>&1", intern=TRUE)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


From murphymv @end|ng |rom gm@||@com  Fri Nov  8 17:48:55 2019
From: murphymv @end|ng |rom gm@||@com (Mark Murphy)
Date: Fri, 8 Nov 2019 16:48:55 +0000
Subject: [R] [R-pkgs] New package 'semEff'
Message-ID: <CAJ+Uk2V_9fZD+bJ+s5DG3Yto0tPLCO-fD_qLS7Vb60m+HGX7Sg@mail.gmail.com>

 Hi,

I have a new package <https://www.rdocumentation.org/packages/semEff>
available for automatic effect calculation for 'piecewise' structural
equation models (SEM), commonly used in ecology and evolution. Confidence
intervals are provided via bootstrapping. It also does some useful things
for linear and generalised linear (mixed) models, like standardised
coefficients, R-squared, etc. Feedback/bug reports welcome. This is my
first CRAN release.

Cheers,
Mark
-- 
*Mark V. Murphy*
*Community Ecologist*
murphymv at gmail.com
* <https://orcid.org/0000-0001-9178-0871>*
ORCID <https://orcid.org/0000-0001-9178-0871> | ResearcherID
<https://publons.com/researcher/2970870/mark-murphy/> | ResearchGate
<https://www.researchgate.net/profile/Mark_Murphy7> | Linkedin
<https://www.linkedin.com/in/murphymv> | Twitter
<https://twitter.com/murphy_mv> | GitHub <https://github.com/murphymv>

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From j@vedbtk111 @end|ng |rom gm@||@com  Mon Nov 11 16:57:19 2019
From: j@vedbtk111 @end|ng |rom gm@||@com (javed khan)
Date: Mon, 11 Nov 2019 16:57:19 +0100
Subject: [R] Long delay due to warnings
Message-ID: <CAJhui+v_tgZ1yaHF=BWKq762E_Cqme4GNWSubd8ZqijXs_ekng@mail.gmail.com>

Hi

I run genetic algorithm for hyperparameter tuning and when I execute it, it
goes to a long waiting, giving warnings like pre processing warnings, these
variables have variances, something like this. My question is can we avoid
these warnings?

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Nov 11 17:12:00 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 11 Nov 2019 08:12:00 -0800
Subject: [R] Long delay due to warnings
In-Reply-To: <CAJhui+v_tgZ1yaHF=BWKq762E_Cqme4GNWSubd8ZqijXs_ekng@mail.gmail.com>
References: <CAJhui+v_tgZ1yaHF=BWKq762E_Cqme4GNWSubd8ZqijXs_ekng@mail.gmail.com>
Message-ID: <88BAA1E9-9D3C-488F-9993-44F8BEDA8370@dcn.davis.ca.us>

Your question is rather too vague to address clearly. (You also need to figure out how to post plain text format or a more precise question will be blendered by the html formatting. Do read the Posting Guide, though controlling your email client is your problem to solve.)

In general, with genetic algorithms for efficiency you need to use domain-specific intelligent methods to minimize the number of invalid parameter combinations generated. We know nothing about your problem domain, and even if we did it would likely be too off-topic to get into that here.

On November 11, 2019 7:57:19 AM PST, javed khan <javedbtk111 at gmail.com> wrote:
>Hi
>
>I run genetic algorithm for hyperparameter tuning and when I execute
>it, it
>goes to a long waiting, giving warnings like pre processing warnings,
>these
>variables have variances, something like this. My question is can we
>avoid
>these warnings?
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Mon Nov 11 23:48:37 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Mon, 11 Nov 2019 16:48:37 -0600
Subject: [R] QQ plot
Message-ID: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>

Hi,

I was using this library, qqman
https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html

to create QQ plot, attached. How would I change this default abline to
start from the beginning of my QQ line?

This is my code:
qq(dd$P, main = "Q-Q plot of GWAS p-values")

Thanks
Ana

-------------- next part --------------
A non-text attachment was scrubbed...
Name: Screen Shot 2019-11-11 at 4.48.13 PM.png
Type: image/png
Size: 101200 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20191111/d4c0c2ea/attachment.png>

From @purd|e@@ @end|ng |rom gm@||@com  Tue Nov 12 03:53:33 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Tue, 12 Nov 2019 15:53:33 +1300
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
Message-ID: <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>

Hi

I'm not familiar with the qqman package, or GWAS studies.
However, my guess would be that you're *not* supposed to change the
position of the line.

On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi,
>
> I was using this library, qqman
> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
>
> to create QQ plot, attached. How would I change this default abline to
> start from the beginning of my QQ line?
>
> This is my code:
> qq(dd$P, main = "Q-Q plot of GWAS p-values")
>
> Thanks
> Ana
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com  Tue Nov 12 14:27:23 2019
From: m@|one @end|ng |rom m@|onequ@nt|t@t|ve@com (Patrick (Malone Quantitative))
Date: Tue, 12 Nov 2019 08:27:23 -0500
Subject: [R] QQ plot
In-Reply-To: <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
Message-ID: <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>

I agree with Abby. That would defeat the purpose of a QQ plot.

On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:

> Hi
>
> I'm not familiar with the qqman package, or GWAS studies.
> However, my guess would be that you're *not* supposed to change the
> position of the line.
>
> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > Hi,
> >
> > I was using this library, qqman
> > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> >
> > to create QQ plot, attached. How would I change this default abline to
> > start from the beginning of my QQ line?
> >
> > This is my code:
> > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> >
> > Thanks
> > Ana
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From btupper @end|ng |rom b|ge|ow@org  Tue Nov 12 20:34:51 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Tue, 12 Nov 2019 14:34:51 -0500
Subject: [R] using xpath with xml2
Message-ID: <79A13D6C-8B86-4969-BAD1-F5057AF7CC1A@bigelow.org>

Hi,

I have mined XML extensively with R before now, but my xpath chops seem to be regressing recently. I know that I can roll up my sleeves and search through the child nodes of the root, but I can't noodle out why using the xpath description returns an empty nodeset.

Any suggestions and nudges most welcome.

### START

library(xml2)
library(httr)
library(magrittr)

daymet_uri <- "https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/1328/catalog.xml"

# run the following to show the node in a browser
# httr::BROWSE(daymet_uri)

daymet <- httr::GET(daymet_uri) %>%
  httr::content(type = "text/xml", encoding = "UTF-8")

# list the children "service" and "dataset"
daymet %>% xml2::xml_children()
#{xml_nodeset (2)}
#[1] <service name="all" serviceType="Compound" base="">\n  <service name="odap" #serviceTyp ...
#[2] <dataset name="Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Ve ...

# find all descendants of node name "dataset"
#
# according to this tutorial we should find 'dataset'
# https://www.w3schools.com/xml/xpath_syntax.asp
daymet %>% xml2::xml_find_all(xpath = "//dataset")
# {xml_nodeset (0)}

# I have also tried every other xpath combination I think of e.g.
#   ".//dataset", "./dataset", "/dataset" and "dataset"
# They each yield an empty nodeset

### END

> sessionInfo()

R version 3.5.1 (2018-07-02)
Platform: x86_64-redhat-linux-gnu (64-bit)
Running under: CentOS Linux 7 (Core)

Matrix products: default
BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
[1] magrittr_1.5 httr_1.4.1   xml2_1.2.2  

loaded via a namespace (and not attached):
[1] compiler_3.5.1 R6_2.4.0       tools_3.5.1    curl_4.2      
[5] yaml_2.2.0     Rcpp_1.0.3    


Thanks,
Ben

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/


From wdun|@p @end|ng |rom t|bco@com  Tue Nov 12 20:50:08 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Tue, 12 Nov 2019 11:50:08 -0800
Subject: [R] using xpath with xml2
In-Reply-To: <79A13D6C-8B86-4969-BAD1-F5057AF7CC1A@bigelow.org>
References: <79A13D6C-8B86-4969-BAD1-F5057AF7CC1A@bigelow.org>
Message-ID: <CAF8bMcb5pAT-7eUw644g-Fsyer=uRRY=Jes8W2p8P3fE-4bGfQ@mail.gmail.com>

> xml_ns(daymet)
d1    <-> http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0
xlink <-> http://www.w3.org/1999/xlink
> daymet %>% xml2::xml_find_all(xpath = "d1:dataset")
{xml_nodeset (1)}
[1] <dataset name="Daymet: Daily Surface Weather Data on a 1-km Grid for
Nort ...

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Nov 12, 2019 at 11:35 AM Ben Tupper <btupper at bigelow.org> wrote:

> Hi,
>
> I have mined XML extensively with R before now, but my xpath chops seem to
> be regressing recently. I know that I can roll up my sleeves and search
> through the child nodes of the root, but I can't noodle out why using the
> xpath description returns an empty nodeset.
>
> Any suggestions and nudges most welcome.
>
> ### START
>
> library(xml2)
> library(httr)
> library(magrittr)
>
> daymet_uri <- "
> https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/1328/catalog.xml"
>
> # run the following to show the node in a browser
> # httr::BROWSE(daymet_uri)
>
> daymet <- httr::GET(daymet_uri) %>%
>   httr::content(type = "text/xml", encoding = "UTF-8")
>
> # list the children "service" and "dataset"
> daymet %>% xml2::xml_children()
> #{xml_nodeset (2)}
> #[1] <service name="all" serviceType="Compound" base="">\n  <service
> name="odap" #serviceTyp ...
> #[2] <dataset name="Daymet: Daily Surface Weather Data on a 1-km Grid for
> North America, Ve ...
>
> # find all descendants of node name "dataset"
> #
> # according to this tutorial we should find 'dataset'
> # https://www.w3schools.com/xml/xpath_syntax.asp
> daymet %>% xml2::xml_find_all(xpath = "//dataset")
> # {xml_nodeset (0)}
>
> # I have also tried every other xpath combination I think of e.g.
> #   ".//dataset", "./dataset", "/dataset" and "dataset"
> # They each yield an empty nodeset
>
> ### END
>
> > sessionInfo()
>
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-redhat-linux-gnu (64-bit)
> Running under: CentOS Linux 7 (Core)
>
> Matrix products: default
> BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods
> [7] base
>
> other attached packages:
> [1] magrittr_1.5 httr_1.4.1   xml2_1.2.2
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.1 R6_2.4.0       tools_3.5.1    curl_4.2
> [5] yaml_2.2.0     Rcpp_1.0.3
>
>
> Thanks,
> Ben
>
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org
>
> Ecological Forecasting: https://eco.bigelow.org/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Tue Nov 12 20:56:41 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 Nov 2019 06:56:41 +1100
Subject: [R] QQ plot
In-Reply-To: <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
Message-ID: <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>

I thought about this and did a little study of GWAS and the use of
p-values to assess significant associations. As Ana's plot begins at
values of about 0.001, this seems to imply that almost everything in
the genome is associated to some degree. One expects that most SNPs
will not be associated with a particular condition (p~1), so perhaps
something is going wrong in the calculations that produce the
p-values.

Jim

On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
<malone at malonequantitative.com> wrote:
>
> I agree with Abby. That would defeat the purpose of a QQ plot.
>
> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> > Hi
> >
> > I'm not familiar with the qqman package, or GWAS studies.
> > However, my guess would be that you're *not* supposed to change the
> > position of the line.
> >
> > On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > wrote:
> > >
> > > Hi,
> > >
> > > I was using this library, qqman
> > > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > >
> > > to create QQ plot, attached. How would I change this default abline to
> > > start from the beginning of my QQ line?
> > >
> > > This is my code:
> > > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > >
> > > Thanks
> > > Ana
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From btupper @end|ng |rom b|ge|ow@org  Tue Nov 12 20:56:42 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Tue, 12 Nov 2019 14:56:42 -0500
Subject: [R] using xpath with xml2
In-Reply-To: <CAF8bMcb5pAT-7eUw644g-Fsyer=uRRY=Jes8W2p8P3fE-4bGfQ@mail.gmail.com>
References: <79A13D6C-8B86-4969-BAD1-F5057AF7CC1A@bigelow.org>
 <CAF8bMcb5pAT-7eUw644g-Fsyer=uRRY=Jes8W2p8P3fE-4bGfQ@mail.gmail.com>
Message-ID: <E2A6D480-A731-4C9F-96ED-1658AD445573@bigelow.org>

Forehead smack!  Of course!

Thank you, Bill!

> On Nov 12, 2019, at 2:50 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
> > xml_ns(daymet)
> d1    <-> http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0 <http://www.unidata.ucar.edu/namespaces/thredds/InvCatalog/v1.0>
> xlink <-> http://www.w3.org/1999/xlink <http://www.w3.org/1999/xlink>
> > daymet %>% xml2::xml_find_all(xpath = "d1:dataset")
> {xml_nodeset (1)}
> [1] <dataset name="Daymet: Daily Surface Weather Data on a 1-km Grid for Nort ...
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com/>
> 
> On Tue, Nov 12, 2019 at 11:35 AM Ben Tupper <btupper at bigelow.org <mailto:btupper at bigelow.org>> wrote:
> Hi,
> 
> I have mined XML extensively with R before now, but my xpath chops seem to be regressing recently. I know that I can roll up my sleeves and search through the child nodes of the root, but I can't noodle out why using the xpath description returns an empty nodeset.
> 
> Any suggestions and nudges most welcome.
> 
> ### START
> 
> library(xml2)
> library(httr)
> library(magrittr)
> 
> daymet_uri <- "https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/1328/catalog.xml <https://thredds.daac.ornl.gov/thredds/catalog/ornldaac/1328/catalog.xml>"
> 
> # run the following to show the node in a browser
> # httr::BROWSE(daymet_uri)
> 
> daymet <- httr::GET(daymet_uri) %>%
>   httr::content(type = "text/xml", encoding = "UTF-8")
> 
> # list the children "service" and "dataset"
> daymet %>% xml2::xml_children()
> #{xml_nodeset (2)}
> #[1] <service name="all" serviceType="Compound" base="">\n  <service name="odap" #serviceTyp ...
> #[2] <dataset name="Daymet: Daily Surface Weather Data on a 1-km Grid for North America, Ve ...
> 
> # find all descendants of node name "dataset"
> #
> # according to this tutorial we should find 'dataset'
> # https://www.w3schools.com/xml/xpath_syntax.asp <https://www.w3schools.com/xml/xpath_syntax.asp>
> daymet %>% xml2::xml_find_all(xpath = "//dataset")
> # {xml_nodeset (0)}
> 
> # I have also tried every other xpath combination I think of e.g.
> #   ".//dataset", "./dataset", "/dataset" and "dataset"
> # They each yield an empty nodeset
> 
> ### END
> 
> > sessionInfo()
> 
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-redhat-linux-gnu (64-bit)
> Running under: CentOS Linux 7 (Core)
> 
> Matrix products: default
> BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so
> 
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods  
> [7] base     
> 
> other attached packages:
> [1] magrittr_1.5 httr_1.4.1   xml2_1.2.2  
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.5.1 R6_2.4.0       tools_3.5.1    curl_4.2      
> [5] yaml_2.2.0     Rcpp_1.0.3    
> 
> 
> Thanks,
> Ben
> 
> Ben Tupper
> Bigelow Laboratory for Ocean Sciences
> 60 Bigelow Drive, P.O. Box 380
> East Boothbay, Maine 04544
> http://www.bigelow.org <http://www.bigelow.org/>
> 
> Ecological Forecasting: https://eco.bigelow.org/ <https://eco.bigelow.org/>
> 
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help>
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html>
> and provide commented, minimal, self-contained, reproducible code.

Ben Tupper
Bigelow Laboratory for Ocean Sciences
60 Bigelow Drive, P.O. Box 380
East Boothbay, Maine 04544
http://www.bigelow.org

Ecological Forecasting: https://eco.bigelow.org/






	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov 12 21:07:59 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 12 Nov 2019 14:07:59 -0600
Subject: [R] QQ plot
In-Reply-To: <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
Message-ID: <CAF9-5jNSgMc1EA3CyBzBtX9s-jdoz8VSoce5pKXV7yNh0s0JcQ@mail.gmail.com>

Hi,

what I know so far that this kind of QQ plot is an indication that
data has non zero mean:
https://stats.stackexchange.com/questions/280634/how-to-interpret-qq-plot-not-on-the-line

but is that an indication that something is wrong with the analysis?

Thanks
Ana

On Tue, Nov 12, 2019 at 2:00 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> I thought about this and did a little study of GWAS and the use of
> p-values to assess significant associations. As Ana's plot begins at
> values of about 0.001, this seems to imply that almost everything in
> the genome is associated to some degree. One expects that most SNPs
> will not be associated with a particular condition (p~1), so perhaps
> something is going wrong in the calculations that produce the
> p-values.
>
> Jim
>
> On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> <malone at malonequantitative.com> wrote:
> >
> > I agree with Abby. That would defeat the purpose of a QQ plot.
> >
> > On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > > Hi
> > >
> > > I'm not familiar with the qqman package, or GWAS studies.
> > > However, my guess would be that you're *not* supposed to change the
> > > position of the line.
> > >
> > > On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > > wrote:
> > > >
> > > > Hi,
> > > >
> > > > I was using this library, qqman
> > > > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > >
> > > > to create QQ plot, attached. How would I change this default abline to
> > > > start from the beginning of my QQ line?
> > > >
> > > > This is my code:
> > > > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > >
> > > > Thanks
> > > > Ana
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m||o@@z@rkov|c @end|ng |rom gm@||@com  Tue Nov 12 21:08:19 2019
From: m||o@@z@rkov|c @end|ng |rom gm@||@com (=?UTF-8?B?TWlsb8WhIMW9YXJrb3ZpxIc=?=)
Date: Tue, 12 Nov 2019 21:08:19 +0100
Subject: [R] QQ plot
In-Reply-To: <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
Message-ID: <CANgWSHADfC-f8cQJxWL9KSobirbQXTzau6eYmvWi491KpEdNzQ@mail.gmail.com>

Just a small comment. In GWAS studies p values are considerate to bi
significant whwn p < 10-6 or smaller
regards,

Milo?

On Tue, 12 Nov 2019 at 21:00, Jim Lemon <drjimlemon at gmail.com> wrote:

> I thought about this and did a little study of GWAS and the use of
> p-values to assess significant associations. As Ana's plot begins at
> values of about 0.001, this seems to imply that almost everything in
> the genome is associated to some degree. One expects that most SNPs
> will not be associated with a particular condition (p~1), so perhaps
> something is going wrong in the calculations that produce the
> p-values.
>
> Jim
>
> On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> <malone at malonequantitative.com> wrote:
> >
> > I agree with Abby. That would defeat the purpose of a QQ plot.
> >
> > On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > > Hi
> > >
> > > I'm not familiar with the qqman package, or GWAS studies.
> > > However, my guess would be that you're *not* supposed to change the
> > > position of the line.
> > >
> > > On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <
> sokovic.anamarija at gmail.com>
> > > wrote:
> > > >
> > > > Hi,
> > > >
> > > > I was using this library, qqman
> > > > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > >
> > > > to create QQ plot, attached. How would I change this default abline
> to
> > > > start from the beginning of my QQ line?
> > > >
> > > > This is my code:
> > > > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > >
> > > > Thanks
> > > > Ana
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov 12 21:15:58 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 12 Nov 2019 14:15:58 -0600
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jNSgMc1EA3CyBzBtX9s-jdoz8VSoce5pKXV7yNh0s0JcQ@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <CAF9-5jNSgMc1EA3CyBzBtX9s-jdoz8VSoce5pKXV7yNh0s0JcQ@mail.gmail.com>
Message-ID: <CAF9-5jOrD1iM=XcDM+QsVQ-y5DVuKF8SxN-Znpn7knNxvJiBgQ@mail.gmail.com>

details about my data if it is helpful:

> median(dd$P,na.rm = FALSE)
[1] 0.000444
> mean(dd$P,na.rm = FALSE)
[1] 0.000461
> min(dd$P,na.rm = FALSE)
[1] 9.89e-08
> max(dd$P,na.rm = FALSE)
[1] 0.001

On Tue, Nov 12, 2019 at 2:07 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi,
>
> what I know so far that this kind of QQ plot is an indication that
> data has non zero mean:
> https://stats.stackexchange.com/questions/280634/how-to-interpret-qq-plot-not-on-the-line
>
> but is that an indication that something is wrong with the analysis?
>
> Thanks
> Ana
>
> On Tue, Nov 12, 2019 at 2:00 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > I thought about this and did a little study of GWAS and the use of
> > p-values to assess significant associations. As Ana's plot begins at
> > values of about 0.001, this seems to imply that almost everything in
> > the genome is associated to some degree. One expects that most SNPs
> > will not be associated with a particular condition (p~1), so perhaps
> > something is going wrong in the calculations that produce the
> > p-values.
> >
> > Jim
> >
> > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > <malone at malonequantitative.com> wrote:
> > >
> > > I agree with Abby. That would defeat the purpose of a QQ plot.
> > >
> > > On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> > >
> > > > Hi
> > > >
> > > > I'm not familiar with the qqman package, or GWAS studies.
> > > > However, my guess would be that you're *not* supposed to change the
> > > > position of the line.
> > > >
> > > > On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > > > wrote:
> > > > >
> > > > > Hi,
> > > > >
> > > > > I was using this library, qqman
> > > > > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > > >
> > > > > to create QQ plot, attached. How would I change this default abline to
> > > > > start from the beginning of my QQ line?
> > > > >
> > > > > This is my code:
> > > > > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > > >
> > > > > Thanks
> > > > > Ana
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Tue Nov 12 21:25:35 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 12 Nov 2019 12:25:35 -0800
Subject: [R] QQ plot
In-Reply-To: <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
Message-ID: <CAGxFJbQNGz5tMNY1_H9bH9g3QwoyDkdC1rzH84-QCxXBdE70NA@mail.gmail.com>

As this is O/T I'll keep it offlist.

Inline:


On Tue, Nov 12, 2019 at 12:00 PM Jim Lemon <drjimlemon at gmail.com> wrote:

> I thought about this and did a little study of GWAS and the use of
> p-values to assess significant associations. As Ana's plot begins at
> values of about 0.001, this seems to imply that almost everything in
> the genome is associated to some degree. One expects that most SNPs
> will not be associated with a particular condition (p~1), so perhaps
> something is going wrong in the calculations that produce the
> p-values.
>

Exactly! Or possibly with the data handling pipeline prior to getting into
R.

-- Bert



>
> Jim
>
> On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> <malone at malonequantitative.com> wrote:
> >
> > I agree with Abby. That would defeat the purpose of a QQ plot.
> >
> > On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > > Hi
> > >
> > > I'm not familiar with the qqman package, or GWAS studies.
> > > However, my guess would be that you're *not* supposed to change the
> > > position of the line.
> > >
> > > On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <
> sokovic.anamarija at gmail.com>
> > > wrote:
> > > >
> > > > Hi,
> > > >
> > > > I was using this library, qqman
> > > > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > >
> > > > to create QQ plot, attached. How would I change this default abline
> to
> > > > start from the beginning of my QQ line?
> > > >
> > > > This is my code:
> > > > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > >
> > > > Thanks
> > > > Ana
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Tue Nov 12 21:28:26 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 13 Nov 2019 07:28:26 +1100
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jNSgMc1EA3CyBzBtX9s-jdoz8VSoce5pKXV7yNh0s0JcQ@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <CAF9-5jNSgMc1EA3CyBzBtX9s-jdoz8VSoce5pKXV7yNh0s0JcQ@mail.gmail.com>
Message-ID: <CA+8X3fU0Y_uvu4GiQyOr9K8VRQayp8G4YzQdF3MT+9jnYbGCAA@mail.gmail.com>

That refers to "normally" distributed data (see Greg Snow's comment
below the one you cite). P-values are not necessarily normally
distributed as you can see, and they must have a non-zero mean.

Jim

On Wed, Nov 13, 2019 at 7:07 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi,
>
> what I know so far that this kind of QQ plot is an indication that
> data has non zero mean:
> https://stats.stackexchange.com/questions/280634/how-to-interpret-qq-plot-not-on-the-line
>
> but is that an indication that something is wrong with the analysis?
>
> Thanks
> Ana
>
> On Tue, Nov 12, 2019 at 2:00 PM Jim Lemon <drjimlemon at gmail.com> wrote:
> >
> > I thought about this and did a little study of GWAS and the use of
> > p-values to assess significant associations. As Ana's plot begins at
> > values of about 0.001, this seems to imply that almost everything in
> > the genome is associated to some degree. One expects that most SNPs
> > will not be associated with a particular condition (p~1), so perhaps
> > something is going wrong in the calculations that produce the
> > p-values.
> >
> > Jim
> >
> > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > <malone at malonequantitative.com> wrote:
> > >
> > > I agree with Abby. That would defeat the purpose of a QQ plot.
> > >
> > > On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> > >
> > > > Hi
> > > >
> > > > I'm not familiar with the qqman package, or GWAS studies.
> > > > However, my guess would be that you're *not* supposed to change the
> > > > position of the line.
> > > >
> > > > On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > > > wrote:
> > > > >
> > > > > Hi,
> > > > >
> > > > > I was using this library, qqman
> > > > > https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > > >
> > > > > to create QQ plot, attached. How would I change this default abline to
> > > > > start from the beginning of my QQ line?
> > > > >
> > > > > This is my code:
> > > > > qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > > >
> > > > > Thanks
> > > > > Ana
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide
> > > > http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Nov 12 21:34:39 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 12 Nov 2019 15:34:39 -0500
Subject: [R] QQ plot
In-Reply-To: <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
Message-ID: <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>

On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> I thought about this and did a little study of GWAS and the use of
> p-values to assess significant associations. As Ana's plot begins at
> values of about 0.001, this seems to imply that almost everything in
> the genome is associated to some degree. One expects that most SNPs
> will not be associated with a particular condition (p~1), so perhaps
> something is going wrong in the calculations that produce the
> p-values.

I may be misunderstanding your last sentence, but if there is no 
association, the p-value would usually have a uniform distribution from 
0 to 1, it wouldn't be near 1.

I'd guess we're not seeing the p values from every test, only those that 
are less than 0.001.  If that's true, and there are no effects, it makes 
sense to multiply all of them by 1000 to get U(0,1) values.  On the 
plot, that would correspond to subtracting 3 from -log10(p), or adding 3 
to the reference line, as Ana requested.

Or just multiply them by 1000 and pass them to qq():

     qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")

As far as I can see, there's no way to tell qqman::qq to move the 
reference line.

Duncan Murdoch

> 
> Jim
> 
> On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> <malone at malonequantitative.com> wrote:
>>
>> I agree with Abby. That would defeat the purpose of a QQ plot.
>>
>> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>>
>>> Hi
>>>
>>> I'm not familiar with the qqman package, or GWAS studies.
>>> However, my guess would be that you're *not* supposed to change the
>>> position of the line.
>>>
>>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
>>> wrote:
>>>>
>>>> Hi,
>>>>
>>>> I was using this library, qqman
>>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
>>>>
>>>> to create QQ plot, attached. How would I change this default abline to
>>>> start from the beginning of my QQ line?
>>>>
>>>> This is my code:
>>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
>>>>
>>>> Thanks
>>>> Ana
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov 12 22:07:56 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 12 Nov 2019 15:07:56 -0600
Subject: [R] QQ plot
In-Reply-To: <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
Message-ID: <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>

Hi Duncan,

yes I choose for QQ plot only P<1e-3 and multiplying everything with
1000 works great!
This should not in my understanding influence the interpretation of
the plot, it is only changing the scale of axis.

Thank you so much,
Ana

On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> > I thought about this and did a little study of GWAS and the use of
> > p-values to assess significant associations. As Ana's plot begins at
> > values of about 0.001, this seems to imply that almost everything in
> > the genome is associated to some degree. One expects that most SNPs
> > will not be associated with a particular condition (p~1), so perhaps
> > something is going wrong in the calculations that produce the
> > p-values.
>
> I may be misunderstanding your last sentence, but if there is no
> association, the p-value would usually have a uniform distribution from
> 0 to 1, it wouldn't be near 1.
>
> I'd guess we're not seeing the p values from every test, only those that
> are less than 0.001.  If that's true, and there are no effects, it makes
> sense to multiply all of them by 1000 to get U(0,1) values.  On the
> plot, that would correspond to subtracting 3 from -log10(p), or adding 3
> to the reference line, as Ana requested.
>
> Or just multiply them by 1000 and pass them to qq():
>
>      qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
>
> As far as I can see, there's no way to tell qqman::qq to move the
> reference line.
>
> Duncan Murdoch
>
> >
> > Jim
> >
> > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > <malone at malonequantitative.com> wrote:
> >>
> >> I agree with Abby. That would defeat the purpose of a QQ plot.
> >>
> >> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >>
> >>> Hi
> >>>
> >>> I'm not familiar with the qqman package, or GWAS studies.
> >>> However, my guess would be that you're *not* supposed to change the
> >>> position of the line.
> >>>
> >>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> >>> wrote:
> >>>>
> >>>> Hi,
> >>>>
> >>>> I was using this library, qqman
> >>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> >>>>
> >>>> to create QQ plot, attached. How would I change this default abline to
> >>>> start from the beginning of my QQ line?
> >>>>
> >>>> This is my code:
> >>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
> >>>>
> >>>> Thanks
> >>>> Ana
> >>>> ______________________________________________
> >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov 12 22:37:00 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 12 Nov 2019 15:37:00 -0600
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
Message-ID: <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>

Just do I need to change the axis when I multiply with 1000 and what
should I put on my axis?

On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hi Duncan,
>
> yes I choose for QQ plot only P<1e-3 and multiplying everything with
> 1000 works great!
> This should not in my understanding influence the interpretation of
> the plot, it is only changing the scale of axis.
>
> Thank you so much,
> Ana
>
> On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >
> > On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> > > I thought about this and did a little study of GWAS and the use of
> > > p-values to assess significant associations. As Ana's plot begins at
> > > values of about 0.001, this seems to imply that almost everything in
> > > the genome is associated to some degree. One expects that most SNPs
> > > will not be associated with a particular condition (p~1), so perhaps
> > > something is going wrong in the calculations that produce the
> > > p-values.
> >
> > I may be misunderstanding your last sentence, but if there is no
> > association, the p-value would usually have a uniform distribution from
> > 0 to 1, it wouldn't be near 1.
> >
> > I'd guess we're not seeing the p values from every test, only those that
> > are less than 0.001.  If that's true, and there are no effects, it makes
> > sense to multiply all of them by 1000 to get U(0,1) values.  On the
> > plot, that would correspond to subtracting 3 from -log10(p), or adding 3
> > to the reference line, as Ana requested.
> >
> > Or just multiply them by 1000 and pass them to qq():
> >
> >      qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
> >
> > As far as I can see, there's no way to tell qqman::qq to move the
> > reference line.
> >
> > Duncan Murdoch
> >
> > >
> > > Jim
> > >
> > > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > > <malone at malonequantitative.com> wrote:
> > >>
> > >> I agree with Abby. That would defeat the purpose of a QQ plot.
> > >>
> > >> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> > >>
> > >>> Hi
> > >>>
> > >>> I'm not familiar with the qqman package, or GWAS studies.
> > >>> However, my guess would be that you're *not* supposed to change the
> > >>> position of the line.
> > >>>
> > >>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > >>> wrote:
> > >>>>
> > >>>> Hi,
> > >>>>
> > >>>> I was using this library, qqman
> > >>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > >>>>
> > >>>> to create QQ plot, attached. How would I change this default abline to
> > >>>> start from the beginning of my QQ line?
> > >>>>
> > >>>> This is my code:
> > >>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > >>>>
> > >>>> Thanks
> > >>>> Ana
> > >>>> ______________________________________________
> > >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>> PLEASE do read the posting guide
> > >>> http://www.R-project.org/posting-guide.html
> > >>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide
> > >>> http://www.R-project.org/posting-guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>>
> > >>
> > >>          [[alternative HTML version deleted]]
> > >>
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov 12 22:42:48 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 12 Nov 2019 15:42:48 -0600
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
 <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
Message-ID: <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>

the smallest p value in my dataset goes to 9.89e-08. How do I make
that known on the new QQ plot with multiplied with 1000 values

On Tue, Nov 12, 2019 at 3:37 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Just do I need to change the axis when I multiply with 1000 and what
> should I put on my axis?
>
> On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > Hi Duncan,
> >
> > yes I choose for QQ plot only P<1e-3 and multiplying everything with
> > 1000 works great!
> > This should not in my understanding influence the interpretation of
> > the plot, it is only changing the scale of axis.
> >
> > Thank you so much,
> > Ana
> >
> > On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > >
> > > On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> > > > I thought about this and did a little study of GWAS and the use of
> > > > p-values to assess significant associations. As Ana's plot begins at
> > > > values of about 0.001, this seems to imply that almost everything in
> > > > the genome is associated to some degree. One expects that most SNPs
> > > > will not be associated with a particular condition (p~1), so perhaps
> > > > something is going wrong in the calculations that produce the
> > > > p-values.
> > >
> > > I may be misunderstanding your last sentence, but if there is no
> > > association, the p-value would usually have a uniform distribution from
> > > 0 to 1, it wouldn't be near 1.
> > >
> > > I'd guess we're not seeing the p values from every test, only those that
> > > are less than 0.001.  If that's true, and there are no effects, it makes
> > > sense to multiply all of them by 1000 to get U(0,1) values.  On the
> > > plot, that would correspond to subtracting 3 from -log10(p), or adding 3
> > > to the reference line, as Ana requested.
> > >
> > > Or just multiply them by 1000 and pass them to qq():
> > >
> > >      qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
> > >
> > > As far as I can see, there's no way to tell qqman::qq to move the
> > > reference line.
> > >
> > > Duncan Murdoch
> > >
> > > >
> > > > Jim
> > > >
> > > > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > > > <malone at malonequantitative.com> wrote:
> > > >>
> > > >> I agree with Abby. That would defeat the purpose of a QQ plot.
> > > >>
> > > >> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> > > >>
> > > >>> Hi
> > > >>>
> > > >>> I'm not familiar with the qqman package, or GWAS studies.
> > > >>> However, my guess would be that you're *not* supposed to change the
> > > >>> position of the line.
> > > >>>
> > > >>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > > >>> wrote:
> > > >>>>
> > > >>>> Hi,
> > > >>>>
> > > >>>> I was using this library, qqman
> > > >>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > >>>>
> > > >>>> to create QQ plot, attached. How would I change this default abline to
> > > >>>> start from the beginning of my QQ line?
> > > >>>>
> > > >>>> This is my code:
> > > >>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > >>>>
> > > >>>> Thanks
> > > >>>> Ana
> > > >>>> ______________________________________________
> > > >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>> PLEASE do read the posting guide
> > > >>> http://www.R-project.org/posting-guide.html
> > > >>>> and provide commented, minimal, self-contained, reproducible code.
> > > >>>
> > > >>> ______________________________________________
> > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>> PLEASE do read the posting guide
> > > >>> http://www.R-project.org/posting-guide.html
> > > >>> and provide commented, minimal, self-contained, reproducible code.
> > > >>>
> > > >>
> > > >>          [[alternative HTML version deleted]]
> > > >>
> > > >> ______________________________________________
> > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > >> and provide commented, minimal, self-contained, reproducible code.
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > > >
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Tue Nov 12 23:04:24 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Tue, 12 Nov 2019 16:04:24 -0600
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
 <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
 <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>
Message-ID: <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>

why I selected only those with P<0.003 to put on QQ plot is because
the original data set contains 5556249 points and when I extract only
P<0.001 I am getting 3713 points. Is there is a way to plot the whole
data set, or choose only the representative points?

On Tue, Nov 12, 2019 at 3:42 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> the smallest p value in my dataset goes to 9.89e-08. How do I make
> that known on the new QQ plot with multiplied with 1000 values
>
> On Tue, Nov 12, 2019 at 3:37 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > Just do I need to change the axis when I multiply with 1000 and what
> > should I put on my axis?
> >
> > On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > Hi Duncan,
> > >
> > > yes I choose for QQ plot only P<1e-3 and multiplying everything with
> > > 1000 works great!
> > > This should not in my understanding influence the interpretation of
> > > the plot, it is only changing the scale of axis.
> > >
> > > Thank you so much,
> > > Ana
> > >
> > > On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > > >
> > > > On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> > > > > I thought about this and did a little study of GWAS and the use of
> > > > > p-values to assess significant associations. As Ana's plot begins at
> > > > > values of about 0.001, this seems to imply that almost everything in
> > > > > the genome is associated to some degree. One expects that most SNPs
> > > > > will not be associated with a particular condition (p~1), so perhaps
> > > > > something is going wrong in the calculations that produce the
> > > > > p-values.
> > > >
> > > > I may be misunderstanding your last sentence, but if there is no
> > > > association, the p-value would usually have a uniform distribution from
> > > > 0 to 1, it wouldn't be near 1.
> > > >
> > > > I'd guess we're not seeing the p values from every test, only those that
> > > > are less than 0.001.  If that's true, and there are no effects, it makes
> > > > sense to multiply all of them by 1000 to get U(0,1) values.  On the
> > > > plot, that would correspond to subtracting 3 from -log10(p), or adding 3
> > > > to the reference line, as Ana requested.
> > > >
> > > > Or just multiply them by 1000 and pass them to qq():
> > > >
> > > >      qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
> > > >
> > > > As far as I can see, there's no way to tell qqman::qq to move the
> > > > reference line.
> > > >
> > > > Duncan Murdoch
> > > >
> > > > >
> > > > > Jim
> > > > >
> > > > > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > > > > <malone at malonequantitative.com> wrote:
> > > > >>
> > > > >> I agree with Abby. That would defeat the purpose of a QQ plot.
> > > > >>
> > > > >> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> > > > >>
> > > > >>> Hi
> > > > >>>
> > > > >>> I'm not familiar with the qqman package, or GWAS studies.
> > > > >>> However, my guess would be that you're *not* supposed to change the
> > > > >>> position of the line.
> > > > >>>
> > > > >>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> > > > >>> wrote:
> > > > >>>>
> > > > >>>> Hi,
> > > > >>>>
> > > > >>>> I was using this library, qqman
> > > > >>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > > >>>>
> > > > >>>> to create QQ plot, attached. How would I change this default abline to
> > > > >>>> start from the beginning of my QQ line?
> > > > >>>>
> > > > >>>> This is my code:
> > > > >>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > > >>>>
> > > > >>>> Thanks
> > > > >>>> Ana
> > > > >>>> ______________________________________________
> > > > >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >>>> PLEASE do read the posting guide
> > > > >>> http://www.R-project.org/posting-guide.html
> > > > >>>> and provide commented, minimal, self-contained, reproducible code.
> > > > >>>
> > > > >>> ______________________________________________
> > > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >>> PLEASE do read the posting guide
> > > > >>> http://www.R-project.org/posting-guide.html
> > > > >>> and provide commented, minimal, self-contained, reproducible code.
> > > > >>>
> > > > >>
> > > > >>          [[alternative HTML version deleted]]
> > > > >>
> > > > >> ______________________________________________
> > > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > >> and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov 13 00:11:46 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 12 Nov 2019 15:11:46 -0800
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
 <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
 <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>
 <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>
Message-ID: <CAGxFJbS9sjrwGoqouYNXD9m=bN34b0zyfwFqZBg4rWynrEj8Mw@mail.gmail.com>

IMO, this thread has now gone totally off the rails and totally off topic
-- it is clearly *not* about R programming and totally about statistics.

I believe Ana Marija would do better to get local statistical help or post
on a statistics or genomics list (stats.stackexchange.com is one such)
where she can engage in a fuller discussion of what an *appropriate* qqplot
would tell her. Of course selecting the lowest 3700 p-values from 55.5
million and plotting them against 3700 expected uniform quantiles will not
give a line with 0 intercept and slope 1. The scale correction is easy to
make, but it is not multiplying by 1000!

Bert


On Tue, Nov 12, 2019 at 2:11 PM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> why I selected only those with P<0.003 to put on QQ plot is because
> the original data set contains 5556249 points and when I extract only
> P<0.001 I am getting 3713 points. Is there is a way to plot the whole
> data set, or choose only the representative points?
>
> On Tue, Nov 12, 2019 at 3:42 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> >
> > the smallest p value in my dataset goes to 9.89e-08. How do I make
> > that known on the new QQ plot with multiplied with 1000 values
> >
> > On Tue, Nov 12, 2019 at 3:37 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
> > >
> > > Just do I need to change the axis when I multiply with 1000 and what
> > > should I put on my axis?
> > >
> > > On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <
> sokovic.anamarija at gmail.com> wrote:
> > > >
> > > > Hi Duncan,
> > > >
> > > > yes I choose for QQ plot only P<1e-3 and multiplying everything with
> > > > 1000 works great!
> > > > This should not in my understanding influence the interpretation of
> > > > the plot, it is only changing the scale of axis.
> > > >
> > > > Thank you so much,
> > > > Ana
> > > >
> > > > On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <
> murdoch.duncan at gmail.com> wrote:
> > > > >
> > > > > On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> > > > > > I thought about this and did a little study of GWAS and the use
> of
> > > > > > p-values to assess significant associations. As Ana's plot
> begins at
> > > > > > values of about 0.001, this seems to imply that almost
> everything in
> > > > > > the genome is associated to some degree. One expects that most
> SNPs
> > > > > > will not be associated with a particular condition (p~1), so
> perhaps
> > > > > > something is going wrong in the calculations that produce the
> > > > > > p-values.
> > > > >
> > > > > I may be misunderstanding your last sentence, but if there is no
> > > > > association, the p-value would usually have a uniform distribution
> from
> > > > > 0 to 1, it wouldn't be near 1.
> > > > >
> > > > > I'd guess we're not seeing the p values from every test, only
> those that
> > > > > are less than 0.001.  If that's true, and there are no effects, it
> makes
> > > > > sense to multiply all of them by 1000 to get U(0,1) values.  On the
> > > > > plot, that would correspond to subtracting 3 from -log10(p), or
> adding 3
> > > > > to the reference line, as Ana requested.
> > > > >
> > > > > Or just multiply them by 1000 and pass them to qq():
> > > > >
> > > > >      qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
> > > > >
> > > > > As far as I can see, there's no way to tell qqman::qq to move the
> > > > > reference line.
> > > > >
> > > > > Duncan Murdoch
> > > > >
> > > > > >
> > > > > > Jim
> > > > > >
> > > > > > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> > > > > > <malone at malonequantitative.com> wrote:
> > > > > >>
> > > > > >> I agree with Abby. That would defeat the purpose of a QQ plot.
> > > > > >>
> > > > > >> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com>
> wrote:
> > > > > >>
> > > > > >>> Hi
> > > > > >>>
> > > > > >>> I'm not familiar with the qqman package, or GWAS studies.
> > > > > >>> However, my guess would be that you're *not* supposed to
> change the
> > > > > >>> position of the line.
> > > > > >>>
> > > > > >>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <
> sokovic.anamarija at gmail.com>
> > > > > >>> wrote:
> > > > > >>>>
> > > > > >>>> Hi,
> > > > > >>>>
> > > > > >>>> I was using this library, qqman
> > > > > >>>>
> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> > > > > >>>>
> > > > > >>>> to create QQ plot, attached. How would I change this default
> abline to
> > > > > >>>> start from the beginning of my QQ line?
> > > > > >>>>
> > > > > >>>> This is my code:
> > > > > >>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
> > > > > >>>>
> > > > > >>>> Thanks
> > > > > >>>> Ana
> > > > > >>>> ______________________________________________
> > > > > >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
> more, see
> > > > > >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > >>>> PLEASE do read the posting guide
> > > > > >>> http://www.R-project.org/posting-guide.html
> > > > > >>>> and provide commented, minimal, self-contained, reproducible
> code.
> > > > > >>>
> > > > > >>> ______________________________________________
> > > > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> > > > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > >>> PLEASE do read the posting guide
> > > > > >>> http://www.R-project.org/posting-guide.html
> > > > > >>> and provide commented, minimal, self-contained, reproducible
> code.
> > > > > >>>
> > > > > >>
> > > > > >>          [[alternative HTML version deleted]]
> > > > > >>
> > > > > >> ______________________________________________
> > > > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> > > > > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > > > >> and provide commented, minimal, self-contained, reproducible
> code.
> > > > > >
> > > > > > ______________________________________________
> > > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
> see
> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > > > > and provide commented, minimal, self-contained, reproducible
> code.
> > > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > > > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov 13 00:58:55 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 12 Nov 2019 15:58:55 -0800
Subject: [R] QQ plot
In-Reply-To: <CAGxFJbS9sjrwGoqouYNXD9m=bN34b0zyfwFqZBg4rWynrEj8Mw@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
 <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
 <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>
 <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>
 <CAGxFJbS9sjrwGoqouYNXD9m=bN34b0zyfwFqZBg4rWynrEj8Mw@mail.gmail.com>
Message-ID: <CAGxFJbQ4eN3v0Dzip0DXwA9q=LX1uhexFCv0DY7=0pDyB-N=Hg@mail.gmail.com>

Typo: "... from 5.5 million..."

Bert


On Tue, Nov 12, 2019 at 3:11 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> IMO, this thread has now gone totally off the rails and totally off topic
> -- it is clearly *not* about R programming and totally about statistics.
>
> I believe Ana Marija would do better to get local statistical help or post
> on a statistics or genomics list (stats.stackexchange.com is one such)
> where she can engage in a fuller discussion of what an *appropriate* qqplot
> would tell her. Of course selecting the lowest 3700 p-values from 55.5
> million and plotting them against 3700 expected uniform quantiles will not
> give a line with 0 intercept and slope 1. The scale correction is easy to
> make, but it is not multiplying by 1000!
>
> Bert
>
>
> On Tue, Nov 12, 2019 at 2:11 PM Ana Marija <sokovic.anamarija at gmail.com>
> wrote:
>
>> why I selected only those with P<0.003 to put on QQ plot is because
>> the original data set contains 5556249 points and when I extract only
>> P<0.001 I am getting 3713 points. Is there is a way to plot the whole
>> data set, or choose only the representative points?
>>
>> On Tue, Nov 12, 2019 at 3:42 PM Ana Marija <sokovic.anamarija at gmail.com>
>> wrote:
>> >
>> > the smallest p value in my dataset goes to 9.89e-08. How do I make
>> > that known on the new QQ plot with multiplied with 1000 values
>> >
>> > On Tue, Nov 12, 2019 at 3:37 PM Ana Marija <sokovic.anamarija at gmail.com>
>> wrote:
>> > >
>> > > Just do I need to change the axis when I multiply with 1000 and what
>> > > should I put on my axis?
>> > >
>> > > On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <
>> sokovic.anamarija at gmail.com> wrote:
>> > > >
>> > > > Hi Duncan,
>> > > >
>> > > > yes I choose for QQ plot only P<1e-3 and multiplying everything with
>> > > > 1000 works great!
>> > > > This should not in my understanding influence the interpretation of
>> > > > the plot, it is only changing the scale of axis.
>> > > >
>> > > > Thank you so much,
>> > > > Ana
>> > > >
>> > > > On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <
>> murdoch.duncan at gmail.com> wrote:
>> > > > >
>> > > > > On 12/11/2019 2:56 p.m., Jim Lemon wrote:
>> > > > > > I thought about this and did a little study of GWAS and the use
>> of
>> > > > > > p-values to assess significant associations. As Ana's plot
>> begins at
>> > > > > > values of about 0.001, this seems to imply that almost
>> everything in
>> > > > > > the genome is associated to some degree. One expects that most
>> SNPs
>> > > > > > will not be associated with a particular condition (p~1), so
>> perhaps
>> > > > > > something is going wrong in the calculations that produce the
>> > > > > > p-values.
>> > > > >
>> > > > > I may be misunderstanding your last sentence, but if there is no
>> > > > > association, the p-value would usually have a uniform
>> distribution from
>> > > > > 0 to 1, it wouldn't be near 1.
>> > > > >
>> > > > > I'd guess we're not seeing the p values from every test, only
>> those that
>> > > > > are less than 0.001.  If that's true, and there are no effects,
>> it makes
>> > > > > sense to multiply all of them by 1000 to get U(0,1) values.  On
>> the
>> > > > > plot, that would correspond to subtracting 3 from -log10(p), or
>> adding 3
>> > > > > to the reference line, as Ana requested.
>> > > > >
>> > > > > Or just multiply them by 1000 and pass them to qq():
>> > > > >
>> > > > >      qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
>> > > > >
>> > > > > As far as I can see, there's no way to tell qqman::qq to move the
>> > > > > reference line.
>> > > > >
>> > > > > Duncan Murdoch
>> > > > >
>> > > > > >
>> > > > > > Jim
>> > > > > >
>> > > > > > On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
>> > > > > > <malone at malonequantitative.com> wrote:
>> > > > > >>
>> > > > > >> I agree with Abby. That would defeat the purpose of a QQ plot.
>> > > > > >>
>> > > > > >> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <
>> spurdle.a at gmail.com> wrote:
>> > > > > >>
>> > > > > >>> Hi
>> > > > > >>>
>> > > > > >>> I'm not familiar with the qqman package, or GWAS studies.
>> > > > > >>> However, my guess would be that you're *not* supposed to
>> change the
>> > > > > >>> position of the line.
>> > > > > >>>
>> > > > > >>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <
>> sokovic.anamarija at gmail.com>
>> > > > > >>> wrote:
>> > > > > >>>>
>> > > > > >>>> Hi,
>> > > > > >>>>
>> > > > > >>>> I was using this library, qqman
>> > > > > >>>>
>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
>> > > > > >>>>
>> > > > > >>>> to create QQ plot, attached. How would I change this default
>> abline to
>> > > > > >>>> start from the beginning of my QQ line?
>> > > > > >>>>
>> > > > > >>>> This is my code:
>> > > > > >>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
>> > > > > >>>>
>> > > > > >>>> Thanks
>> > > > > >>>> Ana
>> > > > > >>>> ______________________________________________
>> > > > > >>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>> more, see
>> > > > > >>>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > > >>>> PLEASE do read the posting guide
>> > > > > >>> http://www.R-project.org/posting-guide.html
>> > > > > >>>> and provide commented, minimal, self-contained, reproducible
>> code.
>> > > > > >>>
>> > > > > >>> ______________________________________________
>> > > > > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and
>> more, see
>> > > > > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > > >>> PLEASE do read the posting guide
>> > > > > >>> http://www.R-project.org/posting-guide.html
>> > > > > >>> and provide commented, minimal, self-contained, reproducible
>> code.
>> > > > > >>>
>> > > > > >>
>> > > > > >>          [[alternative HTML version deleted]]
>> > > > > >>
>> > > > > >> ______________________________________________
>> > > > > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>> > > > > >> https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > > >> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > > > > >> and provide commented, minimal, self-contained, reproducible
>> code.
>> > > > > >
>> > > > > > ______________________________________________
>> > > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>> see
>> > > > > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > > > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > > > > > and provide commented, minimal, self-contained, reproducible
>> code.
>> > > > > >
>> > > > >
>> > > > > ______________________________________________
>> > > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > > > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > > > > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From g@@@uu| @end|ng |rom gm@||@com  Wed Nov 13 08:50:53 2019
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Wed, 13 Nov 2019 16:50:53 +0900
Subject: [R] data load from excel files
Message-ID: <CAHXS41yxS27QeUXYUrhHZjgqCrRVQUQEpqpMCtZVweqFyAhucg@mail.gmail.com>

Dear R-Help,

I have 30 of year-based excel files and each file contain month sheets. I
have some problem here. My data is daily rainfall but there is extra 1 day
(first date of next month) for several sheets. My main goal is to get the
minimum value for every month.

First, how to extract those data to list of data frame based on year and
delete every overlapping date?
Second, how to sort it based on date with ascending order (old to new)?
Third, how to get the maximum together with the date?

I did this one,

...
file.list <- list.files(pattern='*.xlsx')
file.list<-mixedsort(file.list)

#
https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames

read_excel_allsheets <- function(filename, tibble = FALSE) {
  sheets <- readxl::excel_sheets(filename)
  x <- lapply(sheets, function(X) read.xlsx(filename, sheet=X, rows=9:40,
cols=1:2))
  if(!tibble) x <- lapply(x, as.data.frame)
  names(x) <- sheets
  x
}

pon<-lapply(file.list, function(i) read_excel_allsheets(i))
pon1<-do.call("rbind",pon)
names(pon1) <- paste0("M.", 1:360)
pon1 <-lapply(pon1,function(x){x$RR[x$RR==8888] <- NA; x})
pon1 <-lapply(pon1,function(x){x$RR[x$RR==""] <- NA; x})
maxi<-lapply(pon1, function(x) max(x$RR,na.rm=T))
maxi<-data.frame(Reduce(rbind, maxi))
names(maxi)<-"maxi"
....

but the list start from January for every year, and move to February and so
on. And there is no date in "maxi". Here some sample what I get from my
simple code.

> pon1[256:258]$M.256
      Tanggal   RR
1  01-09-2001  5.2
2  02-09-2001  0.3
3  03-09-2001 29.0
4  04-09-2001  0.7
5  05-09-2001  9.6
6  06-09-2001  0.7
7  07-09-2001   NA
8  08-09-2001 13.2
9  09-09-2001   NA
10 10-09-2001   NA
11 11-09-2001  0.0
12 12-09-2001 66.0
13 13-09-2001  0.0
14 14-09-2001 57.6
15 15-09-2001 18.0
16 16-09-2001 29.2
17 17-09-2001 52.2
18 18-09-2001  7.0
19 19-09-2001   NA
20 20-09-2001 74.5
21 21-09-2001 20.3
22 22-09-2001 49.6
23 23-09-2001  0.0
24 24-09-2001  1.3
25 25-09-2001  0.0
26 26-09-2001  1.0
27 27-09-2001  0.1
28 28-09-2001  1.9
29 29-09-2001  9.5
30 30-09-2001  3.3
31 01-10-2001  0.0

$M.257
      Tanggal   RR
1  01-09-2002  0.0
2  02-09-2002  0.0
3  03-09-2002  0.0
4  04-09-2002 12.8
5  05-09-2002  1.0
6  06-09-2002  0.0
7  07-09-2002   NA
8  08-09-2002 22.2
9  09-09-2002   NA
10 10-09-2002   NA
11 11-09-2002  0.0
12 12-09-2002  0.0
13 13-09-2002  0.0
14 14-09-2002   NA
15 15-09-2002  0.0
16 16-09-2002  0.0
17 17-09-2002  0.0
18 18-09-2002 13.3
19 19-09-2002  0.0
20 20-09-2002  0.0
21 21-09-2002  0.0
22 22-09-2002  0.0
23 23-09-2002  0.0
24 24-09-2002  0.0
25 25-09-2002  0.0
26 26-09-2002  0.5
27 27-09-2002  2.1
28 28-09-2002   NA
29 29-09-2002 18.5
30 30-09-2002  0.0
31 01-10-2002   NA

$M.258
      Tanggal   RR
1  01-09-2003  0.0
2  02-09-2003  0.0
3  03-09-2003  0.0
4  04-09-2003  4.0
5  05-09-2003  0.3
6  06-09-2003  0.0
7  07-09-2003   NA
8  08-09-2003  0.0
9  09-09-2003  0.0
10 10-09-2003  0.0
11 11-09-2003   NA
12 12-09-2003  1.0
13 13-09-2003  0.0
14 14-09-2003 60.0
15 15-09-2003  4.5
16 16-09-2003  0.1
17 17-09-2003  2.1
18 18-09-2003   NA
19 19-09-2003  0.0
20 20-09-2003   NA
21 21-09-2003   NA
22 22-09-2003 31.5
23 23-09-2003 42.0
24 24-09-2003 43.3
25 25-09-2003  2.8
26 26-09-2003 21.4
27 27-09-2003  0.8
28 28-09-2003 42.3
29 29-09-2003  5.3
30 30-09-2003 17.3
31 01-10-2003  0.0


Any lead or help is very appreciate.

Best,

Ani

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Nov 13 09:49:36 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 13 Nov 2019 08:49:36 +0000
Subject: [R] data load from excel files
In-Reply-To: <CAHXS41yxS27QeUXYUrhHZjgqCrRVQUQEpqpMCtZVweqFyAhucg@mail.gmail.com>
References: <CAHXS41yxS27QeUXYUrhHZjgqCrRVQUQEpqpMCtZVweqFyAhucg@mail.gmail.com>
Message-ID: <1805c0c5-2856-ee79-f447-ed6551622e91@sapo.pt>

Hello,

Maybe the following will get you close to what you want.


# remove the last row from every df
pon1 <- lapply(pon1, function(DF){
   DF[[1]] <- as.Date(DF[["Tanggal"]], "%d-%m-%Y")
   DF[-nrow(DF), ]
})


# order the list by year-month
inx_ym <- sapply(pon1, function(DF){
   format(DF[["Tanggal"]][1], "%Y-%m")
})
pon1 <- pon1[order(inx_ym)]


# get the minimum and maximum of every "RR"
min.RR <- sapply(pon1, function(DF) min(DF[["RR"]], na.rm = TRUE))
max.RR <- sapply(pon1, function(DF) max(DF[["RR"]], na.rm = TRUE))


Hope this helps,

Rui Barradas



?s 07:50 de 13/11/19, ani jaya escreveu:
> Dear R-Help,
> 
> I have 30 of year-based excel files and each file contain month sheets. I
> have some problem here. My data is daily rainfall but there is extra 1 day
> (first date of next month) for several sheets. My main goal is to get the
> minimum value for every month.
> 
> First, how to extract those data to list of data frame based on year and
> delete every overlapping date?
> Second, how to sort it based on date with ascending order (old to new)?
> Third, how to get the maximum together with the date?
> 
> I did this one,
> 
> ...
> file.list <- list.files(pattern='*.xlsx')
> file.list<-mixedsort(file.list)
> 
> #
> https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
> 
> read_excel_allsheets <- function(filename, tibble = FALSE) {
>    sheets <- readxl::excel_sheets(filename)
>    x <- lapply(sheets, function(X) read.xlsx(filename, sheet=X, rows=9:40,
> cols=1:2))
>    if(!tibble) x <- lapply(x, as.data.frame)
>    names(x) <- sheets
>    x
> }
> 
> pon<-lapply(file.list, function(i) read_excel_allsheets(i))
> pon1<-do.call("rbind",pon)
> names(pon1) <- paste0("M.", 1:360)
> pon1 <-lapply(pon1,function(x){x$RR[x$RR==8888] <- NA; x})
> pon1 <-lapply(pon1,function(x){x$RR[x$RR==""] <- NA; x})
> maxi<-lapply(pon1, function(x) max(x$RR,na.rm=T))
> maxi<-data.frame(Reduce(rbind, maxi))
> names(maxi)<-"maxi"
> ....
> 
> but the list start from January for every year, and move to February and so
> on. And there is no date in "maxi". Here some sample what I get from my
> simple code.
> 
>> pon1[256:258]$M.256
>        Tanggal   RR
> 1  01-09-2001  5.2
> 2  02-09-2001  0.3
> 3  03-09-2001 29.0
> 4  04-09-2001  0.7
> 5  05-09-2001  9.6
> 6  06-09-2001  0.7
> 7  07-09-2001   NA
> 8  08-09-2001 13.2
> 9  09-09-2001   NA
> 10 10-09-2001   NA
> 11 11-09-2001  0.0
> 12 12-09-2001 66.0
> 13 13-09-2001  0.0
> 14 14-09-2001 57.6
> 15 15-09-2001 18.0
> 16 16-09-2001 29.2
> 17 17-09-2001 52.2
> 18 18-09-2001  7.0
> 19 19-09-2001   NA
> 20 20-09-2001 74.5
> 21 21-09-2001 20.3
> 22 22-09-2001 49.6
> 23 23-09-2001  0.0
> 24 24-09-2001  1.3
> 25 25-09-2001  0.0
> 26 26-09-2001  1.0
> 27 27-09-2001  0.1
> 28 28-09-2001  1.9
> 29 29-09-2001  9.5
> 30 30-09-2001  3.3
> 31 01-10-2001  0.0
> 
> $M.257
>        Tanggal   RR
> 1  01-09-2002  0.0
> 2  02-09-2002  0.0
> 3  03-09-2002  0.0
> 4  04-09-2002 12.8
> 5  05-09-2002  1.0
> 6  06-09-2002  0.0
> 7  07-09-2002   NA
> 8  08-09-2002 22.2
> 9  09-09-2002   NA
> 10 10-09-2002   NA
> 11 11-09-2002  0.0
> 12 12-09-2002  0.0
> 13 13-09-2002  0.0
> 14 14-09-2002   NA
> 15 15-09-2002  0.0
> 16 16-09-2002  0.0
> 17 17-09-2002  0.0
> 18 18-09-2002 13.3
> 19 19-09-2002  0.0
> 20 20-09-2002  0.0
> 21 21-09-2002  0.0
> 22 22-09-2002  0.0
> 23 23-09-2002  0.0
> 24 24-09-2002  0.0
> 25 25-09-2002  0.0
> 26 26-09-2002  0.5
> 27 27-09-2002  2.1
> 28 28-09-2002   NA
> 29 29-09-2002 18.5
> 30 30-09-2002  0.0
> 31 01-10-2002   NA
> 
> $M.258
>        Tanggal   RR
> 1  01-09-2003  0.0
> 2  02-09-2003  0.0
> 3  03-09-2003  0.0
> 4  04-09-2003  4.0
> 5  05-09-2003  0.3
> 6  06-09-2003  0.0
> 7  07-09-2003   NA
> 8  08-09-2003  0.0
> 9  09-09-2003  0.0
> 10 10-09-2003  0.0
> 11 11-09-2003   NA
> 12 12-09-2003  1.0
> 13 13-09-2003  0.0
> 14 14-09-2003 60.0
> 15 15-09-2003  4.5
> 16 16-09-2003  0.1
> 17 17-09-2003  2.1
> 18 18-09-2003   NA
> 19 19-09-2003  0.0
> 20 20-09-2003   NA
> 21 21-09-2003   NA
> 22 22-09-2003 31.5
> 23 23-09-2003 42.0
> 24 24-09-2003 43.3
> 25 25-09-2003  2.8
> 26 26-09-2003 21.4
> 27 27-09-2003  0.8
> 28 28-09-2003 42.3
> 29 29-09-2003  5.3
> 30 30-09-2003 17.3
> 31 01-10-2003  0.0
> 
> 
> Any lead or help is very appreciate.
> 
> Best,
> 
> Ani
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Wed Nov 13 12:46:58 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Wed, 13 Nov 2019 11:46:58 +0000
Subject: [R] QQ plot
In-Reply-To: <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
 <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
 <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>
 <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>
Message-ID: <9c95d83f-d5d4-5ef4-14a4-067dcb344c35@dewey.myzen.co.uk>

Dear Ana

As others have commented this is getting a bit off-topic but here are 
some hints.

It is helpful to distinguish  two sorts of plot: archival plots and 
impact plots. If you want to have an impact plot which gives you a 
picture but possibly at the cost of completeness and accuracy then why not:

1 - plot a sample of your 5 million drawn at random
2 - bin the data and plot median p-value against median expected
3 - deal with overlap by choosing a graphical device which supports 
transparency and plot points in very light grey so the overlap is more 
visible.

Michael

On 12/11/2019 22:04, Ana Marija wrote:
> why I selected only those with P<0.003 to put on QQ plot is because
> the original data set contains 5556249 points and when I extract only
> P<0.001 I am getting 3713 points. Is there is a way to plot the whole
> data set, or choose only the representative points?
> 
> On Tue, Nov 12, 2019 at 3:42 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>
>> the smallest p value in my dataset goes to 9.89e-08. How do I make
>> that known on the new QQ plot with multiplied with 1000 values
>>
>> On Tue, Nov 12, 2019 at 3:37 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>
>>> Just do I need to change the axis when I multiply with 1000 and what
>>> should I put on my axis?
>>>
>>> On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>>
>>>> Hi Duncan,
>>>>
>>>> yes I choose for QQ plot only P<1e-3 and multiplying everything with
>>>> 1000 works great!
>>>> This should not in my understanding influence the interpretation of
>>>> the plot, it is only changing the scale of axis.
>>>>
>>>> Thank you so much,
>>>> Ana
>>>>
>>>> On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>>>>
>>>>> On 12/11/2019 2:56 p.m., Jim Lemon wrote:
>>>>>> I thought about this and did a little study of GWAS and the use of
>>>>>> p-values to assess significant associations. As Ana's plot begins at
>>>>>> values of about 0.001, this seems to imply that almost everything in
>>>>>> the genome is associated to some degree. One expects that most SNPs
>>>>>> will not be associated with a particular condition (p~1), so perhaps
>>>>>> something is going wrong in the calculations that produce the
>>>>>> p-values.
>>>>>
>>>>> I may be misunderstanding your last sentence, but if there is no
>>>>> association, the p-value would usually have a uniform distribution from
>>>>> 0 to 1, it wouldn't be near 1.
>>>>>
>>>>> I'd guess we're not seeing the p values from every test, only those that
>>>>> are less than 0.001.  If that's true, and there are no effects, it makes
>>>>> sense to multiply all of them by 1000 to get U(0,1) values.  On the
>>>>> plot, that would correspond to subtracting 3 from -log10(p), or adding 3
>>>>> to the reference line, as Ana requested.
>>>>>
>>>>> Or just multiply them by 1000 and pass them to qq():
>>>>>
>>>>>       qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
>>>>>
>>>>> As far as I can see, there's no way to tell qqman::qq to move the
>>>>> reference line.
>>>>>
>>>>> Duncan Murdoch
>>>>>
>>>>>>
>>>>>> Jim
>>>>>>
>>>>>> On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
>>>>>> <malone at malonequantitative.com> wrote:
>>>>>>>
>>>>>>> I agree with Abby. That would defeat the purpose of a QQ plot.
>>>>>>>
>>>>>>> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>>>>>>>
>>>>>>>> Hi
>>>>>>>>
>>>>>>>> I'm not familiar with the qqman package, or GWAS studies.
>>>>>>>> However, my guess would be that you're *not* supposed to change the
>>>>>>>> position of the line.
>>>>>>>>
>>>>>>>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>> Hi,
>>>>>>>>>
>>>>>>>>> I was using this library, qqman
>>>>>>>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
>>>>>>>>>
>>>>>>>>> to create QQ plot, attached. How would I change this default abline to
>>>>>>>>> start from the beginning of my QQ line?
>>>>>>>>>
>>>>>>>>> This is my code:
>>>>>>>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
>>>>>>>>>
>>>>>>>>> Thanks
>>>>>>>>> Ana
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>> PLEASE do read the posting guide
>>>>>>>> http://www.R-project.org/posting-guide.html
>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>
>>>>>>>
>>>>>>>           [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From pd@me@ @end|ng |rom cb@@dk  Wed Nov 13 13:08:25 2019
From: pd@me@ @end|ng |rom cb@@dk (Peter Dalgaard)
Date: Wed, 13 Nov 2019 12:08:25 +0000
Subject: [R] [Rd] R 3.6.2 scheduled for December 12
Message-ID: <8FF6B0D5-7342-4F5C-8B82-E0E884AF6F7F@cbs.dk>

Full schedule is available on developer.r-project.org.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From g@@@uu| @end|ng |rom gm@||@com  Wed Nov 13 15:10:01 2019
From: g@@@uu| @end|ng |rom gm@||@com (ani jaya)
Date: Wed, 13 Nov 2019 23:10:01 +0900
Subject: [R] data load from excel files
In-Reply-To: <1805c0c5-2856-ee79-f447-ed6551622e91@sapo.pt>
References: <CAHXS41yxS27QeUXYUrhHZjgqCrRVQUQEpqpMCtZVweqFyAhucg@mail.gmail.com>
 <1805c0c5-2856-ee79-f447-ed6551622e91@sapo.pt>
Message-ID: <CAHXS41x4DG5QuJQXd1f7WDtta88Cxg0R3BjOOaHje-i+8RTvZQ@mail.gmail.com>

Thank you very much Mr. Rui, but for delete the duplicated row I use:

...
library(tidyverse)
alldata<-data.frame(Reduce(rbind, pon1))
c<-(which(duplicated(alldata$Tanggal))) #duplicate
alldata<-alldata[-c,]
attach(alldata)
....

because not every last row from every df is bad one.

Another problem is I want to know when the max value is occurred. So
basically I have maximum value every month (maxi, n=360, from 1986 to 2015)
and I want to find annual_maxima.


...
maxi<-lapply(pon1, function(x) max(x$RR,na.rm=T))
maxi<-data.frame(Reduce(rbind, maxi))
names(maxi)<-"maxi"
annual_maxima <- rep(NA,30)
date <- rep(NA,30)
for(i in 1:30){

  annual_maxima[i] <- max(maxi$maxi[(i*12-11):(i*12)])
  date[i]<-Tanggal[which(RR==annual_maxima[i])]
}
....

Here "alldata" contain "Tanggal" in this case is date and rainfall ("RR").
What I get is error stated that:

In date[i] <- Tanggal[which(RR == annual_maxima[i])] :  number of
items to replace is not a multiple of replacement length


Maybe you have some idea where the problem is, I would be thankful.

Best,
Ani





On Wed, Nov 13, 2019 at 5:49 PM Rui Barradas <ruipbarradas at sapo.pt> wrote:

> Hello,
>
> Maybe the following will get you close to what you want.
>
>
> # remove the last row from every df
> pon1 <- lapply(pon1, function(DF){
>    DF[[1]] <- as.Date(DF[["Tanggal"]], "%d-%m-%Y")
>    DF[-nrow(DF), ]
> })
>
>
> # order the list by year-month
> inx_ym <- sapply(pon1, function(DF){
>    format(DF[["Tanggal"]][1], "%Y-%m")
> })
> pon1 <- pon1[order(inx_ym)]
>
>
> # get the minimum and maximum of every "RR"
> min.RR <- sapply(pon1, function(DF) min(DF[["RR"]], na.rm = TRUE))
> max.RR <- sapply(pon1, function(DF) max(DF[["RR"]], na.rm = TRUE))
>
>
> Hope this helps,
>
> Rui Barradas
>
>
>
> ?s 07:50 de 13/11/19, ani jaya escreveu:
> > Dear R-Help,
> >
> > I have 30 of year-based excel files and each file contain month sheets. I
> > have some problem here. My data is daily rainfall but there is extra 1
> day
> > (first date of next month) for several sheets. My main goal is to get the
> > minimum value for every month.
> >
> > First, how to extract those data to list of data frame based on year and
> > delete every overlapping date?
> > Second, how to sort it based on date with ascending order (old to new)?
> > Third, how to get the maximum together with the date?
> >
> > I did this one,
> >
> > ...
> > file.list <- list.files(pattern='*.xlsx')
> > file.list<-mixedsort(file.list)
> >
> > #
> >
> https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
> >
> > read_excel_allsheets <- function(filename, tibble = FALSE) {
> >    sheets <- readxl::excel_sheets(filename)
> >    x <- lapply(sheets, function(X) read.xlsx(filename, sheet=X,
> rows=9:40,
> > cols=1:2))
> >    if(!tibble) x <- lapply(x, as.data.frame)
> >    names(x) <- sheets
> >    x
> > }
> >
> > pon<-lapply(file.list, function(i) read_excel_allsheets(i))
> > pon1<-do.call("rbind",pon)
> > names(pon1) <- paste0("M.", 1:360)
> > pon1 <-lapply(pon1,function(x){x$RR[x$RR==8888] <- NA; x})
> > pon1 <-lapply(pon1,function(x){x$RR[x$RR==""] <- NA; x})
> > maxi<-lapply(pon1, function(x) max(x$RR,na.rm=T))
> > maxi<-data.frame(Reduce(rbind, maxi))
> > names(maxi)<-"maxi"
> > ....
> >
> > but the list start from January for every year, and move to February and
> so
> > on. And there is no date in "maxi". Here some sample what I get from my
> > simple code.
> >
> >> pon1[256:258]$M.256
> >        Tanggal   RR
> > 1  01-09-2001  5.2
> > 2  02-09-2001  0.3
> > 3  03-09-2001 29.0
> > 4  04-09-2001  0.7
> > 5  05-09-2001  9.6
> > 6  06-09-2001  0.7
> > 7  07-09-2001   NA
> > 8  08-09-2001 13.2
> > 9  09-09-2001   NA
> > 10 10-09-2001   NA
> > 11 11-09-2001  0.0
> > 12 12-09-2001 66.0
> > 13 13-09-2001  0.0
> > 14 14-09-2001 57.6
> > 15 15-09-2001 18.0
> > 16 16-09-2001 29.2
> > 17 17-09-2001 52.2
> > 18 18-09-2001  7.0
> > 19 19-09-2001   NA
> > 20 20-09-2001 74.5
> > 21 21-09-2001 20.3
> > 22 22-09-2001 49.6
> > 23 23-09-2001  0.0
> > 24 24-09-2001  1.3
> > 25 25-09-2001  0.0
> > 26 26-09-2001  1.0
> > 27 27-09-2001  0.1
> > 28 28-09-2001  1.9
> > 29 29-09-2001  9.5
> > 30 30-09-2001  3.3
> > 31 01-10-2001  0.0
> >
> > $M.257
> >        Tanggal   RR
> > 1  01-09-2002  0.0
> > 2  02-09-2002  0.0
> > 3  03-09-2002  0.0
> > 4  04-09-2002 12.8
> > 5  05-09-2002  1.0
> > 6  06-09-2002  0.0
> > 7  07-09-2002   NA
> > 8  08-09-2002 22.2
> > 9  09-09-2002   NA
> > 10 10-09-2002   NA
> > 11 11-09-2002  0.0
> > 12 12-09-2002  0.0
> > 13 13-09-2002  0.0
> > 14 14-09-2002   NA
> > 15 15-09-2002  0.0
> > 16 16-09-2002  0.0
> > 17 17-09-2002  0.0
> > 18 18-09-2002 13.3
> > 19 19-09-2002  0.0
> > 20 20-09-2002  0.0
> > 21 21-09-2002  0.0
> > 22 22-09-2002  0.0
> > 23 23-09-2002  0.0
> > 24 24-09-2002  0.0
> > 25 25-09-2002  0.0
> > 26 26-09-2002  0.5
> > 27 27-09-2002  2.1
> > 28 28-09-2002   NA
> > 29 29-09-2002 18.5
> > 30 30-09-2002  0.0
> > 31 01-10-2002   NA
> >
> > $M.258
> >        Tanggal   RR
> > 1  01-09-2003  0.0
> > 2  02-09-2003  0.0
> > 3  03-09-2003  0.0
> > 4  04-09-2003  4.0
> > 5  05-09-2003  0.3
> > 6  06-09-2003  0.0
> > 7  07-09-2003   NA
> > 8  08-09-2003  0.0
> > 9  09-09-2003  0.0
> > 10 10-09-2003  0.0
> > 11 11-09-2003   NA
> > 12 12-09-2003  1.0
> > 13 13-09-2003  0.0
> > 14 14-09-2003 60.0
> > 15 15-09-2003  4.5
> > 16 16-09-2003  0.1
> > 17 17-09-2003  2.1
> > 18 18-09-2003   NA
> > 19 19-09-2003  0.0
> > 20 20-09-2003   NA
> > 21 21-09-2003   NA
> > 22 22-09-2003 31.5
> > 23 23-09-2003 42.0
> > 24 24-09-2003 43.3
> > 25 25-09-2003  2.8
> > 26 26-09-2003 21.4
> > 27 27-09-2003  0.8
> > 28 28-09-2003 42.3
> > 29 29-09-2003  5.3
> > 30 30-09-2003 17.3
> > 31 01-10-2003  0.0
> >
> >
> > Any lead or help is very appreciate.
> >
> > Best,
> >
> > Ani
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Nov 13 16:10:28 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 13 Nov 2019 15:10:28 +0000
Subject: [R] data load from excel files
In-Reply-To: <CAHXS41x4DG5QuJQXd1f7WDtta88Cxg0R3BjOOaHje-i+8RTvZQ@mail.gmail.com>
References: <CAHXS41yxS27QeUXYUrhHZjgqCrRVQUQEpqpMCtZVweqFyAhucg@mail.gmail.com>
 <1805c0c5-2856-ee79-f447-ed6551622e91@sapo.pt>
 <CAHXS41x4DG5QuJQXd1f7WDtta88Cxg0R3BjOOaHje-i+8RTvZQ@mail.gmail.com>
Message-ID: <c0016fce-9c9d-fd11-2cc1-f053ebbcb1c7@sapo.pt>

Hello,

Try which.max?

Hope this helps,

Rui Barradas

?s 14:10 de 13/11/19, ani jaya escreveu:
> Thank you very much Mr. Rui, but for delete the duplicated row I use:
> 
> ...
> library(tidyverse)
> alldata<-data.frame(Reduce(rbind, pon1))
> c<-(which(duplicated(alldata$Tanggal))) #duplicate
> alldata<-alldata[-c,]
> attach(alldata)
> ....
> 
> because not every last row from every df is bad one.
> 
> Another problem is I want to know when the max value is occurred. So 
> basically I have maximum value every month (maxi, n=360, from 1986 to 
> 2015) and I want to find annual_maxima.
> 
> 
> ...
> maxi<-lapply(pon1, function(x) max(x$RR,na.rm=T))
> maxi<-data.frame(Reduce(rbind, maxi))
> names(maxi)<-"maxi"
> annual_maxima <- rep(NA,30)
> date <- rep(NA,30)
> for(i in 1:30){
> 
>  ? annual_maxima[i] <- max(maxi$maxi[(i*12-11):(i*12)])
>  ? date[i]<-Tanggal[which(RR==annual_maxima[i])]
> }
> ....
> 
> Here "alldata" contain "Tanggal" in this case is date and rainfall 
> ("RR"). What I get is error stated that:
> 
> In date[i] <- Tanggal[which(RR == annual_maxima[i])] : number of items 
> to replace is not a multiple of replacement length
> 
> 
> Maybe you have some idea where the problem is, I would be thankful.
> 
> Best,
> Ani
> 
> 
> 
> 
> 
> On Wed, Nov 13, 2019 at 5:49 PM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     Maybe the following will get you close to what you want.
> 
> 
>     # remove the last row from every df
>     pon1 <- lapply(pon1, function(DF){
>      ? ?DF[[1]] <- as.Date(DF[["Tanggal"]], "%d-%m-%Y")
>      ? ?DF[-nrow(DF), ]
>     })
> 
> 
>     # order the list by year-month
>     inx_ym <- sapply(pon1, function(DF){
>      ? ?format(DF[["Tanggal"]][1], "%Y-%m")
>     })
>     pon1 <- pon1[order(inx_ym)]
> 
> 
>     # get the minimum and maximum of every "RR"
>     min.RR <- sapply(pon1, function(DF) min(DF[["RR"]], na.rm = TRUE))
>     max.RR <- sapply(pon1, function(DF) max(DF[["RR"]], na.rm = TRUE))
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
> 
> 
>     ?s 07:50 de 13/11/19, ani jaya escreveu:
>      > Dear R-Help,
>      >
>      > I have 30 of year-based excel files and each file contain month
>     sheets. I
>      > have some problem here. My data is daily rainfall but there is
>     extra 1 day
>      > (first date of next month) for several sheets. My main goal is to
>     get the
>      > minimum value for every month.
>      >
>      > First, how to extract those data to list of data frame based on
>     year and
>      > delete every overlapping date?
>      > Second, how to sort it based on date with ascending order (old to
>     new)?
>      > Third, how to get the maximum together with the date?
>      >
>      > I did this one,
>      >
>      > ...
>      > file.list <- list.files(pattern='*.xlsx')
>      > file.list<-mixedsort(file.list)
>      >
>      > #
>      >
>     https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
>      >
>      > read_excel_allsheets <- function(filename, tibble = FALSE) {
>      >? ? sheets <- readxl::excel_sheets(filename)
>      >? ? x <- lapply(sheets, function(X) read.xlsx(filename, sheet=X,
>     rows=9:40,
>      > cols=1:2))
>      >? ? if(!tibble) x <- lapply(x, as.data.frame)
>      >? ? names(x) <- sheets
>      >? ? x
>      > }
>      >
>      > pon<-lapply(file.list, function(i) read_excel_allsheets(i))
>      > pon1<-do.call("rbind",pon)
>      > names(pon1) <- paste0("M.", 1:360)
>      > pon1 <-lapply(pon1,function(x){x$RR[x$RR==8888] <- NA; x})
>      > pon1 <-lapply(pon1,function(x){x$RR[x$RR==""] <- NA; x})
>      > maxi<-lapply(pon1, function(x) max(x$RR,na.rm=T))
>      > maxi<-data.frame(Reduce(rbind, maxi))
>      > names(maxi)<-"maxi"
>      > ....
>      >
>      > but the list start from January for every year, and move to
>     February and so
>      > on. And there is no date in "maxi". Here some sample what I get
>     from my
>      > simple code.
>      >
>      >> pon1[256:258]$M.256
>      >? ? ? ? Tanggal? ?RR
>      > 1? 01-09-2001? 5.2
>      > 2? 02-09-2001? 0.3
>      > 3? 03-09-2001 29.0
>      > 4? 04-09-2001? 0.7
>      > 5? 05-09-2001? 9.6
>      > 6? 06-09-2001? 0.7
>      > 7? 07-09-2001? ?NA
>      > 8? 08-09-2001 13.2
>      > 9? 09-09-2001? ?NA
>      > 10 10-09-2001? ?NA
>      > 11 11-09-2001? 0.0
>      > 12 12-09-2001 66.0
>      > 13 13-09-2001? 0.0
>      > 14 14-09-2001 57.6
>      > 15 15-09-2001 18.0
>      > 16 16-09-2001 29.2
>      > 17 17-09-2001 52.2
>      > 18 18-09-2001? 7.0
>      > 19 19-09-2001? ?NA
>      > 20 20-09-2001 74.5
>      > 21 21-09-2001 20.3
>      > 22 22-09-2001 49.6
>      > 23 23-09-2001? 0.0
>      > 24 24-09-2001? 1.3
>      > 25 25-09-2001? 0.0
>      > 26 26-09-2001? 1.0
>      > 27 27-09-2001? 0.1
>      > 28 28-09-2001? 1.9
>      > 29 29-09-2001? 9.5
>      > 30 30-09-2001? 3.3
>      > 31 01-10-2001? 0.0
>      >
>      > $M.257
>      >? ? ? ? Tanggal? ?RR
>      > 1? 01-09-2002? 0.0
>      > 2? 02-09-2002? 0.0
>      > 3? 03-09-2002? 0.0
>      > 4? 04-09-2002 12.8
>      > 5? 05-09-2002? 1.0
>      > 6? 06-09-2002? 0.0
>      > 7? 07-09-2002? ?NA
>      > 8? 08-09-2002 22.2
>      > 9? 09-09-2002? ?NA
>      > 10 10-09-2002? ?NA
>      > 11 11-09-2002? 0.0
>      > 12 12-09-2002? 0.0
>      > 13 13-09-2002? 0.0
>      > 14 14-09-2002? ?NA
>      > 15 15-09-2002? 0.0
>      > 16 16-09-2002? 0.0
>      > 17 17-09-2002? 0.0
>      > 18 18-09-2002 13.3
>      > 19 19-09-2002? 0.0
>      > 20 20-09-2002? 0.0
>      > 21 21-09-2002? 0.0
>      > 22 22-09-2002? 0.0
>      > 23 23-09-2002? 0.0
>      > 24 24-09-2002? 0.0
>      > 25 25-09-2002? 0.0
>      > 26 26-09-2002? 0.5
>      > 27 27-09-2002? 2.1
>      > 28 28-09-2002? ?NA
>      > 29 29-09-2002 18.5
>      > 30 30-09-2002? 0.0
>      > 31 01-10-2002? ?NA
>      >
>      > $M.258
>      >? ? ? ? Tanggal? ?RR
>      > 1? 01-09-2003? 0.0
>      > 2? 02-09-2003? 0.0
>      > 3? 03-09-2003? 0.0
>      > 4? 04-09-2003? 4.0
>      > 5? 05-09-2003? 0.3
>      > 6? 06-09-2003? 0.0
>      > 7? 07-09-2003? ?NA
>      > 8? 08-09-2003? 0.0
>      > 9? 09-09-2003? 0.0
>      > 10 10-09-2003? 0.0
>      > 11 11-09-2003? ?NA
>      > 12 12-09-2003? 1.0
>      > 13 13-09-2003? 0.0
>      > 14 14-09-2003 60.0
>      > 15 15-09-2003? 4.5
>      > 16 16-09-2003? 0.1
>      > 17 17-09-2003? 2.1
>      > 18 18-09-2003? ?NA
>      > 19 19-09-2003? 0.0
>      > 20 20-09-2003? ?NA
>      > 21 21-09-2003? ?NA
>      > 22 22-09-2003 31.5
>      > 23 23-09-2003 42.0
>      > 24 24-09-2003 43.3
>      > 25 25-09-2003? 2.8
>      > 26 26-09-2003 21.4
>      > 27 27-09-2003? 0.8
>      > 28 28-09-2003 42.3
>      > 29 29-09-2003? 5.3
>      > 30 30-09-2003 17.3
>      > 31 01-10-2003? 0.0
>      >
>      >
>      > Any lead or help is very appreciate.
>      >
>      > Best,
>      >
>      > Ani
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>      >
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Wed Nov 13 21:12:50 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Wed, 13 Nov 2019 14:12:50 -0600
Subject: [R] QQ plot
In-Reply-To: <9c95d83f-d5d4-5ef4-14a4-067dcb344c35@dewey.myzen.co.uk>
References: <CAF9-5jMEB51ZmpOD=vv4ETcGfXfE7xkmn=zV8de9-cB-1O9ciA@mail.gmail.com>
 <CAB8pepzDEQOA_owo6MK7w7KvgQNuZxZEdM-taPgnenAVEG6Usg@mail.gmail.com>
 <CAJc=yOFUPh5+4qUTrf7MdrGJPy_Yo8++bRx=oNk57fd8jsto2g@mail.gmail.com>
 <CA+8X3fUQ7gTOSZgcmVgaXRJOYJcMX_QVwkYyx78swcaX5b0w3A@mail.gmail.com>
 <108af875-08f6-a807-fa8c-d23e08854a84@gmail.com>
 <CAF9-5jNKx4tVoSpt1XtWOT09yzQMJXw3fH7Hsz3pGj9WrUnLpw@mail.gmail.com>
 <CAF9-5jPPtL8HGh7U_VhoJKN1-keDAqVJW1YHOwmnby3BDk=cJw@mail.gmail.com>
 <CAF9-5jOQKcNNe2BFLz871-dVpUwQ1r0zk6y-2R9AttrYaogn0w@mail.gmail.com>
 <CAF9-5jMyk8yQoJd1h+1P=iKM87YHTzN7sa8z2DLJ=Jhp-_QXNQ@mail.gmail.com>
 <9c95d83f-d5d4-5ef4-14a4-067dcb344c35@dewey.myzen.co.uk>
Message-ID: <CAF9-5jO4_a6s28w7GxvxHZwc0D6rKUaTQNkS45JcKv0NyHeZbg@mail.gmail.com>

Hi Michael,

Thank you so much for that valuable idea!
I will try first to clump or remove SNPs in LD and maybe the situation
would improve.
But this procedure of yours is definitely something that would come
handy in future!

Cheers,
Ana

On Wed, Nov 13, 2019 at 5:47 AM Michael Dewey <lists at dewey.myzen.co.uk> wrote:
>
> Dear Ana
>
> As others have commented this is getting a bit off-topic but here are
> some hints.
>
> It is helpful to distinguish  two sorts of plot: archival plots and
> impact plots. If you want to have an impact plot which gives you a
> picture but possibly at the cost of completeness and accuracy then why not:
>
> 1 - plot a sample of your 5 million drawn at random
> 2 - bin the data and plot median p-value against median expected
> 3 - deal with overlap by choosing a graphical device which supports
> transparency and plot points in very light grey so the overlap is more
> visible.
>
> Michael
>
> On 12/11/2019 22:04, Ana Marija wrote:
> > why I selected only those with P<0.003 to put on QQ plot is because
> > the original data set contains 5556249 points and when I extract only
> > P<0.001 I am getting 3713 points. Is there is a way to plot the whole
> > data set, or choose only the representative points?
> >
> > On Tue, Nov 12, 2019 at 3:42 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>
> >> the smallest p value in my dataset goes to 9.89e-08. How do I make
> >> that known on the new QQ plot with multiplied with 1000 values
> >>
> >> On Tue, Nov 12, 2019 at 3:37 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>
> >>> Just do I need to change the axis when I multiply with 1000 and what
> >>> should I put on my axis?
> >>>
> >>> On Tue, Nov 12, 2019 at 3:07 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>>
> >>>> Hi Duncan,
> >>>>
> >>>> yes I choose for QQ plot only P<1e-3 and multiplying everything with
> >>>> 1000 works great!
> >>>> This should not in my understanding influence the interpretation of
> >>>> the plot, it is only changing the scale of axis.
> >>>>
> >>>> Thank you so much,
> >>>> Ana
> >>>>
> >>>> On Tue, Nov 12, 2019 at 2:51 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >>>>>
> >>>>> On 12/11/2019 2:56 p.m., Jim Lemon wrote:
> >>>>>> I thought about this and did a little study of GWAS and the use of
> >>>>>> p-values to assess significant associations. As Ana's plot begins at
> >>>>>> values of about 0.001, this seems to imply that almost everything in
> >>>>>> the genome is associated to some degree. One expects that most SNPs
> >>>>>> will not be associated with a particular condition (p~1), so perhaps
> >>>>>> something is going wrong in the calculations that produce the
> >>>>>> p-values.
> >>>>>
> >>>>> I may be misunderstanding your last sentence, but if there is no
> >>>>> association, the p-value would usually have a uniform distribution from
> >>>>> 0 to 1, it wouldn't be near 1.
> >>>>>
> >>>>> I'd guess we're not seeing the p values from every test, only those that
> >>>>> are less than 0.001.  If that's true, and there are no effects, it makes
> >>>>> sense to multiply all of them by 1000 to get U(0,1) values.  On the
> >>>>> plot, that would correspond to subtracting 3 from -log10(p), or adding 3
> >>>>> to the reference line, as Ana requested.
> >>>>>
> >>>>> Or just multiply them by 1000 and pass them to qq():
> >>>>>
> >>>>>       qq(dd$P*1000, main = "Q-Q plot of small GWAS p-values")
> >>>>>
> >>>>> As far as I can see, there's no way to tell qqman::qq to move the
> >>>>> reference line.
> >>>>>
> >>>>> Duncan Murdoch
> >>>>>
> >>>>>>
> >>>>>> Jim
> >>>>>>
> >>>>>> On Wed, Nov 13, 2019 at 12:28 AM Patrick (Malone Quantitative)
> >>>>>> <malone at malonequantitative.com> wrote:
> >>>>>>>
> >>>>>>> I agree with Abby. That would defeat the purpose of a QQ plot.
> >>>>>>>
> >>>>>>> On Mon, Nov 11, 2019, 9:54 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >>>>>>>
> >>>>>>>> Hi
> >>>>>>>>
> >>>>>>>> I'm not familiar with the qqman package, or GWAS studies.
> >>>>>>>> However, my guess would be that you're *not* supposed to change the
> >>>>>>>> position of the line.
> >>>>>>>>
> >>>>>>>> On Tue, Nov 12, 2019 at 11:48 AM Ana Marija <sokovic.anamarija at gmail.com>
> >>>>>>>> wrote:
> >>>>>>>>>
> >>>>>>>>> Hi,
> >>>>>>>>>
> >>>>>>>>> I was using this library, qqman
> >>>>>>>>> https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html
> >>>>>>>>>
> >>>>>>>>> to create QQ plot, attached. How would I change this default abline to
> >>>>>>>>> start from the beginning of my QQ line?
> >>>>>>>>>
> >>>>>>>>> This is my code:
> >>>>>>>>> qq(dd$P, main = "Q-Q plot of GWAS p-values")
> >>>>>>>>>
> >>>>>>>>> Thanks
> >>>>>>>>> Ana
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>
> >>>>>>>> ______________________________________________
> >>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>> PLEASE do read the posting guide
> >>>>>>>> http://www.R-project.org/posting-guide.html
> >>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>
> >>>>>>>
> >>>>>>>           [[alternative HTML version deleted]]
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html


From Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com  Thu Nov 14 16:40:42 2019
From: Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Thu, 14 Nov 2019 15:40:42 +0000
Subject: [R] Can file size affect how na.strings operates in a read.table
 call?
Message-ID: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>

Hi,

I have this generic function to read ASCII data files. It is essentially a wrapper around the read.table function. My function is used in a large variety of situations and has no a priori knowledge about the data file it is asked to read. Nothing is known about file size, variable types, variable names, or data table dimensions.

One argument of my function is na.strings which is passed down to read.table.

Recently, a user tried to read a data file of ~ 80 Mo (~ 93000 rows by ~ 160 columns) using na.strings = c('-99', '.') with the intention of interpreting '.' and '-99'
strings as the internal missing data NA. Dots were converted to NA appropriately. However, not all -99 values in the data were interpreted as NA. In some variables, -99 were converted to NA, while in others -99 was read as a number. More surprisingly, when the data file was cut in smaller chunks (ie, by dropping either rows or columns) saved in multiple files, the function calls applied on the new data files resulted in the correct conversion of the -99 values into NAs.

In all cases, the data frames produced by read.table contained the expected number of records.

While, on face value, it appears that file size affects how the na.strings argument operates, I wondering if there is something else at play here. 

Unfortunately, I cannot share the data file for confidentiality reason but was wondering if you could suggest some checks I could perform to get to the bottom on this issue.

Thank you in advance for your help and sorry for the lack of reproducible example.



From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Nov 14 16:52:54 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 Nov 2019 07:52:54 -0800
Subject: [R] 
 Can file size affect how na.strings operates in a read.table call?
In-Reply-To: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>
References: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>
Message-ID: <1AD5435A-C48C-419B-A509-952C414381F3@dcn.davis.ca.us>

Check for extraneous spaces. You may need more variations of the na.strings.

On November 14, 2019 7:40:42 AM PST, Sebastien Bihorel via R-help <r-help at r-project.org> wrote:
>Hi,
>
>I have this generic function to read ASCII data files. It is
>essentially a wrapper around the read.table function. My function is
>used in a large variety of situations and has no a priori knowledge
>about the data file it is asked to read. Nothing is known about file
>size, variable types, variable names, or data table dimensions.
>
>One argument of my function is na.strings which is passed down to
>read.table.
>
>Recently, a user tried to read a data file of ~ 80 Mo (~ 93000 rows by
>~ 160 columns) using na.strings = c('-99', '.') with the intention of
>interpreting '.' and '-99'
>strings as the internal missing data NA. Dots were converted to NA
>appropriately. However, not all -99 values in the data were interpreted
>as NA. In some variables, -99 were converted to NA, while in others -99
>was read as a number. More surprisingly, when the data file was cut in
>smaller chunks (ie, by dropping either rows or columns) saved in
>multiple files, the function calls applied on the new data files
>resulted in the correct conversion of the -99 values into NAs.
>
>In all cases, the data frames produced by read.table contained the
>expected number of records.
>
>While, on face value, it appears that file size affects how the
>na.strings argument operates, I wondering if there is something else at
>play here. 
>
>Unfortunately, I cannot share the data file for confidentiality reason
>but was wondering if you could suggest some checks I could perform to
>get to the bottom on this issue.
>
>Thank you in advance for your help and sorry for the lack of
>reproducible example.
>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com  Thu Nov 14 16:57:10 2019
From: Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Thu, 14 Nov 2019 15:57:10 +0000
Subject: [R] 
 Can file size affect how na.strings operates in a read.table call?
In-Reply-To: <1AD5435A-C48C-419B-A509-952C414381F3@dcn.davis.ca.us>
References: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>,
 <1AD5435A-C48C-419B-A509-952C414381F3@dcn.davis.ca.us>
Message-ID: <CH2PR19MB3864E9BDD184EBD83003EA2992710@CH2PR19MB3864.namprd19.prod.outlook.com>

The data file is a csv file. Some text variables contain spaces.

"Check for extraneous spaces"
Are there specific locations that would be more critical than others?


________________________________
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Sent: Thursday, November 14, 2019 10:52
To: Sebastien Bihorel <Sebastien.Bihorel at cognigencorp.com>; Sebastien Bihorel via R-help <r-help at r-project.org>; r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Can file size affect how na.strings operates in a read.table call?

Check for extraneous spaces. You may need more variations of the na.strings.

On November 14, 2019 7:40:42 AM PST, Sebastien Bihorel via R-help <r-help at r-project.org> wrote:
>Hi,
>
>I have this generic function to read ASCII data files. It is
>essentially a wrapper around the read.table function. My function is
>used in a large variety of situations and has no a priori knowledge
>about the data file it is asked to read. Nothing is known about file
>size, variable types, variable names, or data table dimensions.
>
>One argument of my function is na.strings which is passed down to
>read.table.
>
>Recently, a user tried to read a data file of ~ 80 Mo (~ 93000 rows by
>~ 160 columns) using na.strings = c('-99', '.') with the intention of
>interpreting '.' and '-99'
>strings as the internal missing data NA. Dots were converted to NA
>appropriately. However, not all -99 values in the data were interpreted
>as NA. In some variables, -99 were converted to NA, while in others -99
>was read as a number. More surprisingly, when the data file was cut in
>smaller chunks (ie, by dropping either rows or columns) saved in
>multiple files, the function calls applied on the new data files
>resulted in the correct conversion of the -99 values into NAs.
>
>In all cases, the data frames produced by read.table contained the
>expected number of records.
>
>While, on face value, it appears that file size affects how the
>na.strings argument operates, I wondering if there is something else at
>play here.
>
>Unfortunately, I cannot share the data file for confidentiality reason
>but was wondering if you could suggest some checks I could perform to
>get to the bottom on this issue.
>
>Thank you in advance for your help and sorry for the lack of
>reproducible example.
>
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

--
Sent from my phone. Please excuse my brevity.

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Thu Nov 14 17:35:06 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Thu, 14 Nov 2019 08:35:06 -0800 (PST)
Subject: [R] 
 Can file size affect how na.strings operates in a read.table call?
In-Reply-To: <CH2PR19MB3864E9BDD184EBD83003EA2992710@CH2PR19MB3864.namprd19.prod.outlook.com>
References: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>,
 <1AD5435A-C48C-419B-A509-952C414381F3@dcn.davis.ca.us>
 <CH2PR19MB3864E9BDD184EBD83003EA2992710@CH2PR19MB3864.namprd19.prod.outlook.com>
Message-ID: <alpine.BSF.2.00.1911140830220.83728@pedal.dcn.davis.ca.us>

Consider the following sample:

#####
s <- "A,B,C
0,0,0
1,-99,-99
2,-99 ,-99
3, -99, -99
"

dta_notok <- read.csv( text = s
                      , header=TRUE
                      , na.strings = c( "-99", "" )
                      )

dta_ok <- read.csv( text = s
                   , header=TRUE
                   , na.strings = c( "-99", " -99"
                                   , "-99 ", ""
                                   )
                   )

library(data.table)

fdt_ok <- fread( text = s, na.strings=c( "-99", "" ) )
fdta_ok <- as.data.frame( fdt_ok )
#####

Leading and trailing spaces cause problems. The data.table::fread function 
has a strip.white argument that defaults to TRUE, but the resulting object 
is a data.table which has different semantics than a data.frame.

On Thu, 14 Nov 2019, Sebastien Bihorel wrote:

> The data file is a csv file. Some text variables contain spaces.
> 
> "Check for extraneous spaces"
> Are there specific locations that would be more critical than others?
> 
> 
> ____________________________________________________________________________
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Sent: Thursday, November 14, 2019 10:52
> To: Sebastien Bihorel <Sebastien.Bihorel at cognigencorp.com>; Sebastien
> Bihorel via R-help <r-help at r-project.org>; r-help at r-project.org
> <r-help at r-project.org>
> Subject: Re: [R] Can file size affect how na.strings operates in a
> read.table call? ?
> Check for extraneous spaces. You may need more variations of the na.strings.
> 
> On November 14, 2019 7:40:42 AM PST, Sebastien Bihorel via R-help
> <r-help at r-project.org> wrote:
> >Hi,
> >
> >I have this generic function to read ASCII data files. It is
> >essentially a wrapper around the read.table function. My function is
> >used in a large variety of situations and has no a priori knowledge
> >about the data file it is asked to read. Nothing is known about file
> >size, variable types, variable names, or data table dimensions.
> >
> >One argument of my function is na.strings which is passed down to
> >read.table.
> >
> >Recently, a user tried to read a data file of ~ 80 Mo (~ 93000 rows by
> >~ 160 columns) using na.strings = c('-99', '.') with the intention of
> >interpreting '.' and '-99'
> >strings as the internal missing data NA. Dots were converted to NA
> >appropriately. However, not all -99 values in the data were interpreted
> >as NA. In some variables, -99 were converted to NA, while in others -99
> >was read as a number. More surprisingly, when the data file was cut in
> >smaller chunks (ie, by dropping either rows or columns) saved in
> >multiple files, the function calls applied on the new data files
> >resulted in the correct conversion of the -99 values into NAs.
> >
> >In all cases, the data frames produced by read.table contained the
> >expected number of records.
> >
> >While, on face value, it appears that file size affects how the
> >na.strings argument operates, I wondering if there is something else at
> >play here.
> >
> >Unfortunately, I cannot share the data file for confidentiality reason
> >but was wondering if you could suggest some checks I could perform to
> >get to the bottom on this issue.
> >
> >Thank you in advance for your help and sorry for the lack of
> >reproducible example.
> >
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Sent from my phone. Please excuse my brevity.
> 
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------

From wdun|@p @end|ng |rom t|bco@com  Thu Nov 14 17:51:51 2019
From: wdun|@p @end|ng |rom t|bco@com (William Dunlap)
Date: Thu, 14 Nov 2019 08:51:51 -0800
Subject: [R] 
 Can file size affect how na.strings operates in a read.table call?
In-Reply-To: <alpine.BSF.2.00.1911140830220.83728@pedal.dcn.davis.ca.us>
References: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>
 <1AD5435A-C48C-419B-A509-952C414381F3@dcn.davis.ca.us>
 <CH2PR19MB3864E9BDD184EBD83003EA2992710@CH2PR19MB3864.namprd19.prod.outlook.com>
 <alpine.BSF.2.00.1911140830220.83728@pedal.dcn.davis.ca.us>
Message-ID: <CAF8bMcaj2T7_p7hg_E-ZE4YK6MBVmW47n9K+5cmwKYWMzc3rxQ@mail.gmail.com>

read.table (and friends) also have the strip.white argument:

> s <- "A,B,C\n0,0,0\n1,-99,-99\n2,-99 ,-99\n3, -99, -99\n"
> read.csv(text=s, header=TRUE, na.strings="-99", strip.white=TRUE)
  A  B  C
1 0  0  0
2 1 NA NA
3 2 NA NA
4 3 NA NA
> read.csv(text=s, header=TRUE, na.strings="-99", strip.white=FALSE)
  A   B   C
1 0   0   0
2 1  NA  NA
3 2 -99  NA
4 3 -99 -99

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Nov 14, 2019 at 8:35 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> Consider the following sample:
>
> #####
> s <- "A,B,C
> 0,0,0
> 1,-99,-99
> 2,-99 ,-99
> 3, -99, -99
> "
>
> dta_notok <- read.csv( text = s
>                       , header=TRUE
>                       , na.strings = c( "-99", "" )
>                       )
>
> dta_ok <- read.csv( text = s
>                    , header=TRUE
>                    , na.strings = c( "-99", " -99"
>                                    , "-99 ", ""
>                                    )
>                    )
>
> library(data.table)
>
> fdt_ok <- fread( text = s, na.strings=c( "-99", "" ) )
> fdta_ok <- as.data.frame( fdt_ok )
> #####
>
> Leading and trailing spaces cause problems. The data.table::fread function
> has a strip.white argument that defaults to TRUE, but the resulting object
> is a data.table which has different semantics than a data.frame.
>
> On Thu, 14 Nov 2019, Sebastien Bihorel wrote:
>
> > The data file is a csv file. Some text variables contain spaces.
> >
> > "Check for extraneous spaces"
> > Are there specific locations that would be more critical than others?
> >
> >
> >
> ____________________________________________________________________________
> > From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> > Sent: Thursday, November 14, 2019 10:52
> > To: Sebastien Bihorel <Sebastien.Bihorel at cognigencorp.com>; Sebastien
> > Bihorel via R-help <r-help at r-project.org>; r-help at r-project.org
> > <r-help at r-project.org>
> > Subject: Re: [R] Can file size affect how na.strings operates in a
> > read.table call?
> > Check for extraneous spaces. You may need more variations of the
> na.strings.
> >
> > On November 14, 2019 7:40:42 AM PST, Sebastien Bihorel via R-help
> > <r-help at r-project.org> wrote:
> > >Hi,
> > >
> > >I have this generic function to read ASCII data files. It is
> > >essentially a wrapper around the read.table function. My function is
> > >used in a large variety of situations and has no a priori knowledge
> > >about the data file it is asked to read. Nothing is known about file
> > >size, variable types, variable names, or data table dimensions.
> > >
> > >One argument of my function is na.strings which is passed down to
> > >read.table.
> > >
> > >Recently, a user tried to read a data file of ~ 80 Mo (~ 93000 rows by
> > >~ 160 columns) using na.strings = c('-99', '.') with the intention of
> > >interpreting '.' and '-99'
> > >strings as the internal missing data NA. Dots were converted to NA
> > >appropriately. However, not all -99 values in the data were interpreted
> > >as NA. In some variables, -99 were converted to NA, while in others -99
> > >was read as a number. More surprisingly, when the data file was cut in
> > >smaller chunks (ie, by dropping either rows or columns) saved in
> > >multiple files, the function calls applied on the new data files
> > >resulted in the correct conversion of the -99 values into NAs.
> > >
> > >In all cases, the data frames produced by read.table contained the
> > >expected number of records.
> > >
> > >While, on face value, it appears that file size affects how the
> > >na.strings argument operates, I wondering if there is something else at
> > >play here.
> > >
> > >Unfortunately, I cannot share the data file for confidentiality reason
> > >but was wondering if you could suggest some checks I could perform to
> > >get to the bottom on this issue.
> > >
> > >Thank you in advance for your help and sorry for the lack of
> > >reproducible example.
> > >
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
> >
>
> ---------------------------------------------------------------------------
> Jeff Newmiller                        The     .....       .....  Go Live...
> DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live
> Go...
>                                        Live:   OO#.. Dead: OO#..  Playing
> Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
> /Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
> ---------------------------------------------------------------------------
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chz|y429 @end|ng |rom 163@com  Thu Nov 14 15:55:16 2019
From: chz|y429 @end|ng |rom 163@com (chziy429)
Date: Thu, 14 Nov 2019 22:55:16 +0800 (CST)
Subject: [R] Ask for help on the preprocessing of GEO microarray data with
 Oligo
Message-ID: <a6eecc7.d4b7.16e6a694eb3.Coremail.chziy429@163.com>

Dear Sir



I have downloaded the raw CEL data included in "GSE41418" from GEO and tried to process the raw microarray data according to the following Rscripts


  affydata <- ReadAffy(cdfname = "mouse4302mmentrezgcdf")
  eset <- oligo::rma(affydata)



The raw data can be read by ReadAffy but failed in the normalization process. I have tried to process it with affy::rma, frma::frma and oligo::rma, but it still can not work. An error saying "address 0x7f854500a000, cause 'invalid permissions'" was returned. However this error message was not appeared in some other datasets like "GSE41342". I was wondering whether this file was corrupted.
Could you please help me address this problem? Any suggestion and recommendation are welcome.





Best
	[[alternative HTML version deleted]]


From ||@her @end|ng |rom p|e@@th@n@com  Thu Nov 14 18:34:30 2019
From: ||@her @end|ng |rom p|e@@th@n@com (Dennis Fisher)
Date: Thu, 14 Nov 2019 09:34:30 -0800
Subject: [R] Problem related to multibyte string in CSV file
Message-ID: <6C94096A-1D8A-4314-9349-8D3094741D6F@plessthan.com>

R 3.6.1
OS X

Colleagues, 

I read the first line of a CSV file using the readLines command; the only option was n=1 (I am interested in only the first line of the file)
	STRING	<- readLines(FILE, n=1)
to which R responded:
	Warning message: 
	In readLines(FILE, n = 1) : line 1 appears to contain an embedded nul 

I then attempted to determine the number of characters in that string:
	nchar(STRING) 
to which R responded:
	Error in nchar(STRING) : invalid multibyte string, element 1 
	
I then went to examine the string:
	print(STRING) 
	[1] "\xff\xfet?
and:
	cat(STRING, "\n?)
	??t

I was surprised to see the difference in the output of cat vs. string (see above).  But I assume this results from the multibyte characters.

Now to my question:  I am trying to automate this process and I would like to see the output from the print command but without the [1] that precedes the string.  
If I am working at the command line, RGUI, or RStudio, I can type
	STRING<CR>
However, in a script, I need to preface STRING with either ?print? or ?cat? (or something else).
Short of writing my own print method, is there any simple way to accomplish this?

Dennis

Dennis Fisher MD
P < (The "P Less Than" Company)
Phone / Fax: 1-866-PLessThan (1-866-753-7784)
www.PLessThan.com


From Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com  Thu Nov 14 18:38:13 2019
From: Seb@@t|en@B|hore| @end|ng |rom cogn|gencorp@com (Sebastien Bihorel)
Date: Thu, 14 Nov 2019 17:38:13 +0000
Subject: [R] 
 Can file size affect how na.strings operates in a read.table call?
In-Reply-To: <CAF8bMcaj2T7_p7hg_E-ZE4YK6MBVmW47n9K+5cmwKYWMzc3rxQ@mail.gmail.com>
References: <CH2PR19MB38643F555825EC6C5B62B56092710@CH2PR19MB3864.namprd19.prod.outlook.com>
 <1AD5435A-C48C-419B-A509-952C414381F3@dcn.davis.ca.us>
 <CH2PR19MB3864E9BDD184EBD83003EA2992710@CH2PR19MB3864.namprd19.prod.outlook.com>
 <alpine.BSF.2.00.1911140830220.83728@pedal.dcn.davis.ca.us>,
 <CAF8bMcaj2T7_p7hg_E-ZE4YK6MBVmW47n9K+5cmwKYWMzc3rxQ@mail.gmail.com>
Message-ID: <CH2PR19MB3864F4E1151E7B8B8948AFAA92710@CH2PR19MB3864.namprd19.prod.outlook.com>

Thanks Bill and Jeff

strip.white did not change the outcomes.

However, your inputs led me to compare the raw content of the files (ie, outside of an IDE) and found difference in how the apparent -99 were stored. In the big file, some -99 are stored as floats rather than integers and thus included a decimal point and trailing zeros.

The creation of the smaller files resulted in the removal of the decimal point and trailing zeros, explaining why read.table provided the "right " response on these smaller files.

So, it looks like this is the problem and that some additional post-processing may be warranted.

Thanks for the hints.

________________________________
From: William Dunlap <wdunlap at tibco.com>
Sent: Thursday, November 14, 2019 11:51
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Cc: Sebastien Bihorel <Sebastien.Bihorel at cognigencorp.com>; r-help at r-project.org <r-help at r-project.org>
Subject: Re: [R] Can file size affect how na.strings operates in a read.table call?

read.table (and friends) also have the strip.white argument:

> s <- "A,B,C\n0,0,0\n1,-99,-99\n2,-99 ,-99\n3, -99, -99\n"
> read.csv(text=s, header=TRUE, na.strings="-99", strip.white=TRUE)
  A  B  C
1 0  0  0
2 1 NA NA
3 2 NA NA
4 3 NA NA
> read.csv(text=s, header=TRUE, na.strings="-99", strip.white=FALSE)
  A   B   C
1 0   0   0
2 1  NA  NA
3 2 -99  NA
4 3 -99 -99

Bill Dunlap
TIBCO Software
wdunlap tibco.com<http://tibco.com>


On Thu, Nov 14, 2019 at 8:35 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>> wrote:
Consider the following sample:

#####
s <- "A,B,C
0,0,0
1,-99,-99
2,-99 ,-99
3, -99, -99
"

dta_notok <- read.csv( text = s
                      , header=TRUE
                      , na.strings = c( "-99", "" )
                      )

dta_ok <- read.csv( text = s
                   , header=TRUE
                   , na.strings = c( "-99", " -99"
                                   , "-99 ", ""
                                   )
                   )

library(data.table)

fdt_ok <- fread( text = s, na.strings=c( "-99", "" ) )
fdta_ok <- as.data.frame( fdt_ok )
#####

Leading and trailing spaces cause problems. The data.table::fread function
has a strip.white argument that defaults to TRUE, but the resulting object
is a data.table which has different semantics than a data.frame.

On Thu, 14 Nov 2019, Sebastien Bihorel wrote:

> The data file is a csv file. Some text variables contain spaces.
>
> "Check for extraneous spaces"
> Are there specific locations that would be more critical than others?
>
>
> ____________________________________________________________________________
> From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>>
> Sent: Thursday, November 14, 2019 10:52
> To: Sebastien Bihorel <Sebastien.Bihorel at cognigencorp.com<mailto:Sebastien.Bihorel at cognigencorp.com>>; Sebastien
> Bihorel via R-help <r-help at r-project.org<mailto:r-help at r-project.org>>; r-help at r-project.org<mailto:r-help at r-project.org>
> <r-help at r-project.org<mailto:r-help at r-project.org>>
> Subject: Re: [R] Can file size affect how na.strings operates in a
> read.table call?
> Check for extraneous spaces. You may need more variations of the na.strings.
>
> On November 14, 2019 7:40:42 AM PST, Sebastien Bihorel via R-help
> <r-help at r-project.org<mailto:r-help at r-project.org>> wrote:
> >Hi,
> >
> >I have this generic function to read ASCII data files. It is
> >essentially a wrapper around the read.table function. My function is
> >used in a large variety of situations and has no a priori knowledge
> >about the data file it is asked to read. Nothing is known about file
> >size, variable types, variable names, or data table dimensions.
> >
> >One argument of my function is na.strings which is passed down to
> >read.table.
> >
> >Recently, a user tried to read a data file of ~ 80 Mo (~ 93000 rows by
> >~ 160 columns) using na.strings = c('-99', '.') with the intention of
> >interpreting '.' and '-99'
> >strings as the internal missing data NA. Dots were converted to NA
> >appropriately. However, not all -99 values in the data were interpreted
> >as NA. In some variables, -99 were converted to NA, while in others -99
> >was read as a number. More surprisingly, when the data file was cut in
> >smaller chunks (ie, by dropping either rows or columns) saved in
> >multiple files, the function calls applied on the new data files
> >resulted in the correct conversion of the -99 values into NAs.
> >
> >In all cases, the data frames produced by read.table contained the
> >expected number of records.
> >
> >While, on face value, it appears that file size affects how the
> >na.strings argument operates, I wondering if there is something else at
> >play here.
> >
> >Unfortunately, I cannot share the data file for confidentiality reason
> >but was wondering if you could suggest some checks I could perform to
> >get to the bottom on this issue.
> >
> >Thank you in advance for your help and sorry for the lack of
> >reproducible example.
> >
> >
> >______________________________________________
> >R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us<mailto:jdnewmil at dcn.davis.ca.us>>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k
---------------------------------------------------------------------------
______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Nov 14 18:36:25 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Nov 2019 09:36:25 -0800
Subject: [R] 
 Ask for help on the preprocessing of GEO microarray data with Oligo
In-Reply-To: <a6eecc7.d4b7.16e6a694eb3.Coremail.chziy429@163.com>
References: <a6eecc7.d4b7.16e6a694eb3.Coremail.chziy429@163.com>
Message-ID: <CAGxFJbQjw6VQmeWCxX13KkEn_HLOeA+qwxXq9+CAmnXS7rJ_KA@mail.gmail.com>

My recommendation is:

Post on the BioConductor site, not here.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 14, 2019 at 9:22 AM chziy429 <chziy429 at 163.com> wrote:

> Dear Sir
>
>
>
> I have downloaded the raw CEL data included in "GSE41418" from GEO and
> tried to process the raw microarray data according to the following Rscripts
>
>
>   affydata <- ReadAffy(cdfname = "mouse4302mmentrezgcdf")
>   eset <- oligo::rma(affydata)
>
>
>
> The raw data can be read by ReadAffy but failed in the normalization
> process. I have tried to process it with affy::rma, frma::frma and
> oligo::rma, but it still can not work. An error saying "address
> 0x7f854500a000, cause 'invalid permissions'" was returned. However this
> error message was not appeared in some other datasets like "GSE41342". I
> was wondering whether this file was corrupted.
> Could you please help me address this problem? Any suggestion and
> recommendation are welcome.
>
>
>
>
>
> Best
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Nov 14 18:49:36 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 14 Nov 2019 20:49:36 +0300
Subject: [R] Problem related to multibyte string in CSV file
In-Reply-To: <6C94096A-1D8A-4314-9349-8D3094741D6F@plessthan.com>
References: <6C94096A-1D8A-4314-9349-8D3094741D6F@plessthan.com>
Message-ID: <20191114204936.23cb198c@trisector>

On Thu, 14 Nov 2019 09:34:30 -0800
Dennis Fisher <fisher at plessthan.com> wrote:

> 	Warning message: 
> 	In readLines(FILE, n = 1) : line 1 appears to contain an
> embedded nul 

<...>

> 	print(STRING) 
> 	[1] "\xff\xfet?

Most probably, this means that the FILE is UCS-2LE-encoded (or maybe
UTF-16). Unlike UTF-8, text encoded using UCS-2LE may contain NUL bytes
if the code points in question are U+00FF and below. You should decode
it before processing it in R; one of the examples in ?readLines shows
how to do it:

# read a 'Windows Unicode' file
A <- readLines(con <- file("Unicode.txt", encoding = "UCS-2LE"))
close(con)
 
> Now to my question:  I am trying to automate this process and I would
> like to see the output from the print command but without the [1]
> that precedes the string.

Try encodeString combined with cat or message.

-- 
Best regards,
Ivan


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Nov 14 19:50:45 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 14 Nov 2019 12:50:45 -0600
Subject: [R] Remove highly correlated variables from a data frame or matrix
Message-ID: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>

Hello,

I have a data frame like this (a matrix):
head(calc.rho)
            rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
rs35350506      0.975     0.309     0.371     0.371     0.371     0.638

> dim(calc.rho)
[1] 246 246

I would like to remove from this data all highly correlated variables,
with correlation more than 0.8

I tried this:

> data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> dim(data)
[1] 246   0

Can you please advise,

Thanks
Ana

But this removes everything.


From bgunter@4567 @end|ng |rom gm@||@com  Thu Nov 14 21:09:08 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Nov 2019 12:09:08 -0800
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
Message-ID: <CAGxFJbSQFEUwbGx2AtyqwX7S1rd6hrSiSCU0UBLgo46NuRQ+xA@mail.gmail.com>

Obvious advice:

DON'T DO THIS!

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com>
wrote:

> Hello,
>
> I have a data frame like this (a matrix):
> head(calc.rho)
>             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
>
> > dim(calc.rho)
> [1] 246 246
>
> I would like to remove from this data all highly correlated variables,
> with correlation more than 0.8
>
> I tried this:
>
> > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > dim(data)
> [1] 246   0
>
> Can you please advise,
>
> Thanks
> Ana
>
> But this removes everything.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Nov 14 21:11:55 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 14 Nov 2019 14:11:55 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAGxFJbSQFEUwbGx2AtyqwX7S1rd6hrSiSCU0UBLgo46NuRQ+xA@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAGxFJbSQFEUwbGx2AtyqwX7S1rd6hrSiSCU0UBLgo46NuRQ+xA@mail.gmail.com>
Message-ID: <CAF9-5jNhYN4EK=jTsi6J+VXU=w1NPKcNu4fVTfBEDJzjk65PfA@mail.gmail.com>

I don't understand. I have to keep only pairs of variables with
correlation less than 0.8 in order to proceed with some calculations

On Thu, Nov 14, 2019 at 2:09 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Obvious advice:
>
> DON'T DO THIS!
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>
>> Hello,
>>
>> I have a data frame like this (a matrix):
>> head(calc.rho)
>>             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
>> rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
>> rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
>> rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
>> rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
>> rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
>> rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
>>
>> > dim(calc.rho)
>> [1] 246 246
>>
>> I would like to remove from this data all highly correlated variables,
>> with correlation more than 0.8
>>
>> I tried this:
>>
>> > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
>> > dim(data)
>> [1] 246   0
>>
>> Can you please advise,
>>
>> Thanks
>> Ana
>>
>> But this removes everything.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Thu Nov 14 21:29:39 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 15 Nov 2019 09:29:39 +1300
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
Message-ID: <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>

Sorry, but I don't understand your question.

When I first looked at this, I thought it was a correlation (or
covariance) matrix.
e.g.

> cor (quakes)
> cov (quakes)

However, your  row and column variables are different, implying two
different data sets.
Also, some of the (correlation?) coefficients are the same, implying
that some of the variables are the same, or very close.

Also, note that a matrix is not a data.frame.


> I have a data frame like this (a matrix):
> head(calc.rho)
>             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> > dim(calc.rho)
> [1] 246 246
> I would like to remove from this data all highly correlated variables,
> with correlation more than 0.8


From @purd|e@@ @end|ng |rom gm@||@com  Thu Nov 14 21:56:38 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 15 Nov 2019 09:56:38 +1300
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
 <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>
Message-ID: <CAB8pepzGFmEGNv6jnGEBz227-vk4O1ON5ytoaei_9NhSzQTCVQ@mail.gmail.com>

> I basically want to remove all entries for pairs which have value in
> between them (correlation calculated not in R, bit it is correlation,
> r2)
> so for example I would not keep: rs883504 because it has r2>0.8 for
> all those rs...

I'm still not sure what "remove all entries" means?
In your example rs883504, has all correlation coefficients > 0.8, in
the data returned by head().
However, most of its correlation coefficients are < 0.8, if you
include the entire matrix.

If you remove a variable that has at least one correlation coefficient
> 0.8, you would remove all the variables.
However, if you remove a variable that has all correlation
coefficients > 0.8, you would (probably) remove no variables.


From @purd|e@@ @end|ng |rom gm@||@com  Thu Nov 14 21:59:18 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 15 Nov 2019 09:59:18 +1300
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAB8pepzGFmEGNv6jnGEBz227-vk4O1ON5ytoaei_9NhSzQTCVQ@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
 <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>
 <CAB8pepzGFmEGNv6jnGEBz227-vk4O1ON5ytoaei_9NhSzQTCVQ@mail.gmail.com>
Message-ID: <CAB8pepwjAOWZj39SvtJhbRbv=Do5DVLZOQb75zNhssVaOa0SZg@mail.gmail.com>

That's assuming your data was returned by head().


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Nov 14 22:18:04 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 14 Nov 2019 15:18:04 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAB8pepwjAOWZj39SvtJhbRbv=Do5DVLZOQb75zNhssVaOa0SZg@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
 <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>
 <CAB8pepzGFmEGNv6jnGEBz227-vk4O1ON5ytoaei_9NhSzQTCVQ@mail.gmail.com>
 <CAB8pepwjAOWZj39SvtJhbRbv=Do5DVLZOQb75zNhssVaOa0SZg@mail.gmail.com>
Message-ID: <CAF9-5jP9Xxxdz+cHpo5c5OnqLjwtOvdyntHw5Qbfd4s1fu=FQA@mail.gmail.com>

what would be the approach to remove variable that has at least 2
correlation coefficients >0.8?
this is the whole output of the head()

> head(calc.rho)
            rs56192520 rs3764410 rs145984817 rs1807401 rs1807402 rs35350506
rs56192520       1.000     0.976       0.927     0.927     0.927      0.927
rs3764410        0.976     1.000       0.952     0.952     0.952      0.952
rs145984817      0.927     0.952       1.000     1.000     1.000      1.000
rs1807401        0.927     0.952       1.000     1.000     1.000      1.000
rs1807402        0.927     0.952       1.000     1.000     1.000      1.000
rs35350506       0.927     0.952       1.000     1.000     1.000      1.000
            rs2089177 rs12325677 rs62064624 rs62064631 rs2349295 rs2174369
rs56192520      0.927      0.927      0.927      0.927     0.709     0.903
rs3764410       0.952      0.952      0.952      0.952     0.728     0.928
rs145984817     1.000      1.000      1.000      1.000     0.771     0.975
rs1807401       1.000      1.000      1.000      1.000     0.771     0.975
rs1807402       1.000      1.000      1.000      1.000     0.771     0.975
rs35350506      1.000      1.000      1.000      1.000     0.771     0.975
            rs7218554 rs62064634 rs4360974 rs4527060 rs6502526 rs6502527
rs56192520      0.903      0.903     0.903     0.903     0.903     0.903
rs3764410       0.928      0.928     0.928     0.928     0.928     0.928
rs145984817     0.975      0.975     0.975     0.975     0.975     0.975
rs1807401       0.975      0.975     0.975     0.975     0.975     0.975
rs1807402       0.975      0.975     0.975     0.975     0.975     0.975
rs35350506      0.975      0.975     0.975     0.975     0.975     0.975
            rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
            rs7211086 rs9905280 rs8073305 rs8072086 rs4312350 rs4313843
rs56192520      0.880     0.268     0.327     0.880     0.880     0.880
rs3764410       0.905     0.276     0.336     0.905     0.905     0.905
rs145984817     0.951     0.309     0.371     0.951     0.951     0.951
rs1807401       0.951     0.309     0.371     0.951     0.951     0.951
rs1807402       0.951     0.309     0.371     0.951     0.951     0.951
rs35350506      0.951     0.309     0.371     0.951     0.951     0.951
            rs8069610 rs883504 rs8072394 rs4280293 rs4465638 rs12602378
rs56192520      0.582    0.903     0.582     0.582     0.811      0.302
rs3764410       0.598    0.928     0.598     0.598     0.836      0.311
rs145984817     0.638    0.975     0.638     0.638     0.879      0.344
rs1807401       0.638    0.975     0.638     0.638     0.879      0.344
rs1807402       0.638    0.975     0.638     0.638     0.879      0.344
rs35350506      0.638    0.975     0.638     0.638     0.879      0.344
            rs9899059 rs6502530 rs4380085 rs6502532 rs4792798 rs4792799
rs56192520      0.302     0.309     0.834     0.251     0.063     0.063
rs3764410       0.311     0.318     0.858     0.259     0.080     0.080
rs145984817     0.344     0.352     0.902     0.291     0.086     0.086
rs1807401       0.344     0.352     0.902     0.291     0.086     0.086
rs1807402       0.344     0.352     0.902     0.291     0.086     0.086
rs35350506      0.344     0.352     0.902     0.291     0.086     0.086
            rs4316813 rs148563931 rs74751226 rs8068857 rs8069441 rs77397878
rs56192520      0.006       0.006      0.006     0.006     0.006      0.006
rs3764410       0.006       0.006      0.006     0.006     0.006      0.006
rs145984817     0.006       0.006      0.006     0.006     0.006      0.006
rs1807401       0.006       0.006      0.006     0.006     0.006      0.006
rs1807402       0.006       0.006      0.006     0.006     0.006      0.006
rs35350506      0.006       0.006      0.006     0.006     0.006      0.006
            rs75339756 rs4608391 rs79569548 rs4275914 rs11870422 rs8075751
rs56192520       0.006     0.006      0.006     0.044      0.007     0.004
rs3764410        0.006     0.006      0.006     0.042      0.005     0.005
rs145984817      0.006     0.006      0.006     0.047      0.002     0.015
rs1807401        0.006     0.006      0.006     0.047      0.002     0.015
rs1807402        0.006     0.006      0.006     0.047      0.002     0.015
rs35350506       0.006     0.006      0.006     0.047      0.002     0.015
            rs11658904 rs138437542 rs80344434 rs7222311 rs7221842 rs7223686
rs56192520       0.003       0.004      0.004     0.033     0.009     0.000
rs3764410        0.004       0.004      0.004     0.031     0.007     0.000
rs145984817      0.010       0.004      0.004     0.035     0.011     0.005
rs1807401        0.010       0.004      0.004     0.035     0.011     0.005
rs1807402        0.010       0.004      0.004     0.035     0.011     0.005
rs35350506       0.010       0.004      0.004     0.035     0.011     0.005
            rs78013597 rs74965036 rs78063986 rs118106233 rs117345712
rs56192520       0.004      0.004      0.004       0.004       0.005
rs3764410        0.004      0.004      0.004       0.004       0.006
rs145984817      0.004      0.004      0.004       0.004       0.005
rs1807401        0.004      0.004      0.004       0.004       0.005
rs1807402        0.004      0.004      0.004       0.004       0.005
rs35350506       0.004      0.004      0.004       0.004       0.005
            rs113004656 rs9898995 rs4985718 rs9893911 rs79110942 rs7208929
rs56192520        0.004     0.033     0.033     0.023      0.004     0.023
rs3764410         0.004     0.031     0.031     0.021      0.004     0.021
rs145984817       0.004     0.035     0.035     0.025      0.004     0.025
rs1807401         0.004     0.035     0.035     0.025      0.004     0.025
rs1807402         0.004     0.035     0.035     0.025      0.004     0.025
rs35350506        0.004     0.035     0.035     0.025      0.004     0.025
            rs12601453 rs4078062 rs75129280 rs76664572 rs78961289 rs146364798
rs56192520       0.004     0.001      0.004      0.004      0.004       0.004
rs3764410        0.004     0.002      0.004      0.004      0.004       0.004
rs145984817      0.004     0.001      0.004      0.004      0.004       0.004
rs1807401        0.004     0.001      0.004      0.004      0.004       0.004
rs1807402        0.004     0.001      0.004      0.004      0.004       0.004
rs35350506       0.004     0.001      0.004      0.004      0.004       0.004
            rs76715413 rs4078534 rs79457460 rs74369938 rs76423171 rs74668400
rs56192520           0     0.004      0.004      0.002      0.004      0.004
rs3764410            0     0.004      0.004      0.001      0.004      0.004
rs145984817          0     0.004      0.004      0.005      0.004      0.004
rs1807401            0     0.004      0.004      0.005      0.004      0.004
rs1807402            0     0.004      0.004      0.005      0.004      0.004
rs35350506           0     0.004      0.004      0.005      0.004      0.004
            rs75146120 rs1135237 rs9914671 rs117759512 rs4985696 rs16961340
rs56192520       0.004     0.003     0.009       0.004     0.009      0.004
rs3764410        0.004     0.003     0.007       0.004     0.007      0.004
rs145984817      0.004     0.003     0.011       0.004     0.011      0.004
rs1807401        0.004     0.003     0.011       0.004     0.011      0.004
rs1807402        0.004     0.003     0.011       0.004     0.011      0.004
rs35350506       0.004     0.003     0.011       0.004     0.011      0.004
            rs17794159 rs4247118 rs78572469 rs12601193 rs2349646 rs2090018
rs56192520       0.001     0.033      0.002      0.004     0.020     0.033
rs3764410        0.002     0.031      0.001      0.004     0.019     0.031
rs145984817      0.001     0.035      0.005      0.004     0.022     0.035
rs1807401        0.001     0.035      0.005      0.004     0.022     0.035
rs1807402        0.001     0.035      0.005      0.004     0.022     0.035
rs35350506       0.001     0.035      0.005      0.004     0.022     0.035
            rs12601424 rs4985701 rs8064550 rs2271521 rs2271520 rs11078374
rs56192520       0.004     0.033     0.033     0.004     0.033      0.014
rs3764410        0.004     0.031     0.031     0.004     0.031      0.012
rs145984817      0.004     0.035     0.035     0.004     0.035      0.016
rs1807401        0.004     0.035     0.035     0.004     0.035      0.016
rs1807402        0.004     0.035     0.035     0.004     0.035      0.016
rs35350506       0.004     0.035     0.035     0.004     0.035      0.016
            rs4985702 rs1124961 rs11652674 rs3924340 rs112450164 rs7208973
rs56192520      0.033     0.003      0.002     0.001       0.004     0.033
rs3764410       0.031     0.003      0.001     0.002       0.004     0.031
rs145984817     0.035     0.003      0.005     0.001       0.004     0.035
rs1807401       0.035     0.003      0.005     0.001       0.004     0.035
rs1807402       0.035     0.003      0.005     0.001       0.004     0.035
rs35350506      0.035     0.003      0.005     0.001       0.004     0.035
            rs9910857 rs78574480 rs8072184 rs12602196 rs6502563 rs3744135
rs56192520      0.006      0.004     0.014      0.004     0.033     0.004
rs3764410       0.005      0.004     0.012      0.004     0.031     0.004
rs145984817     0.002      0.004     0.016      0.004     0.035     0.004
rs1807401       0.002      0.004     0.016      0.004     0.035     0.004
rs1807402       0.002      0.004     0.016      0.004     0.035     0.004
rs35350506      0.002      0.004     0.016      0.004     0.035     0.004
            rs148779543 rs77689691 rs41319048 rs117340532 rs78647096 rs77712968
rs56192520            0      0.004      0.004       0.002      0.004      0.004
rs3764410             0      0.004      0.004       0.001      0.004      0.004
rs145984817           0      0.004      0.004       0.005      0.004      0.004
rs1807401             0      0.004      0.004       0.005      0.004      0.004
rs1807402             0      0.004      0.004       0.005      0.004      0.004
rs35350506            0      0.004      0.004       0.005      0.004      0.004
            rs16961396 rs80054920 rs7206981 rs4985740 rs3803762 rs77103270
rs56192520       0.004      0.004     0.033     0.023     0.004      0.002
rs3764410        0.004      0.004     0.031     0.021     0.004      0.001
rs145984817      0.004      0.004     0.035     0.025     0.004      0.005
rs1807401        0.004      0.004     0.035     0.025     0.004      0.005
rs1807402        0.004      0.004     0.035     0.025     0.004      0.005
rs35350506       0.004      0.004     0.035     0.025     0.004      0.005
            rs7207485 rs77342773 rs3826304 rs3744126 rs7210879 rs7211576
rs56192520      0.029      0.004     0.004     0.004     0.023     0.006
rs3764410       0.027      0.004     0.004     0.004     0.021     0.005
rs145984817     0.031      0.004     0.004     0.004     0.025     0.002
rs1807401       0.031      0.004     0.004     0.004     0.025     0.002
rs1807402       0.031      0.004     0.004     0.004     0.025     0.002
rs35350506      0.031      0.004     0.004     0.004     0.025     0.002
            rs117967362 rs75978745 rs6502564 rs9894565 rs36079048 rs8076621
rs56192520        0.004      0.004     0.007     0.017          0     0.004
rs3764410         0.004      0.004     0.005     0.015          0     0.004
rs145984817       0.004      0.004     0.009     0.019          0     0.004
rs1807401         0.004      0.004     0.009     0.019          0     0.004
rs1807402         0.004      0.004     0.009     0.019          0     0.004
rs35350506        0.004      0.004     0.009     0.019          0     0.004
            rs7218795 rs3803761 rs12602675 rs7208065 rs4985705 rs8080386
rs56192520      0.026     0.032          0     0.018     0.014     0.003
rs3764410       0.024     0.029          0     0.015     0.011     0.002
rs145984817     0.028     0.034          0     0.021     0.016     0.004
rs1807401       0.028     0.034          0     0.021     0.016     0.004
rs1807402       0.028     0.034          0     0.021     0.016     0.004
rs35350506      0.028     0.034          0     0.021     0.016     0.004
            rs8065832 rs2018781 rs1736221 rs1736220 rs1736217 rs1708620
rs56192520      0.008     0.039     0.003     0.003     0.021     0.009
rs3764410       0.006     0.037     0.002     0.002     0.019     0.007
rs145984817     0.010     0.042     0.004     0.004     0.024     0.011
rs1807401       0.010     0.042     0.004     0.004     0.024     0.011
rs1807402       0.010     0.042     0.004     0.004     0.024     0.011
rs35350506      0.010     0.042     0.004     0.004     0.024     0.011
            rs1708619 rs1736216 rs76319098 rs1736215 rs1736214 rs1708617
rs56192520      0.009     0.024      0.017     0.012     0.019     0.029
rs3764410       0.007     0.021      0.016     0.009     0.016     0.026
rs145984817     0.011     0.026      0.018     0.014     0.022     0.031
rs1807401       0.011     0.026      0.018     0.014     0.022     0.031
rs1807402       0.011     0.026      0.018     0.014     0.022     0.031
rs35350506      0.011     0.026      0.018     0.014     0.022     0.031
            rs12602831 rs12602871 rs1736213 rs1736212 rs76045368 rs34518797
rs56192520       0.000      0.000     0.015     0.029      0.001      0.001
rs3764410        0.001      0.001     0.013     0.026      0.001      0.001
rs145984817      0.000      0.000     0.018     0.031      0.000      0.000
rs1807401        0.000      0.000     0.018     0.031      0.000      0.000
rs1807402        0.000      0.000     0.018     0.031      0.000      0.000
rs35350506       0.000      0.000     0.018     0.031      0.000      0.000
            rs11078378 rs8079562 rs8065774 rs8066090 rs41337846 rs1736209
rs56192520       0.043     0.001     0.001     0.029      0.000     0.029
rs3764410        0.041     0.001     0.001     0.026      0.001     0.026
rs145984817      0.046     0.000     0.000     0.031      0.000     0.031
rs1807401        0.046     0.000     0.000     0.031      0.000     0.031
rs1807402        0.046     0.000     0.000     0.031      0.000     0.031
rs35350506       0.046     0.000     0.000     0.031      0.000     0.031
            rs1736208 rs12949822 rs76246042 rs12600635 rs55689224 rs1736207
rs56192520      0.043      0.043      0.000      0.000      0.000     0.015
rs3764410       0.041      0.041      0.001      0.001      0.001     0.013
rs145984817     0.046      0.046      0.000      0.000      0.000     0.018
rs1807401       0.046      0.046      0.000      0.000      0.000     0.018
rs1807402       0.046      0.046      0.000      0.000      0.000     0.018
rs35350506      0.046      0.046      0.000      0.000      0.000     0.018
            rs1708626 rs1736206 rs9896078 rs16961474 rs1708627 rs1736205
rs56192520      0.015     0.015     0.001      0.001     0.017     0.021
rs3764410       0.013     0.013     0.001      0.001     0.014     0.019
rs145984817     0.018     0.018     0.000      0.001     0.020     0.024
rs1807401       0.018     0.018     0.000      0.001     0.020     0.024
rs1807402       0.018     0.018     0.000      0.001     0.020     0.024
rs35350506      0.018     0.018     0.000      0.001     0.020     0.024
            rs1708628 rs7220577 rs2294155 rs1736204 rs1736203 rs1736202
rs56192520      0.011     0.011     0.000     0.021     0.014     0.014
rs3764410       0.009     0.009     0.000     0.019     0.011     0.011
rs145984817     0.013     0.013     0.001     0.024     0.016     0.016
rs1807401       0.013     0.013     0.001     0.024     0.016     0.016
rs1807402       0.013     0.013     0.001     0.024     0.016     0.016
rs35350506      0.013     0.013     0.001     0.024     0.016     0.016
            rs12937908 rs1736200 rs1708623 rs1708624 rs9894884 rs9901894
rs56192520       0.009     0.009     0.008     0.007     0.009     0.009
rs3764410        0.007     0.007     0.006     0.005     0.007     0.007
rs145984817      0.011     0.011     0.010     0.008     0.011     0.011
rs1807401        0.011     0.011     0.010     0.008     0.011     0.011
rs1807402        0.011     0.011     0.010     0.008     0.011     0.011
rs35350506       0.011     0.011     0.010     0.008     0.011     0.011
            rs9903294 rs2472689 rs1630656 rs111478970 rs3182911 rs7219012
rs56192520      0.008     0.011     0.007       0.007     0.008     0.000
rs3764410       0.006     0.009     0.005       0.005     0.006     0.000
rs145984817     0.010     0.013     0.008       0.008     0.010     0.001
rs1807401       0.010     0.013     0.008       0.008     0.010     0.001
rs1807402       0.010     0.013     0.008       0.008     0.010     0.001
rs35350506      0.010     0.013     0.008       0.008     0.010     0.001
            rs9890657 rs12453455 rs12947291 rs150267386 rs16961493 rs11652745
rs56192520      0.009      0.007      0.008       0.013      0.000      0.009
rs3764410       0.007      0.005      0.006       0.012      0.000      0.007
rs145984817     0.011      0.008      0.010       0.014      0.001      0.011
rs1807401       0.011      0.008      0.010       0.014      0.001      0.011
rs1807402       0.011      0.008      0.010       0.014      0.001      0.011
rs35350506      0.011      0.008      0.010       0.014      0.001      0.011
            rs9907107 rs8070574 rs4985759 rs3866959 rs7219248 rs6502568
rs56192520      0.009     0.009     0.009     0.011     0.009     0.011
rs3764410       0.007     0.007     0.007     0.009     0.007     0.009
rs145984817     0.011     0.011     0.011     0.013     0.011     0.013
rs1807401       0.011     0.011     0.011     0.013     0.011     0.013
rs1807402       0.011     0.011     0.011     0.013     0.011     0.013
rs35350506      0.011     0.011     0.011     0.013     0.011     0.013
            rs7220275 rs12450037 rs7225876 rs9892352 rs4985760 rs6502569
rs56192520      0.009      0.008     0.007     0.011     0.011     0.011
rs3764410       0.007      0.006     0.005     0.009     0.009     0.009
rs145984817     0.011      0.010     0.008     0.013     0.013     0.013
rs1807401       0.011      0.010     0.008     0.013     0.013     0.013
rs1807402       0.011      0.010     0.008     0.013     0.013     0.013
rs35350506      0.011      0.010     0.008     0.013     0.013     0.013
            rs1029830 rs2012954 rs1029832 rs2270180 rs8072402 rs7221553
rs56192520      0.009     0.011     0.008     0.000     0.009     0.011
rs3764410       0.007     0.009     0.006     0.000     0.007     0.009
rs145984817     0.011     0.013     0.010     0.001     0.011     0.013
rs1807401       0.011     0.013     0.010     0.001     0.011     0.013
rs1807402       0.011     0.013     0.010     0.001     0.011     0.013
rs35350506      0.011     0.013     0.010     0.001     0.011     0.013
            rs145597919 rs150772017 rs2041393 rs6502578 rs11078382 rs9912109
rs56192520        0.013       0.013     0.005     0.005          0     0.005
rs3764410         0.012       0.012     0.004     0.004          0     0.004
rs145984817       0.014       0.014     0.006     0.006          0     0.006
rs1807401         0.014       0.014     0.006     0.006          0     0.006
rs1807402         0.014       0.014     0.006     0.006          0     0.006
rs35350506        0.014       0.014     0.006     0.006          0     0.006
            rs12601631 rs11869054 rs11869079 rs9912599 rs7220057 rs9896970
rs56192520           0          0          0         0         0         0
rs3764410            0          0          0         0         0         0
rs145984817          0          0          0         0         0         0
rs1807401            0          0          0         0         0         0
rs1807402            0          0          0         0         0         0
rs35350506           0          0          0         0         0         0
            rs34121330 rs34668117 rs67773570 rs242252 rs955893 rs28583584
rs56192520       0.000      0.000          0    0.002    0.001      0.013
rs3764410        0.001      0.001          0    0.003    0.001      0.005
rs145984817      0.000      0.000          0    0.002    0.001      0.004
rs1807401        0.000      0.000          0    0.002    0.001      0.004
rs1807402        0.000      0.000          0    0.002    0.001      0.004
rs35350506       0.000      0.000          0    0.002    0.001      0.004
            rs9944423 rs7217764 rs11651957 rs73978990 rs8071007 rs56044345
rs56192520      0.013     0.011      0.011      0.011     0.011      0.011
rs3764410       0.005     0.004      0.004      0.004     0.004      0.004
rs145984817     0.004     0.003      0.003      0.003     0.003      0.003
rs1807401       0.004     0.003      0.003      0.003     0.003      0.003
rs1807402       0.004     0.003      0.003      0.003     0.003      0.003
rs35350506      0.004     0.003      0.003      0.003     0.003      0.003
            rs17804843
rs56192520           0
rs3764410            0
rs145984817          0
rs1807401            0
rs1807402            0
rs35350506           0


On Thu, Nov 14, 2019 at 2:59 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> That's assuming your data was returned by head().


From drj|m|emon @end|ng |rom gm@||@com  Thu Nov 14 22:18:28 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 15 Nov 2019 08:18:28 +1100
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
Message-ID: <CA+8X3fU9bn6UFrRW1+WoTUKTJDOrCPWqtvM2WwJLD8hUiCJbbQ@mail.gmail.com>

Hi Ana,
Rather than addressing the question of why you want to do this, Let's
get make the question easier to answer:

calc.rho<-matrix(c(0.903,0.268,0.327,0.327,0.327,0.582,
0.928,0.276,0.336,0.336,0.336,0.598,
0.975,0.309,0.371,0.371,0.371,0.638,
0.975,0.309,0.371,0.371,0.371,0.638,
0.975,0.309,0.371,0.371,0.371,0.638,
0.975,0.309,0.371,0.371,0.371,0.638),ncol=6,byrow=TRUE)
rnames<-c("rs56192520","rs3764410","rs145984817","rs1807401",
"rs1807402","rs35350506")
rownames(calc.rho)<-rnames
cnames<-c("rs9900318","rs8069906","rs9908521","rs9908336",
"rs9908870","rs9895995")
colnames(calc.rho)<-cnames

Now if you  just want a vector of the values less than 0.8, it's trivial:

calc.rho[calc.rho<0.8]

However, based on your previous questions, I suspect you want
something else. Maybe the pairs of row/column names that correspond to
the values less than 0.8. To ensure that you haven't tricked us by not
including columns in which values range around 0.8, I'll do it this
way:

# make the new variable name possible to decode
calc.lt.8<-calc.rho<0.8
varnames.lt.8<-data.frame(var1=NA,var2=NA)
for(row in 1:nrow(calc.rho)) {
 for(col in 1:ncol(calc.rho))
  if(calc.lt.8[row,col])
   varnames.lt.8<-rbind(varnames.lt.8,c(rnames[row],cnames[col]))
}
# now get rid of the first row of NA values
varnames.lt.8<-varnames.lt.8[-1,]

Clunky, but effective. You now have those variable pairs that you may
want. Let us know in the next episode of this soap operation.

Jim

On Fri, Nov 15, 2019 at 5:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame like this (a matrix):
> head(calc.rho)
>             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
>
> > dim(calc.rho)
> [1] 246 246
>
> I would like to remove from this data all highly correlated variables,
> with correlation more than 0.8
>
> I tried this:
>
> > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > dim(data)
> [1] 246   0
>
> Can you please advise,
>
> Thanks
> Ana
>
> But this removes everything.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Thu Nov 14 22:39:25 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Fri, 15 Nov 2019 08:39:25 +1100
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jP9Xxxdz+cHpo5c5OnqLjwtOvdyntHw5Qbfd4s1fu=FQA@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
 <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>
 <CAB8pepzGFmEGNv6jnGEBz227-vk4O1ON5ytoaei_9NhSzQTCVQ@mail.gmail.com>
 <CAB8pepwjAOWZj39SvtJhbRbv=Do5DVLZOQb75zNhssVaOa0SZg@mail.gmail.com>
 <CAF9-5jP9Xxxdz+cHpo5c5OnqLjwtOvdyntHw5Qbfd4s1fu=FQA@mail.gmail.com>
Message-ID: <CA+8X3fXpScw+_CW7KsXNUeiEWvxggxtFwAgjG0W5Pw+-70w+aQ@mail.gmail.com>

I thought you were going to trick us. What I think you are asking now
is how to get the variable names in the columns that have at most one
_absolute_ value greater than 0.8. OK:

# I'm not going to try to recreate your correlation matrix
calc.jim<-matrix(runif(100,min=-1,max=1),nrow=10)
for(i in 1:10) calc.jim[i,i]<-1
rownames(calc.jim)<-<-colnames(calc.jim)<-paste0("rs",1:10)

Now that we have a plausible fake correlation matrix, all we have to
do is extract the column names:

colnames(calc.jim)[colSums(abs(calc.jim)>0.8)<2]

Of course, what you really meant could have been, "I want the column
names of the variables with at most one absolute value greater than
0.8 ignoring the diagonal values because I don't care about those". If
so:

colnames(calc.jim)[colSums(abs(calc.jim)>0.8)<3]

Any more tricks?

Jim

On Fri, Nov 15, 2019 at 8:17 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> what would be the approach to remove variable that has at least 2
> correlation coefficients >0.8?
> this is the whole output of the head()
>
> > head(calc.rho)
>             rs56192520 rs3764410 rs145984817 rs1807401 rs1807402 rs35350506
> rs56192520       1.000     0.976       0.927     0.927     0.927      0.927
> rs3764410        0.976     1.000       0.952     0.952     0.952      0.952
> rs145984817      0.927     0.952       1.000     1.000     1.000      1.000
> rs1807401        0.927     0.952       1.000     1.000     1.000      1.000
> rs1807402        0.927     0.952       1.000     1.000     1.000      1.000
> rs35350506       0.927     0.952       1.000     1.000     1.000      1.000
>             rs2089177 rs12325677 rs62064624 rs62064631 rs2349295 rs2174369
> rs56192520      0.927      0.927      0.927      0.927     0.709     0.903
> rs3764410       0.952      0.952      0.952      0.952     0.728     0.928
> rs145984817     1.000      1.000      1.000      1.000     0.771     0.975
> rs1807401       1.000      1.000      1.000      1.000     0.771     0.975
> rs1807402       1.000      1.000      1.000      1.000     0.771     0.975
> rs35350506      1.000      1.000      1.000      1.000     0.771     0.975
>             rs7218554 rs62064634 rs4360974 rs4527060 rs6502526 rs6502527
> rs56192520      0.903      0.903     0.903     0.903     0.903     0.903
> rs3764410       0.928      0.928     0.928     0.928     0.928     0.928
> rs145984817     0.975      0.975     0.975     0.975     0.975     0.975
> rs1807401       0.975      0.975     0.975     0.975     0.975     0.975
> rs1807402       0.975      0.975     0.975     0.975     0.975     0.975
> rs35350506      0.975      0.975     0.975     0.975     0.975     0.975
>             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
>             rs7211086 rs9905280 rs8073305 rs8072086 rs4312350 rs4313843
> rs56192520      0.880     0.268     0.327     0.880     0.880     0.880
> rs3764410       0.905     0.276     0.336     0.905     0.905     0.905
> rs145984817     0.951     0.309     0.371     0.951     0.951     0.951
> rs1807401       0.951     0.309     0.371     0.951     0.951     0.951
> rs1807402       0.951     0.309     0.371     0.951     0.951     0.951
> rs35350506      0.951     0.309     0.371     0.951     0.951     0.951
>             rs8069610 rs883504 rs8072394 rs4280293 rs4465638 rs12602378
> rs56192520      0.582    0.903     0.582     0.582     0.811      0.302
> rs3764410       0.598    0.928     0.598     0.598     0.836      0.311
> rs145984817     0.638    0.975     0.638     0.638     0.879      0.344
> rs1807401       0.638    0.975     0.638     0.638     0.879      0.344
> rs1807402       0.638    0.975     0.638     0.638     0.879      0.344
> rs35350506      0.638    0.975     0.638     0.638     0.879      0.344
>             rs9899059 rs6502530 rs4380085 rs6502532 rs4792798 rs4792799
> rs56192520      0.302     0.309     0.834     0.251     0.063     0.063
> rs3764410       0.311     0.318     0.858     0.259     0.080     0.080
> rs145984817     0.344     0.352     0.902     0.291     0.086     0.086
> rs1807401       0.344     0.352     0.902     0.291     0.086     0.086
> rs1807402       0.344     0.352     0.902     0.291     0.086     0.086
> rs35350506      0.344     0.352     0.902     0.291     0.086     0.086
>             rs4316813 rs148563931 rs74751226 rs8068857 rs8069441 rs77397878
> rs56192520      0.006       0.006      0.006     0.006     0.006      0.006
> rs3764410       0.006       0.006      0.006     0.006     0.006      0.006
> rs145984817     0.006       0.006      0.006     0.006     0.006      0.006
> rs1807401       0.006       0.006      0.006     0.006     0.006      0.006
> rs1807402       0.006       0.006      0.006     0.006     0.006      0.006
> rs35350506      0.006       0.006      0.006     0.006     0.006      0.006
>             rs75339756 rs4608391 rs79569548 rs4275914 rs11870422 rs8075751
> rs56192520       0.006     0.006      0.006     0.044      0.007     0.004
> rs3764410        0.006     0.006      0.006     0.042      0.005     0.005
> rs145984817      0.006     0.006      0.006     0.047      0.002     0.015
> rs1807401        0.006     0.006      0.006     0.047      0.002     0.015
> rs1807402        0.006     0.006      0.006     0.047      0.002     0.015
> rs35350506       0.006     0.006      0.006     0.047      0.002     0.015
>             rs11658904 rs138437542 rs80344434 rs7222311 rs7221842 rs7223686
> rs56192520       0.003       0.004      0.004     0.033     0.009     0.000
> rs3764410        0.004       0.004      0.004     0.031     0.007     0.000
> rs145984817      0.010       0.004      0.004     0.035     0.011     0.005
> rs1807401        0.010       0.004      0.004     0.035     0.011     0.005
> rs1807402        0.010       0.004      0.004     0.035     0.011     0.005
> rs35350506       0.010       0.004      0.004     0.035     0.011     0.005
>             rs78013597 rs74965036 rs78063986 rs118106233 rs117345712
> rs56192520       0.004      0.004      0.004       0.004       0.005
> rs3764410        0.004      0.004      0.004       0.004       0.006
> rs145984817      0.004      0.004      0.004       0.004       0.005
> rs1807401        0.004      0.004      0.004       0.004       0.005
> rs1807402        0.004      0.004      0.004       0.004       0.005
> rs35350506       0.004      0.004      0.004       0.004       0.005
>             rs113004656 rs9898995 rs4985718 rs9893911 rs79110942 rs7208929
> rs56192520        0.004     0.033     0.033     0.023      0.004     0.023
> rs3764410         0.004     0.031     0.031     0.021      0.004     0.021
> rs145984817       0.004     0.035     0.035     0.025      0.004     0.025
> rs1807401         0.004     0.035     0.035     0.025      0.004     0.025
> rs1807402         0.004     0.035     0.035     0.025      0.004     0.025
> rs35350506        0.004     0.035     0.035     0.025      0.004     0.025
>             rs12601453 rs4078062 rs75129280 rs76664572 rs78961289 rs146364798
> rs56192520       0.004     0.001      0.004      0.004      0.004       0.004
> rs3764410        0.004     0.002      0.004      0.004      0.004       0.004
> rs145984817      0.004     0.001      0.004      0.004      0.004       0.004
> rs1807401        0.004     0.001      0.004      0.004      0.004       0.004
> rs1807402        0.004     0.001      0.004      0.004      0.004       0.004
> rs35350506       0.004     0.001      0.004      0.004      0.004       0.004
>             rs76715413 rs4078534 rs79457460 rs74369938 rs76423171 rs74668400
> rs56192520           0     0.004      0.004      0.002      0.004      0.004
> rs3764410            0     0.004      0.004      0.001      0.004      0.004
> rs145984817          0     0.004      0.004      0.005      0.004      0.004
> rs1807401            0     0.004      0.004      0.005      0.004      0.004
> rs1807402            0     0.004      0.004      0.005      0.004      0.004
> rs35350506           0     0.004      0.004      0.005      0.004      0.004
>             rs75146120 rs1135237 rs9914671 rs117759512 rs4985696 rs16961340
> rs56192520       0.004     0.003     0.009       0.004     0.009      0.004
> rs3764410        0.004     0.003     0.007       0.004     0.007      0.004
> rs145984817      0.004     0.003     0.011       0.004     0.011      0.004
> rs1807401        0.004     0.003     0.011       0.004     0.011      0.004
> rs1807402        0.004     0.003     0.011       0.004     0.011      0.004
> rs35350506       0.004     0.003     0.011       0.004     0.011      0.004
>             rs17794159 rs4247118 rs78572469 rs12601193 rs2349646 rs2090018
> rs56192520       0.001     0.033      0.002      0.004     0.020     0.033
> rs3764410        0.002     0.031      0.001      0.004     0.019     0.031
> rs145984817      0.001     0.035      0.005      0.004     0.022     0.035
> rs1807401        0.001     0.035      0.005      0.004     0.022     0.035
> rs1807402        0.001     0.035      0.005      0.004     0.022     0.035
> rs35350506       0.001     0.035      0.005      0.004     0.022     0.035
>             rs12601424 rs4985701 rs8064550 rs2271521 rs2271520 rs11078374
> rs56192520       0.004     0.033     0.033     0.004     0.033      0.014
> rs3764410        0.004     0.031     0.031     0.004     0.031      0.012
> rs145984817      0.004     0.035     0.035     0.004     0.035      0.016
> rs1807401        0.004     0.035     0.035     0.004     0.035      0.016
> rs1807402        0.004     0.035     0.035     0.004     0.035      0.016
> rs35350506       0.004     0.035     0.035     0.004     0.035      0.016
>             rs4985702 rs1124961 rs11652674 rs3924340 rs112450164 rs7208973
> rs56192520      0.033     0.003      0.002     0.001       0.004     0.033
> rs3764410       0.031     0.003      0.001     0.002       0.004     0.031
> rs145984817     0.035     0.003      0.005     0.001       0.004     0.035
> rs1807401       0.035     0.003      0.005     0.001       0.004     0.035
> rs1807402       0.035     0.003      0.005     0.001       0.004     0.035
> rs35350506      0.035     0.003      0.005     0.001       0.004     0.035
>             rs9910857 rs78574480 rs8072184 rs12602196 rs6502563 rs3744135
> rs56192520      0.006      0.004     0.014      0.004     0.033     0.004
> rs3764410       0.005      0.004     0.012      0.004     0.031     0.004
> rs145984817     0.002      0.004     0.016      0.004     0.035     0.004
> rs1807401       0.002      0.004     0.016      0.004     0.035     0.004
> rs1807402       0.002      0.004     0.016      0.004     0.035     0.004
> rs35350506      0.002      0.004     0.016      0.004     0.035     0.004
>             rs148779543 rs77689691 rs41319048 rs117340532 rs78647096 rs77712968
> rs56192520            0      0.004      0.004       0.002      0.004      0.004
> rs3764410             0      0.004      0.004       0.001      0.004      0.004
> rs145984817           0      0.004      0.004       0.005      0.004      0.004
> rs1807401             0      0.004      0.004       0.005      0.004      0.004
> rs1807402             0      0.004      0.004       0.005      0.004      0.004
> rs35350506            0      0.004      0.004       0.005      0.004      0.004
>             rs16961396 rs80054920 rs7206981 rs4985740 rs3803762 rs77103270
> rs56192520       0.004      0.004     0.033     0.023     0.004      0.002
> rs3764410        0.004      0.004     0.031     0.021     0.004      0.001
> rs145984817      0.004      0.004     0.035     0.025     0.004      0.005
> rs1807401        0.004      0.004     0.035     0.025     0.004      0.005
> rs1807402        0.004      0.004     0.035     0.025     0.004      0.005
> rs35350506       0.004      0.004     0.035     0.025     0.004      0.005
>             rs7207485 rs77342773 rs3826304 rs3744126 rs7210879 rs7211576
> rs56192520      0.029      0.004     0.004     0.004     0.023     0.006
> rs3764410       0.027      0.004     0.004     0.004     0.021     0.005
> rs145984817     0.031      0.004     0.004     0.004     0.025     0.002
> rs1807401       0.031      0.004     0.004     0.004     0.025     0.002
> rs1807402       0.031      0.004     0.004     0.004     0.025     0.002
> rs35350506      0.031      0.004     0.004     0.004     0.025     0.002
>             rs117967362 rs75978745 rs6502564 rs9894565 rs36079048 rs8076621
> rs56192520        0.004      0.004     0.007     0.017          0     0.004
> rs3764410         0.004      0.004     0.005     0.015          0     0.004
> rs145984817       0.004      0.004     0.009     0.019          0     0.004
> rs1807401         0.004      0.004     0.009     0.019          0     0.004
> rs1807402         0.004      0.004     0.009     0.019          0     0.004
> rs35350506        0.004      0.004     0.009     0.019          0     0.004
>             rs7218795 rs3803761 rs12602675 rs7208065 rs4985705 rs8080386
> rs56192520      0.026     0.032          0     0.018     0.014     0.003
> rs3764410       0.024     0.029          0     0.015     0.011     0.002
> rs145984817     0.028     0.034          0     0.021     0.016     0.004
> rs1807401       0.028     0.034          0     0.021     0.016     0.004
> rs1807402       0.028     0.034          0     0.021     0.016     0.004
> rs35350506      0.028     0.034          0     0.021     0.016     0.004
>             rs8065832 rs2018781 rs1736221 rs1736220 rs1736217 rs1708620
> rs56192520      0.008     0.039     0.003     0.003     0.021     0.009
> rs3764410       0.006     0.037     0.002     0.002     0.019     0.007
> rs145984817     0.010     0.042     0.004     0.004     0.024     0.011
> rs1807401       0.010     0.042     0.004     0.004     0.024     0.011
> rs1807402       0.010     0.042     0.004     0.004     0.024     0.011
> rs35350506      0.010     0.042     0.004     0.004     0.024     0.011
>             rs1708619 rs1736216 rs76319098 rs1736215 rs1736214 rs1708617
> rs56192520      0.009     0.024      0.017     0.012     0.019     0.029
> rs3764410       0.007     0.021      0.016     0.009     0.016     0.026
> rs145984817     0.011     0.026      0.018     0.014     0.022     0.031
> rs1807401       0.011     0.026      0.018     0.014     0.022     0.031
> rs1807402       0.011     0.026      0.018     0.014     0.022     0.031
> rs35350506      0.011     0.026      0.018     0.014     0.022     0.031
>             rs12602831 rs12602871 rs1736213 rs1736212 rs76045368 rs34518797
> rs56192520       0.000      0.000     0.015     0.029      0.001      0.001
> rs3764410        0.001      0.001     0.013     0.026      0.001      0.001
> rs145984817      0.000      0.000     0.018     0.031      0.000      0.000
> rs1807401        0.000      0.000     0.018     0.031      0.000      0.000
> rs1807402        0.000      0.000     0.018     0.031      0.000      0.000
> rs35350506       0.000      0.000     0.018     0.031      0.000      0.000
>             rs11078378 rs8079562 rs8065774 rs8066090 rs41337846 rs1736209
> rs56192520       0.043     0.001     0.001     0.029      0.000     0.029
> rs3764410        0.041     0.001     0.001     0.026      0.001     0.026
> rs145984817      0.046     0.000     0.000     0.031      0.000     0.031
> rs1807401        0.046     0.000     0.000     0.031      0.000     0.031
> rs1807402        0.046     0.000     0.000     0.031      0.000     0.031
> rs35350506       0.046     0.000     0.000     0.031      0.000     0.031
>             rs1736208 rs12949822 rs76246042 rs12600635 rs55689224 rs1736207
> rs56192520      0.043      0.043      0.000      0.000      0.000     0.015
> rs3764410       0.041      0.041      0.001      0.001      0.001     0.013
> rs145984817     0.046      0.046      0.000      0.000      0.000     0.018
> rs1807401       0.046      0.046      0.000      0.000      0.000     0.018
> rs1807402       0.046      0.046      0.000      0.000      0.000     0.018
> rs35350506      0.046      0.046      0.000      0.000      0.000     0.018
>             rs1708626 rs1736206 rs9896078 rs16961474 rs1708627 rs1736205
> rs56192520      0.015     0.015     0.001      0.001     0.017     0.021
> rs3764410       0.013     0.013     0.001      0.001     0.014     0.019
> rs145984817     0.018     0.018     0.000      0.001     0.020     0.024
> rs1807401       0.018     0.018     0.000      0.001     0.020     0.024
> rs1807402       0.018     0.018     0.000      0.001     0.020     0.024
> rs35350506      0.018     0.018     0.000      0.001     0.020     0.024
>             rs1708628 rs7220577 rs2294155 rs1736204 rs1736203 rs1736202
> rs56192520      0.011     0.011     0.000     0.021     0.014     0.014
> rs3764410       0.009     0.009     0.000     0.019     0.011     0.011
> rs145984817     0.013     0.013     0.001     0.024     0.016     0.016
> rs1807401       0.013     0.013     0.001     0.024     0.016     0.016
> rs1807402       0.013     0.013     0.001     0.024     0.016     0.016
> rs35350506      0.013     0.013     0.001     0.024     0.016     0.016
>             rs12937908 rs1736200 rs1708623 rs1708624 rs9894884 rs9901894
> rs56192520       0.009     0.009     0.008     0.007     0.009     0.009
> rs3764410        0.007     0.007     0.006     0.005     0.007     0.007
> rs145984817      0.011     0.011     0.010     0.008     0.011     0.011
> rs1807401        0.011     0.011     0.010     0.008     0.011     0.011
> rs1807402        0.011     0.011     0.010     0.008     0.011     0.011
> rs35350506       0.011     0.011     0.010     0.008     0.011     0.011
>             rs9903294 rs2472689 rs1630656 rs111478970 rs3182911 rs7219012
> rs56192520      0.008     0.011     0.007       0.007     0.008     0.000
> rs3764410       0.006     0.009     0.005       0.005     0.006     0.000
> rs145984817     0.010     0.013     0.008       0.008     0.010     0.001
> rs1807401       0.010     0.013     0.008       0.008     0.010     0.001
> rs1807402       0.010     0.013     0.008       0.008     0.010     0.001
> rs35350506      0.010     0.013     0.008       0.008     0.010     0.001
>             rs9890657 rs12453455 rs12947291 rs150267386 rs16961493 rs11652745
> rs56192520      0.009      0.007      0.008       0.013      0.000      0.009
> rs3764410       0.007      0.005      0.006       0.012      0.000      0.007
> rs145984817     0.011      0.008      0.010       0.014      0.001      0.011
> rs1807401       0.011      0.008      0.010       0.014      0.001      0.011
> rs1807402       0.011      0.008      0.010       0.014      0.001      0.011
> rs35350506      0.011      0.008      0.010       0.014      0.001      0.011
>             rs9907107 rs8070574 rs4985759 rs3866959 rs7219248 rs6502568
> rs56192520      0.009     0.009     0.009     0.011     0.009     0.011
> rs3764410       0.007     0.007     0.007     0.009     0.007     0.009
> rs145984817     0.011     0.011     0.011     0.013     0.011     0.013
> rs1807401       0.011     0.011     0.011     0.013     0.011     0.013
> rs1807402       0.011     0.011     0.011     0.013     0.011     0.013
> rs35350506      0.011     0.011     0.011     0.013     0.011     0.013
>             rs7220275 rs12450037 rs7225876 rs9892352 rs4985760 rs6502569
> rs56192520      0.009      0.008     0.007     0.011     0.011     0.011
> rs3764410       0.007      0.006     0.005     0.009     0.009     0.009
> rs145984817     0.011      0.010     0.008     0.013     0.013     0.013
> rs1807401       0.011      0.010     0.008     0.013     0.013     0.013
> rs1807402       0.011      0.010     0.008     0.013     0.013     0.013
> rs35350506      0.011      0.010     0.008     0.013     0.013     0.013
>             rs1029830 rs2012954 rs1029832 rs2270180 rs8072402 rs7221553
> rs56192520      0.009     0.011     0.008     0.000     0.009     0.011
> rs3764410       0.007     0.009     0.006     0.000     0.007     0.009
> rs145984817     0.011     0.013     0.010     0.001     0.011     0.013
> rs1807401       0.011     0.013     0.010     0.001     0.011     0.013
> rs1807402       0.011     0.013     0.010     0.001     0.011     0.013
> rs35350506      0.011     0.013     0.010     0.001     0.011     0.013
>             rs145597919 rs150772017 rs2041393 rs6502578 rs11078382 rs9912109
> rs56192520        0.013       0.013     0.005     0.005          0     0.005
> rs3764410         0.012       0.012     0.004     0.004          0     0.004
> rs145984817       0.014       0.014     0.006     0.006          0     0.006
> rs1807401         0.014       0.014     0.006     0.006          0     0.006
> rs1807402         0.014       0.014     0.006     0.006          0     0.006
> rs35350506        0.014       0.014     0.006     0.006          0     0.006
>             rs12601631 rs11869054 rs11869079 rs9912599 rs7220057 rs9896970
> rs56192520           0          0          0         0         0         0
> rs3764410            0          0          0         0         0         0
> rs145984817          0          0          0         0         0         0
> rs1807401            0          0          0         0         0         0
> rs1807402            0          0          0         0         0         0
> rs35350506           0          0          0         0         0         0
>             rs34121330 rs34668117 rs67773570 rs242252 rs955893 rs28583584
> rs56192520       0.000      0.000          0    0.002    0.001      0.013
> rs3764410        0.001      0.001          0    0.003    0.001      0.005
> rs145984817      0.000      0.000          0    0.002    0.001      0.004
> rs1807401        0.000      0.000          0    0.002    0.001      0.004
> rs1807402        0.000      0.000          0    0.002    0.001      0.004
> rs35350506       0.000      0.000          0    0.002    0.001      0.004
>             rs9944423 rs7217764 rs11651957 rs73978990 rs8071007 rs56044345
> rs56192520      0.013     0.011      0.011      0.011     0.011      0.011
> rs3764410       0.005     0.004      0.004      0.004     0.004      0.004
> rs145984817     0.004     0.003      0.003      0.003     0.003      0.003
> rs1807401       0.004     0.003      0.003      0.003     0.003      0.003
> rs1807402       0.004     0.003      0.003      0.003     0.003      0.003
> rs35350506      0.004     0.003      0.003      0.003     0.003      0.003
>             rs17804843
> rs56192520           0
> rs3764410            0
> rs145984817          0
> rs1807401            0
> rs1807402            0
> rs35350506           0
>
>
> On Thu, Nov 14, 2019 at 2:59 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> >
> > That's assuming your data was returned by head().
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Nov 14 23:25:59 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 14 Nov 2019 16:25:59 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CA+8X3fXpScw+_CW7KsXNUeiEWvxggxtFwAgjG0W5Pw+-70w+aQ@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
 <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>
 <CAB8pepzGFmEGNv6jnGEBz227-vk4O1ON5ytoaei_9NhSzQTCVQ@mail.gmail.com>
 <CAB8pepwjAOWZj39SvtJhbRbv=Do5DVLZOQb75zNhssVaOa0SZg@mail.gmail.com>
 <CAF9-5jP9Xxxdz+cHpo5c5OnqLjwtOvdyntHw5Qbfd4s1fu=FQA@mail.gmail.com>
 <CA+8X3fXpScw+_CW7KsXNUeiEWvxggxtFwAgjG0W5Pw+-70w+aQ@mail.gmail.com>
Message-ID: <CAF9-5jMvxY-_m=DeQf+iTT9M7rRg8zU01WDaJ8h=G4X6ZdamWQ@mail.gmail.com>

HI Jim,

This:
colnames(calc.jim)[colSums(abs(calc.jim)>0.8)<3]

was the master take!

Thank you so much!!!

On Thu, Nov 14, 2019 at 3:39 PM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> I thought you were going to trick us. What I think you are asking now
> is how to get the variable names in the columns that have at most one
> _absolute_ value greater than 0.8. OK:
>
> # I'm not going to try to recreate your correlation matrix
> calc.jim<-matrix(runif(100,min=-1,max=1),nrow=10)
> for(i in 1:10) calc.jim[i,i]<-1
> rownames(calc.jim)<-<-colnames(calc.jim)<-paste0("rs",1:10)
>
> Now that we have a plausible fake correlation matrix, all we have to
> do is extract the column names:
>
> colnames(calc.jim)[colSums(abs(calc.jim)>0.8)<2]
>
> Of course, what you really meant could have been, "I want the column
> names of the variables with at most one absolute value greater than
> 0.8 ignoring the diagonal values because I don't care about those". If
> so:
>
> colnames(calc.jim)[colSums(abs(calc.jim)>0.8)<3]
>
> Any more tricks?
>
> Jim
>
> On Fri, Nov 15, 2019 at 8:17 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > what would be the approach to remove variable that has at least 2
> > correlation coefficients >0.8?
> > this is the whole output of the head()
> >
> > > head(calc.rho)
> >             rs56192520 rs3764410 rs145984817 rs1807401 rs1807402 rs35350506
> > rs56192520       1.000     0.976       0.927     0.927     0.927      0.927
> > rs3764410        0.976     1.000       0.952     0.952     0.952      0.952
> > rs145984817      0.927     0.952       1.000     1.000     1.000      1.000
> > rs1807401        0.927     0.952       1.000     1.000     1.000      1.000
> > rs1807402        0.927     0.952       1.000     1.000     1.000      1.000
> > rs35350506       0.927     0.952       1.000     1.000     1.000      1.000
> >             rs2089177 rs12325677 rs62064624 rs62064631 rs2349295 rs2174369
> > rs56192520      0.927      0.927      0.927      0.927     0.709     0.903
> > rs3764410       0.952      0.952      0.952      0.952     0.728     0.928
> > rs145984817     1.000      1.000      1.000      1.000     0.771     0.975
> > rs1807401       1.000      1.000      1.000      1.000     0.771     0.975
> > rs1807402       1.000      1.000      1.000      1.000     0.771     0.975
> > rs35350506      1.000      1.000      1.000      1.000     0.771     0.975
> >             rs7218554 rs62064634 rs4360974 rs4527060 rs6502526 rs6502527
> > rs56192520      0.903      0.903     0.903     0.903     0.903     0.903
> > rs3764410       0.928      0.928     0.928     0.928     0.928     0.928
> > rs145984817     0.975      0.975     0.975     0.975     0.975     0.975
> > rs1807401       0.975      0.975     0.975     0.975     0.975     0.975
> > rs1807402       0.975      0.975     0.975     0.975     0.975     0.975
> > rs35350506      0.975      0.975     0.975     0.975     0.975     0.975
> >             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> > rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> > rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> > rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> > rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> > rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> > rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> >             rs7211086 rs9905280 rs8073305 rs8072086 rs4312350 rs4313843
> > rs56192520      0.880     0.268     0.327     0.880     0.880     0.880
> > rs3764410       0.905     0.276     0.336     0.905     0.905     0.905
> > rs145984817     0.951     0.309     0.371     0.951     0.951     0.951
> > rs1807401       0.951     0.309     0.371     0.951     0.951     0.951
> > rs1807402       0.951     0.309     0.371     0.951     0.951     0.951
> > rs35350506      0.951     0.309     0.371     0.951     0.951     0.951
> >             rs8069610 rs883504 rs8072394 rs4280293 rs4465638 rs12602378
> > rs56192520      0.582    0.903     0.582     0.582     0.811      0.302
> > rs3764410       0.598    0.928     0.598     0.598     0.836      0.311
> > rs145984817     0.638    0.975     0.638     0.638     0.879      0.344
> > rs1807401       0.638    0.975     0.638     0.638     0.879      0.344
> > rs1807402       0.638    0.975     0.638     0.638     0.879      0.344
> > rs35350506      0.638    0.975     0.638     0.638     0.879      0.344
> >             rs9899059 rs6502530 rs4380085 rs6502532 rs4792798 rs4792799
> > rs56192520      0.302     0.309     0.834     0.251     0.063     0.063
> > rs3764410       0.311     0.318     0.858     0.259     0.080     0.080
> > rs145984817     0.344     0.352     0.902     0.291     0.086     0.086
> > rs1807401       0.344     0.352     0.902     0.291     0.086     0.086
> > rs1807402       0.344     0.352     0.902     0.291     0.086     0.086
> > rs35350506      0.344     0.352     0.902     0.291     0.086     0.086
> >             rs4316813 rs148563931 rs74751226 rs8068857 rs8069441 rs77397878
> > rs56192520      0.006       0.006      0.006     0.006     0.006      0.006
> > rs3764410       0.006       0.006      0.006     0.006     0.006      0.006
> > rs145984817     0.006       0.006      0.006     0.006     0.006      0.006
> > rs1807401       0.006       0.006      0.006     0.006     0.006      0.006
> > rs1807402       0.006       0.006      0.006     0.006     0.006      0.006
> > rs35350506      0.006       0.006      0.006     0.006     0.006      0.006
> >             rs75339756 rs4608391 rs79569548 rs4275914 rs11870422 rs8075751
> > rs56192520       0.006     0.006      0.006     0.044      0.007     0.004
> > rs3764410        0.006     0.006      0.006     0.042      0.005     0.005
> > rs145984817      0.006     0.006      0.006     0.047      0.002     0.015
> > rs1807401        0.006     0.006      0.006     0.047      0.002     0.015
> > rs1807402        0.006     0.006      0.006     0.047      0.002     0.015
> > rs35350506       0.006     0.006      0.006     0.047      0.002     0.015
> >             rs11658904 rs138437542 rs80344434 rs7222311 rs7221842 rs7223686
> > rs56192520       0.003       0.004      0.004     0.033     0.009     0.000
> > rs3764410        0.004       0.004      0.004     0.031     0.007     0.000
> > rs145984817      0.010       0.004      0.004     0.035     0.011     0.005
> > rs1807401        0.010       0.004      0.004     0.035     0.011     0.005
> > rs1807402        0.010       0.004      0.004     0.035     0.011     0.005
> > rs35350506       0.010       0.004      0.004     0.035     0.011     0.005
> >             rs78013597 rs74965036 rs78063986 rs118106233 rs117345712
> > rs56192520       0.004      0.004      0.004       0.004       0.005
> > rs3764410        0.004      0.004      0.004       0.004       0.006
> > rs145984817      0.004      0.004      0.004       0.004       0.005
> > rs1807401        0.004      0.004      0.004       0.004       0.005
> > rs1807402        0.004      0.004      0.004       0.004       0.005
> > rs35350506       0.004      0.004      0.004       0.004       0.005
> >             rs113004656 rs9898995 rs4985718 rs9893911 rs79110942 rs7208929
> > rs56192520        0.004     0.033     0.033     0.023      0.004     0.023
> > rs3764410         0.004     0.031     0.031     0.021      0.004     0.021
> > rs145984817       0.004     0.035     0.035     0.025      0.004     0.025
> > rs1807401         0.004     0.035     0.035     0.025      0.004     0.025
> > rs1807402         0.004     0.035     0.035     0.025      0.004     0.025
> > rs35350506        0.004     0.035     0.035     0.025      0.004     0.025
> >             rs12601453 rs4078062 rs75129280 rs76664572 rs78961289 rs146364798
> > rs56192520       0.004     0.001      0.004      0.004      0.004       0.004
> > rs3764410        0.004     0.002      0.004      0.004      0.004       0.004
> > rs145984817      0.004     0.001      0.004      0.004      0.004       0.004
> > rs1807401        0.004     0.001      0.004      0.004      0.004       0.004
> > rs1807402        0.004     0.001      0.004      0.004      0.004       0.004
> > rs35350506       0.004     0.001      0.004      0.004      0.004       0.004
> >             rs76715413 rs4078534 rs79457460 rs74369938 rs76423171 rs74668400
> > rs56192520           0     0.004      0.004      0.002      0.004      0.004
> > rs3764410            0     0.004      0.004      0.001      0.004      0.004
> > rs145984817          0     0.004      0.004      0.005      0.004      0.004
> > rs1807401            0     0.004      0.004      0.005      0.004      0.004
> > rs1807402            0     0.004      0.004      0.005      0.004      0.004
> > rs35350506           0     0.004      0.004      0.005      0.004      0.004
> >             rs75146120 rs1135237 rs9914671 rs117759512 rs4985696 rs16961340
> > rs56192520       0.004     0.003     0.009       0.004     0.009      0.004
> > rs3764410        0.004     0.003     0.007       0.004     0.007      0.004
> > rs145984817      0.004     0.003     0.011       0.004     0.011      0.004
> > rs1807401        0.004     0.003     0.011       0.004     0.011      0.004
> > rs1807402        0.004     0.003     0.011       0.004     0.011      0.004
> > rs35350506       0.004     0.003     0.011       0.004     0.011      0.004
> >             rs17794159 rs4247118 rs78572469 rs12601193 rs2349646 rs2090018
> > rs56192520       0.001     0.033      0.002      0.004     0.020     0.033
> > rs3764410        0.002     0.031      0.001      0.004     0.019     0.031
> > rs145984817      0.001     0.035      0.005      0.004     0.022     0.035
> > rs1807401        0.001     0.035      0.005      0.004     0.022     0.035
> > rs1807402        0.001     0.035      0.005      0.004     0.022     0.035
> > rs35350506       0.001     0.035      0.005      0.004     0.022     0.035
> >             rs12601424 rs4985701 rs8064550 rs2271521 rs2271520 rs11078374
> > rs56192520       0.004     0.033     0.033     0.004     0.033      0.014
> > rs3764410        0.004     0.031     0.031     0.004     0.031      0.012
> > rs145984817      0.004     0.035     0.035     0.004     0.035      0.016
> > rs1807401        0.004     0.035     0.035     0.004     0.035      0.016
> > rs1807402        0.004     0.035     0.035     0.004     0.035      0.016
> > rs35350506       0.004     0.035     0.035     0.004     0.035      0.016
> >             rs4985702 rs1124961 rs11652674 rs3924340 rs112450164 rs7208973
> > rs56192520      0.033     0.003      0.002     0.001       0.004     0.033
> > rs3764410       0.031     0.003      0.001     0.002       0.004     0.031
> > rs145984817     0.035     0.003      0.005     0.001       0.004     0.035
> > rs1807401       0.035     0.003      0.005     0.001       0.004     0.035
> > rs1807402       0.035     0.003      0.005     0.001       0.004     0.035
> > rs35350506      0.035     0.003      0.005     0.001       0.004     0.035
> >             rs9910857 rs78574480 rs8072184 rs12602196 rs6502563 rs3744135
> > rs56192520      0.006      0.004     0.014      0.004     0.033     0.004
> > rs3764410       0.005      0.004     0.012      0.004     0.031     0.004
> > rs145984817     0.002      0.004     0.016      0.004     0.035     0.004
> > rs1807401       0.002      0.004     0.016      0.004     0.035     0.004
> > rs1807402       0.002      0.004     0.016      0.004     0.035     0.004
> > rs35350506      0.002      0.004     0.016      0.004     0.035     0.004
> >             rs148779543 rs77689691 rs41319048 rs117340532 rs78647096 rs77712968
> > rs56192520            0      0.004      0.004       0.002      0.004      0.004
> > rs3764410             0      0.004      0.004       0.001      0.004      0.004
> > rs145984817           0      0.004      0.004       0.005      0.004      0.004
> > rs1807401             0      0.004      0.004       0.005      0.004      0.004
> > rs1807402             0      0.004      0.004       0.005      0.004      0.004
> > rs35350506            0      0.004      0.004       0.005      0.004      0.004
> >             rs16961396 rs80054920 rs7206981 rs4985740 rs3803762 rs77103270
> > rs56192520       0.004      0.004     0.033     0.023     0.004      0.002
> > rs3764410        0.004      0.004     0.031     0.021     0.004      0.001
> > rs145984817      0.004      0.004     0.035     0.025     0.004      0.005
> > rs1807401        0.004      0.004     0.035     0.025     0.004      0.005
> > rs1807402        0.004      0.004     0.035     0.025     0.004      0.005
> > rs35350506       0.004      0.004     0.035     0.025     0.004      0.005
> >             rs7207485 rs77342773 rs3826304 rs3744126 rs7210879 rs7211576
> > rs56192520      0.029      0.004     0.004     0.004     0.023     0.006
> > rs3764410       0.027      0.004     0.004     0.004     0.021     0.005
> > rs145984817     0.031      0.004     0.004     0.004     0.025     0.002
> > rs1807401       0.031      0.004     0.004     0.004     0.025     0.002
> > rs1807402       0.031      0.004     0.004     0.004     0.025     0.002
> > rs35350506      0.031      0.004     0.004     0.004     0.025     0.002
> >             rs117967362 rs75978745 rs6502564 rs9894565 rs36079048 rs8076621
> > rs56192520        0.004      0.004     0.007     0.017          0     0.004
> > rs3764410         0.004      0.004     0.005     0.015          0     0.004
> > rs145984817       0.004      0.004     0.009     0.019          0     0.004
> > rs1807401         0.004      0.004     0.009     0.019          0     0.004
> > rs1807402         0.004      0.004     0.009     0.019          0     0.004
> > rs35350506        0.004      0.004     0.009     0.019          0     0.004
> >             rs7218795 rs3803761 rs12602675 rs7208065 rs4985705 rs8080386
> > rs56192520      0.026     0.032          0     0.018     0.014     0.003
> > rs3764410       0.024     0.029          0     0.015     0.011     0.002
> > rs145984817     0.028     0.034          0     0.021     0.016     0.004
> > rs1807401       0.028     0.034          0     0.021     0.016     0.004
> > rs1807402       0.028     0.034          0     0.021     0.016     0.004
> > rs35350506      0.028     0.034          0     0.021     0.016     0.004
> >             rs8065832 rs2018781 rs1736221 rs1736220 rs1736217 rs1708620
> > rs56192520      0.008     0.039     0.003     0.003     0.021     0.009
> > rs3764410       0.006     0.037     0.002     0.002     0.019     0.007
> > rs145984817     0.010     0.042     0.004     0.004     0.024     0.011
> > rs1807401       0.010     0.042     0.004     0.004     0.024     0.011
> > rs1807402       0.010     0.042     0.004     0.004     0.024     0.011
> > rs35350506      0.010     0.042     0.004     0.004     0.024     0.011
> >             rs1708619 rs1736216 rs76319098 rs1736215 rs1736214 rs1708617
> > rs56192520      0.009     0.024      0.017     0.012     0.019     0.029
> > rs3764410       0.007     0.021      0.016     0.009     0.016     0.026
> > rs145984817     0.011     0.026      0.018     0.014     0.022     0.031
> > rs1807401       0.011     0.026      0.018     0.014     0.022     0.031
> > rs1807402       0.011     0.026      0.018     0.014     0.022     0.031
> > rs35350506      0.011     0.026      0.018     0.014     0.022     0.031
> >             rs12602831 rs12602871 rs1736213 rs1736212 rs76045368 rs34518797
> > rs56192520       0.000      0.000     0.015     0.029      0.001      0.001
> > rs3764410        0.001      0.001     0.013     0.026      0.001      0.001
> > rs145984817      0.000      0.000     0.018     0.031      0.000      0.000
> > rs1807401        0.000      0.000     0.018     0.031      0.000      0.000
> > rs1807402        0.000      0.000     0.018     0.031      0.000      0.000
> > rs35350506       0.000      0.000     0.018     0.031      0.000      0.000
> >             rs11078378 rs8079562 rs8065774 rs8066090 rs41337846 rs1736209
> > rs56192520       0.043     0.001     0.001     0.029      0.000     0.029
> > rs3764410        0.041     0.001     0.001     0.026      0.001     0.026
> > rs145984817      0.046     0.000     0.000     0.031      0.000     0.031
> > rs1807401        0.046     0.000     0.000     0.031      0.000     0.031
> > rs1807402        0.046     0.000     0.000     0.031      0.000     0.031
> > rs35350506       0.046     0.000     0.000     0.031      0.000     0.031
> >             rs1736208 rs12949822 rs76246042 rs12600635 rs55689224 rs1736207
> > rs56192520      0.043      0.043      0.000      0.000      0.000     0.015
> > rs3764410       0.041      0.041      0.001      0.001      0.001     0.013
> > rs145984817     0.046      0.046      0.000      0.000      0.000     0.018
> > rs1807401       0.046      0.046      0.000      0.000      0.000     0.018
> > rs1807402       0.046      0.046      0.000      0.000      0.000     0.018
> > rs35350506      0.046      0.046      0.000      0.000      0.000     0.018
> >             rs1708626 rs1736206 rs9896078 rs16961474 rs1708627 rs1736205
> > rs56192520      0.015     0.015     0.001      0.001     0.017     0.021
> > rs3764410       0.013     0.013     0.001      0.001     0.014     0.019
> > rs145984817     0.018     0.018     0.000      0.001     0.020     0.024
> > rs1807401       0.018     0.018     0.000      0.001     0.020     0.024
> > rs1807402       0.018     0.018     0.000      0.001     0.020     0.024
> > rs35350506      0.018     0.018     0.000      0.001     0.020     0.024
> >             rs1708628 rs7220577 rs2294155 rs1736204 rs1736203 rs1736202
> > rs56192520      0.011     0.011     0.000     0.021     0.014     0.014
> > rs3764410       0.009     0.009     0.000     0.019     0.011     0.011
> > rs145984817     0.013     0.013     0.001     0.024     0.016     0.016
> > rs1807401       0.013     0.013     0.001     0.024     0.016     0.016
> > rs1807402       0.013     0.013     0.001     0.024     0.016     0.016
> > rs35350506      0.013     0.013     0.001     0.024     0.016     0.016
> >             rs12937908 rs1736200 rs1708623 rs1708624 rs9894884 rs9901894
> > rs56192520       0.009     0.009     0.008     0.007     0.009     0.009
> > rs3764410        0.007     0.007     0.006     0.005     0.007     0.007
> > rs145984817      0.011     0.011     0.010     0.008     0.011     0.011
> > rs1807401        0.011     0.011     0.010     0.008     0.011     0.011
> > rs1807402        0.011     0.011     0.010     0.008     0.011     0.011
> > rs35350506       0.011     0.011     0.010     0.008     0.011     0.011
> >             rs9903294 rs2472689 rs1630656 rs111478970 rs3182911 rs7219012
> > rs56192520      0.008     0.011     0.007       0.007     0.008     0.000
> > rs3764410       0.006     0.009     0.005       0.005     0.006     0.000
> > rs145984817     0.010     0.013     0.008       0.008     0.010     0.001
> > rs1807401       0.010     0.013     0.008       0.008     0.010     0.001
> > rs1807402       0.010     0.013     0.008       0.008     0.010     0.001
> > rs35350506      0.010     0.013     0.008       0.008     0.010     0.001
> >             rs9890657 rs12453455 rs12947291 rs150267386 rs16961493 rs11652745
> > rs56192520      0.009      0.007      0.008       0.013      0.000      0.009
> > rs3764410       0.007      0.005      0.006       0.012      0.000      0.007
> > rs145984817     0.011      0.008      0.010       0.014      0.001      0.011
> > rs1807401       0.011      0.008      0.010       0.014      0.001      0.011
> > rs1807402       0.011      0.008      0.010       0.014      0.001      0.011
> > rs35350506      0.011      0.008      0.010       0.014      0.001      0.011
> >             rs9907107 rs8070574 rs4985759 rs3866959 rs7219248 rs6502568
> > rs56192520      0.009     0.009     0.009     0.011     0.009     0.011
> > rs3764410       0.007     0.007     0.007     0.009     0.007     0.009
> > rs145984817     0.011     0.011     0.011     0.013     0.011     0.013
> > rs1807401       0.011     0.011     0.011     0.013     0.011     0.013
> > rs1807402       0.011     0.011     0.011     0.013     0.011     0.013
> > rs35350506      0.011     0.011     0.011     0.013     0.011     0.013
> >             rs7220275 rs12450037 rs7225876 rs9892352 rs4985760 rs6502569
> > rs56192520      0.009      0.008     0.007     0.011     0.011     0.011
> > rs3764410       0.007      0.006     0.005     0.009     0.009     0.009
> > rs145984817     0.011      0.010     0.008     0.013     0.013     0.013
> > rs1807401       0.011      0.010     0.008     0.013     0.013     0.013
> > rs1807402       0.011      0.010     0.008     0.013     0.013     0.013
> > rs35350506      0.011      0.010     0.008     0.013     0.013     0.013
> >             rs1029830 rs2012954 rs1029832 rs2270180 rs8072402 rs7221553
> > rs56192520      0.009     0.011     0.008     0.000     0.009     0.011
> > rs3764410       0.007     0.009     0.006     0.000     0.007     0.009
> > rs145984817     0.011     0.013     0.010     0.001     0.011     0.013
> > rs1807401       0.011     0.013     0.010     0.001     0.011     0.013
> > rs1807402       0.011     0.013     0.010     0.001     0.011     0.013
> > rs35350506      0.011     0.013     0.010     0.001     0.011     0.013
> >             rs145597919 rs150772017 rs2041393 rs6502578 rs11078382 rs9912109
> > rs56192520        0.013       0.013     0.005     0.005          0     0.005
> > rs3764410         0.012       0.012     0.004     0.004          0     0.004
> > rs145984817       0.014       0.014     0.006     0.006          0     0.006
> > rs1807401         0.014       0.014     0.006     0.006          0     0.006
> > rs1807402         0.014       0.014     0.006     0.006          0     0.006
> > rs35350506        0.014       0.014     0.006     0.006          0     0.006
> >             rs12601631 rs11869054 rs11869079 rs9912599 rs7220057 rs9896970
> > rs56192520           0          0          0         0         0         0
> > rs3764410            0          0          0         0         0         0
> > rs145984817          0          0          0         0         0         0
> > rs1807401            0          0          0         0         0         0
> > rs1807402            0          0          0         0         0         0
> > rs35350506           0          0          0         0         0         0
> >             rs34121330 rs34668117 rs67773570 rs242252 rs955893 rs28583584
> > rs56192520       0.000      0.000          0    0.002    0.001      0.013
> > rs3764410        0.001      0.001          0    0.003    0.001      0.005
> > rs145984817      0.000      0.000          0    0.002    0.001      0.004
> > rs1807401        0.000      0.000          0    0.002    0.001      0.004
> > rs1807402        0.000      0.000          0    0.002    0.001      0.004
> > rs35350506       0.000      0.000          0    0.002    0.001      0.004
> >             rs9944423 rs7217764 rs11651957 rs73978990 rs8071007 rs56044345
> > rs56192520      0.013     0.011      0.011      0.011     0.011      0.011
> > rs3764410       0.005     0.004      0.004      0.004     0.004      0.004
> > rs145984817     0.004     0.003      0.003      0.003     0.003      0.003
> > rs1807401       0.004     0.003      0.003      0.003     0.003      0.003
> > rs1807402       0.004     0.003      0.003      0.003     0.003      0.003
> > rs35350506      0.004     0.003      0.003      0.003     0.003      0.003
> >             rs17804843
> > rs56192520           0
> > rs3764410            0
> > rs145984817          0
> > rs1807401            0
> > rs1807402            0
> > rs35350506           0
> >
> >
> > On Thu, Nov 14, 2019 at 2:59 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
> > >
> > > That's assuming your data was returned by head().
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Thu Nov 14 21:42:19 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Thu, 14 Nov 2019 14:42:19 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CAB8pepx2PYgS1adBQPxGxc4idOjSpncmsxrpLDG=LD7mSP9Q0A@mail.gmail.com>
Message-ID: <CAF9-5jNoo-xGbF6fKbDo=oJ_5LTkyWKDUKCrr0FZziMqkf26Cw@mail.gmail.com>

it can be converted between data frame and matrix. I am attaching here
the whole file for examination

I basically want to remove all entries for pairs which have value in
between them (correlation calculated not in R, bit it is correlation,
r2)
so for example I would not keep: rs883504 because it has r2>0.8 for
all those rs...

                  rs8069610 rs883504 rs8072394 rs4280293 rs4465638 rs12602378
rs56192520      0.582    0.903     0.582     0.582     0.811      0.302
rs3764410       0.598    0.928     0.598     0.598     0.836      0.311
rs145984817     0.638    0.975     0.638     0.638     0.879      0.344
rs1807401       0.638    0.975     0.638     0.638     0.879      0.344
rs1807402       0.638    0.975     0.638     0.638     0.879      0.344
rs35350506      0.638    0.975     0.638     0.638     0.879      0.344


On Thu, Nov 14, 2019 at 2:29 PM Abby Spurdle <spurdle.a at gmail.com> wrote:
>
> Sorry, but I don't understand your question.
>
> When I first looked at this, I thought it was a correlation (or
> covariance) matrix.
> e.g.
>
> > cor (quakes)
> > cov (quakes)
>
> However, your  row and column variables are different, implying two
> different data sets.
> Also, some of the (correlation?) coefficients are the same, implying
> that some of the variables are the same, or very close.
>
> Also, note that a matrix is not a data.frame.
>
>
> > I have a data frame like this (a matrix):
> > head(calc.rho)
> >             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> > rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> > rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> > rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> > rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> > rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> > rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> > > dim(calc.rho)
> > [1] 246 246
> > I would like to remove from this data all highly correlated variables,
> > with correlation more than 0.8

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ro246_matrix.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20191114/2577162a/attachment.txt>

From |@th@nhnh@n @end|ng |rom gm@||@com  Fri Nov 15 00:04:43 2019
From: |@th@nhnh@n @end|ng |rom gm@||@com (Nhan La)
Date: Fri, 15 Nov 2019 10:04:43 +1100
Subject: [R] How to import and create time series data frames in an
 efficient way?
Message-ID: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>

I have many separate data files in csv format for a lot of daily stock
prices. Over a few years there are hundreds of those data files, whose
names are the dates of data record.

In each file there are variables of ticker (or stock trading code), date,
open price, high price, low price, close price, and trading volume. For
example, inside a data file named 20150128.txt it looks like this:

FB,20150128,1.075,1.075,0.97,0.97,725221
AAPL,20150128,2.24,2.24,2.2,2.24,63682
AMZN,20150128,0.4,0.415,0.4,0.415,194900
NFLX,20150128,50.19,50.21,50.19,50.19,761845
GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and many
more..................

In case it's relevant, the number of stocks in these files are not
necessarily the same (so there will be missing data). I need to import and
create 5 separate time series data frames from those files, one each for
Open, High, Low, Close and Volume. In each data frame, rows are indexed by
date, and columns by ticker. For example, the data frame Open may look like
this:

DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
20150129,NA,2.3,0.5,5.2,1.7,... ...

What will be an efficient way to do that? I've used the following codes to
read the files into a list of data frames but don't know what to do next
from here.

files = list.files(pattern="*.txt") mydata = lapply(files,
read.csv,head=FALSE)

Thanks,

Nathan

Disclaimer: In case it's relevant, this question is also posted on
stackoverflow.

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov 15 01:34:21 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Nov 2019 16:34:21 -0800
Subject: [R] How to import and create time series data frames in an
 efficient way?
In-Reply-To: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
References: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
Message-ID: <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>

So you've made no attempt at all to do this for yourself?!

That suggests to me that you need to spend time with some R tutorials.

Also, please post in plain text on this plain text list. HTML can get
mangled, as it may have here.

-- Bert
"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 14, 2019 at 4:11 PM Nhan La <lathanhnhan at gmail.com> wrote:

> I have many separate data files in csv format for a lot of daily stock
> prices. Over a few years there are hundreds of those data files, whose
> names are the dates of data record.
>
> In each file there are variables of ticker (or stock trading code), date,
> open price, high price, low price, close price, and trading volume. For
> example, inside a data file named 20150128.txt it looks like this:
>
> FB,20150128,1.075,1.075,0.97,0.97,725221
> AAPL,20150128,2.24,2.24,2.2,2.24,63682
> AMZN,20150128,0.4,0.415,0.4,0.415,194900
> NFLX,20150128,50.19,50.21,50.19,50.19,761845
> GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and many
> more..................
>
> In case it's relevant, the number of stocks in these files are not
> necessarily the same (so there will be missing data). I need to import and
> create 5 separate time series data frames from those files, one each for
> Open, High, Low, Close and Volume. In each data frame, rows are indexed by
> date, and columns by ticker. For example, the data frame Open may look like
> this:
>
> DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
> 20150129,NA,2.3,0.5,5.2,1.7,... ...
>
> What will be an efficient way to do that? I've used the following codes to
> read the files into a list of data frames but don't know what to do next
> from here.
>
> files = list.files(pattern="*.txt") mydata = lapply(files,
> read.csv,head=FALSE)
>
> Thanks,
>
> Nathan
>
> Disclaimer: In case it's relevant, this question is also posted on
> stackoverflow.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From |@th@nhnh@n @end|ng |rom gm@||@com  Fri Nov 15 01:57:29 2019
From: |@th@nhnh@n @end|ng |rom gm@||@com (Nhan La)
Date: Fri, 15 Nov 2019 11:57:29 +1100
Subject: [R] How to import and create time series data frames in an
 efficient way?
In-Reply-To: <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>
References: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
 <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>
Message-ID: <CAB_bKVs54TwuZ6u7mFO=fEceLnWa9go_cJaw5eH_mZ28afW+fQ@mail.gmail.com>

Hi Bert,

I've attempted to find the answer and actually been able to import the
individual data sets into a list of data frames.

But I'm not sure how to go ahead with the next step. I'm not necessarily
asking for a final answer. Perhaps if you (I mean others as well) would
like a constructive coaching, you would suggest a few key words to look at?

Sorry for the HTML thing, this is my first post. I'll do better next times.

Thanks,
Nathan



On Fri, Nov 15, 2019 at 11:34 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> So you've made no attempt at all to do this for yourself?!
>
> That suggests to me that you need to spend time with some R tutorials.
>
> Also, please post in plain text on this plain text list. HTML can get
> mangled, as it may have here.
>
> -- Bert
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Nov 14, 2019 at 4:11 PM Nhan La <lathanhnhan at gmail.com> wrote:
>
>> I have many separate data files in csv format for a lot of daily stock
>> prices. Over a few years there are hundreds of those data files, whose
>> names are the dates of data record.
>>
>> In each file there are variables of ticker (or stock trading code), date,
>> open price, high price, low price, close price, and trading volume. For
>> example, inside a data file named 20150128.txt it looks like this:
>>
>> FB,20150128,1.075,1.075,0.97,0.97,725221
>> AAPL,20150128,2.24,2.24,2.2,2.24,63682
>> AMZN,20150128,0.4,0.415,0.4,0.415,194900
>> NFLX,20150128,50.19,50.21,50.19,50.19,761845
>> GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and many
>> more..................
>>
>> In case it's relevant, the number of stocks in these files are not
>> necessarily the same (so there will be missing data). I need to import and
>> create 5 separate time series data frames from those files, one each for
>> Open, High, Low, Close and Volume. In each data frame, rows are indexed by
>> date, and columns by ticker. For example, the data frame Open may look
>> like
>> this:
>>
>> DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
>> 20150129,NA,2.3,0.5,5.2,1.7,... ...
>>
>> What will be an efficient way to do that? I've used the following codes to
>> read the files into a list of data frames but don't know what to do next
>> from here.
>>
>> files = list.files(pattern="*.txt") mydata = lapply(files,
>> read.csv,head=FALSE)
>>
>> Thanks,
>>
>> Nathan
>>
>> Disclaimer: In case it's relevant, this question is also posted on
>> stackoverflow.
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From peter@|@ng|e|der @end|ng |rom gm@||@com  Fri Nov 15 02:37:24 2019
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Thu, 14 Nov 2019 17:37:24 -0800
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
Message-ID: <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>

I suspect that you want to identify which variables are highly
correlated, and then keep only "representative" variables, i.e.,
remove redundant ones. This is a bit of a risky procedure but I have
done such things before as well sometimes to simplify large sets of
highly related variables. If your threshold of 0.8 is approximate, you
could simply use average linkage hierarchical clustering with
dissimilarity = 1-correlation, cut the tree at the appropriate height
(1-0.8=0.2), and from each cluster keep a single representative (e.g.,
the one with the highest mean correlation with other members of the
cluster). Something along these lines (untested)

tree = hclust(1-calc.rho, method = "average")
clusts = cutree(tree, h = 0.2)
clustLevels = sort(unique(clusts))
representatives = unlist(lapply(clustLevels, function(cl)
{
  inClust = which(clusts==cl);
  rho1 = calc.rho[inClust, inClust, drop = FALSE];
  repr = inClust[ which.max(colSums(rho1)) ]
  repr
}))

the variable representatives now contains indices of the variables you
want to retain, so you could subset the calc.rho matrix as
rho.retained = calc.rho[representatives, representatives]

I haven't tested the code and it may contain bugs, but something along
these lines should get you where you want to be.

Oh, and depending on how strict you want to be with the remaining
correlations, you could use complete linkage clustering (will retain
more variables, some correlations will be above 0.8).

Peter

On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I have a data frame like this (a matrix):
> head(calc.rho)
>             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
>
> > dim(calc.rho)
> [1] 246 246
>
> I would like to remove from this data all highly correlated variables,
> with correlation more than 0.8
>
> I tried this:
>
> > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > dim(data)
> [1] 246   0
>
> Can you please advise,
>
> Thanks
> Ana
>
> But this removes everything.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov 15 05:10:03 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Nov 2019 20:10:03 -0800
Subject: [R] How to import and create time series data frames in an
 efficient way?
In-Reply-To: <CAB_bKVs54TwuZ6u7mFO=fEceLnWa9go_cJaw5eH_mZ28afW+fQ@mail.gmail.com>
References: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
 <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>
 <CAB_bKVs54TwuZ6u7mFO=fEceLnWa9go_cJaw5eH_mZ28afW+fQ@mail.gmail.com>
Message-ID: <CAGxFJbQwcnQHXWsKmZkHon+4cFymGnxBAnsdZUUy_yojLns+uw@mail.gmail.com>

Brute force approach, possibly inefficient:

1. You have a vector of file names. Sort them in the appropriate (time)
order. These names are also the component names of all the data frames in
your list that you read in, call it yourlist.

2. Create a vector of all the unique ticker names, perhaps by creating a
vector of all the names and then unique() -ing it. Call this vector snames
with n.names in it. It will probably have length several hundred at least I
assume.

3. Suppose the  6 columns of data of each data frame that you want are
named cnames = c("stocknames","Open", "High", "Low", "Close", "Volume").

4. You could proceed as you suggested, but it would likely be more
efficient, since all data that you want are numeric, to create a 3D array
of NA's via:

yourdat <- array(dim = c(n.dates, n.names, 5), dimnames = list(NULL,
snames, cnames[-1]))

5. Then just loop  through your list of files and use indexing to fill in
the columns x category slices for each date. Stocks that are missing will
be NA automatically. e.g. (warning: UNTESTED):

For date "d", let df be the data frame from date "d" in your list, i.e.

df <- yourlist[["d"]][, cnames]
## Note The order of the listed stocks in the "stocknames" column can be
different from frame to frame of your master list.

Then fill in the flat for the dth date (i.e. dth row) in your array by:

yourdat[ ,"stocknames", cnames[-1] <- as.matrix(df[ ,-1]) ## need to omit
the column names so it converts to numeric matrix

This should fill in  the values of the 2nd and 3rd dimensions of the array
for all the stocks on the dth date with the data for each stock in the data
frame matched to the appropriate column in the array.

The entire loop will give all dates for all stocks and all categories with
NA's for missing days. (*IF IT WORKS!*)
You may need to modify this sightly if, for example, your stock names are
row names rather than a field in your data frame. I leave such adjustments
to you.

Note again that this is fairly elementary with just arrays and indexing.
Basic tutorials should tell you about all of this. Also, when plotting,
you'll have to convert your dates to suitable date-time format.

Cheers,
Bert




On Thu, Nov 14, 2019 at 4:55 PM Nhan La <lathanhnhan at gmail.com> wrote:

> Hi Bert,
>
> I've attempted to find the answer and actually been able to import the
> individual data sets into a list of data frames.
>
> But I'm not sure how to go ahead with the next step. I'm not necessarily
> asking for a final answer. Perhaps if you (I mean others as well) would
> like a constructive coaching, you would suggest a few key words to look at?
>
> Sorry for the HTML thing, this is my first post. I'll do better next times.
>
> Thanks,
> Nathan
>
>
>
> On Fri, Nov 15, 2019 at 11:34 AM Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
>> So you've made no attempt at all to do this for yourself?!
>>
>> That suggests to me that you need to spend time with some R tutorials.
>>
>> Also, please post in plain text on this plain text list. HTML can get
>> mangled, as it may have here.
>>
>> -- Bert
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, Nov 14, 2019 at 4:11 PM Nhan La <lathanhnhan at gmail.com> wrote:
>>
>>> I have many separate data files in csv format for a lot of daily stock
>>> prices. Over a few years there are hundreds of those data files, whose
>>> names are the dates of data record.
>>>
>>> In each file there are variables of ticker (or stock trading code), date,
>>> open price, high price, low price, close price, and trading volume. For
>>> example, inside a data file named 20150128.txt it looks like this:
>>>
>>> FB,20150128,1.075,1.075,0.97,0.97,725221
>>> AAPL,20150128,2.24,2.24,2.2,2.24,63682
>>> AMZN,20150128,0.4,0.415,0.4,0.415,194900
>>> NFLX,20150128,50.19,50.21,50.19,50.19,761845
>>> GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and many
>>> more..................
>>>
>>> In case it's relevant, the number of stocks in these files are not
>>> necessarily the same (so there will be missing data). I need to import
>>> and
>>> create 5 separate time series data frames from those files, one each for
>>> Open, High, Low, Close and Volume. In each data frame, rows are indexed
>>> by
>>> date, and columns by ticker. For example, the data frame Open may look
>>> like
>>> this:
>>>
>>> DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
>>> 20150129,NA,2.3,0.5,5.2,1.7,... ...
>>>
>>> What will be an efficient way to do that? I've used the following codes
>>> to
>>> read the files into a list of data frames but don't know what to do next
>>> from here.
>>>
>>> files = list.files(pattern="*.txt") mydata = lapply(files,
>>> read.csv,head=FALSE)
>>>
>>> Thanks,
>>>
>>> Nathan
>>>
>>> Disclaimer: In case it's relevant, this question is also posted on
>>> stackoverflow.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov 15 05:44:54 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 14 Nov 2019 20:44:54 -0800
Subject: [R] How to import and create time series data frames in an
 efficient way?
In-Reply-To: <CAGxFJbQwcnQHXWsKmZkHon+4cFymGnxBAnsdZUUy_yojLns+uw@mail.gmail.com>
References: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
 <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>
 <CAB_bKVs54TwuZ6u7mFO=fEceLnWa9go_cJaw5eH_mZ28afW+fQ@mail.gmail.com>
 <CAGxFJbQwcnQHXWsKmZkHon+4cFymGnxBAnsdZUUy_yojLns+uw@mail.gmail.com>
Message-ID: <CAGxFJbSh4H-T3c41-0wQ4KrGxjw80U4Q-ZQdmOdYBOH6bM=-FQ@mail.gmail.com>

Ha! -- A bug! "Corrected" version inline below:
Bert Gunter

On Thu, Nov 14, 2019 at 8:10 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Brute force approach, possibly inefficient:
>
> 1. You have a vector of file names. Sort them in the appropriate (time)
> order. These names are also the component names of all the data frames in
> your list that you read in, call it yourlist.
>
> 2. Create a vector of all the unique ticker names, perhaps by creating a
> vector of all the names and then unique() -ing it. Call this vector snames
> with n.names in it. It will probably have length several hundred at least I
> assume.
>
> 3. Suppose the  6 columns of data of each data frame that you want are
> named cnames = c("stocknames","Open", "High", "Low", "Close", "Volume").
>
> 4. You could proceed as you suggested, but it would likely be more
> efficient, since all data that you want are numeric, to create a 3D array
> of NA's via:
>
> yourdat <- array(dim = c(n.dates, n.names, 5), dimnames = list(NULL,
> snames, cnames[-1]))
>
> 5. Then just loop  through your list of files and use indexing to fill in
> the columns x category slices for each date. Stocks that are missing will
> be NA automatically. e.g. (warning: UNTESTED):
>
> For date "d", let df be the data frame from date "d" in your list, i.e.
>
> df <- yourlist[["d"]][, cnames]
> ## Note The order of the listed stocks in the "stocknames" column can be
> different from frame to frame of your master list.
>
> Then fill in the flat for the dth date (i.e. dth row) in your array by:
>
> ## corrected line here:

> yourdat[ ,df[ ,"stocknames"], cnames[-1] <- as.matrix(df[ ,-1]) ## need
> to omit the column names so it converts to numeric matrix
>
## need to get the names of the stocks in the "stocknames" column in the
order they appear in df.

>
> This should fill in  the values of the 2nd and 3rd dimensions of the array
> for all the stocks on the dth date with the data for each stock in the data
> frame matched to the appropriate column in the array.
>
> The entire loop will give all dates for all stocks and all categories with
> NA's for missing days. (*IF IT WORKS!*)
> You may need to modify this sightly if, for example, your stock names are
> row names rather than a field in your data frame. I leave such adjustments
> to you.
>
> Note again that this is fairly elementary with just arrays and indexing.
> Basic tutorials should tell you about all of this. Also, when plotting,
> you'll have to convert your dates to suitable date-time format.
>
> Cheers,
> Bert
>
>
>
>
> On Thu, Nov 14, 2019 at 4:55 PM Nhan La <lathanhnhan at gmail.com> wrote:
>
>> Hi Bert,
>>
>> I've attempted to find the answer and actually been able to import the
>> individual data sets into a list of data frames.
>>
>> But I'm not sure how to go ahead with the next step. I'm not necessarily
>> asking for a final answer. Perhaps if you (I mean others as well) would
>> like a constructive coaching, you would suggest a few key words to look at?
>>
>> Sorry for the HTML thing, this is my first post. I'll do better next
>> times.
>>
>> Thanks,
>> Nathan
>>
>>
>>
>> On Fri, Nov 15, 2019 at 11:34 AM Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>>> So you've made no attempt at all to do this for yourself?!
>>>
>>> That suggests to me that you need to spend time with some R tutorials.
>>>
>>> Also, please post in plain text on this plain text list. HTML can get
>>> mangled, as it may have here.
>>>
>>> -- Bert
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>>
>>> On Thu, Nov 14, 2019 at 4:11 PM Nhan La <lathanhnhan at gmail.com> wrote:
>>>
>>>> I have many separate data files in csv format for a lot of daily stock
>>>> prices. Over a few years there are hundreds of those data files, whose
>>>> names are the dates of data record.
>>>>
>>>> In each file there are variables of ticker (or stock trading code),
>>>> date,
>>>> open price, high price, low price, close price, and trading volume. For
>>>> example, inside a data file named 20150128.txt it looks like this:
>>>>
>>>> FB,20150128,1.075,1.075,0.97,0.97,725221
>>>> AAPL,20150128,2.24,2.24,2.2,2.24,63682
>>>> AMZN,20150128,0.4,0.415,0.4,0.415,194900
>>>> NFLX,20150128,50.19,50.21,50.19,50.19,761845
>>>> GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and many
>>>> more..................
>>>>
>>>> In case it's relevant, the number of stocks in these files are not
>>>> necessarily the same (so there will be missing data). I need to import
>>>> and
>>>> create 5 separate time series data frames from those files, one each for
>>>> Open, High, Low, Close and Volume. In each data frame, rows are indexed
>>>> by
>>>> date, and columns by ticker. For example, the data frame Open may look
>>>> like
>>>> this:
>>>>
>>>> DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
>>>> 20150129,NA,2.3,0.5,5.2,1.7,... ...
>>>>
>>>> What will be an efficient way to do that? I've used the following codes
>>>> to
>>>> read the files into a list of data frames but don't know what to do next
>>>> from here.
>>>>
>>>> files = list.files(pattern="*.txt") mydata = lapply(files,
>>>> read.csv,head=FALSE)
>>>>
>>>> Thanks,
>>>>
>>>> Nathan
>>>>
>>>> Disclaimer: In case it's relevant, this question is also posted on
>>>> stackoverflow.
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov 15 19:03:41 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 15 Nov 2019 12:03:41 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>
Message-ID: <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>

HI Peter,

Thank you for getting back to me and shedding light on this. I see
your point, doing Jim's method:

> keeprows<-apply(calc.rho,1,function(x) return(sum(x>0.8)<3))
> ro246.lt.8<-calc.rho[keeprows,keeprows]
> ro246.lt.8[ro246.lt.8 == 1] <- NA
> (mmax <- max(abs(ro246.lt.8), na.rm=TRUE))
[1] 0.566

Which is good in general, correlations in my matrix  should not be
exceeding 0.8. I need to run Mendelian Rendomization on it later on so
I can not be having there highly correlated SNPs. But with Jim's
method I am only left with 17 SNPs (out of 246) and that means that
both pairs of highly correlated SNPs are removed and it would be good
to keep one of those highly correlated ones.

I tried to do your code:
> tree = hclust(1-calc.rho, method = "average")
Error in if (is.na(n) || n > 65536L) stop("size cannot be NA nor
exceed 65536") :
  missing value where TRUE/FALSE needed

Please advise.

Thanks
Ana

On Thu, Nov 14, 2019 at 7:37 PM Peter Langfelder
<peter.langfelder at gmail.com> wrote:
>
> I suspect that you want to identify which variables are highly
> correlated, and then keep only "representative" variables, i.e.,
> remove redundant ones. This is a bit of a risky procedure but I have
> done such things before as well sometimes to simplify large sets of
> highly related variables. If your threshold of 0.8 is approximate, you
> could simply use average linkage hierarchical clustering with
> dissimilarity = 1-correlation, cut the tree at the appropriate height
> (1-0.8=0.2), and from each cluster keep a single representative (e.g.,
> the one with the highest mean correlation with other members of the
> cluster). Something along these lines (untested)
>
> tree = hclust(1-calc.rho, method = "average")
> clusts = cutree(tree, h = 0.2)
> clustLevels = sort(unique(clusts))
> representatives = unlist(lapply(clustLevels, function(cl)
> {
>   inClust = which(clusts==cl);
>   rho1 = calc.rho[inClust, inClust, drop = FALSE];
>   repr = inClust[ which.max(colSums(rho1)) ]
>   repr
> }))
>
> the variable representatives now contains indices of the variables you
> want to retain, so you could subset the calc.rho matrix as
> rho.retained = calc.rho[representatives, representatives]
>
> I haven't tested the code and it may contain bugs, but something along
> these lines should get you where you want to be.
>
> Oh, and depending on how strict you want to be with the remaining
> correlations, you could use complete linkage clustering (will retain
> more variables, some correlations will be above 0.8).
>
> Peter
>
> On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > Hello,
> >
> > I have a data frame like this (a matrix):
> > head(calc.rho)
> >             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> > rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> > rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> > rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> > rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> > rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> > rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> >
> > > dim(calc.rho)
> > [1] 246 246
> >
> > I would like to remove from this data all highly correlated variables,
> > with correlation more than 0.8
> >
> > I tried this:
> >
> > > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > > dim(data)
> > [1] 246   0
> >
> > Can you please advise,
> >
> > Thanks
> > Ana
> >
> > But this removes everything.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Fri Nov 15 19:31:28 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 15 Nov 2019 12:31:28 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>
 <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>
Message-ID: <CAF9-5jM71OpcP8aQjxXehT_Pyq83zww-jjgJHfG-XCmzUtT+Kw@mail.gmail.com>

if it is of any help my correlation matrix (calc.rho) was done here,
under LDmatrix tab https://ldlink.nci.nih.gov/?tab=ldmatrix
and dataset of 246 is bellow

rs56192520
rs3764410
rs145984817
rs1807401
rs1807402
rs35350506
rs2089177
rs12325677
rs62064624
rs62064631
rs2349295
rs2174369
rs7218554
rs62064634
rs4360974
rs4527060
rs6502526
rs6502527
rs9900318
rs8069906
rs9908521
rs9908336
rs9908870
rs9895995
rs7211086
rs9905280
rs8073305
rs8072086
rs4312350
rs4313843
rs8069610
rs883504
rs8072394
rs4280293
rs4465638
rs12602378
rs9899059
rs6502530
rs4380085
rs6502532
rs4792798
rs4792799
rs4316813
rs148563931
rs74751226
rs8068857
rs8069441
rs77397878
rs75339756
rs4608391
rs79569548
rs4275914
rs11870422
rs8075751
rs11658904
rs138437542
rs80344434
rs7222311
rs7221842
rs7223686
rs78013597
rs74965036
rs78063986
rs118106233
rs117345712
rs113004656
rs9898995
rs4985718
rs9893911
rs79110942
rs7208929
rs12601453
rs4078062
rs75129280
rs76664572
rs78961289
rs146364798
rs76715413
rs4078534
rs79457460
rs74369938
rs76423171
rs74668400
rs75146120
rs1135237
rs9914671
rs117759512
rs4985696
rs16961340
rs17794159
rs4247118
rs78572469
rs12601193
rs2349646
rs2090018
rs12601424
rs4985701
rs8064550
rs2271521
rs2271520
rs11078374
rs4985702
rs1124961
rs11652674
rs3924340
rs112450164
rs7208973
rs9910857
rs78574480
rs8072184
rs12602196
rs6502563
rs3744135
rs148779543
rs77689691
rs41319048
rs117340532
rs78647096
rs77712968
rs16961396
rs80054920
rs7206981
rs4985740
rs3803762
rs77103270
rs7207485
rs77342773
rs3826304
rs3744126
rs7210879
rs7211576
rs117967362
rs75978745
rs6502564
rs9894565
rs36079048
rs8076621
rs7218795
rs3803761
rs12602675
rs7208065
rs4985705
rs8080386
rs8065832
rs2018781
rs1736221
rs1736220
rs1736217
rs1708620
rs1708619
rs1736216
rs76319098
rs1736215
rs1736214
rs1708617
rs12602831
rs12602871
rs1736213
rs1736212
rs76045368
rs34518797
rs11078378
rs8079562
rs8065774
rs8066090
rs41337846
rs1736209
rs1736208
rs12949822
rs76246042
rs12600635
rs55689224
rs1736207
rs1708626
rs1736206
rs9896078
rs16961474
rs1708627
rs1736205
rs1708628
rs7220577
rs2294155
rs1736204
rs1736203
rs1736202
rs12937908
rs1736200
rs1708623
rs1708624
rs9894884
rs9901894
rs9903294
rs2472689
rs1630656
rs111478970
rs3182911
rs7219012
rs9890657
rs12453455
rs12947291
rs150267386
rs16961493
rs11652745
rs9907107
rs8070574
rs4985759
rs3866959
rs7219248
rs6502568
rs7220275
rs12450037
rs7225876
rs9892352
rs4985760
rs6502569
rs1029830
rs2012954
rs1029832
rs2270180
rs8072402
rs7221553
rs145597919
rs150772017
rs2041393
rs6502578
rs11078382
rs9912109
rs12601631
rs11869054
rs11869079
rs9912599
rs7220057
rs9896970
rs34121330
rs34668117
rs67773570
rs242252
rs955893
rs28583584
rs9944423
rs7217764
rs11651957
rs73978990
rs8071007
rs56044345
rs17804843


On Fri, Nov 15, 2019 at 12:03 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> HI Peter,
>
> Thank you for getting back to me and shedding light on this. I see
> your point, doing Jim's method:
>
> > keeprows<-apply(calc.rho,1,function(x) return(sum(x>0.8)<3))
> > ro246.lt.8<-calc.rho[keeprows,keeprows]
> > ro246.lt.8[ro246.lt.8 == 1] <- NA
> > (mmax <- max(abs(ro246.lt.8), na.rm=TRUE))
> [1] 0.566
>
> Which is good in general, correlations in my matrix  should not be
> exceeding 0.8. I need to run Mendelian Rendomization on it later on so
> I can not be having there highly correlated SNPs. But with Jim's
> method I am only left with 17 SNPs (out of 246) and that means that
> both pairs of highly correlated SNPs are removed and it would be good
> to keep one of those highly correlated ones.
>
> I tried to do your code:
> > tree = hclust(1-calc.rho, method = "average")
> Error in if (is.na(n) || n > 65536L) stop("size cannot be NA nor
> exceed 65536") :
>   missing value where TRUE/FALSE needed
>
> Please advise.
>
> Thanks
> Ana
>
> On Thu, Nov 14, 2019 at 7:37 PM Peter Langfelder
> <peter.langfelder at gmail.com> wrote:
> >
> > I suspect that you want to identify which variables are highly
> > correlated, and then keep only "representative" variables, i.e.,
> > remove redundant ones. This is a bit of a risky procedure but I have
> > done such things before as well sometimes to simplify large sets of
> > highly related variables. If your threshold of 0.8 is approximate, you
> > could simply use average linkage hierarchical clustering with
> > dissimilarity = 1-correlation, cut the tree at the appropriate height
> > (1-0.8=0.2), and from each cluster keep a single representative (e.g.,
> > the one with the highest mean correlation with other members of the
> > cluster). Something along these lines (untested)
> >
> > tree = hclust(1-calc.rho, method = "average")
> > clusts = cutree(tree, h = 0.2)
> > clustLevels = sort(unique(clusts))
> > representatives = unlist(lapply(clustLevels, function(cl)
> > {
> >   inClust = which(clusts==cl);
> >   rho1 = calc.rho[inClust, inClust, drop = FALSE];
> >   repr = inClust[ which.max(colSums(rho1)) ]
> >   repr
> > }))
> >
> > the variable representatives now contains indices of the variables you
> > want to retain, so you could subset the calc.rho matrix as
> > rho.retained = calc.rho[representatives, representatives]
> >
> > I haven't tested the code and it may contain bugs, but something along
> > these lines should get you where you want to be.
> >
> > Oh, and depending on how strict you want to be with the remaining
> > correlations, you could use complete linkage clustering (will retain
> > more variables, some correlations will be above 0.8).
> >
> > Peter
> >
> > On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > Hello,
> > >
> > > I have a data frame like this (a matrix):
> > > head(calc.rho)
> > >             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> > > rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> > > rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> > > rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> > > rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> > > rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> > > rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> > >
> > > > dim(calc.rho)
> > > [1] 246 246
> > >
> > > I would like to remove from this data all highly correlated variables,
> > > with correlation more than 0.8
> > >
> > > I tried this:
> > >
> > > > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > > > dim(data)
> > > [1] 246   0
> > >
> > > Can you please advise,
> > >
> > > Thanks
> > > Ana
> > >
> > > But this removes everything.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From drj|m|emon @end|ng |rom gm@||@com  Fri Nov 15 21:17:50 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sat, 16 Nov 2019 07:17:50 +1100
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jM71OpcP8aQjxXehT_Pyq83zww-jjgJHfG-XCmzUtT+Kw@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>
 <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>
 <CAF9-5jM71OpcP8aQjxXehT_Pyq83zww-jjgJHfG-XCmzUtT+Kw@mail.gmail.com>
Message-ID: <CA+8X3fXP7qgv+O8vfAT8XyP=V4kY+61mY3K58R4B75JdtMfHiA@mail.gmail.com>

While the remedy for your dissatisfaction with my previous solution
should be obvious, I will make it explicit.

# that is rows containing at most one value > 0.8
# ignoring the diagonal
keeprows<-apply(ro246,1,function(x) return(sum(x>0.8)<2))
ro246.lt.8<-ro246[keeprows,keeprows]

Jim


From @m|r@h@d @end|ng |rom gm@||@com  Fri Nov 15 19:49:52 2019
From: @m|r@h@d @end|ng |rom gm@||@com (Amir Hadanny)
Date: Fri, 15 Nov 2019 20:49:52 +0200
Subject: [R] Glmnet survival cox predict
Message-ID: <CAKpCQaWWu0O49SimFrX2B0WbURu9kM_C7Nqz-1zD6R_QiWU07g@mail.gmail.com>

Hi all,
i'm trying to get the prediction probabilities for a survival elastic net.
When i use try to predict using the train model on the test set, it creates
an object with the number rows of the train data (6400 rows) instead of the
test data (2400 rows). I really don't understand why, and that doesn't let
me check for performance c-index.
the code:

data<-read.csv("old4.csv", header=TRUE)
library(imputeMissings)
data<-impute(data,object = NULL ,method = "median/mode")

trainstatus<-train$DIED1095
trainTime<-train$TIME
y<-Surv(trainTime,trainstatus)

trainX<-train[-c(12,63,64,65,66,67,68,69,70,71)]
x<-data.matrix(trainX)


library(glmnet)
fit <- glmnet(x,Surv(trainTime,trainstatus),family="cox",alpha=0.1,
,maxit=10000)
max.dev.index     <- which.max(fit$dev.ratio)
optimal.lambda <- fit$lambda[max.dev.index]
optimal.beta  <- fit$beta[,max.dev.index]
nonzero.coef <- abs(optimal.beta)>0
selectedBeta <- optimal.beta[nonzero.coef]
selectedTrainX   <- x[,nonzero.coef]

coxph.model<- coxph(Surv(train$TIME,train$DIED365) ~x,data=train,
init=selectedBeta,iter=0)
coxph.predict<-predict(coxph.model,test)

nrow(test)
2872

nrow(train
6701

length(coxph.predict)
6701

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Fri Nov 15 23:00:17 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Fri, 15 Nov 2019 14:00:17 -0800
Subject: [R] Glmnet survival cox predict
In-Reply-To: <CAKpCQaWWu0O49SimFrX2B0WbURu9kM_C7Nqz-1zD6R_QiWU07g@mail.gmail.com>
References: <CAKpCQaWWu0O49SimFrX2B0WbURu9kM_C7Nqz-1zD6R_QiWU07g@mail.gmail.com>
Message-ID: <35fa62d5-ce6c-353c-44af-19f68820df03@comcast.net>


On 11/15/19 10:49 AM, Amir Hadanny wrote:
> Hi all,
> i'm trying to get the prediction probabilities for a survival elastic net.
> When i use try to predict using the train model on the test set, it creates
> an object with the number rows of the train data (6400 rows) instead of the
> test data (2400 rows). I really don't understand why, and that doesn't let
> me check for performance c-index.


If you call most `predict` functions with a second argument that fails 
to contain the predictors in the model, it returns the predictions on 
the original data. The only place where the `test` object appears prior 
to the predict operation is in your call to `predict.coxph`, so my guess 
is that it fails to meet the requirements of the function for a valid 
newdata argument. (Another thought was that maybe `test` didn't exist, 
but that should have thrown an error with the predict call and the nrow 
call.)


But since you don't provide code that creates `test` or even an 
unambiguous way of examining its structure, that is entirely a guess.


And finally ... Rhelp is a plain text mailing list, so please to read 
the message at the bottom of every transmission from the mailserver ... 
i.e.? read the Posting Guide. (It is not at all difficult to get 
gmail.com to send plain text.)


-- 

David.

> the code:
>
> data<-read.csv("old4.csv", header=TRUE)
> library(imputeMissings)
> data<-impute(data,object = NULL ,method = "median/mode")
>
> trainstatus<-train$DIED1095
> trainTime<-train$TIME
> y<-Surv(trainTime,trainstatus)
>
> trainX<-train[-c(12,63,64,65,66,67,68,69,70,71)]
> x<-data.matrix(trainX)
>
>
> library(glmnet)
> fit <- glmnet(x,Surv(trainTime,trainstatus),family="cox",alpha=0.1,
> ,maxit=10000)
> max.dev.index     <- which.max(fit$dev.ratio)
> optimal.lambda <- fit$lambda[max.dev.index]
> optimal.beta  <- fit$beta[,max.dev.index]
> nonzero.coef <- abs(optimal.beta)>0
> selectedBeta <- optimal.beta[nonzero.coef]
> selectedTrainX   <- x[,nonzero.coef]
>
> coxph.model<- coxph(Surv(train$TIME,train$DIED365) ~x,data=train,
> init=selectedBeta,iter=0)
> coxph.predict<-predict(coxph.model,test)
>
> nrow(test)
> 2872
>
> nrow(train
> 6701
>
> length(coxph.predict)
> 6701
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jb@rth1235 @end|ng |rom gm@||@com  Sat Nov 16 00:56:20 2019
From: jb@rth1235 @end|ng |rom gm@||@com (Josh B)
Date: Fri, 15 Nov 2019 18:56:20 -0500
Subject: [R] Labeling Stacked Bar Plots Representing Percentages with Count
 Data
Message-ID: <CAGi70XUZc09hGzC4QGTNrywabDJ5ppRQh3Ea1r3BsX-wDsOPHw@mail.gmail.com>

Hello,

I am trying to include the count labels on stacked bar plots which
represent percentages. I want to show x-amount of individuals make up the
graphed percentages. However, when I include the count labels my y-axis
gets blown out of proportion because it changes to match the count data,
not the percentages. Also, the bars are removed from the graph too? I have
reviewed other posts similar to this, such as: "How to add percentage or
count labels above percentage bar plot?". I cannot find the error in my r
command.

My command used is as follows:

sumplot<-ggplot(tagSummary,aes(x=recvDeployName,y=nDet,fill=speciesSci))+
  geom_bar(position="fill",stat="identity")+
  geom_text(aes(label=nDet),position=position_stack(vjust=0.5))+
  theme(axis.text.x=element_text(angle=90,hjust=1))+
  scale_y_continous(labels=scales::percent_format())

Example of data being graphed:

speciesSci         recvDeployName    nDet
1 Arenaria interpres Bucktoe Preserve    96
2 Arenaria interpres CHDE               132
3 Arenaria interpres Fortescue        22133
4 Arenaria interpres Mispillion        2031
5 Arenaria interpres Norbury           3709
6 Arenaria interpres Penn - DRL          49

What my graph looks like when I use the command example provided above:
graph <https://i.stack.imgur.com/TLLGh.png>

Any help would be greatly appreciated. Thank you.

*Joshua N. Barth*

	[[alternative HTML version deleted]]


From peter@|@ng|e|der @end|ng |rom gm@||@com  Sat Nov 16 03:01:41 2019
From: peter@|@ng|e|der @end|ng |rom gm@||@com (Peter Langfelder)
Date: Fri, 15 Nov 2019 18:01:41 -0800
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>
 <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>
Message-ID: <CA+hbrhXp_-Hx0XXoOn2Pq7H5_3mNk2F0YCKPzZ=dvprJW4adwg@mail.gmail.com>

Try hclust(as.dist(1-calc.rho), method = "average").

Peter

On Fri, Nov 15, 2019 at 10:02 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> HI Peter,
>
> Thank you for getting back to me and shedding light on this. I see
> your point, doing Jim's method:
>
> > keeprows<-apply(calc.rho,1,function(x) return(sum(x>0.8)<3))
> > ro246.lt.8<-calc.rho[keeprows,keeprows]
> > ro246.lt.8[ro246.lt.8 == 1] <- NA
> > (mmax <- max(abs(ro246.lt.8), na.rm=TRUE))
> [1] 0.566
>
> Which is good in general, correlations in my matrix  should not be
> exceeding 0.8. I need to run Mendelian Rendomization on it later on so
> I can not be having there highly correlated SNPs. But with Jim's
> method I am only left with 17 SNPs (out of 246) and that means that
> both pairs of highly correlated SNPs are removed and it would be good
> to keep one of those highly correlated ones.
>
> I tried to do your code:
> > tree = hclust(1-calc.rho, method = "average")
> Error in if (is.na(n) || n > 65536L) stop("size cannot be NA nor
> exceed 65536") :
>   missing value where TRUE/FALSE needed
>
> Please advise.
>
> Thanks
> Ana
>
> On Thu, Nov 14, 2019 at 7:37 PM Peter Langfelder
> <peter.langfelder at gmail.com> wrote:
> >
> > I suspect that you want to identify which variables are highly
> > correlated, and then keep only "representative" variables, i.e.,
> > remove redundant ones. This is a bit of a risky procedure but I have
> > done such things before as well sometimes to simplify large sets of
> > highly related variables. If your threshold of 0.8 is approximate, you
> > could simply use average linkage hierarchical clustering with
> > dissimilarity = 1-correlation, cut the tree at the appropriate height
> > (1-0.8=0.2), and from each cluster keep a single representative (e.g.,
> > the one with the highest mean correlation with other members of the
> > cluster). Something along these lines (untested)
> >
> > tree = hclust(1-calc.rho, method = "average")
> > clusts = cutree(tree, h = 0.2)
> > clustLevels = sort(unique(clusts))
> > representatives = unlist(lapply(clustLevels, function(cl)
> > {
> >   inClust = which(clusts==cl);
> >   rho1 = calc.rho[inClust, inClust, drop = FALSE];
> >   repr = inClust[ which.max(colSums(rho1)) ]
> >   repr
> > }))
> >
> > the variable representatives now contains indices of the variables you
> > want to retain, so you could subset the calc.rho matrix as
> > rho.retained = calc.rho[representatives, representatives]
> >
> > I haven't tested the code and it may contain bugs, but something along
> > these lines should get you where you want to be.
> >
> > Oh, and depending on how strict you want to be with the remaining
> > correlations, you could use complete linkage clustering (will retain
> > more variables, some correlations will be above 0.8).
> >
> > Peter
> >
> > On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > Hello,
> > >
> > > I have a data frame like this (a matrix):
> > > head(calc.rho)
> > >             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> > > rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> > > rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> > > rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> > > rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> > > rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> > > rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> > >
> > > > dim(calc.rho)
> > > [1] 246 246
> > >
> > > I would like to remove from this data all highly correlated variables,
> > > with correlation more than 0.8
> > >
> > > I tried this:
> > >
> > > > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > > > dim(data)
> > > [1] 246   0
> > >
> > > Can you please advise,
> > >
> > > Thanks
> > > Ana
> > >
> > > But this removes everything.
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Fri Nov 15 23:59:48 2019
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Fri, 15 Nov 2019 23:59:48 +0100
Subject: [R] Adaptive resampling in r
Message-ID: <CA+nrPnu5BB5PYbpggaEz8rK8kbxyd9w_UKw93FW1ZAZd742rBw@mail.gmail.com>

How the hyperparameter settings via adaptive resampling is different from
the one obtained from grid search or random search? Both grid and random
searches provide best parameters values and the same is achieved using
adaptive resampling.

	[[alternative HTML version deleted]]


From @m|r@h@d @end|ng |rom gm@||@com  Sat Nov 16 00:16:00 2019
From: @m|r@h@d @end|ng |rom gm@||@com (Amir Hadanny)
Date: Sat, 16 Nov 2019 01:16:00 +0200
Subject: [R] Glmnet survival cox predict
In-Reply-To: <35fa62d5-ce6c-353c-44af-19f68820df03@comcast.net>
References: <CAKpCQaWWu0O49SimFrX2B0WbURu9kM_C7Nqz-1zD6R_QiWU07g@mail.gmail.com>
 <35fa62d5-ce6c-353c-44af-19f68820df03@comcast.net>
Message-ID: <CAKpCQaV4T9x32Fibye6PiGDetfH9qZkoqo4cGbMr5A5y5v9N6Q@mail.gmail.com>

Thank you,
both train and test are originated from the same data object.

attached the missing code:

data<-read.csv("old4.csv", header=TRUE)

library(imputeMissings)
data<-impute(data,object = NULL ,method = "median/mode")

for (i in col[13:68]) {
  data[i]<-lapply(data[i], factor)
}
for (i in col[1:12]) {
  data[i]<-lapply(data[i], numeric)
}

data$TIME<-as.numeric(data$TIME)

  data<-data[-c(61,62,64,65,66,67,68)]
data$TIME<-ceiling(data$TIME/12)
data$TIME[which(data$TIME==37)]<-36

data1 = sort(sample(nrow(data), nrow(data)*.7))
train<-data[data1,]
test<-data[-data1,]


so test should be the exact same, and i still can't find the issue,

thank you
Amir

On Sat, Nov 16, 2019 at 12:00 AM David Winsemius <dwinsemius at comcast.net>
wrote:

>
> On 11/15/19 10:49 AM, Amir Hadanny wrote:
> > Hi all,
> > i'm trying to get the prediction probabilities for a survival elastic
> net.
> > When i use try to predict using the train model on the test set, it
> creates
> > an object with the number rows of the train data (6400 rows) instead of
> the
> > test data (2400 rows). I really don't understand why, and that doesn't
> let
> > me check for performance c-index.
>
>
> If you call most `predict` functions with a second argument that fails
> to contain the predictors in the model, it returns the predictions on
> the original data. The only place where the `test` object appears prior
> to the predict operation is in your call to `predict.coxph`, so my guess
> is that it fails to meet the requirements of the function for a valid
> newdata argument. (Another thought was that maybe `test` didn't exist,
> but that should have thrown an error with the predict call and the nrow
> call.)
>
>
> But since you don't provide code that creates `test` or even an
> unambiguous way of examining its structure, that is entirely a guess.
>
>
> And finally ... Rhelp is a plain text mailing list, so please to read
> the message at the bottom of every transmission from the mailserver ...
> i.e.  read the Posting Guide. (It is not at all difficult to get
> gmail.com to send plain text.)
>
>
> --
>
> David.
>
> > the code:
> >
> > data<-read.csv("old4.csv", header=TRUE)
> > library(imputeMissings)
> > data<-impute(data,object = NULL ,method = "median/mode")
> >
> > trainstatus<-train$DIED1095
> > trainTime<-train$TIME
> > y<-Surv(trainTime,trainstatus)
> >
> > trainX<-train[-c(12,63,64,65,66,67,68,69,70,71)]
> > x<-data.matrix(trainX)
> >
> >
> > library(glmnet)
> > fit <- glmnet(x,Surv(trainTime,trainstatus),family="cox",alpha=0.1,
> > ,maxit=10000)
> > max.dev.index     <- which.max(fit$dev.ratio)
> > optimal.lambda <- fit$lambda[max.dev.index]
> > optimal.beta  <- fit$beta[,max.dev.index]
> > nonzero.coef <- abs(optimal.beta)>0
> > selectedBeta <- optimal.beta[nonzero.coef]
> > selectedTrainX   <- x[,nonzero.coef]
> >
> > coxph.model<- coxph(Surv(train$TIME,train$DIED365) ~x,data=train,
> > init=selectedBeta,iter=0)
> > coxph.predict<-predict(coxph.model,test)
> >
> > nrow(test)
> > 2872
> >
> > nrow(train
> > 6701
> >
> > length(coxph.predict)
> > 6701
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 16 17:10:07 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Sat, 16 Nov 2019 10:10:07 -0600
Subject: [R] 
 Remove highly correlated variables from a data frame or matrix
In-Reply-To: <CA+hbrhXp_-Hx0XXoOn2Pq7H5_3mNk2F0YCKPzZ=dvprJW4adwg@mail.gmail.com>
References: <CAF9-5jNFMK6mbDYaVW++gNBQ5rH8cV1QKbju-_7Pxn91dX0xLw@mail.gmail.com>
 <CA+hbrhWqkaUtNvPBpAQHNwUhx5ex7qKqoaA2fbqQ8CFnvdFsLQ@mail.gmail.com>
 <CAF9-5jMcg_Ly+JkMpQy2DMyG=pV_eUR2XCE9paJDQhedzsNf1Q@mail.gmail.com>
 <CA+hbrhXp_-Hx0XXoOn2Pq7H5_3mNk2F0YCKPzZ=dvprJW4adwg@mail.gmail.com>
Message-ID: <CAF9-5jM=9hm475162Of4MaMgnkPvYkwnxKQ6xL6qR2hj-wybnw@mail.gmail.com>

Hi Peter,

Thank you so much!!! I will use complete linkage clustering because
Mendelian Randomization function
(https://cran.r-project.org/web/packages/MendelianRandomization/vignettes/Vignette_MR.pdf)
I plan to use allows for correlations but not as high as 0.9 or more.
I got 40 SNPs out of 246 so improvement!

Regards,
Ana

On Fri, Nov 15, 2019 at 8:01 PM Peter Langfelder
<peter.langfelder at gmail.com> wrote:
>
> Try hclust(as.dist(1-calc.rho), method = "average").
>
> Peter
>
> On Fri, Nov 15, 2019 at 10:02 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > HI Peter,
> >
> > Thank you for getting back to me and shedding light on this. I see
> > your point, doing Jim's method:
> >
> > > keeprows<-apply(calc.rho,1,function(x) return(sum(x>0.8)<3))
> > > ro246.lt.8<-calc.rho[keeprows,keeprows]
> > > ro246.lt.8[ro246.lt.8 == 1] <- NA
> > > (mmax <- max(abs(ro246.lt.8), na.rm=TRUE))
> > [1] 0.566
> >
> > Which is good in general, correlations in my matrix  should not be
> > exceeding 0.8. I need to run Mendelian Rendomization on it later on so
> > I can not be having there highly correlated SNPs. But with Jim's
> > method I am only left with 17 SNPs (out of 246) and that means that
> > both pairs of highly correlated SNPs are removed and it would be good
> > to keep one of those highly correlated ones.
> >
> > I tried to do your code:
> > > tree = hclust(1-calc.rho, method = "average")
> > Error in if (is.na(n) || n > 65536L) stop("size cannot be NA nor
> > exceed 65536") :
> >   missing value where TRUE/FALSE needed
> >
> > Please advise.
> >
> > Thanks
> > Ana
> >
> > On Thu, Nov 14, 2019 at 7:37 PM Peter Langfelder
> > <peter.langfelder at gmail.com> wrote:
> > >
> > > I suspect that you want to identify which variables are highly
> > > correlated, and then keep only "representative" variables, i.e.,
> > > remove redundant ones. This is a bit of a risky procedure but I have
> > > done such things before as well sometimes to simplify large sets of
> > > highly related variables. If your threshold of 0.8 is approximate, you
> > > could simply use average linkage hierarchical clustering with
> > > dissimilarity = 1-correlation, cut the tree at the appropriate height
> > > (1-0.8=0.2), and from each cluster keep a single representative (e.g.,
> > > the one with the highest mean correlation with other members of the
> > > cluster). Something along these lines (untested)
> > >
> > > tree = hclust(1-calc.rho, method = "average")
> > > clusts = cutree(tree, h = 0.2)
> > > clustLevels = sort(unique(clusts))
> > > representatives = unlist(lapply(clustLevels, function(cl)
> > > {
> > >   inClust = which(clusts==cl);
> > >   rho1 = calc.rho[inClust, inClust, drop = FALSE];
> > >   repr = inClust[ which.max(colSums(rho1)) ]
> > >   repr
> > > }))
> > >
> > > the variable representatives now contains indices of the variables you
> > > want to retain, so you could subset the calc.rho matrix as
> > > rho.retained = calc.rho[representatives, representatives]
> > >
> > > I haven't tested the code and it may contain bugs, but something along
> > > these lines should get you where you want to be.
> > >
> > > Oh, and depending on how strict you want to be with the remaining
> > > correlations, you could use complete linkage clustering (will retain
> > > more variables, some correlations will be above 0.8).
> > >
> > > Peter
> > >
> > > On Thu, Nov 14, 2019 at 10:50 AM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >
> > > > Hello,
> > > >
> > > > I have a data frame like this (a matrix):
> > > > head(calc.rho)
> > > >             rs9900318 rs8069906 rs9908521 rs9908336 rs9908870 rs9895995
> > > > rs56192520      0.903     0.268     0.327     0.327     0.327     0.582
> > > > rs3764410       0.928     0.276     0.336     0.336     0.336     0.598
> > > > rs145984817     0.975     0.309     0.371     0.371     0.371     0.638
> > > > rs1807401       0.975     0.309     0.371     0.371     0.371     0.638
> > > > rs1807402       0.975     0.309     0.371     0.371     0.371     0.638
> > > > rs35350506      0.975     0.309     0.371     0.371     0.371     0.638
> > > >
> > > > > dim(calc.rho)
> > > > [1] 246 246
> > > >
> > > > I would like to remove from this data all highly correlated variables,
> > > > with correlation more than 0.8
> > > >
> > > > I tried this:
> > > >
> > > > > data<- calc.rho[,!apply(calc.rho,2,function(x) any(abs(x) > 0.80))]
> > > > > dim(data)
> > > > [1] 246   0
> > > >
> > > > Can you please advise,
> > > >
> > > > Thanks
> > > > Ana
> > > >
> > > > But this removes everything.
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Nov 16 17:16:58 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 16 Nov 2019 16:16:58 +0000
Subject: [R] 
 Labeling Stacked Bar Plots Representing Percentages with Count Data
In-Reply-To: <CAGi70XUZc09hGzC4QGTNrywabDJ5ppRQh3Ea1r3BsX-wDsOPHw@mail.gmail.com>
References: <CAGi70XUZc09hGzC4QGTNrywabDJ5ppRQh3Ea1r3BsX-wDsOPHw@mail.gmail.com>
Message-ID: <e732932f-d7eb-6505-1efe-bf8bdc93117c@sapo.pt>

Hello,

In geom_text change to position = position_fill(vjust=0.5).
What's important is to have position = "fill" in geom_bar match geom_text.
Something like :


library(dplyr)
library(ggplot2)

data(mtcars)

mtcars %>%
   group_by(cyl, gear) %>%
   summarise(count = n()) %>%
   ggplot(aes(factor(cyl), y = count, fill = factor(gear))) +
   geom_bar(position = "fill", stat = "identity") +
   geom_text(aes(label = count),
             position = position_fill(vjust=0.5)) +
   theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
   scale_y_discrete(labels = scales::percent_format())


Hope this helps,

Rui Barradas

?s 23:56 de 15/11/19, Josh B escreveu:
> Hello,
> 
> I am trying to include the count labels on stacked bar plots which
> represent percentages. I want to show x-amount of individuals make up the
> graphed percentages. However, when I include the count labels my y-axis
> gets blown out of proportion because it changes to match the count data,
> not the percentages. Also, the bars are removed from the graph too? I have
> reviewed other posts similar to this, such as: "How to add percentage or
> count labels above percentage bar plot?". I cannot find the error in my r
> command.
> 
> My command used is as follows:
> 
> sumplot<-ggplot(tagSummary,aes(x=recvDeployName,y=nDet,fill=speciesSci))+
>    geom_bar(position="fill",stat="identity")+
>    geom_text(aes(label=nDet),position=position_stack(vjust=0.5))+
>    theme(axis.text.x=element_text(angle=90,hjust=1))+
>    scale_y_continous(labels=scales::percent_format())
> 
> Example of data being graphed:
> 
> speciesSci         recvDeployName    nDet
> 1 Arenaria interpres Bucktoe Preserve    96
> 2 Arenaria interpres CHDE               132
> 3 Arenaria interpres Fortescue        22133
> 4 Arenaria interpres Mispillion        2031
> 5 Arenaria interpres Norbury           3709
> 6 Arenaria interpres Penn - DRL          49
> 
> What my graph looks like when I use the command example provided above:
> graph <https://i.stack.imgur.com/TLLGh.png>
> 
> Any help would be greatly appreciated. Thank you.
> 
> *Joshua N. Barth*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From drj|m|emon @end|ng |rom gm@||@com  Sat Nov 16 23:16:00 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Sun, 17 Nov 2019 09:16:00 +1100
Subject: [R] 
 Labeling Stacked Bar Plots Representing Percentages with Count Data
In-Reply-To: <CAGi70XUZc09hGzC4QGTNrywabDJ5ppRQh3Ea1r3BsX-wDsOPHw@mail.gmail.com>
References: <CAGi70XUZc09hGzC4QGTNrywabDJ5ppRQh3Ea1r3BsX-wDsOPHw@mail.gmail.com>
Message-ID: <CA+8X3fURPOourGwya6qpa5KTANdFBbp5+L9v-3U27z=rWi9BEg@mail.gmail.com>

Hi Josh,
I couldn't work out how to do this in ggplot, but here is a possible solution:

tagSummary<-read.csv(text="speciesSci,recvDeployName,nDet
 Arenaria interpres,Bucktoe Preserve,96
 Arenaria interpres,CHDE,132
 Arenaria interpres,Fortescue,22133
 Arenaria interpres,Mispillion,2031
 Arenaria interpres,Norbury,3709
 Arenaria interpres,Penn - DRL,49
 Calidris alba,Bucktoe Preserve,56
 Calidris alba,CHDE,145
 Calidris alba,Fortescue,19000
 Calidris alba,Mispillion,2200
 Calidris alba,Norbury,3900
 Calidris alba,Penn - DRL,40
 Calidris canutus,Bucktoe Preserve,77
 Calidris canutus,CHDE,100
 Calidris canutus,Fortescue,15000
 Calidris canutus,Mispillion,1831
 Calidris canutus,Norbury,3100
 Calidris canutus,Penn - DRL,60")
par(mar=c(10,6,4,2),las=2)
barpos<-barplot(nDet~speciesSci+recvDeployName,tagSummary,xlab="",ylab="",
col=2:4)
par(las=0)
mtext("recvDeployName",side=1,line=8)
mtext("nDet",side=2,line=4)
xpos<-matrix(rep(barpos,3),ncol=3)
# get stacked heights
ypos<-matrix(tagSummary$nDet,ncol=3)
library(plotrix)
ypos<-t(matrix(tagSummary$nDet,ncol=3))
barlabels(barpos,ypos)
legendpos<-locator(1)
legend(legendpos[1],legendpos[2],unique(tagSummary$speciesSci),fill=2:4)

Your big problem is that three observation sites have tiny values
compared to the rest. As always there is a kludge available. Replace
the call to barlabels with the following:

barlabels(barpos[3:5],ypos[,3:5])
text(rep(barpos[c(1,2,6)],each=3),rep(c(5000,7500,10000),3),
 ypos[,c(1,2,6)],col=rep(2:4,3))

This gives you a legible plot, although you will probably have to play
with the "rep" numbers to get it to work for your entire data set.

Jim

On Sat, Nov 16, 2019 at 10:57 AM Josh B <jbarth1235 at gmail.com> wrote:
>
> Hello,
>
> I am trying to include the count labels on stacked bar plots which
> represent percentages. I want to show x-amount of individuals make up the
> graphed percentages. However, when I include the count labels my y-axis
> gets blown out of proportion because it changes to match the count data,
> not the percentages. Also, the bars are removed from the graph too? I have
> reviewed other posts similar to this, such as: "How to add percentage or
> count labels above percentage bar plot?". I cannot find the error in my r
> command.
>
> My command used is as follows:
>
> sumplot<-ggplot(tagSummary,aes(x=recvDeployName,y=nDet,fill=speciesSci))+
>   geom_bar(position="fill",stat="identity")+
>   geom_text(aes(label=nDet),position=position_stack(vjust=0.5))+
>   theme(axis.text.x=element_text(angle=90,hjust=1))+
>   scale_y_continous(labels=scales::percent_format())
>
> Example of data being graphed:
>
> speciesSci         recvDeployName    nDet
> 1 Arenaria interpres Bucktoe Preserve    96
> 2 Arenaria interpres CHDE               132
> 3 Arenaria interpres Fortescue        22133
> 4 Arenaria interpres Mispillion        2031
> 5 Arenaria interpres Norbury           3709
> 6 Arenaria interpres Penn - DRL          49
>
> What my graph looks like when I use the command example provided above:
> graph <https://i.stack.imgur.com/TLLGh.png>
>
> Any help would be greatly appreciated. Thank you.
>
> *Joshua N. Barth*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Nov 17 00:11:38 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 17 Nov 2019 12:11:38 +1300
Subject: [R] Can't get facet_grid_paginate() from ggforce to work.
Message-ID: <acb44048-512f-5fd7-322a-fcb75d729d9d@auckland.ac.nz>


Clearly there's something that I'm not understanding, but 
facet_grid_paginate() seems to be ignoring the "ncol" argument.

Here's a reprex:

library(ggforce)
X <- dget("testData.txt")
ncols <- length(levels(X$LifeStage))
npages <- length(levels(X$degC))
plotObj <- vector("list",npages)
for(page in 1:npages) {
plotObj[[page]] <- ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets=Species~LifeStage:degC,
                         page=page,ncol=ncols)+
     ylab("Success fraction") +
     theme_bw()
}

The data set "testData.txt" is attached.

You will see if you "print" plotObj[[1]] that the plot has 8 columns, 
not 4 (4 being the value of "ncols").

The first and second pages seem to be identical; facet_grid_paginate() 
has put everything on one page, and then repeated that page.

I must be doing something utterly stupid, but I can't see what it is.

The example given in the help for ggforce, using the diamonds data, 
looks to me just like my example, except that it works and my example 
doesn't! What am I missing?

What I'm after is something like the plot in the attached pdf file
test.pdf (which I managed to produce using lattice).

Can anyone point me in the right direction?

Thanks.

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: testData.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20191117/8f04e41f/attachment.txt>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: test.pdf
Type: application/pdf
Size: 12321 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20191117/8f04e41f/attachment.pdf>

From r@turner @end|ng |rom @uck|@nd@@c@nz  Sun Nov 17 06:54:49 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Sun, 17 Nov 2019 18:54:49 +1300
Subject: [R] Supplying names to vars() in ggplot2.
Message-ID: <ecffee2d-88bf-245b-a59c-347fcfb342bf@auckland.ac.nz>


I need to call ggplot() from another function with the names of the 
faceting variables supplied as arguments to the calling function.  These 
names (which are names of columns in the relevant data frame)
are given as character arguments, say "rowName" and "colName".
Suppose that rowName is equal to "clyde" and colName is equal to "irving".

I'd like to do something like

... + facet_grid(row=vars(rowName), col=vars(colName)) + ...

but this does not work.  For instance, vars(rowName) gives

> [[1]]
> <quosure>
> expr: ^rowName
> env:  global

I'd like to get the same thing as if I said vars(clyde) which gives

> <list_of<quosure>>
> 
> [[1]]
> <quosure>
> expr: ^clyde
> env:  global

There *must* be some magic arcane incantation that I can apply to 
rowName (and colName) to get what I want.  Mustn't there?

I tried things like vars(as.name(rowName)) --- nope, no help at all.

Can anyone help me out?

cheers,

Rolf Turner

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From |mr@nd@mk@r222 @end|ng |rom gm@||@com  Sun Nov 17 07:55:32 2019
From: |mr@nd@mk@r222 @end|ng |rom gm@||@com (imran damkar)
Date: Sun, 17 Nov 2019 12:25:32 +0530
Subject: [R] (no subject)
Message-ID: <CA+ysrC6BvB3wyxaRVmzqwK46z7fZJOWn8zzzP3S5x8bjvWCtkg@mail.gmail.com>

I want to check if boot strapping can prove helpful in improving
homogeneity of variance. If it does so, I would like to perform hypothesis
testing on same transformed data( bootstrapped data),by using
non-parametric anova . How can I store new bootstrapped data set in data
frame? Data is 5 point  *likert* based. Respondents are from three
different organizations. Sample size of each group is *unequal*.  Please
help me how it can be done in R.

Thanks.

	[[alternative HTML version deleted]]


From kr|@t|@g|over @end|ng |rom hotm@||@com  Sun Nov 17 10:48:03 2019
From: kr|@t|@g|over @end|ng |rom hotm@||@com (Kristi Glover)
Date: Sun, 17 Nov 2019 09:48:03 +0000
Subject: [R] How to save google map in r ?
Message-ID: <MWHPR0201MB344904E9AAFBF7D424C3719EFA720@MWHPR0201MB3449.namprd02.prod.outlook.com>

Hi R users,
I am struggling to save the map with georeferenced (TIFF) that was imported from google map. I would be very grateful with your input. How can I save this image and reuse it when I need? I used the following code
library(ggmap)
library(mapproj)
map <- get_map(location = 'Europe', zoom = 4)
ggmap(map)
writeRaster(map, filename = "texting.tif")

but got the following message

"Error in (function (classes, fdef, mtable)  :
  unable to find an inherited method for function ?writeRaster? for signature ?"gg", "character"?"

thanks


	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Nov 17 14:07:12 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 17 Nov 2019 13:07:12 +0000
Subject: [R] Can't get facet_grid_paginate() from ggforce to work.
In-Reply-To: <acb44048-512f-5fd7-322a-fcb75d729d9d@auckland.ac.nz>
References: <acb44048-512f-5fd7-322a-fcb75d729d9d@auckland.ac.nz>
Message-ID: <a61774b0-8a91-1e59-0b36-acafe0e2ec63@sapo.pt>

Hello,

I don't know why but use an explicit nrow = nrows with 'nrows' defined as

nrows <- length(levels(X$Species))

It seems to work (remove the device pdf() code if it's more annoying 
than useful):


nrows <- length(levels(X$Species))
ncols <- length(levels(X$LifeStage))
npages <- length(levels(X$degC))

plotObj <- vector("list", npages)

pdf(file = 'Rhelp_test.pdf')
for(page in 1:npages) {nrows <- length(levels(X$Species))
ncols <- length(levels(X$LifeStage))
npages <- length(levels(X$degC))

plotObj <- vector("list", npages)

pdf(file = 'Rhelp_test.pdf')
for(page in 1:npages) {
   plotObj[[page]] <- ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets = Species ~ LifeStage:degC,
                         page = page,
                         nrow = nrows, ncol = ncols)+
     ylab("Success fraction") +
     theme_bw()
   print(ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets = Species ~ LifeStage:degC,
                         page = page,
                         nrow = nrows, ncol = ncols)+
     ylab("Success fraction") +
     theme_bw()
   )
}
dev.off()

print(plotObj[[1]])
print(plotObj[[2]])

   plotObj[[page]] <- ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets = Species ~ LifeStage:degC,
                         page = page,
                         nrow = nrows, ncol = ncols)+
     ylab("Success fraction") +
     theme_bw()
   print(ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets = Species ~ LifeStage:degC,
                         page = page,
                         nrow = nrows, ncol = ncols)+
     ylab("Success fraction") +
     theme_bw()
   )
}
dev.off()

print(plotObj[[1]])
print(plotObj[[2]])


Hope this helps,

Rui Barradas

?s 23:11 de 16/11/19, Rolf Turner escreveu:
> 
> Clearly there's something that I'm not understanding, but 
> facet_grid_paginate() seems to be ignoring the "ncol" argument.
> 
> Here's a reprex:
> 
> library(ggforce)
> X <- dget("testData.txt")
> ncols <- length(levels(X$LifeStage))
> npages <- length(levels(X$degC))
> plotObj <- vector("list",npages)
> for(page in 1:npages) {
> plotObj[[page]] <- ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets=Species~LifeStage:degC,
>  ??????????????????????? page=page,ncol=ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
> }
> 
> The data set "testData.txt" is attached.
> 
> You will see if you "print" plotObj[[1]] that the plot has 8 columns, 
> not 4 (4 being the value of "ncols").
> 
> The first and second pages seem to be identical; facet_grid_paginate() 
> has put everything on one page, and then repeated that page.
> 
> I must be doing something utterly stupid, but I can't see what it is.
> 
> The example given in the help for ggforce, using the diamonds data, 
> looks to me just like my example, except that it works and my example 
> doesn't! What am I missing?
> 
> What I'm after is something like the plot in the attached pdf file
> test.pdf (which I managed to produce using lattice).
> 
> Can anyone point me in the right direction?
> 
> Thanks.
> 
> cheers,
> 
> Rolf Turner
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Nov 17 14:12:29 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 17 Nov 2019 13:12:29 +0000
Subject: [R] Can't get facet_grid_paginate() from ggforce to work.
In-Reply-To: <a61774b0-8a91-1e59-0b36-acafe0e2ec63@sapo.pt>
References: <acb44048-512f-5fd7-322a-fcb75d729d9d@auckland.ac.nz>
 <a61774b0-8a91-1e59-0b36-acafe0e2ec63@sapo.pt>
Message-ID: <adfec765-cead-4b4f-a075-3aba7971b2ac@sapo.pt>

Hello,

Sorry, copy&paste did a poor job. The right code now.


library(ggforce)

X <- dget("testData.txt")

nrows <- length(levels(X$Species))
ncols <- length(levels(X$LifeStage))
npages <- length(levels(X$degC))

plotObj <- vector("list", npages)

pdf(file = 'Rhelp_test.pdf')
for(page in 1:npages) {
   plotObj[[page]] <- ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets = Species ~ LifeStage:degC,
                         page = page,
                         nrow = nrows, ncol = ncols)+
     ylab("Success fraction") +
     theme_bw()
   print(ggplot(X) +
     geom_point(aes(y = Sfrac , x = x)) +
     facet_grid_paginate(facets = Species ~ LifeStage:degC,
                         page = page,
                         nrow = nrows, ncol = ncols)+
     ylab("Success fraction") +
     theme_bw()
   )
}
dev.off()

print(plotObj[[1]])
print(plotObj[[2]])


Hope this helps,

Rui Barradas

?s 13:07 de 17/11/19, Rui Barradas escreveu:
> Hello,
> 
> I don't know why but use an explicit nrow = nrows with 'nrows' defined as
> 
> nrows <- length(levels(X$Species))
> 
> It seems to work (remove the device pdf() code if it's more annoying 
> than useful):
> 
> 
> nrows <- length(levels(X$Species))
> ncols <- length(levels(X$LifeStage))
> npages <- length(levels(X$degC))
> 
> plotObj <- vector("list", npages)
> 
> pdf(file = 'Rhelp_test.pdf')
> for(page in 1:npages) {nrows <- length(levels(X$Species))
> ncols <- length(levels(X$LifeStage))
> npages <- length(levels(X$degC))
> 
> plotObj <- vector("list", npages)
> 
> pdf(file = 'Rhelp_test.pdf')
> for(page in 1:npages) {
>  ? plotObj[[page]] <- ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>  ??????????????????????? page = page,
>  ??????????????????????? nrow = nrows, ncol = ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
>  ? print(ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>  ??????????????????????? page = page,
>  ??????????????????????? nrow = nrows, ncol = ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
>  ? )
> }
> dev.off()
> 
> print(plotObj[[1]])
> print(plotObj[[2]])
> 
>  ? plotObj[[page]] <- ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>  ??????????????????????? page = page,
>  ??????????????????????? nrow = nrows, ncol = ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
>  ? print(ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>  ??????????????????????? page = page,
>  ??????????????????????? nrow = nrows, ncol = ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
>  ? )
> }
> dev.off()
> 
> print(plotObj[[1]])
> print(plotObj[[2]])
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 23:11 de 16/11/19, Rolf Turner escreveu:
>>
>> Clearly there's something that I'm not understanding, but 
>> facet_grid_paginate() seems to be ignoring the "ncol" argument.
>>
>> Here's a reprex:
>>
>> library(ggforce)
>> X <- dget("testData.txt")
>> ncols <- length(levels(X$LifeStage))
>> npages <- length(levels(X$degC))
>> plotObj <- vector("list",npages)
>> for(page in 1:npages) {
>> plotObj[[page]] <- ggplot(X) +
>> ???? geom_point(aes(y = Sfrac , x = x)) +
>> ???? facet_grid_paginate(facets=Species~LifeStage:degC,
>> ???????????????????????? page=page,ncol=ncols)+
>> ???? ylab("Success fraction") +
>> ???? theme_bw()
>> }
>>
>> The data set "testData.txt" is attached.
>>
>> You will see if you "print" plotObj[[1]] that the plot has 8 columns, 
>> not 4 (4 being the value of "ncols").
>>
>> The first and second pages seem to be identical; facet_grid_paginate() 
>> has put everything on one page, and then repeated that page.
>>
>> I must be doing something utterly stupid, but I can't see what it is.
>>
>> The example given in the help for ggforce, using the diamonds data, 
>> looks to me just like my example, except that it works and my example 
>> doesn't! What am I missing?
>>
>> What I'm after is something like the plot in the attached pdf file
>> test.pdf (which I managed to produce using lattice).
>>
>> Can anyone point me in the right direction?
>>
>> Thanks.
>>
>> cheers,
>>
>> Rolf Turner
>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h@w|ckh@m @end|ng |rom gm@||@com  Sun Nov 17 14:28:30 2019
From: h@w|ckh@m @end|ng |rom gm@||@com (Hadley Wickham)
Date: Sun, 17 Nov 2019 07:28:30 -0600
Subject: [R] Supplying names to vars() in ggplot2.
In-Reply-To: <ecffee2d-88bf-245b-a59c-347fcfb342bf@auckland.ac.nz>
References: <ecffee2d-88bf-245b-a59c-347fcfb342bf@auckland.ac.nz>
Message-ID: <CABdHhvHjGECFpGMO2K_ggKRXf14UH2WSv+T-Nxt0BPjGGCLTUg@mail.gmail.com>

See this new vignette in dev ggplot2:
https://ggplot2.tidyverse.org/dev/articles/ggplot2-in-packages.html

Hadley

On Saturday, November 16, 2019, Rolf Turner <r.turner at auckland.ac.nz> wrote:

>
> I need to call ggplot() from another function with the names of the
> faceting variables supplied as arguments to the calling function.  These
> names (which are names of columns in the relevant data frame)
> are given as character arguments, say "rowName" and "colName".
> Suppose that rowName is equal to "clyde" and colName is equal to "irving".
>
> I'd like to do something like
>
> ... + facet_grid(row=vars(rowName), col=vars(colName)) + ...
>
> but this does not work.  For instance, vars(rowName) gives
>
> [[1]]
>> <quosure>
>> expr: ^rowName
>> env:  global
>>
>
> I'd like to get the same thing as if I said vars(clyde) which gives
>
> <list_of<quosure>>
>>
>> [[1]]
>> <quosure>
>> expr: ^clyde
>> env:  global
>>
>
> There *must* be some magic arcane incantation that I can apply to rowName
> (and colName) to get what I want.  Mustn't there?
>
> I tried things like vars(as.name(rowName)) --- nope, no help at all.
>
> Can anyone help me out?
>
> cheers,
>
> Rolf Turner
>
> --
> Honorary Research Fellow
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posti
> ng-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://hadley.nz

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Sun Nov 17 19:12:16 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 17 Nov 2019 10:12:16 -0800
Subject: [R] How to save google map in r ?
In-Reply-To: <MWHPR0201MB344904E9AAFBF7D424C3719EFA720@MWHPR0201MB3449.namprd02.prod.outlook.com>
References: <MWHPR0201MB344904E9AAFBF7D424C3719EFA720@MWHPR0201MB3449.namprd02.prod.outlook.com>
Message-ID: <CAGxFJbRrG3huv4FT=L0uHTdwRqpddiJx8g0BBF8D71kBRgyXiA@mail.gmail.com>

Please use search facilities before posting such questions here.

I searched on "How to save google map in R" at rseek.org and got many
relevant hits.

If you have *already* done this, you should tell us why what you got did
not suffice.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Nov 17, 2019 at 1:48 AM Kristi Glover <kristi.glover at hotmail.com>
wrote:

> Hi R users,
> I am struggling to save the map with georeferenced (TIFF) that was
> imported from google map. I would be very grateful with your input. How can
> I save this image and reuse it when I need? I used the following code
> library(ggmap)
> library(mapproj)
> map <- get_map(location = 'Europe', zoom = 4)
> ggmap(map)
> writeRaster(map, filename = "texting.tif")
>
> but got the following message
>
> "Error in (function (classes, fdef, mtable)  :
>   unable to find an inherited method for function ?writeRaster? for
> signature ?"gg", "character"?"
>
> thanks
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sun Nov 17 19:56:45 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sun, 17 Nov 2019 18:56:45 +0000
Subject: [R] How to save google map in r ?
In-Reply-To: <MWHPR0201MB344904E9AAFBF7D424C3719EFA720@MWHPR0201MB3449.namprd02.prod.outlook.com>
References: <MWHPR0201MB344904E9AAFBF7D424C3719EFA720@MWHPR0201MB3449.namprd02.prod.outlook.com>
Message-ID: <9ab66327-6d10-023d-563a-7ffb73752567@sapo.pt>

Hello,

See the docs: get_amp returns a ggmap object and ggmap returns a ggplot 
object. So ggsave() should do what you want.


Hope this helps,

Rui Barradas

?s 09:48 de 17/11/19, Kristi Glover escreveu:
> Hi R users,
> I am struggling to save the map with georeferenced (TIFF) that was imported from google map. I would be very grateful with your input. How can I save this image and reuse it when I need? I used the following code
> library(ggmap)
> library(mapproj)
> map <- get_map(location = 'Europe', zoom = 4)
> ggmap(map)
> writeRaster(map, filename = "texting.tif")
> 
> but got the following message
> 
> "Error in (function (classes, fdef, mtable)  :
>    unable to find an inherited method for function ?writeRaster? for signature ?"gg", "character"?"
> 
> thanks
> 
> 
> 	[[alternative HTML version deleted]]
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From m@ho@p|erre @end|ng |rom gm@||@com  Sun Nov 17 17:50:21 2019
From: m@ho@p|erre @end|ng |rom gm@||@com (Pierre Maho)
Date: Sun, 17 Nov 2019 17:50:21 +0100
Subject: [R] Glmnet R - can't modify fdev parameter when lower = 0
Message-ID: <CAAZqq8DvYVRnYqRYUgeyWCaHMTthT7r=PxtppMMugxV44mnzYQ@mail.gmail.com>

Hi,

I want to solve the following optimisation problem:

[image: \hat{\beta} = \arg \min_{\beta \geq 0} \| y-A\beta \|_2^2 + \lambda
\|\beta\|_1]

For that, I am using glmnet package (cv.glmnet for finding ? and
lower.limits = 0 to impose non-negativity).

I would like to modify the fdev parameter (minimum fractional change in
deviance for stopping path) in glmnet function. This modification seems
impossible when lower.limits=0 (non-negative coefficients) is specified.

Here is a minimal working example

# MWE
# Generate data
P = 100 # number of sensors
R = 10 # number of sources
beta = runif(R,0,1) %>% matrix
beta[1:7] = 0 # optimal solution is sparse
A = replicate(R,runif(P,0,1))
y = A %*% beta

# set all control parameters of glmnet to factory parameters (fdev = 1e-5)
glmnet.control(factory = T)

# Now set the stopping criterion for the lambda path (fdev) to a
bigger value, say 1e-1
glmnet.control(fdev = 1e-1)

# Without any constraint
cvfit = glmnet::cv.glmnet(A, y, type.measure = "mse", nfolds =
10,intercept=T,nlambda=100,parallel = F)
cvfit_fdev = diff(cvfit$glmnet.fit$dev.ratio)/cvfit$glmnet.fit$dev.ratio[-1]
print(cvfit_fdev) # fdev = 0.1 is respected

# With non-negativity constraint
cvfit = glmnet::cv.glmnet(A, y, type.measure = "mse", nfolds =
10,intercept=T,lower.limits=0,nlambda=100,parallel = F)
cvfit_fdev = diff(cvfit$glmnet.fit$dev.ratio)/cvfit$glmnet.fit$dev.ratio[-1]
print(cvfit_fdev) # fdev = 0.1 is not respected

I would like to know if it is a known bug (I couldn't find it on Google) or
if I am simply doing something wrong.

Thanks a lot

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Sun Nov 17 21:26:34 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Sun, 17 Nov 2019 12:26:34 -0800
Subject: [R] Adaptive resampling in r
In-Reply-To: <CA+nrPnu5BB5PYbpggaEz8rK8kbxyd9w_UKw93FW1ZAZd742rBw@mail.gmail.com>
References: <CA+nrPnu5BB5PYbpggaEz8rK8kbxyd9w_UKw93FW1ZAZd742rBw@mail.gmail.com>
Message-ID: <3c4866d2-0350-8968-5513-5e902507f250@comcast.net>

This doesn't appear to be a question about R coding. Please review the 
Posting Guide.

-- 

David.


On 11/15/19 2:59 PM, Neha gupta wrote:
> How the hyperparameter settings via adaptive resampling is different from
> the one obtained from grid search or random search? Both grid and random
> searches provide best parameters values and the same is achieved using
> adaptive resampling.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Nov 18 00:56:24 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 18 Nov 2019 12:56:24 +1300
Subject: [R] Can't get facet_grid_paginate() from ggforce to work.
In-Reply-To: <adfec765-cead-4b4f-a075-3aba7971b2ac@sapo.pt>
References: <acb44048-512f-5fd7-322a-fcb75d729d9d@auckland.ac.nz>
 <a61774b0-8a91-1e59-0b36-acafe0e2ec63@sapo.pt>
 <adfec765-cead-4b4f-a075-3aba7971b2ac@sapo.pt>
Message-ID: <40d6fdb1-eb87-78bb-873e-18fb4aca08bf@auckland.ac.nz>


Yep.  That worked.  Thanks very much Rui.

cheers,

Rolf

On 18/11/19 2:12 AM, Rui Barradas wrote:

> Hello,
> 
> Sorry, copy&paste did a poor job. The right code now.
> 
> 
> library(ggforce)
> 
> X <- dget("testData.txt")
> 
> nrows <- length(levels(X$Species))
> ncols <- length(levels(X$LifeStage))
> npages <- length(levels(X$degC))
> 
> plotObj <- vector("list", npages)
> 
> pdf(file = 'Rhelp_test.pdf')
> for(page in 1:npages) {
>  ? plotObj[[page]] <- ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>  ??????????????????????? page = page,
>  ??????????????????????? nrow = nrows, ncol = ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
>  ? print(ggplot(X) +
>  ??? geom_point(aes(y = Sfrac , x = x)) +
>  ??? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>  ??????????????????????? page = page,
>  ??????????????????????? nrow = nrows, ncol = ncols)+
>  ??? ylab("Success fraction") +
>  ??? theme_bw()
>  ? )
> }
> dev.off()
> 
> print(plotObj[[1]])
> print(plotObj[[2]])
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 13:07 de 17/11/19, Rui Barradas escreveu:
>> Hello,
>>
>> I don't know why but use an explicit nrow = nrows with 'nrows' defined as
>>
>> nrows <- length(levels(X$Species))
>>
>> It seems to work (remove the device pdf() code if it's more annoying 
>> than useful):
>>
>>
>> nrows <- length(levels(X$Species))
>> ncols <- length(levels(X$LifeStage))
>> npages <- length(levels(X$degC))
>>
>> plotObj <- vector("list", npages)
>>
>> pdf(file = 'Rhelp_test.pdf')
>> for(page in 1:npages) {nrows <- length(levels(X$Species))
>> ncols <- length(levels(X$LifeStage))
>> npages <- length(levels(X$degC))
>>
>> plotObj <- vector("list", npages)
>>
>> pdf(file = 'Rhelp_test.pdf')
>> for(page in 1:npages) {
>> ?? plotObj[[page]] <- ggplot(X) +
>> ???? geom_point(aes(y = Sfrac , x = x)) +
>> ???? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>> ???????????????????????? page = page,
>> ???????????????????????? nrow = nrows, ncol = ncols)+
>> ???? ylab("Success fraction") +
>> ???? theme_bw()
>> ?? print(ggplot(X) +
>> ???? geom_point(aes(y = Sfrac , x = x)) +
>> ???? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>> ???????????????????????? page = page,
>> ???????????????????????? nrow = nrows, ncol = ncols)+
>> ???? ylab("Success fraction") +
>> ???? theme_bw()
>> ?? )
>> }
>> dev.off()
>>
>> print(plotObj[[1]])
>> print(plotObj[[2]])
>>
>> ?? plotObj[[page]] <- ggplot(X) +
>> ???? geom_point(aes(y = Sfrac , x = x)) +
>> ???? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>> ???????????????????????? page = page,
>> ???????????????????????? nrow = nrows, ncol = ncols)+
>> ???? ylab("Success fraction") +
>> ???? theme_bw()
>> ?? print(ggplot(X) +
>> ???? geom_point(aes(y = Sfrac , x = x)) +
>> ???? facet_grid_paginate(facets = Species ~ LifeStage:degC,
>> ???????????????????????? page = page,
>> ???????????????????????? nrow = nrows, ncol = ncols)+
>> ???? ylab("Success fraction") +
>> ???? theme_bw()
>> ?? )
>> }
>> dev.off()
>>
>> print(plotObj[[1]])
>> print(plotObj[[2]])
>>
>>
>> Hope this helps,
>>
>> Rui Barradas
>>
>> ?s 23:11 de 16/11/19, Rolf Turner escreveu:
>>>
>>> Clearly there's something that I'm not understanding, but 
>>> facet_grid_paginate() seems to be ignoring the "ncol" argument.
>>>
>>> Here's a reprex:
>>>
>>> library(ggforce)
>>> X <- dget("testData.txt")
>>> ncols <- length(levels(X$LifeStage))
>>> npages <- length(levels(X$degC))
>>> plotObj <- vector("list",npages)
>>> for(page in 1:npages) {
>>> plotObj[[page]] <- ggplot(X) +
>>> ???? geom_point(aes(y = Sfrac , x = x)) +
>>> ???? facet_grid_paginate(facets=Species~LifeStage:degC,
>>> ???????????????????????? page=page,ncol=ncols)+
>>> ???? ylab("Success fraction") +
>>> ???? theme_bw()
>>> }
>>>
>>> The data set "testData.txt" is attached.
>>>
>>> You will see if you "print" plotObj[[1]] that the plot has 8 columns, 
>>> not 4 (4 being the value of "ncols").
>>>
>>> The first and second pages seem to be identical; 
>>> facet_grid_paginate() has put everything on one page, and then 
>>> repeated that page.
>>>
>>> I must be doing something utterly stupid, but I can't see what it is.
>>>
>>> The example given in the help for ggforce, using the diamonds data, 
>>> looks to me just like my example, except that it works and my example 
>>> doesn't! What am I missing?
>>>
>>> What I'm after is something like the plot in the attached pdf file
>>> test.pdf (which I managed to produce using lattice).
>>>
>>> Can anyone point me in the right direction?
>>>
>>> Thanks.
>>>
>>> cheers,
>>>
>>> Rolf Turner


From r@turner @end|ng |rom @uck|@nd@@c@nz  Mon Nov 18 01:03:26 2019
From: r@turner @end|ng |rom @uck|@nd@@c@nz (Rolf Turner)
Date: Mon, 18 Nov 2019 13:03:26 +1300
Subject: [R] Supplying names to vars() in ggplot2.
In-Reply-To: <CABdHhvHjGECFpGMO2K_ggKRXf14UH2WSv+T-Nxt0BPjGGCLTUg@mail.gmail.com>
References: <ecffee2d-88bf-245b-a59c-347fcfb342bf@auckland.ac.nz>
 <CABdHhvHjGECFpGMO2K_ggKRXf14UH2WSv+T-Nxt0BPjGGCLTUg@mail.gmail.com>
Message-ID: <7885c755-dc2c-e7cc-fc5d-c80dc5bd6484@auckland.ac.nz>


On 18/11/19 2:28 AM, Hadley Wickham wrote:

> See this new vignette in dev ggplot2: 
> https://ggplot2.tidyverse.org/dev/articles/ggplot2-in-packages.html
> 
> Hadley

Yes!  Thank you.  Bottom line:  for what I want to do the syntax is

facet_grid(row=vars(.data[[rowName]]),col=vars(.data[[colName]]))

Thanks again.

cheers,

Rolf

> On Saturday, November 16, 2019, Rolf Turner <r.turner at auckland.ac.nz 
> <mailto:r.turner at auckland.ac.nz>> wrote:
> 
> 
>     I need to call ggplot() from another function with the names of the
>     faceting variables supplied as arguments to the calling function. 
>     These names (which are names of columns in the relevant data frame)
>     are given as character arguments, say "rowName" and "colName".
>     Suppose that rowName is equal to "clyde" and colName is equal to
>     "irving".
> 
>     I'd like to do something like
> 
>     ... + facet_grid(row=vars(rowName), col=vars(colName)) + ...
> 
>     but this does not work.? For instance, vars(rowName) gives
> 
>         [[1]]
>         <quosure>
>         expr: ^rowName
>         env:? global
> 
> 
>     I'd like to get the same thing as if I said vars(clyde) which gives
> 
>         <list_of<quosure>>
> 
>         [[1]]
>         <quosure>
>         expr: ^clyde
>         env:? global
> 
> 
>     There *must* be some magic arcane incantation that I can apply to
>     rowName (and colName) to get what I want.? Mustn't there?
> 
>     I tried things like vars(as.name <http://as.name>(rowName)) ---
>     nope, no help at all.
> 
>     Can anyone help me out?

-- 
Honorary Research Fellow
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From |@th@nhnh@n @end|ng |rom gm@||@com  Mon Nov 18 03:33:49 2019
From: |@th@nhnh@n @end|ng |rom gm@||@com (Nhan La)
Date: Mon, 18 Nov 2019 13:33:49 +1100
Subject: [R] How to import and create time series data frames in an
 efficient way?
In-Reply-To: <CAGxFJbSh4H-T3c41-0wQ4KrGxjw80U4Q-ZQdmOdYBOH6bM=-FQ@mail.gmail.com>
References: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
 <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>
 <CAB_bKVs54TwuZ6u7mFO=fEceLnWa9go_cJaw5eH_mZ28afW+fQ@mail.gmail.com>
 <CAGxFJbQwcnQHXWsKmZkHon+4cFymGnxBAnsdZUUy_yojLns+uw@mail.gmail.com>
 <CAGxFJbSh4H-T3c41-0wQ4KrGxjw80U4Q-ZQdmOdYBOH6bM=-FQ@mail.gmail.com>
Message-ID: <CAB_bKVt0G=edwUAyVHN9+_qQUUi0vSRvdv17Xz7FPYA8L21tzQ@mail.gmail.com>

Thanks Bert. I also managed to get this work

files = list.files(pattern="*.txt")
df = ldply(files, read_csv,col_names=c("ticker","date","open","high",
"low", "close", "volume"))
Cheers,
Nathan

On Fri, Nov 15, 2019 at 3:45 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Ha! -- A bug! "Corrected" version inline below:
> Bert Gunter
>
> On Thu, Nov 14, 2019 at 8:10 PM Bert Gunter <bgunter.4567 at gmail.com>
> wrote:
>
>> Brute force approach, possibly inefficient:
>>
>> 1. You have a vector of file names. Sort them in the appropriate (time)
>> order. These names are also the component names of all the data frames in
>> your list that you read in, call it yourlist.
>>
>> 2. Create a vector of all the unique ticker names, perhaps by creating a
>> vector of all the names and then unique() -ing it. Call this vector snames
>> with n.names in it. It will probably have length several hundred at least I
>> assume.
>>
>> 3. Suppose the  6 columns of data of each data frame that you want are
>> named cnames = c("stocknames","Open", "High", "Low", "Close", "Volume").
>>
>> 4. You could proceed as you suggested, but it would likely be more
>> efficient, since all data that you want are numeric, to create a 3D array
>> of NA's via:
>>
>> yourdat <- array(dim = c(n.dates, n.names, 5), dimnames = list(NULL,
>> snames, cnames[-1]))
>>
>> 5. Then just loop  through your list of files and use indexing to fill in
>> the columns x category slices for each date. Stocks that are missing will
>> be NA automatically. e.g. (warning: UNTESTED):
>>
>> For date "d", let df be the data frame from date "d" in your list, i.e.
>>
>> df <- yourlist[["d"]][, cnames]
>> ## Note The order of the listed stocks in the "stocknames" column can be
>> different from frame to frame of your master list.
>>
>> Then fill in the flat for the dth date (i.e. dth row) in your array by:
>>
>> ## corrected line here:
>
>> yourdat[ ,df[ ,"stocknames"], cnames[-1] <- as.matrix(df[ ,-1]) ## need
>> to omit the column names so it converts to numeric matrix
>>
> ## need to get the names of the stocks in the "stocknames" column in the
> order they appear in df.
>
>>
>> This should fill in  the values of the 2nd and 3rd dimensions of the
>> array for all the stocks on the dth date with the data for each stock in
>> the data frame matched to the appropriate column in the array.
>>
>> The entire loop will give all dates for all stocks and all categories
>> with NA's for missing days. (*IF IT WORKS!*)
>> You may need to modify this sightly if, for example, your stock names are
>> row names rather than a field in your data frame. I leave such adjustments
>> to you.
>>
>> Note again that this is fairly elementary with just arrays and indexing.
>> Basic tutorials should tell you about all of this. Also, when plotting,
>> you'll have to convert your dates to suitable date-time format.
>>
>> Cheers,
>> Bert
>>
>>
>>
>>
>> On Thu, Nov 14, 2019 at 4:55 PM Nhan La <lathanhnhan at gmail.com> wrote:
>>
>>> Hi Bert,
>>>
>>> I've attempted to find the answer and actually been able to import the
>>> individual data sets into a list of data frames.
>>>
>>> But I'm not sure how to go ahead with the next step. I'm not necessarily
>>> asking for a final answer. Perhaps if you (I mean others as well) would
>>> like a constructive coaching, you would suggest a few key words to look at?
>>>
>>> Sorry for the HTML thing, this is my first post. I'll do better next
>>> times.
>>>
>>> Thanks,
>>> Nathan
>>>
>>>
>>>
>>> On Fri, Nov 15, 2019 at 11:34 AM Bert Gunter <bgunter.4567 at gmail.com>
>>> wrote:
>>>
>>>> So you've made no attempt at all to do this for yourself?!
>>>>
>>>> That suggests to me that you need to spend time with some R tutorials.
>>>>
>>>> Also, please post in plain text on this plain text list. HTML can get
>>>> mangled, as it may have here.
>>>>
>>>> -- Bert
>>>> "The trouble with having an open mind is that people keep coming along
>>>> and sticking things into it."
>>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>>
>>>>
>>>> On Thu, Nov 14, 2019 at 4:11 PM Nhan La <lathanhnhan at gmail.com> wrote:
>>>>
>>>>> I have many separate data files in csv format for a lot of daily stock
>>>>> prices. Over a few years there are hundreds of those data files, whose
>>>>> names are the dates of data record.
>>>>>
>>>>> In each file there are variables of ticker (or stock trading code),
>>>>> date,
>>>>> open price, high price, low price, close price, and trading volume. For
>>>>> example, inside a data file named 20150128.txt it looks like this:
>>>>>
>>>>> FB,20150128,1.075,1.075,0.97,0.97,725221
>>>>> AAPL,20150128,2.24,2.24,2.2,2.24,63682
>>>>> AMZN,20150128,0.4,0.415,0.4,0.415,194900
>>>>> NFLX,20150128,50.19,50.21,50.19,50.19,761845
>>>>> GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and many
>>>>> more..................
>>>>>
>>>>> In case it's relevant, the number of stocks in these files are not
>>>>> necessarily the same (so there will be missing data). I need to import
>>>>> and
>>>>> create 5 separate time series data frames from those files, one each
>>>>> for
>>>>> Open, High, Low, Close and Volume. In each data frame, rows are
>>>>> indexed by
>>>>> date, and columns by ticker. For example, the data frame Open may look
>>>>> like
>>>>> this:
>>>>>
>>>>> DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
>>>>> 20150129,NA,2.3,0.5,5.2,1.7,... ...
>>>>>
>>>>> What will be an efficient way to do that? I've used the following
>>>>> codes to
>>>>> read the files into a list of data frames but don't know what to do
>>>>> next
>>>>> from here.
>>>>>
>>>>> files = list.files(pattern="*.txt") mydata = lapply(files,
>>>>> read.csv,head=FALSE)
>>>>>
>>>>> Thanks,
>>>>>
>>>>> Nathan
>>>>>
>>>>> Disclaimer: In case it's relevant, this question is also posted on
>>>>> stackoverflow.
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Mon Nov 18 07:50:20 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Mon, 18 Nov 2019 07:50:20 +0100
Subject: [R] How to import and create time series data frames in an
 efficient way?
In-Reply-To: <CAB_bKVt0G=edwUAyVHN9+_qQUUi0vSRvdv17Xz7FPYA8L21tzQ@mail.gmail.com>
References: <CAB_bKVvbYEHm8_SmnKJpHGC4GULnrzWRZN_repkuU9D-n926uw@mail.gmail.com>
 <CAGxFJbQ6OSDfVOtuGRW6BR0oED9vkQBY6HGm7BXj5+m9+TDsfQ@mail.gmail.com>
 <CAB_bKVs54TwuZ6u7mFO=fEceLnWa9go_cJaw5eH_mZ28afW+fQ@mail.gmail.com>
 <CAGxFJbQwcnQHXWsKmZkHon+4cFymGnxBAnsdZUUy_yojLns+uw@mail.gmail.com>
 <CAGxFJbSh4H-T3c41-0wQ4KrGxjw80U4Q-ZQdmOdYBOH6bM=-FQ@mail.gmail.com>
 <CAB_bKVt0G=edwUAyVHN9+_qQUUi0vSRvdv17Xz7FPYA8L21tzQ@mail.gmail.com>
Message-ID: <78579122-F9E3-46DF-BCB7-FDEA092D94CB@dcn.davis.ca.us>

The pattern argument is supposed to be a regular expression, not a file globbing expression, so "." matches anything, and "*" is not supposed to be the first character of the search string (read ?regex).

Also, I think you forgot to indicate where the ldply function came from.. the plyr package.. and the read_csv function comes from the readr package which is part of the tidyverse. The plyr package doesn't play well with the tidyverse packages, so if you are going there then it is better to use the tidyr unnest function... something like:

library(dplyr)
library(tidyr)
library(readr)

dta <- (   tibble( fname = list.files( pattern="\\.txt$" )
       %>% mutate( data = lapply( fnames, read_csv, col_names=c( "ticker", "date", "open", "high", "low", "close", "volume" ) )
       %>% unnest( cols = data )
       )

On November 18, 2019 3:33:49 AM GMT+01:00, Nhan La <lathanhnhan at gmail.com> wrote:
>Thanks Bert. I also managed to get this work
>
>files = list.files(pattern="*.txt")
>df = ldply(files, read_csv,col_names=c("ticker","date","open","high",
>"low", "close", "volume"))
>Cheers,
>Nathan
>
>On Fri, Nov 15, 2019 at 3:45 PM Bert Gunter <bgunter.4567 at gmail.com>
>wrote:
>
>> Ha! -- A bug! "Corrected" version inline below:
>> Bert Gunter
>>
>> On Thu, Nov 14, 2019 at 8:10 PM Bert Gunter <bgunter.4567 at gmail.com>
>> wrote:
>>
>>> Brute force approach, possibly inefficient:
>>>
>>> 1. You have a vector of file names. Sort them in the appropriate
>(time)
>>> order. These names are also the component names of all the data
>frames in
>>> your list that you read in, call it yourlist.
>>>
>>> 2. Create a vector of all the unique ticker names, perhaps by
>creating a
>>> vector of all the names and then unique() -ing it. Call this vector
>snames
>>> with n.names in it. It will probably have length several hundred at
>least I
>>> assume.
>>>
>>> 3. Suppose the  6 columns of data of each data frame that you want
>are
>>> named cnames = c("stocknames","Open", "High", "Low", "Close",
>"Volume").
>>>
>>> 4. You could proceed as you suggested, but it would likely be more
>>> efficient, since all data that you want are numeric, to create a 3D
>array
>>> of NA's via:
>>>
>>> yourdat <- array(dim = c(n.dates, n.names, 5), dimnames = list(NULL,
>>> snames, cnames[-1]))
>>>
>>> 5. Then just loop  through your list of files and use indexing to
>fill in
>>> the columns x category slices for each date. Stocks that are missing
>will
>>> be NA automatically. e.g. (warning: UNTESTED):
>>>
>>> For date "d", let df be the data frame from date "d" in your list,
>i.e.
>>>
>>> df <- yourlist[["d"]][, cnames]
>>> ## Note The order of the listed stocks in the "stocknames" column
>can be
>>> different from frame to frame of your master list.
>>>
>>> Then fill in the flat for the dth date (i.e. dth row) in your array
>by:
>>>
>>> ## corrected line here:
>>
>>> yourdat[ ,df[ ,"stocknames"], cnames[-1] <- as.matrix(df[ ,-1]) ##
>need
>>> to omit the column names so it converts to numeric matrix
>>>
>> ## need to get the names of the stocks in the "stocknames" column in
>the
>> order they appear in df.
>>
>>>
>>> This should fill in  the values of the 2nd and 3rd dimensions of the
>>> array for all the stocks on the dth date with the data for each
>stock in
>>> the data frame matched to the appropriate column in the array.
>>>
>>> The entire loop will give all dates for all stocks and all
>categories
>>> with NA's for missing days. (*IF IT WORKS!*)
>>> You may need to modify this sightly if, for example, your stock
>names are
>>> row names rather than a field in your data frame. I leave such
>adjustments
>>> to you.
>>>
>>> Note again that this is fairly elementary with just arrays and
>indexing.
>>> Basic tutorials should tell you about all of this. Also, when
>plotting,
>>> you'll have to convert your dates to suitable date-time format.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>>
>>>
>>> On Thu, Nov 14, 2019 at 4:55 PM Nhan La <lathanhnhan at gmail.com>
>wrote:
>>>
>>>> Hi Bert,
>>>>
>>>> I've attempted to find the answer and actually been able to import
>the
>>>> individual data sets into a list of data frames.
>>>>
>>>> But I'm not sure how to go ahead with the next step. I'm not
>necessarily
>>>> asking for a final answer. Perhaps if you (I mean others as well)
>would
>>>> like a constructive coaching, you would suggest a few key words to
>look at?
>>>>
>>>> Sorry for the HTML thing, this is my first post. I'll do better
>next
>>>> times.
>>>>
>>>> Thanks,
>>>> Nathan
>>>>
>>>>
>>>>
>>>> On Fri, Nov 15, 2019 at 11:34 AM Bert Gunter
><bgunter.4567 at gmail.com>
>>>> wrote:
>>>>
>>>>> So you've made no attempt at all to do this for yourself?!
>>>>>
>>>>> That suggests to me that you need to spend time with some R
>tutorials.
>>>>>
>>>>> Also, please post in plain text on this plain text list. HTML can
>get
>>>>> mangled, as it may have here.
>>>>>
>>>>> -- Bert
>>>>> "The trouble with having an open mind is that people keep coming
>along
>>>>> and sticking things into it."
>>>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>>>
>>>>>
>>>>> On Thu, Nov 14, 2019 at 4:11 PM Nhan La <lathanhnhan at gmail.com>
>wrote:
>>>>>
>>>>>> I have many separate data files in csv format for a lot of daily
>stock
>>>>>> prices. Over a few years there are hundreds of those data files,
>whose
>>>>>> names are the dates of data record.
>>>>>>
>>>>>> In each file there are variables of ticker (or stock trading
>code),
>>>>>> date,
>>>>>> open price, high price, low price, close price, and trading
>volume. For
>>>>>> example, inside a data file named 20150128.txt it looks like
>this:
>>>>>>
>>>>>> FB,20150128,1.075,1.075,0.97,0.97,725221
>>>>>> AAPL,20150128,2.24,2.24,2.2,2.24,63682
>>>>>> AMZN,20150128,0.4,0.415,0.4,0.415,194900
>>>>>> NFLX,20150128,50.19,50.21,50.19,50.19,761845
>>>>>> GOOGL,20150128,1.62,1.645,1.59,1.63,684835 ...................and
>many
>>>>>> more..................
>>>>>>
>>>>>> In case it's relevant, the number of stocks in these files are
>not
>>>>>> necessarily the same (so there will be missing data). I need to
>import
>>>>>> and
>>>>>> create 5 separate time series data frames from those files, one
>each
>>>>>> for
>>>>>> Open, High, Low, Close and Volume. In each data frame, rows are
>>>>>> indexed by
>>>>>> date, and columns by ticker. For example, the data frame Open may
>look
>>>>>> like
>>>>>> this:
>>>>>>
>>>>>> DATE,FB,AAPL,AMZN,NFLX,GOOGL,... 20150128,1.5,2.2,0.4,5.1,1.6,...
>>>>>> 20150129,NA,2.3,0.5,5.2,1.7,... ...
>>>>>>
>>>>>> What will be an efficient way to do that? I've used the following
>>>>>> codes to
>>>>>> read the files into a list of data frames but don't know what to
>do
>>>>>> next
>>>>>> from here.
>>>>>>
>>>>>> files = list.files(pattern="*.txt") mydata = lapply(files,
>>>>>> read.csv,head=FALSE)
>>>>>>
>>>>>> Thanks,
>>>>>>
>>>>>> Nathan
>>>>>>
>>>>>> Disclaimer: In case it's relevant, this question is also posted
>on
>>>>>> stackoverflow.
>>>>>>
>>>>>>         [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible
>code.
>>>>>>
>>>>>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Mon Nov 18 08:21:41 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Mon, 18 Nov 2019 07:21:41 +0000
Subject: [R] 
 Labeling Stacked Bar Plots Representing Percentages with Count Data
In-Reply-To: <CAGi70XWSXKBsfiXY8pLYeW8224YW_F7-+hnfwAQmNy9NrmhTqw@mail.gmail.com>
References: <CAGi70XUZc09hGzC4QGTNrywabDJ5ppRQh3Ea1r3BsX-wDsOPHw@mail.gmail.com>
 <e732932f-d7eb-6505-1efe-bf8bdc93117c@sapo.pt>
 <CAGi70XWSXKBsfiXY8pLYeW8224YW_F7-+hnfwAQmNy9NrmhTqw@mail.gmail.com>
Message-ID: <0ccc6e28-334d-baaf-ae65-7df7022ee8af@sapo.pt>

Hello,

It's dificult to tell without data. Can you post the output of

dput(head(tagSummary, 20))  # or 30

?
(If it's private data, something mimicking its structure.)

Rui Barradas

?s 02:06 de 18/11/19, Josh B escreveu:
> Hello Rui,
> 
> I worked through your suggestion and appear to be getting an error?? Not 
> quite sure what the error is?? Maybe a conflict of two packages I have 
> loaded (dplyr and plyr)? Anyhow, I used code you suggested and received 
> an error in return:
> 
> sumplot<-tagSummary %>%
>  ? group_by(recvDeployName, speciesSci) %>%
>  ? summarize(count = n()) %>%
>  ? ggplot(aes(factor(recvDeployName), y = count, fill = 
> factor(speciesSci))) +
>  ? geom_bar(position = "fill", stat = "identity") +
>  ? geom_text(aes(label = count),
>  ??????????? position = position_fill(vjust=0.5)) +
>  ? theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
>  ? scale_y_discrete(labels = scales::percent_format())
> 
> Error: n() should only be called in a data context Call 
> `rlang::last_error()` to see a backtrace.> rlang::last_error() 
> <error/rlang_error>n() should only be called in a data context 
> Backtrace:1. dplyr::group_by(., recvDeployName, speciesSci) 8. 
> plyr::summarize(., count = n()) 9. [ base::eval(...) ]with 1 more 
> call11. dplyr::n() 12. dplyr:::from_context("..group_size") 13. 
> `%||%`(...) Call `rlang::last_trace()` to see the full backtrace.
> 
> **//___^
> *I did try this on my own and it seems promising... *
> sumplot<-ggplot(tagSummary,aes(x=recvDeployName,y=nDet,fill=speciesSci))+
>  ? geom_text(aes(label=nDet),position="fill", stat="identity")+
>  ? theme(axis.text.x=element_text(angle=90,hjust=1))+
>  ? scale_y_continuous(labels=scales::percent_format())
> 
> The code I tried on my own above gives me stacked data labels 
> representing the number of times a species was detected.? The stacked 
> data labels also match the previous stacked barplot I made without the 
> labels too, which is also promising.? However, the code above does not 
> make the stack bar graph show the percentages?? Again, I want the color 
> bars depicting the percentage while the count data is labeled within 
> each representative stack.
> 
> 
> 
> Rplot02.jpeg
> 
> 
> This was the graph I had before trying to add data labels... I want the 
> data labels above to be represented within the graph below.? Thank you 
> for your time and help.
> Rplot03.jpeg
> 
> On Sat, Nov 16, 2019 at 11:17 AM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
>     Hello,
> 
>     In geom_text change to position = position_fill(vjust=0.5).
>     What's important is to have position = "fill" in geom_bar match
>     geom_text.
>     Something like :
> 
> 
>     library(dplyr)
>     library(ggplot2)
> 
>     data(mtcars)
> 
>     mtcars %>%
>      ? ?group_by(cyl, gear) %>%
>      ? ?summarise(count = n()) %>%
>      ? ?ggplot(aes(factor(cyl), y = count, fill = factor(gear))) +
>      ? ?geom_bar(position = "fill", stat = "identity") +
>      ? ?geom_text(aes(label = count),
>      ? ? ? ? ? ? ?position = position_fill(vjust=0.5)) +
>      ? ?theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
>      ? ?scale_y_discrete(labels = scales::percent_format())
> 
> 
>     Hope this helps,
> 
>     Rui Barradas
> 
>     ?s 23:56 de 15/11/19, Josh B escreveu:
>      > Hello,
>      >
>      > I am trying to include the count labels on stacked bar plots which
>      > represent percentages. I want to show x-amount of individuals
>     make up the
>      > graphed percentages. However, when I include the count labels my
>     y-axis
>      > gets blown out of proportion because it changes to match the
>     count data,
>      > not the percentages. Also, the bars are removed from the graph
>     too? I have
>      > reviewed other posts similar to this, such as: "How to add
>     percentage or
>      > count labels above percentage bar plot?". I cannot find the error
>     in my r
>      > command.
>      >
>      > My command used is as follows:
>      >
>      >
>     sumplot<-ggplot(tagSummary,aes(x=recvDeployName,y=nDet,fill=speciesSci))+
>      >? ? geom_bar(position="fill",stat="identity")+
>      >? ? geom_text(aes(label=nDet),position=position_stack(vjust=0.5))+
>      >? ? theme(axis.text.x=element_text(angle=90,hjust=1))+
>      >? ? scale_y_continous(labels=scales::percent_format())
>      >
>      > Example of data being graphed:
>      >
>      > speciesSci? ? ? ? ?recvDeployName? ? nDet
>      > 1 Arenaria interpres Bucktoe Preserve? ? 96
>      > 2 Arenaria interpres CHDE? ? ? ? ? ? ? ?132
>      > 3 Arenaria interpres Fortescue? ? ? ? 22133
>      > 4 Arenaria interpres Mispillion? ? ? ? 2031
>      > 5 Arenaria interpres Norbury? ? ? ? ? ?3709
>      > 6 Arenaria interpres Penn - DRL? ? ? ? ? 49
>      >
>      > What my graph looks like when I use the command example provided
>     above:
>      > graph <https://i.stack.imgur.com/TLLGh.png>
>      >
>      > Any help would be greatly appreciated. Thank you.
>      >
>      > *Joshua N. Barth*
>      >
>      >? ? ? ?[[alternative HTML version deleted]]
>      >
>      > ______________________________________________
>      > R-help at r-project.org <mailto:R-help at r-project.org> mailing list
>     -- To UNSUBSCRIBE and more, see
>      > https://stat.ethz.ch/mailman/listinfo/r-help
>      > PLEASE do read the posting guide
>     http://www.R-project.org/posting-guide.html
>      > and provide commented, minimal, self-contained, reproducible code.
>      >
>


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Nov 18 14:44:19 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 18 Nov 2019 08:44:19 -0500
Subject: [R] Problems with lambda argument
Message-ID: <CAMOcQfOtkfR1Tp-3SGzj2m1prc+cOA_fq6gAeWvizCF42NHrdg@mail.gmail.com>

Dear friends,

Hope you are doing great. When setting the lambda = "auto" in the
auto.arima function, I get an error message saying non-numeric argument to
binary operator.

I have performed several tests changing parameters and going through the
other lines of code and as soon as I set lambda = "auto" the error pops up,
otherwise the model runs perfectly fine.

Any guidance will be greatly appreciated,

Best regards,

Paul

	[[alternative HTML version deleted]]


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Nov 18 14:49:25 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 18 Nov 2019 08:49:25 -0500
Subject: [R] lambad and biasadj parameters in auto.arima and forecast
 functions
Message-ID: <CAMOcQfNoZoCmSR-fOMJGhWTxk+ZvB0dYsgRgOa4uyZ01FWnVZA@mail.gmail.com>

Dear friends,

Hope you are doing great. I noticed that both the auto.arima and the
forecast function share parameters, like for example lambda and biasadj. If
I set this parameters = TRUE in function auto.arima, do I also have to
specify them and set them = TRUE in the forecast function?

Best regards,

Paul

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Nov 18 15:26:49 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 18 Nov 2019 09:26:49 -0500
Subject: [R] Problems with lambda argument
In-Reply-To: <CAMOcQfOtkfR1Tp-3SGzj2m1prc+cOA_fq6gAeWvizCF42NHrdg@mail.gmail.com>
References: <CAMOcQfOtkfR1Tp-3SGzj2m1prc+cOA_fq6gAeWvizCF42NHrdg@mail.gmail.com>
Message-ID: <7a4dd0ba-9c85-cded-368c-86cd11b4a47d@gmail.com>

On 18/11/2019 8:44 a.m., Paul Bernal wrote:
> Dear friends,
> 
> Hope you are doing great. When setting the lambda = "auto" in the
> auto.arima function, I get an error message saying non-numeric argument to
> binary operator.
> 
> I have performed several tests changing parameters and going through the
> other lines of code and as soon as I set lambda = "auto" the error pops up,
> otherwise the model runs perfectly fine.
> 
> Any guidance will be greatly appreciated,

If I run the example(auto.arima) code from the forecast package but add 
lambda="auto", I don't get any error, so I think you'll have to post a 
reproducible example if you want help.

Duncan Murdoch


From p@u|bern@|07 @end|ng |rom gm@||@com  Mon Nov 18 15:30:37 2019
From: p@u|bern@|07 @end|ng |rom gm@||@com (Paul Bernal)
Date: Mon, 18 Nov 2019 09:30:37 -0500
Subject: [R] Problems with lambda argument
In-Reply-To: <7a4dd0ba-9c85-cded-368c-86cd11b4a47d@gmail.com>
References: <CAMOcQfOtkfR1Tp-3SGzj2m1prc+cOA_fq6gAeWvizCF42NHrdg@mail.gmail.com>
 <7a4dd0ba-9c85-cded-368c-86cd11b4a47d@gmail.com>
Message-ID: <CAMOcQfNPc9R_cCUFHr502605Tu2i_36fTuXTOD186LCv9LHnVA@mail.gmail.com>

Dear friend Duncan,

Thank you for your kind reply, I appreciate the fact that you always try to
help. I got the error fixed, to fix the problem, instead of setting
lambda="auto", I did lambda=BoxCox.lambda(tsobject), not sure if this is
equivalent to setting lambda="auto", but I guess is a workaround at the
very least.

Cheers,

Paul

El lun., 18 nov. 2019 a las 9:26, Duncan Murdoch (<murdoch.duncan at gmail.com>)
escribi?:

> On 18/11/2019 8:44 a.m., Paul Bernal wrote:
> > Dear friends,
> >
> > Hope you are doing great. When setting the lambda = "auto" in the
> > auto.arima function, I get an error message saying non-numeric argument
> to
> > binary operator.
> >
> > I have performed several tests changing parameters and going through the
> > other lines of code and as soon as I set lambda = "auto" the error pops
> up,
> > otherwise the model runs perfectly fine.
> >
> > Any guidance will be greatly appreciated,
>
> If I run the example(auto.arima) code from the forecast package but add
> lambda="auto", I don't get any error, so I think you'll have to post a
> reproducible example if you want help.
>
> Duncan Murdoch
>

	[[alternative HTML version deleted]]


From bjoern@||@@e|er @end|ng |rom goog|em@||@com  Mon Nov 18 16:11:44 2019
From: bjoern@||@@e|er @end|ng |rom goog|em@||@com (=?UTF-8?Q?Bj=c3=b6rn_Fisseler?=)
Date: Mon, 18 Nov 2019 16:11:44 +0100
Subject: [R] Problem comparing two strings
Message-ID: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>

Hello,

I'm struggling comparing two strings, which come from different data 
sets. This strings are identical: "Alexander J?ger"

But when I compare these strings: string1 == string2
the result is FALSE.

Looking at the raw bytes used to encode the strings, the results are 
different:

string1: 41 6c 65 78 61 6e 64 65 72 20 4a c3 a4 67 65 72
string2: 41 6c 65 78 61 6e 64 65 72 20 4a 61 cc 88 67 65 72

string2 comes from the file names of different files on my machine 
(macOS), string1 comes from a data file (csv, UTF8 encoding).

It's obviously the umlaut "?" in this example which is encoded with two 
respectively three bytes. The question is how to change this? This 
problem makes it impossible to join the two data sets based on the 
names. I already checked the settings on my machine: Sys.getlocale() 
returns "de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8". 
Changing/forcing the encoding of the data didn't bring the results I 
expected.

What else can I try?

Best regards

 ??? ??? Bj?rn


	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Nov 18 16:34:34 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Mon, 18 Nov 2019 18:34:34 +0300
Subject: [R] Problem comparing two strings
In-Reply-To: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
References: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
Message-ID: <20191118183434.41cb46b6@trisector>

On Mon, 18 Nov 2019 16:11:44 +0100
"Bj?rn Fisseler" <bjoern.fisseler at googlemail.com> wrote:

> It's obviously the umlaut "?" in this example which is encoded with
> two respectively three bytes. The question is how to change this?

Welcome to the wonderful world of Unicode-related problems! It is,
indeed, possible to represent the same glyph using either one
code-point (LATIN SMALL LETTER A WITH DIAERESIS) or two code points
(LATIN SMALL LETTER A followed by COMBINING DIAERESIS). (Other
combinations of code points resulting in the same glyph are probably
also possible.)

What you are looking for is called "Unicode normalization" and it is
implemented in the stringi package, in functions stri_trans_nfc
(normalization: there are multiple normal forms to choose from but W3C
guidelines recommend NFC) and stri_compare / stri_cmp (test for
canonical equivalence).

See also: ?stringi::stri_cmp and https://stackoverflow.com/a/20684794

-- 
Best regards,
Ivan


From bjoern@||@@e|er @end|ng |rom goog|em@||@com  Mon Nov 18 16:39:06 2019
From: bjoern@||@@e|er @end|ng |rom goog|em@||@com (=?UTF-8?Q?Bj=c3=b6rn_Fisseler?=)
Date: Mon, 18 Nov 2019 16:39:06 +0100
Subject: [R] Problem comparing two strings
In-Reply-To: <20191118183434.41cb46b6@trisector>
References: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
 <20191118183434.41cb46b6@trisector>
Message-ID: <d7e96d44-590d-fe43-45b9-57c0fe234ed3@gmail.com>

Thank you! That solved my problem!

Best

 ??? ??? Bj?rn

Am 18.11.19 um 16:34 schrieb Ivan Krylov:
> On Mon, 18 Nov 2019 16:11:44 +0100
> "Bj?rn Fisseler" <bjoern.fisseler at googlemail.com> wrote:
>
>> It's obviously the umlaut "?" in this example which is encoded with
>> two respectively three bytes. The question is how to change this?
> Welcome to the wonderful world of Unicode-related problems! It is,
> indeed, possible to represent the same glyph using either one
> code-point (LATIN SMALL LETTER A WITH DIAERESIS) or two code points
> (LATIN SMALL LETTER A followed by COMBINING DIAERESIS). (Other
> combinations of code points resulting in the same glyph are probably
> also possible.)
>
> What you are looking for is called "Unicode normalization" and it is
> implemented in the stringi package, in functions stri_trans_nfc
> (normalization: there are multiple normal forms to choose from but W3C
> guidelines recommend NFC) and stri_compare / stri_cmp (test for
> canonical equivalence).
>
> See also: ?stringi::stri_cmp and https://stackoverflow.com/a/20684794
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Nov 18 16:45:55 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 18 Nov 2019 10:45:55 -0500
Subject: [R] Problem comparing two strings
In-Reply-To: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
References: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
Message-ID: <8e22a517-e9cb-36a5-fd53-2139fa7b4b6c@gmail.com>

On 18/11/2019 10:11 a.m., Bj?rn Fisseler wrote:
> Hello,
> 
> I'm struggling comparing two strings, which come from different data
> sets. This strings are identical: "Alexander J?ger"
> 
> But when I compare these strings: string1 == string2
> the result is FALSE.
> 
> Looking at the raw bytes used to encode the strings, the results are
> different:
> 
> string1: 41 6c 65 78 61 6e 64 65 72 20 4a c3 a4 67 65 72
> string2: 41 6c 65 78 61 6e 64 65 72 20 4a 61 cc 88 67 65 72
> 
> string2 comes from the file names of different files on my machine
> (macOS), string1 comes from a data file (csv, UTF8 encoding).
> 
> It's obviously the umlaut "?" in this example which is encoded with two
> respectively three bytes. The question is how to change this? This
> problem makes it impossible to join the two data sets based on the
> names. I already checked the settings on my machine: Sys.getlocale()
> returns "de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8".
> Changing/forcing the encoding of the data didn't bring the results I
> expected.
> 
> What else can I try?

Characters like ? have two (or more) representations in Unicode:  a 
single code point, or the code point for "a" followed by a code point 
that says "add an umlaut".

If you want to compare strings, you need a consistent representation. 
This is called normalizing the string.

There are several possible normalizations; for your purposes it doesn't 
matter which one you use, as long as you use the same normalization for 
both strings.  See 
<https://withblue.ink/2019/03/11/why-you-need-to-normalize-unicode-strings.html> 
for details.

In R, there are several functions that do the normalization for you. 
Two are utf8::utf8_normalize or stringi::stri_trans_nfc.  So you'd want 
something like

   library(utf8)
   string1 <- utf8_normalize(string1)
   string2 <- utf8_normalize(string2)
   string1 == string2  # Should now work as expected

Duncan Murdoch


From pd@|gd @end|ng |rom gm@||@com  Mon Nov 18 16:48:04 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 18 Nov 2019 16:48:04 +0100
Subject: [R] Problem comparing two strings
In-Reply-To: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
References: <31e2072a-0fee-038a-8747-f2adc416f4d5@gmail.com>
Message-ID: <F818CA34-A1CE-499A-BAA6-97723B363D36@gmail.com>

A version of this came up not long ago in a slightly different context (bug 17369: parse() doesn't honor unicode in NFD normalization). 

The basic issue is that there are different unicode normalizations (look it up...).

Briefly, accented characters exist in two forms, one as a single code point and another as the base letter followed by the accent. 

I.e. there is the single letter "?" and then "a\u308" which is a followed by "combining diaeresis" which effectively put a ? on top of the preceding character.

The utf8 package has code for normalizing strings.

-pd

> On 18 Nov 2019, at 16:11 , Bj?rn Fisseler <bjoern.fisseler at googlemail.com> wrote:
> 
> Hello,
> 
> I'm struggling comparing two strings, which come from different data 
> sets. This strings are identical: "Alexander J?ger"
> 
> But when I compare these strings: string1 == string2
> the result is FALSE.
> 
> Looking at the raw bytes used to encode the strings, the results are 
> different:
> 
> string1: 41 6c 65 78 61 6e 64 65 72 20 4a c3 a4 67 65 72
> string2: 41 6c 65 78 61 6e 64 65 72 20 4a 61 cc 88 67 65 72
> 
> string2 comes from the file names of different files on my machine 
> (macOS), string1 comes from a data file (csv, UTF8 encoding).
> 
> It's obviously the umlaut "?" in this example which is encoded with two 
> respectively three bytes. The question is how to change this? This 
> problem makes it impossible to join the two data sets based on the 
> names. I already checked the settings on my machine: Sys.getlocale() 
> returns "de_DE.UTF-8/de_DE.UTF-8/de_DE.UTF-8/C/de_DE.UTF-8/de_DE.UTF-8". 
> Changing/forcing the encoding of the data didn't bring the results I 
> expected.
> 
> What else can I try?
> 
> Best regards
> 
>         Bj?rn
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From |ordpreet@m @end|ng |rom gm@||@com  Mon Nov 18 20:58:48 2019
From: |ordpreet@m @end|ng |rom gm@||@com (Preetam Pal)
Date: Tue, 19 Nov 2019 01:28:48 +0530
Subject: [R] Cross validation multivariate kernel regression
Message-ID: <CAHVFrXEK_G7mOE4TjQBmo=Y5Tc+R77-MSBqQiqDy0AcMAa9t+g@mail.gmail.com>

Hi,

This question is general- I have a data set of n observations, consisting
of a single response variable y and p regressor variables.( n ~50, p~3 or
4).
I am planning to implement Nadaraya-Watson regression model, with
bandwidths optimized via cross-validation.
For cross-validation, I will need to choose 10 outsample/test data sets of
a given size ( =n/10 ) for each choice of the bandwidth vector, and then
choose the optimum bandwidth vector (in terms of MSE or any reasonable loss
function-we can take it to be MSE,  as example).

The difficulty is I can't find any code to do this under:
A) multiple regressors (p>1) AND
B) I'll get to choose to the outsample datasets.

Thanks for any help/insight you can provide.
Regards,
Preetam

	[[alternative HTML version deleted]]


From biii m@iii@g oii de@@ey@ws  Tue Nov 19 03:50:48 2019
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Mon, 18 Nov 2019 21:50:48 -0500
Subject: [R] Why does nlme::getVarCov not Work for nlme Models?
In-Reply-To: a001afb3e622b1364580aed9d4bc36d5@mail.gmail.com
References: a001afb3e622b1364580aed9d4bc36d5@mail.gmail.com
Message-ID: <037201d59e84$2692bc40$73b834c0$@denney.ws>

Hi,

 

I was just trying to summarize an nlme model using `broom.mixed::tidy()`,
and it doesn't give random effects or residual variability for nlme models.
As I looked deeper, this appears to be related to the fact that
`nlme::getVarCorr()` doesn't support nlme models for no reason that I can
discern [1].  I do see that it simply has a `stop()` call if the model is
nlme, but I don't know or see a commented reason why this would be.

 

I can imagine that in a nonlinear model the interpretation of variance is
different than in a linear model (namely that it is more likely to be
asymmetric and this result is asymptotic), but that doesn't seem like a
reason to prevent the use of the tool.  When I pretended that my model was
an lme model (set the class temporarily to "lme"), it gave the expected
result.

 

My questions are:

 

1.	Why does nlme::getVarCorr() not support nlme models?
2.	If there is not a known reason, does someone in R-core think that a
patch removing the stop below [1] would be accepted?

 

[1] https://svn.r-project.org/R-packages/trunk/nlme/R/VarCov.R the lines
are:

    if(any("nlme" == class(obj)))

        stop("not implemented for \"nlme\" objects")

 

Thanks,

 

Bill


	[[alternative HTML version deleted]]


From j@vedbtk111 @end|ng |rom gm@||@com  Tue Nov 19 12:21:53 2019
From: j@vedbtk111 @end|ng |rom gm@||@com (javed khan)
Date: Tue, 19 Nov 2019 12:21:53 +0100
Subject: [R] Shift from rmse to auc in objective function
Message-ID: <CAJhui+syFp9pcAVPqoGt4n7y5gwvfv6JBo0+yw0dKQ8jgrJGpA@mail.gmail.com>

Hi
I am using genetic algorithm to tune svm parameters and reduce the rmse
value. I used return - svm_model$results$rmse

Now if I have to increase the accuracy, do I just need to remove the
negative sign and write as

return svm_model$results$Accuracy

Thanks

	[[alternative HTML version deleted]]


From m@ho@p|erre @end|ng |rom gm@||@com  Tue Nov 19 12:39:48 2019
From: m@ho@p|erre @end|ng |rom gm@||@com (Pierre Maho)
Date: Tue, 19 Nov 2019 12:39:48 +0100
Subject: [R] Fractional deviance for lambda in cv.glmnet for non-negative
 Lasso
Message-ID: <CAAZqq8AOH2urFVFpFjnCK0AdcGLEVinfF=-eOUQsSoxSJaCXaA@mail.gmail.com>

Hi,

I want to solve the following optimisation problem:

[image: \hat{\beta} = \arg \min_{\beta \geq 0} \| y-A\beta \|_2^2 + \lambda
\|\beta\|_1]

For that, I am using glmnet package (cv.glmnet for finding ? and
lower.limits = 0 to impose non-negativity).

I would like to modify the fdev parameter (minimum fractional change in
deviance for stopping path) in glmnet function. This modification seems
impossible when lower.limits=0 (non-negative coefficients) is specified.

Here is a minimal working example

# MWE
# Generate data
P = 100 # number of sensors
R = 10 # number of sources
beta = runif(R,0,1) %>% matrix
beta[1:7] = 0 # optimal solution is sparse
A = replicate(R,runif(P,0,1))
y = A %*% beta

# set all control parameters of glmnet to factory parameters (fdev = 1e-5)
glmnet.control(factory = T)

# Now set the stopping criterion for the lambda path (fdev) to a
bigger value, say 1e-1
glmnet.control(fdev = 1e-1)

# Without any constraint
cvfit = glmnet::cv.glmnet(A, y, type.measure = "mse", nfolds =
10,intercept=T,nlambda=100,parallel = F)
cvfit_fdev = diff(cvfit$glmnet.fit$dev.ratio)/cvfit$glmnet.fit$dev.ratio[-1]
print(cvfit_fdev) # fdev = 0.1 is respected

# With non-negativity constraint
cvfit = glmnet::cv.glmnet(A, y, type.measure = "mse", nfolds =
10,intercept=T,lower.limits=0,nlambda=100,parallel = F)
cvfit_fdev = diff(cvfit$glmnet.fit$dev.ratio)/cvfit$glmnet.fit$dev.ratio[-1]
print(cvfit_fdev) # fdev = 0.1 is not respected

I would like to know if it is a known bug (I couldn't find it on Google) or
if I am simply doing something wrong.

Thanks a lot

	[[alternative HTML version deleted]]


From m@|||P@dpo@t @end|ng |rom gm@||@com  Tue Nov 19 14:46:36 2019
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Tue, 19 Nov 2019 16:46:36 +0300
Subject: [R] pairs.panels ()
Message-ID: <CAH6117J97OuKPThvYqYRvaYYigADK-fM=FKZv7LJykWMhFv0_w@mail.gmail.com>

Point me, please, the parameter for changing the font size for the
variable name in pairs.panels ()

From @@r@h@go@|ee @end|ng |rom gm@||@com  Tue Nov 19 15:59:36 2019
From: @@r@h@go@|ee @end|ng |rom gm@||@com (Sarah Goslee)
Date: Tue, 19 Nov 2019 09:59:36 -0500
Subject: [R] pairs.panels ()
In-Reply-To: <CAH6117J97OuKPThvYqYRvaYYigADK-fM=FKZv7LJykWMhFv0_w@mail.gmail.com>
References: <CAH6117J97OuKPThvYqYRvaYYigADK-fM=FKZv7LJykWMhFv0_w@mail.gmail.com>
Message-ID: <CAM_vjumD0Pij3m0NVfKENySYLP_BfYfL2XzaLo7oWcmQ+=DUMA@mail.gmail.com>

Hi, nameless querent,

It's important to tell us where you got your function from. In this
case, I'm assuming that you are using pairs.panels() from the psych
package.

Looking at the help for that function, I see, in the arguments section:

? other options for pairs

Which means that if you don't see a configuration option you want
directly, you should look at the help for pairs().

I admit, that if you don't already know that cex controls font size,
the help for pairs that says,

cex.labels, font.labels: graphics parameters for the text panel.

might not enlighten you, but if I understand your question, cex.labels
is what you're looking for.

?par gives (possibly) more insight into controlling base graphics.

Sarah

On Tue, Nov 19, 2019 at 8:47 AM Medic <mailiPadpost at gmail.com> wrote:
>
> Point me, please, the parameter for changing the font size for the
> variable name in pairs.panels ()
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Sarah Goslee (she/her)
http://www.numberwright.com


From che|@e@rh|nton @end|ng |rom gm@||@com  Tue Nov 19 14:50:49 2019
From: che|@e@rh|nton @end|ng |rom gm@||@com (Chelsea Hinton)
Date: Tue, 19 Nov 2019 08:50:49 -0500
Subject: [R] Separating Output of Modularity Analyses
Message-ID: <CAA3E8s3oeCiqR+vpYmJVjiyGBYdVeGDriDfAGuzXp_u_HYPiow@mail.gmail.com>

Hello,

I am working to evaluate the structure of shrub-pollinator networks using
modularity analyses. I have generated 100 null model networks to test for
modularity and extract c and z scores for each (measuring statistical
significance in real networks). However, the output of the "czvalues" for
all 100 null models comes out in one mass clump (See below code). The
highlighted section in the below code is an example of the null c and z
values. I have 17 plants in my shrub-pollinator network (represented by the
whole numbers) and so the first 17 values are the c scores followed by the
next 17 which are z scores and so on.

Is there a way I can easily separate out the c values from the z values? Or
should I instead generate 100 null networks individually?

R Code for null model generation and c and z output for nulls:

library(bipartite)

network<-read.csv(file.choose(), header = TRUE, row.names = 1)

nw<-sortweb(network)
weighted<-as.matrix(nw)

nulls <- nullmodel(weighted, N=100, method=3)
null.res <- unlist(sapply(nulls, metaComputeModules, USE.NAMES = TRUE)) #takes
a while ...
null.res
## [[1]]
## Slot "likelihood":
## [1] 0.1309568
##
##
## [[2]]
## Slot "likelihood":
## [1] 0.1303186
##
##
## [[3]]
## Slot "likelihood":
## [1] 0.14044
.............

## [[100]]
## Slot "likelihood":
## [1] 0.1332231

null.cz<-unlist(sapply(null.res, czvalues, level = "lower", USE.NAMES = TRUE
))
null.cz
##           1           2           3           4           5           6
##  0.64147140  0.74959438  0.62019013  0.65895062  0.73724490  0.72664360
##           7           8           9          10          11          12
##  0.73553719  0.57500000  0.60204082  0.66000000  0.72000000  0.37500000
##          13          14          15          16          17           1
##  0.37500000  0.77777778  0.27777778  0.44444444  0.00000000  0.70710678
##           2           3           4           5           6           7
##  0.70710678  1.10729957  1.68025904 -0.70710678 -0.12539247  0.99339927
##           8           9          10          11          12          13
##  0.62696233  0.72849280 -0.27007306 -0.86094603 -0.83722650 -0.72727630
##          14          15          16          17           1           2
## -0.86094603 -0.42633438 -0.70710678 -1.02821822  0.65504683  0.62195782
##           3           4           5           6           7           8
##  0.65606509  0.70226843  0.71777778  0.58500000  0.71500000  0.74048443
##           9          10          11          12          13          14
##  0.70000000  0.44444444  0.57142857  0.75000000  0.62500000  0.66666667
##          15          16          17           1           2           3
##  0.44444444  0.44444444  0.00000000  0.70710678  1.04053196          NA
##           4           5           6           7           8           9
##  0.70710678 -0.08671100  2.15643011  0.40204629  0.40204629 -0.47514562

Thank you for your time,

Chelsea

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Tue Nov 19 17:52:26 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 19 Nov 2019 08:52:26 -0800
Subject: [R] Why does nlme::getVarCov not Work for nlme Models?
In-Reply-To: <037201d59e84$2692bc40$73b834c0$@denney.ws>
References: <037201d59e84$2692bc40$73b834c0$@denney.ws>
Message-ID: <CAGxFJbQe6P7WCpvExLspd9NQD4SirAi65Nx1KUpG3AOArcx0Yg@mail.gmail.com>

1. The Help page for getVarCov says that it supports lme and gls models
only. So not nonlinear models. As to why it does not, that is off topic
here, as that is a statistical question.

2. As this is longstanding, it almost certainly has good reason for being
there, so I would seriously doubt it. But of course I don't speak for R
Core.

Cheers,
Bert

On Mon, Nov 18, 2019 at 9:43 PM <bill at denney.ws> wrote:

> Hi,
>
>
>
> I was just trying to summarize an nlme model using `broom.mixed::tidy()`,
> and it doesn't give random effects or residual variability for nlme models.
> As I looked deeper, this appears to be related to the fact that
> `nlme::getVarCorr()` doesn't support nlme models for no reason that I can
> discern [1].  I do see that it simply has a `stop()` call if the model is
> nlme, but I don't know or see a commented reason why this would be.
>
>
>
> I can imagine that in a nonlinear model the interpretation of variance is
> different than in a linear model (namely that it is more likely to be
> asymmetric and this result is asymptotic), but that doesn't seem like a
> reason to prevent the use of the tool.  When I pretended that my model was
> an lme model (set the class temporarily to "lme"), it gave the expected
> result.
>
>
>
> My questions are:
>
>
>
> 1.      Why does nlme::getVarCorr() not support nlme models?
> 2.      If there is not a known reason, does someone in R-core think that a
> patch removing the stop below [1] would be accepted?
>
>
>
> [1] https://svn.r-project.org/R-packages/trunk/nlme/R/VarCov.R the lines
> are:
>
>     if(any("nlme" == class(obj)))
>
>         stop("not implemented for \"nlme\" objects")
>
>
>
> Thanks,
>
>
>
> Bill
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From biii m@iii@g oii de@@ey@ws  Tue Nov 19 17:59:05 2019
From: biii m@iii@g oii de@@ey@ws (biii m@iii@g oii de@@ey@ws)
Date: Tue, 19 Nov 2019 11:59:05 -0500
Subject: [R] Why does nlme::getVarCov not Work for nlme Models?
In-Reply-To: <CAGxFJbQe6P7WCpvExLspd9NQD4SirAi65Nx1KUpG3AOArcx0Yg@mail.gmail.com>
References: <037201d59e84$2692bc40$73b834c0$@denney.ws>
 <CAGxFJbQe6P7WCpvExLspd9NQD4SirAi65Nx1KUpG3AOArcx0Yg@mail.gmail.com>
Message-ID: <044301d59efa$a7402870$f5c07950$@denney.ws>

Hi Bert,

 

If the statistical reason is off topic here, do you have a suggestion of a better forum?

 

Thanks,

 

Bill

 

From: Bert Gunter <bgunter.4567 at gmail.com> 
Sent: Tuesday, November 19, 2019 11:52 AM
To: bill at denney.ws
Cc: R-help <r-help at r-project.org>
Subject: Re: [R] Why does nlme::getVarCov not Work for nlme Models?

 

1. The Help page for getVarCov says that it supports lme and gls models only. So not nonlinear models. As to why it does not, that is off topic here, as that is a statistical question.

 

2. As this is longstanding, it almost certainly has good reason for being there, so I would seriously doubt it. But of course I don't speak for R Core. 

 

Cheers,

Bert

 

On Mon, Nov 18, 2019 at 9:43 PM <bill at denney.ws <mailto:bill at denney.ws> > wrote:

Hi,



I was just trying to summarize an nlme model using `broom.mixed::tidy()`,
and it doesn't give random effects or residual variability for nlme models.
As I looked deeper, this appears to be related to the fact that
`nlme::getVarCorr()` doesn't support nlme models for no reason that I can
discern [1].  I do see that it simply has a `stop()` call if the model is
nlme, but I don't know or see a commented reason why this would be.



I can imagine that in a nonlinear model the interpretation of variance is
different than in a linear model (namely that it is more likely to be
asymmetric and this result is asymptotic), but that doesn't seem like a
reason to prevent the use of the tool.  When I pretended that my model was
an lme model (set the class temporarily to "lme"), it gave the expected
result.



My questions are:



1.      Why does nlme::getVarCorr() not support nlme models?
2.      If there is not a known reason, does someone in R-core think that a
patch removing the stop below [1] would be accepted?



[1] https://svn.r-project.org/R-packages/trunk/nlme/R/VarCov.R the lines
are:

    if(any("nlme" == class(obj)))

        stop("not implemented for \"nlme\" objects")



Thanks,



Bill


        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Tue Nov 19 18:11:27 2019
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Tue, 19 Nov 2019 09:11:27 -0800
Subject: [R] Why does nlme::getVarCov not Work for nlme Models?
In-Reply-To: <044301d59efa$a7402870$f5c07950$@denney.ws>
References: <037201d59e84$2692bc40$73b834c0$@denney.ws>
 <CAGxFJbQe6P7WCpvExLspd9NQD4SirAi65Nx1KUpG3AOArcx0Yg@mail.gmail.com>
 <044301d59efa$a7402870$f5c07950$@denney.ws>
Message-ID: <c9a5e0b8-5f10-c72a-3a0f-9e13f0355206@comcast.net>


On 11/19/19 8:59 AM, bill at denney.ws wrote:
> Hi Bert,
>
>   
>
> If the statistical reason is off topic here, do you have a suggestion of a better forum?

There are two forums of the top of my head. The first should have been 
obvious if you had reviewed the mailing lists under the r-project.org 
domain, i.e. the mixed effects SIG list. The other would be 
CrossValidated.com


-- 

David.

>
>   
>
> Thanks,
>
>   
>
> Bill
>
>   
>
> From: Bert Gunter <bgunter.4567 at gmail.com>
> Sent: Tuesday, November 19, 2019 11:52 AM
> To: bill at denney.ws
> Cc: R-help <r-help at r-project.org>
> Subject: Re: [R] Why does nlme::getVarCov not Work for nlme Models?
>
>   
>
> 1. The Help page for getVarCov says that it supports lme and gls models only. So not nonlinear models. As to why it does not, that is off topic here, as that is a statistical question.
>
>   
>
> 2. As this is longstanding, it almost certainly has good reason for being there, so I would seriously doubt it. But of course I don't speak for R Core.
>
>   
>
> Cheers,
>
> Bert
>
>   
>
> On Mon, Nov 18, 2019 at 9:43 PM <bill at denney.ws <mailto:bill at denney.ws> > wrote:
>
> Hi,
>
>
>
> I was just trying to summarize an nlme model using `broom.mixed::tidy()`,
> and it doesn't give random effects or residual variability for nlme models.
> As I looked deeper, this appears to be related to the fact that
> `nlme::getVarCorr()` doesn't support nlme models for no reason that I can
> discern [1].  I do see that it simply has a `stop()` call if the model is
> nlme, but I don't know or see a commented reason why this would be.
>
>
>
> I can imagine that in a nonlinear model the interpretation of variance is
> different than in a linear model (namely that it is more likely to be
> asymmetric and this result is asymptotic), but that doesn't seem like a
> reason to prevent the use of the tool.  When I pretended that my model was
> an lme model (set the class temporarily to "lme"), it gave the expected
> result.
>
>
>
> My questions are:
>
>
>
> 1.      Why does nlme::getVarCorr() not support nlme models?
> 2.      If there is not a known reason, does someone in R-core think that a
> patch removing the stop below [1] would be accepted?
>
>
>
> [1] https://svn.r-project.org/R-packages/trunk/nlme/R/VarCov.R the lines
> are:
>
>      if(any("nlme" == class(obj)))
>
>          stop("not implemented for \"nlme\" objects")
>
>
>
> Thanks,
>
>
>
> Bill
>
>
>          [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org <mailto:R-help at r-project.org>  mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @purd|e@@ @end|ng |rom gm@||@com  Tue Nov 19 21:14:30 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Wed, 20 Nov 2019 09:14:30 +1300
Subject: [R] Cross validation multivariate kernel regression
In-Reply-To: <CAHVFrXEK_G7mOE4TjQBmo=Y5Tc+R77-MSBqQiqDy0AcMAa9t+g@mail.gmail.com>
References: <CAHVFrXEK_G7mOE4TjQBmo=Y5Tc+R77-MSBqQiqDy0AcMAa9t+g@mail.gmail.com>
Message-ID: <CAB8pepwXnPknqk8Ru7hWNEe=-3xuMJ9KYHW+6s8ATN4586wp9Q@mail.gmail.com>

> I am planning to implement Nadaraya-Watson regression model, with

I'm not sure what you mean by "implement".
Write a package, fit a model, or something else...

Reading your whole post, I get the impression you want mid-level
"building blocks", so you customize the model fitting process, in some
way.
But maybe I've got that wrong...

If you want fine control over the model fitting process (including the
cross validation), then you may have to write your own package,
including your own building blocks.
Otherwise, I think you should just use what's available.

Also, I'm not familiar with every flavor of nonparametric regression available.
If I wanted to fit a nonparametric regression model, I would start
with the mgcv package, which is hard to beat.


From t@ub|@ @end|ng |rom |mgprec|@|on@com  Tue Nov 19 21:42:01 2019
From: t@ub|@ @end|ng |rom |mgprec|@|on@com (Thomas Subia)
Date: Tue, 19 Nov 2019 20:42:01 +0000
Subject: [R] Extracting specific lines from pdfs
Message-ID: <CH2PR17MB3749FD017D5900BCE87ED36EB84C0@CH2PR17MB3749.namprd17.prod.outlook.com>

Colleagues,

I can extract specific data from lines in a pdf using:

library(pdftools)
pdf_text("10619.pdf")
txt <- pdf_text(".pdf")
write.table(txt,file="mydata.txt")
con <- file('mydata.txt')
open(con)
serial <- read.table(con,skip=5,nrow=1) #Extract[3]
flatness <- read.table(con,skip=11,nrow=1)# Extract [5]
parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
close(con)

# note here that serial has 4 variables
# flatness had 6 variables
# parallel1 has 5 variables
# parallel2 has 5 variables

# this outputs the specific data I need
serial[3]
flatness[5]
parallel1[5] # Note here that the txt format shows 0.0007 not scientific, is there a way to format this to display the original data?
parallel2[5] # Note here that the txt format shows 0.0006 not scientific, , is there a way to format this to display the original data?

I'd like to extend this code to all of the pdf files in a directory and to generate a table of all the serial, flatness, parallel1 and parallel2 data.
I'm not having a lot of success trying to build the script for this. Some pointers would be appreciated.

All the best

Thomas Subia 
Statistician / Senior Quality Engineer

IMG Companies?
E. tsubia at imgprecision.com


From drj|m|emon @end|ng |rom gm@||@com  Tue Nov 19 22:22:47 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 20 Nov 2019 08:22:47 +1100
Subject: [R] Separating Output of Modularity Analyses
In-Reply-To: <CAA3E8s3oeCiqR+vpYmJVjiyGBYdVeGDriDfAGuzXp_u_HYPiow@mail.gmail.com>
References: <CAA3E8s3oeCiqR+vpYmJVjiyGBYdVeGDriDfAGuzXp_u_HYPiow@mail.gmail.com>
Message-ID: <CA+8X3fVYrRT9fDGyyCmozjsjJshK_3QU7JVUQP_fvvaojbrSPg@mail.gmail.com>

Hi Chelsea,
A brute force method, but I think it does what you want:

# create a sequence of integers to make checking easy
null.cz<-1:68
separate_interdigitated_vectors<-function(x,nv=2,vlen=17) {
 xlen<-length(x)
 starts<-seq(1,xlen-vlen*nv+1,by=vlen*nv)
 cat(xlen,starts,"\n")
 for(start in starts) {
  if(start ==1) {
   x1<-x[start:(start+vlen-1)]
   x2<-x[(start+vlen):(start+vlen*2-1)]
  } else {
   x1<-c(x1,x[start:(start+vlen-1)])
   x2<-c(x2,x[(start+vlen):(start+vlen*2-1)])
  }
 }
 return(list(x1=matrix(x1,ncol=vlen,byrow=TRUE),
  x2=matrix(x2,ncol=vlen,byrow=TRUE)))
}
separate_interdigitated_vectors(null.cz)

Jim

On Wed, Nov 20, 2019 at 2:43 AM Chelsea Hinton <chelsearhinton at gmail.com> wrote:
>
> Hello,
>
> I am working to evaluate the structure of shrub-pollinator networks using
> modularity analyses. I have generated 100 null model networks to test for
> modularity and extract c and z scores for each (measuring statistical
> significance in real networks). However, the output of the "czvalues" for
> all 100 null models comes out in one mass clump (See below code). The
> highlighted section in the below code is an example of the null c and z
> values. I have 17 plants in my shrub-pollinator network (represented by the
> whole numbers) and so the first 17 values are the c scores followed by the
> next 17 which are z scores and so on.
>
> Is there a way I can easily separate out the c values from the z values? Or
> should I instead generate 100 null networks individually?
>
> R Code for null model generation and c and z output for nulls:
>
> library(bipartite)
>
> network<-read.csv(file.choose(), header = TRUE, row.names = 1)
>
> nw<-sortweb(network)
> weighted<-as.matrix(nw)
>
> nulls <- nullmodel(weighted, N=100, method=3)
> null.res <- unlist(sapply(nulls, metaComputeModules, USE.NAMES = TRUE)) #takes
> a while ...
> null.res
> ## [[1]]
> ## Slot "likelihood":
> ## [1] 0.1309568
> ##
> ##
> ## [[2]]
> ## Slot "likelihood":
> ## [1] 0.1303186
> ##
> ##
> ## [[3]]
> ## Slot "likelihood":
> ## [1] 0.14044
> .............
>
> ## [[100]]
> ## Slot "likelihood":
> ## [1] 0.1332231
>
> null.cz<-unlist(sapply(null.res, czvalues, level = "lower", USE.NAMES = TRUE
> ))
> null.cz
> ##           1           2           3           4           5           6
> ##  0.64147140  0.74959438  0.62019013  0.65895062  0.73724490  0.72664360
> ##           7           8           9          10          11          12
> ##  0.73553719  0.57500000  0.60204082  0.66000000  0.72000000  0.37500000
> ##          13          14          15          16          17           1
> ##  0.37500000  0.77777778  0.27777778  0.44444444  0.00000000  0.70710678
> ##           2           3           4           5           6           7
> ##  0.70710678  1.10729957  1.68025904 -0.70710678 -0.12539247  0.99339927
> ##           8           9          10          11          12          13
> ##  0.62696233  0.72849280 -0.27007306 -0.86094603 -0.83722650 -0.72727630
> ##          14          15          16          17           1           2
> ## -0.86094603 -0.42633438 -0.70710678 -1.02821822  0.65504683  0.62195782
> ##           3           4           5           6           7           8
> ##  0.65606509  0.70226843  0.71777778  0.58500000  0.71500000  0.74048443
> ##           9          10          11          12          13          14
> ##  0.70000000  0.44444444  0.57142857  0.75000000  0.62500000  0.66666667
> ##          15          16          17           1           2           3
> ##  0.44444444  0.44444444  0.00000000  0.70710678  1.04053196          NA
> ##           4           5           6           7           8           9
> ##  0.70710678 -0.08671100  2.15643011  0.40204629  0.40204629 -0.47514562
>
> Thank you for your time,
>
> Chelsea
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tg@77m @end|ng |rom y@hoo@com  Tue Nov 19 23:52:20 2019
From: tg@77m @end|ng |rom y@hoo@com (Thomas Subia)
Date: Tue, 19 Nov 2019 22:52:20 +0000 (UTC)
Subject: [R] Extract lines from pdf files
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
Message-ID: <774496205.2589227.1574203940481@mail.yahoo.com>


Colleagues,

?

I can extract specific data from lines in a pdf using:

?

library(pdftools)

pdf_text("10619.pdf")

txt <- pdf_text(".pdf")

write.table(txt,file="mydata.txt")

con <- file('mydata.txt')

open(con)

serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <- read.table(con,skip=11,nrow=1)# Extract [5]

parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]

parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]

close(con)

?

# note here that serial has 4 variables

# flatness had 6 variables

# parallel1 has 5 variables

# parallel2 has 5 variables

?

# this outputs the specific data I need

serial[3]

flatness[5]

parallel1[5] # Note here that the txt format shows 0.0007not scientific, is there a way to format this to display the original data?

parallel2[5] # Note here that the txt format shows 0.0006not scientific, , is there a way to format this to display the original data?

?

I'd like to extend this code to all of the pdf files in adirectory and to generate a table of all the serial, flatness, parallel1 andparallel2 data.

I'm not having a lot of success trying to build thescript for this. Some pointers would be appreciated.
All the best.
 
Thomas Subia

Statistician / Senior Quality Engineer



	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Nov 20 00:08:34 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 20 Nov 2019 00:08:34 +0100
Subject: [R] Extract lines from pdf files
In-Reply-To: <774496205.2589227.1574203940481@mail.yahoo.com>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
Message-ID: <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>

Please don't spam the mailing list. Especially with HTML format messages. See the Posting Guide.

PDF is designed to present data graphically. It is literally possible to place every character in the page in random order and still achieve this visual readability while practically making it nearly impossible to read. I have encountered many PDF files with the same text placed on the page multiple times... again scrambling your option to read it digitally. Tools like "pdftools" can sometimes work when the program that generated the file does so in a simple and extraction-friendly way... but there are no guarantees, and your description suggests that it is likely that you won't be able to accomplish your goal with this file.

On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help <r-help at r-project.org> wrote:
>
>Colleagues,
>
>?
>
>I can extract specific data from lines in a pdf using:
>
>?
>
>library(pdftools)
>
>pdf_text("10619.pdf")
>
>txt <- pdf_text(".pdf")
>
>write.table(txt,file="mydata.txt")
>
>con <- file('mydata.txt')
>
>open(con)
>
>serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <-
>read.table(con,skip=11,nrow=1)# Extract [5]
>
>parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
>
>parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
>
>close(con)
>
>?
>
># note here that serial has 4 variables
>
># flatness had 6 variables
>
># parallel1 has 5 variables
>
># parallel2 has 5 variables
>
>?
>
># this outputs the specific data I need
>
>serial[3]
>
>flatness[5]
>
>parallel1[5] # Note here that the txt format shows 0.0007not
>scientific, is there a way to format this to display the original data?
>
>parallel2[5] # Note here that the txt format shows 0.0006not
>scientific, , is there a way to format this to display the original
>data?
>
>?
>
>I'd like to extend this code to all of the pdf files in adirectory and
>to generate a table of all the serial, flatness, parallel1 andparallel2
>data.
>
>I'm not having a lot of success trying to build thescript for this.
>Some pointers would be appreciated.
>All the best.
> 
>Thomas Subia
>
>Statistician / Senior Quality Engineer
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From re|chm@nj @end|ng |rom @bcg|ob@|@net  Wed Nov 20 16:44:35 2019
From: re|chm@nj @end|ng |rom @bcg|ob@|@net (Jeff Reichman)
Date: Wed, 20 Nov 2019 09:44:35 -0600
Subject: [R] Obtaining the time to execute a R command
References: <000a01d59fb9$69f4e180$3ddea480$.ref@sbcglobal.net>
Message-ID: <000a01d59fb9$69f4e180$3ddea480$@sbcglobal.net>

R- Help

 

Is there a command or way to obtain the time it took R to execute a command?

 

 

Sincerely

 

Jeff Reichman

(314) 457-1966

 


	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Wed Nov 20 16:48:46 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Wed, 20 Nov 2019 17:48:46 +0200
Subject: [R] Obtaining the time to execute a R command
In-Reply-To: <000a01d59fb9$69f4e180$3ddea480$@sbcglobal.net>
References: <000a01d59fb9$69f4e180$3ddea480$.ref@sbcglobal.net>
 <000a01d59fb9$69f4e180$3ddea480$@sbcglobal.net>
Message-ID: <CAGgJW74mO6VWi_22XdK+cqad_60ODVQQSBBE-RuK4tkCpSrThA@mail.gmail.com>

Hi Jeff,
You might want to check out the microbenchmark() function in the
microbenchmark package.

install.packages("microbenchmark")
library(microbenchmark)
?microbenchmark

HTH,
Eric


On Wed, Nov 20, 2019 at 5:45 PM Jeff Reichman <reichmanj at sbcglobal.net>
wrote:

> R- Help
>
>
>
> Is there a command or way to obtain the time it took R to execute a
> command?
>
>
>
>
>
> Sincerely
>
>
>
> Jeff Reichman
>
> (314) 457-1966
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Wed Nov 20 16:57:40 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Wed, 20 Nov 2019 17:57:40 +0200
Subject: [R] Extract lines from pdf files
In-Reply-To: <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
Message-ID: <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>

Hi Thomas,
As Jeff wrote, your HTML email is difficult to read. This is a "plain
text" forum.
As for "pointers", here is one suggestion.
Since you write that you can do the necessary actions with a specific
file, try to write a function that carries out those actions for that
same file.
Except when implementing the function, replace any specific data with
the value of an argument passed into the function.
e.g.
txt <- pdf_text("10619.pdf")
would be replaced by
txt <- pdf_text(pdfFile)

and your function would have pdfFile as an argument, as in

myfunc <- function( pdfFile )

Since you can accomplish the task for this file without a function,
you should be able to accomplish the task with a function.
Once you succeed to do that you can then try passing the function
arguments that refer to the other files you need to process.

HTH,
Eric


On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Please don't spam the mailing list. Especially with HTML format messages. See the Posting Guide.
>
> PDF is designed to present data graphically. It is literally possible to place every character in the page in random order and still achieve this visual readability while practically making it nearly impossible to read. I have encountered many PDF files with the same text placed on the page multiple times... again scrambling your option to read it digitally. Tools like "pdftools" can sometimes work when the program that generated the file does so in a simple and extraction-friendly way... but there are no guarantees, and your description suggests that it is likely that you won't be able to accomplish your goal with this file.
>
> On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help <r-help at r-project.org> wrote:
> >
> >Colleagues,
> >
> >
> >
> >I can extract specific data from lines in a pdf using:
> >
> >
> >
> >library(pdftools)
> >
> >pdf_text("10619.pdf")
> >
> >txt <- pdf_text(".pdf")
> >
> >write.table(txt,file="mydata.txt")
> >
> >con <- file('mydata.txt')
> >
> >open(con)
> >
> >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <-
> >read.table(con,skip=11,nrow=1)# Extract [5]
> >
> >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
> >
> >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
> >
> >close(con)
> >
> >
> >
> ># note here that serial has 4 variables
> >
> ># flatness had 6 variables
> >
> ># parallel1 has 5 variables
> >
> ># parallel2 has 5 variables
> >
> >
> >
> ># this outputs the specific data I need
> >
> >serial[3]
> >
> >flatness[5]
> >
> >parallel1[5] # Note here that the txt format shows 0.0007not
> >scientific, is there a way to format this to display the original data?
> >
> >parallel2[5] # Note here that the txt format shows 0.0006not
> >scientific, , is there a way to format this to display the original
> >data?
> >
> >
> >
> >I'd like to extend this code to all of the pdf files in adirectory and
> >to generate a table of all the serial, flatness, parallel1 andparallel2
> >data.
> >
> >I'm not having a lot of success trying to build thescript for this.
> >Some pointers would be appreciated.
> >All the best.
> >
> >Thomas Subia
> >
> >Statistician / Senior Quality Engineer
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Nov 20 17:01:44 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 20 Nov 2019 16:01:44 +0000
Subject: [R] Obtaining the time to execute a R command
In-Reply-To: <000a01d59fb9$69f4e180$3ddea480$@sbcglobal.net>
References: <000a01d59fb9$69f4e180$3ddea480$.ref@sbcglobal.net>
 <000a01d59fb9$69f4e180$3ddea480$@sbcglobal.net>
Message-ID: <889f57d8-caf8-17e9-71fb-ca6bb9182b51@sapo.pt>

Hello,

There is

system.time {base}
CPU Time Used

Description

Return CPU (and other) times that expr used.


There are also packages microbenchmark or bench.


Hope this helps,

Rui Barradas

?s 15:44 de 20/11/19, Jeff Reichman escreveu:
> R- Help
> 
>   
> 
> Is there a command or way to obtain the time it took R to execute a command?
> 
>   
> 
>   
> 
> Sincerely
> 
>   
> 
> Jeff Reichman
> 
> (314) 457-1966
> 
>   
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From m@|||P@dpo@t @end|ng |rom gm@||@com  Wed Nov 20 17:36:55 2019
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Wed, 20 Nov 2019 19:36:55 +0300
Subject: [R] pairs.panels ()
Message-ID: <CAH6117+NaLp3iZkK_dhY60uCySJvd6kTmV1tpLNjr+0wcz7Kyw@mail.gmail.com>

Hi, Sarah,
Thank you not just for the answer, but for the teaching (!) answer!
Yes, cex.labels is what I need!


From re|chm@nj @end|ng |rom @bcg|ob@|@net  Wed Nov 20 19:56:09 2019
From: re|chm@nj @end|ng |rom @bcg|ob@|@net (Jeff Reichman)
Date: Wed, 20 Nov 2019 12:56:09 -0600
Subject: [R] Extract lines from pdf files
In-Reply-To: <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
 <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
Message-ID: <002901d59fd4$2cc5a090$8650e1b0$@sbcglobal.net>

Eric

I will have to give that a try. Thanks.  
For a "it works" method I used ....

start_time <- Sys.time()

	insert code of interest

end_time <- Sys.time()
end_time - start_time

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Eric Berger
Sent: Wednesday, November 20, 2019 9:58 AM
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Cc: Thomas Subia <tgs77m at yahoo.com>; Thomas Subia via R-help
<r-help at r-project.org>
Subject: Re: [R] Extract lines from pdf files

Hi Thomas,
As Jeff wrote, your HTML email is difficult to read. This is a "plain text"
forum.
As for "pointers", here is one suggestion.
Since you write that you can do the necessary actions with a specific file,
try to write a function that carries out those actions for that same file.
Except when implementing the function, replace any specific data with the
value of an argument passed into the function.
e.g.
txt <- pdf_text("10619.pdf")
would be replaced by
txt <- pdf_text(pdfFile)

and your function would have pdfFile as an argument, as in

myfunc <- function( pdfFile )

Since you can accomplish the task for this file without a function, you
should be able to accomplish the task with a function.
Once you succeed to do that you can then try passing the function arguments
that refer to the other files you need to process.

HTH,
Eric


On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:
>
> Please don't spam the mailing list. Especially with HTML format messages.
See the Posting Guide.
>
> PDF is designed to present data graphically. It is literally possible to
place every character in the page in random order and still achieve this
visual readability while practically making it nearly impossible to read. I
have encountered many PDF files with the same text placed on the page
multiple times... again scrambling your option to read it digitally. Tools
like "pdftools" can sometimes work when the program that generated the file
does so in a simple and extraction-friendly way... but there are no
guarantees, and your description suggests that it is likely that you won't
be able to accomplish your goal with this file.
>
> On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help
<r-help at r-project.org> wrote:
> >
> >Colleagues,
> >
> >
> >
> >I can extract specific data from lines in a pdf using:
> >
> >
> >
> >library(pdftools)
> >
> >pdf_text("10619.pdf")
> >
> >txt <- pdf_text(".pdf")
> >
> >write.table(txt,file="mydata.txt")
> >
> >con <- file('mydata.txt')
> >
> >open(con)
> >
> >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <- 
> >read.table(con,skip=11,nrow=1)# Extract [5]
> >
> >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
> >
> >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
> >
> >close(con)
> >
> >
> >
> ># note here that serial has 4 variables
> >
> ># flatness had 6 variables
> >
> ># parallel1 has 5 variables
> >
> ># parallel2 has 5 variables
> >
> >
> >
> ># this outputs the specific data I need
> >
> >serial[3]
> >
> >flatness[5]
> >
> >parallel1[5] # Note here that the txt format shows 0.0007not 
> >scientific, is there a way to format this to display the original data?
> >
> >parallel2[5] # Note here that the txt format shows 0.0006not 
> >scientific, , is there a way to format this to display the original 
> >data?
> >
> >
> >
> >I'd like to extend this code to all of the pdf files in adirectory 
> >and to generate a table of all the serial, flatness, parallel1 
> >andparallel2 data.
> >
> >I'm not having a lot of success trying to build thescript for this.
> >Some pointers would be appreciated.
> >All the best.
> >
> >Thomas Subia
> >
> >Statistician / Senior Quality Engineer
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From re|chm@nj @end|ng |rom @bcg|ob@|@net  Wed Nov 20 19:56:09 2019
From: re|chm@nj @end|ng |rom @bcg|ob@|@net (Jeff Reichman)
Date: Wed, 20 Nov 2019 12:56:09 -0600
Subject: [R] Extract lines from pdf files
In-Reply-To: <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
 <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
Message-ID: <002901d59fd4$2cc5a090$8650e1b0$@sbcglobal.net>

Eric

I will have to give that a try. Thanks.  
For a "it works" method I used ....

start_time <- Sys.time()

	insert code of interest

end_time <- Sys.time()
end_time - start_time

-----Original Message-----
From: R-help <r-help-bounces at r-project.org> On Behalf Of Eric Berger
Sent: Wednesday, November 20, 2019 9:58 AM
To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
Cc: Thomas Subia <tgs77m at yahoo.com>; Thomas Subia via R-help
<r-help at r-project.org>
Subject: Re: [R] Extract lines from pdf files

Hi Thomas,
As Jeff wrote, your HTML email is difficult to read. This is a "plain text"
forum.
As for "pointers", here is one suggestion.
Since you write that you can do the necessary actions with a specific file,
try to write a function that carries out those actions for that same file.
Except when implementing the function, replace any specific data with the
value of an argument passed into the function.
e.g.
txt <- pdf_text("10619.pdf")
would be replaced by
txt <- pdf_text(pdfFile)

and your function would have pdfFile as an argument, as in

myfunc <- function( pdfFile )

Since you can accomplish the task for this file without a function, you
should be able to accomplish the task with a function.
Once you succeed to do that you can then try passing the function arguments
that refer to the other files you need to process.

HTH,
Eric


On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:
>
> Please don't spam the mailing list. Especially with HTML format messages.
See the Posting Guide.
>
> PDF is designed to present data graphically. It is literally possible to
place every character in the page in random order and still achieve this
visual readability while practically making it nearly impossible to read. I
have encountered many PDF files with the same text placed on the page
multiple times... again scrambling your option to read it digitally. Tools
like "pdftools" can sometimes work when the program that generated the file
does so in a simple and extraction-friendly way... but there are no
guarantees, and your description suggests that it is likely that you won't
be able to accomplish your goal with this file.
>
> On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help
<r-help at r-project.org> wrote:
> >
> >Colleagues,
> >
> >
> >
> >I can extract specific data from lines in a pdf using:
> >
> >
> >
> >library(pdftools)
> >
> >pdf_text("10619.pdf")
> >
> >txt <- pdf_text(".pdf")
> >
> >write.table(txt,file="mydata.txt")
> >
> >con <- file('mydata.txt')
> >
> >open(con)
> >
> >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <- 
> >read.table(con,skip=11,nrow=1)# Extract [5]
> >
> >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
> >
> >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
> >
> >close(con)
> >
> >
> >
> ># note here that serial has 4 variables
> >
> ># flatness had 6 variables
> >
> ># parallel1 has 5 variables
> >
> ># parallel2 has 5 variables
> >
> >
> >
> ># this outputs the specific data I need
> >
> >serial[3]
> >
> >flatness[5]
> >
> >parallel1[5] # Note here that the txt format shows 0.0007not 
> >scientific, is there a way to format this to display the original data?
> >
> >parallel2[5] # Note here that the txt format shows 0.0006not 
> >scientific, , is there a way to format this to display the original 
> >data?
> >
> >
> >
> >I'd like to extend this code to all of the pdf files in adirectory 
> >and to generate a table of all the serial, flatness, parallel1 
> >andparallel2 data.
> >
> >I'm not having a lot of success trying to build thescript for this.
> >Some pointers would be appreciated.
> >All the best.
> >
> >Thomas Subia
> >
> >Statistician / Senior Quality Engineer
> >
> >
> >
> >       [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov 20 20:19:31 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Nov 2019 11:19:31 -0800
Subject: [R] Extract lines from pdf files
In-Reply-To: <002901d59fd4$2cc5a090$8650e1b0$@sbcglobal.net>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
 <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
 <002901d59fd4$2cc5a090$8650e1b0$@sbcglobal.net>
Message-ID: <CAGxFJbQ=TMit4nX2L2pgBqFvaboM7oz-PZxiYQLZJWPi=qKYTA@mail.gmail.com>

Don't do this (the timing code you showed, not Eric's suggestions).

Do this:

sys.time( {
    code of interest
})

(or use the microbenchmark package functionality)

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 20, 2019 at 10:56 AM Jeff Reichman <reichmanj at sbcglobal.net>
wrote:

> Eric
>
> I will have to give that a try. Thanks.
> For a "it works" method I used ....
>
> start_time <- Sys.time()
>
>         insert code of interest
>
> end_time <- Sys.time()
> end_time - start_time
>
> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Eric Berger
> Sent: Wednesday, November 20, 2019 9:58 AM
> To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> Cc: Thomas Subia <tgs77m at yahoo.com>; Thomas Subia via R-help
> <r-help at r-project.org>
> Subject: Re: [R] Extract lines from pdf files
>
> Hi Thomas,
> As Jeff wrote, your HTML email is difficult to read. This is a "plain text"
> forum.
> As for "pointers", here is one suggestion.
> Since you write that you can do the necessary actions with a specific file,
> try to write a function that carries out those actions for that same file.
> Except when implementing the function, replace any specific data with the
> value of an argument passed into the function.
> e.g.
> txt <- pdf_text("10619.pdf")
> would be replaced by
> txt <- pdf_text(pdfFile)
>
> and your function would have pdfFile as an argument, as in
>
> myfunc <- function( pdfFile )
>
> Since you can accomplish the task for this file without a function, you
> should be able to accomplish the task with a function.
> Once you succeed to do that you can then try passing the function arguments
> that refer to the other files you need to process.
>
> HTH,
> Eric
>
>
> On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> >
> > Please don't spam the mailing list. Especially with HTML format messages.
> See the Posting Guide.
> >
> > PDF is designed to present data graphically. It is literally possible to
> place every character in the page in random order and still achieve this
> visual readability while practically making it nearly impossible to read. I
> have encountered many PDF files with the same text placed on the page
> multiple times... again scrambling your option to read it digitally. Tools
> like "pdftools" can sometimes work when the program that generated the file
> does so in a simple and extraction-friendly way... but there are no
> guarantees, and your description suggests that it is likely that you won't
> be able to accomplish your goal with this file.
> >
> > On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help
> <r-help at r-project.org> wrote:
> > >
> > >Colleagues,
> > >
> > >
> > >
> > >I can extract specific data from lines in a pdf using:
> > >
> > >
> > >
> > >library(pdftools)
> > >
> > >pdf_text("10619.pdf")
> > >
> > >txt <- pdf_text(".pdf")
> > >
> > >write.table(txt,file="mydata.txt")
> > >
> > >con <- file('mydata.txt')
> > >
> > >open(con)
> > >
> > >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <-
> > >read.table(con,skip=11,nrow=1)# Extract [5]
> > >
> > >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
> > >
> > >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
> > >
> > >close(con)
> > >
> > >
> > >
> > ># note here that serial has 4 variables
> > >
> > ># flatness had 6 variables
> > >
> > ># parallel1 has 5 variables
> > >
> > ># parallel2 has 5 variables
> > >
> > >
> > >
> > ># this outputs the specific data I need
> > >
> > >serial[3]
> > >
> > >flatness[5]
> > >
> > >parallel1[5] # Note here that the txt format shows 0.0007not
> > >scientific, is there a way to format this to display the original data?
> > >
> > >parallel2[5] # Note here that the txt format shows 0.0006not
> > >scientific, , is there a way to format this to display the original
> > >data?
> > >
> > >
> > >
> > >I'd like to extend this code to all of the pdf files in adirectory
> > >and to generate a table of all the serial, flatness, parallel1
> > >andparallel2 data.
> > >
> > >I'm not having a lot of success trying to build thescript for this.
> > >Some pointers would be appreciated.
> > >All the best.
> > >
> > >Thomas Subia
> > >
> > >Statistician / Senior Quality Engineer
> > >
> > >
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov 20 20:24:52 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Nov 2019 11:24:52 -0800
Subject: [R] Extract lines from pdf files
In-Reply-To: <CAGxFJbQ=TMit4nX2L2pgBqFvaboM7oz-PZxiYQLZJWPi=qKYTA@mail.gmail.com>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
 <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
 <002901d59fd4$2cc5a090$8650e1b0$@sbcglobal.net>
 <CAGxFJbQ=TMit4nX2L2pgBqFvaboM7oz-PZxiYQLZJWPi=qKYTA@mail.gmail.com>
Message-ID: <CAGxFJbQEFqW9UJMywrqjW5DVviZmCQBT1g_E0fM9RPJmk_qukA@mail.gmail.com>

rather, system.time() of course.

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 20, 2019 at 11:19 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Don't do this (the timing code you showed, not Eric's suggestions).
>
> Do this:
>
> sys.time( {
>     code of interest
> })
>
> (or use the microbenchmark package functionality)
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Nov 20, 2019 at 10:56 AM Jeff Reichman <reichmanj at sbcglobal.net>
> wrote:
>
>> Eric
>>
>> I will have to give that a try. Thanks.
>> For a "it works" method I used ....
>>
>> start_time <- Sys.time()
>>
>>         insert code of interest
>>
>> end_time <- Sys.time()
>> end_time - start_time
>>
>> -----Original Message-----
>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Eric Berger
>> Sent: Wednesday, November 20, 2019 9:58 AM
>> To: Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> Cc: Thomas Subia <tgs77m at yahoo.com>; Thomas Subia via R-help
>> <r-help at r-project.org>
>> Subject: Re: [R] Extract lines from pdf files
>>
>> Hi Thomas,
>> As Jeff wrote, your HTML email is difficult to read. This is a "plain
>> text"
>> forum.
>> As for "pointers", here is one suggestion.
>> Since you write that you can do the necessary actions with a specific
>> file,
>> try to write a function that carries out those actions for that same file.
>> Except when implementing the function, replace any specific data with the
>> value of an argument passed into the function.
>> e.g.
>> txt <- pdf_text("10619.pdf")
>> would be replaced by
>> txt <- pdf_text(pdfFile)
>>
>> and your function would have pdfFile as an argument, as in
>>
>> myfunc <- function( pdfFile )
>>
>> Since you can accomplish the task for this file without a function, you
>> should be able to accomplish the task with a function.
>> Once you succeed to do that you can then try passing the function
>> arguments
>> that refer to the other files you need to process.
>>
>> HTH,
>> Eric
>>
>>
>> On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>> wrote:
>> >
>> > Please don't spam the mailing list. Especially with HTML format
>> messages.
>> See the Posting Guide.
>> >
>> > PDF is designed to present data graphically. It is literally possible to
>> place every character in the page in random order and still achieve this
>> visual readability while practically making it nearly impossible to read.
>> I
>> have encountered many PDF files with the same text placed on the page
>> multiple times... again scrambling your option to read it digitally. Tools
>> like "pdftools" can sometimes work when the program that generated the
>> file
>> does so in a simple and extraction-friendly way... but there are no
>> guarantees, and your description suggests that it is likely that you won't
>> be able to accomplish your goal with this file.
>> >
>> > On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help
>> <r-help at r-project.org> wrote:
>> > >
>> > >Colleagues,
>> > >
>> > >
>> > >
>> > >I can extract specific data from lines in a pdf using:
>> > >
>> > >
>> > >
>> > >library(pdftools)
>> > >
>> > >pdf_text("10619.pdf")
>> > >
>> > >txt <- pdf_text(".pdf")
>> > >
>> > >write.table(txt,file="mydata.txt")
>> > >
>> > >con <- file('mydata.txt')
>> > >
>> > >open(con)
>> > >
>> > >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <-
>> > >read.table(con,skip=11,nrow=1)# Extract [5]
>> > >
>> > >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
>> > >
>> > >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
>> > >
>> > >close(con)
>> > >
>> > >
>> > >
>> > ># note here that serial has 4 variables
>> > >
>> > ># flatness had 6 variables
>> > >
>> > ># parallel1 has 5 variables
>> > >
>> > ># parallel2 has 5 variables
>> > >
>> > >
>> > >
>> > ># this outputs the specific data I need
>> > >
>> > >serial[3]
>> > >
>> > >flatness[5]
>> > >
>> > >parallel1[5] # Note here that the txt format shows 0.0007not
>> > >scientific, is there a way to format this to display the original data?
>> > >
>> > >parallel2[5] # Note here that the txt format shows 0.0006not
>> > >scientific, , is there a way to format this to display the original
>> > >data?
>> > >
>> > >
>> > >
>> > >I'd like to extend this code to all of the pdf files in adirectory
>> > >and to generate a table of all the serial, flatness, parallel1
>> > >andparallel2 data.
>> > >
>> > >I'm not having a lot of success trying to build thescript for this.
>> > >Some pointers would be appreciated.
>> > >All the best.
>> > >
>> > >Thomas Subia
>> > >
>> > >Statistician / Senior Quality Engineer
>> > >
>> > >
>> > >
>> > >       [[alternative HTML version deleted]]
>> > >
>> > >______________________________________________
>> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > >https://stat.ethz.ch/mailman/listinfo/r-help
>> > >PLEASE do read the posting guide
>> > >http://www.R-project.org/posting-guide.html
>> > >and provide commented, minimal, self-contained, reproducible code.
>> >
>> > --
>> > Sent from my phone. Please excuse my brevity.
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From tg@77m @end|ng |rom y@hoo@com  Wed Nov 20 21:21:21 2019
From: tg@77m @end|ng |rom y@hoo@com (Thomas Subia)
Date: Wed, 20 Nov 2019 20:21:21 +0000 (UTC)
Subject: [R] Extract lines from pdf files
In-Reply-To: <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
 <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
Message-ID: <1504234014.3103202.1574281281258@mail.yahoo.com>

Thanks all for the help. I appreciate the feedback
I've developed another method to extract my desired data from multiple pdfs in a directory.

# Combine all pdfs to a combined pdf
files <- list.files(pattern = "pdf$")
pdf_combine(files, output = "joined.pdf")

# creates a text file from joined.pdf
pdf_text("joined.pdf")
txt <- pdf_text("joined.pdf")
write.table(txt,file="mydata.txt")

# I need to extract the lines which match a line beginning with AMAT
lines <- readLines("mydata.txt")
date <- grep("AMAT",lines)

# output for date looks like?[1]? ?6? 62 118 174 230 286 342 398
# These are exactly the line positions I need.

Now that I've got the desired lines, I don't know how to extract the data from those lines.

Any advice would be appreciated.

All the best,

Thomas Subia
Statistician / Quality Engineer
IMG Precision Inc.








On Wednesday, November 20, 2019, 07:58:08 AM PST, Eric Berger <ericjberger at gmail.com> wrote: 





Hi Thomas,
As Jeff wrote, your HTML email is difficult to read. This is a "plain
text" forum.
As for "pointers", here is one suggestion.
Since you write that you can do the necessary actions with a specific
file, try to write a function that carries out those actions for that
same file.
Except when implementing the function, replace any specific data with
the value of an argument passed into the function.
e.g.
txt <- pdf_text("10619.pdf")
would be replaced by
txt <- pdf_text(pdfFile)

and your function would have pdfFile as an argument, as in

myfunc <- function( pdfFile )

Since you can accomplish the task for this file without a function,
you should be able to accomplish the task with a function.
Once you succeed to do that you can then try passing the function
arguments that refer to the other files you need to process.

HTH,
Eric


On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> Please don't spam the mailing list. Especially with HTML format messages. See the Posting Guide.
>
> PDF is designed to present data graphically. It is literally possible to place every character in the page in random order and still achieve this visual readability while practically making it nearly impossible to read. I have encountered many PDF files with the same text placed on the page multiple times... again scrambling your option to read it digitally. Tools like "pdftools" can sometimes work when the program that generated the file does so in a simple and extraction-friendly way... but there are no guarantees, and your description suggests that it is likely that you won't be able to accomplish your goal with this file.
>
> On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help <r-help at r-project.org> wrote:
> >
> >Colleagues,
> >
> >
> >
> >I can extract specific data from lines in a pdf using:
> >
> >
> >
> >library(pdftools)
> >
> >pdf_text("10619.pdf")
> >
> >txt <- pdf_text(".pdf")
> >
> >write.table(txt,file="mydata.txt")
> >
> >con <- file('mydata.txt')
> >
> >open(con)
> >
> >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <-
> >read.table(con,skip=11,nrow=1)# Extract [5]
> >
> >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
> >
> >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
> >
> >close(con)
> >
> >
> >
> ># note here that serial has 4 variables
> >
> ># flatness had 6 variables
> >
> ># parallel1 has 5 variables
> >
> ># parallel2 has 5 variables
> >
> >
> >
> ># this outputs the specific data I need
> >
> >serial[3]
> >
> >flatness[5]
> >
> >parallel1[5] # Note here that the txt format shows 0.0007not
> >scientific, is there a way to format this to display the original data?
> >
> >parallel2[5] # Note here that the txt format shows 0.0006not
> >scientific, , is there a way to format this to display the original
> >data?
> >
> >
> >
> >I'd like to extend this code to all of the pdf files in adirectory and
> >to generate a table of all the serial, flatness, parallel1 andparallel2
> >data.
> >
> >I'm not having a lot of success trying to build thescript for this.
> >Some pointers would be appreciated.
> >All the best.
> >
> >Thomas Subia
> >
> >Statistician / Senior Quality Engineer
> >
> >
> >
> >? ? ? [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.

>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov 20 23:53:13 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 20 Nov 2019 14:53:13 -0800
Subject: [R] Extract lines from pdf files
In-Reply-To: <1504234014.3103202.1574281281258@mail.yahoo.com>
References: <774496205.2589227.1574203940481.ref@mail.yahoo.com>
 <774496205.2589227.1574203940481@mail.yahoo.com>
 <FC4FB905-880B-4F69-8776-EDC7BB6FD626@dcn.davis.ca.us>
 <CAGgJW74xSWkhG8LWqvVnLG0tr4LsdOugu1E6NsXPCF4G2-0meg@mail.gmail.com>
 <1504234014.3103202.1574281281258@mail.yahoo.com>
Message-ID: <CAGxFJbR5QB24okK=Nsd2GmUJj-9sUkrZJbdzh+gN0SzFFBhX0g@mail.gmail.com>

I think you are more likely to get a helpful answer if you give a minimal
example of what your lines look like. I certainly don't have a clue, though
maybe someone else will.

Cheers,
Bert


On Wed, Nov 20, 2019 at 12:21 PM Thomas Subia via R-help <
r-help at r-project.org> wrote:

> Thanks all for the help. I appreciate the feedback
> I've developed another method to extract my desired data from multiple
> pdfs in a directory.
>
> # Combine all pdfs to a combined pdf
> files <- list.files(pattern = "pdf$")
> pdf_combine(files, output = "joined.pdf")
>
> # creates a text file from joined.pdf
> pdf_text("joined.pdf")
> txt <- pdf_text("joined.pdf")
> write.table(txt,file="mydata.txt")
>
> # I need to extract the lines which match a line beginning with AMAT
> lines <- readLines("mydata.txt")
> date <- grep("AMAT",lines)
>
> # output for date looks like [1]   6  62 118 174 230 286 342 398
> # These are exactly the line positions I need.
>
> Now that I've got the desired lines, I don't know how to extract the data
> from those lines.
>
> Any advice would be appreciated.
>
> All the best,
>
> Thomas Subia
> Statistician / Quality Engineer
> IMG Precision Inc.
>
>
>
>
>
>
>
>
> On Wednesday, November 20, 2019, 07:58:08 AM PST, Eric Berger <
> ericjberger at gmail.com> wrote:
>
>
>
>
>
> Hi Thomas,
> As Jeff wrote, your HTML email is difficult to read. This is a "plain
> text" forum.
> As for "pointers", here is one suggestion.
> Since you write that you can do the necessary actions with a specific
> file, try to write a function that carries out those actions for that
> same file.
> Except when implementing the function, replace any specific data with
> the value of an argument passed into the function.
> e.g.
> txt <- pdf_text("10619.pdf")
> would be replaced by
> txt <- pdf_text(pdfFile)
>
> and your function would have pdfFile as an argument, as in
>
> myfunc <- function( pdfFile )
>
> Since you can accomplish the task for this file without a function,
> you should be able to accomplish the task with a function.
> Once you succeed to do that you can then try passing the function
> arguments that refer to the other files you need to process.
>
> HTH,
> Eric
>
>
> On Wed, Nov 20, 2019 at 1:09 AM Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> >
> > Please don't spam the mailing list. Especially with HTML format
> messages. See the Posting Guide.
> >
> > PDF is designed to present data graphically. It is literally possible to
> place every character in the page in random order and still achieve this
> visual readability while practically making it nearly impossible to read. I
> have encountered many PDF files with the same text placed on the page
> multiple times... again scrambling your option to read it digitally. Tools
> like "pdftools" can sometimes work when the program that generated the file
> does so in a simple and extraction-friendly way... but there are no
> guarantees, and your description suggests that it is likely that you won't
> be able to accomplish your goal with this file.
> >
> > On November 19, 2019 11:52:20 PM GMT+01:00, Thomas Subia via R-help <
> r-help at r-project.org> wrote:
> > >
> > >Colleagues,
> > >
> > >
> > >
> > >I can extract specific data from lines in a pdf using:
> > >
> > >
> > >
> > >library(pdftools)
> > >
> > >pdf_text("10619.pdf")
> > >
> > >txt <- pdf_text(".pdf")
> > >
> > >write.table(txt,file="mydata.txt")
> > >
> > >con <- file('mydata.txt')
> > >
> > >open(con)
> > >
> > >serial <- read.table(con,skip=5,nrow=1) #Extract[3]flatness <-
> > >read.table(con,skip=11,nrow=1)# Extract [5]
> > >
> > >parallel1 <-read.table(con,skip=2,nrow=1)# Extract [5]
> > >
> > >parallel2 <-read.table(con,skip=4,nrow=1)# Extract [5]
> > >
> > >close(con)
> > >
> > >
> > >
> > ># note here that serial has 4 variables
> > >
> > ># flatness had 6 variables
> > >
> > ># parallel1 has 5 variables
> > >
> > ># parallel2 has 5 variables
> > >
> > >
> > >
> > ># this outputs the specific data I need
> > >
> > >serial[3]
> > >
> > >flatness[5]
> > >
> > >parallel1[5] # Note here that the txt format shows 0.0007not
> > >scientific, is there a way to format this to display the original data?
> > >
> > >parallel2[5] # Note here that the txt format shows 0.0006not
> > >scientific, , is there a way to format this to display the original
> > >data?
> > >
> > >
> > >
> > >I'd like to extend this code to all of the pdf files in adirectory and
> > >to generate a table of all the serial, flatness, parallel1 andparallel2
> > >data.
> > >
> > >I'm not having a lot of success trying to build thescript for this.
> > >Some pointers would be appreciated.
> > >All the best.
> > >
> > >Thomas Subia
> > >
> > >Statistician / Senior Quality Engineer
> > >
> > >
> > >
> > >      [[alternative HTML version deleted]]
> > >
> > >______________________________________________
> > >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >https://stat.ethz.ch/mailman/listinfo/r-help
> > >PLEASE do read the posting guide
> > >http://www.R-project.org/posting-guide.html
> > >and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Sent from my phone. Please excuse my brevity.
>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From g||ted|||e2014 @end|ng |rom gm@||@com  Thu Nov 21 09:49:06 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Thu, 21 Nov 2019 09:49:06 +0100
Subject: [R] Datetime in doubly y-axis plot
Message-ID: <CAC8ss30zt3mW4bxkBEtG-1eo3e5uy_KTKqLvaAb0xqtfmmpm3g@mail.gmail.com>

Dear Members,
I have some hourly data. Usin:
dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
I converted the hour to time format and stored in Year.
The data consists of two different observations and I wish to plot
same on one graph (two y-axis and common x-axis).

Part of my script is of the form:
library(plotrix)
plot(Year,Li,pch=16,axes=FALSE,xlab="",ylab="",type="l",col="black",ylim=c(1,22909))
axis(2, ylim=c(1,22909),col="black",las=1)
mtext("Lightning count/day", side=2, line=4)
box()
par(new=TRUE)
plot(Year,CR, pch=15,
xlab="",ylab="",ylim=c(7302,9983),axes=FALSE,type="l",col="red")
mtext("GCR count/day",side=4,col="red",line=3)
axis(4, ylim=c(7302,9983), col="red",col.axis="red",las=1)
The two y-axis worked and looked fine.

But I have difficulties with x-axis.
I tried things like in an attempt to draw the x-axis.
## Draw the time axis
#axis(1,pretty(range(Year),10))
axis(side=1,at=c(as.datetime("2005-01-01 01:00:00
GMT"),as.datetime("2007-01-01 01:00:00 GMT"),as.datetime("2009-01-01
01:00:00 GMT"),labels=c("2005","2007","2009"))
#axis(side=1, at =Year)
#axis(1,Year)

But none worked.

However, when I plotted the two graphs separately, the x-axis was
correctly labelled from 2005 to 2010 (the time range of my data).

Small portion of the data is:
04 12 31 10  8637 4992 0.310676228913214 49.7113438132607
04 12 31 11  8634 4183 0.275834035016405 25.4492290005748
04 12 31 12  8652 4160 0.484887198397259 24.7594531777172
04 12 31 13  8626 3411 0.182921517958247 2.2967535550946
04 12 31 15  8618 3904 0.09000900090009 17.0819483667808
04 12 31 16  8620 3428 0.113237130164629 2.80658785894585
04 12 31 17  8603 3041 -0.0842019685839552 -8.79964011696198
04 12 31 18  8579 2340 -0.362939519758427 -29.8228075875341
04 12 31 19  8588 3006 -0.258412938068 -9.8492989778322
04 12 31 20  8604 3280 -0.0725879039516855 -1.63196960987679
04 12 31 21  8568 3696 -0.490694230713394 10.8439757078949
04 12 31 22  8567 2307 -0.502308295345664 -30.8124859420688
04 12 31 23  8570 2726 -0.467466101448855 -18.2465698647939
05 01 01 00  8579 2879 -0.568878758318289 8.46755203918244
05 01 01 01  8581 1586 -0.545698639133843 -40.2467740416313
05 01 01 02  8562 1183 -0.765909771386082 -55.4299708015447
05 01 01 03  8602 1491 -0.302307387697159 -43.8259395309409
05 01 01 04  8576 1742 -0.603648937094958 -34.369407553923
05 01 01 05  8622 2001 -0.0705061958526974 -24.6114721672789
05 01 01 06  8613 2675 -0.174816732182705 0.781765093717623
05 01 01 07  8622 3293 -0.0705061958526974 24.0651784873316
05 01 01 08  8628 4333 -0.00096583829935895 63.2476217387209
05 01 01 09  8669 16 0.474226604981787 -99.3971931807479
05 01 01 10  8698 3671 0.810338333156256 38.3064895921635
05 01 01 11  8671 3856 0.497406724166233 45.2764434397664
05 01 01 12  8679 3411 0.590127200904018 28.5108787793162
05 01 01 13  8647 3428 0.219245293952879 29.1513610247716
05 01 01 14  8651 4596 0.265605532321772 73.156258830178
05 01 01 15  8638 3154 0.114934757622872 18.8282942450786
05 01 01 16  8635 2365 0.0801645788462025 -10.8976170292926
05 01 01 17  8628 2697 -0.00096583829935895 1.61062447018932
05 01 01 18  8653 3231 0.288785651506218 21.7293020627296
05 01 01 19  8657 3548 0.33514588987511 33.6724121691627
05 01 01 20  8650 2645 0.254015472729549 -0.348497692380145
05 01 01 21  8605 2153 -0.26753720892049 -18.8848073843835
05 01 01 22  8600 1759 -0.325487506881605 -33.7289253084676
Where the first four columns are year, month, day and hour. The last
four columns are the parameters of interest which I am trying to plot,
taking two at a time.
I will remain indebted for your assistance.
Best Regards
Ogbos


From petr@p|k@| @end|ng |rom prechez@@cz  Thu Nov 21 10:19:31 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Thu, 21 Nov 2019 09:19:31 +0000
Subject: [R] Datetime in doubly y-axis plot
In-Reply-To: <CAC8ss30zt3mW4bxkBEtG-1eo3e5uy_KTKqLvaAb0xqtfmmpm3g@mail.gmail.com>
References: <CAC8ss30zt3mW4bxkBEtG-1eo3e5uy_KTKqLvaAb0xqtfmmpm3g@mail.gmail.com>
Message-ID: <a57243abd4e34f91a96dcbc6e832c2e1@SRVEXCHCM1301.precheza.cz>

Hi

Why you do not use twoord.plot from plotrix which should be designed for
such issue.

Something like

twoord.plot(lx = Year, ly = Li, rx = Year, ry= CR, data=yourdata)

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
> Sent: Thursday, November 21, 2019 9:49 AM
> To: r-help <r-help at r-project.org>
> Subject: [R] Datetime in doubly y-axis plot
> 
> Dear Members,
> I have some hourly data. Usin:
> dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
> dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year,
> month,day,hour,0,0)))
> I converted the hour to time format and stored in Year.
> The data consists of two different observations and I wish to plot
> same on one graph (two y-axis and common x-axis).
> 
> Part of my script is of the form:
> library(plotrix)
>
plot(Year,Li,pch=16,axes=FALSE,xlab="",ylab="",type="l",col="black",ylim=c(
> 1,22909))
> axis(2, ylim=c(1,22909),col="black",las=1)
> mtext("Lightning count/day", side=2, line=4)
> box()
> par(new=TRUE)
> plot(Year,CR, pch=15,
> xlab="",ylab="",ylim=c(7302,9983),axes=FALSE,type="l",col="red")
> mtext("GCR count/day",side=4,col="red",line=3)
> axis(4, ylim=c(7302,9983), col="red",col.axis="red",las=1)
> The two y-axis worked and looked fine.
> 
> But I have difficulties with x-axis.
> I tried things like in an attempt to draw the x-axis.
> ## Draw the time axis
> #axis(1,pretty(range(Year),10))
> axis(side=1,at=c(as.datetime("2005-01-01 01:00:00
> GMT"),as.datetime("2007-01-01 01:00:00 GMT"),as.datetime("2009-01-01
> 01:00:00 GMT"),labels=c("2005","2007","2009"))
> #axis(side=1, at =Year)
> #axis(1,Year)
> 
> But none worked.
> 
> However, when I plotted the two graphs separately, the x-axis was
> correctly labelled from 2005 to 2010 (the time range of my data).
> 
> Small portion of the data is:
> 04 12 31 10  8637 4992 0.310676228913214 49.7113438132607
> 04 12 31 11  8634 4183 0.275834035016405 25.4492290005748
> 04 12 31 12  8652 4160 0.484887198397259 24.7594531777172
> 04 12 31 13  8626 3411 0.182921517958247 2.2967535550946
> 04 12 31 15  8618 3904 0.09000900090009 17.0819483667808
> 04 12 31 16  8620 3428 0.113237130164629 2.80658785894585
> 04 12 31 17  8603 3041 -0.0842019685839552 -8.79964011696198
> 04 12 31 18  8579 2340 -0.362939519758427 -29.8228075875341
> 04 12 31 19  8588 3006 -0.258412938068 -9.8492989778322
> 04 12 31 20  8604 3280 -0.0725879039516855 -1.63196960987679
> 04 12 31 21  8568 3696 -0.490694230713394 10.8439757078949
> 04 12 31 22  8567 2307 -0.502308295345664 -30.8124859420688
> 04 12 31 23  8570 2726 -0.467466101448855 -18.2465698647939
> 05 01 01 00  8579 2879 -0.568878758318289 8.46755203918244
> 05 01 01 01  8581 1586 -0.545698639133843 -40.2467740416313
> 05 01 01 02  8562 1183 -0.765909771386082 -55.4299708015447
> 05 01 01 03  8602 1491 -0.302307387697159 -43.8259395309409
> 05 01 01 04  8576 1742 -0.603648937094958 -34.369407553923
> 05 01 01 05  8622 2001 -0.0705061958526974 -24.6114721672789
> 05 01 01 06  8613 2675 -0.174816732182705 0.781765093717623
> 05 01 01 07  8622 3293 -0.0705061958526974 24.0651784873316
> 05 01 01 08  8628 4333 -0.00096583829935895 63.2476217387209
> 05 01 01 09  8669 16 0.474226604981787 -99.3971931807479
> 05 01 01 10  8698 3671 0.810338333156256 38.3064895921635
> 05 01 01 11  8671 3856 0.497406724166233 45.2764434397664
> 05 01 01 12  8679 3411 0.590127200904018 28.5108787793162
> 05 01 01 13  8647 3428 0.219245293952879 29.1513610247716
> 05 01 01 14  8651 4596 0.265605532321772 73.156258830178
> 05 01 01 15  8638 3154 0.114934757622872 18.8282942450786
> 05 01 01 16  8635 2365 0.0801645788462025 -10.8976170292926
> 05 01 01 17  8628 2697 -0.00096583829935895 1.61062447018932
> 05 01 01 18  8653 3231 0.288785651506218 21.7293020627296
> 05 01 01 19  8657 3548 0.33514588987511 33.6724121691627
> 05 01 01 20  8650 2645 0.254015472729549 -0.348497692380145
> 05 01 01 21  8605 2153 -0.26753720892049 -18.8848073843835
> 05 01 01 22  8600 1759 -0.325487506881605 -33.7289253084676
> Where the first four columns are year, month, day and hour. The last
> four columns are the parameters of interest which I am trying to plot,
> taking two at a time.
> I will remain indebted for your assistance.
> Best Regards
> Ogbos
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From g||ted|||e2014 @end|ng |rom gm@||@com  Thu Nov 21 11:25:54 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Thu, 21 Nov 2019 11:25:54 +0100
Subject: [R] Datetime in doubly y-axis plot: Solved
In-Reply-To: <CA+8X3fVSvsUhNdvO4RvJBH0gc9OZ+u9_gA2==ECA0GjEvJ2MSA@mail.gmail.com>
References: <CAC8ss30zt3mW4bxkBEtG-1eo3e5uy_KTKqLvaAb0xqtfmmpm3g@mail.gmail.com>
 <CA+8X3fVSvsUhNdvO4RvJBH0gc9OZ+u9_gA2==ECA0GjEvJ2MSA@mail.gmail.com>
Message-ID: <CAC8ss33zJ2=y9TYee2Y0S_qfTKxvua1ezY=rqOxiXRVri-=aDw@mail.gmail.com>

Dear Jim and Petr,

Thank you.

The last code by Jim worked like magic. A great relief. Thank you very much.
Best wishes
Ogbos

On Thu, Nov 21, 2019 at 10:59 AM Jim Lemon <drjimlemon at gmail.com> wrote:
>
> Hi Ogbos,
> As Petr says, you can use twoord.plot for this:
>
> dta<-read.table(text="year month day hour Li CR x1 x2
> 04 12 31 10  8637 4992 0.310676228913214 49.7113438132607
> 04 12 31 11  8634 4183 0.275834035016405 25.4492290005748
> 04 12 31 12  8652 4160 0.484887198397259 24.7594531777172
> 04 12 31 13  8626 3411 0.182921517958247 2.2967535550946
> 04 12 31 15  8618 3904 0.09000900090009 17.0819483667808
> 04 12 31 16  8620 3428 0.113237130164629 2.80658785894585
> 04 12 31 17  8603 3041 -0.0842019685839552 -8.79964011696198
> 04 12 31 18  8579 2340 -0.362939519758427 -29.8228075875341
> 04 12 31 19  8588 3006 -0.258412938068 -9.8492989778322
> 04 12 31 20  8604 3280 -0.0725879039516855 -1.63196960987679
> 04 12 31 21  8568 3696 -0.490694230713394 10.8439757078949
> 04 12 31 22  8567 2307 -0.502308295345664 -30.8124859420688
> 04 12 31 23  8570 2726 -0.467466101448855 -18.2465698647939
> 05 01 01 00  8579 2879 -0.568878758318289 8.46755203918244
> 05 01 01 01  8581 1586 -0.545698639133843 -40.2467740416313
> 05 01 01 02  8562 1183 -0.765909771386082 -55.4299708015447
> 05 01 01 03  8602 1491 -0.302307387697159 -43.8259395309409
> 05 01 01 04  8576 1742 -0.603648937094958 -34.369407553923
> 05 01 01 05  8622 2001 -0.0705061958526974 -24.6114721672789
> 05 01 01 06  8613 2675 -0.174816732182705 0.781765093717623
> 05 01 01 07  8622 3293 -0.0705061958526974 24.0651784873316
> 05 01 01 08  8628 4333 -0.00096583829935895 63.2476217387209
> 05 01 01 09  8669 16 0.474226604981787 -99.3971931807479
> 05 01 01 10  8698 3671 0.810338333156256 38.3064895921635
> 05 01 01 11  8671 3856 0.497406724166233 45.2764434397664
> 05 01 01 12  8679 3411 0.590127200904018 28.5108787793162
> 05 01 01 13  8647 3428 0.219245293952879 29.1513610247716
> 05 01 01 14  8651 4596 0.265605532321772 73.156258830178
> 05 01 01 15  8638 3154 0.114934757622872 18.8282942450786
> 05 01 01 16  8635 2365 0.0801645788462025 -10.8976170292926
> 05 01 01 17  8628 2697 -0.00096583829935895 1.61062447018932
> 05 01 01 18  8653 3231 0.288785651506218 21.7293020627296
> 05 01 01 19  8657 3548 0.33514588987511 33.6724121691627
> 05 01 01 20  8650 2645 0.254015472729549 -0.348497692380145
> 05 01 01 21  8605 2153 -0.26753720892049 -18.8848073843835
> 05 01 01 22  8600 1759 -0.325487506881605 -33.7289253084676",
> header=TRUE,stringsAsFactors=FALSE)
> library(plotrix)
> dta$year<-ifelse(dta$year < 50,dta$year+2000,dta$year+1900)
> dta$date<-strptime(paste(paste(paste(dta$year,dta$month,dta$day,sep="-"),
>  paste(dta$hour,0,0,sep=":"),sep=" ")),"%Y-%m-%d %H:%M:%S")
> xlim<-range(as.numeric(dta$date))
> xorigin<-as.POSIXct("1970-01-01 0:0:0","%Y-%m-%d %H:%M:%S")
> xticks<-as.POSIXct(pretty(range(as.numeric(dta$date))),origin=xorigin)
> twoord.plot(dta$date,dta$Li,dta$date,dta$CR,xlim=xlim,xaxt="n",
>  main="Lightning and GCR frequency",
>  xlab="Date",ylab="Lightning count/day",rylab="GCR count/day",
>  xtickpos=xticks,xticklab=format(xticks,"%Y-%m-%d %H"))
>
> Jim
>
> On Thu, Nov 21, 2019 at 7:49 PM Ogbos Okike <giftedlife2014 at gmail.com> wrote:
> >
> > Dear Members,
> > I have some hourly data. Usin:
> > dta$year <- with( dta, ifelse(year < 50, year + 2000, year + 1900))
> > dta$datetime <- with( dta, as.POSIXct(ISOdatetime(year, month,day,hour,0,0)))
> > I converted the hour to time format and stored in Year.
> > The data consists of two different observations and I wish to plot
> > same on one graph (two y-axis and common x-axis).
> >


From m@|||P@dpo@t @end|ng |rom gm@||@com  Thu Nov 21 19:43:15 2019
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Thu, 21 Nov 2019 21:43:15 +0300
Subject: [R] Read shp file
Message-ID: <CAH6117KyOb+nP3NrQiSQ_pfQj6yDSUsc9Qh3r2XATBSytC0zYg@mail.gmail.com>

Help me. pls, to read .shp file.

`library("tmaptools")
geo <- read_shape("Rom.shp", as.sf = TRUE)
This function is deprecated and has been migrated to
github.com/mtennekes/oldtmaptools`

I have to turn to another function, but I get an unclear message

`library(raster)
geo <- shapefile ("Rus.shp")
Error in .local(x, ...) : file.exists(extension(x, ".shx")) is not TRUE`


From jmh@nnon@ucd@v|@ @end|ng |rom gm@||@com  Fri Nov 22 01:52:59 2019
From: jmh@nnon@ucd@v|@ @end|ng |rom gm@||@com (Michael Hannon)
Date: Thu, 21 Nov 2019 16:52:59 -0800
Subject: [R] Read shp file
In-Reply-To: <CAH6117KyOb+nP3NrQiSQ_pfQj6yDSUsc9Qh3r2XATBSytC0zYg@mail.gmail.com>
References: <CAH6117KyOb+nP3NrQiSQ_pfQj6yDSUsc9Qh3r2XATBSytC0zYg@mail.gmail.com>
Message-ID: <CACdH2ZZCik7H5xVqPysNYfpjhgw-mRtDix4dxpif-raS90Jz=A@mail.gmail.com>

I can't help you locate the .shx file, but the gist of it is that a
"shapefile" actually requires a minimum of three files:

https://knowledge.autodesk.com/support/autocad-map-3d/learn-explore/caas/sfdcarticles/sfdcarticles/Required-files-that-make-up-a-shapefile.html

The .shx file is an index of the .shp file.

-- Mike

On Thu, Nov 21, 2019 at 10:44 AM Medic <mailiPadpost at gmail.com> wrote:
>
> Help me. pls, to read .shp file.
>
> `library("tmaptools")
> geo <- read_shape("Rom.shp", as.sf = TRUE)
> This function is deprecated and has been migrated to
> github.com/mtennekes/oldtmaptools`
>
> I have to turn to another function, but I get an unclear message
>
> `library(raster)
> geo <- shapefile ("Rus.shp")
> Error in .local(x, ...) : file.exists(extension(x, ".shx")) is not TRUE`
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@|||P@dpo@t @end|ng |rom gm@||@com  Fri Nov 22 03:54:33 2019
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Fri, 22 Nov 2019 05:54:33 +0300
Subject: [R] Creating map in R
Message-ID: <CAH6117LRT_vYS8OGXJagXf5f=VsZGQz34Vngt+_p6p-3vZ4QWg@mail.gmail.com>

Creating map in R. I have all the files (with different extensions)
for the country I need. The problem is that I don?t know how to use
them in R. I began according to information on the Internet, and
immediately ran into difficulties (see below):

`library("tmaptools")
geo <- read_shape("Rom.shp", as.sf = TRUE)
This function is deprecated and has been migrated to
github.com/mtennekes/oldtmaptools`

I turned to a similar function, but get an UNCLEAR message

`library(raster)
geo <- shapefile ("Rus.shp")
Error in .local(x, ...) : file.exists(extension(x, ".shx")) is not TRUE`


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Fri Nov 22 06:47:28 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Fri, 22 Nov 2019 06:47:28 +0100
Subject: [R] Creating map in R
In-Reply-To: <CAH6117LRT_vYS8OGXJagXf5f=VsZGQz34Vngt+_p6p-3vZ4QWg@mail.gmail.com>
References: <CAH6117LRT_vYS8OGXJagXf5f=VsZGQz34Vngt+_p6p-3vZ4QWg@mail.gmail.com>
Message-ID: <2FB49723-BDA7-4AE8-9D1F-52E69300D4FA@dcn.davis.ca.us>

If you READ the Posting Guide mentioned at the bottom of each post you would know that there is a more appropriate mailing list for questions about maps and R where experts in that topic hang out.

On November 22, 2019 3:54:33 AM GMT+01:00, Medic <mailiPadpost at gmail.com> wrote:
>Creating map in R. I have all the files (with different extensions)
>for the country I need. The problem is that I don?t know how to use
>them in R. I began according to information on the Internet, and
>immediately ran into difficulties (see below):
>
>`library("tmaptools")
>geo <- read_shape("Rom.shp", as.sf = TRUE)
>This function is deprecated and has been migrated to
>github.com/mtennekes/oldtmaptools`
>
>I turned to a similar function, but get an UNCLEAR message
>
>`library(raster)
>geo <- shapefile ("Rus.shp")
>Error in .local(x, ...) : file.exists(extension(x, ".shx")) is not
>TRUE`
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From jmh@nnon@ucd@v|@ @end|ng |rom gm@||@com  Fri Nov 22 11:23:36 2019
From: jmh@nnon@ucd@v|@ @end|ng |rom gm@||@com (Michael Hannon)
Date: Fri, 22 Nov 2019 02:23:36 -0800
Subject: [R] Creating map in R
In-Reply-To: <2FB49723-BDA7-4AE8-9D1F-52E69300D4FA@dcn.davis.ca.us>
References: <CAH6117LRT_vYS8OGXJagXf5f=VsZGQz34Vngt+_p6p-3vZ4QWg@mail.gmail.com>
 <2FB49723-BDA7-4AE8-9D1F-52E69300D4FA@dcn.davis.ca.us>
Message-ID: <CACdH2ZaHR60+YiUh=v04VS0t2mJZ5-DbZXwTEBMHJ+XeBCr0hw@mail.gmail.com>

Yep, I had that same thought after I posted a reply to the OP.  Not to
discourage anybody from reading the posting guide, but a quick look at
it suggests that the following might be useful in this case:

https://stat.ethz.ch/mailman/listinfo/r-sig-geo

On Thu, Nov 21, 2019 at 9:48 PM Jeff Newmiller <jdnewmil at dcn.davis.ca.us> wrote:
>
> If you READ the Posting Guide mentioned at the bottom of each post you would know that there is a more appropriate mailing list for questions about maps and R where experts in that topic hang out.
>
> On November 22, 2019 3:54:33 AM GMT+01:00, Medic <mailiPadpost at gmail.com> wrote:
> >Creating map in R. I have all the files (with different extensions)
> >for the country I need. The problem is that I don?t know how to use
> >them in R. I began according to information on the Internet, and
> >immediately ran into difficulties (see below):
> >
> >`library("tmaptools")
> >geo <- read_shape("Rom.shp", as.sf = TRUE)
> >This function is deprecated and has been migrated to
> >github.com/mtennekes/oldtmaptools`
> >
> >I turned to a similar function, but get an UNCLEAR message
> >
> >`library(raster)
> >geo <- shapefile ("Rus.shp")
> >Error in .local(x, ...) : file.exists(extension(x, ".shx")) is not
> >TRUE`
> >
> >______________________________________________
> >R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide
> >http://www.R-project.org/posting-guide.html
> >and provide commented, minimal, self-contained, reproducible code.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From m@|||P@dpo@t @end|ng |rom gm@||@com  Fri Nov 22 16:41:04 2019
From: m@|||P@dpo@t @end|ng |rom gm@||@com (Medic)
Date: Fri, 22 Nov 2019 18:41:04 +0300
Subject: [R] Creating map in R
Message-ID: <CAH6117Kk1s8X6LJaHMfov78nbwCLdBVPUa_KBD0hVT2zOkWGUA@mail.gmail.com>

Thank you very much!
> From: Michael Hannon <jmhannon.ucdavis at gmail.com>
> ... following might be useful in this case:
> https://stat.ethz.ch/mailman/listinfo/r-sig-geo


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 02:17:54 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 22 Nov 2019 19:17:54 -0600
Subject: [R] cannot open file '--no-restore.matrix'
Message-ID: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>

Hello,

I am trying to run this code:
https://github.com/eleporcu/TWMR/blob/master/MR.R

with r-3.6.1

via:

Rscript MR.R --no-save ENSG00000154803

in the current directory I have saved: ENSG00000154803.ld and
ENSG00000154803.matrix as the software requires

but I am getting this error:

Error in file(file, "rt") : cannot open the connection
Calls: read.table -> file
In addition: Warning message:
In file(file, "rt") :
  cannot open file '--no-restore.matrix': No such file or directory
Execution halted


Please advise,

Thanks
Ana


From btupper @end|ng |rom b|ge|ow@org  Sat Nov 23 02:44:18 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Fri, 22 Nov 2019 20:44:18 -0500
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
Message-ID: <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>

Hi,

You might check the order of your arguments.   Options come before the
script filename. See the details here...

https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript

Ben

On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> Hello,
>
> I am trying to run this code:
> https://github.com/eleporcu/TWMR/blob/master/MR.R
>
> with r-3.6.1
>
> via:
>
> Rscript MR.R --no-save ENSG00000154803
>
> in the current directory I have saved: ENSG00000154803.ld and
> ENSG00000154803.matrix as the software requires
>
> but I am getting this error:
>
> Error in file(file, "rt") : cannot open the connection
> Calls: read.table -> file
> In addition: Warning message:
> In file(file, "rt") :
>   cannot open file '--no-restore.matrix': No such file or directory
> Execution halted
>
>
> Please advise,
>
> Thanks
> Ana
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
West Boothbay Harbor, Maine
http://www.bigelow.org/
https://eco.bigelow.org


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 03:00:08 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Fri, 22 Nov 2019 20:00:08 -0600
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
Message-ID: <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>

HI Ben,

thank you so much , I did this:

Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
Error: unexpected numeric constant in "1.000 0.089"
Execution halted

I made ENSG00000154803.ld with:
library(MASS)
write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")

and it looks like this:

1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
0.000 0.000 0.000 0.001 0.003 0.000
0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
0.001 0.012 0.005 0.000 0.004 0.004
0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
0.054 0.006 0.002 0.010 0.001 0.000
0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
0.066 0.010 0.030 0.001 0.003 0.000
0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
0.002 0.003 0.066 0.006 0.004 0.004
0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
0.016 0.000 0.025 0.000 0.005 0.000
0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
0.128 0.019 0.005 0.030 0.002 0.016
0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
0.098 0.010 0.012 0.001 0.006 0.003
0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
0.012 0.000 0.006 0.018 0.004 0.013
0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
0.001 0.091 0.057 0.062 0.002 0.005
0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
0.238 0.180 0.073 0.058 0.000 0.006
0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
1.000 0.158 0.006 0.044 0.006 0.001
0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
0.158 1.000 0.077 0.237 0.009 0.000
0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
0.006 0.077 1.000 0.056 0.000 0.004
0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
0.044 0.237 0.056 1.000 0.000 0.003
0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
0.006 0.009 0.000 0.000 1.000 0.002
0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
0.001 0.000 0.004 0.003 0.002 1.000

the other file (ENSG00000154803.matrix) looks like this:

GENES ENSG00000154803 BETA_GWAS
rs12601631 -0.320577 -0.0160778
rs1708623 0.708706 0.0717719
rs1708628 -0.645996 -0.0973019
rs17804843 -0.78984 0.0059607
rs4078062 -0.340732 -0.0716837
rs4316813 -0.721137 -0.00502219
rs7217764 -0.61641 0.16997
rs7221842 -0.377727 -0.00184011
rs12602831 -0.397059 0.0154625
rs138437542 -0.590669 0.0145733
rs2174369 -0.167913 -0.0268728
rs242252 0.20184 0.0161709
rs34121330 0.328602 0.0753894
rs4792798 -0.303601 0.00227314
rs7222311 -0.367686 -0.0419168
rs74369938 0.687555 -0.223105
rs8075751 -0.261916 -0.0313484

On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
>
> Hi,
>
> You might check the order of your arguments.   Options come before the
> script filename. See the details here...
>
> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
>
> Ben
>
> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > Hello,
> >
> > I am trying to run this code:
> > https://github.com/eleporcu/TWMR/blob/master/MR.R
> >
> > with r-3.6.1
> >
> > via:
> >
> > Rscript MR.R --no-save ENSG00000154803
> >
> > in the current directory I have saved: ENSG00000154803.ld and
> > ENSG00000154803.matrix as the software requires
> >
> > but I am getting this error:
> >
> > Error in file(file, "rt") : cannot open the connection
> > Calls: read.table -> file
> > In addition: Warning message:
> > In file(file, "rt") :
> >   cannot open file '--no-restore.matrix': No such file or directory
> > Execution halted
> >
> >
> > Please advise,
> >
> > Thanks
> > Ana
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Ben Tupper
> Bigelow Laboratory for Ocean Science
> West Boothbay Harbor, Maine
> http://www.bigelow.org/
> https://eco.bigelow.org


From btupper @end|ng |rom b|ge|ow@org  Sat Nov 23 11:16:37 2019
From: btupper @end|ng |rom b|ge|ow@org (Ben Tupper)
Date: Sat, 23 Nov 2019 05:16:37 -0500
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
Message-ID: <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>

Hi,

I think you want this order...

Rscript [options for R] script_file.R argument_1 argument_2 ...

So, like this ...

Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix

Cheers,
Ben

On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> HI Ben,
>
> thank you so much , I did this:
>
> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> Error: unexpected numeric constant in "1.000 0.089"
> Execution halted
>
> I made ENSG00000154803.ld with:
> library(MASS)
> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
>
> and it looks like this:
>
> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> 0.000 0.000 0.000 0.001 0.003 0.000
> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> 0.001 0.012 0.005 0.000 0.004 0.004
> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> 0.054 0.006 0.002 0.010 0.001 0.000
> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> 0.066 0.010 0.030 0.001 0.003 0.000
> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> 0.002 0.003 0.066 0.006 0.004 0.004
> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> 0.016 0.000 0.025 0.000 0.005 0.000
> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> 0.128 0.019 0.005 0.030 0.002 0.016
> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> 0.098 0.010 0.012 0.001 0.006 0.003
> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> 0.012 0.000 0.006 0.018 0.004 0.013
> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> 0.001 0.091 0.057 0.062 0.002 0.005
> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> 0.238 0.180 0.073 0.058 0.000 0.006
> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> 1.000 0.158 0.006 0.044 0.006 0.001
> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> 0.158 1.000 0.077 0.237 0.009 0.000
> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> 0.006 0.077 1.000 0.056 0.000 0.004
> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> 0.044 0.237 0.056 1.000 0.000 0.003
> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> 0.006 0.009 0.000 0.000 1.000 0.002
> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> 0.001 0.000 0.004 0.003 0.002 1.000
>
> the other file (ENSG00000154803.matrix) looks like this:
>
> GENES ENSG00000154803 BETA_GWAS
> rs12601631 -0.320577 -0.0160778
> rs1708623 0.708706 0.0717719
> rs1708628 -0.645996 -0.0973019
> rs17804843 -0.78984 0.0059607
> rs4078062 -0.340732 -0.0716837
> rs4316813 -0.721137 -0.00502219
> rs7217764 -0.61641 0.16997
> rs7221842 -0.377727 -0.00184011
> rs12602831 -0.397059 0.0154625
> rs138437542 -0.590669 0.0145733
> rs2174369 -0.167913 -0.0268728
> rs242252 0.20184 0.0161709
> rs34121330 0.328602 0.0753894
> rs4792798 -0.303601 0.00227314
> rs7222311 -0.367686 -0.0419168
> rs74369938 0.687555 -0.223105
> rs8075751 -0.261916 -0.0313484
>
> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> >
> > Hi,
> >
> > You might check the order of your arguments.   Options come before the
> > script filename. See the details here...
> >
> > https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> >
> > Ben
> >
> > On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >
> > > Hello,
> > >
> > > I am trying to run this code:
> > > https://github.com/eleporcu/TWMR/blob/master/MR.R
> > >
> > > with r-3.6.1
> > >
> > > via:
> > >
> > > Rscript MR.R --no-save ENSG00000154803
> > >
> > > in the current directory I have saved: ENSG00000154803.ld and
> > > ENSG00000154803.matrix as the software requires
> > >
> > > but I am getting this error:
> > >
> > > Error in file(file, "rt") : cannot open the connection
> > > Calls: read.table -> file
> > > In addition: Warning message:
> > > In file(file, "rt") :
> > >   cannot open file '--no-restore.matrix': No such file or directory
> > > Execution halted
> > >
> > >
> > > Please advise,
> > >
> > > Thanks
> > > Ana
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > Ben Tupper
> > Bigelow Laboratory for Ocean Science
> > West Boothbay Harbor, Maine
> > http://www.bigelow.org/
> > https://eco.bigelow.org



-- 
Ben Tupper
Bigelow Laboratory for Ocean Science
West Boothbay Harbor, Maine
http://www.bigelow.org/
https://eco.bigelow.org


From ggrothend|eck @end|ng |rom gm@||@com  Sat Nov 23 14:24:54 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Sat, 23 Nov 2019 08:24:54 -0500
Subject: [R] giving priority to stats package
Message-ID: <CAP01uRnzNN0Bcs-d8XHyBf7y4FgrAVgXWDt+zpiq=NaijZLRKA@mail.gmail.com>

library and require have new args in 3.6 giving additional control
over conflicts.  This seems very useful but I was wondering if there
were some, preferabley simple, way to give existing loaded packages
priority without knowing the actual conflicts in advance.  For example

    library(dplyr, exclude = c("filter", "lag"))

works to avoid masking those names in stats that would otherwise be
masked by that package but I had to know in advance that filter and
lag were the conflicting names.

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 16:26:46 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Sat, 23 Nov 2019 09:26:46 -0600
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
Message-ID: <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>

HI Ben,

I tried it but it doesn't work:

Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
Error in file(file, "rt") : cannot open the connection
Calls: read.table -> file
In addition: Warning message:
In file(file, "rt") :
  cannot open file '--no-restore.matrix': No such file or directory
Execution halted

Please advise,
Ana

On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
>
> Hi,
>
> I think you want this order...
>
> Rscript [options for R] script_file.R argument_1 argument_2 ...
>
> So, like this ...
>
> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>
> Cheers,
> Ben
>
> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > HI Ben,
> >
> > thank you so much , I did this:
> >
> > Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> > Error: unexpected numeric constant in "1.000 0.089"
> > Execution halted
> >
> > I made ENSG00000154803.ld with:
> > library(MASS)
> > write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> >
> > and it looks like this:
> >
> > 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> > 0.000 0.000 0.000 0.001 0.003 0.000
> > 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> > 0.001 0.012 0.005 0.000 0.004 0.004
> > 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> > 0.054 0.006 0.002 0.010 0.001 0.000
> > 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> > 0.066 0.010 0.030 0.001 0.003 0.000
> > 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> > 0.002 0.003 0.066 0.006 0.004 0.004
> > 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> > 0.016 0.000 0.025 0.000 0.005 0.000
> > 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> > 0.128 0.019 0.005 0.030 0.002 0.016
> > 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> > 0.098 0.010 0.012 0.001 0.006 0.003
> > 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> > 0.012 0.000 0.006 0.018 0.004 0.013
> > 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> > 0.001 0.091 0.057 0.062 0.002 0.005
> > 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> > 0.238 0.180 0.073 0.058 0.000 0.006
> > 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> > 1.000 0.158 0.006 0.044 0.006 0.001
> > 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> > 0.158 1.000 0.077 0.237 0.009 0.000
> > 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> > 0.006 0.077 1.000 0.056 0.000 0.004
> > 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> > 0.044 0.237 0.056 1.000 0.000 0.003
> > 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> > 0.006 0.009 0.000 0.000 1.000 0.002
> > 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> > 0.001 0.000 0.004 0.003 0.002 1.000
> >
> > the other file (ENSG00000154803.matrix) looks like this:
> >
> > GENES ENSG00000154803 BETA_GWAS
> > rs12601631 -0.320577 -0.0160778
> > rs1708623 0.708706 0.0717719
> > rs1708628 -0.645996 -0.0973019
> > rs17804843 -0.78984 0.0059607
> > rs4078062 -0.340732 -0.0716837
> > rs4316813 -0.721137 -0.00502219
> > rs7217764 -0.61641 0.16997
> > rs7221842 -0.377727 -0.00184011
> > rs12602831 -0.397059 0.0154625
> > rs138437542 -0.590669 0.0145733
> > rs2174369 -0.167913 -0.0268728
> > rs242252 0.20184 0.0161709
> > rs34121330 0.328602 0.0753894
> > rs4792798 -0.303601 0.00227314
> > rs7222311 -0.367686 -0.0419168
> > rs74369938 0.687555 -0.223105
> > rs8075751 -0.261916 -0.0313484
> >
> > On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> > >
> > > Hi,
> > >
> > > You might check the order of your arguments.   Options come before the
> > > script filename. See the details here...
> > >
> > > https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> > >
> > > Ben
> > >
> > > On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >
> > > > Hello,
> > > >
> > > > I am trying to run this code:
> > > > https://github.com/eleporcu/TWMR/blob/master/MR.R
> > > >
> > > > with r-3.6.1
> > > >
> > > > via:
> > > >
> > > > Rscript MR.R --no-save ENSG00000154803
> > > >
> > > > in the current directory I have saved: ENSG00000154803.ld and
> > > > ENSG00000154803.matrix as the software requires
> > > >
> > > > but I am getting this error:
> > > >
> > > > Error in file(file, "rt") : cannot open the connection
> > > > Calls: read.table -> file
> > > > In addition: Warning message:
> > > > In file(file, "rt") :
> > > >   cannot open file '--no-restore.matrix': No such file or directory
> > > > Execution halted
> > > >
> > > >
> > > > Please advise,
> > > >
> > > > Thanks
> > > > Ana
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > >
> > >
> > > --
> > > Ben Tupper
> > > Bigelow Laboratory for Ocean Science
> > > West Boothbay Harbor, Maine
> > > http://www.bigelow.org/
> > > https://eco.bigelow.org
>
>
>
> --
> Ben Tupper
> Bigelow Laboratory for Ocean Science
> West Boothbay Harbor, Maine
> http://www.bigelow.org/
> https://eco.bigelow.org


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Nov 23 16:44:41 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 23 Nov 2019 10:44:41 -0500
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
Message-ID: <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>

On 23/11/2019 10:26 a.m., Ana Marija wrote:
> HI Ben,
> 
> I tried it but it doesn't work:
> 
> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> Error in file(file, "rt") : cannot open the connection
> Calls: read.table -> file
> In addition: Warning message:
> In file(file, "rt") :
>    cannot open file '--no-restore.matrix': No such file or directory
> Execution halted
> 

You should print the cmd_args variable that is set on the first line of 
that script.  When I run a script that prints it using your command 
line, this is what it looks like:

$ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
[1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
[2] "--slave"
[3] "--no-restore"
[4] "--no-save"
[5] "--file=MR.R"
[6] "--args"
[7] "ENSG00000154803.ld"
[8] "ENSG00000154803.matrix"

The next line

gene <- cmd_args[3]

is obviously wrong for my system, because it would set gene to 
"--no-restore".  Your results will probably be somewhat different, but 
it might be clear what you should use instead of the third element.

By the way, changing the first line

cmd_args=commandArgs()

to

cmd_args <- commandArgs(TRUE)

makes a lot of sense in most cases.  I haven't read your whole script so 
I don't know it it makes sense for you.

Duncan Murdoch


> Please advise,
> Ana
> 
> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
>>
>> Hi,
>>
>> I think you want this order...
>>
>> Rscript [options for R] script_file.R argument_1 argument_2 ...
>>
>> So, like this ...
>>
>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>
>> Cheers,
>> Ben
>>
>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>
>>> HI Ben,
>>>
>>> thank you so much , I did this:
>>>
>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
>>> Error: unexpected numeric constant in "1.000 0.089"
>>> Execution halted
>>>
>>> I made ENSG00000154803.ld with:
>>> library(MASS)
>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
>>>
>>> and it looks like this:
>>>
>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
>>> 0.000 0.000 0.000 0.001 0.003 0.000
>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
>>> 0.001 0.012 0.005 0.000 0.004 0.004
>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
>>> 0.054 0.006 0.002 0.010 0.001 0.000
>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
>>> 0.066 0.010 0.030 0.001 0.003 0.000
>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
>>> 0.002 0.003 0.066 0.006 0.004 0.004
>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
>>> 0.016 0.000 0.025 0.000 0.005 0.000
>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
>>> 0.128 0.019 0.005 0.030 0.002 0.016
>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
>>> 0.098 0.010 0.012 0.001 0.006 0.003
>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
>>> 0.012 0.000 0.006 0.018 0.004 0.013
>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
>>> 0.001 0.091 0.057 0.062 0.002 0.005
>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
>>> 0.238 0.180 0.073 0.058 0.000 0.006
>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
>>> 1.000 0.158 0.006 0.044 0.006 0.001
>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
>>> 0.158 1.000 0.077 0.237 0.009 0.000
>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
>>> 0.006 0.077 1.000 0.056 0.000 0.004
>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
>>> 0.044 0.237 0.056 1.000 0.000 0.003
>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
>>> 0.006 0.009 0.000 0.000 1.000 0.002
>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
>>> 0.001 0.000 0.004 0.003 0.002 1.000
>>>
>>> the other file (ENSG00000154803.matrix) looks like this:
>>>
>>> GENES ENSG00000154803 BETA_GWAS
>>> rs12601631 -0.320577 -0.0160778
>>> rs1708623 0.708706 0.0717719
>>> rs1708628 -0.645996 -0.0973019
>>> rs17804843 -0.78984 0.0059607
>>> rs4078062 -0.340732 -0.0716837
>>> rs4316813 -0.721137 -0.00502219
>>> rs7217764 -0.61641 0.16997
>>> rs7221842 -0.377727 -0.00184011
>>> rs12602831 -0.397059 0.0154625
>>> rs138437542 -0.590669 0.0145733
>>> rs2174369 -0.167913 -0.0268728
>>> rs242252 0.20184 0.0161709
>>> rs34121330 0.328602 0.0753894
>>> rs4792798 -0.303601 0.00227314
>>> rs7222311 -0.367686 -0.0419168
>>> rs74369938 0.687555 -0.223105
>>> rs8075751 -0.261916 -0.0313484
>>>
>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
>>>>
>>>> Hi,
>>>>
>>>> You might check the order of your arguments.   Options come before the
>>>> script filename. See the details here...
>>>>
>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
>>>>
>>>> Ben
>>>>
>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>>>
>>>>> Hello,
>>>>>
>>>>> I am trying to run this code:
>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
>>>>>
>>>>> with r-3.6.1
>>>>>
>>>>> via:
>>>>>
>>>>> Rscript MR.R --no-save ENSG00000154803
>>>>>
>>>>> in the current directory I have saved: ENSG00000154803.ld and
>>>>> ENSG00000154803.matrix as the software requires
>>>>>
>>>>> but I am getting this error:
>>>>>
>>>>> Error in file(file, "rt") : cannot open the connection
>>>>> Calls: read.table -> file
>>>>> In addition: Warning message:
>>>>> In file(file, "rt") :
>>>>>    cannot open file '--no-restore.matrix': No such file or directory
>>>>> Execution halted
>>>>>
>>>>>
>>>>> Please advise,
>>>>>
>>>>> Thanks
>>>>> Ana
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>>
>>>> --
>>>> Ben Tupper
>>>> Bigelow Laboratory for Ocean Science
>>>> West Boothbay Harbor, Maine
>>>> http://www.bigelow.org/
>>>> https://eco.bigelow.org
>>
>>
>>
>> --
>> Ben Tupper
>> Bigelow Laboratory for Ocean Science
>> West Boothbay Harbor, Maine
>> http://www.bigelow.org/
>> https://eco.bigelow.org
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 17:05:19 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Sat, 23 Nov 2019 10:05:19 -0600
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
Message-ID: <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>

Hi Ben,

I am not sure what you mean when you say to print, is it this?

> cmd_args=commandArgs(TRUE)
> print(cmd_args)
character(0)
> cmd_args=commandArgs()
> print(cmd_args)
[1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"

I changed in the first line of this script:
https://github.com/eleporcu/TWMR/blob/master/MR.R

cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)

but again I get the same error:

Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
Error in file(file, "rt") : cannot open the connection
Calls: read.table -> file
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'NA.matrix': No such file or directory
Execution halted


Please advise,
Ana

On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> On 23/11/2019 10:26 a.m., Ana Marija wrote:
> > HI Ben,
> >
> > I tried it but it doesn't work:
> >
> > Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > Error in file(file, "rt") : cannot open the connection
> > Calls: read.table -> file
> > In addition: Warning message:
> > In file(file, "rt") :
> >    cannot open file '--no-restore.matrix': No such file or directory
> > Execution halted
> >
>
> You should print the cmd_args variable that is set on the first line of
> that script.  When I run a script that prints it using your command
> line, this is what it looks like:
>
> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
> [2] "--slave"
> [3] "--no-restore"
> [4] "--no-save"
> [5] "--file=MR.R"
> [6] "--args"
> [7] "ENSG00000154803.ld"
> [8] "ENSG00000154803.matrix"
>
> The next line
>
> gene <- cmd_args[3]
>
> is obviously wrong for my system, because it would set gene to
> "--no-restore".  Your results will probably be somewhat different, but
> it might be clear what you should use instead of the third element.
>
> By the way, changing the first line
>
> cmd_args=commandArgs()
>
> to
>
> cmd_args <- commandArgs(TRUE)
>
> makes a lot of sense in most cases.  I haven't read your whole script so
> I don't know it it makes sense for you.
>
> Duncan Murdoch
>
>
> > Please advise,
> > Ana
> >
> > On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
> >>
> >> Hi,
> >>
> >> I think you want this order...
> >>
> >> Rscript [options for R] script_file.R argument_1 argument_2 ...
> >>
> >> So, like this ...
> >>
> >> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>
> >> Cheers,
> >> Ben
> >>
> >> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>
> >>> HI Ben,
> >>>
> >>> thank you so much , I did this:
> >>>
> >>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> >>> Error: unexpected numeric constant in "1.000 0.089"
> >>> Execution halted
> >>>
> >>> I made ENSG00000154803.ld with:
> >>> library(MASS)
> >>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> >>>
> >>> and it looks like this:
> >>>
> >>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> >>> 0.000 0.000 0.000 0.001 0.003 0.000
> >>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> >>> 0.001 0.012 0.005 0.000 0.004 0.004
> >>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> >>> 0.054 0.006 0.002 0.010 0.001 0.000
> >>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> >>> 0.066 0.010 0.030 0.001 0.003 0.000
> >>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> >>> 0.002 0.003 0.066 0.006 0.004 0.004
> >>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> >>> 0.016 0.000 0.025 0.000 0.005 0.000
> >>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> >>> 0.128 0.019 0.005 0.030 0.002 0.016
> >>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> >>> 0.098 0.010 0.012 0.001 0.006 0.003
> >>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> >>> 0.012 0.000 0.006 0.018 0.004 0.013
> >>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> >>> 0.001 0.091 0.057 0.062 0.002 0.005
> >>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> >>> 0.238 0.180 0.073 0.058 0.000 0.006
> >>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> >>> 1.000 0.158 0.006 0.044 0.006 0.001
> >>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> >>> 0.158 1.000 0.077 0.237 0.009 0.000
> >>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> >>> 0.006 0.077 1.000 0.056 0.000 0.004
> >>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> >>> 0.044 0.237 0.056 1.000 0.000 0.003
> >>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> >>> 0.006 0.009 0.000 0.000 1.000 0.002
> >>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> >>> 0.001 0.000 0.004 0.003 0.002 1.000
> >>>
> >>> the other file (ENSG00000154803.matrix) looks like this:
> >>>
> >>> GENES ENSG00000154803 BETA_GWAS
> >>> rs12601631 -0.320577 -0.0160778
> >>> rs1708623 0.708706 0.0717719
> >>> rs1708628 -0.645996 -0.0973019
> >>> rs17804843 -0.78984 0.0059607
> >>> rs4078062 -0.340732 -0.0716837
> >>> rs4316813 -0.721137 -0.00502219
> >>> rs7217764 -0.61641 0.16997
> >>> rs7221842 -0.377727 -0.00184011
> >>> rs12602831 -0.397059 0.0154625
> >>> rs138437542 -0.590669 0.0145733
> >>> rs2174369 -0.167913 -0.0268728
> >>> rs242252 0.20184 0.0161709
> >>> rs34121330 0.328602 0.0753894
> >>> rs4792798 -0.303601 0.00227314
> >>> rs7222311 -0.367686 -0.0419168
> >>> rs74369938 0.687555 -0.223105
> >>> rs8075751 -0.261916 -0.0313484
> >>>
> >>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> >>>>
> >>>> Hi,
> >>>>
> >>>> You might check the order of your arguments.   Options come before the
> >>>> script filename. See the details here...
> >>>>
> >>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> >>>>
> >>>> Ben
> >>>>
> >>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>>>
> >>>>> Hello,
> >>>>>
> >>>>> I am trying to run this code:
> >>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> >>>>>
> >>>>> with r-3.6.1
> >>>>>
> >>>>> via:
> >>>>>
> >>>>> Rscript MR.R --no-save ENSG00000154803
> >>>>>
> >>>>> in the current directory I have saved: ENSG00000154803.ld and
> >>>>> ENSG00000154803.matrix as the software requires
> >>>>>
> >>>>> but I am getting this error:
> >>>>>
> >>>>> Error in file(file, "rt") : cannot open the connection
> >>>>> Calls: read.table -> file
> >>>>> In addition: Warning message:
> >>>>> In file(file, "rt") :
> >>>>>    cannot open file '--no-restore.matrix': No such file or directory
> >>>>> Execution halted
> >>>>>
> >>>>>
> >>>>> Please advise,
> >>>>>
> >>>>> Thanks
> >>>>> Ana
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>>
> >>>>
> >>>> --
> >>>> Ben Tupper
> >>>> Bigelow Laboratory for Ocean Science
> >>>> West Boothbay Harbor, Maine
> >>>> http://www.bigelow.org/
> >>>> https://eco.bigelow.org
> >>
> >>
> >>
> >> --
> >> Ben Tupper
> >> Bigelow Laboratory for Ocean Science
> >> West Boothbay Harbor, Maine
> >> http://www.bigelow.org/
> >> https://eco.bigelow.org
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>


From dd|@@b01 @end|ng |rom gm@||@com  Sat Nov 23 17:58:42 2019
From: dd|@@b01 @end|ng |rom gm@||@com (David Disabato)
Date: Sat, 23 Nov 2019 11:58:42 -0500
Subject: [R] Why does `[<-.matrix` not exist in base R
Message-ID: <CACg022-CQ_cqiTnT6vmdKdi2FhYFTzVoP5o_vVx_pFGvmN9Fiw@mail.gmail.com>

Whenever going from working with a data.frame to a matrix, I get annoyed
that I cannot assign and subset at the same time with matrices - like I can
with data.frames.

For example, if I want to add a new column to a data.frame, I can do
something like `myDataFrame[, "newColumn"] <- NA`.

However, with a matrix, this syntax does not work and I have to use a call
to `cbind` and create a new object. For example, `mymatrix2 <-
cbind(mymatrix, "newColumn" = NA)`.

Is there a programming reason that base R does not have a matrix method for
`[<-` or is it something that arguably should be added?

-- 
David J. Disabato, Ph.D.
Postdoctoral Research Scholar
Kent State University
ddisab01 at gmail.com

Email is not a secure form of communication as information and
confidentiality cannot be guaranteed. Information provided in an email is
not intended to be a professional service. In the case of a crisis or
emergency situation, call 911.

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Nov 23 19:13:23 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 23 Nov 2019 13:13:23 -0500
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
Message-ID: <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>

On 23/11/2019 11:05 a.m., Ana Marija wrote:
> Hi Ben,
> 
> I am not sure what you mean when you say to print, is it this?
> 
>> cmd_args=commandArgs(TRUE)
>> print(cmd_args)
> character(0)
>> cmd_args=commandArgs()
>> print(cmd_args)
> [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
> 
> I changed in the first line of this script:
> https://github.com/eleporcu/TWMR/blob/master/MR.R
> 
> cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
> 
> but again I get the same error:
> 
> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> Error in file(file, "rt") : cannot open the connection
> Calls: read.table -> file
> In addition: Warning message:
> In file(file, "rt") :
>    cannot open file 'NA.matrix': No such file or directory
> Execution halted

You didn't put the print(cmd_args) into the script.

Duncan Murdoch
> 
> 
> Please advise,
> Ana
> 
> On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> On 23/11/2019 10:26 a.m., Ana Marija wrote:
>>> HI Ben,
>>>
>>> I tried it but it doesn't work:
>>>
>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>> Error in file(file, "rt") : cannot open the connection
>>> Calls: read.table -> file
>>> In addition: Warning message:
>>> In file(file, "rt") :
>>>     cannot open file '--no-restore.matrix': No such file or directory
>>> Execution halted
>>>
>>
>> You should print the cmd_args variable that is set on the first line of
>> that script.  When I run a script that prints it using your command
>> line, this is what it looks like:
>>
>> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
>> [2] "--slave"
>> [3] "--no-restore"
>> [4] "--no-save"
>> [5] "--file=MR.R"
>> [6] "--args"
>> [7] "ENSG00000154803.ld"
>> [8] "ENSG00000154803.matrix"
>>
>> The next line
>>
>> gene <- cmd_args[3]
>>
>> is obviously wrong for my system, because it would set gene to
>> "--no-restore".  Your results will probably be somewhat different, but
>> it might be clear what you should use instead of the third element.
>>
>> By the way, changing the first line
>>
>> cmd_args=commandArgs()
>>
>> to
>>
>> cmd_args <- commandArgs(TRUE)
>>
>> makes a lot of sense in most cases.  I haven't read your whole script so
>> I don't know it it makes sense for you.
>>
>> Duncan Murdoch
>>
>>
>>> Please advise,
>>> Ana
>>>
>>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
>>>>
>>>> Hi,
>>>>
>>>> I think you want this order...
>>>>
>>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
>>>>
>>>> So, like this ...
>>>>
>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>>>
>>>> Cheers,
>>>> Ben
>>>>
>>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>>>
>>>>> HI Ben,
>>>>>
>>>>> thank you so much , I did this:
>>>>>
>>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
>>>>> Error: unexpected numeric constant in "1.000 0.089"
>>>>> Execution halted
>>>>>
>>>>> I made ENSG00000154803.ld with:
>>>>> library(MASS)
>>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
>>>>>
>>>>> and it looks like this:
>>>>>
>>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
>>>>> 0.000 0.000 0.000 0.001 0.003 0.000
>>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
>>>>> 0.001 0.012 0.005 0.000 0.004 0.004
>>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
>>>>> 0.054 0.006 0.002 0.010 0.001 0.000
>>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
>>>>> 0.066 0.010 0.030 0.001 0.003 0.000
>>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
>>>>> 0.002 0.003 0.066 0.006 0.004 0.004
>>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
>>>>> 0.016 0.000 0.025 0.000 0.005 0.000
>>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
>>>>> 0.128 0.019 0.005 0.030 0.002 0.016
>>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
>>>>> 0.098 0.010 0.012 0.001 0.006 0.003
>>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
>>>>> 0.012 0.000 0.006 0.018 0.004 0.013
>>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
>>>>> 0.001 0.091 0.057 0.062 0.002 0.005
>>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
>>>>> 0.238 0.180 0.073 0.058 0.000 0.006
>>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
>>>>> 1.000 0.158 0.006 0.044 0.006 0.001
>>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
>>>>> 0.158 1.000 0.077 0.237 0.009 0.000
>>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
>>>>> 0.006 0.077 1.000 0.056 0.000 0.004
>>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
>>>>> 0.044 0.237 0.056 1.000 0.000 0.003
>>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
>>>>> 0.006 0.009 0.000 0.000 1.000 0.002
>>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
>>>>> 0.001 0.000 0.004 0.003 0.002 1.000
>>>>>
>>>>> the other file (ENSG00000154803.matrix) looks like this:
>>>>>
>>>>> GENES ENSG00000154803 BETA_GWAS
>>>>> rs12601631 -0.320577 -0.0160778
>>>>> rs1708623 0.708706 0.0717719
>>>>> rs1708628 -0.645996 -0.0973019
>>>>> rs17804843 -0.78984 0.0059607
>>>>> rs4078062 -0.340732 -0.0716837
>>>>> rs4316813 -0.721137 -0.00502219
>>>>> rs7217764 -0.61641 0.16997
>>>>> rs7221842 -0.377727 -0.00184011
>>>>> rs12602831 -0.397059 0.0154625
>>>>> rs138437542 -0.590669 0.0145733
>>>>> rs2174369 -0.167913 -0.0268728
>>>>> rs242252 0.20184 0.0161709
>>>>> rs34121330 0.328602 0.0753894
>>>>> rs4792798 -0.303601 0.00227314
>>>>> rs7222311 -0.367686 -0.0419168
>>>>> rs74369938 0.687555 -0.223105
>>>>> rs8075751 -0.261916 -0.0313484
>>>>>
>>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
>>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> You might check the order of your arguments.   Options come before the
>>>>>> script filename. See the details here...
>>>>>>
>>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
>>>>>>
>>>>>> Ben
>>>>>>
>>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>>>>>
>>>>>>> Hello,
>>>>>>>
>>>>>>> I am trying to run this code:
>>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
>>>>>>>
>>>>>>> with r-3.6.1
>>>>>>>
>>>>>>> via:
>>>>>>>
>>>>>>> Rscript MR.R --no-save ENSG00000154803
>>>>>>>
>>>>>>> in the current directory I have saved: ENSG00000154803.ld and
>>>>>>> ENSG00000154803.matrix as the software requires
>>>>>>>
>>>>>>> but I am getting this error:
>>>>>>>
>>>>>>> Error in file(file, "rt") : cannot open the connection
>>>>>>> Calls: read.table -> file
>>>>>>> In addition: Warning message:
>>>>>>> In file(file, "rt") :
>>>>>>>     cannot open file '--no-restore.matrix': No such file or directory
>>>>>>> Execution halted
>>>>>>>
>>>>>>>
>>>>>>> Please advise,
>>>>>>>
>>>>>>> Thanks
>>>>>>> Ana
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Ben Tupper
>>>>>> Bigelow Laboratory for Ocean Science
>>>>>> West Boothbay Harbor, Maine
>>>>>> http://www.bigelow.org/
>>>>>> https://eco.bigelow.org
>>>>
>>>>
>>>>
>>>> --
>>>> Ben Tupper
>>>> Bigelow Laboratory for Ocean Science
>>>> West Boothbay Harbor, Maine
>>>> http://www.bigelow.org/
>>>> https://eco.bigelow.org
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 19:21:30 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Sat, 23 Nov 2019 12:21:30 -0600
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
 <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
Message-ID: <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>

Hi Duncan,

thanks, I just did,
 Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
[1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
Error in file(file, "rt") : cannot open the connection
Calls: read.table -> file
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'NA.matrix': No such file or directory
Execution halted


Please advise

On Sat, Nov 23, 2019 at 12:13 PM Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
>
> On 23/11/2019 11:05 a.m., Ana Marija wrote:
> > Hi Ben,
> >
> > I am not sure what you mean when you say to print, is it this?
> >
> >> cmd_args=commandArgs(TRUE)
> >> print(cmd_args)
> > character(0)
> >> cmd_args=commandArgs()
> >> print(cmd_args)
> > [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
> >
> > I changed in the first line of this script:
> > https://github.com/eleporcu/TWMR/blob/master/MR.R
> >
> > cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
> >
> > but again I get the same error:
> >
> > Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > Error in file(file, "rt") : cannot open the connection
> > Calls: read.table -> file
> > In addition: Warning message:
> > In file(file, "rt") :
> >    cannot open file 'NA.matrix': No such file or directory
> > Execution halted
>
> You didn't put the print(cmd_args) into the script.
>
> Duncan Murdoch
> >
> >
> > Please advise,
> > Ana
> >
> > On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >>
> >> On 23/11/2019 10:26 a.m., Ana Marija wrote:
> >>> HI Ben,
> >>>
> >>> I tried it but it doesn't work:
> >>>
> >>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>> Error in file(file, "rt") : cannot open the connection
> >>> Calls: read.table -> file
> >>> In addition: Warning message:
> >>> In file(file, "rt") :
> >>>     cannot open file '--no-restore.matrix': No such file or directory
> >>> Execution halted
> >>>
> >>
> >> You should print the cmd_args variable that is set on the first line of
> >> that script.  When I run a script that prints it using your command
> >> line, this is what it looks like:
> >>
> >> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
> >> [2] "--slave"
> >> [3] "--no-restore"
> >> [4] "--no-save"
> >> [5] "--file=MR.R"
> >> [6] "--args"
> >> [7] "ENSG00000154803.ld"
> >> [8] "ENSG00000154803.matrix"
> >>
> >> The next line
> >>
> >> gene <- cmd_args[3]
> >>
> >> is obviously wrong for my system, because it would set gene to
> >> "--no-restore".  Your results will probably be somewhat different, but
> >> it might be clear what you should use instead of the third element.
> >>
> >> By the way, changing the first line
> >>
> >> cmd_args=commandArgs()
> >>
> >> to
> >>
> >> cmd_args <- commandArgs(TRUE)
> >>
> >> makes a lot of sense in most cases.  I haven't read your whole script so
> >> I don't know it it makes sense for you.
> >>
> >> Duncan Murdoch
> >>
> >>
> >>> Please advise,
> >>> Ana
> >>>
> >>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
> >>>>
> >>>> Hi,
> >>>>
> >>>> I think you want this order...
> >>>>
> >>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
> >>>>
> >>>> So, like this ...
> >>>>
> >>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>>>
> >>>> Cheers,
> >>>> Ben
> >>>>
> >>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>>>
> >>>>> HI Ben,
> >>>>>
> >>>>> thank you so much , I did this:
> >>>>>
> >>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> >>>>> Error: unexpected numeric constant in "1.000 0.089"
> >>>>> Execution halted
> >>>>>
> >>>>> I made ENSG00000154803.ld with:
> >>>>> library(MASS)
> >>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> >>>>>
> >>>>> and it looks like this:
> >>>>>
> >>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> >>>>> 0.000 0.000 0.000 0.001 0.003 0.000
> >>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> >>>>> 0.001 0.012 0.005 0.000 0.004 0.004
> >>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> >>>>> 0.054 0.006 0.002 0.010 0.001 0.000
> >>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> >>>>> 0.066 0.010 0.030 0.001 0.003 0.000
> >>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> >>>>> 0.002 0.003 0.066 0.006 0.004 0.004
> >>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> >>>>> 0.016 0.000 0.025 0.000 0.005 0.000
> >>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> >>>>> 0.128 0.019 0.005 0.030 0.002 0.016
> >>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> >>>>> 0.098 0.010 0.012 0.001 0.006 0.003
> >>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> >>>>> 0.012 0.000 0.006 0.018 0.004 0.013
> >>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> >>>>> 0.001 0.091 0.057 0.062 0.002 0.005
> >>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> >>>>> 0.238 0.180 0.073 0.058 0.000 0.006
> >>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> >>>>> 1.000 0.158 0.006 0.044 0.006 0.001
> >>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> >>>>> 0.158 1.000 0.077 0.237 0.009 0.000
> >>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> >>>>> 0.006 0.077 1.000 0.056 0.000 0.004
> >>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> >>>>> 0.044 0.237 0.056 1.000 0.000 0.003
> >>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> >>>>> 0.006 0.009 0.000 0.000 1.000 0.002
> >>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> >>>>> 0.001 0.000 0.004 0.003 0.002 1.000
> >>>>>
> >>>>> the other file (ENSG00000154803.matrix) looks like this:
> >>>>>
> >>>>> GENES ENSG00000154803 BETA_GWAS
> >>>>> rs12601631 -0.320577 -0.0160778
> >>>>> rs1708623 0.708706 0.0717719
> >>>>> rs1708628 -0.645996 -0.0973019
> >>>>> rs17804843 -0.78984 0.0059607
> >>>>> rs4078062 -0.340732 -0.0716837
> >>>>> rs4316813 -0.721137 -0.00502219
> >>>>> rs7217764 -0.61641 0.16997
> >>>>> rs7221842 -0.377727 -0.00184011
> >>>>> rs12602831 -0.397059 0.0154625
> >>>>> rs138437542 -0.590669 0.0145733
> >>>>> rs2174369 -0.167913 -0.0268728
> >>>>> rs242252 0.20184 0.0161709
> >>>>> rs34121330 0.328602 0.0753894
> >>>>> rs4792798 -0.303601 0.00227314
> >>>>> rs7222311 -0.367686 -0.0419168
> >>>>> rs74369938 0.687555 -0.223105
> >>>>> rs8075751 -0.261916 -0.0313484
> >>>>>
> >>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> >>>>>>
> >>>>>> Hi,
> >>>>>>
> >>>>>> You might check the order of your arguments.   Options come before the
> >>>>>> script filename. See the details here...
> >>>>>>
> >>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> >>>>>>
> >>>>>> Ben
> >>>>>>
> >>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>>>>>
> >>>>>>> Hello,
> >>>>>>>
> >>>>>>> I am trying to run this code:
> >>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> >>>>>>>
> >>>>>>> with r-3.6.1
> >>>>>>>
> >>>>>>> via:
> >>>>>>>
> >>>>>>> Rscript MR.R --no-save ENSG00000154803
> >>>>>>>
> >>>>>>> in the current directory I have saved: ENSG00000154803.ld and
> >>>>>>> ENSG00000154803.matrix as the software requires
> >>>>>>>
> >>>>>>> but I am getting this error:
> >>>>>>>
> >>>>>>> Error in file(file, "rt") : cannot open the connection
> >>>>>>> Calls: read.table -> file
> >>>>>>> In addition: Warning message:
> >>>>>>> In file(file, "rt") :
> >>>>>>>     cannot open file '--no-restore.matrix': No such file or directory
> >>>>>>> Execution halted
> >>>>>>>
> >>>>>>>
> >>>>>>> Please advise,
> >>>>>>>
> >>>>>>> Thanks
> >>>>>>> Ana
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> --
> >>>>>> Ben Tupper
> >>>>>> Bigelow Laboratory for Ocean Science
> >>>>>> West Boothbay Harbor, Maine
> >>>>>> http://www.bigelow.org/
> >>>>>> https://eco.bigelow.org
> >>>>
> >>>>
> >>>>
> >>>> --
> >>>> Ben Tupper
> >>>> Bigelow Laboratory for Ocean Science
> >>>> West Boothbay Harbor, Maine
> >>>> http://www.bigelow.org/
> >>>> https://eco.bigelow.org
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>
>


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Nov 23 20:55:22 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 23 Nov 2019 14:55:22 -0500
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
 <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
 <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>
Message-ID: <05656494-5a29-8895-40c8-c069c8ddda35@gmail.com>

On 23/11/2019 1:21 p.m., Ana Marija wrote:
> Hi Duncan,
> 
> thanks, I just did,
>   Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> Error in file(file, "rt") : cannot open the connection
> Calls: read.table -> file
> In addition: Warning message:
> In file(file, "rt") :
>    cannot open file 'NA.matrix': No such file or directory
> Execution halted
> 
Your script works with the third element in the list of arguments, and 
there are only two.

Duncan Murdoch

> 
> Please advise
> 
> On Sat, Nov 23, 2019 at 12:13 PM Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
>>
>> On 23/11/2019 11:05 a.m., Ana Marija wrote:
>>> Hi Ben,
>>>
>>> I am not sure what you mean when you say to print, is it this?
>>>
>>>> cmd_args=commandArgs(TRUE)
>>>> print(cmd_args)
>>> character(0)
>>>> cmd_args=commandArgs()
>>>> print(cmd_args)
>>> [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
>>>
>>> I changed in the first line of this script:
>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
>>>
>>> cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
>>>
>>> but again I get the same error:
>>>
>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>> Error in file(file, "rt") : cannot open the connection
>>> Calls: read.table -> file
>>> In addition: Warning message:
>>> In file(file, "rt") :
>>>     cannot open file 'NA.matrix': No such file or directory
>>> Execution halted
>>
>> You didn't put the print(cmd_args) into the script.
>>
>> Duncan Murdoch
>>>
>>>
>>> Please advise,
>>> Ana
>>>
>>> On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>>>
>>>> On 23/11/2019 10:26 a.m., Ana Marija wrote:
>>>>> HI Ben,
>>>>>
>>>>> I tried it but it doesn't work:
>>>>>
>>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>>>> Error in file(file, "rt") : cannot open the connection
>>>>> Calls: read.table -> file
>>>>> In addition: Warning message:
>>>>> In file(file, "rt") :
>>>>>      cannot open file '--no-restore.matrix': No such file or directory
>>>>> Execution halted
>>>>>
>>>>
>>>> You should print the cmd_args variable that is set on the first line of
>>>> that script.  When I run a script that prints it using your command
>>>> line, this is what it looks like:
>>>>
>>>> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>>> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
>>>> [2] "--slave"
>>>> [3] "--no-restore"
>>>> [4] "--no-save"
>>>> [5] "--file=MR.R"
>>>> [6] "--args"
>>>> [7] "ENSG00000154803.ld"
>>>> [8] "ENSG00000154803.matrix"
>>>>
>>>> The next line
>>>>
>>>> gene <- cmd_args[3]
>>>>
>>>> is obviously wrong for my system, because it would set gene to
>>>> "--no-restore".  Your results will probably be somewhat different, but
>>>> it might be clear what you should use instead of the third element.
>>>>
>>>> By the way, changing the first line
>>>>
>>>> cmd_args=commandArgs()
>>>>
>>>> to
>>>>
>>>> cmd_args <- commandArgs(TRUE)
>>>>
>>>> makes a lot of sense in most cases.  I haven't read your whole script so
>>>> I don't know it it makes sense for you.
>>>>
>>>> Duncan Murdoch
>>>>
>>>>
>>>>> Please advise,
>>>>> Ana
>>>>>
>>>>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
>>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> I think you want this order...
>>>>>>
>>>>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
>>>>>>
>>>>>> So, like this ...
>>>>>>
>>>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
>>>>>>
>>>>>> Cheers,
>>>>>> Ben
>>>>>>
>>>>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>>>>>
>>>>>>> HI Ben,
>>>>>>>
>>>>>>> thank you so much , I did this:
>>>>>>>
>>>>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
>>>>>>> Error: unexpected numeric constant in "1.000 0.089"
>>>>>>> Execution halted
>>>>>>>
>>>>>>> I made ENSG00000154803.ld with:
>>>>>>> library(MASS)
>>>>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
>>>>>>>
>>>>>>> and it looks like this:
>>>>>>>
>>>>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
>>>>>>> 0.000 0.000 0.000 0.001 0.003 0.000
>>>>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
>>>>>>> 0.001 0.012 0.005 0.000 0.004 0.004
>>>>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
>>>>>>> 0.054 0.006 0.002 0.010 0.001 0.000
>>>>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
>>>>>>> 0.066 0.010 0.030 0.001 0.003 0.000
>>>>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
>>>>>>> 0.002 0.003 0.066 0.006 0.004 0.004
>>>>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
>>>>>>> 0.016 0.000 0.025 0.000 0.005 0.000
>>>>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
>>>>>>> 0.128 0.019 0.005 0.030 0.002 0.016
>>>>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
>>>>>>> 0.098 0.010 0.012 0.001 0.006 0.003
>>>>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
>>>>>>> 0.012 0.000 0.006 0.018 0.004 0.013
>>>>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
>>>>>>> 0.001 0.091 0.057 0.062 0.002 0.005
>>>>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
>>>>>>> 0.238 0.180 0.073 0.058 0.000 0.006
>>>>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
>>>>>>> 1.000 0.158 0.006 0.044 0.006 0.001
>>>>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
>>>>>>> 0.158 1.000 0.077 0.237 0.009 0.000
>>>>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
>>>>>>> 0.006 0.077 1.000 0.056 0.000 0.004
>>>>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
>>>>>>> 0.044 0.237 0.056 1.000 0.000 0.003
>>>>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
>>>>>>> 0.006 0.009 0.000 0.000 1.000 0.002
>>>>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
>>>>>>> 0.001 0.000 0.004 0.003 0.002 1.000
>>>>>>>
>>>>>>> the other file (ENSG00000154803.matrix) looks like this:
>>>>>>>
>>>>>>> GENES ENSG00000154803 BETA_GWAS
>>>>>>> rs12601631 -0.320577 -0.0160778
>>>>>>> rs1708623 0.708706 0.0717719
>>>>>>> rs1708628 -0.645996 -0.0973019
>>>>>>> rs17804843 -0.78984 0.0059607
>>>>>>> rs4078062 -0.340732 -0.0716837
>>>>>>> rs4316813 -0.721137 -0.00502219
>>>>>>> rs7217764 -0.61641 0.16997
>>>>>>> rs7221842 -0.377727 -0.00184011
>>>>>>> rs12602831 -0.397059 0.0154625
>>>>>>> rs138437542 -0.590669 0.0145733
>>>>>>> rs2174369 -0.167913 -0.0268728
>>>>>>> rs242252 0.20184 0.0161709
>>>>>>> rs34121330 0.328602 0.0753894
>>>>>>> rs4792798 -0.303601 0.00227314
>>>>>>> rs7222311 -0.367686 -0.0419168
>>>>>>> rs74369938 0.687555 -0.223105
>>>>>>> rs8075751 -0.261916 -0.0313484
>>>>>>>
>>>>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
>>>>>>>>
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> You might check the order of your arguments.   Options come before the
>>>>>>>> script filename. See the details here...
>>>>>>>>
>>>>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
>>>>>>>>
>>>>>>>> Ben
>>>>>>>>
>>>>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>>>>>>>>>
>>>>>>>>> Hello,
>>>>>>>>>
>>>>>>>>> I am trying to run this code:
>>>>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
>>>>>>>>>
>>>>>>>>> with r-3.6.1
>>>>>>>>>
>>>>>>>>> via:
>>>>>>>>>
>>>>>>>>> Rscript MR.R --no-save ENSG00000154803
>>>>>>>>>
>>>>>>>>> in the current directory I have saved: ENSG00000154803.ld and
>>>>>>>>> ENSG00000154803.matrix as the software requires
>>>>>>>>>
>>>>>>>>> but I am getting this error:
>>>>>>>>>
>>>>>>>>> Error in file(file, "rt") : cannot open the connection
>>>>>>>>> Calls: read.table -> file
>>>>>>>>> In addition: Warning message:
>>>>>>>>> In file(file, "rt") :
>>>>>>>>>      cannot open file '--no-restore.matrix': No such file or directory
>>>>>>>>> Execution halted
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Please advise,
>>>>>>>>>
>>>>>>>>> Thanks
>>>>>>>>> Ana
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> Ben Tupper
>>>>>>>> Bigelow Laboratory for Ocean Science
>>>>>>>> West Boothbay Harbor, Maine
>>>>>>>> http://www.bigelow.org/
>>>>>>>> https://eco.bigelow.org
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Ben Tupper
>>>>>> Bigelow Laboratory for Ocean Science
>>>>>> West Boothbay Harbor, Maine
>>>>>> http://www.bigelow.org/
>>>>>> https://eco.bigelow.org
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>


From henr|k@bengt@@on @end|ng |rom gm@||@com  Sat Nov 23 21:08:03 2019
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Sat, 23 Nov 2019 12:08:03 -0800
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <05656494-5a29-8895-40c8-c069c8ddda35@gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
 <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
 <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>
 <05656494-5a29-8895-40c8-c069c8ddda35@gmail.com>
Message-ID: <CAFDcVCS8F0EhO62wcMiOHK7WEuwN80bnYxLM9re+GMVM27Szvw@mail.gmail.com>

Maybe it would help to add:

file<-paste(gene,"matrix",sep=".")
if (!file.exists(file)) stop("File not found: ", file)
filecluster<-read.table(file,header=T,sep=" ",dec=".")

/Henrik

On Sat, Nov 23, 2019 at 11:55 AM Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
>
> On 23/11/2019 1:21 p.m., Ana Marija wrote:
> > Hi Duncan,
> >
> > thanks, I just did,
> >   Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> > Error in file(file, "rt") : cannot open the connection
> > Calls: read.table -> file
> > In addition: Warning message:
> > In file(file, "rt") :
> >    cannot open file 'NA.matrix': No such file or directory
> > Execution halted
> >
> Your script works with the third element in the list of arguments, and
> there are only two.
>
> Duncan Murdoch
>
> >
> > Please advise
> >
> > On Sat, Nov 23, 2019 at 12:13 PM Duncan Murdoch
> > <murdoch.duncan at gmail.com> wrote:
> >>
> >> On 23/11/2019 11:05 a.m., Ana Marija wrote:
> >>> Hi Ben,
> >>>
> >>> I am not sure what you mean when you say to print, is it this?
> >>>
> >>>> cmd_args=commandArgs(TRUE)
> >>>> print(cmd_args)
> >>> character(0)
> >>>> cmd_args=commandArgs()
> >>>> print(cmd_args)
> >>> [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
> >>>
> >>> I changed in the first line of this script:
> >>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> >>>
> >>> cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
> >>>
> >>> but again I get the same error:
> >>>
> >>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>> Error in file(file, "rt") : cannot open the connection
> >>> Calls: read.table -> file
> >>> In addition: Warning message:
> >>> In file(file, "rt") :
> >>>     cannot open file 'NA.matrix': No such file or directory
> >>> Execution halted
> >>
> >> You didn't put the print(cmd_args) into the script.
> >>
> >> Duncan Murdoch
> >>>
> >>>
> >>> Please advise,
> >>> Ana
> >>>
> >>> On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >>>>
> >>>> On 23/11/2019 10:26 a.m., Ana Marija wrote:
> >>>>> HI Ben,
> >>>>>
> >>>>> I tried it but it doesn't work:
> >>>>>
> >>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>>>> Error in file(file, "rt") : cannot open the connection
> >>>>> Calls: read.table -> file
> >>>>> In addition: Warning message:
> >>>>> In file(file, "rt") :
> >>>>>      cannot open file '--no-restore.matrix': No such file or directory
> >>>>> Execution halted
> >>>>>
> >>>>
> >>>> You should print the cmd_args variable that is set on the first line of
> >>>> that script.  When I run a script that prints it using your command
> >>>> line, this is what it looks like:
> >>>>
> >>>> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>>> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
> >>>> [2] "--slave"
> >>>> [3] "--no-restore"
> >>>> [4] "--no-save"
> >>>> [5] "--file=MR.R"
> >>>> [6] "--args"
> >>>> [7] "ENSG00000154803.ld"
> >>>> [8] "ENSG00000154803.matrix"
> >>>>
> >>>> The next line
> >>>>
> >>>> gene <- cmd_args[3]
> >>>>
> >>>> is obviously wrong for my system, because it would set gene to
> >>>> "--no-restore".  Your results will probably be somewhat different, but
> >>>> it might be clear what you should use instead of the third element.
> >>>>
> >>>> By the way, changing the first line
> >>>>
> >>>> cmd_args=commandArgs()
> >>>>
> >>>> to
> >>>>
> >>>> cmd_args <- commandArgs(TRUE)
> >>>>
> >>>> makes a lot of sense in most cases.  I haven't read your whole script so
> >>>> I don't know it it makes sense for you.
> >>>>
> >>>> Duncan Murdoch
> >>>>
> >>>>
> >>>>> Please advise,
> >>>>> Ana
> >>>>>
> >>>>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
> >>>>>>
> >>>>>> Hi,
> >>>>>>
> >>>>>> I think you want this order...
> >>>>>>
> >>>>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
> >>>>>>
> >>>>>> So, like this ...
> >>>>>>
> >>>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> >>>>>>
> >>>>>> Cheers,
> >>>>>> Ben
> >>>>>>
> >>>>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>>>>>
> >>>>>>> HI Ben,
> >>>>>>>
> >>>>>>> thank you so much , I did this:
> >>>>>>>
> >>>>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> >>>>>>> Error: unexpected numeric constant in "1.000 0.089"
> >>>>>>> Execution halted
> >>>>>>>
> >>>>>>> I made ENSG00000154803.ld with:
> >>>>>>> library(MASS)
> >>>>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> >>>>>>>
> >>>>>>> and it looks like this:
> >>>>>>>
> >>>>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> >>>>>>> 0.000 0.000 0.000 0.001 0.003 0.000
> >>>>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> >>>>>>> 0.001 0.012 0.005 0.000 0.004 0.004
> >>>>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> >>>>>>> 0.054 0.006 0.002 0.010 0.001 0.000
> >>>>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> >>>>>>> 0.066 0.010 0.030 0.001 0.003 0.000
> >>>>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> >>>>>>> 0.002 0.003 0.066 0.006 0.004 0.004
> >>>>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> >>>>>>> 0.016 0.000 0.025 0.000 0.005 0.000
> >>>>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> >>>>>>> 0.128 0.019 0.005 0.030 0.002 0.016
> >>>>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> >>>>>>> 0.098 0.010 0.012 0.001 0.006 0.003
> >>>>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> >>>>>>> 0.012 0.000 0.006 0.018 0.004 0.013
> >>>>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> >>>>>>> 0.001 0.091 0.057 0.062 0.002 0.005
> >>>>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> >>>>>>> 0.238 0.180 0.073 0.058 0.000 0.006
> >>>>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> >>>>>>> 1.000 0.158 0.006 0.044 0.006 0.001
> >>>>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> >>>>>>> 0.158 1.000 0.077 0.237 0.009 0.000
> >>>>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> >>>>>>> 0.006 0.077 1.000 0.056 0.000 0.004
> >>>>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> >>>>>>> 0.044 0.237 0.056 1.000 0.000 0.003
> >>>>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> >>>>>>> 0.006 0.009 0.000 0.000 1.000 0.002
> >>>>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> >>>>>>> 0.001 0.000 0.004 0.003 0.002 1.000
> >>>>>>>
> >>>>>>> the other file (ENSG00000154803.matrix) looks like this:
> >>>>>>>
> >>>>>>> GENES ENSG00000154803 BETA_GWAS
> >>>>>>> rs12601631 -0.320577 -0.0160778
> >>>>>>> rs1708623 0.708706 0.0717719
> >>>>>>> rs1708628 -0.645996 -0.0973019
> >>>>>>> rs17804843 -0.78984 0.0059607
> >>>>>>> rs4078062 -0.340732 -0.0716837
> >>>>>>> rs4316813 -0.721137 -0.00502219
> >>>>>>> rs7217764 -0.61641 0.16997
> >>>>>>> rs7221842 -0.377727 -0.00184011
> >>>>>>> rs12602831 -0.397059 0.0154625
> >>>>>>> rs138437542 -0.590669 0.0145733
> >>>>>>> rs2174369 -0.167913 -0.0268728
> >>>>>>> rs242252 0.20184 0.0161709
> >>>>>>> rs34121330 0.328602 0.0753894
> >>>>>>> rs4792798 -0.303601 0.00227314
> >>>>>>> rs7222311 -0.367686 -0.0419168
> >>>>>>> rs74369938 0.687555 -0.223105
> >>>>>>> rs8075751 -0.261916 -0.0313484
> >>>>>>>
> >>>>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> >>>>>>>>
> >>>>>>>> Hi,
> >>>>>>>>
> >>>>>>>> You might check the order of your arguments.   Options come before the
> >>>>>>>> script filename. See the details here...
> >>>>>>>>
> >>>>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> >>>>>>>>
> >>>>>>>> Ben
> >>>>>>>>
> >>>>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >>>>>>>>>
> >>>>>>>>> Hello,
> >>>>>>>>>
> >>>>>>>>> I am trying to run this code:
> >>>>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> >>>>>>>>>
> >>>>>>>>> with r-3.6.1
> >>>>>>>>>
> >>>>>>>>> via:
> >>>>>>>>>
> >>>>>>>>> Rscript MR.R --no-save ENSG00000154803
> >>>>>>>>>
> >>>>>>>>> in the current directory I have saved: ENSG00000154803.ld and
> >>>>>>>>> ENSG00000154803.matrix as the software requires
> >>>>>>>>>
> >>>>>>>>> but I am getting this error:
> >>>>>>>>>
> >>>>>>>>> Error in file(file, "rt") : cannot open the connection
> >>>>>>>>> Calls: read.table -> file
> >>>>>>>>> In addition: Warning message:
> >>>>>>>>> In file(file, "rt") :
> >>>>>>>>>      cannot open file '--no-restore.matrix': No such file or directory
> >>>>>>>>> Execution halted
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> Please advise,
> >>>>>>>>>
> >>>>>>>>> Thanks
> >>>>>>>>> Ana
> >>>>>>>>>
> >>>>>>>>> ______________________________________________
> >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> --
> >>>>>>>> Ben Tupper
> >>>>>>>> Bigelow Laboratory for Ocean Science
> >>>>>>>> West Boothbay Harbor, Maine
> >>>>>>>> http://www.bigelow.org/
> >>>>>>>> https://eco.bigelow.org
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> --
> >>>>>> Ben Tupper
> >>>>>> Bigelow Laboratory for Ocean Science
> >>>>>> West Boothbay Harbor, Maine
> >>>>>> http://www.bigelow.org/
> >>>>>> https://eco.bigelow.org
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>
> >>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 21:25:47 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Sat, 23 Nov 2019 14:25:47 -0600
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAFDcVCS8F0EhO62wcMiOHK7WEuwN80bnYxLM9re+GMVM27Szvw@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
 <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
 <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>
 <05656494-5a29-8895-40c8-c069c8ddda35@gmail.com>
 <CAFDcVCS8F0EhO62wcMiOHK7WEuwN80bnYxLM9re+GMVM27Szvw@mail.gmail.com>
Message-ID: <CAF9-5jO0+yx-zM7agxAE-Ncj+7_U7sM7tLq2aF0mYn=o4Yuyew@mail.gmail.com>

it is confusing because in documentation they say this is how you run
the script:
https://github.com/eleporcu/TWMR

I tried changing this on the script:

cmd_args=commandArgs(TRUE)
print(cmd_args)
gene<-cmd_args[3]
Ngwas<-239087
N_eQTLs<-32000
out<-c("gene","alpha","SE","P","Nsnps","Ngene")

file<-paste(gene,"matrix",sep=".")
if (!file.exists(file)) stop("File not found: ", file)
filecluster<-read.table(file,header=T,sep=" ",dec=".")
#file<-paste(gene,"matrix",sep=".")
#filecluster<-read.table(file,header=T,sep=".",dec=" ")
beta<-as.matrix(filecluster[,2:(length(filecluster[1,])-1)])

and I run it:
Rscript< MR.R --no-save ENSG00000154803.ld ENSG00000154803.matrix
Error: unexpected numeric constant in "1.000 0.089"
Execution halted


I also tried this:
Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
[1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
Error: File not found: NA.matrix
Execution halted


Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
[1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
Error: File not found: NA.matrix
Execution halted

On Sat, Nov 23, 2019 at 2:08 PM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> Maybe it would help to add:
>
> file<-paste(gene,"matrix",sep=".")
> if (!file.exists(file)) stop("File not found: ", file)
> filecluster<-read.table(file,header=T,sep=" ",dec=".")
>
> /Henrik
>
> On Sat, Nov 23, 2019 at 11:55 AM Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
> >
> > On 23/11/2019 1:21 p.m., Ana Marija wrote:
> > > Hi Duncan,
> > >
> > > thanks, I just did,
> > >   Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> > > Error in file(file, "rt") : cannot open the connection
> > > Calls: read.table -> file
> > > In addition: Warning message:
> > > In file(file, "rt") :
> > >    cannot open file 'NA.matrix': No such file or directory
> > > Execution halted
> > >
> > Your script works with the third element in the list of arguments, and
> > there are only two.
> >
> > Duncan Murdoch
> >
> > >
> > > Please advise
> > >
> > > On Sat, Nov 23, 2019 at 12:13 PM Duncan Murdoch
> > > <murdoch.duncan at gmail.com> wrote:
> > >>
> > >> On 23/11/2019 11:05 a.m., Ana Marija wrote:
> > >>> Hi Ben,
> > >>>
> > >>> I am not sure what you mean when you say to print, is it this?
> > >>>
> > >>>> cmd_args=commandArgs(TRUE)
> > >>>> print(cmd_args)
> > >>> character(0)
> > >>>> cmd_args=commandArgs()
> > >>>> print(cmd_args)
> > >>> [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
> > >>>
> > >>> I changed in the first line of this script:
> > >>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> > >>>
> > >>> cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
> > >>>
> > >>> but again I get the same error:
> > >>>
> > >>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > >>> Error in file(file, "rt") : cannot open the connection
> > >>> Calls: read.table -> file
> > >>> In addition: Warning message:
> > >>> In file(file, "rt") :
> > >>>     cannot open file 'NA.matrix': No such file or directory
> > >>> Execution halted
> > >>
> > >> You didn't put the print(cmd_args) into the script.
> > >>
> > >> Duncan Murdoch
> > >>>
> > >>>
> > >>> Please advise,
> > >>> Ana
> > >>>
> > >>> On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > >>>>
> > >>>> On 23/11/2019 10:26 a.m., Ana Marija wrote:
> > >>>>> HI Ben,
> > >>>>>
> > >>>>> I tried it but it doesn't work:
> > >>>>>
> > >>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > >>>>> Error in file(file, "rt") : cannot open the connection
> > >>>>> Calls: read.table -> file
> > >>>>> In addition: Warning message:
> > >>>>> In file(file, "rt") :
> > >>>>>      cannot open file '--no-restore.matrix': No such file or directory
> > >>>>> Execution halted
> > >>>>>
> > >>>>
> > >>>> You should print the cmd_args variable that is set on the first line of
> > >>>> that script.  When I run a script that prints it using your command
> > >>>> line, this is what it looks like:
> > >>>>
> > >>>> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > >>>> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
> > >>>> [2] "--slave"
> > >>>> [3] "--no-restore"
> > >>>> [4] "--no-save"
> > >>>> [5] "--file=MR.R"
> > >>>> [6] "--args"
> > >>>> [7] "ENSG00000154803.ld"
> > >>>> [8] "ENSG00000154803.matrix"
> > >>>>
> > >>>> The next line
> > >>>>
> > >>>> gene <- cmd_args[3]
> > >>>>
> > >>>> is obviously wrong for my system, because it would set gene to
> > >>>> "--no-restore".  Your results will probably be somewhat different, but
> > >>>> it might be clear what you should use instead of the third element.
> > >>>>
> > >>>> By the way, changing the first line
> > >>>>
> > >>>> cmd_args=commandArgs()
> > >>>>
> > >>>> to
> > >>>>
> > >>>> cmd_args <- commandArgs(TRUE)
> > >>>>
> > >>>> makes a lot of sense in most cases.  I haven't read your whole script so
> > >>>> I don't know it it makes sense for you.
> > >>>>
> > >>>> Duncan Murdoch
> > >>>>
> > >>>>
> > >>>>> Please advise,
> > >>>>> Ana
> > >>>>>
> > >>>>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
> > >>>>>>
> > >>>>>> Hi,
> > >>>>>>
> > >>>>>> I think you want this order...
> > >>>>>>
> > >>>>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
> > >>>>>>
> > >>>>>> So, like this ...
> > >>>>>>
> > >>>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > >>>>>>
> > >>>>>> Cheers,
> > >>>>>> Ben
> > >>>>>>
> > >>>>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >>>>>>>
> > >>>>>>> HI Ben,
> > >>>>>>>
> > >>>>>>> thank you so much , I did this:
> > >>>>>>>
> > >>>>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> > >>>>>>> Error: unexpected numeric constant in "1.000 0.089"
> > >>>>>>> Execution halted
> > >>>>>>>
> > >>>>>>> I made ENSG00000154803.ld with:
> > >>>>>>> library(MASS)
> > >>>>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> > >>>>>>>
> > >>>>>>> and it looks like this:
> > >>>>>>>
> > >>>>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> > >>>>>>> 0.000 0.000 0.000 0.001 0.003 0.000
> > >>>>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> > >>>>>>> 0.001 0.012 0.005 0.000 0.004 0.004
> > >>>>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> > >>>>>>> 0.054 0.006 0.002 0.010 0.001 0.000
> > >>>>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> > >>>>>>> 0.066 0.010 0.030 0.001 0.003 0.000
> > >>>>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> > >>>>>>> 0.002 0.003 0.066 0.006 0.004 0.004
> > >>>>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> > >>>>>>> 0.016 0.000 0.025 0.000 0.005 0.000
> > >>>>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> > >>>>>>> 0.128 0.019 0.005 0.030 0.002 0.016
> > >>>>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> > >>>>>>> 0.098 0.010 0.012 0.001 0.006 0.003
> > >>>>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> > >>>>>>> 0.012 0.000 0.006 0.018 0.004 0.013
> > >>>>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> > >>>>>>> 0.001 0.091 0.057 0.062 0.002 0.005
> > >>>>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> > >>>>>>> 0.238 0.180 0.073 0.058 0.000 0.006
> > >>>>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> > >>>>>>> 1.000 0.158 0.006 0.044 0.006 0.001
> > >>>>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> > >>>>>>> 0.158 1.000 0.077 0.237 0.009 0.000
> > >>>>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> > >>>>>>> 0.006 0.077 1.000 0.056 0.000 0.004
> > >>>>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> > >>>>>>> 0.044 0.237 0.056 1.000 0.000 0.003
> > >>>>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> > >>>>>>> 0.006 0.009 0.000 0.000 1.000 0.002
> > >>>>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> > >>>>>>> 0.001 0.000 0.004 0.003 0.002 1.000
> > >>>>>>>
> > >>>>>>> the other file (ENSG00000154803.matrix) looks like this:
> > >>>>>>>
> > >>>>>>> GENES ENSG00000154803 BETA_GWAS
> > >>>>>>> rs12601631 -0.320577 -0.0160778
> > >>>>>>> rs1708623 0.708706 0.0717719
> > >>>>>>> rs1708628 -0.645996 -0.0973019
> > >>>>>>> rs17804843 -0.78984 0.0059607
> > >>>>>>> rs4078062 -0.340732 -0.0716837
> > >>>>>>> rs4316813 -0.721137 -0.00502219
> > >>>>>>> rs7217764 -0.61641 0.16997
> > >>>>>>> rs7221842 -0.377727 -0.00184011
> > >>>>>>> rs12602831 -0.397059 0.0154625
> > >>>>>>> rs138437542 -0.590669 0.0145733
> > >>>>>>> rs2174369 -0.167913 -0.0268728
> > >>>>>>> rs242252 0.20184 0.0161709
> > >>>>>>> rs34121330 0.328602 0.0753894
> > >>>>>>> rs4792798 -0.303601 0.00227314
> > >>>>>>> rs7222311 -0.367686 -0.0419168
> > >>>>>>> rs74369938 0.687555 -0.223105
> > >>>>>>> rs8075751 -0.261916 -0.0313484
> > >>>>>>>
> > >>>>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> > >>>>>>>>
> > >>>>>>>> Hi,
> > >>>>>>>>
> > >>>>>>>> You might check the order of your arguments.   Options come before the
> > >>>>>>>> script filename. See the details here...
> > >>>>>>>>
> > >>>>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> > >>>>>>>>
> > >>>>>>>> Ben
> > >>>>>>>>
> > >>>>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > >>>>>>>>>
> > >>>>>>>>> Hello,
> > >>>>>>>>>
> > >>>>>>>>> I am trying to run this code:
> > >>>>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> > >>>>>>>>>
> > >>>>>>>>> with r-3.6.1
> > >>>>>>>>>
> > >>>>>>>>> via:
> > >>>>>>>>>
> > >>>>>>>>> Rscript MR.R --no-save ENSG00000154803
> > >>>>>>>>>
> > >>>>>>>>> in the current directory I have saved: ENSG00000154803.ld and
> > >>>>>>>>> ENSG00000154803.matrix as the software requires
> > >>>>>>>>>
> > >>>>>>>>> but I am getting this error:
> > >>>>>>>>>
> > >>>>>>>>> Error in file(file, "rt") : cannot open the connection
> > >>>>>>>>> Calls: read.table -> file
> > >>>>>>>>> In addition: Warning message:
> > >>>>>>>>> In file(file, "rt") :
> > >>>>>>>>>      cannot open file '--no-restore.matrix': No such file or directory
> > >>>>>>>>> Execution halted
> > >>>>>>>>>
> > >>>>>>>>>
> > >>>>>>>>> Please advise,
> > >>>>>>>>>
> > >>>>>>>>> Thanks
> > >>>>>>>>> Ana
> > >>>>>>>>>
> > >>>>>>>>> ______________________________________________
> > >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>>>>>>
> > >>>>>>>>
> > >>>>>>>>
> > >>>>>>>> --
> > >>>>>>>> Ben Tupper
> > >>>>>>>> Bigelow Laboratory for Ocean Science
> > >>>>>>>> West Boothbay Harbor, Maine
> > >>>>>>>> http://www.bigelow.org/
> > >>>>>>>> https://eco.bigelow.org
> > >>>>>>
> > >>>>>>
> > >>>>>>
> > >>>>>> --
> > >>>>>> Ben Tupper
> > >>>>>> Bigelow Laboratory for Ocean Science
> > >>>>>> West Boothbay Harbor, Maine
> > >>>>>> http://www.bigelow.org/
> > >>>>>> https://eco.bigelow.org
> > >>>>>
> > >>>>> ______________________________________________
> > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > >>>>>
> > >>>>
> > >>
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From henr|k@bengt@@on @end|ng |rom gm@||@com  Sat Nov 23 22:05:42 2019
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Sat, 23 Nov 2019 13:05:42 -0800
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAF9-5jO0+yx-zM7agxAE-Ncj+7_U7sM7tLq2aF0mYn=o4Yuyew@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
 <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
 <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>
 <05656494-5a29-8895-40c8-c069c8ddda35@gmail.com>
 <CAFDcVCS8F0EhO62wcMiOHK7WEuwN80bnYxLM9re+GMVM27Szvw@mail.gmail.com>
 <CAF9-5jO0+yx-zM7agxAE-Ncj+7_U7sM7tLq2aF0mYn=o4Yuyew@mail.gmail.com>
Message-ID: <CAFDcVCSKVj3pv3WdNUHSos0wzSfYvrq0P_yAwBAToeerFyFyyQ@mail.gmail.com>

Looking at the https://github.com/eleporcu/TWMR/blob/master/README.txt,
it looks like you should pass a single argument when you call the MR.R
script and it should be the name of a gene, e.g. 'ENSG00000154803'.
You are passing two arguments and they look like filenames.

Update the script, because it should really use commandArgs(TRUE), to:

cmd_args <- commandArgs(TRUE)
if (length(cmd_args) == 0L) stop("No arguments specified.")
print(cmd_args)
gene<-cmd_args[length(cmd_args)]   ## Last argument is the 'gene'
Ngwas<-239087
N_eQTLs<-32000
out<-c("gene","alpha","SE","P","Nsnps","Ngene")

file<-paste(gene,"matrix",sep=".")
if (!file.exists(file)) stop("File not found: ", file)
filecluster<-read.table(file,header=T,sep=" ",dec=".")
...

The call the script from the command line as:

$ Rscript MR.R ENSG00000154803

That should do it

/Henrik


On Sat, Nov 23, 2019 at 12:24 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
>
> it is confusing because in documentation they say this is how you run
> the script:
> https://github.com/eleporcu/TWMR
>
> I tried changing this on the script:
>
> cmd_args=commandArgs(TRUE)
> print(cmd_args)
> gene<-cmd_args[3]
> Ngwas<-239087
> N_eQTLs<-32000
> out<-c("gene","alpha","SE","P","Nsnps","Ngene")
>
> file<-paste(gene,"matrix",sep=".")
> if (!file.exists(file)) stop("File not found: ", file)
> filecluster<-read.table(file,header=T,sep=" ",dec=".")
> #file<-paste(gene,"matrix",sep=".")
> #filecluster<-read.table(file,header=T,sep=".",dec=" ")
> beta<-as.matrix(filecluster[,2:(length(filecluster[1,])-1)])
>
> and I run it:
> Rscript< MR.R --no-save ENSG00000154803.ld ENSG00000154803.matrix
> Error: unexpected numeric constant in "1.000 0.089"
> Execution halted
>
>
> I also tried this:
> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> Error: File not found: NA.matrix
> Execution halted
>
>
> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> Error: File not found: NA.matrix
> Execution halted
>
> On Sat, Nov 23, 2019 at 2:08 PM Henrik Bengtsson
> <henrik.bengtsson at gmail.com> wrote:
> >
> > Maybe it would help to add:
> >
> > file<-paste(gene,"matrix",sep=".")
> > if (!file.exists(file)) stop("File not found: ", file)
> > filecluster<-read.table(file,header=T,sep=" ",dec=".")
> >
> > /Henrik
> >
> > On Sat, Nov 23, 2019 at 11:55 AM Duncan Murdoch
> > <murdoch.duncan at gmail.com> wrote:
> > >
> > > On 23/11/2019 1:21 p.m., Ana Marija wrote:
> > > > Hi Duncan,
> > > >
> > > > thanks, I just did,
> > > >   Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > > [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> > > > Error in file(file, "rt") : cannot open the connection
> > > > Calls: read.table -> file
> > > > In addition: Warning message:
> > > > In file(file, "rt") :
> > > >    cannot open file 'NA.matrix': No such file or directory
> > > > Execution halted
> > > >
> > > Your script works with the third element in the list of arguments, and
> > > there are only two.
> > >
> > > Duncan Murdoch
> > >
> > > >
> > > > Please advise
> > > >
> > > > On Sat, Nov 23, 2019 at 12:13 PM Duncan Murdoch
> > > > <murdoch.duncan at gmail.com> wrote:
> > > >>
> > > >> On 23/11/2019 11:05 a.m., Ana Marija wrote:
> > > >>> Hi Ben,
> > > >>>
> > > >>> I am not sure what you mean when you say to print, is it this?
> > > >>>
> > > >>>> cmd_args=commandArgs(TRUE)
> > > >>>> print(cmd_args)
> > > >>> character(0)
> > > >>>> cmd_args=commandArgs()
> > > >>>> print(cmd_args)
> > > >>> [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
> > > >>>
> > > >>> I changed in the first line of this script:
> > > >>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> > > >>>
> > > >>> cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
> > > >>>
> > > >>> but again I get the same error:
> > > >>>
> > > >>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > >>> Error in file(file, "rt") : cannot open the connection
> > > >>> Calls: read.table -> file
> > > >>> In addition: Warning message:
> > > >>> In file(file, "rt") :
> > > >>>     cannot open file 'NA.matrix': No such file or directory
> > > >>> Execution halted
> > > >>
> > > >> You didn't put the print(cmd_args) into the script.
> > > >>
> > > >> Duncan Murdoch
> > > >>>
> > > >>>
> > > >>> Please advise,
> > > >>> Ana
> > > >>>
> > > >>> On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > > >>>>
> > > >>>> On 23/11/2019 10:26 a.m., Ana Marija wrote:
> > > >>>>> HI Ben,
> > > >>>>>
> > > >>>>> I tried it but it doesn't work:
> > > >>>>>
> > > >>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > >>>>> Error in file(file, "rt") : cannot open the connection
> > > >>>>> Calls: read.table -> file
> > > >>>>> In addition: Warning message:
> > > >>>>> In file(file, "rt") :
> > > >>>>>      cannot open file '--no-restore.matrix': No such file or directory
> > > >>>>> Execution halted
> > > >>>>>
> > > >>>>
> > > >>>> You should print the cmd_args variable that is set on the first line of
> > > >>>> that script.  When I run a script that prints it using your command
> > > >>>> line, this is what it looks like:
> > > >>>>
> > > >>>> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > >>>> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
> > > >>>> [2] "--slave"
> > > >>>> [3] "--no-restore"
> > > >>>> [4] "--no-save"
> > > >>>> [5] "--file=MR.R"
> > > >>>> [6] "--args"
> > > >>>> [7] "ENSG00000154803.ld"
> > > >>>> [8] "ENSG00000154803.matrix"
> > > >>>>
> > > >>>> The next line
> > > >>>>
> > > >>>> gene <- cmd_args[3]
> > > >>>>
> > > >>>> is obviously wrong for my system, because it would set gene to
> > > >>>> "--no-restore".  Your results will probably be somewhat different, but
> > > >>>> it might be clear what you should use instead of the third element.
> > > >>>>
> > > >>>> By the way, changing the first line
> > > >>>>
> > > >>>> cmd_args=commandArgs()
> > > >>>>
> > > >>>> to
> > > >>>>
> > > >>>> cmd_args <- commandArgs(TRUE)
> > > >>>>
> > > >>>> makes a lot of sense in most cases.  I haven't read your whole script so
> > > >>>> I don't know it it makes sense for you.
> > > >>>>
> > > >>>> Duncan Murdoch
> > > >>>>
> > > >>>>
> > > >>>>> Please advise,
> > > >>>>> Ana
> > > >>>>>
> > > >>>>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
> > > >>>>>>
> > > >>>>>> Hi,
> > > >>>>>>
> > > >>>>>> I think you want this order...
> > > >>>>>>
> > > >>>>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
> > > >>>>>>
> > > >>>>>> So, like this ...
> > > >>>>>>
> > > >>>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > >>>>>>
> > > >>>>>> Cheers,
> > > >>>>>> Ben
> > > >>>>>>
> > > >>>>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >>>>>>>
> > > >>>>>>> HI Ben,
> > > >>>>>>>
> > > >>>>>>> thank you so much , I did this:
> > > >>>>>>>
> > > >>>>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> > > >>>>>>> Error: unexpected numeric constant in "1.000 0.089"
> > > >>>>>>> Execution halted
> > > >>>>>>>
> > > >>>>>>> I made ENSG00000154803.ld with:
> > > >>>>>>> library(MASS)
> > > >>>>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> > > >>>>>>>
> > > >>>>>>> and it looks like this:
> > > >>>>>>>
> > > >>>>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> > > >>>>>>> 0.000 0.000 0.000 0.001 0.003 0.000
> > > >>>>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> > > >>>>>>> 0.001 0.012 0.005 0.000 0.004 0.004
> > > >>>>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> > > >>>>>>> 0.054 0.006 0.002 0.010 0.001 0.000
> > > >>>>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> > > >>>>>>> 0.066 0.010 0.030 0.001 0.003 0.000
> > > >>>>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> > > >>>>>>> 0.002 0.003 0.066 0.006 0.004 0.004
> > > >>>>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> > > >>>>>>> 0.016 0.000 0.025 0.000 0.005 0.000
> > > >>>>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> > > >>>>>>> 0.128 0.019 0.005 0.030 0.002 0.016
> > > >>>>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> > > >>>>>>> 0.098 0.010 0.012 0.001 0.006 0.003
> > > >>>>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> > > >>>>>>> 0.012 0.000 0.006 0.018 0.004 0.013
> > > >>>>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> > > >>>>>>> 0.001 0.091 0.057 0.062 0.002 0.005
> > > >>>>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> > > >>>>>>> 0.238 0.180 0.073 0.058 0.000 0.006
> > > >>>>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> > > >>>>>>> 1.000 0.158 0.006 0.044 0.006 0.001
> > > >>>>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> > > >>>>>>> 0.158 1.000 0.077 0.237 0.009 0.000
> > > >>>>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> > > >>>>>>> 0.006 0.077 1.000 0.056 0.000 0.004
> > > >>>>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> > > >>>>>>> 0.044 0.237 0.056 1.000 0.000 0.003
> > > >>>>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> > > >>>>>>> 0.006 0.009 0.000 0.000 1.000 0.002
> > > >>>>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> > > >>>>>>> 0.001 0.000 0.004 0.003 0.002 1.000
> > > >>>>>>>
> > > >>>>>>> the other file (ENSG00000154803.matrix) looks like this:
> > > >>>>>>>
> > > >>>>>>> GENES ENSG00000154803 BETA_GWAS
> > > >>>>>>> rs12601631 -0.320577 -0.0160778
> > > >>>>>>> rs1708623 0.708706 0.0717719
> > > >>>>>>> rs1708628 -0.645996 -0.0973019
> > > >>>>>>> rs17804843 -0.78984 0.0059607
> > > >>>>>>> rs4078062 -0.340732 -0.0716837
> > > >>>>>>> rs4316813 -0.721137 -0.00502219
> > > >>>>>>> rs7217764 -0.61641 0.16997
> > > >>>>>>> rs7221842 -0.377727 -0.00184011
> > > >>>>>>> rs12602831 -0.397059 0.0154625
> > > >>>>>>> rs138437542 -0.590669 0.0145733
> > > >>>>>>> rs2174369 -0.167913 -0.0268728
> > > >>>>>>> rs242252 0.20184 0.0161709
> > > >>>>>>> rs34121330 0.328602 0.0753894
> > > >>>>>>> rs4792798 -0.303601 0.00227314
> > > >>>>>>> rs7222311 -0.367686 -0.0419168
> > > >>>>>>> rs74369938 0.687555 -0.223105
> > > >>>>>>> rs8075751 -0.261916 -0.0313484
> > > >>>>>>>
> > > >>>>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> > > >>>>>>>>
> > > >>>>>>>> Hi,
> > > >>>>>>>>
> > > >>>>>>>> You might check the order of your arguments.   Options come before the
> > > >>>>>>>> script filename. See the details here...
> > > >>>>>>>>
> > > >>>>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> > > >>>>>>>>
> > > >>>>>>>> Ben
> > > >>>>>>>>
> > > >>>>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > >>>>>>>>>
> > > >>>>>>>>> Hello,
> > > >>>>>>>>>
> > > >>>>>>>>> I am trying to run this code:
> > > >>>>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> > > >>>>>>>>>
> > > >>>>>>>>> with r-3.6.1
> > > >>>>>>>>>
> > > >>>>>>>>> via:
> > > >>>>>>>>>
> > > >>>>>>>>> Rscript MR.R --no-save ENSG00000154803
> > > >>>>>>>>>
> > > >>>>>>>>> in the current directory I have saved: ENSG00000154803.ld and
> > > >>>>>>>>> ENSG00000154803.matrix as the software requires
> > > >>>>>>>>>
> > > >>>>>>>>> but I am getting this error:
> > > >>>>>>>>>
> > > >>>>>>>>> Error in file(file, "rt") : cannot open the connection
> > > >>>>>>>>> Calls: read.table -> file
> > > >>>>>>>>> In addition: Warning message:
> > > >>>>>>>>> In file(file, "rt") :
> > > >>>>>>>>>      cannot open file '--no-restore.matrix': No such file or directory
> > > >>>>>>>>> Execution halted
> > > >>>>>>>>>
> > > >>>>>>>>>
> > > >>>>>>>>> Please advise,
> > > >>>>>>>>>
> > > >>>>>>>>> Thanks
> > > >>>>>>>>> Ana
> > > >>>>>>>>>
> > > >>>>>>>>> ______________________________________________
> > > >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> > > >>>>>>>>
> > > >>>>>>>>
> > > >>>>>>>>
> > > >>>>>>>> --
> > > >>>>>>>> Ben Tupper
> > > >>>>>>>> Bigelow Laboratory for Ocean Science
> > > >>>>>>>> West Boothbay Harbor, Maine
> > > >>>>>>>> http://www.bigelow.org/
> > > >>>>>>>> https://eco.bigelow.org
> > > >>>>>>
> > > >>>>>>
> > > >>>>>>
> > > >>>>>> --
> > > >>>>>> Ben Tupper
> > > >>>>>> Bigelow Laboratory for Ocean Science
> > > >>>>>> West Boothbay Harbor, Maine
> > > >>>>>> http://www.bigelow.org/
> > > >>>>>> https://eco.bigelow.org
> > > >>>>>
> > > >>>>> ______________________________________________
> > > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > > >>>>>
> > > >>>>
> > > >>
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.


From @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com  Sat Nov 23 22:27:46 2019
From: @okov|c@@n@m@r|j@ @end|ng |rom gm@||@com (Ana Marija)
Date: Sat, 23 Nov 2019 15:27:46 -0600
Subject: [R] cannot open file '--no-restore.matrix'
In-Reply-To: <CAFDcVCSKVj3pv3WdNUHSos0wzSfYvrq0P_yAwBAToeerFyFyyQ@mail.gmail.com>
References: <CAF9-5jOSm=eFkpmOK_TrVA2p2zFo5AQok83Nm=Tx29TXL7NU4A@mail.gmail.com>
 <CALrbzg3qJ7gOrXxXC-pssm26Eqsgo5RwOvReNjOB=8s36A7wDw@mail.gmail.com>
 <CAF9-5jMeKJOOFJOyGULBeB32MH7h5dZ3b8Dm=+Kb-7kchhNLqQ@mail.gmail.com>
 <CALrbzg0nPB=rno3EsYy_wLg8A6dc8Yr+vw8ut8worczTH6mc=w@mail.gmail.com>
 <CAF9-5jOsEFAddS-3f3uHoagq-oYbZJzF0PwknJSgWUzr+9te1A@mail.gmail.com>
 <770ea0ba-c7f6-2a07-9356-de7768145b03@gmail.com>
 <CAF9-5jMKE4pHG2iWyYQsHuoUGa=y_NwS0UB39JU8vCVycJY=JQ@mail.gmail.com>
 <e1f75329-20c6-8cd5-70c5-5a6c409b2255@gmail.com>
 <CAF9-5jM3sgwET82hU_g8T=t4Z-WYYVQi6dQwAd2u_9L2WkmEng@mail.gmail.com>
 <05656494-5a29-8895-40c8-c069c8ddda35@gmail.com>
 <CAFDcVCS8F0EhO62wcMiOHK7WEuwN80bnYxLM9re+GMVM27Szvw@mail.gmail.com>
 <CAF9-5jO0+yx-zM7agxAE-Ncj+7_U7sM7tLq2aF0mYn=o4Yuyew@mail.gmail.com>
 <CAFDcVCSKVj3pv3WdNUHSos0wzSfYvrq0P_yAwBAToeerFyFyyQ@mail.gmail.com>
Message-ID: <CAF9-5jPj1NdJOMtqBxA09Ri442SRK7Fb3z832ArPfSCTvrRb0w@mail.gmail.com>

Thank you so much this indeed solved my issue!

On Sat, Nov 23, 2019 at 3:05 PM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> Looking at the https://github.com/eleporcu/TWMR/blob/master/README.txt,
> it looks like you should pass a single argument when you call the MR.R
> script and it should be the name of a gene, e.g. 'ENSG00000154803'.
> You are passing two arguments and they look like filenames.
>
> Update the script, because it should really use commandArgs(TRUE), to:
>
> cmd_args <- commandArgs(TRUE)
> if (length(cmd_args) == 0L) stop("No arguments specified.")
> print(cmd_args)
> gene<-cmd_args[length(cmd_args)]   ## Last argument is the 'gene'
> Ngwas<-239087
> N_eQTLs<-32000
> out<-c("gene","alpha","SE","P","Nsnps","Ngene")
>
> file<-paste(gene,"matrix",sep=".")
> if (!file.exists(file)) stop("File not found: ", file)
> filecluster<-read.table(file,header=T,sep=" ",dec=".")
> ...
>
> The call the script from the command line as:
>
> $ Rscript MR.R ENSG00000154803
>
> That should do it
>
> /Henrik
>
>
> On Sat, Nov 23, 2019 at 12:24 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> >
> > it is confusing because in documentation they say this is how you run
> > the script:
> > https://github.com/eleporcu/TWMR
> >
> > I tried changing this on the script:
> >
> > cmd_args=commandArgs(TRUE)
> > print(cmd_args)
> > gene<-cmd_args[3]
> > Ngwas<-239087
> > N_eQTLs<-32000
> > out<-c("gene","alpha","SE","P","Nsnps","Ngene")
> >
> > file<-paste(gene,"matrix",sep=".")
> > if (!file.exists(file)) stop("File not found: ", file)
> > filecluster<-read.table(file,header=T,sep=" ",dec=".")
> > #file<-paste(gene,"matrix",sep=".")
> > #filecluster<-read.table(file,header=T,sep=".",dec=" ")
> > beta<-as.matrix(filecluster[,2:(length(filecluster[1,])-1)])
> >
> > and I run it:
> > Rscript< MR.R --no-save ENSG00000154803.ld ENSG00000154803.matrix
> > Error: unexpected numeric constant in "1.000 0.089"
> > Execution halted
> >
> >
> > I also tried this:
> > Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> > Error: File not found: NA.matrix
> > Execution halted
> >
> >
> > Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> > Error: File not found: NA.matrix
> > Execution halted
> >
> > On Sat, Nov 23, 2019 at 2:08 PM Henrik Bengtsson
> > <henrik.bengtsson at gmail.com> wrote:
> > >
> > > Maybe it would help to add:
> > >
> > > file<-paste(gene,"matrix",sep=".")
> > > if (!file.exists(file)) stop("File not found: ", file)
> > > filecluster<-read.table(file,header=T,sep=" ",dec=".")
> > >
> > > /Henrik
> > >
> > > On Sat, Nov 23, 2019 at 11:55 AM Duncan Murdoch
> > > <murdoch.duncan at gmail.com> wrote:
> > > >
> > > > On 23/11/2019 1:21 p.m., Ana Marija wrote:
> > > > > Hi Duncan,
> > > > >
> > > > > thanks, I just did,
> > > > >   Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > > > [1] "ENSG00000154803.ld"     "ENSG00000154803.matrix"
> > > > > Error in file(file, "rt") : cannot open the connection
> > > > > Calls: read.table -> file
> > > > > In addition: Warning message:
> > > > > In file(file, "rt") :
> > > > >    cannot open file 'NA.matrix': No such file or directory
> > > > > Execution halted
> > > > >
> > > > Your script works with the third element in the list of arguments, and
> > > > there are only two.
> > > >
> > > > Duncan Murdoch
> > > >
> > > > >
> > > > > Please advise
> > > > >
> > > > > On Sat, Nov 23, 2019 at 12:13 PM Duncan Murdoch
> > > > > <murdoch.duncan at gmail.com> wrote:
> > > > >>
> > > > >> On 23/11/2019 11:05 a.m., Ana Marija wrote:
> > > > >>> Hi Ben,
> > > > >>>
> > > > >>> I am not sure what you mean when you say to print, is it this?
> > > > >>>
> > > > >>>> cmd_args=commandArgs(TRUE)
> > > > >>>> print(cmd_args)
> > > > >>> character(0)
> > > > >>>> cmd_args=commandArgs()
> > > > >>>> print(cmd_args)
> > > > >>> [1] "/software/linux-el7-x86_64/compilers/r-3.6.1/lib64/R/bin/exec/R"
> > > > >>>
> > > > >>> I changed in the first line of this script:
> > > > >>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> > > > >>>
> > > > >>> cmd_args=commandArgs() to be cmd_args=commandArgs(TRUE)
> > > > >>>
> > > > >>> but again I get the same error:
> > > > >>>
> > > > >>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > > >>> Error in file(file, "rt") : cannot open the connection
> > > > >>> Calls: read.table -> file
> > > > >>> In addition: Warning message:
> > > > >>> In file(file, "rt") :
> > > > >>>     cannot open file 'NA.matrix': No such file or directory
> > > > >>> Execution halted
> > > > >>
> > > > >> You didn't put the print(cmd_args) into the script.
> > > > >>
> > > > >> Duncan Murdoch
> > > > >>>
> > > > >>>
> > > > >>> Please advise,
> > > > >>> Ana
> > > > >>>
> > > > >>> On Sat, Nov 23, 2019 at 9:44 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > > > >>>>
> > > > >>>> On 23/11/2019 10:26 a.m., Ana Marija wrote:
> > > > >>>>> HI Ben,
> > > > >>>>>
> > > > >>>>> I tried it but it doesn't work:
> > > > >>>>>
> > > > >>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > > >>>>> Error in file(file, "rt") : cannot open the connection
> > > > >>>>> Calls: read.table -> file
> > > > >>>>> In addition: Warning message:
> > > > >>>>> In file(file, "rt") :
> > > > >>>>>      cannot open file '--no-restore.matrix': No such file or directory
> > > > >>>>> Execution halted
> > > > >>>>>
> > > > >>>>
> > > > >>>> You should print the cmd_args variable that is set on the first line of
> > > > >>>> that script.  When I run a script that prints it using your command
> > > > >>>> line, this is what it looks like:
> > > > >>>>
> > > > >>>> $ Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > > >>>> [1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
> > > > >>>> [2] "--slave"
> > > > >>>> [3] "--no-restore"
> > > > >>>> [4] "--no-save"
> > > > >>>> [5] "--file=MR.R"
> > > > >>>> [6] "--args"
> > > > >>>> [7] "ENSG00000154803.ld"
> > > > >>>> [8] "ENSG00000154803.matrix"
> > > > >>>>
> > > > >>>> The next line
> > > > >>>>
> > > > >>>> gene <- cmd_args[3]
> > > > >>>>
> > > > >>>> is obviously wrong for my system, because it would set gene to
> > > > >>>> "--no-restore".  Your results will probably be somewhat different, but
> > > > >>>> it might be clear what you should use instead of the third element.
> > > > >>>>
> > > > >>>> By the way, changing the first line
> > > > >>>>
> > > > >>>> cmd_args=commandArgs()
> > > > >>>>
> > > > >>>> to
> > > > >>>>
> > > > >>>> cmd_args <- commandArgs(TRUE)
> > > > >>>>
> > > > >>>> makes a lot of sense in most cases.  I haven't read your whole script so
> > > > >>>> I don't know it it makes sense for you.
> > > > >>>>
> > > > >>>> Duncan Murdoch
> > > > >>>>
> > > > >>>>
> > > > >>>>> Please advise,
> > > > >>>>> Ana
> > > > >>>>>
> > > > >>>>> On Sat, Nov 23, 2019 at 4:16 AM Ben Tupper <btupper at bigelow.org> wrote:
> > > > >>>>>>
> > > > >>>>>> Hi,
> > > > >>>>>>
> > > > >>>>>> I think you want this order...
> > > > >>>>>>
> > > > >>>>>> Rscript [options for R] script_file.R argument_1 argument_2 ...
> > > > >>>>>>
> > > > >>>>>> So, like this ...
> > > > >>>>>>
> > > > >>>>>> Rscript --no-save MR.R ENSG00000154803.ld ENSG00000154803.matrix
> > > > >>>>>>
> > > > >>>>>> Cheers,
> > > > >>>>>> Ben
> > > > >>>>>>
> > > > >>>>>> On Fri, Nov 22, 2019 at 8:59 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > > >>>>>>>
> > > > >>>>>>> HI Ben,
> > > > >>>>>>>
> > > > >>>>>>> thank you so much , I did this:
> > > > >>>>>>>
> > > > >>>>>>> Rscript --no-save ENSG00000154803.ld ENSG00000154803.matrix  MR.R
> > > > >>>>>>> Error: unexpected numeric constant in "1.000 0.089"
> > > > >>>>>>> Execution halted
> > > > >>>>>>>
> > > > >>>>>>> I made ENSG00000154803.ld with:
> > > > >>>>>>> library(MASS)
> > > > >>>>>>> write.matrix(ENSG00000154803.ld, file="ENSG00000154803.ld")
> > > > >>>>>>>
> > > > >>>>>>> and it looks like this:
> > > > >>>>>>>
> > > > >>>>>>> 1.000 0.089 0.006 0.038 0.012 0.014 0.003 0.001 0.005 0.015 0.013
> > > > >>>>>>> 0.000 0.000 0.000 0.001 0.003 0.000
> > > > >>>>>>> 0.089 1.000 0.002 0.007 0.005 0.001 0.004 0.005 0.000 0.003 0.014
> > > > >>>>>>> 0.001 0.012 0.005 0.000 0.004 0.004
> > > > >>>>>>> 0.006 0.002 1.000 0.004 0.008 0.029 0.040 0.001 0.001 0.006 0.013
> > > > >>>>>>> 0.054 0.006 0.002 0.010 0.001 0.000
> > > > >>>>>>> 0.038 0.007 0.004 1.000 0.460 0.044 0.008 0.022 0.010 0.229 0.095
> > > > >>>>>>> 0.066 0.010 0.030 0.001 0.003 0.000
> > > > >>>>>>> 0.012 0.005 0.008 0.460 1.000 0.151 0.018 0.047 0.021 0.272 0.185
> > > > >>>>>>> 0.002 0.003 0.066 0.006 0.004 0.004
> > > > >>>>>>> 0.014 0.001 0.029 0.044 0.151 1.000 0.007 0.018 0.218 0.010 0.023
> > > > >>>>>>> 0.016 0.000 0.025 0.000 0.005 0.000
> > > > >>>>>>> 0.003 0.004 0.040 0.008 0.018 0.007 1.000 0.003 0.002 0.020 0.031
> > > > >>>>>>> 0.128 0.019 0.005 0.030 0.002 0.016
> > > > >>>>>>> 0.001 0.005 0.001 0.022 0.047 0.018 0.003 1.000 0.004 0.014 0.004
> > > > >>>>>>> 0.098 0.010 0.012 0.001 0.006 0.003
> > > > >>>>>>> 0.005 0.000 0.001 0.010 0.021 0.218 0.002 0.004 1.000 0.004 0.000
> > > > >>>>>>> 0.012 0.000 0.006 0.018 0.004 0.013
> > > > >>>>>>> 0.015 0.003 0.006 0.229 0.272 0.010 0.020 0.014 0.004 1.000 0.466
> > > > >>>>>>> 0.001 0.091 0.057 0.062 0.002 0.005
> > > > >>>>>>> 0.013 0.014 0.013 0.095 0.185 0.023 0.031 0.004 0.000 0.466 1.000
> > > > >>>>>>> 0.238 0.180 0.073 0.058 0.000 0.006
> > > > >>>>>>> 0.000 0.001 0.054 0.066 0.002 0.016 0.128 0.098 0.012 0.001 0.238
> > > > >>>>>>> 1.000 0.158 0.006 0.044 0.006 0.001
> > > > >>>>>>> 0.000 0.012 0.006 0.010 0.003 0.000 0.019 0.010 0.000 0.091 0.180
> > > > >>>>>>> 0.158 1.000 0.077 0.237 0.009 0.000
> > > > >>>>>>> 0.000 0.005 0.002 0.030 0.066 0.025 0.005 0.012 0.006 0.057 0.073
> > > > >>>>>>> 0.006 0.077 1.000 0.056 0.000 0.004
> > > > >>>>>>> 0.001 0.000 0.010 0.001 0.006 0.000 0.030 0.001 0.018 0.062 0.058
> > > > >>>>>>> 0.044 0.237 0.056 1.000 0.000 0.003
> > > > >>>>>>> 0.003 0.004 0.001 0.003 0.004 0.005 0.002 0.006 0.004 0.002 0.000
> > > > >>>>>>> 0.006 0.009 0.000 0.000 1.000 0.002
> > > > >>>>>>> 0.000 0.004 0.000 0.000 0.004 0.000 0.016 0.003 0.013 0.005 0.006
> > > > >>>>>>> 0.001 0.000 0.004 0.003 0.002 1.000
> > > > >>>>>>>
> > > > >>>>>>> the other file (ENSG00000154803.matrix) looks like this:
> > > > >>>>>>>
> > > > >>>>>>> GENES ENSG00000154803 BETA_GWAS
> > > > >>>>>>> rs12601631 -0.320577 -0.0160778
> > > > >>>>>>> rs1708623 0.708706 0.0717719
> > > > >>>>>>> rs1708628 -0.645996 -0.0973019
> > > > >>>>>>> rs17804843 -0.78984 0.0059607
> > > > >>>>>>> rs4078062 -0.340732 -0.0716837
> > > > >>>>>>> rs4316813 -0.721137 -0.00502219
> > > > >>>>>>> rs7217764 -0.61641 0.16997
> > > > >>>>>>> rs7221842 -0.377727 -0.00184011
> > > > >>>>>>> rs12602831 -0.397059 0.0154625
> > > > >>>>>>> rs138437542 -0.590669 0.0145733
> > > > >>>>>>> rs2174369 -0.167913 -0.0268728
> > > > >>>>>>> rs242252 0.20184 0.0161709
> > > > >>>>>>> rs34121330 0.328602 0.0753894
> > > > >>>>>>> rs4792798 -0.303601 0.00227314
> > > > >>>>>>> rs7222311 -0.367686 -0.0419168
> > > > >>>>>>> rs74369938 0.687555 -0.223105
> > > > >>>>>>> rs8075751 -0.261916 -0.0313484
> > > > >>>>>>>
> > > > >>>>>>> On Fri, Nov 22, 2019 at 7:44 PM Ben Tupper <btupper at bigelow.org> wrote:
> > > > >>>>>>>>
> > > > >>>>>>>> Hi,
> > > > >>>>>>>>
> > > > >>>>>>>> You might check the order of your arguments.   Options come before the
> > > > >>>>>>>> script filename. See the details here...
> > > > >>>>>>>>
> > > > >>>>>>>> https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/Rscript
> > > > >>>>>>>>
> > > > >>>>>>>> Ben
> > > > >>>>>>>>
> > > > >>>>>>>> On Fri, Nov 22, 2019 at 8:17 PM Ana Marija <sokovic.anamarija at gmail.com> wrote:
> > > > >>>>>>>>>
> > > > >>>>>>>>> Hello,
> > > > >>>>>>>>>
> > > > >>>>>>>>> I am trying to run this code:
> > > > >>>>>>>>> https://github.com/eleporcu/TWMR/blob/master/MR.R
> > > > >>>>>>>>>
> > > > >>>>>>>>> with r-3.6.1
> > > > >>>>>>>>>
> > > > >>>>>>>>> via:
> > > > >>>>>>>>>
> > > > >>>>>>>>> Rscript MR.R --no-save ENSG00000154803
> > > > >>>>>>>>>
> > > > >>>>>>>>> in the current directory I have saved: ENSG00000154803.ld and
> > > > >>>>>>>>> ENSG00000154803.matrix as the software requires
> > > > >>>>>>>>>
> > > > >>>>>>>>> but I am getting this error:
> > > > >>>>>>>>>
> > > > >>>>>>>>> Error in file(file, "rt") : cannot open the connection
> > > > >>>>>>>>> Calls: read.table -> file
> > > > >>>>>>>>> In addition: Warning message:
> > > > >>>>>>>>> In file(file, "rt") :
> > > > >>>>>>>>>      cannot open file '--no-restore.matrix': No such file or directory
> > > > >>>>>>>>> Execution halted
> > > > >>>>>>>>>
> > > > >>>>>>>>>
> > > > >>>>>>>>> Please advise,
> > > > >>>>>>>>>
> > > > >>>>>>>>> Thanks
> > > > >>>>>>>>> Ana
> > > > >>>>>>>>>
> > > > >>>>>>>>> ______________________________________________
> > > > >>>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > >>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >>>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > >>>>>>>>> and provide commented, minimal, self-contained, reproducible code.
> > > > >>>>>>>>
> > > > >>>>>>>>
> > > > >>>>>>>>
> > > > >>>>>>>> --
> > > > >>>>>>>> Ben Tupper
> > > > >>>>>>>> Bigelow Laboratory for Ocean Science
> > > > >>>>>>>> West Boothbay Harbor, Maine
> > > > >>>>>>>> http://www.bigelow.org/
> > > > >>>>>>>> https://eco.bigelow.org
> > > > >>>>>>
> > > > >>>>>>
> > > > >>>>>>
> > > > >>>>>> --
> > > > >>>>>> Ben Tupper
> > > > >>>>>> Bigelow Laboratory for Ocean Science
> > > > >>>>>> West Boothbay Harbor, Maine
> > > > >>>>>> http://www.bigelow.org/
> > > > >>>>>> https://eco.bigelow.org
> > > > >>>>>
> > > > >>>>> ______________________________________________
> > > > >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> > > > >>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > >>>>> and provide commented, minimal, self-contained, reproducible code.
> > > > >>>>>
> > > > >>>>
> > > > >>
> > > >
> > > > ______________________________________________
> > > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > > and provide commented, minimal, self-contained, reproducible code.


From kry|ov@r00t @end|ng |rom gm@||@com  Sun Nov 24 15:06:14 2019
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sun, 24 Nov 2019 17:06:14 +0300
Subject: [R] Why does `[<-.matrix` not exist in base R
In-Reply-To: <CACg022-CQ_cqiTnT6vmdKdi2FhYFTzVoP5o_vVx_pFGvmN9Fiw@mail.gmail.com>
References: <CACg022-CQ_cqiTnT6vmdKdi2FhYFTzVoP5o_vVx_pFGvmN9Fiw@mail.gmail.com>
Message-ID: <20191124170614.019427a1@Tarkus>

Hello David,

On Sat, 23 Nov 2019 11:58:42 -0500
David Disabato <ddisab01 at gmail.com> wrote:

> For example, if I want to add a new column to a data.frame, I can do
> something like `myDataFrame[, "newColumn"] <- NA`.

<Opinion>

Arguably, iterative growth of data structures is not the "R style",
since it may lead to costly reallocations, resulting in the worst case
scenario of quadratic behaviour for linear operations.

If iterative processing is unavoidable, it might help to store partial
results in a list, then build the final matrix with a single call to
do.call(cbind, results).

</Opinion>

> However, with a matrix, this syntax does not work and I have to use a
> call to `cbind` and create a new object. For example, `mymatrix2 <-
> cbind(mymatrix, "newColumn" = NA)`.

> Is there a programming reason that base R does not have a matrix
> method for `[<-` or is it something that arguably should be added?

A data frame is a list of columns, so adding a new column is relatively
cheap: allocate enough memory for one column and append (roughly
speaking) a pointer to the list of pointers-to-column-data. This
results in reallocation of the *latter* list, but, since that list is
small in comparison to the whole data frame, it's okay. Note that this
operation does not affect any of the other columns belonging to the
same data frame.

A matrix, on the other hand, is a vector containing the whole matrix
with array dimensions stored as an attribute. Since R matrices are
stored by column [*], adding a new column to the matrix means resizing
the buffer to hold length(matrix) + nrow(matrix) elements, then
appending the new column to the end of the buffer. If the allocator
cannot enlarge the buffer in place (because the buffer is followed in
memory by another buffer), it has to allocate the new buffer elsewhere,
copy the memory, then free the old buffer.

To build a matrix by appending columns, one needs to perform this O(n)
operation O(n) times, resulting in O(n^2) performance. Adding rows is
even worse because memory has to be copied in parts, not as a whole.

Disclaimer: this is one reason I can think about why doesn't R offer
subassignment to non-existent matrix columns by default. The actual
reason might be different.

-- 
Best regards,
Ivan

[*]
https://github.com/wch/r-source/blob/bac4cd3013ead1379e20127d056ee036278b47ff/src/main/duplicate.c#L443


From pd@|gd @end|ng |rom gm@||@com  Sun Nov 24 15:47:02 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sun, 24 Nov 2019 15:47:02 +0100
Subject: [R] Why does `[<-.matrix` not exist in base R
In-Reply-To: <CACg022-CQ_cqiTnT6vmdKdi2FhYFTzVoP5o_vVx_pFGvmN9Fiw@mail.gmail.com>
References: <CACg022-CQ_cqiTnT6vmdKdi2FhYFTzVoP5o_vVx_pFGvmN9Fiw@mail.gmail.com>
Message-ID: <2E85F37E-FC88-4F9D-8AE3-6A196F378EEB@gmail.com>

The subject is misguided. It is not a problem to assign to a subset of columns. 

The issue is that the assignment operation does not want to _expand_ the matrix automatically upon seeing an out-of-bounds index. E.g.:

> M <- matrix(0,2,2)
> M[,3]<-1
Error in `[<-`(`*tmp*`, , 3, value = 1) : subscript out of bounds
> M[,2]<-1
> M
     [,1] [,2]
[1,]    0    1
[2,]    0    1

You can, however, do things like this:

> M <- M[,c(1,2,2)]
> M[,3]<-3
> M
     [,1] [,2] [,3]
[1,]    0    1    3
[2,]    0    1    3

-pd 

> On 23 Nov 2019, at 17:58 , David Disabato <ddisab01 at gmail.com> wrote:
> 
> Whenever going from working with a data.frame to a matrix, I get annoyed
> that I cannot assign and subset at the same time with matrices - like I can
> with data.frames.
> 
> For example, if I want to add a new column to a data.frame, I can do
> something like `myDataFrame[, "newColumn"] <- NA`.
> 
> However, with a matrix, this syntax does not work and I have to use a call
> to `cbind` and create a new object. For example, `mymatrix2 <-
> cbind(mymatrix, "newColumn" = NA)`.
> 
> Is there a programming reason that base R does not have a matrix method for
> `[<-` or is it something that arguably should be added?
> 
> -- 
> David J. Disabato, Ph.D.
> Postdoctoral Research Scholar
> Kent State University
> ddisab01 at gmail.com
> 
> Email is not a secure form of communication as information and
> confidentiality cannot be guaranteed. Information provided in an email is
> not intended to be a professional service. In the case of a crisis or
> emergency situation, call 911.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From M@Thev@r@j@ @end|ng |rom m@@@ey@@c@nz  Mon Nov 25 02:53:54 2019
From: M@Thev@r@j@ @end|ng |rom m@@@ey@@c@nz (Thevaraja, Mayooran)
Date: Mon, 25 Nov 2019 01:53:54 +0000
Subject: [R] ggplot inside of the function
Message-ID: <SYXPR01MB130992459DCE9AF8E24351EAC94A0@SYXPR01MB1309.ausprd01.prod.outlook.com>

Hello Folks
               I am working some R package development. When I was using ggplot inside of the function, I need to get the output graph's legend must be corresponding to the input parameters numerical value (need to be automatically changed when we input different parameters).  Therefore anyone has any ideas? Here, I have given below a simple example. I need to get my output graph's legend should be Norm(mu=0.2, var=0.8),...


example <- function(mu1,mu2,mu3,var1,var2,var3){
  x <- seq(-3,3, by=0.01)
  p_1 <- dnorm(x,mu1,var1)
  p_2 <- dnorm(x,mu2,var2)
  p_3 <- dnorm(x,mu3,var3)
  Prob_df <- data.frame(x,p_1,p_2,p_3)
  Prob <- plyr::rename(Prob_df, c("p_1"="Norm(mu=mu1, var=var1)","p_2"="Norm(mu=mu2, var=var2)","p_3"="Norm(mu=mu3, var=var3)"))
  melten.Prob <- reshape2::melt(Prob, id = "x", variable.name = "Methods", value.name = "Prob")
ggplot2::ggplot(melten.Prob)+
    ggplot2::geom_point(ggplot2::aes(x = x, y = Prob, group= Methods, colour = Methods ))+
    ggplot2::geom_line(ggplot2::aes(x = x, y = Prob, group= Methods, colour = Methods ))

}

mu1 <- 0.2
mu2 <- 0.5
mu3 <- 1
var1 <- 0.8
var2 <- 1.2
var3 <- 2.1
example(mu1,mu2,mu3,var1,var2,var3)


Thanks
Mayooran

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon Nov 25 03:59:48 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Sun, 24 Nov 2019 18:59:48 -0800
Subject: [R] Why does `[<-.matrix` not exist in base R
In-Reply-To: <2E85F37E-FC88-4F9D-8AE3-6A196F378EEB@gmail.com>
References: <CACg022-CQ_cqiTnT6vmdKdi2FhYFTzVoP5o_vVx_pFGvmN9Fiw@mail.gmail.com>
 <2E85F37E-FC88-4F9D-8AE3-6A196F378EEB@gmail.com>
Message-ID: <CAGxFJbQjBZ1WGBLmACjXt+dEmuwK3aPNQrfPeuT6JzJejuBOPA@mail.gmail.com>

Re: [<-.

It is perhaps worth noting that the OP seems "misguided" in another sense.
His complaint seems to rest on the assumption that because matrices and
data frames both have a row/column structure, certain operations on them
should be similar. I disagree. In fact, data frames and matrices are very
different structures with very different semantics and wholly different
purposes. Their "similarity" is superficial. First and foremost, (numeric)
matrices are numerical objects, the basic building blocks for linear
algebra with a whole devoted set of algebraic functionality for them (see
also: BLAS) ; while data frames are essentially data storage/manipulation
structures, internal data bases for R. As a result, imo, there is good
reason that [<-. should *not* behave with matrices as it does with data
frames: when doing complex matrix calculations, returning an error message
when indices go out of range seems much more desirable than silently
changing dimensions. Indeed, I think one might make a better argument for
doing that for data frames also, but, as it is both relativey innocuous and
convenient to add columns in that context -- the data frame method is just
a wrapper for data.frame() as the man page says -- it's not really an issue
(and certainly shouldn't be altered now).

Perhaps a moral: one should be very wary of assuming that behavior that you
think is "natural" and "desirable" would be assumed to be so by others.
Especially for long used and extensively exercised core functionality.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Nov 24, 2019 at 6:47 AM peter dalgaard <pdalgd at gmail.com> wrote:

> The subject is misguided. It is not a problem to assign to a subset of
> columns.
>
> The issue is that the assignment operation does not want to _expand_ the
> matrix automatically upon seeing an out-of-bounds index. E.g.:
>
> > M <- matrix(0,2,2)
> > M[,3]<-1
> Error in `[<-`(`*tmp*`, , 3, value = 1) : subscript out of bounds
> > M[,2]<-1
> > M
>      [,1] [,2]
> [1,]    0    1
> [2,]    0    1
>
> You can, however, do things like this:
>
> > M <- M[,c(1,2,2)]
> > M[,3]<-3
> > M
>      [,1] [,2] [,3]
> [1,]    0    1    3
> [2,]    0    1    3
>
> -pd
>
> > On 23 Nov 2019, at 17:58 , David Disabato <ddisab01 at gmail.com> wrote:
> >
> > Whenever going from working with a data.frame to a matrix, I get annoyed
> > that I cannot assign and subset at the same time with matrices - like I
> can
> > with data.frames.
> >
> > For example, if I want to add a new column to a data.frame, I can do
> > something like `myDataFrame[, "newColumn"] <- NA`.
> >
> > However, with a matrix, this syntax does not work and I have to use a
> call
> > to `cbind` and create a new object. For example, `mymatrix2 <-
> > cbind(mymatrix, "newColumn" = NA)`.
> >
> > Is there a programming reason that base R does not have a matrix method
> for
> > `[<-` or is it something that arguably should be added?
> >
> > --
> > David J. Disabato, Ph.D.
> > Postdoctoral Research Scholar
> > Kent State University
> > ddisab01 at gmail.com
> >
> > Email is not a secure form of communication as information and
> > confidentiality cannot be guaranteed. Information provided in an email is
> > not intended to be a professional service. In the case of a crisis or
> > emergency situation, call 911.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From er|cjberger @end|ng |rom gm@||@com  Mon Nov 25 11:22:17 2019
From: er|cjberger @end|ng |rom gm@||@com (Eric Berger)
Date: Mon, 25 Nov 2019 12:22:17 +0200
Subject: [R] ggplot inside of the function
In-Reply-To: <SYXPR01MB130992459DCE9AF8E24351EAC94A0@SYXPR01MB1309.ausprd01.prod.outlook.com>
References: <SYXPR01MB130992459DCE9AF8E24351EAC94A0@SYXPR01MB1309.ausprd01.prod.outlook.com>
Message-ID: <CAGgJW76DBkW6Y5RmpXULCktFTBu0OfMDQiXgbEUsREvPHmfbQg@mail.gmail.com>

Hi Mayooran,
If you define the following function

f <- function(m,v) { sprintf("Norm(mu=%.1f, var=%.1f)",m,v) }

Then you can modify the setting of Prob as follows

 Prob <- plyr::rename(Prob_df,
c("p_1"=f(mu1,var1),"p_2"=f(mu2,var2),"p_3"=f(mu3,var3)))

The lesson here is that wherever you set a variable to a string, as in
"p_1"="Norm(mu=mu1,var=var1)",
you can create the string dynamically using the function sprintf(),
which returns a string.

Best,
Eric

On Mon, Nov 25, 2019 at 3:54 AM Thevaraja, Mayooran
<M.Thevaraja at massey.ac.nz> wrote:
>
> Hello Folks
>                I am working some R package development. When I was using ggplot inside of the function, I need to get the output graph's legend must be corresponding to the input parameters numerical value (need to be automatically changed when we input different parameters).  Therefore anyone has any ideas? Here, I have given below a simple example. I need to get my output graph's legend should be Norm(mu=0.2, var=0.8),...
>
>
> example <- function(mu1,mu2,mu3,var1,var2,var3){
>   x <- seq(-3,3, by=0.01)
>   p_1 <- dnorm(x,mu1,var1)
>   p_2 <- dnorm(x,mu2,var2)
>   p_3 <- dnorm(x,mu3,var3)
>   Prob_df <- data.frame(x,p_1,p_2,p_3)
>   Prob <- plyr::rename(Prob_df, c("p_1"="Norm(mu=mu1, var=var1)","p_2"="Norm(mu=mu2, var=var2)","p_3"="Norm(mu=mu3, var=var3)"))
>   melten.Prob <- reshape2::melt(Prob, id = "x", variable.name = "Methods", value.name = "Prob")
> ggplot2::ggplot(melten.Prob)+
>     ggplot2::geom_point(ggplot2::aes(x = x, y = Prob, group= Methods, colour = Methods ))+
>     ggplot2::geom_line(ggplot2::aes(x = x, y = Prob, group= Methods, colour = Methods ))
>
> }
>
> mu1 <- 0.2
> mu2 <- 0.5
> mu3 <- 1
> var1 <- 0.8
> var2 <- 1.2
> var3 <- 2.1
> example(mu1,mu2,mu3,var1,var2,var3)
>
>
> Thanks
> Mayooran
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Mon Nov 25 11:34:30 2019
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Mon, 25 Nov 2019 16:04:30 +0530
Subject: [R] [r] How to obtain the correlation coefficients between two
 variables in the R object
Message-ID: <CAOFE=kP91e+jo1nu-1PCNc_P+sPcpxyC8+dzoNwO-caqj2=kaQ@mail.gmail.com>

Dear R-users,

I am estimating the DCC-model by using the following code and successfully
making the individual plots indicating the dynamic correlation between the
two variables stored in the R data frame (namely dcc.fit in my code). My
code is

Dat = Date2[,2:13, drop = FALSE]
library(rmgarch)
library(rugarch)
xspec = ugarchspec(mean.model = list(armaOrder = c(1, 1)), variance.model =
list(garchOrder = c(1,1), model = 'eGARCH'), distribution.model = 'norm')
uspec = multispec(replicate(12, xspec)) # Define a function using xspec
dcc.garch11spec = dccspec(uspec = uspec, dccOrder = c(1, 1), distribution =
'mvnorm')
dcc.fit = dccfit(dcc.garch11spec, data = Dat)
r1=rcor(dcc.fit, type="R")
print(r1)

Here, I need to obtain the DCC relationship between the two variables. I
successfully make a separate plot for the correlation between the two
variables with the following code.
plot(rcor(dcc.fit, type="R")['A1','A2',], type='l')
plot(rcor(dcc.fit, type="R")['B1','B2',], type='l')
plot(rcor(dcc.fit, type="R")['C1','C2',], type='l')
plot(rcor(dcc.fit, type="R")['D1','D2',], type='l')
plot(rcor(dcc.fit, type="R")['E1','E2',], type='l')
plot(rcor(dcc.fit, type="R")['F1','F2',], type='l')

Here, I request your help on two things which I am unable to do.

1. I need to arrange all the above individual plots (counted as 6) on one
page.
2. I need to obtain the correlation coefficients between the particular two
variables (i.e. A1 versus A2, B1 versus B2, .......so on) throughout the
entire period.

For your convenience, I have attached my sample data.

I definitely expect a positive help from the experts as the suggestions
from the expert through this R forum has always solved my previous problem.

Thank you.

-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
11/25/19,
04:00:39 PM

From ||@t@ @end|ng |rom dewey@myzen@co@uk  Mon Nov 25 14:40:57 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Mon, 25 Nov 2019 13:40:57 +0000
Subject: [R] [r] How to obtain the correlation coefficients between two
 variables in the R object
In-Reply-To: <CAOFE=kP91e+jo1nu-1PCNc_P+sPcpxyC8+dzoNwO-caqj2=kaQ@mail.gmail.com>
References: <CAOFE=kP91e+jo1nu-1PCNc_P+sPcpxyC8+dzoNwO-caqj2=kaQ@mail.gmail.com>
Message-ID: <a66549fd-fe91-3c32-1c70-91a5547e7634@dewey.myzen.co.uk>

To get multiple plots on the same physical page try
?layout
ot look through the various option under
?par

I do not know enough about DCC models to answer how you get correlations.

Michael

On 25/11/2019 10:34, Subhamitra Patra wrote:
> Dear R-users,
> 
> I am estimating the DCC-model by using the following code and successfully
> making the individual plots indicating the dynamic correlation between the
> two variables stored in the R data frame (namely dcc.fit in my code). My
> code is
> 
> Dat = Date2[,2:13, drop = FALSE]
> library(rmgarch)
> library(rugarch)
> xspec = ugarchspec(mean.model = list(armaOrder = c(1, 1)), variance.model =
> list(garchOrder = c(1,1), model = 'eGARCH'), distribution.model = 'norm')
> uspec = multispec(replicate(12, xspec)) # Define a function using xspec
> dcc.garch11spec = dccspec(uspec = uspec, dccOrder = c(1, 1), distribution =
> 'mvnorm')
> dcc.fit = dccfit(dcc.garch11spec, data = Dat)
> r1=rcor(dcc.fit, type="R")
> print(r1)
> 
> Here, I need to obtain the DCC relationship between the two variables. I
> successfully make a separate plot for the correlation between the two
> variables with the following code.
> plot(rcor(dcc.fit, type="R")['A1','A2',], type='l')
> plot(rcor(dcc.fit, type="R")['B1','B2',], type='l')
> plot(rcor(dcc.fit, type="R")['C1','C2',], type='l')
> plot(rcor(dcc.fit, type="R")['D1','D2',], type='l')
> plot(rcor(dcc.fit, type="R")['E1','E2',], type='l')
> plot(rcor(dcc.fit, type="R")['F1','F2',], type='l')
> 
> Here, I request your help on two things which I am unable to do.
> 
> 1. I need to arrange all the above individual plots (counted as 6) on one
> page.
> 2. I need to obtain the correlation coefficients between the particular two
> variables (i.e. A1 versus A2, B1 versus B2, .......so on) throughout the
> entire period.
> 
> For your convenience, I have attached my sample data.
> 
> I definitely expect a positive help from the experts as the suggestions
> from the expert through this R forum has always solved my previous problem.
> 
> Thank you.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com  Mon Nov 25 14:46:18 2019
From: @ubh@m|tr@@p@tr@ @end|ng |rom gm@||@com (Subhamitra Patra)
Date: Mon, 25 Nov 2019 19:16:18 +0530
Subject: [R] [r] How to obtain the correlation coefficients between two
 variables in the R object
In-Reply-To: <a66549fd-fe91-3c32-1c70-91a5547e7634@dewey.myzen.co.uk>
References: <CAOFE=kP91e+jo1nu-1PCNc_P+sPcpxyC8+dzoNwO-caqj2=kaQ@mail.gmail.com>
 <a66549fd-fe91-3c32-1c70-91a5547e7634@dewey.myzen.co.uk>
Message-ID: <CAOFE=kPjGaWRUdi=ZH9Szfg7dP+x7vmoJz-mHixOE+1ccRqYgA@mail.gmail.com>

Thank you for your suggestion. Actually, the correlation is coming in the
DCC model through my above code which I am able to plot it. Is there any
solution that I can extract data from a particular plot. Actually, I am
able to obtain the correlation matrix, but I need the correlation
coefficient series indicating the correlation coefficients between the two
variables.



[image: Mailtrack]
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
Sender
notified by
Mailtrack
<https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_campaign=signaturevirality5&>
11/25/19,
07:12:49 PM

On Mon, Nov 25, 2019 at 7:10 PM Michael Dewey <lists at dewey.myzen.co.uk>
wrote:

> To get multiple plots on the same physical page try
> ?layout
> ot look through the various option under
> ?par
>
> I do not know enough about DCC models to answer how you get correlations.
>
> Michael
>
> On 25/11/2019 10:34, Subhamitra Patra wrote:
> > Dear R-users,
> >
> > I am estimating the DCC-model by using the following code and
> successfully
> > making the individual plots indicating the dynamic correlation between
> the
> > two variables stored in the R data frame (namely dcc.fit in my code). My
> > code is
> >
> > Dat = Date2[,2:13, drop = FALSE]
> > library(rmgarch)
> > library(rugarch)
> > xspec = ugarchspec(mean.model = list(armaOrder = c(1, 1)),
> variance.model =
> > list(garchOrder = c(1,1), model = 'eGARCH'), distribution.model = 'norm')
> > uspec = multispec(replicate(12, xspec)) # Define a function using xspec
> > dcc.garch11spec = dccspec(uspec = uspec, dccOrder = c(1, 1),
> distribution =
> > 'mvnorm')
> > dcc.fit = dccfit(dcc.garch11spec, data = Dat)
> > r1=rcor(dcc.fit, type="R")
> > print(r1)
> >
> > Here, I need to obtain the DCC relationship between the two variables. I
> > successfully make a separate plot for the correlation between the two
> > variables with the following code.
> > plot(rcor(dcc.fit, type="R")['A1','A2',], type='l')
> > plot(rcor(dcc.fit, type="R")['B1','B2',], type='l')
> > plot(rcor(dcc.fit, type="R")['C1','C2',], type='l')
> > plot(rcor(dcc.fit, type="R")['D1','D2',], type='l')
> > plot(rcor(dcc.fit, type="R")['E1','E2',], type='l')
> > plot(rcor(dcc.fit, type="R")['F1','F2',], type='l')
> >
> > Here, I request your help on two things which I am unable to do.
> >
> > 1. I need to arrange all the above individual plots (counted as 6) on one
> > page.
> > 2. I need to obtain the correlation coefficients between the particular
> two
> > variables (i.e. A1 versus A2, B1 versus B2, .......so on) throughout the
> > entire period.
> >
> > For your convenience, I have attached my sample data.
> >
> > I definitely expect a positive help from the experts as the suggestions
> > from the expert through this R forum has always solved my previous
> problem.
> >
> > Thank you.
> >
>
> --
> Michael
> http://www.dewey.myzen.co.uk/home.html
>


-- 
*Best Regards,*
*Subhamitra Patra*
*Phd. Research Scholar*
*Department of Humanities and Social Sciences*
*Indian Institute of Technology, Kharagpur*
*INDIA*

	[[alternative HTML version deleted]]


From bickis m@iii@g oii m@th@us@sk@c@  Mon Nov 25 00:06:26 2019
From: bickis m@iii@g oii m@th@us@sk@c@ (bickis m@iii@g oii m@th@us@sk@c@)
Date: Sun, 24 Nov 2019 17:06:26 -0600
Subject: [R] Why don't the comments appear in the function?
Message-ID: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>

I have made a list in which each element is a function.   If I print
individual elements of the list, then the function code is shown along
with embedded comments.  However, if I print the list or sublist, then the
function code is shown without comments.   Why (and how) are the comments
hidden?

>flist[[3]]
function(y,brackets,rates){
	# Calculates before-tax income required to realized value y
	ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
	x<-(y+ints)/(1-rates)
	x[sum(x>brackets)]
}


>flist[3]
$btv
function (y, brackets, rates)
{
    ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
        1)])) - brackets * rates
    x <- (y + ints)/(1 - rates)
    x[sum(x > brackets)]
}

I am running R 3.3.2 on Mac OS X  10.10.5

Mik Bickis


From mehd|d@dkh@h91 @end|ng |rom gm@||@com  Mon Nov 25 16:14:04 2019
From: mehd|d@dkh@h91 @end|ng |rom gm@||@com (Mehdi Dadkhah)
Date: Mon, 25 Nov 2019 18:44:04 +0330
Subject: [R] NRC lexicon usage sentiment analysis
In-Reply-To: <CAGN=ytO0ZXaEYwM3d7=0muOu4U2LKD8uihM4N3QvOE5ST1hoOg@mail.gmail.com>
References: <CAGN=ytPD3ohiKDvSui1AAE99sM14=K0Xt2c8yQkKtxBocKGxfA@mail.gmail.com>
 <8EDBF469-2D51-4801-9A68-0885F44313AC@gmail.com>
 <CAGN=ytOfpqUFg_C2gMgxC7osiEuPZicqQBmXyo8OYRs7fW-GCg@mail.gmail.com>
 <CAGN=ytMxqoavgMtNCt4jTn=PZk_zrhPFFMN595_VdY0tE19YbA@mail.gmail.com>
 <CAGN=ytMDcqtW00ah7vSejGYN=Rk_j3zMtys7xcHQ9pVazyuvHw@mail.gmail.com>
 <CAGN=ytNoFNFfMyb+gw1KXkXYbo3v2hvm9bvffpzvLratpG1F4A@mail.gmail.com>
 <CAGN=ytMYYnMzUKCNBgrPTd53Y1ikrpiB-6FQ2FGZjE9qXpyL2Q@mail.gmail.com>
 <CAGN=ytO0ZXaEYwM3d7=0muOu4U2LKD8uihM4N3QvOE5ST1hoOg@mail.gmail.com>
Message-ID: <CAGN=ytPTVRtYnjFSnqjJCGn5bE+qj3XYM90qsqvtovkudUKmUA@mail.gmail.com>

Dear Colleagues,
I hope you are doing well!
I did sentiment analysis on two datasets. As these datasets do not have
same size, to compare emotions in two datasets, i calculated percentages.
In my work, i calculated total occurrence for each emotion for two datasets
separately.  To calculate percentage of each emotion. i used below equation:

Percentage of an emotion=(total occurrence of an emotion in a dataset/Total
occurrence of all emotions in a dataset)*100

when i calculated percentages for each emotion, in each dataset to
calculate di?erence in percentages of an emotion, i used below equation:

di?erence in percentages of an emotion= percentages of an emotion in
dataset A  - percentages of an emotion in dataset B
Is my practice  true?
Could you please answer my question?
Thank you very much!
With best regards,


-- 
*Mehdi Dadkhah*

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Mon Nov 25 17:06:53 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Mon, 25 Nov 2019 16:06:53 +0000
Subject: [R] [r] How to obtain the correlation coefficients between two
 variables in the R object
In-Reply-To: <CAOFE=kPjGaWRUdi=ZH9Szfg7dP+x7vmoJz-mHixOE+1ccRqYgA@mail.gmail.com>
References: <CAOFE=kP91e+jo1nu-1PCNc_P+sPcpxyC8+dzoNwO-caqj2=kaQ@mail.gmail.com>
 <a66549fd-fe91-3c32-1c70-91a5547e7634@dewey.myzen.co.uk>
 <CAOFE=kPjGaWRUdi=ZH9Szfg7dP+x7vmoJz-mHixOE+1ccRqYgA@mail.gmail.com>
Message-ID: <7c6e02ddac314287875f5e703ba3cc6d@SRVEXCHCM1301.precheza.cz>

Hi

Without knowing much about functions you use, instead of plotting you could
save results of rcor(dcc.fit, type="R") to

something <- rcor(dcc.fit, type="R")

and use

str(something) to inspect the structure.

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Subhamitra
> Patra
> Sent: Monday, November 25, 2019 2:46 PM
> To: Michael Dewey <lists at dewey.myzen.co.uk>; r-help mailing list <r-
> help at r-project.org>
> Subject: Re: [R] [r] How to obtain the correlation coefficients between
two
> variables in the R object
> 
> Thank you for your suggestion. Actually, the correlation is coming in the
> DCC model through my above code which I am able to plot it. Is there any
> solution that I can extract data from a particular plot. Actually, I am
> able to obtain the correlation matrix, but I need the correlation
> coefficient series indicating the correlation coefficients between the two
> variables.
> 
> 
> 
> [image: Mailtrack]
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_ca
> mpaign=signaturevirality5&>
> Sender
> notified by
> Mailtrack
> <https://mailtrack.io?utm_source=gmail&utm_medium=signature&utm_ca
> mpaign=signaturevirality5&>
> 11/25/19,
> 07:12:49 PM
> 
> On Mon, Nov 25, 2019 at 7:10 PM Michael Dewey
> <lists at dewey.myzen.co.uk>
> wrote:
> 
> > To get multiple plots on the same physical page try
> > ?layout
> > ot look through the various option under
> > ?par
> >
> > I do not know enough about DCC models to answer how you get
> correlations.
> >
> > Michael
> >
> > On 25/11/2019 10:34, Subhamitra Patra wrote:
> > > Dear R-users,
> > >
> > > I am estimating the DCC-model by using the following code and
> > successfully
> > > making the individual plots indicating the dynamic correlation between
> > the
> > > two variables stored in the R data frame (namely dcc.fit in my code).
My
> > > code is
> > >
> > > Dat = Date2[,2:13, drop = FALSE]
> > > library(rmgarch)
> > > library(rugarch)
> > > xspec = ugarchspec(mean.model = list(armaOrder = c(1, 1)),
> > variance.model =
> > > list(garchOrder = c(1,1), model = 'eGARCH'), distribution.model =
'norm')
> > > uspec = multispec(replicate(12, xspec)) # Define a function using
xspec
> > > dcc.garch11spec = dccspec(uspec = uspec, dccOrder = c(1, 1),
> > distribution =
> > > 'mvnorm')
> > > dcc.fit = dccfit(dcc.garch11spec, data = Dat)
> > > r1=rcor(dcc.fit, type="R")
> > > print(r1)
> > >
> > > Here, I need to obtain the DCC relationship between the two variables.
I
> > > successfully make a separate plot for the correlation between the two
> > > variables with the following code.
> > > plot(rcor(dcc.fit, type="R")['A1','A2',], type='l')
> > > plot(rcor(dcc.fit, type="R")['B1','B2',], type='l')
> > > plot(rcor(dcc.fit, type="R")['C1','C2',], type='l')
> > > plot(rcor(dcc.fit, type="R")['D1','D2',], type='l')
> > > plot(rcor(dcc.fit, type="R")['E1','E2',], type='l')
> > > plot(rcor(dcc.fit, type="R")['F1','F2',], type='l')
> > >
> > > Here, I request your help on two things which I am unable to do.
> > >
> > > 1. I need to arrange all the above individual plots (counted as 6) on
one
> > > page.
> > > 2. I need to obtain the correlation coefficients between the
particular
> > two
> > > variables (i.e. A1 versus A2, B1 versus B2, .......so on) throughout
the
> > > entire period.
> > >
> > > For your convenience, I have attached my sample data.
> > >
> > > I definitely expect a positive help from the experts as the
suggestions
> > > from the expert through this R forum has always solved my previous
> > problem.
> > >
> > > Thank you.
> > >
> >
> > --
> > Michael
> > http://www.dewey.myzen.co.uk/home.html
> >
> 
> 
> --
> *Best Regards,*
> *Subhamitra Patra*
> *Phd. Research Scholar*
> *Department of Humanities and Social Sciences*
> *Indian Institute of Technology, Kharagpur*
> *INDIA*
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From bgunter@4567 @end|ng |rom gm@||@com  Mon Nov 25 17:34:31 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 25 Nov 2019 08:34:31 -0800
Subject: [R] Why don't the comments appear in the function?
In-Reply-To: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
References: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
Message-ID: <CAGxFJbTP6p1g5KKgsSRwXz5-ZO7b2Fr4jgsPhkA4pktdC1mS1Q@mail.gmail.com>

I do not see this behavior.

f <- function(x){
   ## a comment
   2
}

g <- list(a =2, fun =f)

> g
$a
[1] 2

$fun
function(x){
   ## a comment
   2
}

> g[[2]]
function(x){
   ## a comment
   2
}
> g[2]
$fun
function(x){
   ## a comment
   2
}

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Nov 25, 2019 at 7:03 AM <bickis at math.usask.ca> wrote:

> I have made a list in which each element is a function.   If I print
> individual elements of the list, then the function code is shown along
> with embedded comments.  However, if I print the list or sublist, then the
> function code is shown without comments.   Why (and how) are the comments
> hidden?
>
> >flist[[3]]
> function(y,brackets,rates){
>         # Calculates before-tax income required to realized value y
>
> ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
>         x<-(y+ints)/(1-rates)
>         x[sum(x>brackets)]
> }
>
>
> >flist[3]
> $btv
> function (y, brackets, rates)
> {
>     ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
>         1)])) - brackets * rates
>     x <- (y + ints)/(1 - rates)
>     x[sum(x > brackets)]
> }
>
> I am running R 3.3.2 on Mac OS X  10.10.5
>
> Mik Bickis
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Mon Nov 25 19:17:53 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Mon, 25 Nov 2019 10:17:53 -0800
Subject: [R] Why don't the comments appear in the function?
In-Reply-To: <1ff362252481cb790dba0d1e7d2a6fab.squirrel@math.usask.ca>
References: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
 <CAGxFJbTP6p1g5KKgsSRwXz5-ZO7b2Fr4jgsPhkA4pktdC1mS1Q@mail.gmail.com>
 <1ff362252481cb790dba0d1e7d2a6fab.squirrel@math.usask.ca>
Message-ID: <CAGxFJbTxThNwEJ9CH6A9JM-jN1AfN6L41vWKW-HO0=qAb=_27A@mail.gmail.com>

Dunno.

> R.Version()
$platform
[1] "x86_64-apple-darwin15.6.0"

$arch
[1] "x86_64"

$os
[1] "darwin15.6.0"

$system
[1] "x86_64, darwin15.6.0"

$status
[1] ""

$major
[1] "3"

$minor
[1] "6.1"

$year
[1] "2019"

$month
[1] "07"

$day
[1] "05"

$`svn rev`
[1] "76782"

$language
[1] "R"

$version.string
[1] "R version 3.6.1 (2019-07-05)"

$nickname
[1] "Action of the Toes"

You should upgrade to the latest version (which I have not done yet) if you
haven't already done so.

Additional notes:
I saw the same results at the command line from R -- vanilla that I got
from the RStudio console(what I showed you). Have you done this? This
should indicate whether it might have something to do with your
environment, etc..

However, executing the file with Rscript stripped the comments from both
g[[2]] and g[2].

Cheers,
Bert



On Mon, Nov 25, 2019 at 9:40 AM <bickis at math.usask.ca> wrote:

> Different version of R perhaps?   Or is there something in my environment
> or "preferences" that is different?
>
> Here's what I get with your example.
>
> >  f <- function(x){
> +     ## a comment
> +     2
> +  }
> > g <- list(a =2, fun =f)
> > g
> $a
> [1] 2
>
> $fun
> function (x)
> {
>     2
> }
>
> > g[[2]]
> function(x){
>     ## a comment
>     2
>  }
> > g[1]
> $a
> [1] 2
>
>
>
>
> Mik
>
>
> > I do not see this behavior.
> >
> > f <- function(x){
> >    ## a comment
> >    2
> > }
> >
> > g <- list(a =2, fun =f)
> >
> >> g
> > $a
> > [1] 2
> >
> > $fun
> > function(x){
> >    ## a comment
> >    2
> > }
> >
> >> g[[2]]
> > function(x){
> >    ## a comment
> >    2
> > }
> >> g[2]
> > $fun
> > function(x){
> >    ## a comment
> >    2
> > }
> >
> > Cheers,
> > Bert
> >
> > Bert Gunter
> >
> > "The trouble with having an open mind is that people keep coming along
> and
> > sticking things into it."
> > -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> >
> >
> > On Mon, Nov 25, 2019 at 7:03 AM
> > <bickis at math.usask.ca<mailto:bickis at math.usask.ca>> wrote:
> > I have made a list in which each element is a function.   If I print
> > individual elements of the list, then the function code is shown along
> > with embedded comments.  However, if I print the list or sublist, then
> the
> > function code is shown without comments.   Why (and how) are the comments
> > hidden?
> >
> >>flist[[3]]
> > function(y,brackets,rates){
> >         # Calculates before-tax income required to realized value y
> >
>  ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
> >         x<-(y+ints)/(1-rates)
> >         x[sum(x>brackets)]
> > }
> >
> >
> >>flist[3]
> > $btv
> > function (y, brackets, rates)
> > {
> >     ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
> >         1)])) - brackets * rates
> >     x <- (y + ints)/(1 - rates)
> >     x[sum(x > brackets)]
> > }
> >
> > I am running R 3.3.2 on Mac OS X  10.10.5
> >
> > Mik Bickis
> >
> > ______________________________________________
> > R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> > UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>

	[[alternative HTML version deleted]]


From 538280 @end|ng |rom gm@||@com  Mon Nov 25 19:27:40 2019
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Mon, 25 Nov 2019 11:27:40 -0700
Subject: [R] giving priority to stats package
In-Reply-To: <CAP01uRnzNN0Bcs-d8XHyBf7y4FgrAVgXWDt+zpiq=NaijZLRKA@mail.gmail.com>
References: <CAP01uRnzNN0Bcs-d8XHyBf7y4FgrAVgXWDt+zpiq=NaijZLRKA@mail.gmail.com>
Message-ID: <CAFEqCdzjgizM+4YCGo6GRLLaFebmA5-T5U9wCrCbiKOZs=y6rQ@mail.gmail.com>

You could use the `pos` arg to place the newly loaded package(s) on
the search path after the stats package.  That would give priority for
any functions in the stats package over the newly loaded package (but
also give priority for any other packages earlier on the search path).

On Sat, Nov 23, 2019 at 6:25 AM Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
>
> library and require have new args in 3.6 giving additional control
> over conflicts.  This seems very useful but I was wondering if there
> were some, preferabley simple, way to give existing loaded packages
> priority without knowing the actual conflicts in advance.  For example
>
>     library(dplyr, exclude = c("filter", "lag"))
>
> works to avoid masking those names in stats that would otherwise be
> masked by that package but I had to know in advance that filter and
> lag were the conflicting names.
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Nov 25 19:42:06 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 25 Nov 2019 13:42:06 -0500
Subject: [R] Why don't the comments appear in the function?
In-Reply-To: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
References: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
Message-ID: <b7b43425-72e8-8131-2f3f-5ef68946ad86@gmail.com>

On 24/11/2019 6:06 p.m., bickis at math.usask.ca wrote:
> I have made a list in which each element is a function.   If I print
> individual elements of the list, then the function code is shown along
> with embedded comments.  However, if I print the list or sublist, then the
> function code is shown without comments.   Why (and how) are the comments
> hidden?

Hi Mik.  I see the same behaviour as Bert, not what you see.

Generally the comments will be shown if the "srcref" attribute is 
attached to the function, and it is a valid one.  That shouldn't change 
between looking at flist[[3]] and flist[3].  If it's not there or not 
valid, you'll see a deparsed version of the function; that's what your 
flist[3]$btv looks like.

It's possible this is R 3.3.2-specific; that's a relatively old version 
now, but I don't know of any change that would cause this.  Can you show 
us the code that you used to create flist, or enough of it to show this 
behaviour?  Showing us (or me privately) dput(flist) might be enough to 
see what's going on.

Duncan Murdoch

> 
>> flist[[3]]
> function(y,brackets,rates){
> 	# Calculates before-tax income required to realized value y
> 	ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
> 	x<-(y+ints)/(1-rates)
> 	x[sum(x>brackets)]
> }
> 
> 
>> flist[3]
> $btv
> function (y, brackets, rates)
> {
>      ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
>          1)])) - brackets * rates
>      x <- (y + ints)/(1 - rates)
>      x[sum(x > brackets)]
> }
> 
> I am running R 3.3.2 on Mac OS X  10.10.5
> 
> Mik Bickis
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From n|u|tz @end|ng |rom gm@||@com  Mon Nov 25 20:27:26 2019
From: n|u|tz @end|ng |rom gm@||@com (Neal Fultz)
Date: Mon, 25 Nov 2019 11:27:26 -0800
Subject: [R] Why don't the comments appear in the function?
In-Reply-To: <b7b43425-72e8-8131-2f3f-5ef68946ad86@gmail.com>
References: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
 <b7b43425-72e8-8131-2f3f-5ef68946ad86@gmail.com>
Message-ID: <CAL9B2vcpNyvafZJu9OMJk6GfS=n=4BN5BG_hfbe6R1yTKcmyjQ@mail.gmail.com>

Hi Mik,

Echoing Bert and Duncan's suggestions, please see also ?srcref and
?getSrcref
and consider upgrading to a currently supported version of R.

I'd also call out the "keep.source" installation option as important if
your functions
are inside a package - there is some discussion around this at
https://github.com/DeclareDesign/DesignLibrary/issues/50 and linked issues
from
the last time I encountered this weird corner of R.

If anyone finds a better solution (or for the related problem of preserving
whitespace),
I would be interested as well.

Very respectfully,

Neal Fultz

On Mon, Nov 25, 2019 at 10:49 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 24/11/2019 6:06 p.m., bickis at math.usask.ca wrote:
> > I have made a list in which each element is a function.   If I print
> > individual elements of the list, then the function code is shown along
> > with embedded comments.  However, if I print the list or sublist, then
> the
> > function code is shown without comments.   Why (and how) are the comments
> > hidden?
>
> Hi Mik.  I see the same behaviour as Bert, not what you see.
>
> Generally the comments will be shown if the "srcref" attribute is
> attached to the function, and it is a valid one.  That shouldn't change
> between looking at flist[[3]] and flist[3].  If it's not there or not
> valid, you'll see a deparsed version of the function; that's what your
> flist[3]$btv looks like.
>
> It's possible this is R 3.3.2-specific; that's a relatively old version
> now, but I don't know of any change that would cause this.  Can you show
> us the code that you used to create flist, or enough of it to show this
> behaviour?  Showing us (or me privately) dput(flist) might be enough to
> see what's going on.
>
> Duncan Murdoch
>
> >
> >> flist[[3]]
> > function(y,brackets,rates){
> >       # Calculates before-tax income required to realized value y
> >
>  ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
> >       x<-(y+ints)/(1-rates)
> >       x[sum(x>brackets)]
> > }
> >
> >
> >> flist[3]
> > $btv
> > function (y, brackets, rates)
> > {
> >      ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
> >          1)])) - brackets * rates
> >      x <- (y + ints)/(1 - rates)
> >      x[sum(x > brackets)]
> > }
> >
> > I am running R 3.3.2 on Mac OS X  10.10.5
> >
> > Mik Bickis
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bickis m@iii@g oii m@th@us@sk@c@  Mon Nov 25 18:40:16 2019
From: bickis m@iii@g oii m@th@us@sk@c@ (bickis m@iii@g oii m@th@us@sk@c@)
Date: Mon, 25 Nov 2019 11:40:16 -0600
Subject: [R] Why don't the comments appear in the function?
In-Reply-To: <CAGxFJbTP6p1g5KKgsSRwXz5-ZO7b2Fr4jgsPhkA4pktdC1mS1Q@mail.gmail.com>
References: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
 <CAGxFJbTP6p1g5KKgsSRwXz5-ZO7b2Fr4jgsPhkA4pktdC1mS1Q@mail.gmail.com>
Message-ID: <1ff362252481cb790dba0d1e7d2a6fab.squirrel@math.usask.ca>

Different version of R perhaps?   Or is there something in my environment
or "preferences" that is different?

Here's what I get with your example.

>  f <- function(x){
+     ## a comment
+     2
+  }
> g <- list(a =2, fun =f)
> g
$a
[1] 2

$fun
function (x)
{
    2
}

> g[[2]]
function(x){
    ## a comment
    2
 }
> g[1]
$a
[1] 2




Mik


> I do not see this behavior.
>
> f <- function(x){
>    ## a comment
>    2
> }
>
> g <- list(a =2, fun =f)
>
>> g
> $a
> [1] 2
>
> $fun
> function(x){
>    ## a comment
>    2
> }
>
>> g[[2]]
> function(x){
>    ## a comment
>    2
> }
>> g[2]
> $fun
> function(x){
>    ## a comment
>    2
> }
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Mon, Nov 25, 2019 at 7:03 AM
> <bickis at math.usask.ca<mailto:bickis at math.usask.ca>> wrote:
> I have made a list in which each element is a function.   If I print
> individual elements of the list, then the function code is shown along
> with embedded comments.  However, if I print the list or sublist, then the
> function code is shown without comments.   Why (and how) are the comments
> hidden?
>
>>flist[[3]]
> function(y,brackets,rates){
>         # Calculates before-tax income required to realized value y
>         ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
>         x<-(y+ints)/(1-rates)
>         x[sum(x>brackets)]
> }
>
>
>>flist[3]
> $btv
> function (y, brackets, rates)
> {
>     ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
>         1)])) - brackets * rates
>     x <- (y + ints)/(1 - rates)
>     x[sum(x > brackets)]
> }
>
> I am running R 3.3.2 on Mac OS X  10.10.5
>
> Mik Bickis
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
> UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Nov 25 21:23:23 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 25 Nov 2019 15:23:23 -0500
Subject: [R] Why don't the comments appear in the function?
In-Reply-To: <CAL9B2vcpNyvafZJu9OMJk6GfS=n=4BN5BG_hfbe6R1yTKcmyjQ@mail.gmail.com>
References: <266a4b8fc24a35cd2cb0f53726f1ea8e.squirrel@math.usask.ca>
 <b7b43425-72e8-8131-2f3f-5ef68946ad86@gmail.com>
 <CAL9B2vcpNyvafZJu9OMJk6GfS=n=4BN5BG_hfbe6R1yTKcmyjQ@mail.gmail.com>
Message-ID: <5cebe8c9-418d-fde7-6e2b-9d53bb689463@gmail.com>

On 25/11/2019 2:27 p.m., Neal Fultz wrote:
> Hi Mik,
> 
> Echoing Bert and Duncan's suggestions, please see also ?srcref and
> ?getSrcref
> and consider upgrading to a currently supported version of R.
> 
> I'd also call out the "keep.source" installation option as important if
> your functions
> are inside a package - there is some discussion around this at
> https://github.com/DeclareDesign/DesignLibrary/issues/50 and linked issues
> from
> the last time I encountered this weird corner of R.
> 
> If anyone finds a better solution (or for the related problem of preserving
> whitespace),
> I would be interested as well.

Preserving comments and whitespace are definitely the same issue.  I'm 
not sure what solution you're trying to improve, but if you want source 
kept in packages, run

Sys.setenv(R_KEEP_PKG_SOURCE="yes")

before doing the install using install.packages(), or use the command 
line option with R CMD INSTALL.

If you want the source kept for your own function regardless of the 
user's wishes, I think you'll have to do some weird things in your 
source files.  For example, if pkg::fn is defined in the file 
pkg/R/fn.R, you could put this into your zzz.R file (any name is fine, 
as long as it collates after fn.R):

  sys.source("R/fn.R", keep.source = TRUE, envir = getNamespace("pkg"))

This will replace fn with a new copy that has source references.  It 
assumes that the namespace has been created at this point in 
installation; I think that's true, but there may be cases where it's not 
true, in which case you'll need to use something like environment(fn) as 
the envir argument.

Duncan Murdoch

> 
> Very respectfully,
> 
> Neal Fultz
> 
> On Mon, Nov 25, 2019 at 10:49 AM Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
> 
>> On 24/11/2019 6:06 p.m., bickis at math.usask.ca wrote:
>>> I have made a list in which each element is a function.   If I print
>>> individual elements of the list, then the function code is shown along
>>> with embedded comments.  However, if I print the list or sublist, then
>> the
>>> function code is shown without comments.   Why (and how) are the comments
>>> hidden?
>>
>> Hi Mik.  I see the same behaviour as Bert, not what you see.
>>
>> Generally the comments will be shown if the "srcref" attribute is
>> attached to the function, and it is a valid one.  That shouldn't change
>> between looking at flist[[3]] and flist[3].  If it's not there or not
>> valid, you'll see a deparsed version of the function; that's what your
>> flist[3]$btv looks like.
>>
>> It's possible this is R 3.3.2-specific; that's a relatively old version
>> now, but I don't know of any change that would cause this.  Can you show
>> us the code that you used to create flist, or enough of it to show this
>> behaviour?  Showing us (or me privately) dput(flist) might be enough to
>> see what's going on.
>>
>> Duncan Murdoch
>>
>>>
>>>> flist[[3]]
>>> function(y,brackets,rates){
>>>        # Calculates before-tax income required to realized value y
>>>
>>   ints<-c(0,cumsum(diff(brackets)*rates[1:(length(rates)-1)]))-brackets*rates
>>>        x<-(y+ints)/(1-rates)
>>>        x[sum(x>brackets)]
>>> }
>>>
>>>
>>>> flist[3]
>>> $btv
>>> function (y, brackets, rates)
>>> {
>>>       ints <- c(0, cumsum(diff(brackets) * rates[1:(length(rates) -
>>>           1)])) - brackets * rates
>>>       x <- (y + ints)/(1 - rates)
>>>       x[sum(x > brackets)]
>>> }
>>>
>>> I am running R 3.3.2 on Mac OS X  10.10.5
>>>
>>> Mik Bickis
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From M@Thev@r@j@ @end|ng |rom m@@@ey@@c@nz  Mon Nov 25 22:57:36 2019
From: M@Thev@r@j@ @end|ng |rom m@@@ey@@c@nz (Thevaraja, Mayooran)
Date: Mon, 25 Nov 2019 21:57:36 +0000
Subject: [R] ggplot inside of the function
In-Reply-To: <CAGgJW76DBkW6Y5RmpXULCktFTBu0OfMDQiXgbEUsREvPHmfbQg@mail.gmail.com>
References: <SYXPR01MB130992459DCE9AF8E24351EAC94A0@SYXPR01MB1309.ausprd01.prod.outlook.com>
 <CAGgJW76DBkW6Y5RmpXULCktFTBu0OfMDQiXgbEUsREvPHmfbQg@mail.gmail.com>
Message-ID: <SYXPR01MB130900E6D55896F93370CCD6C94A0@SYXPR01MB1309.ausprd01.prod.outlook.com>

Great it works. Thank you so much Eric.

Cheers,

Mayooran

-----Original Message-----
From: Eric Berger <ericjberger at gmail.com> 
Sent: Monday, 25 November 2019 11:22 PM
To: Thevaraja, Mayooran <M.Thevaraja at massey.ac.nz>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] ggplot inside of the function

Hi Mayooran,
If you define the following function

f <- function(m,v) { sprintf("Norm(mu=%.1f, var=%.1f)",m,v) }

Then you can modify the setting of Prob as follows

 Prob <- plyr::rename(Prob_df,
c("p_1"=f(mu1,var1),"p_2"=f(mu2,var2),"p_3"=f(mu3,var3)))

The lesson here is that wherever you set a variable to a string, as in "p_1"="Norm(mu=mu1,var=var1)", you can create the string dynamically using the function sprintf(), which returns a string.

Best,
Eric

On Mon, Nov 25, 2019 at 3:54 AM Thevaraja, Mayooran <M.Thevaraja at massey.ac.nz> wrote:
>
> Hello Folks
>                I am working some R package development. When I was using ggplot inside of the function, I need to get the output graph's legend must be corresponding to the input parameters numerical value (need to be automatically changed when we input different parameters).  Therefore anyone has any ideas? Here, I have given below a simple example. I need to get my output graph's legend should be Norm(mu=0.2, var=0.8),...
>
>
> example <- function(mu1,mu2,mu3,var1,var2,var3){
>   x <- seq(-3,3, by=0.01)
>   p_1 <- dnorm(x,mu1,var1)
>   p_2 <- dnorm(x,mu2,var2)
>   p_3 <- dnorm(x,mu3,var3)
>   Prob_df <- data.frame(x,p_1,p_2,p_3)
>   Prob <- plyr::rename(Prob_df, c("p_1"="Norm(mu=mu1, var=var1)","p_2"="Norm(mu=mu2, var=var2)","p_3"="Norm(mu=mu3, var=var3)"))
>   melten.Prob <- reshape2::melt(Prob, id = "x", variable.name = 
> "Methods", value.name = "Prob") ggplot2::ggplot(melten.Prob)+
>     ggplot2::geom_point(ggplot2::aes(x = x, y = Prob, group= Methods, colour = Methods ))+
>     ggplot2::geom_line(ggplot2::aes(x = x, y = Prob, group= Methods, 
> colour = Methods ))
>
> }
>
> mu1 <- 0.2
> mu2 <- 0.5
> mu3 <- 1
> var1 <- 0.8
> var2 <- 1.2
> var3 <- 2.1
> example(mu1,mu2,mu3,var1,var2,var3)
>
>
> Thanks
> Mayooran
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From ggrothend|eck @end|ng |rom gm@||@com  Mon Nov 25 23:06:21 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Mon, 25 Nov 2019 17:06:21 -0500
Subject: [R] giving priority to stats package
In-Reply-To: <CAFEqCdzjgizM+4YCGo6GRLLaFebmA5-T5U9wCrCbiKOZs=y6rQ@mail.gmail.com>
References: <CAP01uRnzNN0Bcs-d8XHyBf7y4FgrAVgXWDt+zpiq=NaijZLRKA@mail.gmail.com>
 <CAFEqCdzjgizM+4YCGo6GRLLaFebmA5-T5U9wCrCbiKOZs=y6rQ@mail.gmail.com>
Message-ID: <CAP01uRmifWci-M2TbouLrbi6hbD3u7XTJFXuRxiZbuLsJJKgTA@mail.gmail.com>

Goold idea.  This seems to work.

  library(dplyr, pos = grep("package:stats", search()) + 1)

On Mon, Nov 25, 2019 at 1:27 PM Greg Snow <538280 at gmail.com> wrote:
>
> You could use the `pos` arg to place the newly loaded package(s) on
> the search path after the stats package.  That would give priority for
> any functions in the stats package over the newly loaded package (but
> also give priority for any other packages earlier on the search path).
>
> On Sat, Nov 23, 2019 at 6:25 AM Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
> >
> > library and require have new args in 3.6 giving additional control
> > over conflicts.  This seems very useful but I was wondering if there
> > were some, preferabley simple, way to give existing loaded packages
> > priority without knowing the actual conflicts in advance.  For example
> >
> >     library(dplyr, exclude = c("filter", "lag"))
> >
> > works to avoid masking those names in stats that would otherwise be
> > masked by that package but I had to know in advance that filter and
> > lag were the conflicting names.
> >
> > --
> > Statistics & Software Consulting
> > GKX Group, GKX Associates Inc.
> > tel: 1-877-GKX-GROUP
> > email: ggrothendieck at gmail.com
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From m@ndecent@gupt@ @end|ng |rom gm@||@com  Tue Nov 26 16:06:48 2019
From: m@ndecent@gupt@ @end|ng |rom gm@||@com (Manish Gupta)
Date: Tue, 26 Nov 2019 10:06:48 -0500
Subject: [R] Error while installing xml2 package in R
Message-ID: <CAKn-XisCdCUsBsdUM_Y98bEe5X8nHe1rWmdpq7bnERdSJ+sXsg@mail.gmail.com>

I am getting following error while installing xml2 package in R.

install.packages("xml2")

Error :  unable to load shared object
'/usr/local/lib/R/site-library/00LOCK-xml2/00new/xml2/libs/xml2.so':

  libiconv.so.2: cannot open shared object file: No such file or directory

it seems it require libconv library but installing libiconv-hook-dev does
nos solve the problem.
how such issue can be solved?
-- 
Manish Gupta (Bioinformatician)

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Tue Nov 26 16:14:17 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Tue, 26 Nov 2019 07:14:17 -0800
Subject: [R] Error while installing xml2 package in R
In-Reply-To: <CAKn-XisCdCUsBsdUM_Y98bEe5X8nHe1rWmdpq7bnERdSJ+sXsg@mail.gmail.com>
References: <CAKn-XisCdCUsBsdUM_Y98bEe5X8nHe1rWmdpq7bnERdSJ+sXsg@mail.gmail.com>
Message-ID: <1C0D51EE-47F3-48FC-8F3D-8411684667D4@dcn.davis.ca.us>

This may or may not fix your problem but I avoid installing R packages in system libraries... using user libraries without running R as root or with sudo makes life much simpler unless you are a professional sysadmin.

On November 26, 2019 7:06:48 AM PST, Manish Gupta <mandecent.gupta at gmail.com> wrote:
>I am getting following error while installing xml2 package in R.
>
>install.packages("xml2")
>
>Error :  unable to load shared object
>'/usr/local/lib/R/site-library/00LOCK-xml2/00new/xml2/libs/xml2.so':
>
>libiconv.so.2: cannot open shared object file: No such file or
>directory
>
>it seems it require libconv library but installing libiconv-hook-dev
>does
>nos solve the problem.
>how such issue can be solved?

-- 
Sent from my phone. Please excuse my brevity.


From neh@@bo|ogn@90 @end|ng |rom gm@||@com  Tue Nov 26 16:51:53 2019
From: neh@@bo|ogn@90 @end|ng |rom gm@||@com (Neha gupta)
Date: Tue, 26 Nov 2019 16:51:53 +0100
Subject: [R] Hyperparameter tuning
Message-ID: <CA+nrPnvnmyyMUthOgtMBoVFvM_wDyuCj0WMuV-+JcU49pb0pWw@mail.gmail.com>

I am using xgboost hyperparameter tuning for the value of rmse. I used the
following method which returns two values ; mean which is like - 1200.12
and Best value like - 960. I guess the first value is average value and the
second is the best rmse value returned by the algorithm.

Now if we have to use Accuracy as estimation which, we know is the higher
it is, the better it will be. So do we have to use the same function but
just remove the negative sign?

For rmse ; return (- xgboost$results$rmse)

For AUC; return (xgboost$results$auc)

	[[alternative HTML version deleted]]


From 538280 @end|ng |rom gm@||@com  Tue Nov 26 22:26:34 2019
From: 538280 @end|ng |rom gm@||@com (Greg Snow)
Date: Tue, 26 Nov 2019 14:26:34 -0700
Subject: [R] giving priority to stats package
In-Reply-To: <CAP01uRmifWci-M2TbouLrbi6hbD3u7XTJFXuRxiZbuLsJJKgTA@mail.gmail.com>
References: <CAP01uRnzNN0Bcs-d8XHyBf7y4FgrAVgXWDt+zpiq=NaijZLRKA@mail.gmail.com>
 <CAFEqCdzjgizM+4YCGo6GRLLaFebmA5-T5U9wCrCbiKOZs=y6rQ@mail.gmail.com>
 <CAP01uRmifWci-M2TbouLrbi6hbD3u7XTJFXuRxiZbuLsJJKgTA@mail.gmail.com>
Message-ID: <CAFEqCdxPqrJQ_C1ubjXR8SSZqSjYy6aLH_gU-T=JN3yV8=8Jbw@mail.gmail.com>

I was thinking of using length(search())+1 to be safe and simple.
Using grep gives higher priority than length while still solving your
issue.

On Mon, Nov 25, 2019 at 3:06 PM Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
>
> Goold idea.  This seems to work.
>
>   library(dplyr, pos = grep("package:stats", search()) + 1)
>
> On Mon, Nov 25, 2019 at 1:27 PM Greg Snow <538280 at gmail.com> wrote:
> >
> > You could use the `pos` arg to place the newly loaded package(s) on
> > the search path after the stats package.  That would give priority for
> > any functions in the stats package over the newly loaded package (but
> > also give priority for any other packages earlier on the search path).
> >
> > On Sat, Nov 23, 2019 at 6:25 AM Gabor Grothendieck
> > <ggrothendieck at gmail.com> wrote:
> > >
> > > library and require have new args in 3.6 giving additional control
> > > over conflicts.  This seems very useful but I was wondering if there
> > > were some, preferabley simple, way to give existing loaded packages
> > > priority without knowing the actual conflicts in advance.  For example
> > >
> > >     library(dplyr, exclude = c("filter", "lag"))
> > >
> > > works to avoid masking those names in stats that would otherwise be
> > > masked by that package but I had to know in advance that filter and
> > > lag were the conflicting names.
> > >
> > > --
> > > Statistics & Software Consulting
> > > GKX Group, GKX Associates Inc.
> > > tel: 1-877-GKX-GROUP
> > > email: ggrothendieck at gmail.com
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> > --
> > Gregory (Greg) L. Snow Ph.D.
> > 538280 at gmail.com
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From v@|kremk @end|ng |rom gm@||@com  Wed Nov 27 00:15:19 2019
From: v@|kremk @end|ng |rom gm@||@com (Val)
Date: Tue, 26 Nov 2019 17:15:19 -0600
Subject: [R] Conditions
Message-ID: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>

HI All, I am having a little issue in my ifelse statement,
The data frame looks like as follow.

dat2 <-read.table(text="ID  d1 d2 d3
A 0 25 35
B 12 22  0
C 0  0  31
E 10 20 30
F 0  0   0",header=TRUE,stringsAsFactors=F)
I want to create d4 and set the value based on the following conditions.
If d1  !=0  then d4=d1
if d1 = 0  and d2 !=0  then d4=d2
if (d1  and d2 = 0) and d3  !=0 then d4=d3
if all d1, d2 and d3 =0 then d4=0

Here is the desired output and my attempt
 ID d1 d2 d3 d4
  A  0 25 35  25
  B 12 22  0  12
  C  0  0 31   31
  E 10 20 30  10
  F  0  0  0  0  0

My attempt
dat2$d4 <-  0
dat2$d4  <- ifelse((dat2$d1 =="0"), dat2$d2, ifelse(dat2$d2 == "0"), dat2$d3, 0)
but not working.

Thank you.


From g||ted|||e2014 @end|ng |rom gm@||@com  Wed Nov 27 03:53:19 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Wed, 27 Nov 2019 03:53:19 +0100
Subject: [R] Stacking two graphs with different x and y scale on the same
 graph
Message-ID: <CAC8ss30T_FyB6Q0cB4_cphuvuTS6pwM85M7Xdup85cg_u8M1AQ@mail.gmail.com>

Dear Contributors,
I have two data. A is of the form:
05 01 01  -0.00376058013285748
05 01 02  -0.0765481943910918
05 01 03  -1.28158758599964
05 01 04  -1.51612545416506
05 01 05  -1.39481276373467
05 01 06  -1.17644992095997
05 01 07  -0.788249311582716
05 01 08  -0.925737027403825
05 01 09  -1.02278717974814
05 01 10  -0.982349616271341
05 01 11  -0.61032403228481
05 01 12  -0.197860884821482
05 01 13  -0.173598346735404
05 01 14  -0.270648499079717
05 01 15  -0.173598346735404
05 01 16  -0.343436113337951
05 01 17  -0.949999565489903
05 01 18  -3.60270372956778
05 01 19  -5.47091916219579
05 01 20  -4.67834291805057
05 01 21  -5.42239408602363
05 01 22  -4.19309215632901
05 01 23  -1.79918839850264
05 01 24  -1.04704971783422
05 01 25  -0.642674083066247
05 01 26  -0.505186367245138
05 01 27  -0.472836316463701
05 01 28  -0.537536418026576
05 01 29  -0.311086062556513
05 01 30  -0.00376058013285748
05 01 31  -0.254473473688998
05 02 01  -0.197860884821482
05 02 02  -0.23021093560292
05 02 03  -0.238298448298279
05 02 04  -0.157423321344685
05 02 05  -0.060373169000373
05 02 06  0.109464597602174
05 02 07  0.02858947064858
05 02 08  -0.0846357070864511
05 02 09  -0.189773372126123
05 02 10  -0.278736011775076
05 02 11  -0.302998549861154
05 02 12  -0.0684606816957324
05 02 13  0.0852020595160955
05 02 14  0.133727135688252
05 02 15  0.0528520087346581
05 02 16  0.0771145468207362
05 02 17  -0.0361106309142949
05 02 18  -0.205948397516842
05 02 19  -0.383873676814748
05 02 20  -0.383873676814748
05 02 21  -0.294911037165795
05 02 22  -0.197860884821482
05 02 23  -0.214035910212201
05 02 24  -0.165510834040045
05 02 25  -0.0522856563050137
05 02 26  0.0366769833439393
05 02 27  0.141814648383611
05 02 28  0.101377084906814
05 03 01  0.15798967377433
05 03 02  0.222689775337205
05 03 03  0.27930236420472
05 03 04  0.327827440376876
05 03 05  0.214602262641845
05 03 06  0.133727135688252
05 03 07  0.166077186469689
 and B is of the form:
05 01 03  1.0401704890785
05 01 04  1.1881431442713
05 01 05  0.899433543239033
05 01 06  0.495029973508058
05 01 18  2.51141960034673
05 01 19  4.80818567931821
05 01 20  3.82649399122216
05 01 21  4.75619054623929
05 01 22  3.25702525028531
05 01 23  0.328654748869008
05 02 10  0.0689360507407491
05 02 11  0.192369729879942
05 02 15  0.0684297713902015
05 02 16  0.100584435166215
05 02 17  0.295302934161718
05 02 18  0.552388788420635
05 02 19  0.811732847306371
05 02 20  0.843313045760178
05 02 21  0.757193220375875
05 02 22  0.65352100387166
05 02 23  0.68252482652902
05 02 24  0.624510062816789
05 02 25  0.479854370620533
05 02 26  0.359002279153697
05 02 27  0.212459089641907
05 02 28  0.240784160160447
05 03 01  0.144583652487177
05 03 02  0.0345028244394553
05 03 21  0.023582430982633
05 03 22  0.000293765928922767
05 03 27  0.0440288222469235
05 03 28  0.106263428254761
05 03 29  0.291212461872628
05 03 30  0.198305017329253
05 03 31  0.186935599530143
05 04 01  0.316471519561273
05 04 02  0.266260602009615
05 04 03  0.0456391152384458
05 04 04  0.113939833419049
05 04 05  0.140500137811164
05 04 06  0.374670064516577
05 04 07  0.295820206701906
05 04 08  0.0833493810907385
05 04 10  0.0253248646840757
05 04 11  0.188773903020133
05 04 12  0.206619775067284
05 04 13  0.408503282817833
05 04 14  0.344129922512134
05 04 15  0.283273728250647
05 04 16  0.155780334719261
05 04 17  0.0815692243668445
B is extracted from A and I wish to plot A and B on the same x-axis
but different y axis on the same graph. Stacking very close helps to
illustrates the common rapid variations in A, represented in B.

One of the codes I tried without success is:
data <- read.table("A", col.names = c("year", "month", "day", "counts"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x1 = data$date
y1=data$counts
#y1=scale(data$counts)


data <- read.table("B", col.names = c("year", "month", "day", "counts"))

new.century <- data$year < 50

data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)

data$date <- as.Date(ISOdate(data$year, data$month, data$day))
x2 = data$date
y2=data$counts


pdf("PLOT.pdf")

 par(mar=c(5,5,5,5))

plot(x1,y1,pch=0,type="b",col="red",yaxt="n",ylim=c(-5.470919,1.298329),ylab="")
axis(side=2, at=c(-6,0,2))
mtext("red line", side = 2, line=2.5, at=0)

par(new=TRUE)
plot(x2,y2,pch=1,type="b",col="blue",yaxt="n",ylim=c(-4.808186,0.0), ylab="")
axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
mtext("blue line", side=4, line=2.5, at=100)
dev.off()

Your assistance is ever appreciated.
Best wishes
Ogbos


From bgunter@4567 @end|ng |rom gm@||@com  Wed Nov 27 04:56:28 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Tue, 26 Nov 2019 19:56:28 -0800
Subject: [R] Conditions
In-Reply-To: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>
References: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>
Message-ID: <CAGxFJbQMPsq9BBmGO+OD2LHyhnLtpGWmg1GQEB0DWe5w0a7eug@mail.gmail.com>

I generally find nested ifelse's to be confusing and prone to error, so I
usually prefer to proceed sequentially using subsetting with logicals or
replicated, but not nested ifelse's. In your example, the translation to
logical indexing seems pretty straightforward.

Using your example:

> dat2 <-within(dat2,
   {
      d4 <- d1 ## d1. 0 when d1 == 0
      d4[!d4]<- d2[!d4]
      d4[!d4]<- d3[!d4]
   })

> dat2
  ID d1 d2 d3 d4
1  A  0 25 35 25
2  B 12 22  0 12
3  C  0  0 31 31
4  E 10 20 30 10
5  F  0  0  0  0


Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Nov 26, 2019 at 3:15 PM Val <valkremk at gmail.com> wrote:

> HI All, I am having a little issue in my ifelse statement,
> The data frame looks like as follow.
>
> dat2 <-read.table(text="ID  d1 d2 d3
> A 0 25 35
> B 12 22  0
> C 0  0  31
> E 10 20 30
> F 0  0   0",header=TRUE,stringsAsFactors=F)
> I want to create d4 and set the value based on the following conditions.
> If d1  !=0  then d4=d1
> if d1 = 0  and d2 !=0  then d4=d2
> if (d1  and d2 = 0) and d3  !=0 then d4=d3
> if all d1, d2 and d3 =0 then d4=0
>
> Here is the desired output and my attempt
>  ID d1 d2 d3 d4
>   A  0 25 35  25
>   B 12 22  0  12
>   C  0  0 31   31
>   E 10 20 30  10
>   F  0  0  0  0  0
>
> My attempt
> dat2$d4 <-  0
> dat2$d4  <- ifelse((dat2$d1 =="0"), dat2$d2, ifelse(dat2$d2 == "0"),
> dat2$d3, 0)
> but not working.
>
> Thank you.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drj|m|emon @end|ng |rom gm@||@com  Wed Nov 27 05:05:35 2019
From: drj|m|emon @end|ng |rom gm@||@com (Jim Lemon)
Date: Wed, 27 Nov 2019 15:05:35 +1100
Subject: [R] Conditions
In-Reply-To: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>
References: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>
Message-ID: <CA+8X3fUaOWh6QacrNkHzC7d5k0pUGZKW7cKckPaWEy6xLMvAsA@mail.gmail.com>

Hi val,
You had a "conditional leak" in your ifelse statements:

dat2 <-read.table(text="ID d1 d2 d3
A 0 25 35
B 12 22 0
C 0 0 31
E 10 20 30
F 0 0 0",
header=TRUE,stringsAsFactors=FALSE)
dat2$d4<-
 ifelse(dat2$d1,dat2$d1,ifelse(dat2$d2,dat2$d2,ifelse(dat2$d3,dat2$d3,0)))

Even though it works, it is probably better to use a string of "if"
statements rather than the above.

Jim

On Wed, Nov 27, 2019 at 10:15 AM Val <valkremk at gmail.com> wrote:
>
> HI All, I am having a little issue in my ifelse statement,
> The data frame looks like as follow.
>
> dat2 <-read.table(text="ID  d1 d2 d3
> A 0 25 35
> B 12 22  0
> C 0  0  31
> E 10 20 30
> F 0  0   0",header=TRUE,stringsAsFactors=F)
> I want to create d4 and set the value based on the following conditions.
> If d1  !=0  then d4=d1
> if d1 = 0  and d2 !=0  then d4=d2
> if (d1  and d2 = 0) and d3  !=0 then d4=d3
> if all d1, d2 and d3 =0 then d4=0
>
> Here is the desired output and my attempt
>  ID d1 d2 d3 d4
>   A  0 25 35  25
>   B 12 22  0  12
>   C  0  0 31   31
>   E 10 20 30  10
>   F  0  0  0  0  0
>
> My attempt
> dat2$d4 <-  0
> dat2$d4  <- ifelse((dat2$d1 =="0"), dat2$d2, ifelse(dat2$d2 == "0"), dat2$d3, 0)
> but not working.
>
> Thank you.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wjm1 @end|ng |rom c@@@co|umb|@@edu  Wed Nov 27 07:03:29 2019
From: wjm1 @end|ng |rom c@@@co|umb|@@edu (William Michels)
Date: Tue, 26 Nov 2019 22:03:29 -0800
Subject: [R] Conditions
In-Reply-To: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>
References: <CAJOiR6ZwpR5dHL_QxbwjF7mh_=Lvxxjsp3uT_NKwTi_HRcFYeQ@mail.gmail.com>
Message-ID: <CAA99HCyys=2--v8gjDueVkt6O03hURdq29GZGhCSKDE4VtXgKQ@mail.gmail.com>

Hi Val,

Here's an answer using a series of ifelse() statements. Because the d4
column is created initially using NA as a placeholder, you can check
your conditional logic at the end using table(!is.na(dat2$d4)):

> dat2 <-read.table(text="ID  d1 d2 d3
+ A 0 25 35
+ B 12 22  0
+ C 0  0  31
+ E 10 20 30
+ F 0  0   0", header=TRUE, stringsAsFactors=F)
>
> dat2$d4 <- NA
> dat2$d4 <- with(dat2, ifelse(d1!=0, yes=d1, no=d4))
> dat2$d4 <- with(dat2, ifelse((d1==0 & d2!=0), yes=d2, no=d4))
> dat2$d4 <- with(dat2, ifelse((d1==0 & d2==0 & d3!=0), yes=d3, no=d4))
> dat2$d4 <- with(dat2, ifelse((d1==0 & d2==0 & d3==0), yes=0, no=d4))
>
> dat2
  ID d1 d2 d3 d4
1  A  0 25 35 25
2  B 12 22  0 12
3  C  0  0 31 31
4  E 10 20 30 10
5  F  0  0  0  0
>
> table(!is.na(dat2$d4))

TRUE
   5
>

Your particular conditionals don't appear sensitive to order, but
someone else using the same strategy may have to take care to run the
ifelse() statements in the correct (desired) order.

HTH, Bill.

W. Michels, Ph.D.



On Tue, Nov 26, 2019 at 3:15 PM Val <valkremk at gmail.com> wrote:
>
> HI All, I am having a little issue in my ifelse statement,
> The data frame looks like as follow.
>
> dat2 <-read.table(text="ID  d1 d2 d3
> A 0 25 35
> B 12 22  0
> C 0  0  31
> E 10 20 30
> F 0  0   0",header=TRUE,stringsAsFactors=F)
> I want to create d4 and set the value based on the following conditions.
> If d1  !=0  then d4=d1
> if d1 = 0  and d2 !=0  then d4=d2
> if (d1  and d2 = 0) and d3  !=0 then d4=d3
> if all d1, d2 and d3 =0 then d4=0
>
> Here is the desired output and my attempt
>  ID d1 d2 d3 d4
>   A  0 25 35  25
>   B 12 22  0  12
>   C  0  0 31   31
>   E 10 20 30  10
>   F  0  0  0  0  0
>
> My attempt
> dat2$d4 <-  0
> dat2$d4  <- ifelse((dat2$d1 =="0"), dat2$d2, ifelse(dat2$d2 == "0"), dat2$d3, 0)
> but not working.
>
> Thank you.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Wed Nov 27 07:58:44 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Wed, 27 Nov 2019 06:58:44 +0000
Subject: [R] 
 Stacking two graphs with different x and y scale on the same graph
In-Reply-To: <CAC8ss30T_FyB6Q0cB4_cphuvuTS6pwM85M7Xdup85cg_u8M1AQ@mail.gmail.com>
References: <CAC8ss30T_FyB6Q0cB4_cphuvuTS6pwM85M7Xdup85cg_u8M1AQ@mail.gmail.com>
Message-ID: <a6e6f822-cfee-a556-bf5c-e1a247d1a9f2@sapo.pt>

Hello,

The following is not a complete solution, the axis ranges are wrong, but 
it gets you closer, I think.


op <- par(mar = c(5, 5, 5, 5))

plot(c(x1, x2), c(y1, y2), type = "n",xaxt="n", yaxt="n",
      ylim = range(c(y1, y2)))

par(new=TRUE)
plot(x1,y1,pch=0,type="b",col="red",yaxt="n",
      xlab = "", ylab="")
axis(side=2, at=c(-5,0,2))
mtext("red line", side = 2, line=2.5, at=0)

par(new=TRUE)
plot(x2, y2, pch = 1,type="b",col="blue",
      xaxt="n", yaxt="n",
      xlab="", ylab="")
axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
mtext("blue line", side=4, line=2.5, at=0)

par(op)



Hope this helps,

Rui Barradas

?s 02:53 de 27/11/19, Ogbos Okike escreveu:
> Dear Contributors,
> I have two data. A is of the form:
> 05 01 01  -0.00376058013285748
> 05 01 02  -0.0765481943910918
> 05 01 03  -1.28158758599964
> 05 01 04  -1.51612545416506
> 05 01 05  -1.39481276373467
> 05 01 06  -1.17644992095997
> 05 01 07  -0.788249311582716
> 05 01 08  -0.925737027403825
> 05 01 09  -1.02278717974814
> 05 01 10  -0.982349616271341
> 05 01 11  -0.61032403228481
> 05 01 12  -0.197860884821482
> 05 01 13  -0.173598346735404
> 05 01 14  -0.270648499079717
> 05 01 15  -0.173598346735404
> 05 01 16  -0.343436113337951
> 05 01 17  -0.949999565489903
> 05 01 18  -3.60270372956778
> 05 01 19  -5.47091916219579
> 05 01 20  -4.67834291805057
> 05 01 21  -5.42239408602363
> 05 01 22  -4.19309215632901
> 05 01 23  -1.79918839850264
> 05 01 24  -1.04704971783422
> 05 01 25  -0.642674083066247
> 05 01 26  -0.505186367245138
> 05 01 27  -0.472836316463701
> 05 01 28  -0.537536418026576
> 05 01 29  -0.311086062556513
> 05 01 30  -0.00376058013285748
> 05 01 31  -0.254473473688998
> 05 02 01  -0.197860884821482
> 05 02 02  -0.23021093560292
> 05 02 03  -0.238298448298279
> 05 02 04  -0.157423321344685
> 05 02 05  -0.060373169000373
> 05 02 06  0.109464597602174
> 05 02 07  0.02858947064858
> 05 02 08  -0.0846357070864511
> 05 02 09  -0.189773372126123
> 05 02 10  -0.278736011775076
> 05 02 11  -0.302998549861154
> 05 02 12  -0.0684606816957324
> 05 02 13  0.0852020595160955
> 05 02 14  0.133727135688252
> 05 02 15  0.0528520087346581
> 05 02 16  0.0771145468207362
> 05 02 17  -0.0361106309142949
> 05 02 18  -0.205948397516842
> 05 02 19  -0.383873676814748
> 05 02 20  -0.383873676814748
> 05 02 21  -0.294911037165795
> 05 02 22  -0.197860884821482
> 05 02 23  -0.214035910212201
> 05 02 24  -0.165510834040045
> 05 02 25  -0.0522856563050137
> 05 02 26  0.0366769833439393
> 05 02 27  0.141814648383611
> 05 02 28  0.101377084906814
> 05 03 01  0.15798967377433
> 05 03 02  0.222689775337205
> 05 03 03  0.27930236420472
> 05 03 04  0.327827440376876
> 05 03 05  0.214602262641845
> 05 03 06  0.133727135688252
> 05 03 07  0.166077186469689
>   and B is of the form:
> 05 01 03  1.0401704890785
> 05 01 04  1.1881431442713
> 05 01 05  0.899433543239033
> 05 01 06  0.495029973508058
> 05 01 18  2.51141960034673
> 05 01 19  4.80818567931821
> 05 01 20  3.82649399122216
> 05 01 21  4.75619054623929
> 05 01 22  3.25702525028531
> 05 01 23  0.328654748869008
> 05 02 10  0.0689360507407491
> 05 02 11  0.192369729879942
> 05 02 15  0.0684297713902015
> 05 02 16  0.100584435166215
> 05 02 17  0.295302934161718
> 05 02 18  0.552388788420635
> 05 02 19  0.811732847306371
> 05 02 20  0.843313045760178
> 05 02 21  0.757193220375875
> 05 02 22  0.65352100387166
> 05 02 23  0.68252482652902
> 05 02 24  0.624510062816789
> 05 02 25  0.479854370620533
> 05 02 26  0.359002279153697
> 05 02 27  0.212459089641907
> 05 02 28  0.240784160160447
> 05 03 01  0.144583652487177
> 05 03 02  0.0345028244394553
> 05 03 21  0.023582430982633
> 05 03 22  0.000293765928922767
> 05 03 27  0.0440288222469235
> 05 03 28  0.106263428254761
> 05 03 29  0.291212461872628
> 05 03 30  0.198305017329253
> 05 03 31  0.186935599530143
> 05 04 01  0.316471519561273
> 05 04 02  0.266260602009615
> 05 04 03  0.0456391152384458
> 05 04 04  0.113939833419049
> 05 04 05  0.140500137811164
> 05 04 06  0.374670064516577
> 05 04 07  0.295820206701906
> 05 04 08  0.0833493810907385
> 05 04 10  0.0253248646840757
> 05 04 11  0.188773903020133
> 05 04 12  0.206619775067284
> 05 04 13  0.408503282817833
> 05 04 14  0.344129922512134
> 05 04 15  0.283273728250647
> 05 04 16  0.155780334719261
> 05 04 17  0.0815692243668445
> B is extracted from A and I wish to plot A and B on the same x-axis
> but different y axis on the same graph. Stacking very close helps to
> illustrates the common rapid variations in A, represented in B.
> 
> One of the codes I tried without success is:
> data <- read.table("A", col.names = c("year", "month", "day", "counts"))
> 
> new.century <- data$year < 50
> 
> data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> 
> data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> x1 = data$date
> y1=data$counts
> #y1=scale(data$counts)
> 
> 
> data <- read.table("B", col.names = c("year", "month", "day", "counts"))
> 
> new.century <- data$year < 50
> 
> data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> 
> data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> x2 = data$date
> y2=data$counts
> 
> 
> pdf("PLOT.pdf")
> 
>   par(mar=c(5,5,5,5))
> 
> plot(x1,y1,pch=0,type="b",col="red",yaxt="n",ylim=c(-5.470919,1.298329),ylab="")
> axis(side=2, at=c(-6,0,2))
> mtext("red line", side = 2, line=2.5, at=0)
> 
> par(new=TRUE)
> plot(x2,y2,pch=1,type="b",col="blue",yaxt="n",ylim=c(-4.808186,0.0), ylab="")
> axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
> mtext("blue line", side=4, line=2.5, at=100)
> dev.off()
> 
> Your assistance is ever appreciated.
> Best wishes
> Ogbos
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From g||ted|||e2014 @end|ng |rom gm@||@com  Wed Nov 27 08:55:32 2019
From: g||ted|||e2014 @end|ng |rom gm@||@com (Ogbos Okike)
Date: Wed, 27 Nov 2019 08:55:32 +0100
Subject: [R] 
 Stacking two graphs with different x and y scale on the same graph
In-Reply-To: <a6e6f822-cfee-a556-bf5c-e1a247d1a9f2@sapo.pt>
References: <CAC8ss30T_FyB6Q0cB4_cphuvuTS6pwM85M7Xdup85cg_u8M1AQ@mail.gmail.com>
 <a6e6f822-cfee-a556-bf5c-e1a247d1a9f2@sapo.pt>
Message-ID: <CAC8ss31upiKGqTMPw0-nhp82NcAVshbxU+fGjYJQTf+rH7ULMA@mail.gmail.com>

Dear Rui,
Many thanks. It is indeed close to what I am looking for apart from
the issues on the axes.

I am attaching the result plot. The minimum and maximum values of y1
are respectively -5.470919 and 1.298329 while they are 0.0002937659
and 4.808186 for y2.

New problems are: (1) I can't labelled the axis as (x1,x2) persist
when I tried to label the axes. (2) The two plots are merging at some
points. I only want them close but not merging as that does not allow
for meaningful comparison.

It would be great if you could adjust the code further to minimize
these problems.

Thank you.
Best wishes
Ogbos

On Wed, Nov 27, 2019 at 7:58 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> Hello,
>
> The following is not a complete solution, the axis ranges are wrong, but
> it gets you closer, I think.
>
>
> op <- par(mar = c(5, 5, 5, 5))
>
> plot(c(x1, x2), c(y1, y2), type = "n",xaxt="n", yaxt="n",
>       ylim = range(c(y1, y2)))
>
> par(new=TRUE)
> plot(x1,y1,pch=0,type="b",col="red",yaxt="n",
>       xlab = "", ylab="")
> axis(side=2, at=c(-5,0,2))
> mtext("red line", side = 2, line=2.5, at=0)
>
> par(new=TRUE)
> plot(x2, y2, pch = 1,type="b",col="blue",
>       xaxt="n", yaxt="n",
>       xlab="", ylab="")
> axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
> mtext("blue line", side=4, line=2.5, at=0)
>
> par(op)
>
>
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 02:53 de 27/11/19, Ogbos Okike escreveu:
> > Dear Contributors,
> > I have two data. A is of the form:
> > 05 01 01  -0.00376058013285748
> > 05 01 02  -0.0765481943910918
> > 05 01 03  -1.28158758599964
> > 05 01 04  -1.51612545416506
> > 05 01 05  -1.39481276373467
> > 05 01 06  -1.17644992095997
> > 05 01 07  -0.788249311582716
> > 05 01 08  -0.925737027403825
> > 05 01 09  -1.02278717974814
> > 05 01 10  -0.982349616271341
> > 05 01 11  -0.61032403228481
> > 05 01 12  -0.197860884821482
> > 05 01 13  -0.173598346735404
> > 05 01 14  -0.270648499079717
> > 05 01 15  -0.173598346735404
> > 05 01 16  -0.343436113337951
> > 05 01 17  -0.949999565489903
> > 05 01 18  -3.60270372956778
> > 05 01 19  -5.47091916219579
> > 05 01 20  -4.67834291805057
> > 05 01 21  -5.42239408602363
> > 05 01 22  -4.19309215632901
> > 05 01 23  -1.79918839850264
> > 05 01 24  -1.04704971783422
> > 05 01 25  -0.642674083066247
> > 05 01 26  -0.505186367245138
> > 05 01 27  -0.472836316463701
> > 05 01 28  -0.537536418026576
> > 05 01 29  -0.311086062556513
> > 05 01 30  -0.00376058013285748
> > 05 01 31  -0.254473473688998
> > 05 02 01  -0.197860884821482
> > 05 02 02  -0.23021093560292
> > 05 02 03  -0.238298448298279
> > 05 02 04  -0.157423321344685
> > 05 02 05  -0.060373169000373
> > 05 02 06  0.109464597602174
> > 05 02 07  0.02858947064858
> > 05 02 08  -0.0846357070864511
> > 05 02 09  -0.189773372126123
> > 05 02 10  -0.278736011775076
> > 05 02 11  -0.302998549861154
> > 05 02 12  -0.0684606816957324
> > 05 02 13  0.0852020595160955
> > 05 02 14  0.133727135688252
> > 05 02 15  0.0528520087346581
> > 05 02 16  0.0771145468207362
> > 05 02 17  -0.0361106309142949
> > 05 02 18  -0.205948397516842
> > 05 02 19  -0.383873676814748
> > 05 02 20  -0.383873676814748
> > 05 02 21  -0.294911037165795
> > 05 02 22  -0.197860884821482
> > 05 02 23  -0.214035910212201
> > 05 02 24  -0.165510834040045
> > 05 02 25  -0.0522856563050137
> > 05 02 26  0.0366769833439393
> > 05 02 27  0.141814648383611
> > 05 02 28  0.101377084906814
> > 05 03 01  0.15798967377433
> > 05 03 02  0.222689775337205
> > 05 03 03  0.27930236420472
> > 05 03 04  0.327827440376876
> > 05 03 05  0.214602262641845
> > 05 03 06  0.133727135688252
> > 05 03 07  0.166077186469689
> >   and B is of the form:
> > 05 01 03  1.0401704890785
> > 05 01 04  1.1881431442713
> > 05 01 05  0.899433543239033
> > 05 01 06  0.495029973508058
> > 05 01 18  2.51141960034673
> > 05 01 19  4.80818567931821
> > 05 01 20  3.82649399122216
> > 05 01 21  4.75619054623929
> > 05 01 22  3.25702525028531
> > 05 01 23  0.328654748869008
> > 05 02 10  0.0689360507407491
> > 05 02 11  0.192369729879942
> > 05 02 15  0.0684297713902015
> > 05 02 16  0.100584435166215
> > 05 02 17  0.295302934161718
> > 05 02 18  0.552388788420635
> > 05 02 19  0.811732847306371
> > 05 02 20  0.843313045760178
> > 05 02 21  0.757193220375875
> > 05 02 22  0.65352100387166
> > 05 02 23  0.68252482652902
> > 05 02 24  0.624510062816789
> > 05 02 25  0.479854370620533
> > 05 02 26  0.359002279153697
> > 05 02 27  0.212459089641907
> > 05 02 28  0.240784160160447
> > 05 03 01  0.144583652487177
> > 05 03 02  0.0345028244394553
> > 05 03 21  0.023582430982633
> > 05 03 22  0.000293765928922767
> > 05 03 27  0.0440288222469235
> > 05 03 28  0.106263428254761
> > 05 03 29  0.291212461872628
> > 05 03 30  0.198305017329253
> > 05 03 31  0.186935599530143
> > 05 04 01  0.316471519561273
> > 05 04 02  0.266260602009615
> > 05 04 03  0.0456391152384458
> > 05 04 04  0.113939833419049
> > 05 04 05  0.140500137811164
> > 05 04 06  0.374670064516577
> > 05 04 07  0.295820206701906
> > 05 04 08  0.0833493810907385
> > 05 04 10  0.0253248646840757
> > 05 04 11  0.188773903020133
> > 05 04 12  0.206619775067284
> > 05 04 13  0.408503282817833
> > 05 04 14  0.344129922512134
> > 05 04 15  0.283273728250647
> > 05 04 16  0.155780334719261
> > 05 04 17  0.0815692243668445
> > B is extracted from A and I wish to plot A and B on the same x-axis
> > but different y axis on the same graph. Stacking very close helps to
> > illustrates the common rapid variations in A, represented in B.
> >
> > One of the codes I tried without success is:
> > data <- read.table("A", col.names = c("year", "month", "day", "counts"))
> >
> > new.century <- data$year < 50
> >
> > data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> >
> > data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > x1 = data$date
> > y1=data$counts
> > #y1=scale(data$counts)
> >
> >
> > data <- read.table("B", col.names = c("year", "month", "day", "counts"))
> >
> > new.century <- data$year < 50
> >
> > data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> >
> > data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > x2 = data$date
> > y2=data$counts
> >
> >
> > pdf("PLOT.pdf")
> >
> >   par(mar=c(5,5,5,5))
> >
> > plot(x1,y1,pch=0,type="b",col="red",yaxt="n",ylim=c(-5.470919,1.298329),ylab="")
> > axis(side=2, at=c(-6,0,2))
> > mtext("red line", side = 2, line=2.5, at=0)
> >
> > par(new=TRUE)
> > plot(x2,y2,pch=1,type="b",col="blue",yaxt="n",ylim=c(-4.808186,0.0), ylab="")
> > axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
> > mtext("blue line", side=4, line=2.5, at=100)
> > dev.off()
> >
> > Your assistance is ever appreciated.
> > Best wishes
> > Ogbos
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >

-------------- next part --------------
A non-text attachment was scrubbed...
Name: TEST.pdf
Type: application/pdf
Size: 7915 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20191127/8a852f98/attachment.pdf>

From petr@p|k@| @end|ng |rom prechez@@cz  Wed Nov 27 09:10:29 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Nov 2019 08:10:29 +0000
Subject: [R] 
 Stacking two graphs with different x and y scale on the same graph
In-Reply-To: <a6e6f822-cfee-a556-bf5c-e1a247d1a9f2@sapo.pt>
References: <CAC8ss30T_FyB6Q0cB4_cphuvuTS6pwM85M7Xdup85cg_u8M1AQ@mail.gmail.com>
 <a6e6f822-cfee-a556-bf5c-e1a247d1a9f2@sapo.pt>
Message-ID: <929efde3537c463fbdd6a27b191e2761@SRVEXCHCM1301.precheza.cz>

Hi

In ancient times I developped function plot.yy which requires some data twisting but should do plotting with one x axis and two y axes and should estimete ranges automatically.

datA <- data.frame(datum=x1, counts=y1)
datA$vzor <- "A"
datB <- data.frame(datum=x2, counts=y2)
datB$vzor <- "B"
mydat <- merge(datA, datB, all=T, by="datum")

plot.yy(mydat$datum, mydat$counts.x, mydat$counts.y, col=c("red", "blue"), linky=T)


## The function is currently defined as
plot.yy <- function (x, yright, yleft, yleftlim = NULL, yrightlim = NULL, 
    xlab = NULL, yylab = list(NA, NA), pch = c(1, 2), 
    col = c(1,2), linky = F, smooth = 0, lwds = 1, length = 10, 
        format = "%d/%m", rect = NULL, type = "p", ...) 
{
    par(mar = c(5, 4, 4, 2), oma = c(0, 0, 0, 3))
    plot(x, yright, ylim = yrightlim, axes = F, ylab = "", xlab = xlab, 
        pch = pch[1], col = col[1], type = type, ...)
    if (!is.null(rect)) 
        rect(x[rect[1]], rect[2], x[rect[3]], rect[4], col = "grey")
    points(x, yright, ylim = yrightlim, ylab = "", xlab = xlab, 
        pch = pch[1], col = col[1], ...)
    axis(4, pretty(range(yright, na.rm = T), 10), col = col[1])
    if (linky) 
        lines(x, yright, col = col[1], ...)
    if (smooth != 0) 
        lines(supsmu(x, yright, span = smooth), col = col[1], 
            lwd = lwds, ...)
    if (is.na(yylab[[1]])) 
        mtext(deparse(substitute(yright)), side = 4, outer = T, 
            line = 1, col = col[1], ...)
    else mtext(yylab[[1]], side = 4, outer = T, line = 1, col = col[1], 
        ...)
    par(new = T)
    plot(x, yleft, ylim = yleftlim, ylab = "", axes = F, xlab = xlab, 
        pch = pch[2], col = col[2], ...)
    box()
    axis(2, pretty(range(yleft, na.rm = T), 10), col = col[2], 
        col.axis = col[2])
    if (!inherits(x, c("Date", "POSIXt"))) 
        axis(1, pretty(range(x, na.rm = T), 10))
    else {
        if (inherits(x, "POSIXt")) {
            l <- length(x)
            axis(1, at = x[seq(1, l, length = length)], 
            labels = format(as.POSIXct(x[seq(1,l, length = length)]), 
            format = format))
        }
        else {
            if (inherits(x, "Date")) {
                l <- length(x)
                axis(1, at = x[seq(1, l, length = length)], 
                labels = format(as.Date(x[seq(1,l, length = length)]), 
                format = format))
            }
            else {
                print("Not suitable x axis")
            }
        }
    }
    if (is.na(yylab[[2]])) 
        mtext(deparse(substitute(yleft)), side = 2, line = 2, 
            col = col[2], ...)
    else mtext(yylab[[2]], side = 2, line = 2, col = col[2], 
        ...)
    if (linky) 
        lines(x, yleft, col = col[2], lty = 2, ...)
    if (smooth != 0) 
        lines(supsmu(x, yleft, span = smooth), col = col[2], 
            lty = 2, lwd = lwds, ...)




> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Rui Barradas
> Sent: Wednesday, November 27, 2019 7:59 AM
> To: Ogbos Okike <giftedlife2014 at gmail.com>; r-help <r-help at r-project.org>
> Subject: Re: [R] Stacking two graphs with different x and y scale on the same
> graph
> 
> Hello,
> 
> The following is not a complete solution, the axis ranges are wrong, but it
> gets you closer, I think.
> 
> 
> op <- par(mar = c(5, 5, 5, 5))
> 
> plot(c(x1, x2), c(y1, y2), type = "n",xaxt="n", yaxt="n",
>       ylim = range(c(y1, y2)))
> 
> par(new=TRUE)
> plot(x1,y1,pch=0,type="b",col="red",yaxt="n",
>       xlab = "", ylab="")
> axis(side=2, at=c(-5,0,2))
> mtext("red line", side = 2, line=2.5, at=0)
> 
> par(new=TRUE)
> plot(x2, y2, pch = 1,type="b",col="blue",
>       xaxt="n", yaxt="n",
>       xlab="", ylab="")
> axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
> mtext("blue line", side=4, line=2.5, at=0)
> 
> par(op)
> 
> 
> 
> Hope this helps,
> 
> Rui Barradas
> 
> ?s 02:53 de 27/11/19, Ogbos Okike escreveu:
> > Dear Contributors,
> > I have two data. A is of the form:
> > 05 01 01  -0.00376058013285748
> > 05 01 02  -0.0765481943910918
> > 05 01 03  -1.28158758599964
> > 05 01 04  -1.51612545416506
> > 05 01 05  -1.39481276373467
> > 05 01 06  -1.17644992095997
> > 05 01 07  -0.788249311582716
> > 05 01 08  -0.925737027403825
> > 05 01 09  -1.02278717974814
> > 05 01 10  -0.982349616271341
> > 05 01 11  -0.61032403228481
> > 05 01 12  -0.197860884821482
> > 05 01 13  -0.173598346735404
> > 05 01 14  -0.270648499079717
> > 05 01 15  -0.173598346735404
> > 05 01 16  -0.343436113337951
> > 05 01 17  -0.949999565489903
> > 05 01 18  -3.60270372956778
> > 05 01 19  -5.47091916219579
> > 05 01 20  -4.67834291805057
> > 05 01 21  -5.42239408602363
> > 05 01 22  -4.19309215632901
> > 05 01 23  -1.79918839850264
> > 05 01 24  -1.04704971783422
> > 05 01 25  -0.642674083066247
> > 05 01 26  -0.505186367245138
> > 05 01 27  -0.472836316463701
> > 05 01 28  -0.537536418026576
> > 05 01 29  -0.311086062556513
> > 05 01 30  -0.00376058013285748
> > 05 01 31  -0.254473473688998
> > 05 02 01  -0.197860884821482
> > 05 02 02  -0.23021093560292
> > 05 02 03  -0.238298448298279
> > 05 02 04  -0.157423321344685
> > 05 02 05  -0.060373169000373
> > 05 02 06  0.109464597602174
> > 05 02 07  0.02858947064858
> > 05 02 08  -0.0846357070864511
> > 05 02 09  -0.189773372126123
> > 05 02 10  -0.278736011775076
> > 05 02 11  -0.302998549861154
> > 05 02 12  -0.0684606816957324
> > 05 02 13  0.0852020595160955
> > 05 02 14  0.133727135688252
> > 05 02 15  0.0528520087346581
> > 05 02 16  0.0771145468207362
> > 05 02 17  -0.0361106309142949
> > 05 02 18  -0.205948397516842
> > 05 02 19  -0.383873676814748
> > 05 02 20  -0.383873676814748
> > 05 02 21  -0.294911037165795
> > 05 02 22  -0.197860884821482
> > 05 02 23  -0.214035910212201
> > 05 02 24  -0.165510834040045
> > 05 02 25  -0.0522856563050137
> > 05 02 26  0.0366769833439393
> > 05 02 27  0.141814648383611
> > 05 02 28  0.101377084906814
> > 05 03 01  0.15798967377433
> > 05 03 02  0.222689775337205
> > 05 03 03  0.27930236420472
> > 05 03 04  0.327827440376876
> > 05 03 05  0.214602262641845
> > 05 03 06  0.133727135688252
> > 05 03 07  0.166077186469689
> >   and B is of the form:
> > 05 01 03  1.0401704890785
> > 05 01 04  1.1881431442713
> > 05 01 05  0.899433543239033
> > 05 01 06  0.495029973508058
> > 05 01 18  2.51141960034673
> > 05 01 19  4.80818567931821
> > 05 01 20  3.82649399122216
> > 05 01 21  4.75619054623929
> > 05 01 22  3.25702525028531
> > 05 01 23  0.328654748869008
> > 05 02 10  0.0689360507407491
> > 05 02 11  0.192369729879942
> > 05 02 15  0.0684297713902015
> > 05 02 16  0.100584435166215
> > 05 02 17  0.295302934161718
> > 05 02 18  0.552388788420635
> > 05 02 19  0.811732847306371
> > 05 02 20  0.843313045760178
> > 05 02 21  0.757193220375875
> > 05 02 22  0.65352100387166
> > 05 02 23  0.68252482652902
> > 05 02 24  0.624510062816789
> > 05 02 25  0.479854370620533
> > 05 02 26  0.359002279153697
> > 05 02 27  0.212459089641907
> > 05 02 28  0.240784160160447
> > 05 03 01  0.144583652487177
> > 05 03 02  0.0345028244394553
> > 05 03 21  0.023582430982633
> > 05 03 22  0.000293765928922767
> > 05 03 27  0.0440288222469235
> > 05 03 28  0.106263428254761
> > 05 03 29  0.291212461872628
> > 05 03 30  0.198305017329253
> > 05 03 31  0.186935599530143
> > 05 04 01  0.316471519561273
> > 05 04 02  0.266260602009615
> > 05 04 03  0.0456391152384458
> > 05 04 04  0.113939833419049
> > 05 04 05  0.140500137811164
> > 05 04 06  0.374670064516577
> > 05 04 07  0.295820206701906
> > 05 04 08  0.0833493810907385
> > 05 04 10  0.0253248646840757
> > 05 04 11  0.188773903020133
> > 05 04 12  0.206619775067284
> > 05 04 13  0.408503282817833
> > 05 04 14  0.344129922512134
> > 05 04 15  0.283273728250647
> > 05 04 16  0.155780334719261
> > 05 04 17  0.0815692243668445
> > B is extracted from A and I wish to plot A and B on the same x-axis
> > but different y axis on the same graph. Stacking very close helps to
> > illustrates the common rapid variations in A, represented in B.
> >
> > One of the codes I tried without success is:
> > data <- read.table("A", col.names = c("year", "month", "day", "counts"))
> >
> > new.century <- data$year < 50
> >
> > data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> >
> > data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > x1 = data$date
> > y1=data$counts
> > #y1=scale(data$counts)
> >
> >
> > data <- read.table("B", col.names = c("year", "month", "day", "counts"))
> >
> > new.century <- data$year < 50
> >
> > data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> >
> > data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > x2 = data$date
> > y2=data$counts
> >
> >
> > pdf("PLOT.pdf")
> >
> >   par(mar=c(5,5,5,5))
> >
> > plot(x1,y1,pch=0,type="b",col="red",yaxt="n",ylim=c(-
> 5.470919,1.298329),ylab="")
> > axis(side=2, at=c(-6,0,2))
> > mtext("red line", side = 2, line=2.5, at=0)
> >
> > par(new=TRUE)
> > plot(x2,y2,pch=1,type="b",col="blue",yaxt="n",ylim=c(-4.808186,0.0),
> ylab="")
> > axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
> > mtext("blue line", side=4, line=2.5, at=100)
> > dev.off()
> >
> > Your assistance is ever appreciated.
> > Best wishes
> > Ogbos
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From petr@p|k@| @end|ng |rom prechez@@cz  Wed Nov 27 09:58:05 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Nov 2019 08:58:05 +0000
Subject: [R] 
 Stacking two graphs with different x and y scale on the same graph
In-Reply-To: <CAC8ss31upiKGqTMPw0-nhp82NcAVshbxU+fGjYJQTf+rH7ULMA@mail.gmail.com>
References: <CAC8ss30T_FyB6Q0cB4_cphuvuTS6pwM85M7Xdup85cg_u8M1AQ@mail.gmail.com>
 <a6e6f822-cfee-a556-bf5c-e1a247d1a9f2@sapo.pt>
 <CAC8ss31upiKGqTMPw0-nhp82NcAVshbxU+fGjYJQTf+rH7ULMA@mail.gmail.com>
Message-ID: <5bf1d68093c7465cb635085c46b2649d@SRVEXCHCM1301.precheza.cz>

Hm,

I just wonder why you do not use ggplot?

datA <- data.frame(datum=x1, counts=y1)
datA$vzor <- "A"
datB <- data.frame(datum=x2, counts=y2)
datB$vzor <- "B"
mydat <- merge(datA, datB, all=T)

library(ggplot2)
p <- ggplot(mydat, aes(x=datum, y=counts, colour=vzor))
p+geom_point(size=3)+geom_line()

The only problem is that it uses only one y axis but allows extensive annotation changing.

Even axes range manipulation is possible, although not recommended.
http://rpubs.com/kohske/dual_axis_in_ggplot2

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ogbos Okike
> Sent: Wednesday, November 27, 2019 8:56 AM
> To: Rui Barradas <ruipbarradas at sapo.pt>
> Cc: r-help <r-help at r-project.org>
> Subject: Re: [R] Stacking two graphs with different x and y scale on the same
> graph
> 
> Dear Rui,
> Many thanks. It is indeed close to what I am looking for apart from the issues
> on the axes.
> 
> I am attaching the result plot. The minimum and maximum values of y1 are
> respectively -5.470919 and 1.298329 while they are 0.0002937659 and
> 4.808186 for y2.
> 
> New problems are: (1) I can't labelled the axis as (x1,x2) persist when I tried
> to label the axes. (2) The two plots are merging at some points. I only want
> them close but not merging as that does not allow for meaningful
> comparison.
> 
> It would be great if you could adjust the code further to minimize these
> problems.
> 
> Thank you.
> Best wishes
> Ogbos
> 
> On Wed, Nov 27, 2019 at 7:58 AM Rui Barradas <ruipbarradas at sapo.pt>
> wrote:
> >
> > Hello,
> >
> > The following is not a complete solution, the axis ranges are wrong,
> > but it gets you closer, I think.
> >
> >
> > op <- par(mar = c(5, 5, 5, 5))
> >
> > plot(c(x1, x2), c(y1, y2), type = "n",xaxt="n", yaxt="n",
> >       ylim = range(c(y1, y2)))
> >
> > par(new=TRUE)
> > plot(x1,y1,pch=0,type="b",col="red",yaxt="n",
> >       xlab = "", ylab="")
> > axis(side=2, at=c(-5,0,2))
> > mtext("red line", side = 2, line=2.5, at=0)
> >
> > par(new=TRUE)
> > plot(x2, y2, pch = 1,type="b",col="blue",
> >       xaxt="n", yaxt="n",
> >       xlab="", ylab="")
> > axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%")) mtext("blue
> > line", side=4, line=2.5, at=0)
> >
> > par(op)
> >
> >
> >
> > Hope this helps,
> >
> > Rui Barradas
> >
> > ?s 02:53 de 27/11/19, Ogbos Okike escreveu:
> > > Dear Contributors,
> > > I have two data. A is of the form:
> > > 05 01 01  -0.00376058013285748
> > > 05 01 02  -0.0765481943910918
> > > 05 01 03  -1.28158758599964
> > > 05 01 04  -1.51612545416506
> > > 05 01 05  -1.39481276373467
> > > 05 01 06  -1.17644992095997
> > > 05 01 07  -0.788249311582716
> > > 05 01 08  -0.925737027403825
> > > 05 01 09  -1.02278717974814
> > > 05 01 10  -0.982349616271341
> > > 05 01 11  -0.61032403228481
> > > 05 01 12  -0.197860884821482
> > > 05 01 13  -0.173598346735404
> > > 05 01 14  -0.270648499079717
> > > 05 01 15  -0.173598346735404
> > > 05 01 16  -0.343436113337951
> > > 05 01 17  -0.949999565489903
> > > 05 01 18  -3.60270372956778
> > > 05 01 19  -5.47091916219579
> > > 05 01 20  -4.67834291805057
> > > 05 01 21  -5.42239408602363
> > > 05 01 22  -4.19309215632901
> > > 05 01 23  -1.79918839850264
> > > 05 01 24  -1.04704971783422
> > > 05 01 25  -0.642674083066247
> > > 05 01 26  -0.505186367245138
> > > 05 01 27  -0.472836316463701
> > > 05 01 28  -0.537536418026576
> > > 05 01 29  -0.311086062556513
> > > 05 01 30  -0.00376058013285748
> > > 05 01 31  -0.254473473688998
> > > 05 02 01  -0.197860884821482
> > > 05 02 02  -0.23021093560292
> > > 05 02 03  -0.238298448298279
> > > 05 02 04  -0.157423321344685
> > > 05 02 05  -0.060373169000373
> > > 05 02 06  0.109464597602174
> > > 05 02 07  0.02858947064858
> > > 05 02 08  -0.0846357070864511
> > > 05 02 09  -0.189773372126123
> > > 05 02 10  -0.278736011775076
> > > 05 02 11  -0.302998549861154
> > > 05 02 12  -0.0684606816957324
> > > 05 02 13  0.0852020595160955
> > > 05 02 14  0.133727135688252
> > > 05 02 15  0.0528520087346581
> > > 05 02 16  0.0771145468207362
> > > 05 02 17  -0.0361106309142949
> > > 05 02 18  -0.205948397516842
> > > 05 02 19  -0.383873676814748
> > > 05 02 20  -0.383873676814748
> > > 05 02 21  -0.294911037165795
> > > 05 02 22  -0.197860884821482
> > > 05 02 23  -0.214035910212201
> > > 05 02 24  -0.165510834040045
> > > 05 02 25  -0.0522856563050137
> > > 05 02 26  0.0366769833439393
> > > 05 02 27  0.141814648383611
> > > 05 02 28  0.101377084906814
> > > 05 03 01  0.15798967377433
> > > 05 03 02  0.222689775337205
> > > 05 03 03  0.27930236420472
> > > 05 03 04  0.327827440376876
> > > 05 03 05  0.214602262641845
> > > 05 03 06  0.133727135688252
> > > 05 03 07  0.166077186469689
> > >   and B is of the form:
> > > 05 01 03  1.0401704890785
> > > 05 01 04  1.1881431442713
> > > 05 01 05  0.899433543239033
> > > 05 01 06  0.495029973508058
> > > 05 01 18  2.51141960034673
> > > 05 01 19  4.80818567931821
> > > 05 01 20  3.82649399122216
> > > 05 01 21  4.75619054623929
> > > 05 01 22  3.25702525028531
> > > 05 01 23  0.328654748869008
> > > 05 02 10  0.0689360507407491
> > > 05 02 11  0.192369729879942
> > > 05 02 15  0.0684297713902015
> > > 05 02 16  0.100584435166215
> > > 05 02 17  0.295302934161718
> > > 05 02 18  0.552388788420635
> > > 05 02 19  0.811732847306371
> > > 05 02 20  0.843313045760178
> > > 05 02 21  0.757193220375875
> > > 05 02 22  0.65352100387166
> > > 05 02 23  0.68252482652902
> > > 05 02 24  0.624510062816789
> > > 05 02 25  0.479854370620533
> > > 05 02 26  0.359002279153697
> > > 05 02 27  0.212459089641907
> > > 05 02 28  0.240784160160447
> > > 05 03 01  0.144583652487177
> > > 05 03 02  0.0345028244394553
> > > 05 03 21  0.023582430982633
> > > 05 03 22  0.000293765928922767
> > > 05 03 27  0.0440288222469235
> > > 05 03 28  0.106263428254761
> > > 05 03 29  0.291212461872628
> > > 05 03 30  0.198305017329253
> > > 05 03 31  0.186935599530143
> > > 05 04 01  0.316471519561273
> > > 05 04 02  0.266260602009615
> > > 05 04 03  0.0456391152384458
> > > 05 04 04  0.113939833419049
> > > 05 04 05  0.140500137811164
> > > 05 04 06  0.374670064516577
> > > 05 04 07  0.295820206701906
> > > 05 04 08  0.0833493810907385
> > > 05 04 10  0.0253248646840757
> > > 05 04 11  0.188773903020133
> > > 05 04 12  0.206619775067284
> > > 05 04 13  0.408503282817833
> > > 05 04 14  0.344129922512134
> > > 05 04 15  0.283273728250647
> > > 05 04 16  0.155780334719261
> > > 05 04 17  0.0815692243668445
> > > B is extracted from A and I wish to plot A and B on the same x-axis
> > > but different y axis on the same graph. Stacking very close helps to
> > > illustrates the common rapid variations in A, represented in B.
> > >
> > > One of the codes I tried without success is:
> > > data <- read.table("A", col.names = c("year", "month", "day",
> > > "counts"))
> > >
> > > new.century <- data$year < 50
> > >
> > > data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > >
> > > data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > > x1 = data$date
> > > y1=data$counts
> > > #y1=scale(data$counts)
> > >
> > >
> > > data <- read.table("B", col.names = c("year", "month", "day",
> > > "counts"))
> > >
> > > new.century <- data$year < 50
> > >
> > > data$year <- ifelse(new.century, data$year + 2000, data$year + 1900)
> > >
> > > data$date <- as.Date(ISOdate(data$year, data$month, data$day))
> > > x2 = data$date
> > > y2=data$counts
> > >
> > >
> > > pdf("PLOT.pdf")
> > >
> > >   par(mar=c(5,5,5,5))
> > >
> > > plot(x1,y1,pch=0,type="b",col="red",yaxt="n",ylim=c(-5.470919,1.2983
> > > 29),ylab="")
> > > axis(side=2, at=c(-6,0,2))
> > > mtext("red line", side = 2, line=2.5, at=0)
> > >
> > > par(new=TRUE)
> > > plot(x2,y2,pch=1,type="b",col="blue",yaxt="n",ylim=c(-4.808186,0.0),
> > > ylab="") axis(side=4, at=c(-5,-1,0), labels=c("98%","100%","102%"))
> > > mtext("blue line", side=4, line=2.5, at=100)
> > > dev.off()
> > >
> > > Your assistance is ever appreciated.
> > > Best wishes
> > > Ogbos
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >

From @@h|mk@poor @end|ng |rom gm@||@com  Wed Nov 27 12:55:17 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 27 Nov 2019 17:25:17 +0530
Subject: [R] Orthogonal polynomials used by R
Message-ID: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>

Dear All,

I have created a time trend by doing x<-1:93 because I have a time series
with 93 data points. Next I did :-

y = lm(series ~ poly(x,4))$residuals

to detrend series.

I choose this 4 as the order of my polynomial using cross validation/
checking the absence of trend in the residuals so I think I have not
overfit this series.

I wish to document the formula of poly(x,4). I am not able to find it in
?poly

Can someone please tell me what the formula for the orthogonal polynomial
used by R is ?

Thank you,
Ashim

	[[alternative HTML version deleted]]


From petr@p|k@| @end|ng |rom prechez@@cz  Wed Nov 27 13:41:11 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Nov 2019 12:41:11 +0000
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
Message-ID: <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>

You could get answer quickly by searching net.

https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
olynomials-how-to-understand-the-coefs-ret/39051154#39051154

Cheers
Petr

> -----Original Message-----
> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim Kapoor
> Sent: Wednesday, November 27, 2019 12:55 PM
> To: R Help <r-help at r-project.org>
> Subject: [R] Orthogonal polynomials used by R
> 
> Dear All,
> 
> I have created a time trend by doing x<-1:93 because I have a time series
> with 93 data points. Next I did :-
> 
> y = lm(series ~ poly(x,4))$residuals
> 
> to detrend series.
> 
> I choose this 4 as the order of my polynomial using cross validation/
> checking the absence of trend in the residuals so I think I have not
overfit
> this series.
> 
> I wish to document the formula of poly(x,4). I am not able to find it in
?poly
> 
> Can someone please tell me what the formula for the orthogonal
> polynomial used by R is ?
> 
> Thank you,
> Ashim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From @@h|mk@poor @end|ng |rom gm@||@com  Wed Nov 27 16:17:51 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Wed, 27 Nov 2019 20:47:51 +0530
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
Message-ID: <CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>

Dear Petr,

Many thanks for the quick response.

I also read this:-
https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials

Also I read  in ?poly:-
     The orthogonal polynomial is summarized by the coefficients, which
     can be used to evaluate it via the three-term recursion given in
     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
     of the code.

I don't have access to the mentioned book.

Out of curiosity, what is the name of the discrete orthogonal polynomial
used by R ?
What discrete measure is it orthogonal with respect to ?

Many thanks,
Ashim




On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:

> You could get answer quickly by searching net.
>
>
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
> <https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154>
>
> Cheers
> Petr
>
> > -----Original Message-----
> > From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim Kapoor
> > Sent: Wednesday, November 27, 2019 12:55 PM
> > To: R Help <r-help at r-project.org>
> > Subject: [R] Orthogonal polynomials used by R
> >
> > Dear All,
> >
> > I have created a time trend by doing x<-1:93 because I have a time series
> > with 93 data points. Next I did :-
> >
> > y = lm(series ~ poly(x,4))$residuals
> >
> > to detrend series.
> >
> > I choose this 4 as the order of my polynomial using cross validation/
> > checking the absence of trend in the residuals so I think I have not
> overfit
> > this series.
> >
> > I wish to document the formula of poly(x,4). I am not able to find it in
> ?poly
> >
> > Can someone please tell me what the formula for the orthogonal
> > polynomial used by R is ?
> >
> > Thank you,
> > Ashim
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From t@ub|@ @end|ng |rom |mgprec|@|on@com  Wed Nov 27 16:28:08 2019
From: t@ub|@ @end|ng |rom |mgprec|@|on@com (Thomas Subia)
Date: Wed, 27 Nov 2019 15:28:08 +0000
Subject: [R] pdf_combine error message
Message-ID: <CH2PR17MB37497E93DA538B86B86D5600B8440@CH2PR17MB3749.namprd17.prod.outlook.com>

Colleagues

When executing pdf_combine(files, output= "joined.pdf") I get the following error:

Error_cpp_pdf_combine(input,output, password)  :
 open c:\temp\10678.pdf: Too many open files

Directory temp contains 720 pdf files. 

I changed the number of files in the temp directory to 500.
I reran pdf_combine(files, output= "joined.pdf") and I didn't get a too many open files warning.

Is there an R setting which I can change to allow more files to be read?


All the best

Thomas Subia 
Statistician / Sr. Quality Engineer
IMG Companies?


From petr@p|k@| @end|ng |rom prechez@@cz  Wed Nov 27 16:30:27 2019
From: petr@p|k@| @end|ng |rom prechez@@cz (PIKAL Petr)
Date: Wed, 27 Nov 2019 15:30:27 +0000
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
Message-ID: <38c01bb43b364d7789adc74e5b4ae381@SRVEXCHCM1301.precheza.cz>

Hi

Your questions are beyound my expertise. Maybe others could answer it.

However, if you know enough theorethical statistics, you could get many answers searching e.g.

R lm poly

Cheers
Petr

From: Ashim Kapoor <ashimkapoor at gmail.com> 
Sent: Wednesday, November 27, 2019 4:18 PM
To: PIKAL Petr <petr.pikal at precheza.cz>
Cc: R Help <r-help at r-project.org>
Subject: Re: [R] Orthogonal polynomials used by R

Dear Petr,

Many thanks for the quick response. 

I also read this:-
https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials

Also I read  in ?poly:-
     The orthogonal polynomial is summarized by the coefficients, which
     can be used to evaluate it via the three-term recursion given in
     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
     of the code.

I don't have access to the mentioned book. 

Out of curiosity, what is the name of the discrete orthogonal polynomial used by R ? 
What discrete measure is it orthogonal with respect to ? 

Many thanks,
Ashim




On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <mailto:petr.pikal at precheza.cz> wrote:
You could get answer quickly by searching net.

https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154

Cheers
Petr

> -----Original Message-----
> From: R-help <mailto:r-help-bounces at r-project.org> On Behalf Of Ashim Kapoor
> Sent: Wednesday, November 27, 2019 12:55 PM
> To: R Help <mailto:r-help at r-project.org>
> Subject: [R] Orthogonal polynomials used by R
> 
> Dear All,
> 
> I have created a time trend by doing x<-1:93 because I have a time series
> with 93 data points. Next I did :-
> 
> y = lm(series ~ poly(x,4))$residuals
> 
> to detrend series.
> 
> I choose this 4 as the order of my polynomial using cross validation/
> checking the absence of trend in the residuals so I think I have not
overfit
> this series.
> 
> I wish to document the formula of poly(x,4). I am not able to find it in
?poly
> 
> Can someone please tell me what the formula for the orthogonal
> polynomial used by R is ?
> 
> Thank you,
> Ashim
> 
>       [[alternative HTML version deleted]]
> 
> ______________________________________________
> mailto:R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From |@r|dcher @end|ng |rom gm@||@com  Wed Nov 27 17:22:07 2019
From: |@r|dcher @end|ng |rom gm@||@com (Farid Cheraghi)
Date: Wed, 27 Nov 2019 19:52:07 +0330
Subject: [R] graphical parameter "srt" has the wrong length
Message-ID: <CAJTBV4Xo3v9ruE+qUbZ6r8smJWJpdc8c1NvEW_CZ6hSRjhXgxg@mail.gmail.com>

Hi all,

Is srt parameter in text() not vectorized?

plot(1:10, type='n')
text(1:10, 1:10, letters[1:10], srt=rep(c(0, 90),5))

gives error:   graphical parameter "srt" has the wrong length. The same
issue is reported here:
https://r.789695.n4.nabble.com/should-srt-in-text-par-recycle-its-parametres-or-not-td2125137.html

thanks

	[[alternative HTML version deleted]]


From jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@  Wed Nov 27 17:41:48 2019
From: jdnewm|| @end|ng |rom dcn@d@v|@@c@@u@ (Jeff Newmiller)
Date: Wed, 27 Nov 2019 08:41:48 -0800
Subject: [R] pdf_combine error message
In-Reply-To: <CH2PR17MB37497E93DA538B86B86D5600B8440@CH2PR17MB3749.namprd17.prod.outlook.com>
References: <CH2PR17MB37497E93DA538B86B86D5600B8440@CH2PR17MB3749.namprd17.prod.outlook.com>
Message-ID: <32C44710-A5FC-4123-881F-D6425C28EEA8@dcn.davis.ca.us>

I don't think this error is from R.

https://serverfault.com/questions/300272/how-to-deal-with-error-too-many-open-files-in-system

On November 27, 2019 7:28:08 AM PST, Thomas Subia <tsubia at imgprecision.com> wrote:
>Colleagues
>
>When executing pdf_combine(files, output= "joined.pdf") I get the
>following error:
>
>Error_cpp_pdf_combine(input,output, password)  :
> open c:\temp\10678.pdf: Too many open files
>
>Directory temp contains 720 pdf files. 
>
>I changed the number of files in the temp directory to 500.
>I reran pdf_combine(files, output= "joined.pdf") and I didn't get a too
>many open files warning.
>
>Is there an R setting which I can change to allow more files to be
>read?
>
>
>All the best
>
>Thomas Subia 
>Statistician / Sr. Quality Engineer
>IMG Companies?
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

-- 
Sent from my phone. Please excuse my brevity.


From j|ox @end|ng |rom mcm@@ter@c@  Wed Nov 27 18:11:04 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Wed, 27 Nov 2019 17:11:04 +0000
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
Message-ID: <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>

Dear Ashim,

Orthogonal polynomials are used because they tend to produce more accurate numerical computations, not because their coefficients are interpretable, so I wonder why you're interested in the coefficients. 

The regressors produced are orthogonal to the constant regressor and are orthogonal to each other (and in fact are orthonormal), as it's simple to demonstrate:

------- snip -------

> x <- 1:93
> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> (m <- lm(y ~ poly(x, 4)))

Call:
lm(formula = y ~ poly(x, 4))

Coefficients:
(Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4  
   15574516    172715069     94769949     27683528      3429259  

> X <- model.matrix(m)
> head(X)
  (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
6           1  -0.1583708   0.1545171  -0.1074869  0.03269145

> zapsmall(crossprod(X))# X'X
            (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
(Intercept)          93           0           0           0           0
poly(x, 4)1           0           1           0           0           0
poly(x, 4)2           0           0           1           0           0
poly(x, 4)3           0           0           0           1           0
poly(x, 4)4           0           0           0           0           1

------- snip -------

If for some not immediately obvious reason you're interested in the regression coefficients, why not just use a "raw" polynomial:

------- snip -------

> (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))

Call:
lm(formula = y ~ poly(x, 4, raw = TRUE))

Coefficients:
            (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw = TRUE)2  poly(x, 4, raw = TRUE)3  
                 1.5640                   0.8985                   1.0037                   1.0000  
poly(x, 4, raw = TRUE)4  
                 1.0000  

------- snip -------

These coefficients are simply interpretable but the model matrix is more poorly conditioned:

------- snip -------

> head(X1)
  (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4, raw = TRUE)3
1           1                       1                       1                       1
2           1                       2                       4                       8
3           1                       3                       9                      27
4           1                       4                      16                      64
5           1                       5                      25                     125
6           1                       6                      36                     216
  poly(x, 4, raw = TRUE)4
1                       1
2                      16
3                      81
4                     256
5                     625
6                    1296
> round(cor(X1[, -1]), 2)
                        poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4, raw = TRUE)3
poly(x, 4, raw = TRUE)1                    1.00                    0.97                    0.92
poly(x, 4, raw = TRUE)2                    0.97                    1.00                    0.99
poly(x, 4, raw = TRUE)3                    0.92                    0.99                    1.00
poly(x, 4, raw = TRUE)4                    0.87                    0.96                    0.99
                        poly(x, 4, raw = TRUE)4
poly(x, 4, raw = TRUE)1                    0.87
poly(x, 4, raw = TRUE)2                    0.96
poly(x, 4, raw = TRUE)3                    0.99
poly(x, 4, raw = TRUE)4                    1.00

------- snip -------

The two parametrizations are equivalent, however, in that they represent the same regression surface, and so, e.g., produce the same fitted values:

------- snip -------

> all.equal(fitted(m), fitted(m1))
[1] TRUE

------- snip -------

Because one is usually not interested in the individual coefficients of a polynomial there usually isn't a reason to prefer one parametrization to the other on the grounds of interpretability, so why do you need to interpret the regression equation?

I hope this helps,
 John

  ----------------------------- 
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> 
> Dear Petr,
> 
> Many thanks for the quick response.
> 
> I also read this:-
> https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
> 
> Also I read  in ?poly:-
>     The orthogonal polynomial is summarized by the coefficients, which
>     can be used to evaluate it via the three-term recursion given in
>     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
>     of the code.
> 
> I don't have access to the mentioned book.
> 
> Out of curiosity, what is the name of the discrete orthogonal polynomial
> used by R ?
> What discrete measure is it orthogonal with respect to ?
> 
> Many thanks,
> Ashim
> 
> 
> 
> 
> On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:
> 
>> You could get answer quickly by searching net.
>> 
>> 
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
>> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
>> <https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154>
>> 
>> Cheers
>> Petr
>> 
>>> -----Original Message-----
>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim Kapoor
>>> Sent: Wednesday, November 27, 2019 12:55 PM
>>> To: R Help <r-help at r-project.org>
>>> Subject: [R] Orthogonal polynomials used by R
>>> 
>>> Dear All,
>>> 
>>> I have created a time trend by doing x<-1:93 because I have a time series
>>> with 93 data points. Next I did :-
>>> 
>>> y = lm(series ~ poly(x,4))$residuals
>>> 
>>> to detrend series.
>>> 
>>> I choose this 4 as the order of my polynomial using cross validation/
>>> checking the absence of trend in the residuals so I think I have not
>> overfit
>>> this series.
>>> 
>>> I wish to document the formula of poly(x,4). I am not able to find it in
>> ?poly
>>> 
>>> Can someone please tell me what the formula for the orthogonal
>>> polynomial used by R is ?
>>> 
>>> Thank you,
>>> Ashim
>>> 
>>>      [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>> guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Nov 27 18:18:14 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 27 Nov 2019 12:18:14 -0500
Subject: [R] graphical parameter "srt" has the wrong length
In-Reply-To: <CAJTBV4Xo3v9ruE+qUbZ6r8smJWJpdc8c1NvEW_CZ6hSRjhXgxg@mail.gmail.com>
References: <CAJTBV4Xo3v9ruE+qUbZ6r8smJWJpdc8c1NvEW_CZ6hSRjhXgxg@mail.gmail.com>
Message-ID: <81ab060d-44f1-acbf-0e96-65095acc0b8f@gmail.com>

On 27/11/2019 11:22 a.m., Farid Cheraghi wrote:
> Hi all,
> 
> Is srt parameter in text() not vectorized?

No, it's documented (indirectly) as "A numerical value". (See ?par.)

Duncan Murdoch

> 
> plot(1:10, type='n')
> text(1:10, 1:10, letters[1:10], srt=rep(c(0, 90),5))
> 
> gives error:   graphical parameter "srt" has the wrong length. The same
> issue is reported here:
> https://r.789695.n4.nabble.com/should-srt-in-text-par-recycle-its-parametres-or-not-td2125137.html
> 
> thanks
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From |@r|dcher @end|ng |rom gm@||@com  Wed Nov 27 20:11:51 2019
From: |@r|dcher @end|ng |rom gm@||@com (Farid Cheraghi)
Date: Wed, 27 Nov 2019 22:41:51 +0330
Subject: [R] graphical parameter "srt" has the wrong length
In-Reply-To: <81ab060d-44f1-acbf-0e96-65095acc0b8f@gmail.com>
References: <CAJTBV4Xo3v9ruE+qUbZ6r8smJWJpdc8c1NvEW_CZ6hSRjhXgxg@mail.gmail.com>
 <81ab060d-44f1-acbf-0e96-65095acc0b8f@gmail.com>
Message-ID: <CAJTBV4WJWLzrz6WRNRvTyQKTQ5Q-Cy-wD18uhtnbAON3jVnm+A@mail.gmail.com>

Thanks for the input.

On Wed, Nov 27, 2019, 8:48 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 27/11/2019 11:22 a.m., Farid Cheraghi wrote:
> > Hi all,
> >
> > Is srt parameter in text() not vectorized?
>
> No, it's documented (indirectly) as "A numerical value". (See ?par.)
>
> Duncan Murdoch
>
> >
> > plot(1:10, type='n')
> > text(1:10, 1:10, letters[1:10], srt=rep(c(0, 90),5))
> >
> > gives error:   graphical parameter "srt" has the wrong length. The same
> > issue is reported here:
> >
> https://r.789695.n4.nabble.com/should-srt-in-text-par-recycle-its-parametres-or-not-td2125137.html
> >
> > thanks
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
>

	[[alternative HTML version deleted]]


From @t@r@kykwe@| @end|ng |rom gm@||@com  Thu Nov 28 01:00:30 2019
From: @t@r@kykwe@| @end|ng |rom gm@||@com (Kwesi A. Quagraine)
Date: Wed, 27 Nov 2019 19:00:30 -0500
Subject: [R] Panel barplot with error bars on one bar
Message-ID: <CAGD2cKeP8NNCA9N_UMCK2qjCn50RSBjotGyQdnHp-QjiHstqsg@mail.gmail.com>

Dear All,

I have been able to make a grouped panel bar plot (please see attached), I
am however trying to add error bars to one of the bar plots (ERA-interim)
across the panel and failing.

I am able to do that for just the first marplot, but my loop to apply to
the others is failing. Here?s a snippet of my code and I will be grateful
for any help. The snippet below shows what I have tried to do to apply to
panels 2:4.

??snip??

postscript(?test.eps",width=10,height=8,paper="special",horizontal=T,onefile=T)

par(mfrow=c(3,4))
#par(mar=c(8.8,3.5,1.0,0), oma = c(4, 4, 3, 3)) # spaces in the order of
c(bottom,left,top,right)
par(mar=c(6,7,1,1))

era_ci = jja.all.b.per[1,]

a =1
for (j in a) {
  jja.bar<- barplot(jja.gcms[,j],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
j,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)

  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,j],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
j,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  ## Add text at top of bars
  text(y = jja.bar, x = jja.gcms[,j], label = round(jja.gcms[,j],
digits=0), pos = 4, cex = 1.0)
}
arrows(x0= (era_ci[1] + sd(era_ci)), x1 = (era_ci[1] - sd(era_ci)), y0 =
jja.bar[1,1], angle = 90, code = 3, length = 0.045)


for (i in 2:4) {
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i, yaxs="i",type="n",font.main = 1, cex.main = 1.1,las=1, axisnames =
FALSE, width = 0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i, yaxs="i",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = jja.bar, x = jja.gcms[,i], label = round(jja.gcms[,i],
digits=0), pos = 4, cex = 1.0)
  arrows(x0= (era_ci[i] + sd(era_ci)), x1 = (era_ci[i] - sd(era_ci)), y0 =
jja.bar[1,i], angle = 90, code = 3, length = 0.045)

}

b=5
for (k in b) {
  jja.bar<-barplot(jja.gcms[,k],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
k,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,k],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
k,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = jja.bar, x = jja.gcms[,k], label = round(jja.gcms[,k],
digits=0), pos = 4, cex = 1.0)

}

for (n in 6:8) {
  barplot(jja.gcms[,n],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
n,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  barplot(jja.gcms[,n],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
n,font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = jja.bar, x = jja.gcms[,n], label = round(jja.gcms[,n],
digits=0), pos = 4, cex = 1.0)

}

c = 9
for (m in c) {
  jja.bar<-barplot(jja.gcms[,m],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
m,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,m],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
m,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  title("Frequency (%)", line = -19.0)
  text(y = jja.bar, x = jja.gcms[,m], label = round(jja.gcms[,m],
digits=0), pos = 4, cex = 1.0)

}

for (i in 10:12) {
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i,font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
0.8,cex.names=1.0,horiz = TRUE) #xlab="Frequency (%)"
  title("Frequency (%)", line = -19.0)
  text(y = jja.bar, x = jja.gcms[,i], label = round(jja.gcms[,i],
digits=0), pos = 4, cex = 1.0)

}
dev.off()


Any help will be much appreciated.

Regards
Kwesi
------------
Try not to become a man of success but rather a man of value- Albert
Einstein

Kwesi A. Quagraine
Department of Physics
School of Physical Sciences
College of Agriculture and Natural Sciences
University of Cape Coast
Cape Coast, Ghana

Alt. Email: kwesi at csag.uct.ac.za
Web: http://www.recycleupghana.org/
Office: +27 21 650 3164
Skype: quagraine_cwasi

From @t@r@kykwe@| @end|ng |rom gm@||@com  Thu Nov 28 01:19:09 2019
From: @t@r@kykwe@| @end|ng |rom gm@||@com (Kwesi A. Quagraine)
Date: Wed, 27 Nov 2019 19:19:09 -0500
Subject: [R] Panel barplot with error bars on one bar
In-Reply-To: <CAGD2cKeP8NNCA9N_UMCK2qjCn50RSBjotGyQdnHp-QjiHstqsg@mail.gmail.com>
References: <CAGD2cKeP8NNCA9N_UMCK2qjCn50RSBjotGyQdnHp-QjiHstqsg@mail.gmail.com>
Message-ID: <CAGD2cKcCk3SjsUPETucR+r0P+k=1BdPR+GA8U0Xg0wV1RBK_3Q@mail.gmail.com>

I have been able to solve my problem. Please don?t bother. Thanks

On 28 November 2019 at 02:00:30, Kwesi A. Quagraine (starskykwesi at gmail.com)
wrote:

Dear All,

I have been able to make a grouped panel bar plot (please see attached), I
am however trying to add error bars to one of the bar plots (ERA-interim)
across the panel and failing.

I am able to do that for just the first marplot, but my loop to apply to
the others is failing. Here?s a snippet of my code and I will be grateful
for any help. The snippet below shows what I have tried to do to apply to
panels 2:4.

??snip??

postscript(?test.eps",width=10,height=8,paper="special",horizontal=T,onefile=T)

par(mfrow=c(3,4))
#par(mar=c(8.8,3.5,1.0,0), oma = c(4, 4, 3, 3)) # spaces in the order of
c(bottom,left,top,right)
par(mar=c(6,7,1,1))

era_ci = jja.all.b.per[1,]

a =1
for (j in a) {
  jja.bar<- barplot(jja.gcms[,j],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
j,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)

  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,j],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
j,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  ## Add text at top of bars
  text(y = jja.bar, x = jja.gcms[,j], label = round(jja.gcms[,j],
digits=0), pos = 4, cex = 1.0)
}
arrows(x0= (era_ci[1] + sd(era_ci)), x1 = (era_ci[1] - sd(era_ci)), y0 =
jja.bar[1,1], angle = 90, code = 3, length = 0.045)


for (i in 2:4) {
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i, yaxs="i",type="n",font.main = 1, cex.main = 1.1,las=1, axisnames =
FALSE, width = 0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i, yaxs="i",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = jja.bar, x = jja.gcms[,i], label = round(jja.gcms[,i],
digits=0), pos = 4, cex = 1.0)
  arrows(x0= (era_ci[i] + sd(era_ci)), x1 = (era_ci[i] - sd(era_ci)), y0 =
jja.bar[1,i], angle = 90, code = 3, length = 0.045)

}

b=5
for (k in b) {
  jja.bar<-barplot(jja.gcms[,k],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
k,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,k],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
k,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = jja.bar, x = jja.gcms[,k], label = round(jja.gcms[,k],
digits=0), pos = 4, cex = 1.0)

}

for (n in 6:8) {
  barplot(jja.gcms[,n],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
n,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  barplot(jja.gcms[,n],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
n,font.main = 1, cex.main = 1.1,las=1, axisnames = FALSE, width =
0.8,cex.names=1.0,horiz = TRUE)
  text(y = jja.bar, x = jja.gcms[,n], label = round(jja.gcms[,n],
digits=0), pos = 4, cex = 1.0)

}

c = 9
for (m in c) {
  jja.bar<-barplot(jja.gcms[,m],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
m,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,m],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
m,font.main = 1, cex.main = 1.1,las=1, axisnames = T, width =
0.8,cex.names=1.0,horiz = TRUE)
  title("Frequency (%)", line = -19.0)
  text(y = jja.bar, x = jja.gcms[,m], label = round(jja.gcms[,m],
digits=0), pos = 4, cex = 1.0)

}

for (i in 10:12) {
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i,type="n",font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
0.8,cex.names=1.0,horiz = TRUE)
  abline(v=c(seq(0,25,5)),col='grey',lwd=0.2)
  par(new=TRUE)
  jja.bar<-barplot(jja.gcms[,i],ylim=c(0,9),xlim=c(0,25),col=cores1,main =
i,font.main = 1, cex.main = 1.1,las=1, axisnames = F, width =
0.8,cex.names=1.0,horiz = TRUE) #xlab="Frequency (%)"
  title("Frequency (%)", line = -19.0)
  text(y = jja.bar, x = jja.gcms[,i], label = round(jja.gcms[,i],
digits=0), pos = 4, cex = 1.0)

}
dev.off()


Any help will be much appreciated.

Regards
Kwesi
------------
Try not to become a man of success but rather a man of value- Albert
Einstein

Kwesi A. Quagraine
Department of Physics
School of Physical Sciences
College of Agriculture and Natural Sciences
University of Cape Coast
Cape Coast, Ghana

Alt. Email: kwesi at csag.uct.ac.za
Web: http://www.recycleupghana.org/
Office: +27 21 650 3164
Skype: quagraine_cwasi

------------
Try not to become a man of success but rather a man of value- Albert
Einstein

Kwesi A. Quagraine
Department of Physics
School of Physical Sciences
College of Agriculture and Natural Sciences
University of Cape Coast
Cape Coast, Ghana

Alt. Email: kwesi at csag.uct.ac.za
Web: http://www.recycleupghana.org/
Office: +27 21 650 3164
Skype: quagraine_cwasi

	[[alternative HTML version deleted]]


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Nov 28 06:46:45 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 28 Nov 2019 11:16:45 +0530
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
Message-ID: <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>

Dear Peter and John,

Many thanks for your prompt replies.

Here is what I was trying to do.  I was trying to build a statistical model
of a given time series using Box Jenkins methodology. The series has 93
data points. Before I analyse the ACF and PACF, I am required to de-trend
the series. The series seems to have an upward trend. I wanted to find out
what order polynomial should I fit the series
without overfitting.  For this I want to use orthogonal polynomials(I think
someone on the internet was talking about preventing overfitting by using
orthogonal polynomials) . This seems to me as a poor man's cross
validation.

So my plan is to keep increasing the degree of the orthogonal polynomials
till the coefficient of the last orthogonal polynomial becomes
insignificant.

Note : If I do NOT use orthogonal polynomials, I will overfit the data set
and I don't think that is a good way to detect the true order of the
polynomial.

Also now that I have detrended the series and built an ARIMA model of the
residuals, now I want to forecast. For this I need to use the original
polynomials and their coefficients.

I hope I was clear and that my methodology is ok.

I have another query here :-

Note : If I used cross-validation to determine the order of the polynomial,
I don't get a clear answer.

See here :-
library(boot)
mydf = data.frame(cbind(gdp,x))
d<-(c(
cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
print(d)
## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
## [6] 4.980159e+11

# Here it chooses 5. (but 4 and 5 are kind of similar).


d1 <- (c(
cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))

print(d1)
## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
## [6] 2.145754e+13

# here it chooses 1 or 6

Query : Why does it choose 1? Notice : Is this just round off noise / noise
due to sampling error created by Cross Validation when it creates the K
folds? Is this due to the ill conditioned model matrix?

Best Regards,
Ashim.





On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:

> Dear Ashim,
>
> Orthogonal polynomials are used because they tend to produce more accurate
> numerical computations, not because their coefficients are interpretable,
> so I wonder why you're interested in the coefficients.
>
> The regressors produced are orthogonal to the constant regressor and are
> orthogonal to each other (and in fact are orthonormal), as it's simple to
> demonstrate:
>
> ------- snip -------
>
> > x <- 1:93
> > y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> > (m <- lm(y ~ poly(x, 4)))
>
> Call:
> lm(formula = y ~ poly(x, 4))
>
> Coefficients:
> (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
>    15574516    172715069     94769949     27683528      3429259
>
> > X <- model.matrix(m)
> > head(X)
>   (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
> 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
> 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
> 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
> 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
> 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
>
> > zapsmall(crossprod(X))# X'X
>             (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> (Intercept)          93           0           0           0           0
> poly(x, 4)1           0           1           0           0           0
> poly(x, 4)2           0           0           1           0           0
> poly(x, 4)3           0           0           0           1           0
> poly(x, 4)4           0           0           0           0           1
>
> ------- snip -------
>
> If for some not immediately obvious reason you're interested in the
> regression coefficients, why not just use a "raw" polynomial:
>
> ------- snip -------
>
> > (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
>
> Call:
> lm(formula = y ~ poly(x, 4, raw = TRUE))
>
> Coefficients:
>             (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw = TRUE)2
> poly(x, 4, raw = TRUE)3
>                  1.5640                   0.8985                   1.0037
>                  1.0000
> poly(x, 4, raw = TRUE)4
>                  1.0000
>
> ------- snip -------
>
> These coefficients are simply interpretable but the model matrix is more
> poorly conditioned:
>
> ------- snip -------
>
> > head(X1)
>   (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
> raw = TRUE)3
> 1           1                       1                       1
>          1
> 2           1                       2                       4
>          8
> 3           1                       3                       9
>         27
> 4           1                       4                      16
>         64
> 5           1                       5                      25
>        125
> 6           1                       6                      36
>        216
>   poly(x, 4, raw = TRUE)4
> 1                       1
> 2                      16
> 3                      81
> 4                     256
> 5                     625
> 6                    1296
> > round(cor(X1[, -1]), 2)
>                         poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
> poly(x, 4, raw = TRUE)3
> poly(x, 4, raw = TRUE)1                    1.00                    0.97
>                 0.92
> poly(x, 4, raw = TRUE)2                    0.97                    1.00
>                 0.99
> poly(x, 4, raw = TRUE)3                    0.92                    0.99
>                 1.00
> poly(x, 4, raw = TRUE)4                    0.87                    0.96
>                 0.99
>                         poly(x, 4, raw = TRUE)4
> poly(x, 4, raw = TRUE)1                    0.87
> poly(x, 4, raw = TRUE)2                    0.96
> poly(x, 4, raw = TRUE)3                    0.99
> poly(x, 4, raw = TRUE)4                    1.00
>
> ------- snip -------
>
> The two parametrizations are equivalent, however, in that they represent
> the same regression surface, and so, e.g., produce the same fitted values:
>
> ------- snip -------
>
> > all.equal(fitted(m), fitted(m1))
> [1] TRUE
>
> ------- snip -------
>
> Because one is usually not interested in the individual coefficients of a
> polynomial there usually isn't a reason to prefer one parametrization to
> the other on the grounds of interpretability, so why do you need to
> interpret the regression equation?
>
> I hope this helps,
>  John
>
>   -----------------------------
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
>
> > On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> wrote:
> >
> > Dear Petr,
> >
> > Many thanks for the quick response.
> >
> > I also read this:-
> > https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
> >
> > Also I read  in ?poly:-
> >     The orthogonal polynomial is summarized by the coefficients, which
> >     can be used to evaluate it via the three-term recursion given in
> >     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
> >     of the code.
> >
> > I don't have access to the mentioned book.
> >
> > Out of curiosity, what is the name of the discrete orthogonal polynomial
> > used by R ?
> > What discrete measure is it orthogonal with respect to ?
> >
> > Many thanks,
> > Ashim
> >
> >
> >
> >
> > On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> >
> >> You could get answer quickly by searching net.
> >>
> >>
> >>
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
> >> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
> >> <
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
> >
> >>
> >> Cheers
> >> Petr
> >>
> >>> -----Original Message-----
> >>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim Kapoor
> >>> Sent: Wednesday, November 27, 2019 12:55 PM
> >>> To: R Help <r-help at r-project.org>
> >>> Subject: [R] Orthogonal polynomials used by R
> >>>
> >>> Dear All,
> >>>
> >>> I have created a time trend by doing x<-1:93 because I have a time
> series
> >>> with 93 data points. Next I did :-
> >>>
> >>> y = lm(series ~ poly(x,4))$residuals
> >>>
> >>> to detrend series.
> >>>
> >>> I choose this 4 as the order of my polynomial using cross validation/
> >>> checking the absence of trend in the residuals so I think I have not
> >> overfit
> >>> this series.
> >>>
> >>> I wish to document the formula of poly(x,4). I am not able to find it
> in
> >> ?poly
> >>>
> >>> Can someone please tell me what the formula for the orthogonal
> >>> polynomial used by R is ?
> >>>
> >>> Thank you,
> >>> Ashim
> >>>
> >>>      [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-
> >>> guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From bgunter@4567 @end|ng |rom gm@||@com  Thu Nov 28 07:12:54 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Wed, 27 Nov 2019 22:12:54 -0800
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
Message-ID: <CAGxFJbSM6WUGJT=UiNRQ2Aq6_J6opxCvOojoEn6T_e7RT+KZ9Q@mail.gmail.com>

Statistical questions are generally off topic on this list. Try
stats.stackexchange.com instead.

But FWIW, I recommend that you work with someone with expertise in time
series analysis, as your efforts to shake and bake your own methodology
seem rather unwise to me.

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Nov 27, 2019 at 9:47 PM Ashim Kapoor <ashimkapoor at gmail.com> wrote:

> Dear Peter and John,
>
> Many thanks for your prompt replies.
>
> Here is what I was trying to do.  I was trying to build a statistical model
> of a given time series using Box Jenkins methodology. The series has 93
> data points. Before I analyse the ACF and PACF, I am required to de-trend
> the series. The series seems to have an upward trend. I wanted to find out
> what order polynomial should I fit the series
> without overfitting.  For this I want to use orthogonal polynomials(I think
> someone on the internet was talking about preventing overfitting by using
> orthogonal polynomials) . This seems to me as a poor man's cross
> validation.
>
> So my plan is to keep increasing the degree of the orthogonal polynomials
> till the coefficient of the last orthogonal polynomial becomes
> insignificant.
>
> Note : If I do NOT use orthogonal polynomials, I will overfit the data set
> and I don't think that is a good way to detect the true order of the
> polynomial.
>
> Also now that I have detrended the series and built an ARIMA model of the
> residuals, now I want to forecast. For this I need to use the original
> polynomials and their coefficients.
>
> I hope I was clear and that my methodology is ok.
>
> I have another query here :-
>
> Note : If I used cross-validation to determine the order of the polynomial,
> I don't get a clear answer.
>
> See here :-
> library(boot)
> mydf = data.frame(cbind(gdp,x))
> d<-(c(
> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
> print(d)
> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
> ## [6] 4.980159e+11
>
> # Here it chooses 5. (but 4 and 5 are kind of similar).
>
>
> d1 <- (c(
> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
>
> print(d1)
> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
> ## [6] 2.145754e+13
>
> # here it chooses 1 or 6
>
> Query : Why does it choose 1? Notice : Is this just round off noise / noise
> due to sampling error created by Cross Validation when it creates the K
> folds? Is this due to the ill conditioned model matrix?
>
> Best Regards,
> Ashim.
>
>
>
>
>
> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
>
> > Dear Ashim,
> >
> > Orthogonal polynomials are used because they tend to produce more
> accurate
> > numerical computations, not because their coefficients are interpretable,
> > so I wonder why you're interested in the coefficients.
> >
> > The regressors produced are orthogonal to the constant regressor and are
> > orthogonal to each other (and in fact are orthonormal), as it's simple to
> > demonstrate:
> >
> > ------- snip -------
> >
> > > x <- 1:93
> > > y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> > > (m <- lm(y ~ poly(x, 4)))
> >
> > Call:
> > lm(formula = y ~ poly(x, 4))
> >
> > Coefficients:
> > (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
> >    15574516    172715069     94769949     27683528      3429259
> >
> > > X <- model.matrix(m)
> > > head(X)
> >   (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> > 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
> > 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
> > 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
> > 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
> > 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
> > 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
> >
> > > zapsmall(crossprod(X))# X'X
> >             (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> > (Intercept)          93           0           0           0           0
> > poly(x, 4)1           0           1           0           0           0
> > poly(x, 4)2           0           0           1           0           0
> > poly(x, 4)3           0           0           0           1           0
> > poly(x, 4)4           0           0           0           0           1
> >
> > ------- snip -------
> >
> > If for some not immediately obvious reason you're interested in the
> > regression coefficients, why not just use a "raw" polynomial:
> >
> > ------- snip -------
> >
> > > (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
> >
> > Call:
> > lm(formula = y ~ poly(x, 4, raw = TRUE))
> >
> > Coefficients:
> >             (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw = TRUE)2
> > poly(x, 4, raw = TRUE)3
> >                  1.5640                   0.8985                   1.0037
> >                  1.0000
> > poly(x, 4, raw = TRUE)4
> >                  1.0000
> >
> > ------- snip -------
> >
> > These coefficients are simply interpretable but the model matrix is more
> > poorly conditioned:
> >
> > ------- snip -------
> >
> > > head(X1)
> >   (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
> > raw = TRUE)3
> > 1           1                       1                       1
> >          1
> > 2           1                       2                       4
> >          8
> > 3           1                       3                       9
> >         27
> > 4           1                       4                      16
> >         64
> > 5           1                       5                      25
> >        125
> > 6           1                       6                      36
> >        216
> >   poly(x, 4, raw = TRUE)4
> > 1                       1
> > 2                      16
> > 3                      81
> > 4                     256
> > 5                     625
> > 6                    1296
> > > round(cor(X1[, -1]), 2)
> >                         poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
> > poly(x, 4, raw = TRUE)3
> > poly(x, 4, raw = TRUE)1                    1.00                    0.97
> >                 0.92
> > poly(x, 4, raw = TRUE)2                    0.97                    1.00
> >                 0.99
> > poly(x, 4, raw = TRUE)3                    0.92                    0.99
> >                 1.00
> > poly(x, 4, raw = TRUE)4                    0.87                    0.96
> >                 0.99
> >                         poly(x, 4, raw = TRUE)4
> > poly(x, 4, raw = TRUE)1                    0.87
> > poly(x, 4, raw = TRUE)2                    0.96
> > poly(x, 4, raw = TRUE)3                    0.99
> > poly(x, 4, raw = TRUE)4                    1.00
> >
> > ------- snip -------
> >
> > The two parametrizations are equivalent, however, in that they represent
> > the same regression surface, and so, e.g., produce the same fitted
> values:
> >
> > ------- snip -------
> >
> > > all.equal(fitted(m), fitted(m1))
> > [1] TRUE
> >
> > ------- snip -------
> >
> > Because one is usually not interested in the individual coefficients of a
> > polynomial there usually isn't a reason to prefer one parametrization to
> > the other on the grounds of interpretability, so why do you need to
> > interpret the regression equation?
> >
> > I hope this helps,
> >  John
> >
> >   -----------------------------
> >   John Fox, Professor Emeritus
> >   McMaster University
> >   Hamilton, Ontario, Canada
> >   Web: http::/socserv.mcmaster.ca/jfox
> >
> > > On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> > wrote:
> > >
> > > Dear Petr,
> > >
> > > Many thanks for the quick response.
> > >
> > > I also read this:-
> > > https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
> > >
> > > Also I read  in ?poly:-
> > >     The orthogonal polynomial is summarized by the coefficients, which
> > >     can be used to evaluate it via the three-term recursion given in
> > >     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
> > >     of the code.
> > >
> > > I don't have access to the mentioned book.
> > >
> > > Out of curiosity, what is the name of the discrete orthogonal
> polynomial
> > > used by R ?
> > > What discrete measure is it orthogonal with respect to ?
> > >
> > > Many thanks,
> > > Ashim
> > >
> > >
> > >
> > >
> > > On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
> > wrote:
> > >
> > >> You could get answer quickly by searching net.
> > >>
> > >>
> > >>
> >
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
> > >> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
> > >> <
> >
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
> > >
> > >>
> > >> Cheers
> > >> Petr
> > >>
> > >>> -----Original Message-----
> > >>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
> Kapoor
> > >>> Sent: Wednesday, November 27, 2019 12:55 PM
> > >>> To: R Help <r-help at r-project.org>
> > >>> Subject: [R] Orthogonal polynomials used by R
> > >>>
> > >>> Dear All,
> > >>>
> > >>> I have created a time trend by doing x<-1:93 because I have a time
> > series
> > >>> with 93 data points. Next I did :-
> > >>>
> > >>> y = lm(series ~ poly(x,4))$residuals
> > >>>
> > >>> to detrend series.
> > >>>
> > >>> I choose this 4 as the order of my polynomial using cross validation/
> > >>> checking the absence of trend in the residuals so I think I have not
> > >> overfit
> > >>> this series.
> > >>>
> > >>> I wish to document the formula of poly(x,4). I am not able to find it
> > in
> > >> ?poly
> > >>>
> > >>> Can someone please tell me what the formula for the orthogonal
> > >>> polynomial used by R is ?
> > >>>
> > >>> Thank you,
> > >>> Ashim
> > >>>
> > >>>      [[alternative HTML version deleted]]
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-
> > >>> guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @@h|mk@poor @end|ng |rom gm@||@com  Thu Nov 28 07:16:42 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 28 Nov 2019 11:46:42 +0530
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <CAGxFJbSM6WUGJT=UiNRQ2Aq6_J6opxCvOojoEn6T_e7RT+KZ9Q@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
 <CAGxFJbSM6WUGJT=UiNRQ2Aq6_J6opxCvOojoEn6T_e7RT+KZ9Q@mail.gmail.com>
Message-ID: <CAC8=1ervdNf4TpEbXJ5Z30-tWK8nEMXsdmTPXmRRWs5zdMbf8A@mail.gmail.com>

Dear Bert,

OK and thank you.

@Fox, John <jfox at mcmaster.ca> will be grateful for an offline reply.

Best,
Ashim

On Thu, Nov 28, 2019 at 11:43 AM Bert Gunter <bgunter.4567 at gmail.com> wrote:

> Statistical questions are generally off topic on this list. Try
> stats.stackexchange.com instead.
>
> But FWIW, I recommend that you work with someone with expertise in time
> series analysis, as your efforts to shake and bake your own methodology
> seem rather unwise to me.
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and
> sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Nov 27, 2019 at 9:47 PM Ashim Kapoor <ashimkapoor at gmail.com>
> wrote:
>
>> Dear Peter and John,
>>
>> Many thanks for your prompt replies.
>>
>> Here is what I was trying to do.  I was trying to build a statistical
>> model
>> of a given time series using Box Jenkins methodology. The series has 93
>> data points. Before I analyse the ACF and PACF, I am required to de-trend
>> the series. The series seems to have an upward trend. I wanted to find out
>> what order polynomial should I fit the series
>> without overfitting.  For this I want to use orthogonal polynomials(I
>> think
>> someone on the internet was talking about preventing overfitting by using
>> orthogonal polynomials) . This seems to me as a poor man's cross
>> validation.
>>
>> So my plan is to keep increasing the degree of the orthogonal polynomials
>> till the coefficient of the last orthogonal polynomial becomes
>> insignificant.
>>
>> Note : If I do NOT use orthogonal polynomials, I will overfit the data set
>> and I don't think that is a good way to detect the true order of the
>> polynomial.
>>
>> Also now that I have detrended the series and built an ARIMA model of the
>> residuals, now I want to forecast. For this I need to use the original
>> polynomials and their coefficients.
>>
>> I hope I was clear and that my methodology is ok.
>>
>> I have another query here :-
>>
>> Note : If I used cross-validation to determine the order of the
>> polynomial,
>> I don't get a clear answer.
>>
>> See here :-
>> library(boot)
>> mydf = data.frame(cbind(gdp,x))
>> d<-(c(
>> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
>> print(d)
>> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
>> ## [6] 4.980159e+11
>>
>> # Here it chooses 5. (but 4 and 5 are kind of similar).
>>
>>
>> d1 <- (c(
>> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
>>
>> print(d1)
>> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
>> ## [6] 2.145754e+13
>>
>> # here it chooses 1 or 6
>>
>> Query : Why does it choose 1? Notice : Is this just round off noise /
>> noise
>> due to sampling error created by Cross Validation when it creates the K
>> folds? Is this due to the ill conditioned model matrix?
>>
>> Best Regards,
>> Ashim.
>>
>>
>>
>>
>>
>> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
>>
>> > Dear Ashim,
>> >
>> > Orthogonal polynomials are used because they tend to produce more
>> accurate
>> > numerical computations, not because their coefficients are
>> interpretable,
>> > so I wonder why you're interested in the coefficients.
>> >
>> > The regressors produced are orthogonal to the constant regressor and are
>> > orthogonal to each other (and in fact are orthonormal), as it's simple
>> to
>> > demonstrate:
>> >
>> > ------- snip -------
>> >
>> > > x <- 1:93
>> > > y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>> > > (m <- lm(y ~ poly(x, 4)))
>> >
>> > Call:
>> > lm(formula = y ~ poly(x, 4))
>> >
>> > Coefficients:
>> > (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
>> >    15574516    172715069     94769949     27683528      3429259
>> >
>> > > X <- model.matrix(m)
>> > > head(X)
>> >   (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>> > 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
>> > 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
>> > 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
>> > 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
>> > 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
>> > 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
>> >
>> > > zapsmall(crossprod(X))# X'X
>> >             (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>> > (Intercept)          93           0           0           0           0
>> > poly(x, 4)1           0           1           0           0           0
>> > poly(x, 4)2           0           0           1           0           0
>> > poly(x, 4)3           0           0           0           1           0
>> > poly(x, 4)4           0           0           0           0           1
>> >
>> > ------- snip -------
>> >
>> > If for some not immediately obvious reason you're interested in the
>> > regression coefficients, why not just use a "raw" polynomial:
>> >
>> > ------- snip -------
>> >
>> > > (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
>> >
>> > Call:
>> > lm(formula = y ~ poly(x, 4, raw = TRUE))
>> >
>> > Coefficients:
>> >             (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw =
>> TRUE)2
>> > poly(x, 4, raw = TRUE)3
>> >                  1.5640                   0.8985
>>  1.0037
>> >                  1.0000
>> > poly(x, 4, raw = TRUE)4
>> >                  1.0000
>> >
>> > ------- snip -------
>> >
>> > These coefficients are simply interpretable but the model matrix is more
>> > poorly conditioned:
>> >
>> > ------- snip -------
>> >
>> > > head(X1)
>> >   (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
>> > raw = TRUE)3
>> > 1           1                       1                       1
>> >          1
>> > 2           1                       2                       4
>> >          8
>> > 3           1                       3                       9
>> >         27
>> > 4           1                       4                      16
>> >         64
>> > 5           1                       5                      25
>> >        125
>> > 6           1                       6                      36
>> >        216
>> >   poly(x, 4, raw = TRUE)4
>> > 1                       1
>> > 2                      16
>> > 3                      81
>> > 4                     256
>> > 5                     625
>> > 6                    1296
>> > > round(cor(X1[, -1]), 2)
>> >                         poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
>> > poly(x, 4, raw = TRUE)3
>> > poly(x, 4, raw = TRUE)1                    1.00                    0.97
>> >                 0.92
>> > poly(x, 4, raw = TRUE)2                    0.97                    1.00
>> >                 0.99
>> > poly(x, 4, raw = TRUE)3                    0.92                    0.99
>> >                 1.00
>> > poly(x, 4, raw = TRUE)4                    0.87                    0.96
>> >                 0.99
>> >                         poly(x, 4, raw = TRUE)4
>> > poly(x, 4, raw = TRUE)1                    0.87
>> > poly(x, 4, raw = TRUE)2                    0.96
>> > poly(x, 4, raw = TRUE)3                    0.99
>> > poly(x, 4, raw = TRUE)4                    1.00
>> >
>> > ------- snip -------
>> >
>> > The two parametrizations are equivalent, however, in that they represent
>> > the same regression surface, and so, e.g., produce the same fitted
>> values:
>> >
>> > ------- snip -------
>> >
>> > > all.equal(fitted(m), fitted(m1))
>> > [1] TRUE
>> >
>> > ------- snip -------
>> >
>> > Because one is usually not interested in the individual coefficients of
>> a
>> > polynomial there usually isn't a reason to prefer one parametrization to
>> > the other on the grounds of interpretability, so why do you need to
>> > interpret the regression equation?
>> >
>> > I hope this helps,
>> >  John
>> >
>> >   -----------------------------
>> >   John Fox, Professor Emeritus
>> >   McMaster University
>> >   Hamilton, Ontario, Canada
>> >   Web: http::/socserv.mcmaster.ca/jfox
>> >
>> > > On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>> > wrote:
>> > >
>> > > Dear Petr,
>> > >
>> > > Many thanks for the quick response.
>> > >
>> > > I also read this:-
>> > > https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
>> > >
>> > > Also I read  in ?poly:-
>> > >     The orthogonal polynomial is summarized by the coefficients, which
>> > >     can be used to evaluate it via the three-term recursion given in
>> > >     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
>> > >     of the code.
>> > >
>> > > I don't have access to the mentioned book.
>> > >
>> > > Out of curiosity, what is the name of the discrete orthogonal
>> polynomial
>> > > used by R ?
>> > > What discrete measure is it orthogonal with respect to ?
>> > >
>> > > Many thanks,
>> > > Ashim
>> > >
>> > >
>> > >
>> > >
>> > > On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
>> > wrote:
>> > >
>> > >> You could get answer quickly by searching net.
>> > >>
>> > >>
>> > >>
>> >
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
>> > >> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
>> > >> <
>> >
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
>> > >
>> > >>
>> > >> Cheers
>> > >> Petr
>> > >>
>> > >>> -----Original Message-----
>> > >>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
>> Kapoor
>> > >>> Sent: Wednesday, November 27, 2019 12:55 PM
>> > >>> To: R Help <r-help at r-project.org>
>> > >>> Subject: [R] Orthogonal polynomials used by R
>> > >>>
>> > >>> Dear All,
>> > >>>
>> > >>> I have created a time trend by doing x<-1:93 because I have a time
>> > series
>> > >>> with 93 data points. Next I did :-
>> > >>>
>> > >>> y = lm(series ~ poly(x,4))$residuals
>> > >>>
>> > >>> to detrend series.
>> > >>>
>> > >>> I choose this 4 as the order of my polynomial using cross
>> validation/
>> > >>> checking the absence of trend in the residuals so I think I have not
>> > >> overfit
>> > >>> this series.
>> > >>>
>> > >>> I wish to document the formula of poly(x,4). I am not able to find
>> it
>> > in
>> > >> ?poly
>> > >>>
>> > >>> Can someone please tell me what the formula for the orthogonal
>> > >>> polynomial used by R is ?
>> > >>>
>> > >>> Thank you,
>> > >>> Ashim
>> > >>>
>> > >>>      [[alternative HTML version deleted]]
>> > >>>
>> > >>> ______________________________________________
>> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
>> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> > >>> guide.html
>> > >>> and provide commented, minimal, self-contained, reproducible code.
>> > >>
>> > >
>> > >       [[alternative HTML version deleted]]
>> > >
>> > > ______________________________________________
>> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > > https://stat.ethz.ch/mailman/listinfo/r-help
>> > > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>

	[[alternative HTML version deleted]]


From j|ox @end|ng |rom mcm@@ter@c@  Thu Nov 28 15:08:15 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Thu, 28 Nov 2019 14:08:15 +0000
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
Message-ID: <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>

Dear Ashim,

I'm afraid that much of what you say here is confused.

First, because poly(x) and poly(x, raw=TRUE) produce the same fitted values (as I previously explained), they also produce the same residuals, and consequently the same CV criteria. From the point of view of CV, there's therefore no reason to prefer orthogonal polynomials. And you still don't explain why you want to interpret the coefficients of the polynomial.

Second, the model formula gdp~1+x+x^2 and other similar formulas in your message don't do what you think. Like + and *, the ^ operator has special meaning on the right-hand side of an R model formula. See ?Formula and perhaps read something about statistical models in R. For example:

> x <- 1:93
> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> (m <- lm(y ~ x + x^2))

Call:
lm(formula = y ~ x + x^2)

Coefficients:
(Intercept)            x  
  -15781393       667147  

While gpp ~ x + I(x^2) would work, a better way to fit a raw quadratic is as gdp ~ poly(x, 2, raw=TRUE), as I suggested in my earlier message.

Finally, as to what you should do, I generally try to avoid statistical consulting by email. If you can find competent statistical help locally, such as at a nearby university, I'd recommend talking to someone about the purpose of your research and the nature of your data. If that's not possible, then others have suggested where you might find help, but to get useful advice you'll have to provide much more information about your research.

Best,
 John

  -----------------------------
  John Fox, Professor Emeritus
  McMaster University
  Hamilton, Ontario, Canada
  Web: http::/socserv.mcmaster.ca/jfox

> On Nov 28, 2019, at 12:46 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> 
> Dear Peter and John,
> 
> Many thanks for your prompt replies. 
> 
> Here is what I was trying to do.  I was trying to build a statistical model of a given time series using Box Jenkins methodology. The series has 93 data points. Before I analyse the ACF and PACF, I am required to de-trend the series. The series seems to have an upward trend. I wanted to find out what order polynomial should I fit the series 
> without overfitting.  For this I want to use orthogonal polynomials(I think someone on the internet was talking about preventing overfitting by using orthogonal polynomials) . This seems to me as a poor man's cross validation. 
> 
> So my plan is to keep increasing the degree of the orthogonal polynomials till the coefficient of the last orthogonal polynomial becomes insignificant.
> 
> Note : If I do NOT use orthogonal polynomials, I will overfit the data set and I don't think that is a good way to detect the true order of the polynomial.
> 
> Also now that I have detrended the series and built an ARIMA model of the residuals, now I want to forecast. For this I need to use the original polynomials and their coefficients.
> 
> I hope I was clear and that my methodology is ok.
> 
> I have another query here :-
> 
> Note : If I used cross-validation to determine the order of the polynomial, I don't get a clear answer.
> 
> See here :-
> library(boot)
> mydf = data.frame(cbind(gdp,x))
> d<-(c(
> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
> print(d)
> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
> ## [6] 4.980159e+11
> 
> # Here it chooses 5. (but 4 and 5 are kind of similar).
> 
> 
> d1 <- (c(
> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
> 
> print(d1)
> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
> ## [6] 2.145754e+13
> 
> # here it chooses 1 or 6
> 
> Query : Why does it choose 1? Notice : Is this just round off noise / noise due to sampling error created by Cross Validation when it creates the K folds? Is this due to the ill conditioned model matrix?
> 
> Best Regards,
> Ashim.
> 
> 
> 
> 
> 
> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
> Dear Ashim,
> 
> Orthogonal polynomials are used because they tend to produce more accurate numerical computations, not because their coefficients are interpretable, so I wonder why you're interested in the coefficients. 
> 
> The regressors produced are orthogonal to the constant regressor and are orthogonal to each other (and in fact are orthonormal), as it's simple to demonstrate:
> 
> ------- snip -------
> 
> > x <- 1:93
> > y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> > (m <- lm(y ~ poly(x, 4)))
> 
> Call:
> lm(formula = y ~ poly(x, 4))
> 
> Coefficients:
> (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4  
>    15574516    172715069     94769949     27683528      3429259  
> 
> > X <- model.matrix(m)
> > head(X)
>   (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
> 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
> 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
> 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
> 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
> 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
> 
> > zapsmall(crossprod(X))# X'X
>             (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> (Intercept)          93           0           0           0           0
> poly(x, 4)1           0           1           0           0           0
> poly(x, 4)2           0           0           1           0           0
> poly(x, 4)3           0           0           0           1           0
> poly(x, 4)4           0           0           0           0           1
> 
> ------- snip -------
> 
> If for some not immediately obvious reason you're interested in the regression coefficients, why not just use a "raw" polynomial:
> 
> ------- snip -------
> 
> > (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
> 
> Call:
> lm(formula = y ~ poly(x, 4, raw = TRUE))
> 
> Coefficients:
>             (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw = TRUE)2  poly(x, 4, raw = TRUE)3  
>                  1.5640                   0.8985                   1.0037                   1.0000  
> poly(x, 4, raw = TRUE)4  
>                  1.0000  
> 
> ------- snip -------
> 
> These coefficients are simply interpretable but the model matrix is more poorly conditioned:
> 
> ------- snip -------
> 
> > head(X1)
>   (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4, raw = TRUE)3
> 1           1                       1                       1                       1
> 2           1                       2                       4                       8
> 3           1                       3                       9                      27
> 4           1                       4                      16                      64
> 5           1                       5                      25                     125
> 6           1                       6                      36                     216
>   poly(x, 4, raw = TRUE)4
> 1                       1
> 2                      16
> 3                      81
> 4                     256
> 5                     625
> 6                    1296
> > round(cor(X1[, -1]), 2)
>                         poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4, raw = TRUE)3
> poly(x, 4, raw = TRUE)1                    1.00                    0.97                    0.92
> poly(x, 4, raw = TRUE)2                    0.97                    1.00                    0.99
> poly(x, 4, raw = TRUE)3                    0.92                    0.99                    1.00
> poly(x, 4, raw = TRUE)4                    0.87                    0.96                    0.99
>                         poly(x, 4, raw = TRUE)4
> poly(x, 4, raw = TRUE)1                    0.87
> poly(x, 4, raw = TRUE)2                    0.96
> poly(x, 4, raw = TRUE)3                    0.99
> poly(x, 4, raw = TRUE)4                    1.00
> 
> ------- snip -------
> 
> The two parametrizations are equivalent, however, in that they represent the same regression surface, and so, e.g., produce the same fitted values:
> 
> ------- snip -------
> 
> > all.equal(fitted(m), fitted(m1))
> [1] TRUE
> 
> ------- snip -------
> 
> Because one is usually not interested in the individual coefficients of a polynomial there usually isn't a reason to prefer one parametrization to the other on the grounds of interpretability, so why do you need to interpret the regression equation?
> 
> I hope this helps,
>  John
> 
>   ----------------------------- 
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
> 
> > On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> > 
> > Dear Petr,
> > 
> > Many thanks for the quick response.
> > 
> > I also read this:-
> > https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
> > 
> > Also I read  in ?poly:-
> >     The orthogonal polynomial is summarized by the coefficients, which
> >     can be used to evaluate it via the three-term recursion given in
> >     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
> >     of the code.
> > 
> > I don't have access to the mentioned book.
> > 
> > Out of curiosity, what is the name of the discrete orthogonal polynomial
> > used by R ?
> > What discrete measure is it orthogonal with respect to ?
> > 
> > Many thanks,
> > Ashim
> > 
> > 
> > 
> > 
> > On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz> wrote:
> > 
> >> You could get answer quickly by searching net.
> >> 
> >> 
> >> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
> >> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
> >> <https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154>
> >> 
> >> Cheers
> >> Petr
> >> 
> >>> -----Original Message-----
> >>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim Kapoor
> >>> Sent: Wednesday, November 27, 2019 12:55 PM
> >>> To: R Help <r-help at r-project.org>
> >>> Subject: [R] Orthogonal polynomials used by R
> >>> 
> >>> Dear All,
> >>> 
> >>> I have created a time trend by doing x<-1:93 because I have a time series
> >>> with 93 data points. Next I did :-
> >>> 
> >>> y = lm(series ~ poly(x,4))$residuals
> >>> 
> >>> to detrend series.
> >>> 
> >>> I choose this 4 as the order of my polynomial using cross validation/
> >>> checking the absence of trend in the residuals so I think I have not
> >> overfit
> >>> this series.
> >>> 
> >>> I wish to document the formula of poly(x,4). I am not able to find it in
> >> ?poly
> >>> 
> >>> Can someone please tell me what the formula for the orthogonal
> >>> polynomial used by R is ?
> >>> 
> >>> Thank you,
> >>> Ashim
> >>> 
> >>>      [[alternative HTML version deleted]]
> >>> 
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide http://www.R-project.org/posting-
> >>> guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >> 
> > 
> >       [[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 



From @@h|mk@poor @end|ng |rom gm@||@com  Thu Nov 28 17:02:45 2019
From: @@h|mk@poor @end|ng |rom gm@||@com (Ashim Kapoor)
Date: Thu, 28 Nov 2019 21:32:45 +0530
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
 <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>
Message-ID: <CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>

On Thu, Nov 28, 2019 at 7:38 PM Fox, John <jfox at mcmaster.ca> wrote:

> Dear Ashim,
>
> I'm afraid that much of what you say here is confused.
>
> First, because poly(x) and poly(x, raw=TRUE) produce the same fitted
> values (as I previously explained), they also produce the same residuals,
> and consequently the same CV criteria. From the point of view of CV,
> there's therefore no reason to prefer orthogonal polynomials. And you still
> don't explain why you want to interpret the coefficients of the polynomial.
>

The trend in the variable that I am trying to create an ARIMA model for is
given by poly(x,4). That is why I wished to know what these polynomials
look like.

I used  :

trend <- predict(lm(gdp~poly(x,4)),newdata = data.frame(
x=94:103),interval="confidence")

and I was able to (numerically) extrapolate the poly(x,4) trend, although,
I think it would be interesting to know what polynomials I was dealing with
in this case. Just some intuition as to if the linear / quadratic / cubic /
fourth order polynomial trend is dominating. I don't know how I would
interpret them, but it would be fun to know.

Please allow me to show you a trick. I read this on the internet, here :-

https://www.datasciencecentral.com/profiles/blogs/deep-dive-into-polynomial-regression-and-overfitting

Please see the LAST comment by Scott Stelljes where he suggests using an
orthogonal polynomial basis. He does not elaborate but leaves the reader to
work out the details.

Here is what I think of this. Take a big number say 20 and take a variable
in which we are trying to find the order of the polynomial in the trend.
Like this :-

> summary(lm(gdp ~ poly(x,20)))

Call:
lm(formula = gdp ~ poly(x, 20))

Residuals:
     Min       1Q   Median       3Q      Max
-1235661  -367798   -80453   240360  1450906

Coefficients:
               Estimate Std. Error t value Pr(>|t|)
(Intercept)    17601482      66934 262.968  < 2e-16 ***
poly(x, 20)1  125679081     645487 194.704  < 2e-16 ***
poly(x, 20)2   43108747     645487  66.785  < 2e-16 ***
poly(x, 20)3    3605839     645487   5.586 3.89e-07 ***
poly(x, 20)4   -2977277     645487  -4.612 1.69e-05 ***
poly(x, 20)5    1085732     645487   1.682   0.0969 .
poly(x, 20)6    1124125     645487   1.742   0.0859 .
poly(x, 20)7    -108676     645487  -0.168   0.8668
poly(x, 20)8    -976915     645487  -1.513   0.1345
poly(x, 20)9   -1635444     645487  -2.534   0.0135 *
poly(x, 20)10   -715019     645487  -1.108   0.2717
poly(x, 20)11    347102     645487   0.538   0.5924
poly(x, 20)12   -176728     645487  -0.274   0.7850
poly(x, 20)13   -634151     645487  -0.982   0.3292
poly(x, 20)14   -537725     645487  -0.833   0.4076
poly(x, 20)15    -58674     645487  -0.091   0.9278
poly(x, 20)16    -67030     645487  -0.104   0.9176
poly(x, 20)17   -809443     645487  -1.254   0.2139
poly(x, 20)18   -668879     645487  -1.036   0.3036
poly(x, 20)19   -302384     645487  -0.468   0.6409
poly(x, 20)20    359134     645487   0.556   0.5797
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 645500 on 72 degrees of freedom
Multiple R-squared:  0.9983, Adjusted R-squared:  0.9978
F-statistic:  2122 on 20 and 72 DF,  p-value: < 2.2e-16

>


The CV estimate of the trend is 4. I am not saying this method is perfect,
but look above this method also suggests n=4.

I CANNOT do this with raw polynomials, since they are correlated and
JOINTLY in the presence of others they may not be significant, please see
below.

> summary(lm(gdp ~ poly(x,20,raw=T)))

Call:
lm(formula = gdp ~ poly(x, 20, raw = T))

Residuals:
     Min       1Q   Median       3Q      Max
-1286007  -372221   -81320   248510  1589130

Coefficients: (4 not defined because of singularities)
                         Estimate Std. Error t value Pr(>|t|)
(Intercept)             2.067e+06  2.649e+06   0.780    0.438
poly(x, 20, raw = T)1   1.633e+06  3.556e+06   0.459    0.647
poly(x, 20, raw = T)2  -7.601e+05  1.679e+06  -0.453    0.652
poly(x, 20, raw = T)3   1.861e+05  3.962e+05   0.470    0.640
poly(x, 20, raw = T)4  -2.634e+04  5.480e+04  -0.481    0.632
poly(x, 20, raw = T)5   2.370e+03  4.854e+03   0.488    0.627
poly(x, 20, raw = T)6  -1.434e+02  2.906e+02  -0.493    0.623
poly(x, 20, raw = T)7   6.022e+00  1.213e+01   0.496    0.621
poly(x, 20, raw = T)8  -1.784e-01  3.587e-01  -0.497    0.620
poly(x, 20, raw = T)9   3.727e-03  7.503e-03   0.497    0.621
poly(x, 20, raw = T)10 -5.373e-05  1.086e-04  -0.495    0.622
poly(x, 20, raw = T)11  5.016e-07  1.018e-06   0.493    0.624
poly(x, 20, raw = T)12 -2.483e-09  5.069e-09  -0.490    0.626
poly(x, 20, raw = T)13         NA         NA      NA       NA
poly(x, 20, raw = T)14  5.656e-14  1.167e-13   0.485    0.629
poly(x, 20, raw = T)15         NA         NA      NA       NA
poly(x, 20, raw = T)16 -1.933e-18  4.011e-18  -0.482    0.631
poly(x, 20, raw = T)17         NA         NA      NA       NA
poly(x, 20, raw = T)18  5.181e-23  1.076e-22   0.482    0.631
poly(x, 20, raw = T)19         NA         NA      NA       NA
poly(x, 20, raw = T)20 -7.173e-28  1.480e-27  -0.485    0.629

Residual standard error: 641000 on 76 degrees of freedom
Multiple R-squared:  0.9982, Adjusted R-squared:  0.9979
F-statistic:  2690 on 16 and 76 DF,  p-value: < 2.2e-16

>

Note,however, once the orthogonal polynomials have suggested a number, 4 in
this case, I can do this :-

 summary(lm(gdp ~ poly(x,4,raw=T)))

Call:
lm(formula = gdp ~ poly(x, 4, raw = T))

Residuals:
     Min       1Q   Median       3Q      Max
-1278673  -424315   -22357   310977  1731813

Coefficients:
                       Estimate Std. Error t value Pr(>|t|)
(Intercept)           3.022e+06  3.676e+05   8.220 1.64e-12 ***
poly(x, 4, raw = T)1  1.741e+05  5.357e+04   3.249  0.00164 **
poly(x, 4, raw = T)2 -6.434e+03  2.300e+03  -2.797  0.00633 **
poly(x, 4, raw = T)3  1.878e+02  3.667e+01   5.123 1.76e-06 ***
poly(x, 4, raw = T)4 -8.682e-01  1.935e-01  -4.486 2.19e-05 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Residual standard error: 663700 on 88 degrees of freedom
Multiple R-squared:  0.9978, Adjusted R-squared:  0.9977
F-statistic: 1.003e+04 on 4 and 88 DF,  p-value: < 2.2e-16

>

Although due to correlations they may not be significant jointly, but in
this case all 4 powers come out significant.


Second, the model formula gdp~1+x+x^2 and other similar formulas in your
> message don't do what you think. Like + and *, the ^ operator has special
> meaning on the right-hand side of an R model formula. See ?Formula and
> perhaps read something about statistical models in R. For example:
>
> > x <- 1:93
> > y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> > (m <- lm(y ~ x + x^2))
>
> Call:
> lm(formula = y ~ x + x^2)
>
> Coefficients:
> (Intercept)            x
>   -15781393       667147
>
> While gpp ~ x + I(x^2) would work, a better way to fit a raw quadratic is
> as gdp ~ poly(x, 2, raw=TRUE), as I suggested in my earlier message.
>

My bad. Yes, I have some idea of the Wilkinson-Rogers notation. I have seen
it in books, although it slipped my mind that I had to use I( ).


> Finally, as to what you should do, I generally try to avoid statistical
> consulting by email. If you can find competent statistical help locally,
> such as at a nearby university, I'd recommend talking to someone about the
> purpose of your research and the nature of your data. If that's not
> possible, then others have suggested where you might find help, but to get
> useful advice you'll have to provide much more information about your
> research.
>

My original query was about the polynomials used by R which I think is ON
topic. My apologies that this query turned into a statistics query.


> Best,
>  John
>
>   -----------------------------
>   John Fox, Professor Emeritus
>   McMaster University
>   Hamilton, Ontario, Canada
>   Web: http::/socserv.mcmaster.ca/jfox
>
> > On Nov 28, 2019, at 12:46 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> wrote:
> >
> > Dear Peter and John,
> >
> > Many thanks for your prompt replies.
> >
> > Here is what I was trying to do.  I was trying to build a statistical
> model of a given time series using Box Jenkins methodology. The series has
> 93 data points. Before I analyse the ACF and PACF, I am required to
> de-trend the series. The series seems to have an upward trend. I wanted to
> find out what order polynomial should I fit the series
> > without overfitting.  For this I want to use orthogonal polynomials(I
> think someone on the internet was talking about preventing overfitting by
> using orthogonal polynomials) . This seems to me as a poor man's cross
> validation.
> >
> > So my plan is to keep increasing the degree of the orthogonal
> polynomials till the coefficient of the last orthogonal polynomial becomes
> insignificant.
> >
> > Note : If I do NOT use orthogonal polynomials, I will overfit the data
> set and I don't think that is a good way to detect the true order of the
> polynomial.
> >
> > Also now that I have detrended the series and built an ARIMA model of
> the residuals, now I want to forecast. For this I need to use the original
> polynomials and their coefficients.
> >
> > I hope I was clear and that my methodology is ok.
> >
> > I have another query here :-
> >
> > Note : If I used cross-validation to determine the order of the
> polynomial, I don't get a clear answer.
> >
> > See here :-
> > library(boot)
> > mydf = data.frame(cbind(gdp,x))
> > d<-(c(
> > cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
> > print(d)
> > ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
> > ## [6] 4.980159e+11
> >
> > # Here it chooses 5. (but 4 and 5 are kind of similar).
> >
> >
> > d1 <- (c(
> > cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
> > cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
> >
> > print(d1)
> > ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
> > ## [6] 2.145754e+13
> >
> > # here it chooses 1 or 6
> >
> > Query : Why does it choose 1? Notice : Is this just round off noise /
> noise due to sampling error created by Cross Validation when it creates the
> K folds? Is this due to the ill conditioned model matrix?
> >
> > Best Regards,
> > Ashim.
> >
> >
> >
> >
> >
> > On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
> > Dear Ashim,
> >
> > Orthogonal polynomials are used because they tend to produce more
> accurate numerical computations, not because their coefficients are
> interpretable, so I wonder why you're interested in the coefficients.
> >
> > The regressors produced are orthogonal to the constant regressor and are
> orthogonal to each other (and in fact are orthonormal), as it's simple to
> demonstrate:
> >
> > ------- snip -------
> >
> > > x <- 1:93
> > > y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> > > (m <- lm(y ~ poly(x, 4)))
> >
> > Call:
> > lm(formula = y ~ poly(x, 4))
> >
> > Coefficients:
> > (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
> >    15574516    172715069     94769949     27683528      3429259
> >
> > > X <- model.matrix(m)
> > > head(X)
> >   (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> > 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
> > 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
> > 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
> > 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
> > 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
> > 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
> >
> > > zapsmall(crossprod(X))# X'X
> >             (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> > (Intercept)          93           0           0           0           0
> > poly(x, 4)1           0           1           0           0           0
> > poly(x, 4)2           0           0           1           0           0
> > poly(x, 4)3           0           0           0           1           0
> > poly(x, 4)4           0           0           0           0           1
> >
> > ------- snip -------
> >
> > If for some not immediately obvious reason you're interested in the
> regression coefficients, why not just use a "raw" polynomial:
> >
> > ------- snip -------
> >
> > > (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
> >
> > Call:
> > lm(formula = y ~ poly(x, 4, raw = TRUE))
> >
> > Coefficients:
> >             (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw =
> TRUE)2  poly(x, 4, raw = TRUE)3
> >                  1.5640                   0.8985
>  1.0037                   1.0000
> > poly(x, 4, raw = TRUE)4
> >                  1.0000
> >
> > ------- snip -------
> >
> > These coefficients are simply interpretable but the model matrix is more
> poorly conditioned:
> >
> > ------- snip -------
> >
> > > head(X1)
> >   (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
> raw = TRUE)3
> > 1           1                       1                       1
>            1
> > 2           1                       2                       4
>            8
> > 3           1                       3                       9
>           27
> > 4           1                       4                      16
>           64
> > 5           1                       5                      25
>          125
> > 6           1                       6                      36
>          216
> >   poly(x, 4, raw = TRUE)4
> > 1                       1
> > 2                      16
> > 3                      81
> > 4                     256
> > 5                     625
> > 6                    1296
> > > round(cor(X1[, -1]), 2)
> >                         poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
> poly(x, 4, raw = TRUE)3
> > poly(x, 4, raw = TRUE)1                    1.00                    0.97
>                   0.92
> > poly(x, 4, raw = TRUE)2                    0.97                    1.00
>                   0.99
> > poly(x, 4, raw = TRUE)3                    0.92                    0.99
>                   1.00
> > poly(x, 4, raw = TRUE)4                    0.87                    0.96
>                   0.99
> >                         poly(x, 4, raw = TRUE)4
> > poly(x, 4, raw = TRUE)1                    0.87
> > poly(x, 4, raw = TRUE)2                    0.96
> > poly(x, 4, raw = TRUE)3                    0.99
> > poly(x, 4, raw = TRUE)4                    1.00
> >
> > ------- snip -------
> >
> > The two parametrizations are equivalent, however, in that they represent
> the same regression surface, and so, e.g., produce the same fitted values:
> >
> > ------- snip -------
> >
> > > all.equal(fitted(m), fitted(m1))
> > [1] TRUE
> >
> > ------- snip -------
> >
> > Because one is usually not interested in the individual coefficients of
> a polynomial there usually isn't a reason to prefer one parametrization to
> the other on the grounds of interpretability, so why do you need to
> interpret the regression equation?
> >
> > I hope this helps,
> >  John
> >
> >   -----------------------------
> >   John Fox, Professor Emeritus
> >   McMaster University
> >   Hamilton, Ontario, Canada
> >   Web: http::/socserv.mcmaster.ca/jfox
> >
> > > On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> wrote:
> > >
> > > Dear Petr,
> > >
> > > Many thanks for the quick response.
> > >
> > > I also read this:-
> > > https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
> > >
> > > Also I read  in ?poly:-
> > >     The orthogonal polynomial is summarized by the coefficients, which
> > >     can be used to evaluate it via the three-term recursion given in
> > >     Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
> > >     of the code.
> > >
> > > I don't have access to the mentioned book.
> > >
> > > Out of curiosity, what is the name of the discrete orthogonal
> polynomial
> > > used by R ?
> > > What discrete measure is it orthogonal with respect to ?
> > >
> > > Many thanks,
> > > Ashim
> > >
> > >
> > >
> > >
> > > On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
> > >
> > >> You could get answer quickly by searching net.
> > >>
> > >>
> > >>
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
> > >> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
> > >> <
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
> >
> > >>
> > >> Cheers
> > >> Petr
> > >>
> > >>> -----Original Message-----
> > >>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
> Kapoor
> > >>> Sent: Wednesday, November 27, 2019 12:55 PM
> > >>> To: R Help <r-help at r-project.org>
> > >>> Subject: [R] Orthogonal polynomials used by R
> > >>>
> > >>> Dear All,
> > >>>
> > >>> I have created a time trend by doing x<-1:93 because I have a time
> series
> > >>> with 93 data points. Next I did :-
> > >>>
> > >>> y = lm(series ~ poly(x,4))$residuals
> > >>>
> > >>> to detrend series.
> > >>>
> > >>> I choose this 4 as the order of my polynomial using cross validation/
> > >>> checking the absence of trend in the residuals so I think I have not
> > >> overfit
> > >>> this series.
> > >>>
> > >>> I wish to document the formula of poly(x,4). I am not able to find
> it in
> > >> ?poly
> > >>>
> > >>> Can someone please tell me what the formula for the orthogonal
> > >>> polynomial used by R is ?
> > >>>
> > >>> Thank you,
> > >>> Ashim
> > >>>
> > >>>      [[alternative HTML version deleted]]
> > >>>
> > >>> ______________________________________________
> > >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide http://www.R-project.org/posting-
> > >>> guide.html
> > >>> and provide commented, minimal, self-contained, reproducible code.
> > >>
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
>
>
>

	[[alternative HTML version deleted]]


From ||@t@ @end|ng |rom dewey@myzen@co@uk  Thu Nov 28 17:25:46 2019
From: ||@t@ @end|ng |rom dewey@myzen@co@uk (Michael Dewey)
Date: Thu, 28 Nov 2019 16:25:46 +0000
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
 <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>
 <CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>
Message-ID: <76702fbc-0f16-78bf-ba49-071ae52d96c9@dewey.myzen.co.uk>

Dear Ashim

As John said your two examples give the same model to within rounding 
error so it is not clear what you see the problem as being. You can 
always remove some of the correlation by subtracting out a large 
constant from x before you use poly() on it.

Michael

On 28/11/2019 16:02, Ashim Kapoor wrote:
> On Thu, Nov 28, 2019 at 7:38 PM Fox, John <jfox at mcmaster.ca> wrote:
> 
>> Dear Ashim,
>>
>> I'm afraid that much of what you say here is confused.
>>
>> First, because poly(x) and poly(x, raw=TRUE) produce the same fitted
>> values (as I previously explained), they also produce the same residuals,
>> and consequently the same CV criteria. From the point of view of CV,
>> there's therefore no reason to prefer orthogonal polynomials. And you still
>> don't explain why you want to interpret the coefficients of the polynomial.
>>
> 
> The trend in the variable that I am trying to create an ARIMA model for is
> given by poly(x,4). That is why I wished to know what these polynomials
> look like.
> 
> I used  :
> 
> trend <- predict(lm(gdp~poly(x,4)),newdata = data.frame(
> x=94:103),interval="confidence")
> 
> and I was able to (numerically) extrapolate the poly(x,4) trend, although,
> I think it would be interesting to know what polynomials I was dealing with
> in this case. Just some intuition as to if the linear / quadratic / cubic /
> fourth order polynomial trend is dominating. I don't know how I would
> interpret them, but it would be fun to know.
> 
> Please allow me to show you a trick. I read this on the internet, here :-
> 
> https://www.datasciencecentral.com/profiles/blogs/deep-dive-into-polynomial-regression-and-overfitting
> 
> Please see the LAST comment by Scott Stelljes where he suggests using an
> orthogonal polynomial basis. He does not elaborate but leaves the reader to
> work out the details.
> 
> Here is what I think of this. Take a big number say 20 and take a variable
> in which we are trying to find the order of the polynomial in the trend.
> Like this :-
> 
>> summary(lm(gdp ~ poly(x,20)))
> 
> Call:
> lm(formula = gdp ~ poly(x, 20))
> 
> Residuals:
>       Min       1Q   Median       3Q      Max
> -1235661  -367798   -80453   240360  1450906
> 
> Coefficients:
>                 Estimate Std. Error t value Pr(>|t|)
> (Intercept)    17601482      66934 262.968  < 2e-16 ***
> poly(x, 20)1  125679081     645487 194.704  < 2e-16 ***
> poly(x, 20)2   43108747     645487  66.785  < 2e-16 ***
> poly(x, 20)3    3605839     645487   5.586 3.89e-07 ***
> poly(x, 20)4   -2977277     645487  -4.612 1.69e-05 ***
> poly(x, 20)5    1085732     645487   1.682   0.0969 .
> poly(x, 20)6    1124125     645487   1.742   0.0859 .
> poly(x, 20)7    -108676     645487  -0.168   0.8668
> poly(x, 20)8    -976915     645487  -1.513   0.1345
> poly(x, 20)9   -1635444     645487  -2.534   0.0135 *
> poly(x, 20)10   -715019     645487  -1.108   0.2717
> poly(x, 20)11    347102     645487   0.538   0.5924
> poly(x, 20)12   -176728     645487  -0.274   0.7850
> poly(x, 20)13   -634151     645487  -0.982   0.3292
> poly(x, 20)14   -537725     645487  -0.833   0.4076
> poly(x, 20)15    -58674     645487  -0.091   0.9278
> poly(x, 20)16    -67030     645487  -0.104   0.9176
> poly(x, 20)17   -809443     645487  -1.254   0.2139
> poly(x, 20)18   -668879     645487  -1.036   0.3036
> poly(x, 20)19   -302384     645487  -0.468   0.6409
> poly(x, 20)20    359134     645487   0.556   0.5797
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 645500 on 72 degrees of freedom
> Multiple R-squared:  0.9983, Adjusted R-squared:  0.9978
> F-statistic:  2122 on 20 and 72 DF,  p-value: < 2.2e-16
> 
>>
> 
> 
> The CV estimate of the trend is 4. I am not saying this method is perfect,
> but look above this method also suggests n=4.
> 
> I CANNOT do this with raw polynomials, since they are correlated and
> JOINTLY in the presence of others they may not be significant, please see
> below.
> 
>> summary(lm(gdp ~ poly(x,20,raw=T)))
> 
> Call:
> lm(formula = gdp ~ poly(x, 20, raw = T))
> 
> Residuals:
>       Min       1Q   Median       3Q      Max
> -1286007  -372221   -81320   248510  1589130
> 
> Coefficients: (4 not defined because of singularities)
>                           Estimate Std. Error t value Pr(>|t|)
> (Intercept)             2.067e+06  2.649e+06   0.780    0.438
> poly(x, 20, raw = T)1   1.633e+06  3.556e+06   0.459    0.647
> poly(x, 20, raw = T)2  -7.601e+05  1.679e+06  -0.453    0.652
> poly(x, 20, raw = T)3   1.861e+05  3.962e+05   0.470    0.640
> poly(x, 20, raw = T)4  -2.634e+04  5.480e+04  -0.481    0.632
> poly(x, 20, raw = T)5   2.370e+03  4.854e+03   0.488    0.627
> poly(x, 20, raw = T)6  -1.434e+02  2.906e+02  -0.493    0.623
> poly(x, 20, raw = T)7   6.022e+00  1.213e+01   0.496    0.621
> poly(x, 20, raw = T)8  -1.784e-01  3.587e-01  -0.497    0.620
> poly(x, 20, raw = T)9   3.727e-03  7.503e-03   0.497    0.621
> poly(x, 20, raw = T)10 -5.373e-05  1.086e-04  -0.495    0.622
> poly(x, 20, raw = T)11  5.016e-07  1.018e-06   0.493    0.624
> poly(x, 20, raw = T)12 -2.483e-09  5.069e-09  -0.490    0.626
> poly(x, 20, raw = T)13         NA         NA      NA       NA
> poly(x, 20, raw = T)14  5.656e-14  1.167e-13   0.485    0.629
> poly(x, 20, raw = T)15         NA         NA      NA       NA
> poly(x, 20, raw = T)16 -1.933e-18  4.011e-18  -0.482    0.631
> poly(x, 20, raw = T)17         NA         NA      NA       NA
> poly(x, 20, raw = T)18  5.181e-23  1.076e-22   0.482    0.631
> poly(x, 20, raw = T)19         NA         NA      NA       NA
> poly(x, 20, raw = T)20 -7.173e-28  1.480e-27  -0.485    0.629
> 
> Residual standard error: 641000 on 76 degrees of freedom
> Multiple R-squared:  0.9982, Adjusted R-squared:  0.9979
> F-statistic:  2690 on 16 and 76 DF,  p-value: < 2.2e-16
> 
>>
> 
> Note,however, once the orthogonal polynomials have suggested a number, 4 in
> this case, I can do this :-
> 
>   summary(lm(gdp ~ poly(x,4,raw=T)))
> 
> Call:
> lm(formula = gdp ~ poly(x, 4, raw = T))
> 
> Residuals:
>       Min       1Q   Median       3Q      Max
> -1278673  -424315   -22357   310977  1731813
> 
> Coefficients:
>                         Estimate Std. Error t value Pr(>|t|)
> (Intercept)           3.022e+06  3.676e+05   8.220 1.64e-12 ***
> poly(x, 4, raw = T)1  1.741e+05  5.357e+04   3.249  0.00164 **
> poly(x, 4, raw = T)2 -6.434e+03  2.300e+03  -2.797  0.00633 **
> poly(x, 4, raw = T)3  1.878e+02  3.667e+01   5.123 1.76e-06 ***
> poly(x, 4, raw = T)4 -8.682e-01  1.935e-01  -4.486 2.19e-05 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 663700 on 88 degrees of freedom
> Multiple R-squared:  0.9978, Adjusted R-squared:  0.9977
> F-statistic: 1.003e+04 on 4 and 88 DF,  p-value: < 2.2e-16
> 
>>
> 
> Although due to correlations they may not be significant jointly, but in
> this case all 4 powers come out significant.
> 
> 
> Second, the model formula gdp~1+x+x^2 and other similar formulas in your
>> message don't do what you think. Like + and *, the ^ operator has special
>> meaning on the right-hand side of an R model formula. See ?Formula and
>> perhaps read something about statistical models in R. For example:
>>
>>> x <- 1:93
>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>>> (m <- lm(y ~ x + x^2))
>>
>> Call:
>> lm(formula = y ~ x + x^2)
>>
>> Coefficients:
>> (Intercept)            x
>>    -15781393       667147
>>
>> While gpp ~ x + I(x^2) would work, a better way to fit a raw quadratic is
>> as gdp ~ poly(x, 2, raw=TRUE), as I suggested in my earlier message.
>>
> 
> My bad. Yes, I have some idea of the Wilkinson-Rogers notation. I have seen
> it in books, although it slipped my mind that I had to use I( ).
> 
> 
>> Finally, as to what you should do, I generally try to avoid statistical
>> consulting by email. If you can find competent statistical help locally,
>> such as at a nearby university, I'd recommend talking to someone about the
>> purpose of your research and the nature of your data. If that's not
>> possible, then others have suggested where you might find help, but to get
>> useful advice you'll have to provide much more information about your
>> research.
>>
> 
> My original query was about the polynomials used by R which I think is ON
> topic. My apologies that this query turned into a statistics query.
> 
> 
>> Best,
>>   John
>>
>>    -----------------------------
>>    John Fox, Professor Emeritus
>>    McMaster University
>>    Hamilton, Ontario, Canada
>>    Web: http::/socserv.mcmaster.ca/jfox
>>
>>> On Nov 28, 2019, at 12:46 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>> wrote:
>>>
>>> Dear Peter and John,
>>>
>>> Many thanks for your prompt replies.
>>>
>>> Here is what I was trying to do.  I was trying to build a statistical
>> model of a given time series using Box Jenkins methodology. The series has
>> 93 data points. Before I analyse the ACF and PACF, I am required to
>> de-trend the series. The series seems to have an upward trend. I wanted to
>> find out what order polynomial should I fit the series
>>> without overfitting.  For this I want to use orthogonal polynomials(I
>> think someone on the internet was talking about preventing overfitting by
>> using orthogonal polynomials) . This seems to me as a poor man's cross
>> validation.
>>>
>>> So my plan is to keep increasing the degree of the orthogonal
>> polynomials till the coefficient of the last orthogonal polynomial becomes
>> insignificant.
>>>
>>> Note : If I do NOT use orthogonal polynomials, I will overfit the data
>> set and I don't think that is a good way to detect the true order of the
>> polynomial.
>>>
>>> Also now that I have detrended the series and built an ARIMA model of
>> the residuals, now I want to forecast. For this I need to use the original
>> polynomials and their coefficients.
>>>
>>> I hope I was clear and that my methodology is ok.
>>>
>>> I have another query here :-
>>>
>>> Note : If I used cross-validation to determine the order of the
>> polynomial, I don't get a clear answer.
>>>
>>> See here :-
>>> library(boot)
>>> mydf = data.frame(cbind(gdp,x))
>>> d<-(c(
>>> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
>>> print(d)
>>> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
>>> ## [6] 4.980159e+11
>>>
>>> # Here it chooses 5. (but 4 and 5 are kind of similar).
>>>
>>>
>>> d1 <- (c(
>>> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
>>>
>>> print(d1)
>>> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
>>> ## [6] 2.145754e+13
>>>
>>> # here it chooses 1 or 6
>>>
>>> Query : Why does it choose 1? Notice : Is this just round off noise /
>> noise due to sampling error created by Cross Validation when it creates the
>> K folds? Is this due to the ill conditioned model matrix?
>>>
>>> Best Regards,
>>> Ashim.
>>>
>>>
>>>
>>>
>>>
>>> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
>>> Dear Ashim,
>>>
>>> Orthogonal polynomials are used because they tend to produce more
>> accurate numerical computations, not because their coefficients are
>> interpretable, so I wonder why you're interested in the coefficients.
>>>
>>> The regressors produced are orthogonal to the constant regressor and are
>> orthogonal to each other (and in fact are orthonormal), as it's simple to
>> demonstrate:
>>>
>>> ------- snip -------
>>>
>>>> x <- 1:93
>>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>>>> (m <- lm(y ~ poly(x, 4)))
>>>
>>> Call:
>>> lm(formula = y ~ poly(x, 4))
>>>
>>> Coefficients:
>>> (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
>>>     15574516    172715069     94769949     27683528      3429259
>>>
>>>> X <- model.matrix(m)
>>>> head(X)
>>>    (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>>> 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
>>> 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
>>> 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
>>> 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
>>> 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
>>> 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
>>>
>>>> zapsmall(crossprod(X))# X'X
>>>              (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>>> (Intercept)          93           0           0           0           0
>>> poly(x, 4)1           0           1           0           0           0
>>> poly(x, 4)2           0           0           1           0           0
>>> poly(x, 4)3           0           0           0           1           0
>>> poly(x, 4)4           0           0           0           0           1
>>>
>>> ------- snip -------
>>>
>>> If for some not immediately obvious reason you're interested in the
>> regression coefficients, why not just use a "raw" polynomial:
>>>
>>> ------- snip -------
>>>
>>>> (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
>>>
>>> Call:
>>> lm(formula = y ~ poly(x, 4, raw = TRUE))
>>>
>>> Coefficients:
>>>              (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw =
>> TRUE)2  poly(x, 4, raw = TRUE)3
>>>                   1.5640                   0.8985
>>   1.0037                   1.0000
>>> poly(x, 4, raw = TRUE)4
>>>                   1.0000
>>>
>>> ------- snip -------
>>>
>>> These coefficients are simply interpretable but the model matrix is more
>> poorly conditioned:
>>>
>>> ------- snip -------
>>>
>>>> head(X1)
>>>    (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
>> raw = TRUE)3
>>> 1           1                       1                       1
>>             1
>>> 2           1                       2                       4
>>             8
>>> 3           1                       3                       9
>>            27
>>> 4           1                       4                      16
>>            64
>>> 5           1                       5                      25
>>           125
>>> 6           1                       6                      36
>>           216
>>>    poly(x, 4, raw = TRUE)4
>>> 1                       1
>>> 2                      16
>>> 3                      81
>>> 4                     256
>>> 5                     625
>>> 6                    1296
>>>> round(cor(X1[, -1]), 2)
>>>                          poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
>> poly(x, 4, raw = TRUE)3
>>> poly(x, 4, raw = TRUE)1                    1.00                    0.97
>>                    0.92
>>> poly(x, 4, raw = TRUE)2                    0.97                    1.00
>>                    0.99
>>> poly(x, 4, raw = TRUE)3                    0.92                    0.99
>>                    1.00
>>> poly(x, 4, raw = TRUE)4                    0.87                    0.96
>>                    0.99
>>>                          poly(x, 4, raw = TRUE)4
>>> poly(x, 4, raw = TRUE)1                    0.87
>>> poly(x, 4, raw = TRUE)2                    0.96
>>> poly(x, 4, raw = TRUE)3                    0.99
>>> poly(x, 4, raw = TRUE)4                    1.00
>>>
>>> ------- snip -------
>>>
>>> The two parametrizations are equivalent, however, in that they represent
>> the same regression surface, and so, e.g., produce the same fitted values:
>>>
>>> ------- snip -------
>>>
>>>> all.equal(fitted(m), fitted(m1))
>>> [1] TRUE
>>>
>>> ------- snip -------
>>>
>>> Because one is usually not interested in the individual coefficients of
>> a polynomial there usually isn't a reason to prefer one parametrization to
>> the other on the grounds of interpretability, so why do you need to
>> interpret the regression equation?
>>>
>>> I hope this helps,
>>>   John
>>>
>>>    -----------------------------
>>>    John Fox, Professor Emeritus
>>>    McMaster University
>>>    Hamilton, Ontario, Canada
>>>    Web: http::/socserv.mcmaster.ca/jfox
>>>
>>>> On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>> wrote:
>>>>
>>>> Dear Petr,
>>>>
>>>> Many thanks for the quick response.
>>>>
>>>> I also read this:-
>>>> https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
>>>>
>>>> Also I read  in ?poly:-
>>>>      The orthogonal polynomial is summarized by the coefficients, which
>>>>      can be used to evaluate it via the three-term recursion given in
>>>>      Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
>>>>      of the code.
>>>>
>>>> I don't have access to the mentioned book.
>>>>
>>>> Out of curiosity, what is the name of the discrete orthogonal
>> polynomial
>>>> used by R ?
>>>> What discrete measure is it orthogonal with respect to ?
>>>>
>>>> Many thanks,
>>>> Ashim
>>>>
>>>>
>>>>
>>>>
>>>> On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
>> wrote:
>>>>
>>>>> You could get answer quickly by searching net.
>>>>>
>>>>>
>>>>>
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
>>>>> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
>>>>> <
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
>>>
>>>>>
>>>>> Cheers
>>>>> Petr
>>>>>
>>>>>> -----Original Message-----
>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
>> Kapoor
>>>>>> Sent: Wednesday, November 27, 2019 12:55 PM
>>>>>> To: R Help <r-help at r-project.org>
>>>>>> Subject: [R] Orthogonal polynomials used by R
>>>>>>
>>>>>> Dear All,
>>>>>>
>>>>>> I have created a time trend by doing x<-1:93 because I have a time
>> series
>>>>>> with 93 data points. Next I did :-
>>>>>>
>>>>>> y = lm(series ~ poly(x,4))$residuals
>>>>>>
>>>>>> to detrend series.
>>>>>>
>>>>>> I choose this 4 as the order of my polynomial using cross validation/
>>>>>> checking the absence of trend in the residuals so I think I have not
>>>>> overfit
>>>>>> this series.
>>>>>>
>>>>>> I wish to document the formula of poly(x,4). I am not able to find
>> it in
>>>>> ?poly
>>>>>>
>>>>>> Can someone please tell me what the formula for the orthogonal
>>>>>> polynomial used by R is ?
>>>>>>
>>>>>> Thank you,
>>>>>> Ashim
>>>>>>
>>>>>>       [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>>> guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>
>>>>
>>>>        [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From j|ox @end|ng |rom mcm@@ter@c@  Thu Nov 28 18:12:49 2019
From: j|ox @end|ng |rom mcm@@ter@c@ (Fox, John)
Date: Thu, 28 Nov 2019 17:12:49 +0000
Subject: [R] Orthogonal polynomials used by R
In-Reply-To: <6861_1574957005_xASG3Aao017128_CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
 <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>
 <6861_1574957005_xASG3Aao017128_CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>
Message-ID: <365344B8-2B7D-4ED1-915C-D8A3D2E05894@mcmaster.ca>

Dear Ashim,

Please see my brief remarks below:

> On Nov 28, 2019, at 11:02 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
> 
> On Thu, Nov 28, 2019 at 7:38 PM Fox, John <jfox at mcmaster.ca> wrote:
> 
>> Dear Ashim,
>> 
>> I'm afraid that much of what you say here is confused.
>> 
>> First, because poly(x) and poly(x, raw=TRUE) produce the same fitted
>> values (as I previously explained), they also produce the same residuals,
>> and consequently the same CV criteria. From the point of view of CV,
>> there's therefore no reason to prefer orthogonal polynomials. And you still
>> don't explain why you want to interpret the coefficients of the polynomial.
>> 
> 
> The trend in the variable that I am trying to create an ARIMA model for is
> given by poly(x,4). That is why I wished to know what these polynomials
> look like.

The polynomial "looks" exactly the same whether or not you use raw or orthogonal regressors as a basis for it. That is, the two bases represent exactly the same regression surface (i.e., curve in the case of one x). To see what the fitted polynomial looks like, graph it. But I've now made essentially this point three times, so if it's not clear I regret the unclarity but I don't really have anything to add.

For other points, see below.

> 
> I used  :
> 
> trend <- predict(lm(gdp~poly(x,4)),newdata = data.frame(
> x=94:103),interval="confidence")
> 
> and I was able to (numerically) extrapolate the poly(x,4) trend, although,
> I think it would be interesting to know what polynomials I was dealing with
> in this case. Just some intuition as to if the linear / quadratic / cubic /
> fourth order polynomial trend is dominating. I don't know how I would
> interpret them, but it would be fun to know.

I'm not sure how you intend to interpret the coefficients, say of the raw polynomial. Their magnitudes shouldn't be compared because the size of the powers of x grows with the powers. 

BTW, it's very risky to use high-order polynomials for extrapolation beyond the observed range of x, even if the model fits well within the observed range of x, and of course raw and orthogonal polynomial produce exactly the same (problematic) extrapolations (although those produced by raw polynomials may be subject to more rounding error). To be clear, I'm not arguing that one should in general use raw polynomials in preference to orthogonal polynomials, just that the former have generally interpretable coefficients and the latter don't.

> 
> Please allow me to show you a trick. I read this on the internet, here :-
> 
> https://www.datasciencecentral.com/profiles/blogs/deep-dive-into-polynomial-regression-and-overfitting
> 
> Please see the LAST comment by Scott Stelljes where he suggests using an
> orthogonal polynomial basis. He does not elaborate buttoleaves the reader to
> work out the details.

This blog focuses on the numerical stability of raw versus orthogonal polynomials. If by "stepwise" you mean adding successive powers to the model, you'll get exactly the same sequence of fits with raw as with orthogonal polynomial, as I've now explained several times.

> 
> Here is what I think of this. Take a big number say 20 and take a variable
> in which we are trying to find the order of the polynomial in the trend.
> Like this :-
> 
>> summary(lm(gdp ~ poly(x,20)))
> 
> Call:
> lm(formula = gdp ~ poly(x, 20))
> 
> Residuals:
>     Min       1Q   Median       3Q      Max
> -1235661  -367798   -80453   240360  1450906
> 
> Coefficients:
>               Estimate Std. Error t value Pr(>|t|)
> (Intercept)    17601482      66934 262.968  < 2e-16 ***
> poly(x, 20)1  125679081     645487 194.704  < 2e-16 ***
> poly(x, 20)2   43108747     645487  66.785  < 2e-16 ***
> poly(x, 20)3    3605839     645487   5.586 3.89e-07 ***
> poly(x, 20)4   -2977277     645487  -4.612 1.69e-05 ***
> poly(x, 20)5    1085732     645487   1.682   0.0969 .
> poly(x, 20)6    1124125     645487   1.742   0.0859 .
> poly(x, 20)7    -108676     645487  -0.168   0.8668
> poly(x, 20)8    -976915     645487  -1.513   0.1345
> poly(x, 20)9   -1635444     645487  -2.534   0.0135 *
> poly(x, 20)10   -715019     645487  -1.108   0.2717
> poly(x, 20)11    347102     645487   0.538   0.5924
> poly(x, 20)12   -176728     645487  -0.274   0.7850
> poly(x, 20)13   -634151     645487  -0.982   0.3292
> poly(x, 20)14   -537725     645487  -0.833   0.4076
> poly(x, 20)15    -58674     645487  -0.091   0.9278
> poly(x, 20)16    -67030     645487  -0.104   0.9176
> poly(x, 20)17   -809443     645487  -1.254   0.2139
> poly(x, 20)18   -668879     645487  -1.036   0.3036
> poly(x, 20)19   -302384     645487  -0.468   0.6409
> poly(x, 20)20    359134     645487   0.556   0.5797
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 645500 on 72 degrees of freedom
> Multiple R-squared:  0.9983, Adjusted R-squared:  0.9978
> F-statistic:  2122 on 20 and 72 DF,  p-value: < 2.2e-16
> 
>> 
> 
> 
> The CV estimate of the trend is 4. I am not saying this method is perfect,
> but look above this method also suggests n=4.
> 
> I CANNOT do this with raw polynomials, since they are correlated and
> JOINTLY in the presence of others they may not be significant, please see
> below.
> 
>> summary(lm(gdp ~ poly(x,20,raw=T)))
> 
> Call:
> lm(formula = gdp ~ poly(x, 20, raw = T))
> 
> Residuals:
>     Min       1Q   Median       3Q      Max
> -1286007  -372221   -81320   248510  1589130
> 
> Coefficients: (4 not defined because of singularities)
>                         Estimate Std. Error t value Pr(>|t|)
> (Intercept)             2.067e+06  2.649e+06   0.780    0.438
> poly(x, 20, raw = T)1   1.633e+06  3.556e+06   0.459    0.647
> poly(x, 20, raw = T)2  -7.601e+05  1.679e+06  -0.453    0.652
> poly(x, 20, raw = T)3   1.861e+05  3.962e+05   0.470    0.640
> poly(x, 20, raw = T)4  -2.634e+04  5.480e+04  -0.481    0.632
> poly(x, 20, raw = T)5   2.370e+03  4.854e+03   0.488    0.627
> poly(x, 20, raw = T)6  -1.434e+02  2.906e+02  -0.493    0.623
> poly(x, 20, raw = T)7   6.022e+00  1.213e+01   0.496    0.621
> poly(x, 20, raw = T)8  -1.784e-01  3.587e-01  -0.497    0.620
> poly(x, 20, raw = T)9   3.727e-03  7.503e-03   0.497    0.621
> poly(x, 20, raw = T)10 -5.373e-05  1.086e-04  -0.495    0.622
> poly(x, 20, raw = T)11  5.016e-07  1.018e-06   0.493    0.624
> poly(x, 20, raw = T)12 -2.483e-09  5.069e-09  -0.490    0.626
> poly(x, 20, raw = T)13         NA         NA      NA       NA
> poly(x, 20, raw = T)14  5.656e-14  1.167e-13   0.485    0.629
> poly(x, 20, raw = T)15         NA         NA      NA       NA
> poly(x, 20, raw = T)16 -1.933e-18  4.011e-18  -0.482    0.631
> poly(x, 20, raw = T)17         NA         NA      NA       NA
> poly(x, 20, raw = T)18  5.181e-23  1.076e-22   0.482    0.631
> poly(x, 20, raw = T)19         NA         NA      NA       NA
> poly(x, 20, raw = T)20 -7.173e-28  1.480e-27  -0.485    0.629
> 
> Residual standard error: 641000 on 76 degrees of freedom
> Multiple R-squared:  0.9982, Adjusted R-squared:  0.9979
> F-statistic:  2690 on 16 and 76 DF,  p-value: < 2.2e-16
> 
>> 
> 
> Note,however, once the orthogonal polynomials have suggested a number, 4 in
> this case, I can do this :-
> 
> summary(lm(gdp ~ poly(x,4,raw=T)))
> 
> Call:
> lm(formula = gdp ~ poly(x, 4, raw = T))
> 
> Residuals:
>     Min       1Q   Median       3Q      Max
> -1278673  -424315   -22357   310977  1731813
> 
> Coefficients:
>                       Estimate Std. Error t value Pr(>|t|)
> (Intercept)           3.022e+06  3.676e+05   8.220 1.64e-12 ***
> poly(x, 4, raw = T)1  1.741e+05  5.357e+04   3.249  0.00164 **
> poly(x, 4, raw = T)2 -6.434e+03  2.300e+03  -2.797  0.00633 **
> poly(x, 4, raw = T)3  1.878e+02  3.667e+01   5.123 1.76e-06 ***
> poly(x, 4, raw = T)4 -8.682e-01  1.935e-01  -4.486 2.19e-05 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Residual standard error: 663700 on 88 degrees of freedom
> Multiple R-squared:  0.9978, Adjusted R-squared:  0.9977
> F-statistic: 1.003e+04 on 4 and 88 DF,  p-value: < 2.2e-16
> 
>> 
> 
> Although due to correlations they may not be significant jointly, but in
> this case all 4 powers come out significant.

Yes, the coefficients of orthogonal polynomials permit stepwise tests of each term after the previous ones because the orthogonalization is done stepwise. But (1) interpreting these tests is problematic because, e.g., of issues of simultaneous inference, and (2) you're using CV for model selection anyway (aren't you?) and you'll get (once more) exactly the same CV results from raw and orthogonal polynomials.

> 
> 
> Second, the model formula gdp~1+x+x^2 and other similar formulas in your
>> message don't do what you think. Like + and *, the ^ operator has special
>> meaning on the right-hand side of an R model formula. See ?Formula and
>> perhaps read something about statistical models in R. For example:
>> 
>>> x <- 1:93
>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>>> (m <- lm(y ~ x + x^2))
>> 
>> Call:
>> lm(formula = y ~ x + x^2)
>> 
>> Coefficients:
>> (Intercept)            x
>>  -15781393       667147
>> 
>> While gpp ~ x + I(x^2) would work, a better way to fit a raw quadratic is
>> as gdp ~ poly(x, 2, raw=TRUE), as I suggested in my earlier message.
>> 
> 
> My bad. Yes, I have some idea of the Wilkinson-Rogers notation. I have seen
> it in books, although it slipped my mind that I had to use I( ).
> 
> 
>> Finally, as to what you should do, I generally try to avoid statistical
>> consulting by email. If you can find competent statistical help locally,
>> such as at a nearby university, I'd recommend talking to someone about the
>> purpose of your research and the nature of your data. If that's not
>> possible, then others have suggested where you might find help, but to get
>> useful advice you'll have to provide much more information about your
>> research.
>> 
> 
> My original query was about the polynomials used by R which I think is ON
> topic.

I think that question was answered a while ago.

> My apologies that this query turned into a statistics query.

I don't feel the need for an apology, and although the list focuses on using R, often related statistical issues arise. On the other hand, I don't have anything more to say about your problem. Others are welcome to pick it up.

Best,
 John

> 
> 
>> Best,
>> John
>> 
>>  -----------------------------
>>  John Fox, Professor Emeritus
>>  McMaster University
>>  Hamilton, Ontario, Canada
>>  Web: http::/socserv.mcmaster.ca/jfox
>> 
>>> On Nov 28, 2019, at 12:46 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>> wrote:
>>> 
>>> Dear Peter and John,
>>> 
>>> Many thanks for your prompt replies.
>>> 
>>> Here is what I was trying to do.  I was trying to build a statistical
>> model of a given time series using Box Jenkins methodology. The series has
>> 93 data points. Before I analyse the ACF and PACF, I am required to
>> de-trend the series. The series seems to have an upward trend. I wanted to
>> find out what order polynomial should I fit the series
>>> without overfitting.  For this I want to use orthogonal polynomials(I
>> think someone on the internet was talking about preventing overfitting by
>> using orthogonal polynomials) . This seems to me as a poor man's cross
>> validation.
>>> 
>>> So my plan is to keep increasing the degree of the orthogonal
>> polynomials till the coefficient of the last orthogonal polynomial becomes
>> insignificant.
>>> 
>>> Note : If I do NOT use orthogonal polynomials, I will overfit the data
>> set and I don't think that is a good way to detect the true order of the
>> polynomial.
>>> 
>>> Also now that I have detrended the series and built an ARIMA model of
>> the residuals, now I want to forecast. For this I need to use the original
>> polynomials and their coefficients.
>>> 
>>> I hope I was clear and that my methodology is ok.
>>> 
>>> I have another query here :-
>>> 
>>> Note : If I used cross-validation to determine the order of the
>> polynomial, I don't get a clear answer.
>>> 
>>> See here :-
>>> library(boot)
>>> mydf = data.frame(cbind(gdp,x))
>>> d<-(c(
>>> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
>>> print(d)
>>> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
>>> ## [6] 4.980159e+11
>>> 
>>> # Here it chooses 5. (but 4 and 5 are kind of similar).
>>> 
>>> 
>>> d1 <- (c(
>>> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
>>> 
>>> print(d1)
>>> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
>>> ## [6] 2.145754e+13
>>> 
>>> # here it chooses 1 or 6
>>> 
>>> Query : Why does it choose 1? Notice : Is this just round off noise /
>> noise due to sampling error created by Cross Validation when it creates the
>> K folds? Is this due to the ill conditioned model matrix?
>>> 
>>> Best Regards,
>>> Ashim.
>>> 
>>> 
>>> 
>>> 
>>> 
>>> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
>>> Dear Ashim,
>>> 
>>> Orthogonal polynomials are used because they tend to produce more
>> accurate numerical computations, not because their coefficients are
>> interpretable, so I wonder why you're interested in the coefficients.
>>> 
>>> The regressors produced are orthogonal to the constant regressor and are
>> orthogonal to each other (and in fact are orthonormal), as it's simple to
>> demonstrate:
>>> 
>>> ------- snip -------
>>> 
>>>> x <- 1:93
>>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>>>> (m <- lm(y ~ poly(x, 4)))
>>> 
>>> Call:
>>> lm(formula = y ~ poly(x, 4))
>>> 
>>> Coefficients:
>>> (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
>>>   15574516    172715069     94769949     27683528      3429259
>>> 
>>>> X <- model.matrix(m)
>>>> head(X)
>>>  (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>>> 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
>>> 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
>>> 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
>>> 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
>>> 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
>>> 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
>>> 
>>>> zapsmall(crossprod(X))# X'X
>>>            (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>>> (Intercept)          93           0           0           0           0
>>> poly(x, 4)1           0           1           0           0           0
>>> poly(x, 4)2           0           0           1           0           0
>>> poly(x, 4)3           0           0           0           1           0
>>> poly(x, 4)4           0           0           0           0           1
>>> 
>>> ------- snip -------
>>> 
>>> If for some not immediately obvious reason you're interested in the
>> regression coefficients, why not just use a "raw" polynomial:
>>> 
>>> ------- snip -------
>>> 
>>>> (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
>>> 
>>> Call:
>>> lm(formula = y ~ poly(x, 4, raw = TRUE))
>>> 
>>> Coefficients:
>>>            (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw =
>> TRUE)2  poly(x, 4, raw = TRUE)3
>>>                 1.5640                   0.8985
>> 1.0037                   1.0000
>>> poly(x, 4, raw = TRUE)4
>>>                 1.0000
>>> 
>>> ------- snip -------
>>> 
>>> These coefficients are simply interpretable but the model matrix is more
>> poorly conditioned:
>>> 
>>> ------- snip -------
>>> 
>>>> head(X1)
>>>  (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
>> raw = TRUE)3
>>> 1           1                       1                       1
>>           1
>>> 2           1                       2                       4
>>           8
>>> 3           1                       3                       9
>>          27
>>> 4           1                       4                      16
>>          64
>>> 5           1                       5                      25
>>         125
>>> 6           1                       6                      36
>>         216
>>>  poly(x, 4, raw = TRUE)4
>>> 1                       1
>>> 2                      16
>>> 3                      81
>>> 4                     256
>>> 5                     625
>>> 6                    1296
>>>> round(cor(X1[, -1]), 2)
>>>                        poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
>> poly(x, 4, raw = TRUE)3
>>> poly(x, 4, raw = TRUE)1                    1.00                    0.97
>>                  0.92
>>> poly(x, 4, raw = TRUE)2                    0.97                    1.00
>>                  0.99
>>> poly(x, 4, raw = TRUE)3                    0.92                    0.99
>>                  1.00
>>> poly(x, 4, raw = TRUE)4                    0.87                    0.96
>>                  0.99
>>>                        poly(x, 4, raw = TRUE)4
>>> poly(x, 4, raw = TRUE)1                    0.87
>>> poly(x, 4, raw = TRUE)2                    0.96
>>> poly(x, 4, raw = TRUE)3                    0.99
>>> poly(x, 4, raw = TRUE)4                    1.00
>>> 
>>> ------- snip -------
>>> 
>>> The two parametrizations are equivalent, however, in that they represent
>> the same regression surface, and so, e.g., produce the same fitted values:
>>> 
>>> ------- snip -------
>>> 
>>>> all.equal(fitted(m), fitted(m1))
>>> [1] TRUE
>>> 
>>> ------- snip -------
>>> 
>>> Because one is usually not interested in the individual coefficients of
>> a polynomial there usually isn't a reason to prefer one parametrization to
>> the other on the grounds of interpretability, so why do you need to
>> interpret the regression equation?
>>> 
>>> I hope this helps,
>>> John
>>> 
>>>  -----------------------------
>>>  John Fox, Professor Emeritus
>>>  McMaster University
>>>  Hamilton, Ontario, Canada
>>>  Web: http::/socserv.mcmaster.ca/jfox
>>> 
>>>> On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>> wrote:
>>>> 
>>>> Dear Petr,
>>>> 
>>>> Many thanks for the quick response.
>>>> 
>>>> I also read this:-
>>>> https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
>>>> 
>>>> Also I read  in ?poly:-
>>>>    The orthogonal polynomial is summarized by the coefficients, which
>>>>    can be used to evaluate it via the three-term recursion given in
>>>>    Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
>>>>    of the code.
>>>> 
>>>> I don't have access to the mentioned book.
>>>> 
>>>> Out of curiosity, what is the name of the discrete orthogonal
>> polynomial
>>>> used by R ?
>>>> What discrete measure is it orthogonal with respect to ?
>>>> 
>>>> Many thanks,
>>>> Ashim
>>>> 
>>>> 
>>>> 
>>>> 
>>>> On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
>> wrote:
>>>> 
>>>>> You could get answer quickly by searching net.
>>>>> 
>>>>> 
>>>>> 
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
>>>>> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
>>>>> <
>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
>>> 
>>>>> 
>>>>> Cheers
>>>>> Petr
>>>>> 
>>>>>> -----Original Message-----
>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
>> Kapoor
>>>>>> Sent: Wednesday, November 27, 2019 12:55 PM
>>>>>> To: R Help <r-help at r-project.org>
>>>>>> Subject: [R] Orthogonal polynomials used by R
>>>>>> 
>>>>>> Dear All,
>>>>>> 
>>>>>> I have created a time trend by doing x<-1:93 because I have a time
>> series
>>>>>> with 93 data points. Next I did :-
>>>>>> 
>>>>>> y = lm(series ~ poly(x,4))$residuals
>>>>>> 
>>>>>> to detrend series.
>>>>>> 
>>>>>> I choose this 4 as the order of my polynomial using cross validation/
>>>>>> checking the absence of trend in the residuals so I think I have not
>>>>> overfit
>>>>>> this series.
>>>>>> 
>>>>>> I wish to document the formula of poly(x,4). I am not able to find
>> it in
>>>>> ?poly
>>>>>> 
>>>>>> Can someone please tell me what the formula for the orthogonal
>>>>>> polynomial used by R is ?
>>>>>> 
>>>>>> Thank you,
>>>>>> Ashim
>>>>>> 
>>>>>>     [[alternative HTML version deleted]]
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>>> guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>>      [[alternative HTML version deleted]]
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pro|jcn@@h @end|ng |rom gm@||@com  Thu Nov 28 18:52:18 2019
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Thu, 28 Nov 2019 12:52:18 -0500
Subject: [R] Orthogonal polynomials used by R / another perspective
In-Reply-To: <365344B8-2B7D-4ED1-915C-D8A3D2E05894@mcmaster.ca>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
 <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>
 <6861_1574957005_xASG3Aao017128_CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>
 <365344B8-2B7D-4ED1-915C-D8A3D2E05894@mcmaster.ca>
Message-ID: <9bd14545-8d5e-acc4-ad46-e2abcc0e0ed4@gmail.com>

I'm not going to comment at all on the original question, but on a very common --
and often troublesome -- mixing of viewpoints about data modelling.

R and other software is used to "fit equations to data" and to "estimate models".
Unfortunately, a good bit of both these tasks is common. Usually (but NOT exclusively),
we fit by minimizing a sum of squared deviations then carry forward the calculations to
make inferences about the parameters of the model equations. Quite frequently, a single
equation can be written different ways e.g., ordinary or orthogonal polynomials.
It gets worse, much worse, for nonlinear models.

Moreover, sometimes (in my case because I get people sending me troublesome problems, about
90% of instances) there are very different sets of numerical values for the parameters of the
modelling equations that give essentially the same sum of squares (or other loss function).

Over several decades, because I am sometimes quite happy to use ANY of the choices, even
when there is a linear dependence in a linear modelling equation for the data given, I'm
often granted a lot of nasty comments. If I'm using the FIT e.g. for approximation, then such
criticism is mis-placed. If I want to use the particular parameters to say something about
the system I'm studying, then indeed I should go back to school (my critics might say
kindergarten). A serious concern about some machine learning is that fit alone is used
as a criteria from which to predict using the "equations" (though some models are just
algorithms). There is a jump from a good fit to existing data to a hope that we can get
good predictions outside of the available data space.

Having taught statistics for several decades also, I know how difficult it is to impart
a good perspective on these issues. However, I'll continue to urge data scientists and
statisticians to keep a wide view and be clear on what they want the software to do for
them.

For now, end of rant.

John Nash (package author of several packages for fitting and optimizing)


On 2019-11-28 12:12 p.m., Fox, John wrote:
> Dear Ashim,
> 
> Please see my brief remarks below:
> 
>> On Nov 28, 2019, at 11:02 AM, Ashim Kapoor <ashimkapoor at gmail.com> wrote:
>>
>> On Thu, Nov 28, 2019 at 7:38 PM Fox, John <jfox at mcmaster.ca> wrote:
>>
>>> Dear Ashim,
>>>
>>> I'm afraid that much of what you say here is confused.
>>>
>>> First, because poly(x) and poly(x, raw=TRUE) produce the same fitted
>>> values (as I previously explained), they also produce the same residuals,
>>> and consequently the same CV criteria. From the point of view of CV,
>>> there's therefore no reason to prefer orthogonal polynomials. And you still
>>> don't explain why you want to interpret the coefficients of the polynomial.
>>>
>>
>> The trend in the variable that I am trying to create an ARIMA model for is
>> given by poly(x,4). That is why I wished to know what these polynomials
>> look like.
> 
> The polynomial "looks" exactly the same whether or not you use raw or orthogonal regressors as a basis for it. That is, the two bases represent exactly the same regression surface (i.e., curve in the case of one x). To see what the fitted polynomial looks like, graph it. But I've now made essentially this point three times, so if it's not clear I regret the unclarity but I don't really have anything to add.
> 
> For other points, see below.
> 
>>
>> I used  :
>>
>> trend <- predict(lm(gdp~poly(x,4)),newdata = data.frame(
>> x=94:103),interval="confidence")
>>
>> and I was able to (numerically) extrapolate the poly(x,4) trend, although,
>> I think it would be interesting to know what polynomials I was dealing with
>> in this case. Just some intuition as to if the linear / quadratic / cubic /
>> fourth order polynomial trend is dominating. I don't know how I would
>> interpret them, but it would be fun to know.
> 
> I'm not sure how you intend to interpret the coefficients, say of the raw polynomial. Their magnitudes shouldn't be compared because the size of the powers of x grows with the powers. 
> 
> BTW, it's very risky to use high-order polynomials for extrapolation beyond the observed range of x, even if the model fits well within the observed range of x, and of course raw and orthogonal polynomial produce exactly the same (problematic) extrapolations (although those produced by raw polynomials may be subject to more rounding error). To be clear, I'm not arguing that one should in general use raw polynomials in preference to orthogonal polynomials, just that the former have generally interpretable coefficients and the latter don't.
> 
>>
>> Please allow me to show you a trick. I read this on the internet, here :-
>>
>> https://www.datasciencecentral.com/profiles/blogs/deep-dive-into-polynomial-regression-and-overfitting
>>
>> Please see the LAST comment by Scott Stelljes where he suggests using an
>> orthogonal polynomial basis. He does not elaborate buttoleaves the reader to
>> work out the details.
> 
> This blog focuses on the numerical stability of raw versus orthogonal polynomials. If by "stepwise" you mean adding successive powers to the model, you'll get exactly the same sequence of fits with raw as with orthogonal polynomial, as I've now explained several times.
> 
>>
>> Here is what I think of this. Take a big number say 20 and take a variable
>> in which we are trying to find the order of the polynomial in the trend.
>> Like this :-
>>
>>> summary(lm(gdp ~ poly(x,20)))
>>
>> Call:
>> lm(formula = gdp ~ poly(x, 20))
>>
>> Residuals:
>>     Min       1Q   Median       3Q      Max
>> -1235661  -367798   -80453   240360  1450906
>>
>> Coefficients:
>>               Estimate Std. Error t value Pr(>|t|)
>> (Intercept)    17601482      66934 262.968  < 2e-16 ***
>> poly(x, 20)1  125679081     645487 194.704  < 2e-16 ***
>> poly(x, 20)2   43108747     645487  66.785  < 2e-16 ***
>> poly(x, 20)3    3605839     645487   5.586 3.89e-07 ***
>> poly(x, 20)4   -2977277     645487  -4.612 1.69e-05 ***
>> poly(x, 20)5    1085732     645487   1.682   0.0969 .
>> poly(x, 20)6    1124125     645487   1.742   0.0859 .
>> poly(x, 20)7    -108676     645487  -0.168   0.8668
>> poly(x, 20)8    -976915     645487  -1.513   0.1345
>> poly(x, 20)9   -1635444     645487  -2.534   0.0135 *
>> poly(x, 20)10   -715019     645487  -1.108   0.2717
>> poly(x, 20)11    347102     645487   0.538   0.5924
>> poly(x, 20)12   -176728     645487  -0.274   0.7850
>> poly(x, 20)13   -634151     645487  -0.982   0.3292
>> poly(x, 20)14   -537725     645487  -0.833   0.4076
>> poly(x, 20)15    -58674     645487  -0.091   0.9278
>> poly(x, 20)16    -67030     645487  -0.104   0.9176
>> poly(x, 20)17   -809443     645487  -1.254   0.2139
>> poly(x, 20)18   -668879     645487  -1.036   0.3036
>> poly(x, 20)19   -302384     645487  -0.468   0.6409
>> poly(x, 20)20    359134     645487   0.556   0.5797
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Residual standard error: 645500 on 72 degrees of freedom
>> Multiple R-squared:  0.9983, Adjusted R-squared:  0.9978
>> F-statistic:  2122 on 20 and 72 DF,  p-value: < 2.2e-16
>>
>>>
>>
>>
>> The CV estimate of the trend is 4. I am not saying this method is perfect,
>> but look above this method also suggests n=4.
>>
>> I CANNOT do this with raw polynomials, since they are correlated and
>> JOINTLY in the presence of others they may not be significant, please see
>> below.
>>
>>> summary(lm(gdp ~ poly(x,20,raw=T)))
>>
>> Call:
>> lm(formula = gdp ~ poly(x, 20, raw = T))
>>
>> Residuals:
>>     Min       1Q   Median       3Q      Max
>> -1286007  -372221   -81320   248510  1589130
>>
>> Coefficients: (4 not defined because of singularities)
>>                         Estimate Std. Error t value Pr(>|t|)
>> (Intercept)             2.067e+06  2.649e+06   0.780    0.438
>> poly(x, 20, raw = T)1   1.633e+06  3.556e+06   0.459    0.647
>> poly(x, 20, raw = T)2  -7.601e+05  1.679e+06  -0.453    0.652
>> poly(x, 20, raw = T)3   1.861e+05  3.962e+05   0.470    0.640
>> poly(x, 20, raw = T)4  -2.634e+04  5.480e+04  -0.481    0.632
>> poly(x, 20, raw = T)5   2.370e+03  4.854e+03   0.488    0.627
>> poly(x, 20, raw = T)6  -1.434e+02  2.906e+02  -0.493    0.623
>> poly(x, 20, raw = T)7   6.022e+00  1.213e+01   0.496    0.621
>> poly(x, 20, raw = T)8  -1.784e-01  3.587e-01  -0.497    0.620
>> poly(x, 20, raw = T)9   3.727e-03  7.503e-03   0.497    0.621
>> poly(x, 20, raw = T)10 -5.373e-05  1.086e-04  -0.495    0.622
>> poly(x, 20, raw = T)11  5.016e-07  1.018e-06   0.493    0.624
>> poly(x, 20, raw = T)12 -2.483e-09  5.069e-09  -0.490    0.626
>> poly(x, 20, raw = T)13         NA         NA      NA       NA
>> poly(x, 20, raw = T)14  5.656e-14  1.167e-13   0.485    0.629
>> poly(x, 20, raw = T)15         NA         NA      NA       NA
>> poly(x, 20, raw = T)16 -1.933e-18  4.011e-18  -0.482    0.631
>> poly(x, 20, raw = T)17         NA         NA      NA       NA
>> poly(x, 20, raw = T)18  5.181e-23  1.076e-22   0.482    0.631
>> poly(x, 20, raw = T)19         NA         NA      NA       NA
>> poly(x, 20, raw = T)20 -7.173e-28  1.480e-27  -0.485    0.629
>>
>> Residual standard error: 641000 on 76 degrees of freedom
>> Multiple R-squared:  0.9982, Adjusted R-squared:  0.9979
>> F-statistic:  2690 on 16 and 76 DF,  p-value: < 2.2e-16
>>
>>>
>>
>> Note,however, once the orthogonal polynomials have suggested a number, 4 in
>> this case, I can do this :-
>>
>> summary(lm(gdp ~ poly(x,4,raw=T)))
>>
>> Call:
>> lm(formula = gdp ~ poly(x, 4, raw = T))
>>
>> Residuals:
>>     Min       1Q   Median       3Q      Max
>> -1278673  -424315   -22357   310977  1731813
>>
>> Coefficients:
>>                       Estimate Std. Error t value Pr(>|t|)
>> (Intercept)           3.022e+06  3.676e+05   8.220 1.64e-12 ***
>> poly(x, 4, raw = T)1  1.741e+05  5.357e+04   3.249  0.00164 **
>> poly(x, 4, raw = T)2 -6.434e+03  2.300e+03  -2.797  0.00633 **
>> poly(x, 4, raw = T)3  1.878e+02  3.667e+01   5.123 1.76e-06 ***
>> poly(x, 4, raw = T)4 -8.682e-01  1.935e-01  -4.486 2.19e-05 ***
>> ---
>> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>>
>> Residual standard error: 663700 on 88 degrees of freedom
>> Multiple R-squared:  0.9978, Adjusted R-squared:  0.9977
>> F-statistic: 1.003e+04 on 4 and 88 DF,  p-value: < 2.2e-16
>>
>>>
>>
>> Although due to correlations they may not be significant jointly, but in
>> this case all 4 powers come out significant.
> 
> Yes, the coefficients of orthogonal polynomials permit stepwise tests of each term after the previous ones because the orthogonalization is done stepwise. But (1) interpreting these tests is problematic because, e.g., of issues of simultaneous inference, and (2) you're using CV for model selection anyway (aren't you?) and you'll get (once more) exactly the same CV results from raw and orthogonal polynomials.
> 
>>
>>
>> Second, the model formula gdp~1+x+x^2 and other similar formulas in your
>>> message don't do what you think. Like + and *, the ^ operator has special
>>> meaning on the right-hand side of an R model formula. See ?Formula and
>>> perhaps read something about statistical models in R. For example:
>>>
>>>> x <- 1:93
>>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>>>> (m <- lm(y ~ x + x^2))
>>>
>>> Call:
>>> lm(formula = y ~ x + x^2)
>>>
>>> Coefficients:
>>> (Intercept)            x
>>>  -15781393       667147
>>>
>>> While gpp ~ x + I(x^2) would work, a better way to fit a raw quadratic is
>>> as gdp ~ poly(x, 2, raw=TRUE), as I suggested in my earlier message.
>>>
>>
>> My bad. Yes, I have some idea of the Wilkinson-Rogers notation. I have seen
>> it in books, although it slipped my mind that I had to use I( ).
>>
>>
>>> Finally, as to what you should do, I generally try to avoid statistical
>>> consulting by email. If you can find competent statistical help locally,
>>> such as at a nearby university, I'd recommend talking to someone about the
>>> purpose of your research and the nature of your data. If that's not
>>> possible, then others have suggested where you might find help, but to get
>>> useful advice you'll have to provide much more information about your
>>> research.
>>>
>>
>> My original query was about the polynomials used by R which I think is ON
>> topic.
> 
> I think that question was answered a while ago.
> 
>> My apologies that this query turned into a statistics query.
> 
> I don't feel the need for an apology, and although the list focuses on using R, often related statistical issues arise. On the other hand, I don't have anything more to say about your problem. Others are welcome to pick it up.
> 
> Best,
>  John
> 
>>
>>
>>> Best,
>>> John
>>>
>>>  -----------------------------
>>>  John Fox, Professor Emeritus
>>>  McMaster University
>>>  Hamilton, Ontario, Canada
>>>  Web: http::/socserv.mcmaster.ca/jfox
>>>
>>>> On Nov 28, 2019, at 12:46 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>>> wrote:
>>>>
>>>> Dear Peter and John,
>>>>
>>>> Many thanks for your prompt replies.
>>>>
>>>> Here is what I was trying to do.  I was trying to build a statistical
>>> model of a given time series using Box Jenkins methodology. The series has
>>> 93 data points. Before I analyse the ACF and PACF, I am required to
>>> de-trend the series. The series seems to have an upward trend. I wanted to
>>> find out what order polynomial should I fit the series
>>>> without overfitting.  For this I want to use orthogonal polynomials(I
>>> think someone on the internet was talking about preventing overfitting by
>>> using orthogonal polynomials) . This seems to me as a poor man's cross
>>> validation.
>>>>
>>>> So my plan is to keep increasing the degree of the orthogonal
>>> polynomials till the coefficient of the last orthogonal polynomial becomes
>>> insignificant.
>>>>
>>>> Note : If I do NOT use orthogonal polynomials, I will overfit the data
>>> set and I don't think that is a good way to detect the true order of the
>>> polynomial.
>>>>
>>>> Also now that I have detrended the series and built an ARIMA model of
>>> the residuals, now I want to forecast. For this I need to use the original
>>> polynomials and their coefficients.
>>>>
>>>> I hope I was clear and that my methodology is ok.
>>>>
>>>> I have another query here :-
>>>>
>>>> Note : If I used cross-validation to determine the order of the
>>> polynomial, I don't get a clear answer.
>>>>
>>>> See here :-
>>>> library(boot)
>>>> mydf = data.frame(cbind(gdp,x))
>>>> d<-(c(
>>>> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
>>>> print(d)
>>>> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11 4.596648e+11
>>>> ## [6] 4.980159e+11
>>>>
>>>> # Here it chooses 5. (but 4 and 5 are kind of similar).
>>>>
>>>>
>>>> d1 <- (c(
>>>> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
>>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
>>>>
>>>> print(d1)
>>>> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13 2.198675e+13
>>>> ## [6] 2.145754e+13
>>>>
>>>> # here it chooses 1 or 6
>>>>
>>>> Query : Why does it choose 1? Notice : Is this just round off noise /
>>> noise due to sampling error created by Cross Validation when it creates the
>>> K folds? Is this due to the ill conditioned model matrix?
>>>>
>>>> Best Regards,
>>>> Ashim.
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
>>>> Dear Ashim,
>>>>
>>>> Orthogonal polynomials are used because they tend to produce more
>>> accurate numerical computations, not because their coefficients are
>>> interpretable, so I wonder why you're interested in the coefficients.
>>>>
>>>> The regressors produced are orthogonal to the constant regressor and are
>>> orthogonal to each other (and in fact are orthonormal), as it's simple to
>>> demonstrate:
>>>>
>>>> ------- snip -------
>>>>
>>>>> x <- 1:93
>>>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
>>>>> (m <- lm(y ~ poly(x, 4)))
>>>>
>>>> Call:
>>>> lm(formula = y ~ poly(x, 4))
>>>>
>>>> Coefficients:
>>>> (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
>>>>   15574516    172715069     94769949     27683528      3429259
>>>>
>>>>> X <- model.matrix(m)
>>>>> head(X)
>>>>  (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>>>> 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
>>>> 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
>>>> 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
>>>> 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
>>>> 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
>>>> 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
>>>>
>>>>> zapsmall(crossprod(X))# X'X
>>>>            (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
>>>> (Intercept)          93           0           0           0           0
>>>> poly(x, 4)1           0           1           0           0           0
>>>> poly(x, 4)2           0           0           1           0           0
>>>> poly(x, 4)3           0           0           0           1           0
>>>> poly(x, 4)4           0           0           0           0           1
>>>>
>>>> ------- snip -------
>>>>
>>>> If for some not immediately obvious reason you're interested in the
>>> regression coefficients, why not just use a "raw" polynomial:
>>>>
>>>> ------- snip -------
>>>>
>>>>> (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
>>>>
>>>> Call:
>>>> lm(formula = y ~ poly(x, 4, raw = TRUE))
>>>>
>>>> Coefficients:
>>>>            (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw =
>>> TRUE)2  poly(x, 4, raw = TRUE)3
>>>>                 1.5640                   0.8985
>>> 1.0037                   1.0000
>>>> poly(x, 4, raw = TRUE)4
>>>>                 1.0000
>>>>
>>>> ------- snip -------
>>>>
>>>> These coefficients are simply interpretable but the model matrix is more
>>> poorly conditioned:
>>>>
>>>> ------- snip -------
>>>>
>>>>> head(X1)
>>>>  (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x, 4,
>>> raw = TRUE)3
>>>> 1           1                       1                       1
>>>           1
>>>> 2           1                       2                       4
>>>           8
>>>> 3           1                       3                       9
>>>          27
>>>> 4           1                       4                      16
>>>          64
>>>> 5           1                       5                      25
>>>         125
>>>> 6           1                       6                      36
>>>         216
>>>>  poly(x, 4, raw = TRUE)4
>>>> 1                       1
>>>> 2                      16
>>>> 3                      81
>>>> 4                     256
>>>> 5                     625
>>>> 6                    1296
>>>>> round(cor(X1[, -1]), 2)
>>>>                        poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
>>> poly(x, 4, raw = TRUE)3
>>>> poly(x, 4, raw = TRUE)1                    1.00                    0.97
>>>                  0.92
>>>> poly(x, 4, raw = TRUE)2                    0.97                    1.00
>>>                  0.99
>>>> poly(x, 4, raw = TRUE)3                    0.92                    0.99
>>>                  1.00
>>>> poly(x, 4, raw = TRUE)4                    0.87                    0.96
>>>                  0.99
>>>>                        poly(x, 4, raw = TRUE)4
>>>> poly(x, 4, raw = TRUE)1                    0.87
>>>> poly(x, 4, raw = TRUE)2                    0.96
>>>> poly(x, 4, raw = TRUE)3                    0.99
>>>> poly(x, 4, raw = TRUE)4                    1.00
>>>>
>>>> ------- snip -------
>>>>
>>>> The two parametrizations are equivalent, however, in that they represent
>>> the same regression surface, and so, e.g., produce the same fitted values:
>>>>
>>>> ------- snip -------
>>>>
>>>>> all.equal(fitted(m), fitted(m1))
>>>> [1] TRUE
>>>>
>>>> ------- snip -------
>>>>
>>>> Because one is usually not interested in the individual coefficients of
>>> a polynomial there usually isn't a reason to prefer one parametrization to
>>> the other on the grounds of interpretability, so why do you need to
>>> interpret the regression equation?
>>>>
>>>> I hope this helps,
>>>> John
>>>>
>>>>  -----------------------------
>>>>  John Fox, Professor Emeritus
>>>>  McMaster University
>>>>  Hamilton, Ontario, Canada
>>>>  Web: http::/socserv.mcmaster.ca/jfox
>>>>
>>>>> On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
>>> wrote:
>>>>>
>>>>> Dear Petr,
>>>>>
>>>>> Many thanks for the quick response.
>>>>>
>>>>> I also read this:-
>>>>> https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
>>>>>
>>>>> Also I read  in ?poly:-
>>>>>    The orthogonal polynomial is summarized by the coefficients, which
>>>>>    can be used to evaluate it via the three-term recursion given in
>>>>>    Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
>>>>>    of the code.
>>>>>
>>>>> I don't have access to the mentioned book.
>>>>>
>>>>> Out of curiosity, what is the name of the discrete orthogonal
>>> polynomial
>>>>> used by R ?
>>>>> What discrete measure is it orthogonal with respect to ?
>>>>>
>>>>> Many thanks,
>>>>> Ashim
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
>>> wrote:
>>>>>
>>>>>> You could get answer quickly by searching net.
>>>>>>
>>>>>>
>>>>>>
>>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
>>>>>> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
>>>>>> <
>>> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
>>>>
>>>>>>
>>>>>> Cheers
>>>>>> Petr
>>>>>>
>>>>>>> -----Original Message-----
>>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
>>> Kapoor
>>>>>>> Sent: Wednesday, November 27, 2019 12:55 PM
>>>>>>> To: R Help <r-help at r-project.org>
>>>>>>> Subject: [R] Orthogonal polynomials used by R
>>>>>>>
>>>>>>> Dear All,
>>>>>>>
>>>>>>> I have created a time trend by doing x<-1:93 because I have a time
>>> series
>>>>>>> with 93 data points. Next I did :-
>>>>>>>
>>>>>>> y = lm(series ~ poly(x,4))$residuals
>>>>>>>
>>>>>>> to detrend series.
>>>>>>>
>>>>>>> I choose this 4 as the order of my polynomial using cross validation/
>>>>>>> checking the absence of trend in the residuals so I think I have not
>>>>>> overfit
>>>>>>> this series.
>>>>>>>
>>>>>>> I wish to document the formula of poly(x,4). I am not able to find
>>> it in
>>>>>> ?poly
>>>>>>>
>>>>>>> Can someone please tell me what the formula for the orthogonal
>>>>>>> polynomial used by R is ?
>>>>>>>
>>>>>>> Thank you,
>>>>>>> Ashim
>>>>>>>
>>>>>>>     [[alternative HTML version deleted]]
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
>>>>>>> guide.html
>>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>>>
>>>>>
>>>>>      [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>>
>>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter@4567 @end|ng |rom gm@||@com  Thu Nov 28 19:15:24 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 28 Nov 2019 10:15:24 -0800
Subject: [R] Orthogonal polynomials used by R / another perspective
In-Reply-To: <9bd14545-8d5e-acc4-ad46-e2abcc0e0ed4@gmail.com>
References: <CAC8=1eo1gZG87+b-6c_1pC+d=AWDOpipiE3NY9UMbA7tOqQ9JQ@mail.gmail.com>
 <6baf7aad2b664871b94ee705e74a9aff@SRVEXCHCM1301.precheza.cz>
 <11456_1574867906_xARFIC8i013702_CAC8=1eoLn6+JzDy4vXg_+FTxs3hrxDUcO0O83BjpTzpOsyCiJg@mail.gmail.com>
 <9FDA4A53-1ED6-48EB-901B-64E8B7D892DD@mcmaster.ca>
 <CAC8=1eo6SG6-RNt7jjOYT5QmVK52fULP444kyGVsMvygKiGMaQ@mail.gmail.com>
 <66A57559-76BB-4577-868E-2CC2966181DA@mcmaster.ca>
 <6861_1574957005_xASG3Aao017128_CAC8=1epA20imTKOWFRs88Wjsg7zOWgKY__AxZtLYXZR5A0=mCQ@mail.gmail.com>
 <365344B8-2B7D-4ED1-915C-D8A3D2E05894@mcmaster.ca>
 <9bd14545-8d5e-acc4-ad46-e2abcc0e0ed4@gmail.com>
Message-ID: <CAGxFJbTQ0yhTnoK-k4g5GzJ0pOQqEefpAZVsND-8nKWhDnb3BQ@mail.gmail.com>

Warning: This may be off topic, but as several ?minences grises have now
offered comments, I recommend this striking discussion on many related
issues by yet another ?minence grise.

https://academic.oup.com/aje/article/186/6/639/3886035

**PLEASE DO NOT REPLY ON LIST**  This is not the place for a discussion of
the many issues Greenland raises. I am open to private replies, but I am
not an authority and my opinions aren't worth much. (See tagline below).

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 28, 2019 at 9:53 AM J C Nash <profjcnash at gmail.com> wrote:

> I'm not going to comment at all on the original question, but on a very
> common --
> and often troublesome -- mixing of viewpoints about data modelling.
>
> R and other software is used to "fit equations to data" and to "estimate
> models".
> Unfortunately, a good bit of both these tasks is common. Usually (but NOT
> exclusively),
> we fit by minimizing a sum of squared deviations then carry forward the
> calculations to
> make inferences about the parameters of the model equations. Quite
> frequently, a single
> equation can be written different ways e.g., ordinary or orthogonal
> polynomials.
> It gets worse, much worse, for nonlinear models.
>
> Moreover, sometimes (in my case because I get people sending me
> troublesome problems, about
> 90% of instances) there are very different sets of numerical values for
> the parameters of the
> modelling equations that give essentially the same sum of squares (or
> other loss function).
>
> Over several decades, because I am sometimes quite happy to use ANY of the
> choices, even
> when there is a linear dependence in a linear modelling equation for the
> data given, I'm
> often granted a lot of nasty comments. If I'm using the FIT e.g. for
> approximation, then such
> criticism is mis-placed. If I want to use the particular parameters to say
> something about
> the system I'm studying, then indeed I should go back to school (my
> critics might say
> kindergarten). A serious concern about some machine learning is that fit
> alone is used
> as a criteria from which to predict using the "equations" (though some
> models are just
> algorithms). There is a jump from a good fit to existing data to a hope
> that we can get
> good predictions outside of the available data space.
>
> Having taught statistics for several decades also, I know how difficult it
> is to impart
> a good perspective on these issues. However, I'll continue to urge data
> scientists and
> statisticians to keep a wide view and be clear on what they want the
> software to do for
> them.
>
> For now, end of rant.
>
> John Nash (package author of several packages for fitting and optimizing)
>
>
> On 2019-11-28 12:12 p.m., Fox, John wrote:
> > Dear Ashim,
> >
> > Please see my brief remarks below:
> >
> >> On Nov 28, 2019, at 11:02 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> wrote:
> >>
> >> On Thu, Nov 28, 2019 at 7:38 PM Fox, John <jfox at mcmaster.ca> wrote:
> >>
> >>> Dear Ashim,
> >>>
> >>> I'm afraid that much of what you say here is confused.
> >>>
> >>> First, because poly(x) and poly(x, raw=TRUE) produce the same fitted
> >>> values (as I previously explained), they also produce the same
> residuals,
> >>> and consequently the same CV criteria. From the point of view of CV,
> >>> there's therefore no reason to prefer orthogonal polynomials. And you
> still
> >>> don't explain why you want to interpret the coefficients of the
> polynomial.
> >>>
> >>
> >> The trend in the variable that I am trying to create an ARIMA model for
> is
> >> given by poly(x,4). That is why I wished to know what these polynomials
> >> look like.
> >
> > The polynomial "looks" exactly the same whether or not you use raw or
> orthogonal regressors as a basis for it. That is, the two bases represent
> exactly the same regression surface (i.e., curve in the case of one x). To
> see what the fitted polynomial looks like, graph it. But I've now made
> essentially this point three times, so if it's not clear I regret the
> unclarity but I don't really have anything to add.
> >
> > For other points, see below.
> >
> >>
> >> I used  :
> >>
> >> trend <- predict(lm(gdp~poly(x,4)),newdata = data.frame(
> >> x=94:103),interval="confidence")
> >>
> >> and I was able to (numerically) extrapolate the poly(x,4) trend,
> although,
> >> I think it would be interesting to know what polynomials I was dealing
> with
> >> in this case. Just some intuition as to if the linear / quadratic /
> cubic /
> >> fourth order polynomial trend is dominating. I don't know how I would
> >> interpret them, but it would be fun to know.
> >
> > I'm not sure how you intend to interpret the coefficients, say of the
> raw polynomial. Their magnitudes shouldn't be compared because the size of
> the powers of x grows with the powers.
> >
> > BTW, it's very risky to use high-order polynomials for extrapolation
> beyond the observed range of x, even if the model fits well within the
> observed range of x, and of course raw and orthogonal polynomial produce
> exactly the same (problematic) extrapolations (although those produced by
> raw polynomials may be subject to more rounding error). To be clear, I'm
> not arguing that one should in general use raw polynomials in preference to
> orthogonal polynomials, just that the former have generally interpretable
> coefficients and the latter don't.
> >
> >>
> >> Please allow me to show you a trick. I read this on the internet, here
> :-
> >>
> >>
> https://www.datasciencecentral.com/profiles/blogs/deep-dive-into-polynomial-regression-and-overfitting
> >>
> >> Please see the LAST comment by Scott Stelljes where he suggests using an
> >> orthogonal polynomial basis. He does not elaborate buttoleaves the
> reader to
> >> work out the details.
> >
> > This blog focuses on the numerical stability of raw versus orthogonal
> polynomials. If by "stepwise" you mean adding successive powers to the
> model, you'll get exactly the same sequence of fits with raw as with
> orthogonal polynomial, as I've now explained several times.
> >
> >>
> >> Here is what I think of this. Take a big number say 20 and take a
> variable
> >> in which we are trying to find the order of the polynomial in the trend.
> >> Like this :-
> >>
> >>> summary(lm(gdp ~ poly(x,20)))
> >>
> >> Call:
> >> lm(formula = gdp ~ poly(x, 20))
> >>
> >> Residuals:
> >>     Min       1Q   Median       3Q      Max
> >> -1235661  -367798   -80453   240360  1450906
> >>
> >> Coefficients:
> >>               Estimate Std. Error t value Pr(>|t|)
> >> (Intercept)    17601482      66934 262.968  < 2e-16 ***
> >> poly(x, 20)1  125679081     645487 194.704  < 2e-16 ***
> >> poly(x, 20)2   43108747     645487  66.785  < 2e-16 ***
> >> poly(x, 20)3    3605839     645487   5.586 3.89e-07 ***
> >> poly(x, 20)4   -2977277     645487  -4.612 1.69e-05 ***
> >> poly(x, 20)5    1085732     645487   1.682   0.0969 .
> >> poly(x, 20)6    1124125     645487   1.742   0.0859 .
> >> poly(x, 20)7    -108676     645487  -0.168   0.8668
> >> poly(x, 20)8    -976915     645487  -1.513   0.1345
> >> poly(x, 20)9   -1635444     645487  -2.534   0.0135 *
> >> poly(x, 20)10   -715019     645487  -1.108   0.2717
> >> poly(x, 20)11    347102     645487   0.538   0.5924
> >> poly(x, 20)12   -176728     645487  -0.274   0.7850
> >> poly(x, 20)13   -634151     645487  -0.982   0.3292
> >> poly(x, 20)14   -537725     645487  -0.833   0.4076
> >> poly(x, 20)15    -58674     645487  -0.091   0.9278
> >> poly(x, 20)16    -67030     645487  -0.104   0.9176
> >> poly(x, 20)17   -809443     645487  -1.254   0.2139
> >> poly(x, 20)18   -668879     645487  -1.036   0.3036
> >> poly(x, 20)19   -302384     645487  -0.468   0.6409
> >> poly(x, 20)20    359134     645487   0.556   0.5797
> >> ---
> >> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>
> >> Residual standard error: 645500 on 72 degrees of freedom
> >> Multiple R-squared:  0.9983, Adjusted R-squared:  0.9978
> >> F-statistic:  2122 on 20 and 72 DF,  p-value: < 2.2e-16
> >>
> >>>
> >>
> >>
> >> The CV estimate of the trend is 4. I am not saying this method is
> perfect,
> >> but look above this method also suggests n=4.
> >>
> >> I CANNOT do this with raw polynomials, since they are correlated and
> >> JOINTLY in the presence of others they may not be significant, please
> see
> >> below.
> >>
> >>> summary(lm(gdp ~ poly(x,20,raw=T)))
> >>
> >> Call:
> >> lm(formula = gdp ~ poly(x, 20, raw = T))
> >>
> >> Residuals:
> >>     Min       1Q   Median       3Q      Max
> >> -1286007  -372221   -81320   248510  1589130
> >>
> >> Coefficients: (4 not defined because of singularities)
> >>                         Estimate Std. Error t value Pr(>|t|)
> >> (Intercept)             2.067e+06  2.649e+06   0.780    0.438
> >> poly(x, 20, raw = T)1   1.633e+06  3.556e+06   0.459    0.647
> >> poly(x, 20, raw = T)2  -7.601e+05  1.679e+06  -0.453    0.652
> >> poly(x, 20, raw = T)3   1.861e+05  3.962e+05   0.470    0.640
> >> poly(x, 20, raw = T)4  -2.634e+04  5.480e+04  -0.481    0.632
> >> poly(x, 20, raw = T)5   2.370e+03  4.854e+03   0.488    0.627
> >> poly(x, 20, raw = T)6  -1.434e+02  2.906e+02  -0.493    0.623
> >> poly(x, 20, raw = T)7   6.022e+00  1.213e+01   0.496    0.621
> >> poly(x, 20, raw = T)8  -1.784e-01  3.587e-01  -0.497    0.620
> >> poly(x, 20, raw = T)9   3.727e-03  7.503e-03   0.497    0.621
> >> poly(x, 20, raw = T)10 -5.373e-05  1.086e-04  -0.495    0.622
> >> poly(x, 20, raw = T)11  5.016e-07  1.018e-06   0.493    0.624
> >> poly(x, 20, raw = T)12 -2.483e-09  5.069e-09  -0.490    0.626
> >> poly(x, 20, raw = T)13         NA         NA      NA       NA
> >> poly(x, 20, raw = T)14  5.656e-14  1.167e-13   0.485    0.629
> >> poly(x, 20, raw = T)15         NA         NA      NA       NA
> >> poly(x, 20, raw = T)16 -1.933e-18  4.011e-18  -0.482    0.631
> >> poly(x, 20, raw = T)17         NA         NA      NA       NA
> >> poly(x, 20, raw = T)18  5.181e-23  1.076e-22   0.482    0.631
> >> poly(x, 20, raw = T)19         NA         NA      NA       NA
> >> poly(x, 20, raw = T)20 -7.173e-28  1.480e-27  -0.485    0.629
> >>
> >> Residual standard error: 641000 on 76 degrees of freedom
> >> Multiple R-squared:  0.9982, Adjusted R-squared:  0.9979
> >> F-statistic:  2690 on 16 and 76 DF,  p-value: < 2.2e-16
> >>
> >>>
> >>
> >> Note,however, once the orthogonal polynomials have suggested a number,
> 4 in
> >> this case, I can do this :-
> >>
> >> summary(lm(gdp ~ poly(x,4,raw=T)))
> >>
> >> Call:
> >> lm(formula = gdp ~ poly(x, 4, raw = T))
> >>
> >> Residuals:
> >>     Min       1Q   Median       3Q      Max
> >> -1278673  -424315   -22357   310977  1731813
> >>
> >> Coefficients:
> >>                       Estimate Std. Error t value Pr(>|t|)
> >> (Intercept)           3.022e+06  3.676e+05   8.220 1.64e-12 ***
> >> poly(x, 4, raw = T)1  1.741e+05  5.357e+04   3.249  0.00164 **
> >> poly(x, 4, raw = T)2 -6.434e+03  2.300e+03  -2.797  0.00633 **
> >> poly(x, 4, raw = T)3  1.878e+02  3.667e+01   5.123 1.76e-06 ***
> >> poly(x, 4, raw = T)4 -8.682e-01  1.935e-01  -4.486 2.19e-05 ***
> >> ---
> >> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >>
> >> Residual standard error: 663700 on 88 degrees of freedom
> >> Multiple R-squared:  0.9978, Adjusted R-squared:  0.9977
> >> F-statistic: 1.003e+04 on 4 and 88 DF,  p-value: < 2.2e-16
> >>
> >>>
> >>
> >> Although due to correlations they may not be significant jointly, but in
> >> this case all 4 powers come out significant.
> >
> > Yes, the coefficients of orthogonal polynomials permit stepwise tests of
> each term after the previous ones because the orthogonalization is done
> stepwise. But (1) interpreting these tests is problematic because, e.g., of
> issues of simultaneous inference, and (2) you're using CV for model
> selection anyway (aren't you?) and you'll get (once more) exactly the same
> CV results from raw and orthogonal polynomials.
> >
> >>
> >>
> >> Second, the model formula gdp~1+x+x^2 and other similar formulas in your
> >>> message don't do what you think. Like + and *, the ^ operator has
> special
> >>> meaning on the right-hand side of an R model formula. See ?Formula and
> >>> perhaps read something about statistical models in R. For example:
> >>>
> >>>> x <- 1:93
> >>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> >>>> (m <- lm(y ~ x + x^2))
> >>>
> >>> Call:
> >>> lm(formula = y ~ x + x^2)
> >>>
> >>> Coefficients:
> >>> (Intercept)            x
> >>>  -15781393       667147
> >>>
> >>> While gpp ~ x + I(x^2) would work, a better way to fit a raw quadratic
> is
> >>> as gdp ~ poly(x, 2, raw=TRUE), as I suggested in my earlier message.
> >>>
> >>
> >> My bad. Yes, I have some idea of the Wilkinson-Rogers notation. I have
> seen
> >> it in books, although it slipped my mind that I had to use I( ).
> >>
> >>
> >>> Finally, as to what you should do, I generally try to avoid statistical
> >>> consulting by email. If you can find competent statistical help
> locally,
> >>> such as at a nearby university, I'd recommend talking to someone about
> the
> >>> purpose of your research and the nature of your data. If that's not
> >>> possible, then others have suggested where you might find help, but to
> get
> >>> useful advice you'll have to provide much more information about your
> >>> research.
> >>>
> >>
> >> My original query was about the polynomials used by R which I think is
> ON
> >> topic.
> >
> > I think that question was answered a while ago.
> >
> >> My apologies that this query turned into a statistics query.
> >
> > I don't feel the need for an apology, and although the list focuses on
> using R, often related statistical issues arise. On the other hand, I don't
> have anything more to say about your problem. Others are welcome to pick it
> up.
> >
> > Best,
> >  John
> >
> >>
> >>
> >>> Best,
> >>> John
> >>>
> >>>  -----------------------------
> >>>  John Fox, Professor Emeritus
> >>>  McMaster University
> >>>  Hamilton, Ontario, Canada
> >>>  Web: http::/socserv.mcmaster.ca/jfox
> >>>
> >>>> On Nov 28, 2019, at 12:46 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> >>> wrote:
> >>>>
> >>>> Dear Peter and John,
> >>>>
> >>>> Many thanks for your prompt replies.
> >>>>
> >>>> Here is what I was trying to do.  I was trying to build a statistical
> >>> model of a given time series using Box Jenkins methodology. The series
> has
> >>> 93 data points. Before I analyse the ACF and PACF, I am required to
> >>> de-trend the series. The series seems to have an upward trend. I
> wanted to
> >>> find out what order polynomial should I fit the series
> >>>> without overfitting.  For this I want to use orthogonal polynomials(I
> >>> think someone on the internet was talking about preventing overfitting
> by
> >>> using orthogonal polynomials) . This seems to me as a poor man's cross
> >>> validation.
> >>>>
> >>>> So my plan is to keep increasing the degree of the orthogonal
> >>> polynomials till the coefficient of the last orthogonal polynomial
> becomes
> >>> insignificant.
> >>>>
> >>>> Note : If I do NOT use orthogonal polynomials, I will overfit the data
> >>> set and I don't think that is a good way to detect the true order of
> the
> >>> polynomial.
> >>>>
> >>>> Also now that I have detrended the series and built an ARIMA model of
> >>> the residuals, now I want to forecast. For this I need to use the
> original
> >>> polynomials and their coefficients.
> >>>>
> >>>> I hope I was clear and that my methodology is ok.
> >>>>
> >>>> I have another query here :-
> >>>>
> >>>> Note : If I used cross-validation to determine the order of the
> >>> polynomial, I don't get a clear answer.
> >>>>
> >>>> See here :-
> >>>> library(boot)
> >>>> mydf = data.frame(cbind(gdp,x))
> >>>> d<-(c(
> >>>> cv.glm(data = mydf,glm(gdp~x),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~poly(x,2)),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~poly(x,3)),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~poly(x,4)),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~poly(x,5)),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~poly(x,6)),K=10)$delta[1]))
> >>>> print(d)
> >>>> ## [1] 2.178574e+13 7.303031e+11 5.994783e+11 4.943586e+11
> 4.596648e+11
> >>>> ## [6] 4.980159e+11
> >>>>
> >>>> # Here it chooses 5. (but 4 and 5 are kind of similar).
> >>>>
> >>>>
> >>>> d1 <- (c(
> >>>> cv.glm(data = mydf,glm(gdp~1+x),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~1+x+x^2),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5),K=10)$delta[1],
> >>>> cv.glm(data = mydf,glm(gdp~1+x+x^2+x^3+x^4+x^5+x^6),K=10)$delta[1]))
> >>>>
> >>>> print(d1)
> >>>> ## [1] 2.149647e+13 2.253999e+13 2.182175e+13 2.177170e+13
> 2.198675e+13
> >>>> ## [6] 2.145754e+13
> >>>>
> >>>> # here it chooses 1 or 6
> >>>>
> >>>> Query : Why does it choose 1? Notice : Is this just round off noise /
> >>> noise due to sampling error created by Cross Validation when it
> creates the
> >>> K folds? Is this due to the ill conditioned model matrix?
> >>>>
> >>>> Best Regards,
> >>>> Ashim.
> >>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>> On Wed, Nov 27, 2019 at 10:41 PM Fox, John <jfox at mcmaster.ca> wrote:
> >>>> Dear Ashim,
> >>>>
> >>>> Orthogonal polynomials are used because they tend to produce more
> >>> accurate numerical computations, not because their coefficients are
> >>> interpretable, so I wonder why you're interested in the coefficients.
> >>>>
> >>>> The regressors produced are orthogonal to the constant regressor and
> are
> >>> orthogonal to each other (and in fact are orthonormal), as it's simple
> to
> >>> demonstrate:
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>>> x <- 1:93
> >>>>> y <- 1 + x + x^2 + x^3 + x^4 + rnorm(93)
> >>>>> (m <- lm(y ~ poly(x, 4)))
> >>>>
> >>>> Call:
> >>>> lm(formula = y ~ poly(x, 4))
> >>>>
> >>>> Coefficients:
> >>>> (Intercept)  poly(x, 4)1  poly(x, 4)2  poly(x, 4)3  poly(x, 4)4
> >>>>   15574516    172715069     94769949     27683528      3429259
> >>>>
> >>>>> X <- model.matrix(m)
> >>>>> head(X)
> >>>>  (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> >>>> 1           1  -0.1776843   0.2245083  -0.2572066  0.27935949
> >>>> 2           1  -0.1738216   0.2098665  -0.2236579  0.21862917
> >>>> 3           1  -0.1699589   0.1955464  -0.1919525  0.16390514
> >>>> 4           1  -0.1660962   0.1815482  -0.1620496  0.11487597
> >>>> 5           1  -0.1622335   0.1678717  -0.1339080  0.07123722
> >>>> 6           1  -0.1583708   0.1545171  -0.1074869  0.03269145
> >>>>
> >>>>> zapsmall(crossprod(X))# X'X
> >>>>            (Intercept) poly(x, 4)1 poly(x, 4)2 poly(x, 4)3 poly(x, 4)4
> >>>> (Intercept)          93           0           0           0
>  0
> >>>> poly(x, 4)1           0           1           0           0
>  0
> >>>> poly(x, 4)2           0           0           1           0
>  0
> >>>> poly(x, 4)3           0           0           0           1
>  0
> >>>> poly(x, 4)4           0           0           0           0
>  1
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>> If for some not immediately obvious reason you're interested in the
> >>> regression coefficients, why not just use a "raw" polynomial:
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>>> (m1 <- lm(y ~ poly(x, 4, raw=TRUE)))
> >>>>
> >>>> Call:
> >>>> lm(formula = y ~ poly(x, 4, raw = TRUE))
> >>>>
> >>>> Coefficients:
> >>>>            (Intercept)  poly(x, 4, raw = TRUE)1  poly(x, 4, raw =
> >>> TRUE)2  poly(x, 4, raw = TRUE)3
> >>>>                 1.5640                   0.8985
> >>> 1.0037                   1.0000
> >>>> poly(x, 4, raw = TRUE)4
> >>>>                 1.0000
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>> These coefficients are simply interpretable but the model matrix is
> more
> >>> poorly conditioned:
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>>> head(X1)
> >>>>  (Intercept) poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2 poly(x,
> 4,
> >>> raw = TRUE)3
> >>>> 1           1                       1                       1
> >>>           1
> >>>> 2           1                       2                       4
> >>>           8
> >>>> 3           1                       3                       9
> >>>          27
> >>>> 4           1                       4                      16
> >>>          64
> >>>> 5           1                       5                      25
> >>>         125
> >>>> 6           1                       6                      36
> >>>         216
> >>>>  poly(x, 4, raw = TRUE)4
> >>>> 1                       1
> >>>> 2                      16
> >>>> 3                      81
> >>>> 4                     256
> >>>> 5                     625
> >>>> 6                    1296
> >>>>> round(cor(X1[, -1]), 2)
> >>>>                        poly(x, 4, raw = TRUE)1 poly(x, 4, raw = TRUE)2
> >>> poly(x, 4, raw = TRUE)3
> >>>> poly(x, 4, raw = TRUE)1                    1.00
> 0.97
> >>>                  0.92
> >>>> poly(x, 4, raw = TRUE)2                    0.97
> 1.00
> >>>                  0.99
> >>>> poly(x, 4, raw = TRUE)3                    0.92
> 0.99
> >>>                  1.00
> >>>> poly(x, 4, raw = TRUE)4                    0.87
> 0.96
> >>>                  0.99
> >>>>                        poly(x, 4, raw = TRUE)4
> >>>> poly(x, 4, raw = TRUE)1                    0.87
> >>>> poly(x, 4, raw = TRUE)2                    0.96
> >>>> poly(x, 4, raw = TRUE)3                    0.99
> >>>> poly(x, 4, raw = TRUE)4                    1.00
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>> The two parametrizations are equivalent, however, in that they
> represent
> >>> the same regression surface, and so, e.g., produce the same fitted
> values:
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>>> all.equal(fitted(m), fitted(m1))
> >>>> [1] TRUE
> >>>>
> >>>> ------- snip -------
> >>>>
> >>>> Because one is usually not interested in the individual coefficients
> of
> >>> a polynomial there usually isn't a reason to prefer one
> parametrization to
> >>> the other on the grounds of interpretability, so why do you need to
> >>> interpret the regression equation?
> >>>>
> >>>> I hope this helps,
> >>>> John
> >>>>
> >>>>  -----------------------------
> >>>>  John Fox, Professor Emeritus
> >>>>  McMaster University
> >>>>  Hamilton, Ontario, Canada
> >>>>  Web: http::/socserv.mcmaster.ca/jfox
> >>>>
> >>>>> On Nov 27, 2019, at 10:17 AM, Ashim Kapoor <ashimkapoor at gmail.com>
> >>> wrote:
> >>>>>
> >>>>> Dear Petr,
> >>>>>
> >>>>> Many thanks for the quick response.
> >>>>>
> >>>>> I also read this:-
> >>>>> https://en.wikipedia.org/wiki/Discrete_orthogonal_polynomials
> >>>>>
> >>>>> Also I read  in ?poly:-
> >>>>>    The orthogonal polynomial is summarized by the coefficients, which
> >>>>>    can be used to evaluate it via the three-term recursion given in
> >>>>>    Kennedy & Gentle (1980, pp. 343-4), and used in the ?predict? part
> >>>>>    of the code.
> >>>>>
> >>>>> I don't have access to the mentioned book.
> >>>>>
> >>>>> Out of curiosity, what is the name of the discrete orthogonal
> >>> polynomial
> >>>>> used by R ?
> >>>>> What discrete measure is it orthogonal with respect to ?
> >>>>>
> >>>>> Many thanks,
> >>>>> Ashim
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>> On Wed, Nov 27, 2019 at 6:11 PM PIKAL Petr <petr.pikal at precheza.cz>
> >>> wrote:
> >>>>>
> >>>>>> You could get answer quickly by searching net.
> >>>>>>
> >>>>>>
> >>>>>>
> >>>
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-p
> >>>>>> olynomials-how-to-understand-the-coefs-ret/39051154#39051154
> >>>>>> <
> >>>
> https://stackoverflow.com/questions/39031172/how-poly-generates-orthogonal-polynomials-how-to-understand-the-coefs-ret/39051154#39051154
> >>>>
> >>>>>>
> >>>>>> Cheers
> >>>>>> Petr
> >>>>>>
> >>>>>>> -----Original Message-----
> >>>>>>> From: R-help <r-help-bounces at r-project.org> On Behalf Of Ashim
> >>> Kapoor
> >>>>>>> Sent: Wednesday, November 27, 2019 12:55 PM
> >>>>>>> To: R Help <r-help at r-project.org>
> >>>>>>> Subject: [R] Orthogonal polynomials used by R
> >>>>>>>
> >>>>>>> Dear All,
> >>>>>>>
> >>>>>>> I have created a time trend by doing x<-1:93 because I have a time
> >>> series
> >>>>>>> with 93 data points. Next I did :-
> >>>>>>>
> >>>>>>> y = lm(series ~ poly(x,4))$residuals
> >>>>>>>
> >>>>>>> to detrend series.
> >>>>>>>
> >>>>>>> I choose this 4 as the order of my polynomial using cross
> validation/
> >>>>>>> checking the absence of trend in the residuals so I think I have
> not
> >>>>>> overfit
> >>>>>>> this series.
> >>>>>>>
> >>>>>>> I wish to document the formula of poly(x,4). I am not able to find
> >>> it in
> >>>>>> ?poly
> >>>>>>>
> >>>>>>> Can someone please tell me what the formula for the orthogonal
> >>>>>>> polynomial used by R is ?
> >>>>>>>
> >>>>>>> Thank you,
> >>>>>>> Ashim
> >>>>>>>
> >>>>>>>     [[alternative HTML version deleted]]
> >>>>>>>
> >>>>>>> ______________________________________________
> >>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>>> PLEASE do read the posting guide http://www.R-project.org/posting-
> >>>>>>> guide.html
> >>>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>>
> >>>>>
> >>>>>      [[alternative HTML version deleted]]
> >>>>>
> >>>>> ______________________________________________
> >>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>> PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>
> >>>
> >>>
> >>>
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @ew@@hm @end|ng |rom gm@||@com  Fri Nov 29 00:16:39 2019
From: @ew@@hm @end|ng |rom gm@||@com (Ashta)
Date: Thu, 28 Nov 2019 17:16:39 -0600
Subject: [R] remove a row
Message-ID: <CADDFq32uK=JRnvjibbrEpTmBpKisbaobuiC5FCi4q5w13ZxUeQ@mail.gmail.com>

Hi all,  I want to remove a row based on a condition in one of the
variables from a data frame.
When we split this string it should be composed of 3-2- 5 format (3
digits numeric, 2 characters and 5 digits  numeric).  Like
area code -region-numeric. The max length of the area code should be
3, the  max length of region be should be 2,  followed by a max length
of  5  numeric digits.  The are code  can  be 1 digit, or 2 digits or
3 digits  but not more than three digits.  So  the  max length of this
variable is 10.  Anything outside of this pattern should be excluded.
As an example

dat <-read.table(text=" rown  varx
1   9F209
2  FL250
3  2F250
4  102250
5  102FL
6   102
7  1212FL250
8  121FL50",header=TRUE,stringsAsFactors=F)

1  9F209           # keep
2  FL250           # remove, no area code
3   2F250          # keep
4  102250         # remove , no region code
5  102FL           # remove , no numeric after region code
6   102              # remove ,  no region code and numeric
7  1212FL250  #remove, area code is more than three digits
8  121FL50      # Keep

The desired output should be
1   9F209
3   2F250
8  121FL50

How do I do this in an efficient way?

Thank you in advance


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov 29 02:31:19 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 28 Nov 2019 17:31:19 -0800
Subject: [R] remove a row
In-Reply-To: <CADDFq32uK=JRnvjibbrEpTmBpKisbaobuiC5FCi4q5w13ZxUeQ@mail.gmail.com>
References: <CADDFq32uK=JRnvjibbrEpTmBpKisbaobuiC5FCi4q5w13ZxUeQ@mail.gmail.com>
Message-ID: <CAGxFJbQ1aREXWh9ZPC9PYL5+7neN5tYinmV+cj1vgkCXDbWs5g@mail.gmail.com>

Use regular expressions.

See ?regexp  and ?grep

Using your example:

> grep("^[[:digit:]]{1,3}[[:alpha:]]{1,2}[[:digit:]]{1,5}$",dat$varx,value
= TRUE)
[1] "9F209"   "2F250"   "121FL50"

Cheers,
Bert

Bert Gunter

"The trouble with having an open mind is that people keep coming along and
sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Nov 28, 2019 at 3:17 PM Ashta <sewashm at gmail.com> wrote:

> Hi all,  I want to remove a row based on a condition in one of the
> variables from a data frame.
> When we split this string it should be composed of 3-2- 5 format (3
> digits numeric, 2 characters and 5 digits  numeric).  Like
> area code -region-numeric. The max length of the area code should be
> 3, the  max length of region be should be 2,  followed by a max length
> of  5  numeric digits.  The are code  can  be 1 digit, or 2 digits or
> 3 digits  but not more than three digits.  So  the  max length of this
> variable is 10.  Anything outside of this pattern should be excluded.
> As an example
>
> dat <-read.table(text=" rown  varx
> 1   9F209
> 2  FL250
> 3  2F250
> 4  102250
> 5  102FL
> 6   102
> 7  1212FL250
> 8  121FL50",header=TRUE,stringsAsFactors=F)
>
> 1  9F209           # keep
> 2  FL250           # remove, no area code
> 3   2F250          # keep
> 4  102250         # remove , no region code
> 5  102FL           # remove , no numeric after region code
> 6   102              # remove ,  no region code and numeric
> 7  1212FL250  #remove, area code is more than three digits
> 8  121FL50      # Keep
>
> The desired output should be
> 1   9F209
> 3   2F250
> 8  121FL50
>
> How do I do this in an efficient way?
>
> Thank you in advance
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From @ew@@hm @end|ng |rom gm@||@com  Fri Nov 29 02:57:58 2019
From: @ew@@hm @end|ng |rom gm@||@com (Ashta)
Date: Thu, 28 Nov 2019 19:57:58 -0600
Subject: [R] remove a row
In-Reply-To: <CAGxFJbQ1aREXWh9ZPC9PYL5+7neN5tYinmV+cj1vgkCXDbWs5g@mail.gmail.com>
References: <CADDFq32uK=JRnvjibbrEpTmBpKisbaobuiC5FCi4q5w13ZxUeQ@mail.gmail.com>
 <CAGxFJbQ1aREXWh9ZPC9PYL5+7neN5tYinmV+cj1vgkCXDbWs5g@mail.gmail.com>
Message-ID: <CADDFq329yd7Zwpn-wjHma5NeQjBz-WvGTCOazr+QnmWuB9E_AA@mail.gmail.com>

Thank you so much Bert.

Is it possible to split the varx into  three ( area code, region and
the numeric part)as a separate variable

On Thu, Nov 28, 2019 at 7:31 PM Bert Gunter <bgunter.4567 at gmail.com> wrote:
>
> Use regular expressions.
>
> See ?regexp  and ?grep
>
> Using your example:
>
> > grep("^[[:digit:]]{1,3}[[:alpha:]]{1,2}[[:digit:]]{1,5}$",dat$varx,value = TRUE)
> [1] "9F209"   "2F250"   "121FL50"
>
> Cheers,
> Bert
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Nov 28, 2019 at 3:17 PM Ashta <sewashm at gmail.com> wrote:
>>
>> Hi all,  I want to remove a row based on a condition in one of the
>> variables from a data frame.
>> When we split this string it should be composed of 3-2- 5 format (3
>> digits numeric, 2 characters and 5 digits  numeric).  Like
>> area code -region-numeric. The max length of the area code should be
>> 3, the  max length of region be should be 2,  followed by a max length
>> of  5  numeric digits.  The are code  can  be 1 digit, or 2 digits or
>> 3 digits  but not more than three digits.  So  the  max length of this
>> variable is 10.  Anything outside of this pattern should be excluded.
>> As an example
>>
>> dat <-read.table(text=" rown  varx
>> 1   9F209
>> 2  FL250
>> 3  2F250
>> 4  102250
>> 5  102FL
>> 6   102
>> 7  1212FL250
>> 8  121FL50",header=TRUE,stringsAsFactors=F)
>>
>> 1  9F209           # keep
>> 2  FL250           # remove, no area code
>> 3   2F250          # keep
>> 4  102250         # remove , no region code
>> 5  102FL           # remove , no numeric after region code
>> 6   102              # remove ,  no region code and numeric
>> 7  1212FL250  #remove, area code is more than three digits
>> 8  121FL50      # Keep
>>
>> The desired output should be
>> 1   9F209
>> 3   2F250
>> 8  121FL50
>>
>> How do I do this in an efficient way?
>>
>> Thank you in advance
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From bgunter@4567 @end|ng |rom gm@||@com  Fri Nov 29 04:24:50 2019
From: bgunter@4567 @end|ng |rom gm@||@com (Bert Gunter)
Date: Thu, 28 Nov 2019 19:24:50 -0800
Subject: [R] remove a row
In-Reply-To: <CADDFq329yd7Zwpn-wjHma5NeQjBz-WvGTCOazr+QnmWuB9E_AA@mail.gmail.com>
References: <CADDFq32uK=JRnvjibbrEpTmBpKisbaobuiC5FCi4q5w13ZxUeQ@mail.gmail.com>
 <CAGxFJbQ1aREXWh9ZPC9PYL5+7neN5tYinmV+cj1vgkCXDbWs5g@mail.gmail.com>
 <CADDFq329yd7Zwpn-wjHma5NeQjBz-WvGTCOazr+QnmWuB9E_AA@mail.gmail.com>
Message-ID: <CAGxFJbQLFM0f7XxBzb2HTM++XgKQ+O6KCEZEa-HZ+u5F7v9Yyg@mail.gmail.com>

Of course! Use regexec() and regmatches()

>
regmatches(dat$varx,regexec("(^[[:digit:]]{1,3})([[:alpha:]]{1,2})([[:digit:]]{1,5}$)",dat$varx))
[[1]]
[1] "9F209" "9"     "F"     "209"

[[2]]
character(0)

[[3]]
[1] "2F250" "2"     "F"     "250"

[[4]]
character(0)

[[5]]
character(0)

[[6]]
character(0)

[[7]]
character(0)

[[8]]
[1] "121FL50" "121"     "FL"      "50"

The list components are character(0) for no match, otherwise a character
vector with the whole text entry first, then the 1st, 2nd, and 3rd strings
matching the 1st, 2nd, and 3rd parenthesized subexpressions of the pattern.
These correspond to area code, region code, and your 3rd numeric of course.
I leave it to you to extract what you want from this list, e.g via lapply().

For details, see the Help pages for the two functions.

-- Bert

	[[alternative HTML version deleted]]


