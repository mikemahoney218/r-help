From alexandra.hua at yale.edu  Tue Dec  1 03:44:45 2015
From: alexandra.hua at yale.edu (Alexandra Hua)
Date: Mon, 30 Nov 2015 21:44:45 -0500
Subject: [R] Graphing a subset of data
Message-ID: <CADoPJut5vjp35v5HTo=V8skJAhdtszg+PpH-hrdXcfTPr9whFg@mail.gmail.com>

I am trying to write a function that will graph a variable of a dataset or
a variable from a subset of the data. My function is as follows:

graphics<-function(dat, var, graph, varname, val, subset){
  if(subset==1) {
    data<-mySubset(dat=dat,varname=varname,val=val)
  }else if(subset!=1){
    data<-(dat)
  }
  if(graph==1) {
    with(data, boxplot(var), main = paste("Vertical box plot of",
myfunc(dat), "variable", myfunc(var), xlab = myfunc(var)))
  }else if(graph==2){
    with(data, boxplot(var), horizontal=TRUE, main = paste("Horizontal
box plot of", myfunc(dat), "variable", myfunc(var), xlab =
myfunc(var)))
  }else if(graph==3){
    hist(var, main="Histogram of", myfunc(var))
  }}

I included "subset" as a parameter for the function, so that subset=1 would
tell the function to subset, and any other value will use the full dataset.
However, when I run the function with the following expression (bolts is
the dataset, SPEED1 is the variable, value=3)

graphics(bolts, bolts$SPEED1, graph=3, bolts$SPEED1, 3, 1)

I receive this error message: Error in eval(substitute(expr), data, enclos
= parent.frame()) : invalid 'envir' argument of type 'logical'

Does anyone know why this is, or if there is something wrong with my code?
Thanks!

-- 
*Alexandra Hua *
Yale University | MPH Candidate Class of 2016
Chronic Disease Epidemiology

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Dec  1 04:15:19 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 30 Nov 2015 19:15:19 -0800
Subject: [R] Graphing a subset of data
In-Reply-To: <CADoPJut5vjp35v5HTo=V8skJAhdtszg+PpH-hrdXcfTPr9whFg@mail.gmail.com>
References: <CADoPJut5vjp35v5HTo=V8skJAhdtszg+PpH-hrdXcfTPr9whFg@mail.gmail.com>
Message-ID: <D0762336-1CB5-465B-A487-C48EEC6D70EE@comcast.net>

Dear Alex (as you are signing yourself on StackOverflow);

It is considered poor manners to cross-post identical questions in multiple venues. 

http://stackoverflow.com/questions/34011669/graphing-function-based-on-subset-of-data

You should choose one or the other of SO and Rhelp. If you do not get a satisfying answer in your first choice, you should wait an appropriate number of hours before posting at the other venue. And when you do end up cross-posting, your should stated where else the question was asked so that potential respondents can check to see if you have already gotten an answer.

You should also read the the posting guide where it is clearly stated the rhelp responders expect that you include a dataset built with R code for purposes of illustration.

> On Nov 30, 2015, at 6:44 PM, Alexandra Hua <alexandra.hua at yale.edu> wrote:
> 
> I am trying to write a function that will graph a variable of a dataset or
> a variable from a subset of the data. My function is as follows:
> 
> graphics<-function(dat, var, graph, varname, val, subset){
>  if(subset==1) {
>    data<-mySubset(dat=dat,varname=varname,val=val)
>  }else if(subset!=1){
>    data<-(dat)
>  }
>  if(graph==1) {
>    with(data, boxplot(var), main = paste("Vertical box plot of",
> myfunc(dat), "variable", myfunc(var), xlab = myfunc(var)))
>  }else if(graph==2){
>    with(data, boxplot(var), horizontal=TRUE, main = paste("Horizontal
> box plot of", myfunc(dat), "variable", myfunc(var), xlab =
> myfunc(var)))
>  }else if(graph==3){
>    hist(var, main="Histogram of", myfunc(var))
>  }}
> 

Generally the use of `with` inside functions is ill-advised. Sometimes it succeeds but nmany times it will fail with puzzling error messages.


> I included "subset" as a parameter for the function, so that subset=1 would
> tell the function to subset, and any other value will use the full dataset.
> However, when I run the function with the following expression (bolts is
> the dataset, SPEED1 is the variable, value=3)
> 
> graphics(bolts, bolts$SPEED1, graph=3, bolts$SPEED1, 3, 1)
> 
> I receive this error message: Error in eval(substitute(expr), data, enclos
> = parent.frame()) : invalid 'envir' argument of type 'logical'
> 
> Does anyone know why this is, or if there is something wrong with my code?
> Thanks!
> 
> -- 
> *Alexandra Hua *
> Yale University | MPH Candidate Class of 2016
> Chronic Disease Epidemiology
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From leptostracan at yahoo.com  Tue Dec  1 07:20:44 2015
From: leptostracan at yahoo.com (Christine Lee)
Date: Tue, 1 Dec 2015 06:20:44 +0000 (UTC)
Subject: [R] filled circle with a black line on the rim in pch function
References: <456635539.12433643.1448950844348.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <456635539.12433643.1448950844348.JavaMail.yahoo@mail.yahoo.com>

Dear All,

I have an embarssing question, I want to put a black line as a rim on the grey symbol in the xyplot, to no avail.
.  I thought it was easy, by changing the pch code from 16 to 21.  I was surpised that I ran into difficulty.
  
My original script is as follows:
library(lattice)
xyplot(Abun~Date1|Station, data=Raw,
        groups = culr,
        par.settings = list(strip.background = list(col = "transparent"),
                            superpose.symbol = list(cex = rep(2, 2),
                                                    col=c("grey","black"),
                                                    pch = rep(16,2))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Abundance",cex=1.5),  
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1))


I have changed pch number into 21, the symbols did show a black rim, but the filled circle colours became blue and pink, instead of the designated grey and black.  This puzzles me.  

My data is as follows:
Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L, 
2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 
10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L, 
5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10", 
"1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", 
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = "factor"), 
    Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"), 
    Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
    2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
    3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I"
    ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92, 
    2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33, 
    0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67, 
    0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92, 
    1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697, 
    16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768, 
    16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737, 
    16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737, 
    16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737, 
    16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769
    ), class = "Date")), .Names = c("Date", "Year", "Station", 
"Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame")

Can any one help me please?

With best regards,
Christine


From drjimlemon at gmail.com  Tue Dec  1 09:01:27 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 1 Dec 2015 19:01:27 +1100
Subject: [R] filled circle with a black line on the rim in pch function
In-Reply-To: <456635539.12433643.1448950844348.JavaMail.yahoo@mail.yahoo.com>
References: <456635539.12433643.1448950844348.JavaMail.yahoo.ref@mail.yahoo.com>
	<456635539.12433643.1448950844348.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fWuW4vJpc95eY=z-Krk-UR0tY4Zjsxn2uTK=KyA8xeYyA@mail.gmail.com>

Hi Christine,
When I try to run your script, the plot fails:

Error in eval(expr, envir, enclos) : object 'culr' not found
> names(Raw)
[1] "Date"    "Year"    "Station" "Abun"    "Date1"

so I changed the second line to:

groups=Year,

and it did work. The default (pink, gray) background colors for the symbols
do appear when the symbol is changed to pch=21. The arguments

col="black",bg=c("gray","black"),

would produce the symbols you want in base graphics, but do not seem to do
so in lattice. I do get sort of what you want by modifying your code pretty
radically:

xyplot(Abun~Date1|Station, data=Raw,
        groups = Year,
        par.settings = list(strip.background = list(col = "transparent")),
        type="p",
        pch = rep(21,2),
        col="black",
        fill=c("gray","black"),
        xlab=list("Month",cex=1.5),
        ylab=list("Abundance",cex=1.5),
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1))

Jim


On Tue, Dec 1, 2015 at 5:20 PM, Christine Lee via R-help <
r-help at r-project.org> wrote:

> Dear All,
>
> I have an embarssing question, I want to put a black line as a rim on the
> grey symbol in the xyplot, to no avail.
> .  I thought it was easy, by changing the pch code from 16 to 21.  I was
> surpised that I ran into difficulty.
>
> My original script is as follows:
> library(lattice)
> xyplot(Abun~Date1|Station, data=Raw,
>         groups = culr,
>         par.settings = list(strip.background = list(col = "transparent"),
>                             superpose.symbol = list(cex = rep(2, 2),
>                                                     col=c("grey","black"),
>                                                     pch = rep(16,2))),
>         type="p",
>         xlab=list("Month",cex=1.5),
>         ylab=list("Abundance",cex=1.5),
>         index.cond=list(c(1,2,3,4)),
>         auto.key = T,
>         layout=c(4,1))
>
>
> I have changed pch number into 21, the symbols did show a black rim, but
> the filled circle colours became blue and pink, instead of the designated
> grey and black.  This puzzles me.
>
> My data is as follows:
> Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L,
> 2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L,
> 10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L,
> 5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10",
> "1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9",
> "26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
> "factor"),
>     Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class =
> "factor"),
>     Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
>     2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I"
>     ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92,
>     2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33,
>     0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67,
>     0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92,
>     1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697,
>     16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768,
>     16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737,
>     16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,
>     16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,
>     16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769
>     ), class = "Date")), .Names = c("Date", "Year", "Station",
> "Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame")
>
> Can any one help me please?
>
> With best regards,
> Christine
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From leptostracan at yahoo.com  Tue Dec  1 09:19:54 2015
From: leptostracan at yahoo.com (Christine Lee)
Date: Tue, 1 Dec 2015 08:19:54 +0000 (UTC)
Subject: [R] =?utf-8?b?5Zue6KaG77iwICBmaWxsZWQgY2lyY2xlIHdpdGggYSBibGFj?=
 =?utf-8?q?k_line_on_the_rim_in_pch_function?=
In-Reply-To: <CA+8X3fWuW4vJpc95eY=z-Krk-UR0tY4Zjsxn2uTK=KyA8xeYyA@mail.gmail.com>
References: <456635539.12433643.1448950844348.JavaMail.yahoo.ref@mail.yahoo.com>
	<456635539.12433643.1448950844348.JavaMail.yahoo@mail.yahoo.com>
	<CA+8X3fWuW4vJpc95eY=z-Krk-UR0tY4Zjsxn2uTK=KyA8xeYyA@mail.gmail.com>
Message-ID: <495496469.12506923.1448957994844.JavaMail.yahoo@mail.yahoo.com>

Sorry all,?My mistake!? The?missing parts are?as follows:?Raw$Date1<-as.Date(Raw$Date,"%d/%m")
culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014")
library(lattice)?Many thanks.? ?With best regards,Christine 


    Jim Lemon <drjimlemon at gmail.com> ? 2015?12?1? (??) 4:01 PM ???
 

 Hi Christine,When I try to run your script, the plot fails:
Error in eval(expr, envir, enclos) : object 'culr' not found> names(Raw)[1] "Date" ? ?"Year" ? ?"Station" "Abun" ? ?"Date1" ?
so I changed the second line to:
groups=Year,
and it did work. The default (pink, gray) background colors for the symbols do appear when the symbol is changed to pch=21. The arguments?
col="black",bg=c("gray","black"),
would produce the symbols you want in base graphics, but do not seem to do so in lattice. I do get sort of what you want by modifying your code pretty radically:
xyplot(Abun~Date1|Station, data=Raw,? ? ? ? groups = Year,? ? ? ? par.settings = list(strip.background = list(col = "transparent")),? ? ? ? type="p",? ? ? ? pch = rep(21,2),? ? ? ? col="black",? ? ? ? fill=c("gray","black"),? ? ? ? xlab=list("Month",cex=1.5),? ? ? ? ylab=list("Abundance",cex=1.5),? ? ? ? index.cond=list(c(1,2,3,4)),? ? ? ? auto.key = T,? ? ? ? layout=c(4,1))
Jim

On Tue, Dec 1, 2015 at 5:20 PM, Christine Lee via R-help <r-help at r-project.org> wrote:

Dear All,

I have an embarssing question, I want to put a black line as a rim on the grey symbol in the xyplot, to no avail.
.? I thought it was easy, by changing the pch code from 16 to 21.? I was surpised that I ran into difficulty.

My original script is as follows:
library(lattice)
xyplot(Abun~Date1|Station, data=Raw,
? ? ? ? groups = culr,
? ? ? ? par.settings = list(strip.background = list(col = "transparent"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? superpose.symbol = list(cex = rep(2, 2),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? col=c("grey","black"),
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? pch = rep(16,2))),
? ? ? ? type="p",
? ? ? ? xlab=list("Month",cex=1.5),
? ? ? ? ylab=list("Abundance",cex=1.5),
? ? ? ? index.cond=list(c(1,2,3,4)),
? ? ? ? auto.key = T,
? ? ? ? layout=c(4,1))


I have changed pch number into 21, the symbols did show a black rim, but the filled circle colours became blue and pink, instead of the designated grey and black.? This puzzles me.

My data is as follows:
Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L,
2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L,
10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L,
5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10",
"1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9",
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = "factor"),
? ? Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
? ? 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
? ? 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
? ? 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"),
? ? Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
? ? 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
? ? 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
? ? 3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I"
? ? ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92,
? ? 2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33,
? ? 0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67,
? ? 0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92,
? ? 1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697,
? ? 16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768,
? ? 16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737,
? ? 16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,
? ? 16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,
? ? 16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769
? ? ), class = "Date")), .Names = c("Date", "Year", "Station",
"Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame")

Can any one help me please?

With best regards,
Christine

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




  
	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Tue Dec  1 11:28:52 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 1 Dec 2015 21:28:52 +1100
Subject: [R] Graphing a subset of data
In-Reply-To: <D0762336-1CB5-465B-A487-C48EEC6D70EE@comcast.net>
References: <CADoPJut5vjp35v5HTo=V8skJAhdtszg+PpH-hrdXcfTPr9whFg@mail.gmail.com>
	<D0762336-1CB5-465B-A487-C48EEC6D70EE@comcast.net>
Message-ID: <CA+8X3fVC_=maxb2zX1Q8xR5sOyyX5adR4RX7fmJiHPXKQ57dXw@mail.gmail.com>

Hi Alexandra,
I think you are going about this in an excessively difficult way. Here is a
rough example:

graphit<-function(x,var,type,subset=NA,...){
 if(!is.na(subset[1])) x<-subset(x,subset)
 do.call(type,list(x=x[[var]],...))
}
# assume that the data are measurements of penetration by crossbow bolts at
specified speeds
bolts<-data.frame(SPEED1=rep(1:3,each=30),
 penetration=c(runif(30,10,15),runif(30,15,20),runif(30,20,25)))
# note the lazy way of adding arguments to the function call
graphit(bolts,"SPEED1",boxplot,main="Boxplot of SPEED1")
graphit(bolts,"penetration",boxplot,subset=bolts$SPEED1==3,
 main="Boxplot of SPEED1 == 3")
graphit(bolts,"SPEED1",hist,main="Histogram of SPEED1")

Jim

	[[alternative HTML version deleted]]


From villarino.ernesto at gmail.com  Tue Dec  1 11:41:59 2015
From: villarino.ernesto at gmail.com (Ernesto Villarino)
Date: Tue, 1 Dec 2015 11:41:59 +0100
Subject: [R] Metanalysis in R using MAVIS
Message-ID: <CAAmrVFpttyU34wRNXhcFa=6nW7OmB51cE0J=_WceSdqQJzNxRA@mail.gmail.com>

I am Ernesto, a Phd student from Bilbao working with climate related
plankton dynamics. I am trying to perform a MAVIS metanalysis (any other
meta techcnique to be applied in R) to see if there is a significant
difference between the moderators in my dataset. Is there any option to do
an ANOVA analysis using MAVIS among groups?



In the Moderator (subgroup) analysis we have one variable per study,
instead of having two (M1,M2).



*Study*

*Response variable*

*N*

*Moderator*

Study-01

-0.1111

8

Dispersing

Study-02

-0.2557

8

Dispersing

Study-03

0.06667

4

Zooplankton

Study-04

0.1956

5

Phytoplankton

Study-05

0.025

5

Phytoplankton

Study-06

0.7768

4

Phytoplankton

Study-07

0.3511

6

Dispersing

Study-08

-0.09821

6

Dispersing

Study-09

0.4286

3

NDL

Study-10

0.5638

7

Dispersing

 ?

 ?

 ?

 ?





I appreciate your help,

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Tue Dec  1 12:31:46 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 1 Dec 2015 11:31:46 +0000
Subject: [R] Metanalysis in R using MAVIS
In-Reply-To: <CAAmrVFpttyU34wRNXhcFa=6nW7OmB51cE0J=_WceSdqQJzNxRA@mail.gmail.com>
References: <CAAmrVFpttyU34wRNXhcFa=6nW7OmB51cE0J=_WceSdqQJzNxRA@mail.gmail.com>
Message-ID: <565D8522.8000905@dewey.myzen.co.uk>

Dear Ernesto

As far as I understand it MAVIS is an application which uses metafor to 
perform the actual meta-analysis. I do not use MAVIS but if it gives you 
access to the full range of metafor facilities then you have a range of 
options with moderators. If MAVIS does not expose all of metafor for you 
then you can always write the commands needed in metafor.

Unfortunately you posted in HTML so your data is unreadable, this is a 
plain text mailing list.

On 01/12/2015 10:41, Ernesto Villarino wrote:
> I am Ernesto, a Phd student from Bilbao working with climate related
> plankton dynamics. I am trying to perform a MAVIS metanalysis (any other
> meta techcnique to be applied in R) to see if there is a significant
> difference between the moderators in my dataset. Is there any option to do
> an ANOVA analysis using MAVIS among groups?
>
>
>
> In the Moderator (subgroup) analysis we have one variable per study,
> instead of having two (M1,M2).
>
>
>
> *Study*
>
> *Response variable*
>
> *N*
>
> *Moderator*
>
> Study-01
>
> -0.1111
>
> 8
>
> Dispersing
>
> Study-02
>
> -0.2557
>
> 8
>
> Dispersing
>
> Study-03
>
> 0.06667
>
> 4
>
> Zooplankton
>
> Study-04
>
> 0.1956
>
> 5
>
> Phytoplankton
>
> Study-05
>
> 0.025
>
> 5
>
> Phytoplankton
>
> Study-06
>
> 0.7768
>
> 4
>
> Phytoplankton
>
> Study-07
>
> 0.3511
>
> 6
>
> Dispersing
>
> Study-08
>
> -0.09821
>
> 6
>
> Dispersing
>
> Study-09
>
> 0.4286
>
> 3
>
> NDL
>
> Study-10
>
> 0.5638
>
> 7
>
> Dispersing
>
>   ?
>
>   ?
>
>   ?
>
>   ?
>
>
>
>
>
> I appreciate your help,
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From sstoline at gmail.com  Tue Dec  1 12:56:00 2015
From: sstoline at gmail.com (Steven Stoline)
Date: Tue, 1 Dec 2015 06:56:00 -0500
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
Message-ID: <CAHDp66ATut2sb+DPusVZRp-B9YXMm47turtpW24NSVJHPvqGfw@mail.gmail.com>

Dear Bill:

It looks great. many thanks


*one more quick help:*


how to graph only the x and y axis crossing through the origin, with
xlim=c(-1,1,0.2) and ylim=c(0,1,0.2)?


with many thanks
steve






On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
> is at 0, but you want it to be at -4.  You should turn this into a function
> so you don't have to remember how the variables in your code depend
> on one another.   E.g.,
>
> showIntegral <- function (f, xmin, xmax, n = 16)
> {
>     curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>     abline(h = 0)
>     dx <- (xmax - xmin)/n
>     right <- xmin + (1:n) * dx
>     left <- right - dx
>     mid <- right - dx/2
>     fm <- f(mid)
>     rect(left, 0, right, fm, density = 20, border = "red")
>     points(mid, fm, col = "red", cex = 1.25, pch = 19)
>     sum(fm * dx)
> }
> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
> [1] 42.5
> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
> [1] 42.66602
> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
> [1] 42.66663
>
> > 2*4^3/3
> [1] 42.66667
> > showIntegral
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
> wrote:
> > Dear Peter: in my previous email I forgot to reply to the list too
> >
> > I used your code for more than one examples, and it works nicely. But
> when
> > I tried to use for the the function: f(x) = x^2, it looks like I am
> missing
> > something, but I could not figured it out.
> >
> > This what I used:
> >
> >
> >
> > f <- function(x) x^2
> >
> > curve(f(x), from=-4, to=4, lwd=2, col="blue")
> > abline(h=0)
> > n <- 16
> > dx <- 8/n
> > right <- (1:n)*dx
> > left <- right - dx
> > mid <- right - dx/2
> > fm <- f(mid)
> > rect(left,0,right,fm, density = 20, border = "red")
> > points(mid, fm, col = "red", cex = 1.25, pch=19)
> > sum(fm*dx)
> >
> >
> >
> > 1/3 * (64+64)
> >
> >
> >
> > with many thanks
> > steve
> >
> > On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
> wrote:
> >
> >> many thanks
> >>
> >> steve
> >>
> >> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
> wrote:
> >>
> >>> Something like this?
> >>>
> >>> f <- function(x) x^3-2*x
> >>> curve(f(x), from=0, to=4)
> >>> abline(h=0)
> >>> n <- 16
> >>> dx <- 4/n
> >>> right <- (1:n)*dx
> >>> left <- right - dx
> >>> mid <- right - dx/2
> >>> fm <- f(mid)
> >>> points(mid, fm)
> >>> rect(left,0,right,fm)
> >>>
> >>> sum(fm*dx)
> >>>
> >>> 1/4 * 4^4 - 4^2
> >>>
> >>>
> >>> -pd
> >>>
> >>>
> >>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com> wrote:
> >>>
> >>> > Dear All:
> >>> >
> >>> > I am trying to explain to my students how to calculate the definite
> >>> > integral using the Riemann sum. Can someone help me to graph the area
> >>> under
> >>> > the curve of the function, showing the curve as well as the
> rectangles
> >>> > between 0 and 4..
> >>> >
> >>> > *f(x) = x^3 - 2*x *
> >>> >
> >>> > over the interval [0 , 4]
> >>> >
> >>> >
> >>> >
> >>> > with many thanks
> >>> > steve
> >>> >
> >>> > --
> >>> > Steven M. Stoline
> >>> > 1123 Forest Avenue
> >>> > Portland, ME 04112
> >>> > sstoline at gmail.com
> >>> >
> >>> >       [[alternative HTML version deleted]]
> >>> >
> >>> > ______________________________________________
> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>> > PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> > and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> --
> >>> Peter Dalgaard, Professor,
> >>> Center for Statistics, Copenhagen Business School
> >>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >>> Phone: (+45)38153501
> >>> Office: A 4.23
> >>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>
> >>
> >> --
> >> Steven M. Stoline
> >> 1123 Forest Avenue
> >> Portland, ME 04112
> >> sstoline at gmail.com
> >>
> >
> >
> >
> > --
> > Steven M. Stoline
> > 1123 Forest Avenue
> > Portland, ME 04112
> > sstoline at gmail.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From aoife.m.doherty at gmail.com  Tue Dec  1 14:51:39 2015
From: aoife.m.doherty at gmail.com (aoife doherty)
Date: Tue, 1 Dec 2015 13:51:39 +0000
Subject: [R] convert.snp.ped in GenAbel
Message-ID: <CAAsJanbA-y4cxVqwuY58xwzS+C-yxd4zNsyv9HS48HGiynATCw@mail.gmail.com>

Having a problem with convert.snp.ped in GenABEL (on linux).

My map file looks like this (tab-separated) (called Genotype.map)


chrom name position

13   SNP1   39703523


My ped file looks like this (Called Genotype.ped)

PF1 PF1 0 0 2 -9 C C

PF2 PF2 0 0 2 -9 C A

PF3 PF3 0 0 1 -9 C C

PF4 PF4 0 0 1 -9 C C


The commands I'm using are:

> library("GenABEL")

> data <-convert.snp.ped(pedfile="~/Genotype.ped"
,mapfile="~/Genotype.map", outfile="~/Genotype.out")


The error I get is:

Error in convert.snp.ped(pedfile = "~/Genotype.ped", : could not open file
'~/Genotype.map' !


Any idea? It's in the same directory as the ped file, so I don't know why
it can find the ped file and not the map file?


Thanks

	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Tue Dec  1 15:00:52 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 1 Dec 2015 14:00:52 +0000
Subject: [R] Graphing a subset of data
In-Reply-To: <CA+8X3fVC_=maxb2zX1Q8xR5sOyyX5adR4RX7fmJiHPXKQ57dXw@mail.gmail.com>
References: <CADoPJut5vjp35v5HTo=V8skJAhdtszg+PpH-hrdXcfTPr9whFg@mail.gmail.com>
	<D0762336-1CB5-465B-A487-C48EEC6D70EE@comcast.net>
	<CA+8X3fVC_=maxb2zX1Q8xR5sOyyX5adR4RX7fmJiHPXKQ57dXw@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403C96CC408@GBTEDVPEXCMB04.corp.lgc-group.com>

> graphit<-function(x,var,type,subset=NA,...){
>  if(!is.na(subset[1])) x<-subset(x,subset)
>  do.call(type,list(x=x[[var]],...))
> }

A further slight simplification is possible using the fact that subset(x, TRUE) returns x unchanged:

graphit<-function(x,var,type,subset=TRUE,...){
  x<-subset(x,subset)
  do.call(type,list(x=x[[var]],...))
}

S Ellison


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From dulcalma at bigpond.com  Tue Dec  1 15:03:20 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Wed, 2 Dec 2015 00:03:20 +1000
Subject: [R] filled circle with a black line on the rim in pch function
In-Reply-To: <CA+8X3fWuW4vJpc95eY=z-Krk-UR0tY4Zjsxn2uTK=KyA8xeYyA@mail.gmail.com>
References: <456635539.12433643.1448950844348.JavaMail.yahoo.ref@mail.yahoo.com>	<456635539.12433643.1448950844348.JavaMail.yahoo@mail.yahoo.com>
	<CA+8X3fWuW4vJpc95eY=z-Krk-UR0tY4Zjsxn2uTK=KyA8xeYyA@mail.gmail.com>
Message-ID: <000601d12c41$09502a00$1bf07e00$@bigpond.com>

This seems to get the colours for the legend correctly

xyplot(Abun~Date1|Station, data=Raw,
        groups = Year,
        par.settings = list(strip.background = list(col = "transparent"),
                                          superpose.symbol = list(pch =
rep(21,2),
                                               col=c(1,1),
                                               fill=c("gray","black"))),
        type="p",
        xlab=list("Month",cex=1.5),
        ylab=list("Abundance",cex=1.5),
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1))

If you use auto.key things get a bit tricky and need values from
trellis.par.set() ie par.settings within xyplot

Regards

Duncan

Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au





-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jim Lemon
Sent: Tuesday, 1 December 2015 18:01
To: Christine Lee
Cc: r-help mailing list
Subject: Re: [R] filled circle with a black line on the rim in pch function

Hi Christine,
When I try to run your script, the plot fails:

Error in eval(expr, envir, enclos) : object 'culr' not found
> names(Raw)
[1] "Date"    "Year"    "Station" "Abun"    "Date1"

so I changed the second line to:

groups=Year,

and it did work. The default (pink, gray) background colors for the symbols
do appear when the symbol is changed to pch=21. The arguments

col="black",bg=c("gray","black"),

would produce the symbols you want in base graphics, but do not seem to do
so in lattice. I do get sort of what you want by modifying your code pretty
radically:

xyplot(Abun~Date1|Station, data=Raw,
        groups = Year,
        par.settings = list(strip.background = list(col = "transparent")),
        type="p",
        pch = rep(21,2),
        col="black",
        fill=c("gray","black"),
        xlab=list("Month",cex=1.5),
        ylab=list("Abundance",cex=1.5),
        index.cond=list(c(1,2,3,4)),
        auto.key = T,
        layout=c(4,1))

Jim


On Tue, Dec 1, 2015 at 5:20 PM, Christine Lee via R-help <
r-help at r-project.org> wrote:

> Dear All,
>
> I have an embarssing question, I want to put a black line as a rim on the
> grey symbol in the xyplot, to no avail.
> .  I thought it was easy, by changing the pch code from 16 to 21.  I was
> surpised that I ran into difficulty.
>
> My original script is as follows:
> library(lattice)
> xyplot(Abun~Date1|Station, data=Raw,
>         groups = culr,
>         par.settings = list(strip.background = list(col = "transparent"),
>                             superpose.symbol = list(cex = rep(2, 2),
>                                                     col=c("grey","black"),
>                                                     pch = rep(16,2))),
>         type="p",
>         xlab=list("Month",cex=1.5),
>         ylab=list("Abundance",cex=1.5),
>         index.cond=list(c(1,2,3,4)),
>         auto.key = T,
>         layout=c(4,1))
>
>
> I have changed pch number into 21, the symbols did show a black rim, but
> the filled circle colours became blue and pink, instead of the designated
> grey and black.  This puzzles me.
>
> My data is as follows:
> Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L,
> 2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L,
> 10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L,
> 5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10",
> "1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9",
> "26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class =
> "factor"),
>     Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
>     1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class =
> "factor"),
>     Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,
>     2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,
>     3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I"
>     ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92,
>     2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33,
>     0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67,
>     0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92,
>     1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697,
>     16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768,
>     16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737,
>     16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,
>     16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,
>     16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769
>     ), class = "Date")), .Names = c("Date", "Year", "Station",
> "Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame")
>
> Can any one help me please?
>
> With best regards,
> Christine
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From lists at dewey.myzen.co.uk  Tue Dec  1 15:34:57 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Tue, 1 Dec 2015 14:34:57 +0000
Subject: [R] convert.snp.ped in GenAbel
In-Reply-To: <CAAsJanbA-y4cxVqwuY58xwzS+C-yxd4zNsyv9HS48HGiynATCw@mail.gmail.com>
References: <CAAsJanbA-y4cxVqwuY58xwzS+C-yxd4zNsyv9HS48HGiynATCw@mail.gmail.com>
Message-ID: <565DB011.8080605@dewey.myzen.co.uk>

Dear Aoife

I do not know the insides of GenABEL but it may be looking for the map 
file first so in fact may not be able to find the ped file either.

1 - Are you sure the files are named as you state?
2 - Are you sure they are placed in ~ which seems an unlikely place to 
put them.

On 01/12/2015 13:51, aoife doherty wrote:
> Having a problem with convert.snp.ped in GenABEL (on linux).
>
> My map file looks like this (tab-separated) (called Genotype.map)
>
>
> chrom name position
>
> 13   SNP1   39703523
>
>
> My ped file looks like this (Called Genotype.ped)
>
> PF1 PF1 0 0 2 -9 C C
>
> PF2 PF2 0 0 2 -9 C A
>
> PF3 PF3 0 0 1 -9 C C
>
> PF4 PF4 0 0 1 -9 C C
>
>
> The commands I'm using are:
>
>> library("GenABEL")
>
>> data <-convert.snp.ped(pedfile="~/Genotype.ped"
> ,mapfile="~/Genotype.map", outfile="~/Genotype.out")
>
>
> The error I get is:
>
> Error in convert.snp.ped(pedfile = "~/Genotype.ped", : could not open file
> '~/Genotype.map' !
>
>
> Any idea? It's in the same directory as the ped file, so I don't know why
> it can find the ped file and not the map file?
>
>
> Thanks
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From Bastien.Ferland-Raymond at mffp.gouv.qc.ca  Tue Dec  1 19:01:54 2015
From: Bastien.Ferland-Raymond at mffp.gouv.qc.ca (Bastien.Ferland-Raymond at mffp.gouv.qc.ca)
Date: Tue, 1 Dec 2015 13:01:54 -0500
Subject: [R] Coding your Secret Santa in R!
Message-ID: <C7B438E60ECB004081D1410F788C62E52C2288103D@HUARD.intranet.MRN.GOUV>

Hello Everyone!

Christmas is coming and with it, gift exchange!  Every year, with my family, we draw names from a hat to decide who gives a gift to who.  Very basic and annoying method, as it doesn't prevent somebody to draw himself, to draw his/her partner, to draw years after years the same person and it forces to either have everybody at the same place at the same time to do the draw or have somebody to manage everything which with break the fun for him/her. 

This year, I decided it was time to upgrade and enter the 2.0 era for secret santa, I've coded it in R!

The principle is simple.  You enter the people names, the draw restrictions and the program randomly picks everyone secret santa and send them a email to tell them.  R is so great...

If you're interested, here is my code.  It's probably not optimal but it still works.  Part of the comments are in french, sorry about that.

Merry Christmas!
Bastien

####


####  code du tirage au sort pour les cadeaux de noel

###  set working directory
setwd("U:\\Dropbox\\Gestion familiale\\tirage No?l Lombardo")

### load required package (only if you want to send emails)
library(sendmailR)

### set the year (use later a little bit, could be more useful)
an <- 2015

### write a vector of all participants
#participants.2014 <- c("Bastien","Isa","Cath","Rob","Matt","Sylvie","John","Myriam","Yolande","Mike", "Audrey")    # if you want history
participants.2015 <- c("Bastien","Isa","Cath","Rob","Matt","Sylvie","John")

participants <- participants.2015       ## The one to use this year

###  If you want the code to send email, make a named list of the email address of participants
list.email <- c(Bastien="<bastien111111 at yandex.com>", Isa="<isabelle111111 at gmail.com>",
                John="<john111111 at gmail.com>", Sylvie="<sylvie111111 at hotmail.com>",
                Cath="<lomb111111 at gmail.com>", Rob="<rp111111 at gmail.com>",
                Matt="<matt111111 at gmail.com>")


###  You can add restrictions, i.e. people who can't give to other people.  Create as many as you want,
###  They are on the form of 2 columns matrix with the first column being the giver and the second column the receiver
###  In this case, there is 3 kinds of restrictions: 
###    1) you don't want to draw yourself
###    2) you don't want to draw your partner, girlfriend or boyfriend
###    3) you don't want to draw the same person as last year 

#1)
restiction.soismeme <- cbind(giver=participants,receiver=participants)                         

#2)
restriction.couple <- matrix(c("Bastien","Isa","Cath","Rob","Sylvie", "John","Mike","Audrey"),4,2,byrow=T)

#3) (restriction 2014 read on my hard drive last years restrictions, will not work on your computer)
#restriction.2013 <- matrix(c("Bastien","Sylvie", "Isa", "Bastien", "Matt", "Yolande","Rob","John","Cath","Rob"),5,2,byrow=T)
restriction.2014 <- cbind(unlist(strsplit(list.files("2014"),".txt")),as.character(unlist(sapply(list.files("2014", full.names=T),read.table))))

##  then you append (rbind) all the restrictions, the order matters!
restrictions <- rbind(restriction.couple,restriction.couple[,2:1],restiction.soismeme,restriction.2014)


###  I created a simple function validating the draw (making sure the draw isn't in the restrictions
###  this function is use latter in a "while" loop
valide.res <- function(paires, restric){
	any(apply(restric, 1, function(xx) all(paires==xx)))
}


###  Draw people as long as you have a restriction in the results
res=T
while(res==T){
tirage <- cbind(giver=sample(participants,length(participants)),receiver=sample(participants,length(participants)))
res <- any(apply(tirage,1,valide.res,restrictions))
}



###  This loop is run to output the draw results
###  It does 2 things:
###   1) save a text file named with the giver's name which contains the receiver's name 
###   2) send an email to the giver with the body of the message being the receiver's name
for(i in 1:nrow(tirage)){ 
  # 1) write text file
    write.table(tirage[i,"receiver"],file=paste0(an,"\\",tirage[i,"giver"],".txt"), quote=F,row.names=F, col.names=F)     
  # 2) send an email
    body <- list(paste0("Voici le r?sultat du tirage pour l'?change de cadeaux ", an, "!","  Vous avez pig? : "),
                 paste0("*** ",tirage[i,"receiver"]," ***"),
                 paste0("Bravo! et Joyeux No?l!"))
    sendmail("<bastien111111 at yandex.com>", list.email[[tirage[i,"giver"]]], "Secret Santa des Lombardo!", body, control=list(smtpServer="relais.videotron.ca"))
}


###  It's all done!


From b.j.chen at unsw.edu.au  Tue Dec  1 15:27:02 2015
From: b.j.chen at unsw.edu.au (Bei Jun Chen)
Date: Tue, 1 Dec 2015 22:27:02 +0800
Subject: [R] cummeRbund MDSplot errors
Message-ID: <565DAE36.8020101@unsw.edu.au>

Dear R mailing list members,


I was trying to make MDS plots using cummeRbund package on cuffdiff 
output files, but got some error messages instead.  I would really 
appreciate it if you could advise me on how to solve the problem.

I am running Ubuntu 15.10, R version 3.2.2, cummeRbund version 2.13.  My 
code and error messages are as follows:

 > library("cummeRbund")
 > cuff <- readCufflinks()
 > MDS <- MDSplot(genes(cuff))
Error in cmdscale(d, eig = TRUE, k = 2) :
   'k' must be in {1, 2, ..  n - 1}
 > MDS <- MDSplot(genes(cuff), replicates=T)
Error in sqliteSendQuery(con, statement, bind.data) :
   error in statement: near "from": syntax error

I've checked the cummeRbund manual and couldn't find clues on how to fix 
the problem.  Again I'd appreciate it if you could help.  Thank you in 
advance.

With regards,

Bei Jun


From Eli.Ateljevich at water.ca.gov  Tue Dec  1 18:58:43 2015
From: Eli.Ateljevich at water.ca.gov (Ateljevich, Eli@DWR)
Date: Tue, 1 Dec 2015 17:58:43 +0000
Subject: [R] disparate data collections and resolutions in a GAM
Message-ID: <73F8F665C0FCF549AF963069AB8CB27F8779D7@057-SN2MPN1-043.057d.mgd.msft.net>

Hi,
I have point values of elevations on land (high resolution lidar) and in the water (some are lower resolution single beam soundings or even just prior elevation maps, others are high res multibeam). Let's say high resolution is 1m and low is 10m, although the coarse case can be worse.

>From this data I want to produce two smoothed datasets, one at 2m resolution where it is justified by the data and the other at 10m. Everywhere there is a 2m map there will be a corresponding 10m map, but not vice versa.

To do this in a mutually compatible way, I envision producing a GAM that does this:
1. partition the surface into variations at higher and lower frequencies, so that the 2m map could be considered the sum of a 10m general shape of the channel plus a zero mean higher frequency fluctuation due to features. The partition could be imperfect ... I'm sure frequencies will bleed and the terrain is inherently anisotropic (channels with long length scales in the downstream direction).
2. somehow deal with the fact that the data come from different collections, and are likely to be different in terms of bias, variance and point density. I'd be willing to call one of the datasets "true" and declare a collection effect for the others, but it would only be identifiable in a narrow region.

Any recommendations? I am most familiar with mgcv but flexible on approach. The GAM with tensors splines in the alongstream and cross-stream direction have worked well for us at 10m in similar terrain without the added twist.

Thanks!


	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Dec  1 20:36:44 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 1 Dec 2015 11:36:44 -0800
Subject: [R] cummeRbund MDSplot errors
In-Reply-To: <565DAE36.8020101@unsw.edu.au>
References: <565DAE36.8020101@unsw.edu.au>
Message-ID: <CAGxFJbRoe7F4LcvswnWHsmp4Smu23L7qptv9NitRY8qq0NSH4Q@mail.gmail.com>

This is a Bioconductor package. So I think you would do better to post
on the Bioconductor list, not here.


Cheers,
Bert
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Dec 1, 2015 at 6:27 AM, Bei Jun Chen <b.j.chen at unsw.edu.au> wrote:
> Dear R mailing list members,
>
>
> I was trying to make MDS plots using cummeRbund package on cuffdiff output
> files, but got some error messages instead.  I would really appreciate it if
> you could advise me on how to solve the problem.
>
> I am running Ubuntu 15.10, R version 3.2.2, cummeRbund version 2.13.  My
> code and error messages are as follows:
>
>> library("cummeRbund")
>> cuff <- readCufflinks()
>> MDS <- MDSplot(genes(cuff))
> Error in cmdscale(d, eig = TRUE, k = 2) :
>   'k' must be in {1, 2, ..  n - 1}
>> MDS <- MDSplot(genes(cuff), replicates=T)
> Error in sqliteSendQuery(con, statement, bind.data) :
>   error in statement: near "from": syntax error
>
> I've checked the cummeRbund manual and couldn't find clues on how to fix the
> problem.  Again I'd appreciate it if you could help.  Thank you in advance.
>
> With regards,
>
> Bei Jun
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Dec  1 20:40:30 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 1 Dec 2015 11:40:30 -0800
Subject: [R] disparate data collections and resolutions in a GAM
In-Reply-To: <73F8F665C0FCF549AF963069AB8CB27F8779D7@057-SN2MPN1-043.057d.mgd.msft.net>
References: <73F8F665C0FCF549AF963069AB8CB27F8779D7@057-SN2MPN1-043.057d.mgd.msft.net>
Message-ID: <CAGxFJbQX8vRkf8cG-umpbu6uD-gp62=8omcM7LKdN8yxCK-DtA@mail.gmail.com>

This is a general purpose R programming help list. Your post appears
to be a very specific, subject matter question that should go to the
R-Sig-geo list, where you are likely to get better and prompter
responses.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Dec 1, 2015 at 9:58 AM, Ateljevich, Eli at DWR
<Eli.Ateljevich at water.ca.gov> wrote:
> Hi,
> I have point values of elevations on land (high resolution lidar) and in the water (some are lower resolution single beam soundings or even just prior elevation maps, others are high res multibeam). Let's say high resolution is 1m and low is 10m, although the coarse case can be worse.
>
> >From this data I want to produce two smoothed datasets, one at 2m resolution where it is justified by the data and the other at 10m. Everywhere there is a 2m map there will be a corresponding 10m map, but not vice versa.
>
> To do this in a mutually compatible way, I envision producing a GAM that does this:
> 1. partition the surface into variations at higher and lower frequencies, so that the 2m map could be considered the sum of a 10m general shape of the channel plus a zero mean higher frequency fluctuation due to features. The partition could be imperfect ... I'm sure frequencies will bleed and the terrain is inherently anisotropic (channels with long length scales in the downstream direction).
> 2. somehow deal with the fact that the data come from different collections, and are likely to be different in terms of bias, variance and point density. I'd be willing to call one of the datasets "true" and declare a collection effect for the others, but it would only be identifiable in a narrow region.
>
> Any recommendations? I am most familiar with mgcv but flexible on approach. The GAM with tensors splines in the alongstream and cross-stream direction have worked well for us at 10m in similar terrain without the added twist.
>
> Thanks!
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From 13351275265 at 163.com  Tue Dec  1 14:37:59 2015
From: 13351275265 at 163.com (13351275265)
Date: Tue, 1 Dec 2015 21:37:59 +0800
Subject: [R] R-help Digest, Vol 154, Issue 1
In-Reply-To: <mailman.5.1448967602.21026.r-help@r-project.org>
References: <mailman.5.1448967602.21026.r-help@r-project.org>
Message-ID: <20c2e970.18862a.1515dc6dba0.Coremail.13351275265@163.com>

I  have  the  same  question  about  the  following  strings:
 sub("^([0-9]*).*$", "\\1", fields)
could  you  explain them  in  detail .
I  would  lookforward  to  your    wonderful  reply.
2015-12-01 

13351275265 



????r-help-request at r-project.org
?????2015-12-01 19:00
???R-help Digest, Vol 154, Issue 1
????"r-help"<r-help at r-project.org>
???

Send R-help mailing list submissions to 
    r-help at r-project.org 

To subscribe or unsubscribe via the World Wide Web, visit 
    https://stat.ethz.ch/mailman/listinfo/r-help 
or, via email, send a message with subject or body 'help' to 
    r-help-request at r-project.org 

You can reach the person managing the list at 
    r-help-owner at r-project.org 

When replying, please edit your Subject line so it is more specific 
than "Re: Contents of R-help digest..." 


Today's Topics: 

   1. Re: Extracting part of alpha numeric string 
      (phgrosjean at sciviews.org) 
   2. Re: Extracting part of alpha numeric string (Abhinaba Roy) 
   3. Re: Error in 'Contrasts<-' while using GBM. (peter dalgaard) 
   4. Re: Extracting part of alpha numeric string (S Ellison) 
   5. Re: Extracting part of alpha numeric string 
      (phgrosjean at sciviews.org) 
   6. PCA plot of variable names only (debra ragland) 
   7. Re: PCA plot of variable names only (Boris Steipe) 
   8. Re: PCA plot of variable names only (S Ellison) 
   9. Re: PCA plot of variable names only (Boris Steipe) 
  10. Re: Extracting part of alpha numeric string (Berend Hasselman) 
  11. Re: PCA plot of variable names only (David L Carlson) 
  12. Re: PCA plot of variable names only (debra ragland) 
  13. General copula model with heterogeneous marginals 
      (Justin Balthrop) 
  14. Re: rjags cannot find JAGS-4.0.0 (Martyn Plummer) 
  15. summation equation whose numerator has subscript (Sherouk Moawad) 
  16. Re: summation equation whose numerator has subscript 
      (Jeff Newmiller) 
  17. Re: summation equation whose numerator has subscript 
      (David Winsemius) 
  18. Graphing a subset of data (Alexandra Hua) 
  19. Re: Graphing a subset of data (David Winsemius) 
  20. filled circle with a black line on the rim in pch function 
      (Christine Lee) 
  21. Re: filled circle with a black line on the rim in pch 
      function (Jim Lemon) 
  22. ???  filled circle with a black line on the rim in pch 
      function (Christine Lee) 
  23. Re: Graphing a subset of data (Jim Lemon) 
  24. Metanalysis in R using MAVIS (Ernesto Villarino) 


---------------------------------------------------------------------- 

Message: 1 
Date: Mon, 30 Nov 2015 12:17:52 +0100 
From: phgrosjean at sciviews.org 
To: Abhinaba Roy <abhinabaroy09 at gmail.com> 
Cc: r-help <r-help at r-project.org> 
Subject: Re: [R] Extracting part of alpha numeric string 
Message-ID: <524A0581-C4DE-4C41-A8C9-D4F7C40906C0 at sciviews.org> 
Content-Type: text/plain; charset=us-ascii 

fields <- c("2154333b-3208-4519-8b76-acaef5b5a479", "980958a0-103b-4ba9-afaf-27b2f5c24e69", 
            "00966654-0dea-4899-b8cf-26e8300b262d") 
sub("^([0-9]*).*$", "\\1", fields) 

Best, 

Philippe Grosjean 

> On 30 Nov 2015, at 11:39, Abhinaba Roy <abhinabaroy09 at gmail.com> wrote: 
>  
> Hi, 
>  
> I have a field with alpha numeric codes like, 
>  
> 2154333b-3208-4519-8b76-acaef5b5a479 980958a0-103b-4ba9-afaf-27b2f5c24e69 
> 00966654-0dea-4899-b8cf-26e8300b262d 
> I want a derived field which will contain ONLY the numeric part before the 
> first alphabet and the first '-', 
>  
> for example the derived field from the sample above will give me 
>  
> 2154333 
> 980958 
> 00966654 
>  
> How can this be achieved in R? 
>  
> P.S. I do not have much knowledge on regex. It would be of great help if 
> you could suggest some reading for beginners. 
>  
> Thanks, 
> Abhinaba 
>  
>     [[alternative HTML version deleted]] 
>  
> ______________________________________________ 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 



------------------------------ 

Message: 2 
Date: Mon, 30 Nov 2015 17:39:00 +0530 
From: Abhinaba Roy <abhinabaroy09 at gmail.com> 
To: phgrosjean at sciviews.org 
Cc: r-help <r-help at r-project.org> 
Subject: Re: [R] Extracting part of alpha numeric string 
Message-ID: 
    <CANtKHPX+SK20N48O_u53DOQhZsDZ1rWpvCK3H=Uh-QHF6XFa-Q at mail.gmail.com> 
Content-Type: text/plain; charset="UTF-8" 

Hey, 

worked like a charm! :) 

Could you please explain about 

sub("^([0-9]*).*$", "\\1", fields) 

Thanks, 
Abhinaba 

On Mon, Nov 30, 2015 at 4:47 PM, <phgrosjean at sciviews.org> wrote: 

> fields <- c("2154333b-3208-4519-8b76-acaef5b5a479", 
> "980958a0-103b-4ba9-afaf-27b2f5c24e69", 
>             "00966654-0dea-4899-b8cf-26e8300b262d") 
> sub("^([0-9]*).*$", "\\1", fields) 
> 
> Best, 
> 
> Philippe Grosjean 
> 
> > On 30 Nov 2015, at 11:39, Abhinaba Roy <abhinabaroy09 at gmail.com> wrote: 
> > 
> > Hi, 
> > 
> > I have a field with alpha numeric codes like, 
> > 
> > 2154333b-3208-4519-8b76-acaef5b5a479 980958a0-103b-4ba9-afaf-27b2f5c24e69 
> > 00966654-0dea-4899-b8cf-26e8300b262d 
> > I want a derived field which will contain ONLY the numeric part before 
> the 
> > first alphabet and the first '-', 
> > 
> > for example the derived field from the sample above will give me 
> > 
> > 2154333 
> > 980958 
> > 00966654 
> > 
> > How can this be achieved in R? 
> > 
> > P.S. I do not have much knowledge on regex. It would be of great help if 
> > you could suggest some reading for beginners. 
> > 
> > Thanks, 
> > Abhinaba 
> > 
> >       [[alternative HTML version deleted]] 
> > 
> > ______________________________________________ 
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
> 
> 

    [[alternative HTML version deleted]] 



------------------------------ 

Message: 3 
Date: Mon, 30 Nov 2015 14:20:17 +0100 
From: peter dalgaard <pdalgd at gmail.com> 
To: Max Kuhn <mxkuhn at gmail.com> 
Cc: Karteek Pradyumna Bulusu <kartikpradyumna92 at gmail.com>, 
    "r-help at r-project.org" <r-help at r-project.org> 
Subject: Re: [R] Error in 'Contrasts<-' while using GBM. 
Message-ID: <AB946621-4F94-4353-BECC-30038BE7104B at gmail.com> 
Content-Type: text/plain; charset=windows-1252 


On 30 Nov 2015, at 02:59 , Max Kuhn <mxkuhn at gmail.com> wrote: 

> Providing a reproducible example and the results of `sessionInfo` will help 
> get your question answered. 
>  
> My only guess is that one or more of your predictors are factors and that 
> the in-sample data (used to build the model during resampling) have 
> different levels than the holdout samples. 

Another guess is that there's a factor in your (Karteek's) data that has only one level and that "ID ~ ." is pullling more variables into the model than you actually want.  

-pf 

>  
> Max 
>  
> On Sat, Nov 28, 2015 at 10:04 PM, Karteek Pradyumna Bulusu < 
> kartikpradyumna92 at gmail.com> wrote: 
>  
>> Hey, 
>>  
>> I was trying to implement Stochastic Gradient Boosting in R. Following is 
>> my code in rstudio: 
>>  
>>  
>>  
>> library(caret); 
>>  
>> library(gbm); 
>>  
>> library(plyr); 
>>  
>> library(survival); 
>>  
>> library(splines); 
>>  
>> library(mlbench); 
>>  
>> set.seed(35); 
>>  
>> stack = read.csv("E:/Semester 3/BDA/PROJECT/Sample_SO.csv", head 
>> =TRUE,sep=","); 
>>  
>> dim(stack); #displaying dimensions of the dataset 
>>  
>>  
>>  
>> #SPLITTING TRAINING AND TESTING SET 
>>  
>> totraining <- createDataPartition(stack$ID, p = .6, list = FALSE); 
>>  
>> training <- stack[ totraining,] 
>>  
>> test <- stack[-totraining,] 
>>  
>>  
>>  
>> #PARAMETER SETTING 
>>  
>> t_control <- trainControl(method = "cv", number = 10); 
>>  
>>  
>>  
>>  
>>  
>> # GLM 
>>  
>> start <- proc.time(); 
>>  
>>  
>>  
>> glm = train(ID ~ ., data = training, 
>>  
>>             method = "gbm", 
>>  
>>             metric = "ROC", 
>>  
>>             trControl = t_control, 
>>  
>>             verbose = FALSE) 
>>  
>>  
>>  
>> When I am compiling last line, I am getting following error: 
>>  
>>  
>>  
>> Error in `contrasts<-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
>>  
>>  contrasts can be applied only to factors with 2 or more levels 
>>  
>>  
>>  
>>  
>>  
>> Can anyone tell me where I am going wrong and How to rectify it. It?ll be 
>> greatful. 
>>  
>>  
>>  
>> Thank you. Looking forward to it. 
>>  
>>  
>>  
>> Regards, 
>> Karteek Pradyumna Bulusu. 
>>  
>>        [[alternative HTML version deleted]] 
>>  
>> ______________________________________________ 
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html 
>> and provide commented, minimal, self-contained, reproducible code. 
>  
>     [[alternative HTML version deleted]] 
>  
> ______________________________________________ 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 

--  
Peter Dalgaard, Professor, 
Center for Statistics, Copenhagen Business School 
Solbjerg Plads 3, 2000 Frederiksberg, Denmark 
Phone: (+45)38153501 
Office: A 4.23 
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com 



------------------------------ 

Message: 4 
Date: Mon, 30 Nov 2015 13:28:46 +0000 
From: S Ellison <S.Ellison at LGCGroup.com> 
To: r-help <r-help at r-project.org> 
Subject: Re: [R] Extracting part of alpha numeric string 
Message-ID: 
    <1A8C1289955EF649A09086A153E2672403C942461B at GBTEDVPEXCMB04.corp.lgc-group.com> 
     
Content-Type: text/plain; charset="us-ascii" 

> Could you please explain about 
>  
> sub("^([0-9]*).*$", "\\1", fields) 

See ?regex and the extensive online literature on regular expressions. 

S Ellison 




******************************************************************* 
This email and any attachments are confidential. Any use...{{dropped:8}} 



------------------------------ 

Message: 5 
Date: Mon, 30 Nov 2015 14:57:58 +0100 
From: phgrosjean at sciviews.org 
To: Abhinaba Roy <abhinabaroy09 at gmail.com> 
Cc: r-help <r-help at r-project.org> 
Subject: Re: [R] Extracting part of alpha numeric string 
Message-ID: <CDA2C92B-B32E-4BEA-AAAA-96342F70B0BC at sciviews.org> 
Content-Type: text/plain; charset="UTF-8" 


> On 30 Nov 2015, at 13:09, Abhinaba Roy <abhinabaroy09 at gmail.com> wrote: 
>  
> Hey, 
>  
> worked like a charm! :) 
>  
> Could you please explain about 
>  
> sub("^([0-9]*).*$", "\\1", fields) 
>  

Yes. 

sub() replaces substrings. The first argument captures the interesting part of the string: 

^ = start of the string, 

([0-9]*) = capture of the interesting part of the string. [0-9] means any figure from 0 to 9. * means 1 or more of these characters, and () is used to capture the substring,  

.* = all the rest. Dot (.) means any character, and * means again one or more of these characters, 

$ = the end of the string. 

The whole regular expression matches the whole string and captures the interesting part inside the (). 

The second argument is the replacement. //1 means the first captured substring. 

Thus, globally, we replace the whole string by the captured substring. 

Best, 

Philippe Grosjean 


> Thanks, 
> Abhinaba 
>  
> On Mon, Nov 30, 2015 at 4:47 PM, <phgrosjean at sciviews.org <mailto:phgrosjean at sciviews.org>> wrote: 
> fields <- c("2154333b-3208-4519-8b76-acaef5b5a479", "980958a0-103b-4ba9-afaf-27b2f5c24e69", 
>             "00966654-0dea-4899-b8cf-26e8300b262d") 
> sub("^([0-9]*).*$", "\\1", fields) 
>  
> Best, 
>  
> Philippe Grosjean 
>  
> > On 30 Nov 2015, at 11:39, Abhinaba Roy <abhinabaroy09 at gmail.com <mailto:abhinabaroy09 at gmail.com>> wrote: 
> > 
> > Hi, 
> > 
> > I have a field with alpha numeric codes like, 
> > 
> > 2154333b-3208-4519-8b76-acaef5b5a479 980958a0-103b-4ba9-afaf-27b2f5c24e69 
> > 00966654-0dea-4899-b8cf-26e8300b262d 
> > I want a derived field which will contain ONLY the numeric part before the 
> > first alphabet and the first '-', 
> > 
> > for example the derived field from the sample above will give me 
> > 
> > 2154333 
> > 980958 
> > 00966654 
> > 
> > How can this be achieved in R? 
> > 
> > P.S. I do not have much knowledge on regex. It would be of great help if 
> > you could suggest some reading for beginners. 
> > 
> > Thanks, 
> > Abhinaba 
> > 
> >       [[alternative HTML version deleted]] 
> > 
> > ______________________________________________ 
> > R-help at r-project.org <mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help <https://stat.ethz.ch/mailman/listinfo/r-help> 
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html <http://www.r-project.org/posting-guide.html> 
> > and provide commented, minimal, self-contained, reproducible code. 
>  
>  


    [[alternative HTML version deleted]] 



------------------------------ 

Message: 6 
Date: Mon, 30 Nov 2015 13:56:40 +0000 (UTC) 
From: debra ragland <ragland.debra at yahoo.com> 
To: R-help <r-help at r-project.org> 
Subject: [R] PCA plot of variable names only 
Message-ID: 
    <1464521238.12631664.1448891800674.JavaMail.yahoo at mail.yahoo.com> 
Content-Type: text/plain; charset="UTF-8" 

Hello,? 

A colleague of mine prepared a PCA plot of my data and I have no clue how he did it. My original data set contains 15 variables and 64 observations. I have been trying to figure out how he did it on my own, and I have asked but he's swamped so his response is taking longer than usual. Anywho, the plot is simply of PC1 vs. PC2 and in the area of the plot there are just the variable names aligned with values I'm guessing are the loadings (?) I have been searching around and I do not think that this was done via biplot. I am also not sure what is normally plotted on a PCA plot of this type (e.g. loadings, scores, sdevs -- no clue). ?Again, the 15 variable names (var1, var2, var3 etc) are all that is contained in this plot, aligned with their respective values projected onto the first 2 PCs.? 

Any idea on how to generate such a plot based on this description? 

    [[alternative HTML version deleted]] 



------------------------------ 

Message: 7 
Date: Mon, 30 Nov 2015 09:25:28 -0500 
From: Boris Steipe <boris.steipe at utoronto.ca> 
To: debra ragland <ragland.debra at yahoo.com> 
Cc: R-help <r-help at r-project.org> 
Subject: Re: [R] PCA plot of variable names only 
Message-ID: <E9AD124D-758C-457A-BA56-D69EB8442028 at utoronto.ca> 
Content-Type: text/plain; charset=us-ascii 

Your description is obscure but the following may get you started. The function prcomp() returns a list in which the matrix x contains the rotated values of your input. Assuming that your "variable names" are the rownames of your input, you can plot them with text().   

Something like (untested): 

myPCA <- prcomp(someData) 
plot(myPCA$x[,1], myPCA$x[,2], type = "n") 
text(myPCA$x[,1], myPCA$x[,2], rownames(someData)) 

B. 



On Nov 30, 2015, at 8:56 AM, debra ragland via R-help <r-help at r-project.org> wrote: 

> Hello,  
>  
> A colleague of mine prepared a PCA plot of my data and I have no clue how he did it. My original data set contains 15 variables and 64 observations. I have been trying to figure out how he did it on my own, and I have asked but he's swamped so his response is taking longer than usual. Anywho, the plot is simply of PC1 vs. PC2 and in the area of the plot there are just the variable names aligned with values I'm guessing are the loadings (?) I have been searching around and I do not think that this was done via biplot. I am also not sure what is normally plotted on a PCA plot of this type (e.g. loadings, scores, sdevs -- no clue).  Again, the 15 variable names (var1, var2, var3 etc) are all that is contained in this plot, aligned with their respective values projected onto the first 2 PCs.  
>  
> Any idea on how to generate such a plot based on this description? 
>  
>     [[alternative HTML version deleted]] 
>  
> ______________________________________________ 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 



------------------------------ 

Message: 8 
Date: Mon, 30 Nov 2015 14:26:13 +0000 
From: S Ellison <S.Ellison at LGCGroup.com> 
To: "r-help at r-project.org" <r-help at r-project.org> 
Subject: Re: [R] PCA plot of variable names only 
Message-ID: 
    <1A8C1289955EF649A09086A153E2672403C9424662 at GBTEDVPEXCMB04.corp.lgc-group.com> 
     
Content-Type: text/plain; charset="utf-8" 

> Any idea on how to generate such a plot based on this description? 

One simple way of suppressing the individual points in biplot() is to give the labels a colour of 0.  
Adapting the biplot.princomp example: 

     biplot(princomp(USArrests), col=c(0,1)) 

But that retains the point plot axes. If it's not what you meant, you'll need to provide the picture. 

S Ellison 



******************************************************************* 
This email and any attachments are confidential. Any use, copying or 
disclosure other than by the intended recipient is unauthorised. If  
you have received this message in error, please notify the sender  
immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com  
and delete this message and any copies from your computer and network.  
LGC Limited. Registered in England 2991879.  
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK 

------------------------------ 

Message: 9 
Date: Mon, 30 Nov 2015 10:00:32 -0500 
From: Boris Steipe <boris.steipe at utoronto.ca> 
To: debra ragland <ragland.debra at yahoo.com> 
Cc: r-help <r-help at r-project.org> 
Subject: Re: [R] PCA plot of variable names only 
Message-ID: <D60958F9-CF69-4B53-AFFE-FE14A552D7CB at utoronto.ca> 
Content-Type: text/plain; charset=us-ascii 

Please keep communications on list. 
This is too confused to continue productively. 

See here: http://adv-r.had.co.nz/Reproducibility.html 
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example 
... and please read the posting guide and don't post in HTML. 





On Nov 30, 2015, at 9:49 AM, debra ragland <ragland.debra at yahoo.com> wrote: 

> Hi, 
>  
> I've tried this -- before your suggestion -- R throws an error at the plot argument stating that the figure margins are too large and the text argument staring that there is an invalid graphics state. 
>  
> The figure that I am referring to is similar to figure 4 here; 
> Computing and visualizing PCA in R 
>  
>   
>   
>  
>   
>   
>   
>   
>   
> Computing and visualizing PCA in R 
> Following my introduction to PCA, I will demonstrate how to apply and visualize PCA in R. There are many packages and functions that can apply PCA in R. In this po... 
> View on www.r-bloggers.com 
> Preview by Yahoo 
>   
> Without the circle (or gray background, but this is minor) enclosing the variables. I am currently trying to figure out how to the adapt the code to my needs but I am struggling.  
>  
>  
>  
> On Monday, November 30, 2015 9:25 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote: 
>  
>  
> Your description is obscure but the following may get you started. The function prcomp() returns a list in which the matrix x contains the rotated values of your input. Assuming that your "variable names" are the rownames of your input, you can plot them with text().   
>  
> Something like (untested): 
>  
> myPCA <- prcomp(someData) 
> plot(myPCA$x[,1], myPCA$x[,2], type = "n") 
> text(myPCA$x[,1], myPCA$x[,2], rownames(someData)) 
>  
> B. 
>  
>  
>  
> On Nov 30, 2015, at 8:56 AM, debra ragland via R-help <r-help at r-project.org> wrote: 
>  
> > Hello,  
> >  
> > A colleague of mine prepared a PCA plot of my data and I have no clue how he did it. My original data set contains 15 variables and 64 observations. I have been trying to figure out how he did it on my own, and I have asked but he's swamped so his response is taking longer than usual. Anywho, the plot is simply of PC1 vs. PC2 and in the area of the plot there are just the variable names aligned with values I'm guessing are the loadings (?) I have been searching around and I do not think that this was done via biplot. I am also not sure what is normally plotted on a PCA plot of this type (e.g. loadings, scores, sdevs -- no clue).  Again, the 15 variable names (var1, var2, var3 etc) are all that is contained in this plot, aligned with their respective values projected onto the first 2 PCs.  
> >  
> > Any idea on how to generate such a plot based on this description? 
>  
> >  
> >     [[alternative HTML version deleted]] 
> >  
> > ______________________________________________ 
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
>  
>  
>  



------------------------------ 

Message: 10 
Date: Mon, 30 Nov 2015 16:34:35 +0100 
From: Berend Hasselman <bhh at xs4all.nl> 
To: phgrosjean at sciviews.org 
Cc: r-help <r-help at r-project.org>, Abhinaba Roy 
    <abhinabaroy09 at gmail.com> 
Subject: Re: [R] Extracting part of alpha numeric string 
Message-ID: <7E3147A0-1296-4440-95C9-24EAEBC73297 at xs4all.nl> 
Content-Type: text/plain; charset=us-ascii 


> On 30 Nov 2015, at 14:57, phgrosjean at sciviews.org wrote: 
>  
>  
>> On 30 Nov 2015, at 13:09, Abhinaba Roy <abhinabaroy09 at gmail.com> wrote: 
>>  
>> Hey, 
>>  
>> worked like a charm! :) 
>>  
>> Could you please explain about 
>>  
>> sub("^([0-9]*).*$", "\\1", fields) 
>>  
>  
> Yes. 
>  
> sub() replaces substrings. The first argument captures the interesting part of the string: 
>  
> ^ = start of the string, 
>  
> ([0-9]*) = capture of the interesting part of the string. [0-9] means any figure from 0 to 9. * means 1 or more of these characters, and () is used to capture the substring,  
>  
> .* = all the rest. Dot (.) means any character, and * means again one or more of these characters, 
>  
> $ = the end of the string. 

Small correction: 

* means zero or more characters 

according to ?regex. 

Berend 



------------------------------ 

Message: 11 
Date: Mon, 30 Nov 2015 15:48:09 +0000 
From: David L Carlson <dcarlson at tamu.edu> 
To: Boris Steipe <boris.steipe at utoronto.ca>, debra ragland 
    <ragland.debra at yahoo.com> 
Cc: r-help <r-help at r-project.org> 
Subject: Re: [R] PCA plot of variable names only 
Message-ID: 
    <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E54EE at mb02.ads.tamu.edu> 
Content-Type: text/plain; charset="us-ascii" 

If it is just a plot of the variables by their loadings on the first two components, this should do it: 

> dat <- data.frame(matrix(rnorm(100), 10, 5)) 
> dat.pca <- prcomp(dat) 
> plot(dat.pca$rotation[, 1:2]) 
> text(dat.pca$rotation[, 1:2], colnames(dat), pos=3) 

Or if you don't want the symbols just the names, change the last two lines: 

> plot(dat.pca$rotation[, 1:2], type="n") 
> text(dat.pca$rotation[, 1:2], colnames(dat)) 

------------------------------------- 
David L Carlson 
Department of Anthropology 
Texas A&M University 
College Station, TX 77840-4352 

-----Original Message----- 
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Boris Steipe 
Sent: Monday, November 30, 2015 9:01 AM 
To: debra ragland 
Cc: r-help 
Subject: Re: [R] PCA plot of variable names only 

Please keep communications on list. 
This is too confused to continue productively. 

See here: http://adv-r.had.co.nz/Reproducibility.html 
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example 
... and please read the posting guide and don't post in HTML. 





On Nov 30, 2015, at 9:49 AM, debra ragland <ragland.debra at yahoo.com> wrote: 

> Hi, 
>  
> I've tried this -- before your suggestion -- R throws an error at the plot argument stating that the figure margins are too large and the text argument staring that there is an invalid graphics state. 
>  
> The figure that I am referring to is similar to figure 4 here; 
> Computing and visualizing PCA in R 
>  
>   
>   
>  
>   
>   
>   
>   
>   
> Computing and visualizing PCA in R 
> Following my introduction to PCA, I will demonstrate how to apply and visualize PCA in R. There are many packages and functions that can apply PCA in R. In this po... 
> View on www.r-bloggers.com 
> Preview by Yahoo 
>   
> Without the circle (or gray background, but this is minor) enclosing the variables. I am currently trying to figure out how to the adapt the code to my needs but I am struggling.  
>  
>  
>  
> On Monday, November 30, 2015 9:25 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote: 
>  
>  
> Your description is obscure but the following may get you started. The function prcomp() returns a list in which the matrix x contains the rotated values of your input. Assuming that your "variable names" are the rownames of your input, you can plot them with text().   
>  
> Something like (untested): 
>  
> myPCA <- prcomp(someData) 
> plot(myPCA$x[,1], myPCA$x[,2], type = "n") 
> text(myPCA$x[,1], myPCA$x[,2], rownames(someData)) 
>  
> B. 
>  
>  
>  
> On Nov 30, 2015, at 8:56 AM, debra ragland via R-help <r-help at r-project.org> wrote: 
>  
> > Hello,  
> >  
> > A colleague of mine prepared a PCA plot of my data and I have no clue how he did it. My original data set contains 15 variables and 64 observations. I have been trying to figure out how he did it on my own, and I have asked but he's swamped so his response is taking longer than usual. Anywho, the plot is simply of PC1 vs. PC2 and in the area of the plot there are just the variable names aligned with values I'm guessing are the loadings (?) I have been searching around and I do not think that this was done via biplot. I am also not sure what is normally plotted on a PCA plot of this type (e.g. loadings, scores, sdevs -- no clue).  Again, the 15 variable names (var1, var2, var3 etc) are all that is contained in this plot, aligned with their respective values projected onto the first 2 PCs.  
> >  
> > Any idea on how to generate such a plot based on this description? 
>  
> >  
> >     [[alternative HTML version deleted]] 
> >  
> > ______________________________________________ 
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
>  
>  
>  

______________________________________________ 
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code. 



------------------------------ 

Message: 12 
Date: Mon, 30 Nov 2015 15:59:18 +0000 (UTC) 
From: debra ragland <ragland.debra at yahoo.com> 
To: David L Carlson <dcarlson at tamu.edu>,    Boris Steipe 
    <boris.steipe at utoronto.ca> 
Cc: r-help <r-help at r-project.org> 
Subject: Re: [R] PCA plot of variable names only 
Message-ID: 
    <818125085.12595190.1448899158984.JavaMail.yahoo at mail.yahoo.com> 
Content-Type: text/plain; charset="UTF-8" 

Thanks David!!! You have helped me tremendously! Thanks to all others for their input. I'll get out of your hair now :)  


    On Monday, November 30, 2015 10:48 AM, David L Carlson <dcarlson at tamu.edu> wrote: 
  

 If it is just a plot of the variables by their loadings on the first two components, this should do it: 

> dat <- data.frame(matrix(rnorm(100), 10, 5)) 
> dat.pca <- prcomp(dat) 
> plot(dat.pca$rotation[, 1:2]) 
> text(dat.pca$rotation[, 1:2], colnames(dat), pos=3) 

Or if you don't want the symbols just the names, change the last two lines: 

> plot(dat.pca$rotation[, 1:2], type="n") 
> text(dat.pca$rotation[, 1:2], colnames(dat)) 

------------------------------------- 
David L Carlson 
Department of Anthropology 
Texas A&M University 
College Station, TX 77840-4352 

-----Original Message----- 
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Boris Steipe 
Sent: Monday, November 30, 2015 9:01 AM 
To: debra ragland 
Cc: r-help 
Subject: Re: [R] PCA plot of variable names only 

Please keep communications on list. 
This is too confused to continue productively. 

See here: http://adv-r.had.co.nz/Reproducibility.html 
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example 
... and please read the posting guide and don't post in HTML. 







> Hi, 
>  
> I've tried this -- before your suggestion -- R throws an error at the plot argument stating that the figure margins are too large and the text argument staring that there is an invalid graphics state. 
>  
> The figure that I am referring to is similar to figure 4 here; 
> Computing and visualizing PCA in R 
>  
>?  
>?  
>  
>?  
>?  
>?  
>?  
>?  
> Computing and visualizing PCA in R 
> Following my introduction to PCA, I will demonstrate how to apply and visualize PCA in R. There are many packages and functions that can apply PCA in R. In this po... 
> View on www.r-bloggers.com 
> Preview by Yahoo 
>?  
> Without the circle (or gray background, but this is minor) enclosing the variables. I am currently trying to figure out how to the adapt the code to my needs but I am struggling.  
>  
>  
>  
> On Monday, November 30, 2015 9:25 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote: 
>  
>  
> Your description is obscure but the following may get you started. The function prcomp() returns a list in which the matrix x contains the rotated values of your input. Assuming that your "variable names" are the rownames of your input, you can plot them with text().?  
>  
> Something like (untested): 
>  
> myPCA <- prcomp(someData) 
> plot(myPCA$x[,1], myPCA$x[,2], type = "n") 
> text(myPCA$x[,1], myPCA$x[,2], rownames(someData)) 
>  
> B. 
>  
>  
>  
> On Nov 30, 2015, at 8:56 AM, debra ragland via R-help <r-help at r-project.org> wrote: 
>  
> > Hello,  
> >  
> > A colleague of mine prepared a PCA plot of my data and I have no clue how he did it. My original data set contains 15 variables and 64 observations. I have been trying to figure out how he did it on my own, and I have asked but he's swamped so his response is taking longer than usual. Anywho, the plot is simply of PC1 vs. PC2 and in the area of the plot there are just the variable names aligned with values I'm guessing are the loadings (?) I have been searching around and I do not think that this was done via biplot. I am also not sure what is normally plotted on a PCA plot of this type (e.g. loadings, scores, sdevs -- no clue).? Again, the 15 variable names (var1, var2, var3 etc) are all that is contained in this plot, aligned with their respective values projected onto the first 2 PCs.  
> >  
> > Any idea on how to generate such a plot based on this description? 
>  
> >  
> >? ? [[alternative HTML version deleted]] 
> >  
> > ______________________________________________ 
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
>  
>  
>  

______________________________________________ 
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code. 

   
    [[alternative HTML version deleted]] 



------------------------------ 

Message: 13 
Date: Mon, 30 Nov 2015 09:19:59 -0600 
From: Justin Balthrop <justin.balthrop at rice.edu> 
To: r-help at r-project.org 
Subject: [R] General copula model with heterogeneous marginals 
Message-ID: 
    <20151130091959.Horde.etl0ALdkrzuzZzZI4khTIg1 at webmail.rice.edu> 
Content-Type: text/plain; charset=UTF-8; format=flowed; DelSp=Yes 


    I am looking to model the sum of a number of random variables with   
arbitrary gamma distributions and an empirical dependence structure   
that I obtain from data. Basically I observe all of the individual   
pieces but I want to model their sum, as opposed to many copula   
questions which observe a single outcome of a multivariate process and   
seek to fit possible marginal and covariance structure. 
It has been years since I coded in R, but this is what I have thus far: 

library(copula) 
library(scatterplot3d) 
library(psych) 
set.seed(1) 
myCop<-   
normalCopula(param=c(.1,.1,.1,.1,.1,.2,.2,.2,.2,.2,.2,.2,.4,.4,.4,.4,.4,.5,.5,.5,.5), dim=7,   
dispstr="un") 
myMvd<-mvdc(copula=myCop, margins=rep("gamma",7),   
paramMargins=list(list(shape=3,scale=4), 
   list(shape=2, scale=5), 
   list(shape=2, scale=5), 
   list(shape=2, scale=5), 
   list(shape=2, scale=5), 
   list(shape=3, scale=5), 
   list(shape=3, scale=5))) 

simulation<- rMvdc(20000,myMvd) 

colnames(simulation)<-c("P1","P2","P3","P4","P5","P6","P7") 

total =   
simulation[,1]+simulation[,2]+simulation[,3]+simulation[,4]+simulation[,5]+simulation[,6]+simulation[,7] 

As you can see, I have forced 7 gamma distributions with a placeholder   
covariance matrix input. The problem is that I am looking to   
generalize this to the order of ~150 different marginals with   
potentially differing distributions and parameters. 
Ultimately I will have the following input: 
?    matrix of 150 marginal distributions with family and parameters 
?    150x150 covariance matrix 
And what I need to produce is the following: 
An empirical CDF/PDF of the sum of realizations from 5-10 of the   
underlying marginal distributions. To be more clear, assume each   
marginal distribution is a person's response to a treatment, and I   
need to calculate the cumulative treatment effect for a sub-group of   
the population of 150. So, I will have a vector of 0s and 1s to   
identify which members of the population are grouped together for a   
trial. Then I will have a separate vector for the next group. Each   
group vector will have dim=150 but have between 5 and 10 1s with the   
rest 0s. I need a different empirical CDF for each vector. 
Any help? 



------------------------------ 

Message: 14 
Date: Mon, 30 Nov 2015 18:09:16 +0000 
From: Martyn Plummer <plummerm at iarc.fr> 
To: "dwinsemius at comcast.net" <dwinsemius at comcast.net> 
Cc: "r-help at r-project.org" <r-help at r-project.org>, 
    "merricks.merricks at gmail.com" <merricks.merricks at gmail.com> 
Subject: Re: [R] rjags cannot find JAGS-4.0.0 
Message-ID: <1448906956.4643.259.camel at iarc.fr> 
Content-Type: text/plain; charset="utf-8" 

On Fri, 2015-11-27 at 11:27 -0800, David Winsemius wrote: 
> > On Nov 26, 2015, at 4:59 PM, Margaret Donald <merricks.merricks at gmail.com> wrote: 
> >  
> > 1. Despite being in R with administrative rights  the library "rjags" loads 
> > in a temporary location. 
> >  
> >> install.packages("rjags", dependencies=TRUE, 
> > +       lib= "C:/Users/Margaret Donald/Documents/R/win-library/3.2") 
> > trying URL 'https://cran.r-project.org/bin/windows/contrib/3.2/rjags_4-4.zip 
> > ' 
> > Content type 'application/zip' length 525871 bytes (513 KB) 
> > downloaded 513 KB 
> >  
> > package ?rjags? successfully unpacked and MD5 sums checked 
> >  
> > The downloaded binary packages are in 
> >        C:\Users\Margaret 
> > Donald\AppData\Local\Temp\RtmpMzv76s\downloaded_packages 
>  
> That?s not an indication of an error. The installation process always does that. 
>  
>  
> > #----------------------------------------------------------------------------------------------------------------- 
> >  
> > 2. Cannot find JAGS-4.0.0 which is in C;\programs\JAGS\JAGS-4.0.0.  How do 
> > I get R to see JAGS-4.0.0 

The Windows installer writes some keys in the Windows registry. These 
keys are then read by the rjags package when it is loaded to locate the 
JAGS DLL. 

I don't know why this is not working in your case. You might try 
uninstalling and reinstalling JAGS (and by this I mean that the user who 
installed it should uninstall it from the Control Panel). 

Otherwise, as David says, you can set the environment variable 
JAGS_HOME. Note that if you previously set JAGS_HOME in .Rprofile and 
have upgraded to a new version of JAGS, then JAGS_HOME will be pointing 
to the wrong place. This might explain why rjags cannot find JAGS and 
this is why I do not recommend this solution except as a last resort. 

> You might try to use Sys.setenv to create a properly directed JAGS_HOME 
>  
> Sys.setenv(JAGS_HOME=?C:\programs\JAGS\JAGS-4.0.0?) 
>  
> (I corrected the semi-colon.)  
> >> library(rjags) 
> > Error : .onLoad failed in loadNamespace() for 'rjags', details: 
> >  call: fun(libname, pkgname) 
> >  error: Failed to locate any version of JAGS version 4 
> >  
> > The rjags package is just an interface to the JAGS library 
> > Make sure you have installed JAGS-4.x.y.exe (for any x >=0, y>=0) from 
> > http://www.sourceforge.net/projects/mcmc-jags/files 
>  
> I?m was having a perhaps similar problem on a Mac. The binary version 
> 3-15 of rjags installed today from CRAN was trying to 
> access /usr/local/lib/libjags.3.dylib, but since I have installed JAGS 
> version 4.0.1 installed from the SourceForge repository, there is no 
> ligjags.3.dylib, but instead there was only 
> a /usr/local/lib/libjags.4.dylib 

rjags_4-4 requires JAGS 4.x.y and the previous version rjags_3-15 is not 
compatible with JAGS 4.0.0. Unfortunately the Mac OS X binaries on CRAN 
are not up to date. I have no control over this but Matt Denwood has 
made a binary for Mavericks or later available on Sourceforge: 

http://sourceforge.net/projects/mcmc-jags/files/rjags/4/rjags_4-4.tgz 

Martyn 

> Going back to SourceForge and tracking down the older version of JAGS 
> and installing version 3.4.0 was successful in getting rjags to load 
> correctly. I suspect that with the release of JAGS v4 that there is 
> some mismatch among the various editions of rjags and JAGS. 
>  
> ?  
> David 
> >  
> > Error: package or namespace load failed for ?rjags? 
> >> library(R2jags) 
> > Loading required package: rjags 
> > Error : .onLoad failed in loadNamespace() for 'rjags', details: 
> >  call: fun(libname, pkgname) 
> >  error: Failed to locate any version of JAGS version 4 
> >  
> > The rjags package is just an interface to the JAGS library 
> > Make sure you have installed JAGS-4.x.y.exe (for any x >=0, y>=0) from 
> > http://www.sourceforge.net/projects/mcmc-jags/files 
> >  
> > Error: package ?rjags? could not be loaded 
> >>  
> >  
> > Regards, 
> > Margaret Donald 
> > --  
> > Margaret Donald 
> > Post Doctoral researcher 
> > University of New South Wales 
> > margaret.donald at unsw.edu.au 
> > 0405 834 550 
> >  
> >     [[alternative HTML version deleted]] 
> >  
> > ______________________________________________ 
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> > and provide commented, minimal, self-contained, reproducible code. 
>  
> David Winsemius 
> Alameda, CA, USA 
>  


------------------------------ 

Message: 15 
Date: Mon, 30 Nov 2015 21:20:34 +0000 (UTC) 
From: Sherouk Moawad <sheroukmoawad at yahoo.com> 
To: R-help Mailing List <r-help at r-project.org> 
Subject: [R] summation equation whose numerator has subscript 
Message-ID: 
    <944655747.7652698.1448918434492.JavaMail.yahoo at mail.yahoo.com> 
Content-Type: text/plain; charset=UTF-8 

Dear R experts  
Please do you have any idea about how this summation can be written in R(the equation can be viewed in the following link):  
http://s16.postimg.org/or2km30ph/equation.jpg  

I've tried out out this code but it gave me error for writing brackets in function of summation: 

>>>  
x=matrix(c(6,2,1),3,1)  

for (l in 1:3){  
sum(sapply(1:3, function(j[l]){if(l>1){sum(sapply(1:j[l-1], function(j[l]){x[j[l]]*(j[l]<j[l-1])}))}}))}>>>Thank you 



------------------------------ 

Message: 16 
Date: Mon, 30 Nov 2015 14:17:55 -0800 
From: Jeff Newmiller <jdnewmil at dcn.davis.ca.us> 
To: Sherouk Moawad <sheroukmoawad at yahoo.com>,    Sherouk Moawad via 
    R-help <r-help at r-project.org>,    R-help Mailing List 
    <r-help at r-project.org> 
Subject: Re: [R] summation equation whose numerator has subscript 
Message-ID: <731B7B27-F965-44B2-8892-771D3D7A57EA at dcn.davis.ca.us> 
Content-Type: text/plain; charset="UTF-8" 

I cannot understand that summation at all,  much less translate it to R. Do you have an original citation for this thing?  

As for putting subscripts in parameter lists,  that is not going to happen. You have to accept that the code that calls your function needs to do any necessary subscripting before it gives that piece to your function. Keep in mind that apply functions do this by their nature without the mess of specifying it yourself. If you know that the automatic subscripting that sapply does is not going to get the result you want then don't use that function.  
--  
Sent from my phone. Please excuse my brevity. 

On November 30, 2015 1:20:34 PM PST, Sherouk Moawad via R-help <r-help at r-project.org> wrote: 
>Dear R experts  
>Please do you have any idea about how this summation can be written in 
>R(the equation can be viewed in the following link):  
>http://s16.postimg.org/or2km30ph/equation.jpg  
> 
>I've tried out out this code but it gave me error for writing brackets 
>in function of summation: 
> 
>>>>  
>x=matrix(c(6,2,1),3,1)  
> 
>for (l in 1:3){  
>sum(sapply(1:3, function(j[l]){if(l>1){sum(sapply(1:j[l-1], 
>function(j[l]){x[j[l]]*(j[l]<j[l-1])}))}}))}>>>Thank you 
> 
>______________________________________________ 
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
>https://stat.ethz.ch/mailman/listinfo/r-help 
>PLEASE do read the posting guide 
>http://www.R-project.org/posting-guide.html 
>and provide commented, minimal, self-contained, reproducible code. 

    [[alternative HTML version deleted]] 



------------------------------ 

Message: 17 
Date: Mon, 30 Nov 2015 14:27:46 -0800 
From: David Winsemius <dwinsemius at comcast.net> 
To: Sherouk Moawad <sheroukmoawad at yahoo.com> 
Cc: R-help Mailing List <r-help at r-project.org> 
Subject: Re: [R] summation equation whose numerator has subscript 
Message-ID: <954081E4-C3D2-4029-A90F-044C636395A2 at comcast.net> 
Content-Type: text/plain; charset=utf-8 


> On Nov 30, 2015, at 1:20 PM, Sherouk Moawad via R-help <r-help at r-project.org> wrote: 
>  
> Dear R experts  
> Please do you have any idea about how this summation can be written in R(the equation can be viewed in the following link):  
> http://s16.postimg.org/or2km30ph/equation.jpg  

Can you explain in natural language the goals of this expression. It makes little sense to me to start with an index of j_sub_l = 0 and to then iterate to up to j_sub_(l-1) -1 . How can there be a value for j_sub(l-1) with a starting point of zero. The notation saying to do something for l = 2:n is not helpful since values of ?l? doesn?t really appear in the looped expression (noting that j_sub_l starts at 0, so it's not being determined by ?l". 

I believe the confused notation was the cause of this question being closed after it appeared last week on SO: 

http://stackoverflow.com/questions/33882285/summation-equation-whose-numerator-has-subscript 

And what intent is meant for the indices of the outer summation? The expression j_sub_1 = 0 seems to have no corresponding reference point inside the looped expression. So you would simply be summing the same value N times, but since N is not defined we cannot write any code. 

>  
> I've tried out out this code but it gave me error for writing brackets in function of summation: 

You should _always_, _always_, _always_ post the entire results of an error. We have no way of seeing your console. Error messages are usually informative. 

>  
>>>>  
> x=matrix(c(6,2,1),3,1)  
>  
> for (l in 1:3){  
> sum(sapply(1:3, function(j[l]){if(l>1){sum(sapply(1:j[l-1], function(j[l]){x[j[l]]*(j[l]<j[l-1])}))}}))} 


You have three nested loops in the code above, but at least it appears you do understand that R is a 1-based language. But since the image-expression goes from 0 to some cryptic value (minus one) then the R version ought to go from one to "one more? than that expression. 


> >>>Thank you 
>  
> ______________________________________________ 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 

David Winsemius 
Alameda, CA, USA 



------------------------------ 

Message: 18 
Date: Mon, 30 Nov 2015 21:44:45 -0500 
From: Alexandra Hua <alexandra.hua at yale.edu> 
To: r-help at r-project.org 
Subject: [R] Graphing a subset of data 
Message-ID: 
    <CADoPJut5vjp35v5HTo=V8skJAhdtszg+PpH-hrdXcfTPr9whFg at mail.gmail.com> 
Content-Type: text/plain; charset="UTF-8" 

I am trying to write a function that will graph a variable of a dataset or 
a variable from a subset of the data. My function is as follows: 

graphics<-function(dat, var, graph, varname, val, subset){ 
  if(subset==1) { 
    data<-mySubset(dat=dat,varname=varname,val=val) 
  }else if(subset!=1){ 
    data<-(dat) 
  } 
  if(graph==1) { 
    with(data, boxplot(var), main = paste("Vertical box plot of", 
myfunc(dat), "variable", myfunc(var), xlab = myfunc(var))) 
  }else if(graph==2){ 
    with(data, boxplot(var), horizontal=TRUE, main = paste("Horizontal 
box plot of", myfunc(dat), "variable", myfunc(var), xlab = 
myfunc(var))) 
  }else if(graph==3){ 
    hist(var, main="Histogram of", myfunc(var)) 
  }} 

I included "subset" as a parameter for the function, so that subset=1 would 
tell the function to subset, and any other value will use the full dataset. 
However, when I run the function with the following expression (bolts is 
the dataset, SPEED1 is the variable, value=3) 

graphics(bolts, bolts$SPEED1, graph=3, bolts$SPEED1, 3, 1) 

I receive this error message: Error in eval(substitute(expr), data, enclos 
= parent.frame()) : invalid 'envir' argument of type 'logical' 

Does anyone know why this is, or if there is something wrong with my code? 
Thanks! 

--  
*Alexandra Hua * 
Yale University | MPH Candidate Class of 2016 
Chronic Disease Epidemiology 

    [[alternative HTML version deleted]] 



------------------------------ 

Message: 19 
Date: Mon, 30 Nov 2015 19:15:19 -0800 
From: David Winsemius <dwinsemius at comcast.net> 
To: Alexandra Hua <alexandra.hua at yale.edu> 
Cc: r-help at r-project.org 
Subject: Re: [R] Graphing a subset of data 
Message-ID: <D0762336-1CB5-465B-A487-C48EEC6D70EE at comcast.net> 
Content-Type: text/plain; charset=us-ascii 

Dear Alex (as you are signing yourself on StackOverflow); 

It is considered poor manners to cross-post identical questions in multiple venues.  

http://stackoverflow.com/questions/34011669/graphing-function-based-on-subset-of-data 

You should choose one or the other of SO and Rhelp. If you do not get a satisfying answer in your first choice, you should wait an appropriate number of hours before posting at the other venue. And when you do end up cross-posting, your should stated where else the question was asked so that potential respondents can check to see if you have already gotten an answer. 

You should also read the the posting guide where it is clearly stated the rhelp responders expect that you include a dataset built with R code for purposes of illustration. 

> On Nov 30, 2015, at 6:44 PM, Alexandra Hua <alexandra.hua at yale.edu> wrote: 
>  
> I am trying to write a function that will graph a variable of a dataset or 
> a variable from a subset of the data. My function is as follows: 
>  
> graphics<-function(dat, var, graph, varname, val, subset){ 
>  if(subset==1) { 
>    data<-mySubset(dat=dat,varname=varname,val=val) 
>  }else if(subset!=1){ 
>    data<-(dat) 
>  } 
>  if(graph==1) { 
>    with(data, boxplot(var), main = paste("Vertical box plot of", 
> myfunc(dat), "variable", myfunc(var), xlab = myfunc(var))) 
>  }else if(graph==2){ 
>    with(data, boxplot(var), horizontal=TRUE, main = paste("Horizontal 
> box plot of", myfunc(dat), "variable", myfunc(var), xlab = 
> myfunc(var))) 
>  }else if(graph==3){ 
>    hist(var, main="Histogram of", myfunc(var)) 
>  }} 
>  

Generally the use of `with` inside functions is ill-advised. Sometimes it succeeds but nmany times it will fail with puzzling error messages. 


> I included "subset" as a parameter for the function, so that subset=1 would 
> tell the function to subset, and any other value will use the full dataset. 
> However, when I run the function with the following expression (bolts is 
> the dataset, SPEED1 is the variable, value=3) 
>  
> graphics(bolts, bolts$SPEED1, graph=3, bolts$SPEED1, 3, 1) 
>  
> I receive this error message: Error in eval(substitute(expr), data, enclos 
> = parent.frame()) : invalid 'envir' argument of type 'logical' 
>  
> Does anyone know why this is, or if there is something wrong with my code? 
> Thanks! 
>  
> --  
> *Alexandra Hua * 
> Yale University | MPH Candidate Class of 2016 
> Chronic Disease Epidemiology 
>  
>     [[alternative HTML version deleted]] 
>  
> ______________________________________________ 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 

David Winsemius 
Alameda, CA, USA 



------------------------------ 

Message: 20 
Date: Tue, 1 Dec 2015 06:20:44 +0000 (UTC) 
From: Christine Lee <leptostracan at yahoo.com> 
To: <r-help at r-project.org> 
Subject: [R] filled circle with a black line on the rim in pch 
    function 
Message-ID: 
    <456635539.12433643.1448950844348.JavaMail.yahoo at mail.yahoo.com> 
Content-Type: text/plain; charset=UTF-8 

Dear All, 

I have an embarssing question, I want to put a black line as a rim on the grey symbol in the xyplot, to no avail. 
.  I thought it was easy, by changing the pch code from 16 to 21.  I was surpised that I ran into difficulty. 
   
My original script is as follows: 
library(lattice) 
xyplot(Abun~Date1|Station, data=Raw, 
        groups = culr, 
        par.settings = list(strip.background = list(col = "transparent"), 
                            superpose.symbol = list(cex = rep(2, 2), 
                                                    col=c("grey","black"), 
                                                    pch = rep(16,2))), 
        type="p", 
        xlab=list("Month",cex=1.5), 
        ylab=list("Abundance",cex=1.5),   
        index.cond=list(c(1,2,3,4)), 
        auto.key = T, 
        layout=c(4,1)) 


I have changed pch number into 21, the symbols did show a black rim, but the filled circle colours became blue and pink, instead of the designated grey and black.  This puzzles me.   

My data is as follows: 
Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L,  
2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L,  
10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L,  
5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10",  
"1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9",  
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = "factor"),  
    Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,  
    1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,  
    2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"),  
    Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L,  
    2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L,  
    2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L,  
    3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I" 
    ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92,  
    2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33,  
    0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67,  
    0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92,  
    1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697,  
    16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768,  
    16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737,  
    16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,  
    16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737,  
    16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769 
    ), class = "Date")), .Names = c("Date", "Year", "Station",  
"Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame") 

Can any one help me please? 

With best regards, 
Christine 



------------------------------ 

Message: 21 
Date: Tue, 1 Dec 2015 19:01:27 +1100 
From: Jim Lemon <drjimlemon at gmail.com> 
To: Christine Lee <leptostracan at yahoo.com> 
Cc: r-help mailing list <r-help at r-project.org> 
Subject: Re: [R] filled circle with a black line on the rim in pch 
    function 
Message-ID: 
    <CA+8X3fWuW4vJpc95eY=z-Krk-UR0tY4Zjsxn2uTK=KyA8xeYyA at mail.gmail.com> 
Content-Type: text/plain; charset="UTF-8" 

Hi Christine, 
When I try to run your script, the plot fails: 

Error in eval(expr, envir, enclos) : object 'culr' not found 
> names(Raw) 
[1] "Date"    "Year"    "Station" "Abun"    "Date1" 

so I changed the second line to: 

groups=Year, 

and it did work. The default (pink, gray) background colors for the symbols 
do appear when the symbol is changed to pch=21. The arguments 

col="black",bg=c("gray","black"), 

would produce the symbols you want in base graphics, but do not seem to do 
so in lattice. I do get sort of what you want by modifying your code pretty 
radically: 

xyplot(Abun~Date1|Station, data=Raw, 
        groups = Year, 
        par.settings = list(strip.background = list(col = "transparent")), 
        type="p", 
        pch = rep(21,2), 
        col="black", 
        fill=c("gray","black"), 
        xlab=list("Month",cex=1.5), 
        ylab=list("Abundance",cex=1.5), 
        index.cond=list(c(1,2,3,4)), 
        auto.key = T, 
        layout=c(4,1)) 

Jim 


On Tue, Dec 1, 2015 at 5:20 PM, Christine Lee via R-help < 
r-help at r-project.org> wrote: 

> Dear All, 
> 
> I have an embarssing question, I want to put a black line as a rim on the 
> grey symbol in the xyplot, to no avail. 
> .  I thought it was easy, by changing the pch code from 16 to 21.  I was 
> surpised that I ran into difficulty. 
> 
> My original script is as follows: 
> library(lattice) 
> xyplot(Abun~Date1|Station, data=Raw, 
>         groups = culr, 
>         par.settings = list(strip.background = list(col = "transparent"), 
>                             superpose.symbol = list(cex = rep(2, 2), 
>                                                     col=c("grey","black"), 
>                                                     pch = rep(16,2))), 
>         type="p", 
>         xlab=list("Month",cex=1.5), 
>         ylab=list("Abundance",cex=1.5), 
>         index.cond=list(c(1,2,3,4)), 
>         auto.key = T, 
>         layout=c(4,1)) 
> 
> 
> I have changed pch number into 21, the symbols did show a black rim, but 
> the filled circle colours became blue and pink, instead of the designated 
> grey and black.  This puzzles me. 
> 
> My data is as follows: 
> Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L, 
> 2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 
> 10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L, 
> 5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10", 
> "1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", 
> "26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = 
> "factor"), 
>     Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
>     1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
>     2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class = 
> "factor"), 
>     Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
>     2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
>     2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
>     3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I" 
>     ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92, 
>     2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33, 
>     0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67, 
>     0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92, 
>     1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697, 
>     16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768, 
>     16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737, 
>     16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737, 
>     16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737, 
>     16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769 
>     ), class = "Date")), .Names = c("Date", "Year", "Station", 
> "Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame") 
> 
> Can any one help me please? 
> 
> With best regards, 
> Christine 
> 
> ______________________________________________ 
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 
> 

    [[alternative HTML version deleted]] 



------------------------------ 

Message: 22 
Date: Tue, 1 Dec 2015 08:19:54 +0000 (UTC) 
From: Christine Lee <leptostracan at yahoo.com> 
To: Jim Lemon <drjimlemon at gmail.com> 
Cc: r-help mailing list <r-help at r-project.org> 
Subject: [R] ???  filled circle with a black line on the rim in pch 
    function 
Message-ID: 
    <495496469.12506923.1448957994844.JavaMail.yahoo at mail.yahoo.com> 
Content-Type: text/plain; charset="UTF-8" 

Sorry all,?My mistake!? The?missing parts are?as follows:?Raw$Date1<-as.Date(Raw$Date,"%d/%m") 
culr<-ifelse(Raw$Year=="Y2002","Year 2002","Year 2014") 
library(lattice)?Many thanks.? ?With best regards,Christine  


    Jim Lemon <drjimlemon at gmail.com> ? 2015?12?1? (??) 4:01 PM ??? 
  

 Hi Christine,When I try to run your script, the plot fails: 
Error in eval(expr, envir, enclos) : object 'culr' not found> names(Raw)[1] "Date" ? ?"Year" ? ?"Station" "Abun" ? ?"Date1" ? 
so I changed the second line to: 
groups=Year, 
and it did work. The default (pink, gray) background colors for the symbols do appear when the symbol is changed to pch=21. The arguments? 
col="black",bg=c("gray","black"), 
would produce the symbols you want in base graphics, but do not seem to do so in lattice. I do get sort of what you want by modifying your code pretty radically: 
xyplot(Abun~Date1|Station, data=Raw,? ? ? ? groups = Year,? ? ? ? par.settings = list(strip.background = list(col = "transparent")),? ? ? ? type="p",? ? ? ? pch = rep(21,2),? ? ? ? col="black",? ? ? ? fill=c("gray","black"),? ? ? ? xlab=list("Month",cex=1.5),? ? ? ? ylab=list("Abundance",cex=1.5),? ? ? ? index.cond=list(c(1,2,3,4)),? ? ? ? auto.key = T,? ? ? ? layout=c(4,1)) 
Jim 

On Tue, Dec 1, 2015 at 5:20 PM, Christine Lee via R-help <r-help at r-project.org> wrote: 

Dear All, 

I have an embarssing question, I want to put a black line as a rim on the grey symbol in the xyplot, to no avail. 
.? I thought it was easy, by changing the pch code from 16 to 21.? I was surpised that I ran into difficulty. 

My original script is as follows: 
library(lattice) 
xyplot(Abun~Date1|Station, data=Raw, 
? ? ? ? groups = culr, 
? ? ? ? par.settings = list(strip.background = list(col = "transparent"), 
? ? ? ? ? ? ? ? ? ? ? ? ? ? superpose.symbol = list(cex = rep(2, 2), 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? col=c("grey","black"), 
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? pch = rep(16,2))), 
? ? ? ? type="p", 
? ? ? ? xlab=list("Month",cex=1.5), 
? ? ? ? ylab=list("Abundance",cex=1.5), 
? ? ? ? index.cond=list(c(1,2,3,4)), 
? ? ? ? auto.key = T, 
? ? ? ? layout=c(4,1)) 


I have changed pch number into 21, the symbols did show a black rim, but the filled circle colours became blue and pink, instead of the designated grey and black.? This puzzles me. 

My data is as follows: 
Raw<-structure(list(Date = structure(c(6L, 7L, 2L, 4L, 12L, 9L, 7L, 
2L, 4L, 12L, 6L, 15L, 14L, 3L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 
10L, 13L, 6L, 1L, 16L, 5L, 11L, 8L, 4L, 10L, 13L, 6L, 1L, 16L, 
5L, 11L, 8L, 4L, 10L, 13L, 11L, 8L, 4L, 10L, 13L), .Label = c("1/10", 
"1/11", "11/11", "12/11", "13/10", "19/9", "2/10", "2/11", "20/9", 
"26/11", "29/10", "29/11", "30/11", "31/10", "4/10", "6/10"), class = "factor"), 
? ? Year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
? ? 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
? ? 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
? ? 2L, 2L, 2L, 2L, 2L, 2L), .Label = c("Y2002", "Y2014"), class = "factor"), 
? ? Station = structure(c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
? ? 2L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
? ? 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
? ? 3L, 3L, 4L, 4L, 4L, 4L, 4L), .Label = c("E", "F", "H", "I" 
? ? ), class = "factor"), Abun = c(3.42, 1.33, 3.67, 3.67, 3.92, 
? ? 2.17, 2.5, 1.67, 6.33, 0.67, 1, 1, 1.33, 2.08, 0, 0, 0.33, 
? ? 0.08, 0.08, 0, 0.5, 0.17, 0.67, 0.67, 0, 1, 0.58, 1.5, 2.67, 
? ? 0.67, 1.33, 3, 0.58, 1.17, 1.25, 0.75, 1.25, 1.75, 0.92, 
? ? 1.5, 0.83, 0.75, 2.33, 0.67, 1.33, 1.58), Date1 = structure(c(16697, 
? ? 16710, 16740, 16751, 16768, 16698, 16710, 16740, 16751, 16768, 
? ? 16697, 16712, 16739, 16750, 16697, 16709, 16714, 16721, 16737, 
? ? 16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737, 
? ? 16741, 16751, 16765, 16769, 16697, 16709, 16714, 16721, 16737, 
? ? 16741, 16751, 16765, 16769, 16737, 16741, 16751, 16765, 16769 
? ? ), class = "Date")), .Names = c("Date", "Year", "Station", 
"Abun", "Date1"), row.names = c(NA, -46L), class = "data.frame") 

Can any one help me please? 

With best regards, 
Christine 

______________________________________________ 
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code. 




   
    [[alternative HTML version deleted]] 



------------------------------ 

Message: 23 
Date: Tue, 1 Dec 2015 21:28:52 +1100 
From: Jim Lemon <drjimlemon at gmail.com> 
Cc: r-help mailing list <r-help at r-project.org>,    Alexandra Hua 
    <alexandra.hua at yale.edu> 
Subject: Re: [R] Graphing a subset of data 
Message-ID: 
    <CA+8X3fVC_=maxb2zX1Q8xR5sOyyX5adR4RX7fmJiHPXKQ57dXw at mail.gmail.com> 
Content-Type: text/plain; charset="UTF-8" 

Hi Alexandra, 
I think you are going about this in an excessively difficult way. Here is a 
rough example: 

graphit<-function(x,var,type,subset=NA,...){ 
 if(!is.na(subset[1])) x<-subset(x,subset) 
 do.call(type,list(x=x[[var]],...)) 
} 
# assume that the data are measurements of penetration by crossbow bolts at 
specified speeds 
bolts<-data.frame(SPEED1=rep(1:3,each=30), 
 penetration=c(runif(30,10,15),runif(30,15,20),runif(30,20,25))) 
# note the lazy way of adding arguments to the function call 
graphit(bolts,"SPEED1",boxplot,main="Boxplot of SPEED1") 
graphit(bolts,"penetration",boxplot,subset=bolts$SPEED1==3, 
 main="Boxplot of SPEED1 == 3") 
graphit(bolts,"SPEED1",hist,main="Histogram of SPEED1") 

Jim 

    [[alternative HTML version deleted]] 



------------------------------ 

Message: 24 
Date: Tue, 1 Dec 2015 11:41:59 +0100 
From: Ernesto Villarino <villarino.ernesto at gmail.com> 
To: r-help at r-project.org 
Subject: [R] Metanalysis in R using MAVIS 
Message-ID: 
    <CAAmrVFpttyU34wRNXhcFa=6nW7OmB51cE0J=_WceSdqQJzNxRA at mail.gmail.com> 
Content-Type: text/plain; charset="UTF-8" 

I am Ernesto, a Phd student from Bilbao working with climate related 
plankton dynamics. I am trying to perform a MAVIS metanalysis (any other 
meta techcnique to be applied in R) to see if there is a significant 
difference between the moderators in my dataset. Is there any option to do 
an ANOVA analysis using MAVIS among groups? 



In the Moderator (subgroup) analysis we have one variable per study, 
instead of having two (M1,M2). 



*Study* 

*Response variable* 

*N* 

*Moderator* 

Study-01 

-0.1111 

8 

Dispersing 

Study-02 

-0.2557 

8 

Dispersing 

Study-03 

0.06667 

4 

Zooplankton 

Study-04 

0.1956 

5 

Phytoplankton 

Study-05 

0.025 

5 

Phytoplankton 

Study-06 

0.7768 

4 

Phytoplankton 

Study-07 

0.3511 

6 

Dispersing 

Study-08 

-0.09821 

6 

Dispersing 

Study-09 

0.4286 

3 

NDL 

Study-10 

0.5638 

7 

Dispersing 

 ? 

 ? 

 ? 

 ? 





I appreciate your help, 

    [[alternative HTML version deleted]] 



------------------------------ 

Subject: Digest Footer 

_______________________________________________ 
R-help at r-project.org mailing list 
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
and provide commented, minimal, self-contained, reproducible code. 

------------------------------ 

End of R-help Digest, Vol 154, Issue 1 
************************************** 
	[[alternative HTML version deleted]]


From jsorkin at grecc.umaryland.edu  Tue Dec  1 23:57:07 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 01 Dec 2015 17:57:07 -0500
Subject: [R] Function calling a function, column name not passed properly
Message-ID: <565DDF73020000CB00141D70@smtp.medicine.umaryland.edu>

I am trying to write a function that calls a function. The first call to SmallFn works without any problem, printing both the passed data and the column Wstscr. The second call does not work (error, Error in d[, column] : subscript out of bounds). 
 
The first call shows what I am trying to do with the second call. I am passing to the outer function doit12 the beginning of the name of the column that I want to access (Wst). doit12 creates the full name of the column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12 as seen by the results of   print(data[,varscr]).
 
SmallFn works when it is called using SmallFn(Wstscr,data), but fails when called with SmallFn(varscr,data). I don't understand why the second call fails. varscr was shown to represent Wstscr.
 
Please tell my why the second call is not working, please put me out of one-full day of misery!
 
Thank you,
John
 
 
 

mydata <- cbind(   patient_id=c(10163,10987,19882,19899,20104,20105,20167,20318,20338,20392),
                   Wstscr=c(139.00,NA,101.80,103.00,76.40,116.00,139.80,111.31,NA,150.00))            
mydata     
 
doit12 <-function(variable,data) {
  
  varxx <- deparse(substitute(variable))
  cat("varxx created from first deparse substitute=",varxx,"\n")
  varscr <- paste(varxx,"scr",sep="")
  cat("1varscr=",varscr,"\n")
  cat("Data inside doit12\n")
  print(data)
  cat("Print the Wstscr column of data. varscr created using paste after deparse substitute\n")
  print(data[,varscr])
  cat("\n\n")
  
  SmallFn <- function(v,d) {
    cat("\nInside SmallFn\n")
    zz <-match.call()
    column <- deparse(substitute(v))
    cat("column=",column,"\n")
    cat("The results of match.call\n")
    print(zz)
    print("Hello world!")
    print(d)
    print(d[,column])
  }
  SmallFn(Wstscr,data)
  SmallFn(varscr,data)
}
doit12(Wst,mydata)
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From dwinsemius at comcast.net  Wed Dec  2 01:32:45 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Dec 2015 16:32:45 -0800
Subject: [R] Function calling a function, column name not passed properly
In-Reply-To: <565DDF73020000CB00141D70@smtp.medicine.umaryland.edu>
References: <565DDF73020000CB00141D70@smtp.medicine.umaryland.edu>
Message-ID: <C10D5502-CD68-4D96-9912-0FE537D9D604@comcast.net>


> On Dec 1, 2015, at 2:57 PM, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> 
> I am trying to write a function that calls a function. The first call to SmallFn works without any problem, printing both the passed data and the column Wstscr. The second call does not work (error, Error in d[, column] : subscript out of bounds). 
> 
> The first call shows what I am trying to do with the second call. I am passing to the outer function doit12 the beginning of the name of the column that I want to access (Wst). doit12 creates the full name of the column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12 as seen by the results of   print(data[,varscr]).
> 
> SmallFn works when it is called using SmallFn(Wstscr,data), but fails when called with SmallFn(varscr,data). I don't understand why the second call fails. varscr was shown to represent Wstscr.
> 
> Please tell my why the second call is not working, please put me out of one-full day of misery!

It?s telling you there is no column in that dataframe with the name ?varscr?.

? 
David.


> Thank you,
> John
> 
> 
> 
> 
> mydata <- cbind(   patient_id=c(10163,10987,19882,19899,20104,20105,20167,20318,20338,20392),
>                   Wstscr=c(139.00,NA,101.80,103.00,76.40,116.00,139.80,111.31,NA,150.00))            
> mydata     
> 
> doit12 <-function(variable,data) {
> 
>  varxx <- deparse(substitute(variable))
>  cat("varxx created from first deparse substitute=",varxx,"\n")
>  varscr <- paste(varxx,"scr",sep="")
>  cat("1varscr=",varscr,"\n")
>  cat("Data inside doit12\n")
>  print(data)
>  cat("Print the Wstscr column of data. varscr created using paste after deparse substitute\n")
>  print(data[,varscr])
>  cat("\n\n")
> 
>  SmallFn <- function(v,d) {
>    cat("\nInside SmallFn\n")
>    zz <-match.call()
>    column <- deparse(substitute(v))
>    cat("column=",column,"\n")
>    cat("The results of match.call\n")
>    print(zz)
>    print("Hello world!")
>    print(d)
>    print(d[,column])
>  }
>  SmallFn(Wstscr,data)
>  SmallFn(varscr,data)
> }
> doit12(Wst,mydata)
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:15}}


From dwinsemius at comcast.net  Wed Dec  2 01:35:57 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Dec 2015 16:35:57 -0800
Subject: [R] Function calling a function, column name not passed properly
In-Reply-To: <C10D5502-CD68-4D96-9912-0FE537D9D604@comcast.net>
References: <565DDF73020000CB00141D70@smtp.medicine.umaryland.edu>
	<C10D5502-CD68-4D96-9912-0FE537D9D604@comcast.net>
Message-ID: <597D4741-2225-4F95-B5BA-FF30BF07FC3D@comcast.net>


> On Dec 1, 2015, at 4:32 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 1, 2015, at 2:57 PM, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
>> 
>> I am trying to write a function that calls a function. The first call to SmallFn works without any problem, printing both the passed data and the column Wstscr. The second call does not work (error, Error in d[, column] : subscript out of bounds). 
>> 
>> The first call shows what I am trying to do with the second call. I am passing to the outer function doit12 the beginning of the name of the column that I want to access (Wst). doit12 creates the full name of the column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12 as seen by the results of   print(data[,varscr]).
>> 
>> SmallFn works when it is called using SmallFn(Wstscr,data), but fails when called with SmallFn(varscr,data). I don't understand why the second call fails. varscr was shown to represent Wstscr.
>> 
>> Please tell my why the second call is not working, please put me out of one-full day of misery!
> 
> It?s telling you there is no column in that dataframe with the name ?varscr?.
> 

Actually it?s a matrix, but the same reasoning applies.

? 
> David.
> 
> 
>> Thank you,
>> John
>> 
>> 
>> 
>> 
>> mydata <- cbind(   patient_id=c(10163,10987,19882,19899,20104,20105,20167,20318,20338,20392),
>>                  Wstscr=c(139.00,NA,101.80,103.00,76.40,116.00,139.80,111.31,NA,150.00))            
>> mydata     
>> 
>> doit12 <-function(variable,data) {
>> 
>> varxx <- deparse(substitute(variable))
>> cat("varxx created from first deparse substitute=",varxx,"\n")
>> varscr <- paste(varxx,"scr",sep="")
>> cat("1varscr=",varscr,"\n")
>> cat("Data inside doit12\n")
>> print(data)
>> cat("Print the Wstscr column of data. varscr created using paste after deparse substitute\n")
>> print(data[,varscr])
>> cat("\n\n")
>> 
>> SmallFn <- function(v,d) {
>>   cat("\nInside SmallFn\n")
>>   zz <-match.call()
>>   column <- deparse(substitute(v))
>>   cat("column=",column,"\n")
>>   cat("The results of match.call\n")
>>   print(zz)
>>   print("Hello world!")
>>   print(d)
>>   print(d[,column])
>> }
>> SmallFn(Wstscr,data)
>> SmallFn(varscr,data)
>> }
>> doit12(Wst,mydata)
>> John David Sorkin M.D., Ph.D.
>> Professor of Medicine
>> Chief, Biostatistics and Informatics
>> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine


David Winsemius
Alameda, CA, USA


From jsorkin at grecc.umaryland.edu  Wed Dec  2 03:17:27 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 01 Dec 2015 21:17:27 -0500
Subject: [R] What URL do I go to manage my subscription to the list? Please
 respond directly to me, not to this list!!!!
Message-ID: <565E0E67020000CB00141E0D@smtp.medicine.umaryland.edu>



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From jsorkin at grecc.umaryland.edu  Wed Dec  2 03:24:55 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 01 Dec 2015 21:24:55 -0500
Subject: [R] What URL do I go to manage my subscription to the r-help?
 Please respond directly to me, not to this list!!!!
Message-ID: <565E1027020000CB00141E20@smtp.medicine.umaryland.edu>

Please respond directly to me JSorkin at grecc.umaryland.edu. If you respond to the list I will not get your response. There is a problem with my subscription. It has been changed to digest. I need to set it back to no digest (i.e. get messages as soon as they are posted)
John


John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 




Call

Call from mobile

Send SMS

Add to Skype

You'll need Skype CreditFree via Skype
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From sarah.goslee at gmail.com  Wed Dec  2 03:28:30 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 1 Dec 2015 21:28:30 -0500
Subject: [R] What URL do I go to manage my subscription to the list?
 Please respond directly to me, not to this list!!!!
In-Reply-To: <565E0E67020000CB00141E0D@smtp.medicine.umaryland.edu>
References: <565E0E67020000CB00141E0D@smtp.medicine.umaryland.edu>
Message-ID: <CAM_vjumgxH8goym6ei5QUQu=KvwdMnfgYWXg7Rrr10CNqEVn0w@mail.gmail.com>

On Tue, Dec 1, 2015 at 9:17 PM, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.




-- 
Sarah Goslee
http://www.functionaldiversity.org


From jsorkin at grecc.umaryland.edu  Wed Dec  2 03:45:54 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 01 Dec 2015 21:45:54 -0500
Subject: [R] Function calling a function, column name not passed properly
Message-ID: <565E1512020000CB00141E3D@smtp.medicine.umaryland.edu>

David has  told me that my problem is because there is not column varscr
in my dataframe. I know this. My question is how can I modify my code so
that the second call to SmallFn will in fact access the column Wstscr
which does, in fact exist in the dataframe.
John
 
Messages sorted by: [ date ] [ thread ] [ subject ] [ author ] 

> On Dec 1, 2015, at 2:57 PM, John Sorkin <jsorkin at
grecc.umaryland.edu> wrote:> > I am trying to write a function that
calls a function. The first call to SmallFn works without any problem,
printing both the passed data and the column Wstscr. The second call
does not work (error, Error in d[, column] : subscript out of bounds). >
> The first call shows what I am trying to do with the second call. I am
passing to the outer function doit12 the beginning of the name of the
column that I want to access (Wst). doit12 creates the full name of the
column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and
varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12
as seen by the results of   print(data[,varscr]).> > SmallFn works when
it is called using SmallFn(Wstscr,data), but fails when called with
SmallFn(varscr,data). I don't understand why the second call fails.
varscr was shown to represent Wstscr.> > Please tell my why the second
call is not working, please put me out of one-full day of misery!It?s
telling you there is no column in that dataframe with the name ?varscr?.?
David.

 
 
 
I am trying to write a function that calls a function. The first call to
SmallFn works without any problem, printing both the passed data and the
column Wstscr. The second call does not work (error, Error in d[,
column] : subscript out of bounds). 
 
The first call shows what I am trying to do with the second call. I am
passing to the outer function doit12 the beginning of the name of the
column that I want to access (Wst). doit12 creates the full name of the
column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and
varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12
as seen by the results of   print(data[,varscr]).
 
SmallFn works when it is called using SmallFn(Wstscr,data), but fails
when called with SmallFn(varscr,data). I don't understand why the second
call fails. varscr was shown to represent Wstscr.
 
Please tell my why the second call is not working, please put me out of
one-full day of misery!
 
Thank you,
John
 
 
 

mydata <- cbind(  
patient_id=c(10163,10987,19882,19899,20104,20105,20167,20318,20338,20392),
                  
Wstscr=c(139.00,NA,101.80,103.00,76.40,116.00,139.80,111.31,NA,150.00)) 
          
mydata     
 
doit12 <-function(variable,data) {
  
  varxx <- deparse(substitute(variable))
  cat("varxx created from first deparse substitute=",varxx,"\n")
  varscr <- paste(varxx,"scr",sep="")
  cat("1varscr=",varscr,"\n")
  cat("Data inside doit12\n")
  print(data)
  cat("Print the Wstscr column of data. varscr created using paste after
deparse substitute\n")
  print(data[,varscr])
  cat("\n\n")
  
  SmallFn <- function(v,d) {
    cat("\nInside SmallFn\n")
    zz <-match.call()
    column <- deparse(substitute(v))
    cat("column=",column,"\n")
    cat("The results of match.call\n")
    print(zz)
    print("Hello world!")
    print(d)
    print(d[,column])
  }
  SmallFn(Wstscr,data)
  SmallFn(varscr,data)
}
doit12(Wst,mydata)



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and
Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 





Call

Call from mobile

Send SMS

Add to Skype

You'll need Skype CreditFree via Skype
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of MedicineBaltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of
the intended recipient(s) and may contain confidential and privileged
information. Any unauthorized use, disclosure or distribution is
prohibited. If you are not the intended recipient, please contact the
sender by reply email and destroy all copies of the original message. 

From wdunlap at tibco.com  Wed Dec  2 04:17:33 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 1 Dec 2015 19:17:33 -0800
Subject: [R] Function calling a function, column name not passed properly
In-Reply-To: <565E1512020000CB00141E3D@smtp.medicine.umaryland.edu>
References: <565E1512020000CB00141E3D@smtp.medicine.umaryland.edu>
Message-ID: <CAF8bMcYFi9rt4PedsZq1Pt5Oz=k_eGXsp_GuSnxNUNAom4AMSg@mail.gmail.com>

The short answer is to provide an argument that your function does not
pass through substitute(), so standard evaluation takes place.  E.g., change
    SmallFn <- function(v,d) {
        column <- deparse(substitute(v))
        d[,column]
    }
to
    SmallFn <- function(v, d, column=deparse(substitute(v))) {
        d[,column]
    }
and call it as either
    SmallFn(firstColumn, d=data.frame(firstColumn=1:3))
or
    col <- "firstColumn"
    SmallFn(column=col, d=data.frame(firstColumn=1:3))

If you want to get fancy, check out Hadley's lazyeval package.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Dec 1, 2015 at 6:45 PM, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> David has  told me that my problem is because there is not column varscr
> in my dataframe. I know this. My question is how can I modify my code so
> that the second call to SmallFn will in fact access the column Wstscr
> which does, in fact exist in the dataframe.
> John
>
> Messages sorted by: [ date ] [ thread ] [ subject ] [ author ]
>
>> On Dec 1, 2015, at 2:57 PM, John Sorkin <jsorkin at
> grecc.umaryland.edu> wrote:> > I am trying to write a function that
> calls a function. The first call to SmallFn works without any problem,
> printing both the passed data and the column Wstscr. The second call
> does not work (error, Error in d[, column] : subscript out of bounds). >
>> The first call shows what I am trying to do with the second call. I am
> passing to the outer function doit12 the beginning of the name of the
> column that I want to access (Wst). doit12 creates the full name of the
> column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and
> varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12
> as seen by the results of   print(data[,varscr]).> > SmallFn works when
> it is called using SmallFn(Wstscr,data), but fails when called with
> SmallFn(varscr,data). I don't understand why the second call fails.
> varscr was shown to represent Wstscr.> > Please tell my why the second
> call is not working, please put me out of one-full day of misery!It?s
> telling you there is no column in that dataframe with the name ?varscr?.?
> David.
>
>
>
>
> I am trying to write a function that calls a function. The first call to
> SmallFn works without any problem, printing both the passed data and the
> column Wstscr. The second call does not work (error, Error in d[,
> column] : subscript out of bounds).
>
> The first call shows what I am trying to do with the second call. I am
> passing to the outer function doit12 the beginning of the name of the
> column that I want to access (Wst). doit12 creates the full name of the
> column (Wstscr) by using varxx <- deparse(susbstitute(variable)) and
> varscr <- paste(varxx,"scr",sep=""). I can access the column in doit12
> as seen by the results of   print(data[,varscr]).
>
> SmallFn works when it is called using SmallFn(Wstscr,data), but fails
> when called with SmallFn(varscr,data). I don't understand why the second
> call fails. varscr was shown to represent Wstscr.
>
> Please tell my why the second call is not working, please put me out of
> one-full day of misery!
>
> Thank you,
> John
>
>
>
>
> mydata <- cbind(
> patient_id=c(10163,10987,19882,19899,20104,20105,20167,20318,20338,20392),
>
> Wstscr=c(139.00,NA,101.80,103.00,76.40,116.00,139.80,111.31,NA,150.00))
>
> mydata
>
> doit12 <-function(variable,data) {
>
>   varxx <- deparse(substitute(variable))
>   cat("varxx created from first deparse substitute=",varxx,"\n")
>   varscr <- paste(varxx,"scr",sep="")
>   cat("1varscr=",varscr,"\n")
>   cat("Data inside doit12\n")
>   print(data)
>   cat("Print the Wstscr column of data. varscr created using paste after
> deparse substitute\n")
>   print(data[,varscr])
>   cat("\n\n")
>
>   SmallFn <- function(v,d) {
>     cat("\nInside SmallFn\n")
>     zz <-match.call()
>     column <- deparse(substitute(v))
>     cat("column=",column,"\n")
>     cat("The results of match.call\n")
>     print(zz)
>     print("Hello world!")
>     print(d)
>     print(d[,column])
>   }
>   SmallFn(Wstscr,data)
>   SmallFn(varscr,data)
> }
> doit12(Wst,mydata)
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
>
>
>
>
> Call
>
> Call from mobile
>
> Send SMS
>
> Add to Skype
>
> You'll need Skype CreditFree via Skype
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of MedicineBaltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:12}}


From merricks.merricks at gmail.com  Wed Dec  2 03:47:47 2015
From: merricks.merricks at gmail.com (Margaret Donald)
Date: Wed, 2 Dec 2015 13:47:47 +1100
Subject: [R] Fwd:  Converting a matrix to an mcmc object
Message-ID: <CAM=LvPycOcVNfKeFq6J2qLP4iRt15710X4PbCSAgzLouSUhhHA@mail.gmail.com>

---------- Forwarded message ----------
From: Margaret Donald <merricks.merricks at gmail.com>
Date: 1 December 2015 at 23:09
Subject: Re: [R] Converting a matrix to an mcmc object
To: Jim Lemon <drjimlemon at gmail.com>


Thanks, Jim.  It may well do, but I am struggling to see how to coerce a
matrix into three chains... I could reprogram the whole show run it in
WinBUGS or OpenBUGS and process the coda, but I would prefer not to.

Regards,
Margaret.

On 30 November 2015 at 17:31, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Margaret,
> Doesn't the "mcmc" function in the "coda" package create an mcmc object
> from a matrix?
>
> Jim
>
>
> On Mon, Nov 30, 2015 at 9:44 AM, Margaret Donald <
> merricks.merricks at gmail.com> wrote:
>
>> Dear List,
>>
>> I am using R2jags and post process some mcmc objects to produce a new
>> object, which is currently a matrix, which replicates the shape of the
>> objects from which it is derived.
>>
>> I want to produce a Gelman-Rubin graph from its (implied) three chains,
>> but
>> I need to coerce the matrix into an mcmc object if I am to use gelman.plot
>> or gelman.diag from the R-package "coda".
>>
>> (The various mcmc functions in "coda" seem to work in the other
>> direction.....)
>>
>> Regards,
>> Margaret Donald
>>
>>
>>
>> --
>> Margaret Donald
>> Post Doctoral researcher
>> University of New South Wales
>> margaret.donald at unsw.edu.au
>> 0405 834 550
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>


-- 
Margaret Donald
Post Doctoral researcher
University of New South Wales
margaret.donald at unsw.edu.au
0405 834 550





Dear List,
>

I failed to make it to the help list.  I have gone to the latest version of
the r coda package,
and fail to find any help there as to how to coerce a vector (15000) or a
matrix (5000 x 3) into an mcmc object with a name (given by me) and having
three chains. I can get a single chain, but I want three.

I would appreciate an example of a matrix as described above and the code
needed to turn it into an mcmc object.   I could arduously produce coda
files and import them, but that seems ridiculous.

Regards

Margaret Donald

>
> I am using R2jags and post process some mcmc objects to produce a new
> object, which is currently a matrix, which replicates the shape of the
> objects from which it is derived.
>
> I want to produce a Gelman-Rubin graph from its (implied) three chains, but
> I need to coerce the matrix into an mcmc object if I am to use gelman.plot
> or gelman.diag from the R-package "coda".
>
> (The various mcmc functions in "coda" seem to work in the other
> direction.....)
>
> Regards,
> Margaret Donald
>
>
>


-- 
Margaret Donald
Post Doctoral researcher
University of New South Wales
margaret.donald at unsw.edu.au
0405 834 550

	[[alternative HTML version deleted]]


From ravi.varadhan at jhu.edu  Tue Dec  1 23:47:21 2015
From: ravi.varadhan at jhu.edu (Ravi Varadhan)
Date: Tue, 1 Dec 2015 22:47:21 +0000
Subject: [R] grep/regexp
Message-ID: <72a66f5aced94adf9f1c3588e242e089@ESGEBEX10.win.ad.jhu.edu>

Hi,
I would appreciate some help with using grep().  I have a bunch of variables in a data frame and I would like to select some of them using grep.  Here is an example of what I am trying to do:

vars <- c("Fr_I_total", "Fr_I_percent_of_CD4", "Ki.67_in_Fr_I_percent_of_Fr_I", "Fr_II_percent_of_CD4", "Ki.67_in_Fr_II_percent_of_Fr_II")

>From the above vector, I would like to select those variables beginning with `Fr' and containing `percent' in them.   In other words, I would like to get the variables "Fr_I_percent_of_CD4" and "Fr_II_percent_of_CD4".

How can I use grep() to do this?

More generally, are there any good online resources with examples like this for the use of grep() and regexp() in R?  I didn't find the help pages for these very user-friendly.

Thank you very much,
Ravi

Ravi Varadhan, Ph.D. (Biostatistics), Ph.D. (Environmental Engg)
Associate Professor,  Department of Oncology
Division of Biostatistics & Bionformatics
Sidney Kimmel Comprehensive Cancer Center
Johns Hopkins University
550 N. Broadway, Suite 1111-E
Baltimore, MD 21205
410-502-2619


	[[alternative HTML version deleted]]


From suttoncarl at ymail.com  Wed Dec  2 05:01:30 2015
From: suttoncarl at ymail.com (Carl Sutton)
Date: Wed, 2 Dec 2015 04:01:30 +0000 (UTC)
Subject: [R] problem with creating a file path using paste0
References: <531325727.13105682.1449028890540.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <531325727.13105682.1449028890540.JavaMail.yahoo@mail.yahoo.com>

I am relatively new?to programming in R.?? 
I have attempted to create a function to replace brute force code, and in doing so need to create a data path and file name?via paste0?(The years and quarter subfolders are different for different years and quarters).? ?It file path?created from paste0 code looks?perfect to me but bombs.? I suspect the problem lies with : and / being r functions and thus changing them to character is a problem.? However, if they are unquoted I get more error messages.? ?Identical states the two character strings are not identical.? Have I done something stupid or is there something happening I am not aware of.? Code script from RStudio follows: then console window results.
require(data.table)
#? load column names
load("profileColNames")
#? Start loading data, read in data, convert to data table for each quarterdataRead <- function(year,quarter){
??? years <- year-2000
??? years <- as.character(years)
??? quarter <- as.character(quarter)
??? print (years)
??? qtrName <- paste0("profile",years,"q",quarter)
??? print (qtrName)
??? qtr <- paste0("Q",quarter)
??? print (qtr)
??? filePath <- paste0("c",":",/year/,"Q",quarter,"_",year/,"pL_profile.txt")??? ?note- this is the problem child!!!!
#what I want the above to be
#??? "c:/2012/Q1_2012/pL_profile.txt", 
??? print (filePath)
#? the following line does not work?????????????????????????????????????? ?without a correct file path it can't work
??? #qtrName <- read.csv(file = fileName, header = FALSE,
???? #?????????????????? stringsAsFactors = FALSE)
??? #names(qtrName) <- profileColNames
?? # return(qtrName)
}
data <- dataRead(2012,1)
data
identical(data,"c:/2012/Q1_2012/pL_profile.txt")
#? the folowing typed line works
profile12q1 <- read.csv (file = "c:/2012/Q1_2012/pL_profile.txt", 
???????????????????????? header = FALSE, sep = ",", stringsAsFactors=FALSE)
and results from the console from first error message:

| +     print (qtr)
+     filePath <- paste0("c",":",/year/,"Q",quarter,"_",year/,"pL_profile.txt")
Error: unexpected '/' in:
"    print (qtr)
    filePath <- paste0("c",":",/"
> #what I want the above to be
> #    "c:/2012/Q1_2012/pL_profile.txt", 
>     print (filePath)
Error in print(filePath) : object 'filePath' not found
> #  the following line does not work
>     #qtrName <- read.csv(file = fileName, header = FALSE,
>      #                   stringsAsFactors = FALSE)
>     #names(qtrName) <- profileColNames
>    # return(qtrName)
> }
Error: unexpected '}' in "}"
> data <- dataRead(2012,1)
[1] "12"
[1] "profile12q1"
[1] "Q1"
[1] "c:/2012/Q1_2012/pL-profile.txt"
> data
[1] "c:/2012/Q1_2012/pL-profile.txt"
> identical(data,"c:/2012/Q1_2012/pL_profile.txt")
[1] FALSE
> #  the folowing typed line works
> profile12q1 <- read.csv (file = "c:/2012/Q1_2012/pL_profile.txt", 
+                          header = FALSE, sep = ",", stringsAsFactors=FALSE)
> dim(profile12q1)
[1] 74971   574
> class(profile12q1)
[1] "data.frame"
 |
|  |
| 
| >  |

 |

?Carl Sutton

	[[alternative HTML version deleted]]


From brant.inman at me.com  Wed Dec  2 06:11:04 2015
From: brant.inman at me.com (Brant Inman)
Date: Wed, 02 Dec 2015 00:11:04 -0500
Subject: [R] Passing variable names in quotes to a function
Message-ID: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>

I am trying to build a function that can accept variables for a regression.  It would work something like this:

---
# Y = my response variable (e.g. income)
# X = my key predictor variable (e.g. education)
# subY = a subsetting variable for Y (e.g. race)
# subY.val = the value of the subsetting value that I want (e.g. ?black?)

foo <- function(Y, X, subY, subY.val, dataset){
  
  if(is.na(subY) == F) {
     Y <- paste(Y, ?[?, subY, ?==?, subY.val, ?]?)
  }
  FORMULA <- paste(Y ~ X)
  fit <- some.regression.tool(FORMULA, data=dataset)

  return(some.data.after.processing)
}
---

If I call this function with, foo(income, education, race, ?black?, my.dataset), I do not get the result that I need because the FORMULA is "income[race==black] ~ education? when what I need is ?income[race==?black?] ~ education?.  How do I get the quotes to stay on ?black??  Or, is there a better way?

Help appreciated.

--
Brant
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec  2 07:21:12 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Dec 2015 22:21:12 -0800
Subject: [R] grep/regexp
In-Reply-To: <72a66f5aced94adf9f1c3588e242e089@ESGEBEX10.win.ad.jhu.edu>
References: <72a66f5aced94adf9f1c3588e242e089@ESGEBEX10.win.ad.jhu.edu>
Message-ID: <927183E3-F836-4D24-AFC3-FC67E45A191D@comcast.net>


> On Dec 1, 2015, at 2:47 PM, Ravi Varadhan <ravi.varadhan at jhu.edu> wrote:
> 
> Hi,
> I would appreciate some help with using grep().  I have a bunch of variables in a data frame and I would like to select some of them using grep.  Here is an example of what I am trying to do:
> 
> vars <- c("Fr_I_total", "Fr_I_percent_of_CD4", "Ki.67_in_Fr_I_percent_of_Fr_I", "Fr_II_percent_of_CD4", "Ki.67_in_Fr_II_percent_of_Fr_II")
> 
>> From the above vector, I would like to select those variables beginning with `Fr' and containing `percent' in them.   In other words, I would like to get the variables "Fr_I_percent_of_CD4" and "Fr_II_percent_of_CD4".
> 
> How can I use grep() to do this?

> grep("^Fr.*percent", vars, value=TRUE)
[1] "Fr_I_percent_of_CD4"  ?Fr_II_percent_of_CD4"


> More generally, are there any good online resources with examples like this for the use of grep() and regexp() in R?  I didn't find the help pages for these very user-friendly.
> 

There are several interactive regex websites where you can get commented and tested solutions. They have the disadvantage that they don?t use the extra backslashes that R requires since both R and regex use backslashes as escape characters.

I learned regex from the R's ?regex page and by watching Gabor Grothendeick?s postings to Rhelp. (I?ve probably gone over the ?regex page 20 or thirty times.)

These days there are a great many regex questions and answers on Stack Overflow , many of them again written by Maestro Grothendeick.

? 

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Wed Dec  2 07:23:52 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Dec 2015 22:23:52 -0800
Subject: [R] What URL do I go to manage my subscription to the r-help?
	Please respond directly to me, not to this list!!!!
In-Reply-To: <565E1027020000CB00141E20@smtp.medicine.umaryland.edu>
References: <565E1027020000CB00141E20@smtp.medicine.umaryland.edu>
Message-ID: <0B2A8062-CD71-487D-860D-6A8436BA062C@comcast.net>


> On Dec 1, 2015, at 6:24 PM, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> 
> Please respond directly to me JSorkin at grecc.umaryland.edu.

I generally respond to both the poster and the list.

> If you respond to the list I will not get your response. There is a problem with my subscription. It has been changed to digest. I need to set it back to no digest (i.e. get messages as soon as they are posted)
> John

The link is found at the bottom of the listinfo page linked at the bottom of every posting from Rhelp.


> 
> 
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> 
> 
> 
> Call
> 
> Call from mobile
> 
> Send SMS
> 
> Add to Skype
> 
> You'll need Skype CreditFree via Skype
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:15}}


From ligges at statistik.tu-dortmund.de  Wed Dec  2 07:33:56 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 2 Dec 2015 07:33:56 +0100
Subject: [R] Fwd: Converting a matrix to an mcmc object
In-Reply-To: <CAM=LvPycOcVNfKeFq6J2qLP4iRt15710X4PbCSAgzLouSUhhHA@mail.gmail.com>
References: <CAM=LvPycOcVNfKeFq6J2qLP4iRt15710X4PbCSAgzLouSUhhHA@mail.gmail.com>
Message-ID: <565E90D4.9080502@statistik.tu-dortmund.de>

If you have a matrix X and you want an mcmc.list object for 3 chains:

as.mcmc.list(lapply(as.data.frame(X), mcmc))

is one way.

Best,
Uwe Ligges


On 02.12.2015 03:47, Margaret Donald wrote:
> ---------- Forwarded message ----------
> From: Margaret Donald <merricks.merricks at gmail.com>
> Date: 1 December 2015 at 23:09
> Subject: Re: [R] Converting a matrix to an mcmc object
> To: Jim Lemon <drjimlemon at gmail.com>
>
>
> Thanks, Jim.  It may well do, but I am struggling to see how to coerce a
> matrix into three chains... I could reprogram the whole show run it in
> WinBUGS or OpenBUGS and process the coda, but I would prefer not to.
>
> Regards,
> Margaret.
>
> On 30 November 2015 at 17:31, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Margaret,
>> Doesn't the "mcmc" function in the "coda" package create an mcmc object
>> from a matrix?
>>
>> Jim
>>
>>
>> On Mon, Nov 30, 2015 at 9:44 AM, Margaret Donald <
>> merricks.merricks at gmail.com> wrote:
>>
>>> Dear List,
>>>
>>> I am using R2jags and post process some mcmc objects to produce a new
>>> object, which is currently a matrix, which replicates the shape of the
>>> objects from which it is derived.
>>>
>>> I want to produce a Gelman-Rubin graph from its (implied) three chains,
>>> but
>>> I need to coerce the matrix into an mcmc object if I am to use gelman.plot
>>> or gelman.diag from the R-package "coda".
>>>
>>> (The various mcmc functions in "coda" seem to work in the other
>>> direction.....)
>>>
>>> Regards,
>>> Margaret Donald
>>>
>>>
>>>
>>> --
>>> Margaret Donald
>>> Post Doctoral researcher
>>> University of New South Wales
>>> margaret.donald at unsw.edu.au
>>> 0405 834 550
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>
>


From ligges at statistik.tu-dortmund.de  Wed Dec  2 07:39:05 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 2 Dec 2015 07:39:05 +0100
Subject: [R] problem with creating a file path using paste0
In-Reply-To: <531325727.13105682.1449028890540.JavaMail.yahoo@mail.yahoo.com>
References: <531325727.13105682.1449028890540.JavaMail.yahoo.ref@mail.yahoo.com>
	<531325727.13105682.1449028890540.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <565E9209.5050505@statistik.tu-dortmund.de>



On 02.12.2015 05:01, Carl Sutton via R-help wrote:
> I am relatively new to programming in R.
> I have attempted to create a function to replace brute force code, and in doing so need to create a data path and file name via paste0 (The years and quarter subfolders are different for different years and quarters).   It file path created from paste0 code looks perfect to me but bombs.  I suspect the problem lies with : and / being r functions and thus changing them to character is a problem.  However, if they are unquoted I get more error messages.   Identical states the two character strings are not identical.  Have I done something stupid or is there something happening I am not aware of.  Code script from RStudio follows: then console window results.
> require(data.table)
> #  load column names
> load("profileColNames")
> #  Start loading data, read in data, convert to data table for each quarterdataRead <- function(year,quarter){
>      years <- year-2000
>      years <- as.character(years)
>      quarter <- as.character(quarter)
>      print (years)
>      qtrName <- paste0("profile",years,"q",quarter)
>      print (qtrName)
>      qtr <- paste0("Q",quarter)
>      print (qtr)
>      filePath <- paste0("c",":",/year/,"Q",quarter,"_",year/,"pL_profile.txt")     note- this is the problem child!!!!


  filePath <- paste0("c:/", year, "/Q", quarter, "_", year, 
"/pL_profile.txt")

or


filePath <- file.path("c:", year, paste0("Q", quarter, "_", year), 
"pL_profile.txt")

Best,
Uwe Ligges


> #what I want the above to be
> #    "c:/2012/Q1_2012/pL_profile.txt",
>      print (filePath)
> #  the following line does not work                                        without a correct file path it can't work
>      #qtrName <- read.csv(file = fileName, header = FALSE,
>       #                   stringsAsFactors = FALSE)
>      #names(qtrName) <- profileColNames
>     # return(qtrName)
> }
> data <- dataRead(2012,1)
> data
> identical(data,"c:/2012/Q1_2012/pL_profile.txt")
> #  the folowing typed line works
> profile12q1 <- read.csv (file = "c:/2012/Q1_2012/pL_profile.txt",
>                           header = FALSE, sep = ",", stringsAsFactors=FALSE)
> and results from the console from first error message:
>
> | +     print (qtr)
> +     filePath <- paste0("c",":",/year/,"Q",quarter,"_",year/,"pL_profile.txt")
> Error: unexpected '/' in:
> "    print (qtr)
>      filePath <- paste0("c",":",/"
>> #what I want the above to be
>> #    "c:/2012/Q1_2012/pL_profile.txt",
>>      print (filePath)
> Error in print(filePath) : object 'filePath' not found
>> #  the following line does not work
>>      #qtrName <- read.csv(file = fileName, header = FALSE,
>>       #                   stringsAsFactors = FALSE)
>>      #names(qtrName) <- profileColNames
>>     # return(qtrName)
>> }
> Error: unexpected '}' in "}"
>> data <- dataRead(2012,1)
> [1] "12"
> [1] "profile12q1"
> [1] "Q1"
> [1] "c:/2012/Q1_2012/pL-profile.txt"
>> data
> [1] "c:/2012/Q1_2012/pL-profile.txt"
>> identical(data,"c:/2012/Q1_2012/pL_profile.txt")
> [1] FALSE
>> #  the folowing typed line works
>> profile12q1 <- read.csv (file = "c:/2012/Q1_2012/pL_profile.txt",
> +                          header = FALSE, sep = ",", stringsAsFactors=FALSE)
>> dim(profile12q1)
> [1] 74971   574
>> class(profile12q1)
> [1] "data.frame"
>   |
> |  |
> |
> | >  |
>
>   |
>
>   Carl Sutton
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dwinsemius at comcast.net  Wed Dec  2 07:40:18 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 1 Dec 2015 22:40:18 -0800
Subject: [R] problem with creating a file path using paste0
In-Reply-To: <531325727.13105682.1449028890540.JavaMail.yahoo@mail.yahoo.com>
References: <531325727.13105682.1449028890540.JavaMail.yahoo.ref@mail.yahoo.com>
	<531325727.13105682.1449028890540.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <B6B06CDF-8FD9-44C0-91E0-27802009B34A@comcast.net>


> On Dec 1, 2015, at 8:01 PM, Carl Sutton via R-help <r-help at r-project.org> wrote:
> 
> I am relatively new to programming in R.   
> I have attempted to create a function to replace brute force code, and in doing so need to create a data path and file name via paste0 (The years and quarter subfolders are different for different years and quarters).   It file path created from paste0 code looks perfect to me but bombs.  I suspect the problem lies with : and / being r functions and thus changing them to character is a problem.

Forward slashes shoudl be acceptable for any OS.


>  However, if they are unquoted I get more error messages.   Identical states the two character strings are not identical.  Have I done something stupid or is there something happening I am not aware of.  Code script from RStudio follows: then console window results.
> require(data.table)
> #  load column names
> load("profileColNames")
> #  Start loading data, read in data, convert to data table for each quarterdataRead <- function(year,quarter){
>     years <- year-2000
>     years <- as.character(years)
>     quarter <- as.character(quarter)
>     print (years)
>     qtrName <- paste0("profile",years,"q",quarter)
>     print (qtrName)
>     qtr <- paste0("Q",quarter)
>     print (qtr)
>     filePath <- paste0("c",":",/year/,"Q",quarter,"_",year/,"pL_profile.txt")     note- this is the problem child!!!!

Right. Why are you putting forward slashes _outside_ the quotes??? Aren?t you getting errors from this.

I will parse out your arguments:

paste0("c?,
         ":?,
        /year/,    # It was spelled ?years? above
        "Q?,
       quarter,
       "_?,
        year/,     # The only reason the misspelling of `years` is not being caught is the slash triggered an error first.
        "pL_profile.txt")


If those forward slashes are some misguided effort or spawn-of-HTML-demons to emphasize something, then you also need to pay attention to the posting guide when it asks for plain text. Try

filePath <- paste0("c:/?, years, ?/Q?, quarter, "_?, years, ?/pL_profile.txt")


> #what I want the above to be
> #    "c:/2012/Q1_2012/pL_profile.txt", 
>     print (filePath)
> #  the following line does not work    s


Well, I didn?t think it looked very promising.


>                                    without a correct file path it can't work
>     #qtrName <- read.csv(file = fileName, header = FALSE,
>      #                   stringsAsFactors = FALSE)
>     #names(qtrName) <- profileColNames
>    # return(qtrName)
> }
> data <- dataRead(2012,1)
> data
> identical(data,"c:/2012/Q1_2012/pL_profile.txt")
> #  the folowing typed line works
> profile12q1 <- read.csv (file = "c:/2012/Q1_2012/pL_profile.txt", 
>                          header = FALSE, sep = ",", stringsAsFactors=FALSE)
> and results from the console from first error message:
> 
> | +     print (qtr)
> +     filePath <- paste0("c",":",/year/,"Q",quarter,"_",year/,"pL_profile.txt")
> Error: unexpected '/' in:


Yes, as I suspected. WTF is the slash doing outside the quotes?


> "    print (qtr)
>    filePath <- paste0("c",":",/"
>> #what I want the above to be
>> #    "c:/2012/Q1_2012/pL_profile.txt", 
>>    print (filePath)
> Error in print(filePath) : object 'filePath' not found
>> #  the following line does not work
>>    #qtrName <- read.csv(file = fileName, header = FALSE,
>>     #                   stringsAsFactors = FALSE)
>>    #names(qtrName) <- profileColNames
>>   # return(qtrName)
>> }
> Error: unexpected '}' in "}"
>> data <- dataRead(2012,1)
> [1] "12"
> [1] "profile12q1"
> [1] "Q1"
> [1] "c:/2012/Q1_2012/pL-profile.txt"
>> data
> [1] "c:/2012/Q1_2012/pL-profile.txt"
>> identical(data,"c:/2012/Q1_2012/pL_profile.txt")
> [1] FALSE
>> #  the folowing typed line works
>> profile12q1 <- read.csv (file = "c:/2012/Q1_2012/pL_profile.txt", 
> +                          header = FALSE, sep = ",", stringsAsFactors=FALSE)
>> dim(profile12q1)
> [1] 74971   574
>> class(profile12q1)
> [1] "data.frame"
> |
> |  |
> | 
> | >  |
> 
> |
> 
>  Carl Sutton
> 
> 	[[alternative HTML version deleted]]

Plain text, plain text, PLAIN TEXT.

 PLEASE do read the posting guide http://www.R-project.org/posting-guide.html

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> 
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From ligges at statistik.tu-dortmund.de  Wed Dec  2 07:45:47 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 2 Dec 2015 07:45:47 +0100
Subject: [R] Passing variable names in quotes to a function
In-Reply-To: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>
References: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>
Message-ID: <565E939B.5030206@statistik.tu-dortmund.de>



On 02.12.2015 06:11, Brant Inman wrote:
> I am trying to build a function that can accept variables for a regression.  It would work something like this:
>
> ---
> # Y = my response variable (e.g. income)
> # X = my key predictor variable (e.g. education)
> # subY = a subsetting variable for Y (e.g. race)
> # subY.val = the value of the subsetting value that I want (e.g. ?black?)
>
> foo <- function(Y, X, subY, subY.val, dataset){
>
>    if(is.na(subY) == F) {


Not sure why this all is needed, but you can insert here:

if(is.character(subY.val) || is.factor(subY.val)) subY.val <- 
shQuote(subY.val)

Best,
Uwe Ligges


>       Y <- paste(Y, ?[?, subY, ?==?, subY.val, ?]?)
>    }
>    FORMULA <- paste(Y ~ X)
>    fit <- some.regression.tool(FORMULA, data=dataset)
>
>    return(some.data.after.processing)
> }
> ---
>
> If I call this function with, foo(income, education, race, ?black?, my.dataset), I do not get the result that I need because the FORMULA is "income[race==black] ~ education? when what I need is ?income[race==?black?] ~ education?.  How do I get the quotes to stay on ?black??  Or, is there a better way?
>
> Help appreciated.
>
> --
> Brant
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From dusa.adrian at unibuc.ro  Wed Dec  2 13:07:14 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 2 Dec 2015 14:07:14 +0200
Subject: [R] fill the area outside a polygon
Message-ID: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>

Dear All,

I know how to fill a polygon, using a basic R graphics device:

par(mai=c(0, 0, 0, 0))
plot(1:100, type="n")
polygon(c(20, 80, 80, 20), c(20, 20, 80, 80), col="lightblue")


But let's say I have an outer polygon like this:
polygon(c(0,100,100,0), c(0,0,100,100))

Is it possible to fill the area between the two polygons?

I realise there is a quick and dirty solution to fill the outer polygon,
then draw the inner polygon colored with white, but I would like to
actually "clip" the outer polygon to the boundaries of the inner one.

Of course, there are various packages which draw maps (i.e. maptools), but
I need this in base R graphics. In other words, is it possible to define a
polygon with a hole inside, in base R?

Thank you,
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From phgrosjean at sciviews.org  Wed Dec  2 13:10:14 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Wed, 2 Dec 2015 13:10:14 +0100
Subject: [R] Passing variable names in quotes to a function
In-Reply-To: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>
References: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>
Message-ID: <86FEC814-C163-480D-B0E4-688353E44F45@sciviews.org>

Your example and explanation are not complete, but I have the gut feeling that you could do all this both more efficiently *and* more R-ish.

First of all, why would you pass Y and X separately, to ultimately build the Y ~ X formula within the body of your function?

Secondly, it seems to me that subY and subY.val does something very similar to the subset argument in, say, lm().

Personally, I would write it like this:

foo <- function(formula, data, subset) {
  if (!missing(subset))
    data <- data[subset, ]
  fit <- some_regression_tool(formula, data = data)

  ## <more code>

  data_after_processing
}
 
with subset = subY == subY.val.

Best,

Philippe

> On 02 Dec 2015, at 06:11, Brant Inman <brant.inman at me.com> wrote:
> 
> I am trying to build a function that can accept variables for a regression.  It would work something like this:
> 
> ---
> # Y = my response variable (e.g. income)
> # X = my key predictor variable (e.g. education)
> # subY = a subsetting variable for Y (e.g. race)
> # subY.val = the value of the subsetting value that I want (e.g. ?black?)
> 
> foo <- function(Y, X, subY, subY.val, dataset){
> 
>  if(is.na(subY) == F) {
>     Y <- paste(Y, ?[?, subY, ?==?, subY.val, ?]?)
>  }
>  FORMULA <- paste(Y ~ X)
>  fit <- some.regression.tool(FORMULA, data=dataset)
> 
>  return(some.data.after.processing)
> }
> ---
> 
> If I call this function with, foo(income, education, race, ?black?, my.dataset), I do not get the result that I need because the FORMULA is "income[race==black] ~ education? when what I need is ?income[race==?black?] ~ education?.  How do I get the quotes to stay on ?black??  Or, is there a better way?
> 
> Help appreciated.
> 
> --
> Brant
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From brant.inman at me.com  Wed Dec  2 16:09:12 2015
From: brant.inman at me.com (Brant Inman)
Date: Wed, 02 Dec 2015 10:09:12 -0500
Subject: [R] Passing variable names in quotes to a function
In-Reply-To: <86FEC814-C163-480D-B0E4-688353E44F45@sciviews.org>
References: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>
	<86FEC814-C163-480D-B0E4-688353E44F45@sciviews.org>
Message-ID: <F079CE0C-13AE-47C8-B9A6-F80A15A17349@me.com>

Thank you for your response.  Here is the problem that I find with your code (which I had tried).  When you pass a value to the subset argument of the function, it will not hold the quotes on the subsetting variable?s value.

For example, if I want the function to do  Y[Z==?skinny?] so that we use only those values of Y where Z is equal to skinny, I need to be able to retain the quotes around skinny. If you try passing ?Z==?skinny?? to the  function, it will remove the quotes and give you Z==skinny, which does not work in the subsetting code.




> On Dec 2, 2015, at 7:10 AM, phgrosjean at sciviews.org wrote:
> 
> Your example and explanation are not complete, but I have the gut feeling that you could do all this both more efficiently *and* more R-ish.
> 
> First of all, why would you pass Y and X separately, to ultimately build the Y ~ X formula within the body of your function?
> 
> Secondly, it seems to me that subY and subY.val does something very similar to the subset argument in, say, lm().
> 
> Personally, I would write it like this:
> 
> foo <- function(formula, data, subset) {
>  if (!missing(subset))
>    data <- data[subset, ]
>  fit <- some_regression_tool(formula, data = data)
> 
>  ## <more code>
> 
>  data_after_processing
> }
> 
> with subset = subY == subY.val.
> 
> Best,
> 
> Philippe
> 
>> On 02 Dec 2015, at 06:11, Brant Inman <brant.inman at me.com> wrote:
>> 
>> I am trying to build a function that can accept variables for a regression.  It would work something like this:
>> 
>> ---
>> # Y = my response variable (e.g. income)
>> # X = my key predictor variable (e.g. education)
>> # subY = a subsetting variable for Y (e.g. race)
>> # subY.val = the value of the subsetting value that I want (e.g. ?black?)
>> 
>> foo <- function(Y, X, subY, subY.val, dataset){
>> 
>> if(is.na(subY) == F) {
>>    Y <- paste(Y, ?[?, subY, ?==?, subY.val, ?]?)
>> }
>> FORMULA <- paste(Y ~ X)
>> fit <- some.regression.tool(FORMULA, data=dataset)
>> 
>> return(some.data.after.processing)
>> }
>> ---
>> 
>> If I call this function with, foo(income, education, race, ?black?, my.dataset), I do not get the result that I need because the FORMULA is "income[race==black] ~ education? when what I need is ?income[race==?black?] ~ education?.  How do I get the quotes to stay on ?black??  Or, is there a better way?
>> 
>> Help appreciated.
>> 
>> --
>> Brant
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 


From dcarlson at tamu.edu  Wed Dec  2 16:19:13 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 2 Dec 2015 15:19:13 +0000
Subject: [R] fill the area outside a polygon
In-Reply-To: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
References: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E6177@mb02.ads.tamu.edu>

Using only base graphics, one solution would be to embed the inner polygon in the outer one and turn off the border:

par(mai=c(0, 0, 0, 0))
plot(1:100, type="n")
polygon(c(0, 100, 100, 0, 0, 20, 80, 80, 20, 20, 0), 
     c(0, 0, 100, 100, 0, 20, 20, 80, 80, 20, 0), col="lightblue", border=NA)

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Adrian Du?a
Sent: Wednesday, December 2, 2015 6:07 AM
To: r-help at r-project.org
Subject: [R] fill the area outside a polygon

Dear All,

I know how to fill a polygon, using a basic R graphics device:

par(mai=c(0, 0, 0, 0))
plot(1:100, type="n")
polygon(c(20, 80, 80, 20), c(20, 20, 80, 80), col="lightblue")


But let's say I have an outer polygon like this:
polygon(c(0,100,100,0), c(0,0,100,100))

Is it possible to fill the area between the two polygons?

I realise there is a quick and dirty solution to fill the outer polygon,
then draw the inner polygon colored with white, but I would like to
actually "clip" the outer polygon to the boundaries of the inner one.

Of course, there are various packages which draw maps (i.e. maptools), but
I need this in base R graphics. In other words, is it possible to define a
polygon with a hole inside, in base R?

Thank you,
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From ragland.debra at yahoo.com  Wed Dec  2 16:39:50 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Wed, 2 Dec 2015 15:39:50 +0000 (UTC)
Subject: [R] Looping through multiple sub elements of a list to compare to
 multiple components of a vector
References: <996837991.14023179.1449070790818.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <996837991.14023179.1449070790818.JavaMail.yahoo@mail.yahoo.com>

I think I am making this problem harder than it has to be and so I keep getting stuck on what might be a trivial problem.?
I have used the seqinr package to load a protein sequence alignment containing 15 protein sequences;
? ? > library(seqinr)? ? > x = read.alignment("proteins.fasta",format="fasta",forceToLower=FALSE)This automatically loads in a list of 4 elements including the sequences and other information.
I store the sequences to a new list;
? ?> mylist = x$seqwhich returns a character vector of 15 strings.
I have found that if I split the long character strings into individual characters it is easy to use lapply to loop over this list. So I use strsplit;
? ? >list.2 = strsplit(mylist, split = NULL)
>From this list I can determine which proteins have changes at certain positions by using;
? ? >lapply(list.2, "[", 10) == "L"This returns a logical T/F vector for those elements of the list that do/do not the letter L at position 10.?
Because each of the protein sequences contains 99amino acids, I want to automate this process so that I do not have to compare/contrast positions 1 x 1. Most of the changes occur between positions/letters 10-95. I have a standard character vector that I wish to use for comparison when looping through the list.?
Should I perhaps combine all -- ?the standard "letter"/aa vector, the list of protein sequences -- into one list? Or is it better to leave them separate for this comparison? I'm not sure what the output should be as I need to use it for another statistical test. Would a list of logical vectors be the most sufficient output to return??
	[[alternative HTML version deleted]]


From jlorenz at uni-goettingen.de  Wed Dec  2 17:26:13 2015
From: jlorenz at uni-goettingen.de (Lorenz, Jennifer)
Date: Wed, 2 Dec 2015 16:26:13 +0000
Subject: [R] Significance level for glm.cluster
Message-ID: <51661F79C9BE0341B082F375A44402FA57373188@UM-EXCDAG-A03.um.gwdg.de>

Hi everyone,

I have a question with reference to "glm.cluster" from the package "miceadds" and hope that someone can help me. I am trying to calculate cluster-robust standard errors for a glm-model with multiply imputed datasets. Everything works just fine with glm.cluster but in the end I just get an output with estimates, standard errors and confidence intervals. But I need to report the significance level and I cannot figure out how to obtain this.
Here is the code I ran:

mod <- lapply(impulist, FUN=function(imp1){
  glm.cluster(data=imp1, formula=AV ~ p123 + ISEI, family=binomial("logit"), cluster=imp1$ID_s)
})
# extract parameters and covariance matrix
betas <- lapply( mod , FUN = function(rr){ coef(rr) } )
vars <- lapply( mod , FUN = function(rr){ vcov(rr) } )
# conduct statistical inference
summary( mitools::MIcombine(betas,vars) )

And that's what the output looks like:


Multiple imputation results:

      MIcombine.default(betas, vars)

                  results           se        (lower        upper) missInfo

(Intercept) -2.785309e+00 1.344773e-01 -3.053180e+00 -2.5174383210     25 %

P123         8.273687e-05 2.834233e-05  1.797328e-05  0.0001475005     74 %

ISEI         3.788065e-02 2.347762e-03  3.270191e-02  0.0430593934     67 %

Thanks for your help!
Best,
Jen


---
Jennifer Lorenz, M.A.
Georg-August-Universit?t G?ttingen
Sozialwissenschaftliche Fakult?t
Institut f?r Erziehungswissenschaft
Lehrstuhl Schulp?dagogik / Empirische Schulforschung

e-mail: jlorenz at uni-goettingen.de<mailto:jlorenz at uni-goettingen.de>
phone: 0551-39-21411
adress: Waldweg 26, 37073 G?ttingen
room: 8.106


	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Wed Dec  2 17:31:44 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 2 Dec 2015 18:31:44 +0200
Subject: [R] fill the area outside a polygon
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E6177@mb02.ads.tamu.edu>
References: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E6177@mb02.ads.tamu.edu>
Message-ID: <CAJ=0CtC5oyd=dzvqYrTiPEee=xEdVByQx3V0BO9E=hM8dcKahw@mail.gmail.com>

On Wed, Dec 2, 2015 at 5:19 PM, David L Carlson <dcarlson at tamu.edu> wrote:
>
> Using only base graphics, one solution would be to embed the inner
polygon in the outer one and turn off the border:
>
> par(mai=c(0, 0, 0, 0))
> plot(1:100, type="n")
> polygon(c(0, 100, 100, 0, 0, 20, 80, 80, 20, 20, 0),
>      c(0, 0, 100, 100, 0, 20, 20, 80, 80, 20, 0), col="lightblue",
border=NA)


I see what you did, interesting.
I do need the border though, although if this is the only solution, perhaps
I could live without it.

It's working on a Windows machine (which I presume you are using), but
under MacOS I only get  the entire big polygon filled, including the hole
in the middle.

Thanks,
Adrian

--
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From phgrosjean at sciviews.org  Wed Dec  2 17:50:08 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Wed, 2 Dec 2015 17:50:08 +0100
Subject: [R] Passing variable names in quotes to a function
In-Reply-To: <F079CE0C-13AE-47C8-B9A6-F80A15A17349@me.com>
References: <2D669C9A-9031-45F0-A508-0C2DD04CD83D@me.com>
	<86FEC814-C163-480D-B0E4-688353E44F45@sciviews.org>
	<F079CE0C-13AE-47C8-B9A6-F80A15A17349@me.com>
Message-ID: <1ACB380E-65F5-4B71-9032-92AAF9B6C31B@sciviews.org>


> On 02 Dec 2015, at 16:09, Brant Inman <brant.inman at me.com> wrote:
> 
> Thank you for your response.  Here is the problem that I find with your code (which I had tried).  When you pass a value to the subset argument of the function, it will not hold the quotes on the subsetting variable?s value.
> 
> For example, if I want the function to do  Y[Z==?skinny?] so that we use only those values of Y where Z is equal to skinny, I need to be able to retain the quotes around skinny. If you try passing ?Z==?skinny?? to the  function, it will remove the quotes and give you Z==skinny, which does not work in the subsetting code.
> 
> 

In this simple version, you must completely specify subset (i.e., data$var == "value"). It differs a little bit from the subset= argument in, say, lm().
An example:

# The lm() way:
lm(Sepal.Length ~ Petal.Length, data = iris, subset = Species == "sets")


# My function
foo <- function(formula, data, subset) {
  if (!missing(subset))
    data <- data[subset, ]
  lm(formula, data = data)
}
foo(Sepal.Length ~ Petal.Length, data = iris, subset = iris$Species == "sets")


Now, if you want the same behaviour as for lm(), it gets a little bit more complicated, and you will have to carefully test your code in various conditions!

foo <- function(formula, data, subset) {
  if (!missing(subset)) {
    rows <- eval(substitute(subset), data)
    data <- data[rows, ]
  }
  lm(formula, data = data)
}

foo(Sepal.Length ~ Petal.Length, data = iris, subset = Species == "setosa")


Philippe
 

> 
> 
>> On Dec 2, 2015, at 7:10 AM, phgrosjean at sciviews.org wrote:
>> 
>> Your example and explanation are not complete, but I have the gut feeling that you could do all this both more efficiently *and* more R-ish.
>> 
>> First of all, why would you pass Y and X separately, to ultimately build the Y ~ X formula within the body of your function?
>> 
>> Secondly, it seems to me that subY and subY.val does something very similar to the subset argument in, say, lm().
>> 
>> Personally, I would write it like this:
>> 
>> foo <- function(formula, data, subset) {
>> if (!missing(subset))
>>   data <- data[subset, ]
>> fit <- some_regression_tool(formula, data = data)
>> 
>> ## <more code>
>> 
>> data_after_processing
>> }
>> 
>> with subset = subY == subY.val.
>> 
>> Best,
>> 
>> Philippe
>> 
>>> On 02 Dec 2015, at 06:11, Brant Inman <brant.inman at me.com> wrote:
>>> 
>>> I am trying to build a function that can accept variables for a regression.  It would work something like this:
>>> 
>>> ---
>>> # Y = my response variable (e.g. income)
>>> # X = my key predictor variable (e.g. education)
>>> # subY = a subsetting variable for Y (e.g. race)
>>> # subY.val = the value of the subsetting value that I want (e.g. ?black?)
>>> 
>>> foo <- function(Y, X, subY, subY.val, dataset){
>>> 
>>> if(is.na(subY) == F) {
>>>   Y <- paste(Y, ?[?, subY, ?==?, subY.val, ?]?)
>>> }
>>> FORMULA <- paste(Y ~ X)
>>> fit <- some.regression.tool(FORMULA, data=dataset)
>>> 
>>> return(some.data.after.processing)
>>> }
>>> ---
>>> 
>>> If I call this function with, foo(income, education, race, ?black?, my.dataset), I do not get the result that I need because the FORMULA is "income[race==black] ~ education? when what I need is ?income[race==?black?] ~ education?.  How do I get the quotes to stay on ?black??  Or, is there a better way?
>>> 
>>> Help appreciated.
>>> 
>>> --
>>> Brant
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From cimentadaj at gmail.com  Wed Dec  2 17:35:21 2015
From: cimentadaj at gmail.com (Jorge Cimentada)
Date: Wed, 2 Dec 2015 17:35:21 +0100
Subject: [R] =?utf-8?q?Error_in_model=2Eframe=2Edefault=E2=80=A6_variable_?=
	=?utf-8?q?lengths_differ_=28Without_NA=27s=29?=
Message-ID: <CALdB+JHHLxTG+6pi8pBR+Lqcf+frH5igqUe=qh991waW4oORhw@mail.gmail.com>

Hello everyone,

I'm running an ordinal logistic and I keep getting this error:

Error in model.frame.default(formula = eduattain ~ dadedu, data =
workdataset,  :
  variable lengths differ (found for '(weights)')

I looked at several similar questions on the internet and ended up deleting
all the NA's from all the variables in the model prior to running the
model. I checked this before(where the length differed) and after deleting
the NA's(where the length was the same)

A similar question
<http://t.sidekickopen29.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nMJN7t5XX4RYygTW5v0Dpn5wvpr8W4X9Hq256dwC8d2Ntnd02?t=http%3A%2F%2Fstackoverflow.com%2Fquestions%2F29220727%2Ferror-in-model-frame-default-variable-lengths-differ-but-no-nas-in-data&si=4832790679388160&pi=5c2682a6-6cfb-4c8b-98c9-ca8956c3fe4c>
posed
a similar problem(same error, no NA's) but no answer was given due to the
problem not being reproducible.
------------------------------

Here is a a reproduction of the problem: here I recoded empty rows into
NA's thinking that empty rows might have something to do with this:

require(MASS)

spain <- read.csv("http://vs-web-fs-1.oecd.org/piaac/puf-data/CSV/Prgespp1.csv")[,c("B_Q01a","J_Q07b","SPFWT0")]
workdataset <- spain
workdataset$eduattain <- workdataset$B_Q01a
workdataset$dadedu <- workdataset$J_Q07b
# Model
ordinalmodel <- polr(eduattain ~ dadedu, data = workdataset, weights =
"SPFWT0", Hess = TRUE)
## Recoding some empty rows into NA's
workdataset$eduattain[workdataset$B_Q01a == ""] <- NA
workdataset$dadedu[workdataset$J_Q07b == ""] <- NA
workdataset <- workdataset[!is.na(workdataset$dadedu) &
!is.na(workdataset$eduattain),]
length(workdataset$SPFWT0[!is.na(workdataset$eduattain)])
length(workdataset$SPFWT0[!is.na(workdataset$dadedu)])
#Problem persists
ordinalmodel <- polr(eduattain ~ dadedu, data = workdataset, weights =
"SPFWT0", Hess = TRUE)

*Note: This study has over 10 datasets for different countries and
regardless of the dataset I use the problem persists.*
I've tried all sorts of different things to fix this(I thought it was the
way I recoded the dependent and independent variables, I thought it was
because of empty rows). If I exclude the weight variable, the model works
just fine. I tried looking at the length of the weight when the two
different variables are not NA's and the length is the same.

Please, any help is appreciated.

*Jorge C.*

	[[alternative HTML version deleted]]


From jvadams at usgs.gov  Wed Dec  2 18:53:32 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Wed, 2 Dec 2015 11:53:32 -0600
Subject: [R] Looping through multiple sub elements of a list to compare
 to multiple components of a vector
In-Reply-To: <996837991.14023179.1449070790818.JavaMail.yahoo@mail.yahoo.com>
References: <996837991.14023179.1449070790818.JavaMail.yahoo.ref@mail.yahoo.com>
	<996837991.14023179.1449070790818.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAN5YmCHYh8Si5nF5rNpdu2BOyBpjvdDgdMUv1=KOVfGjYVQHbQ@mail.gmail.com>

First, a couple posting tips.  It's helpful to provide some example data
people can work with.  Also, please post in plain text (not html).

If you have a single standard for comparison, you might find an approach
like this helpful.

# example data
mylist <- c("AAEBCC", "AABDCC", "AABBCD")
list.2 <- strsplit(mylist, split=NULL)

# setting a standard for comparison
std.string <- "AABBCC"
standard <- unlist(strsplit(std.string, split=NULL))

sapply(list.2, function(x) x==standard)

This gives you a matrix of logicals with the number of rows the same length
as your original strings (the 99 amino acids) and the number of columns the
same length as the number of strings you're comparing (the 15 sequences).

      [,1]  [,2]  [,3]
[1,]  TRUE  TRUE  TRUE
[2,]  TRUE  TRUE  TRUE
[3,] FALSE  TRUE  TRUE
[4,]  TRUE FALSE  TRUE
[5,]  TRUE  TRUE  TRUE
[6,]  TRUE  TRUE FALSE

Jean

On Wed, Dec 2, 2015 at 9:39 AM, debra ragland via R-help <
r-help at r-project.org> wrote:

> I think I am making this problem harder than it has to be and so I keep
> getting stuck on what might be a trivial problem.
> I have used the seqinr package to load a protein sequence alignment
> containing 15 protein sequences;
>     > library(seqinr)    > x =
> read.alignment("proteins.fasta",format="fasta",forceToLower=FALSE)This
> automatically loads in a list of 4 elements including the sequences and
> other information.
> I store the sequences to a new list;
>    > mylist = x$seqwhich returns a character vector of 15 strings.
> I have found that if I split the long character strings into individual
> characters it is easy to use lapply to loop over this list. So I use
> strsplit;
>     >list.2 = strsplit(mylist, split = NULL)
> >From this list I can determine which proteins have changes at certain
> positions by using;
>     >lapply(list.2, "[", 10) == "L"This returns a logical T/F vector for
> those elements of the list that do/do not the letter L at position 10.
> Because each of the protein sequences contains 99amino acids, I want to
> automate this process so that I do not have to compare/contrast positions 1
> x 1. Most of the changes occur between positions/letters 10-95. I have a
> standard character vector that I wish to use for comparison when looping
> through the list.
> Should I perhaps combine all --  the standard "letter"/aa vector, the list
> of protein sequences -- into one list? Or is it better to leave them
> separate for this comparison? I'm not sure what the output should be as I
> need to use it for another statistical test. Would a list of logical
> vectors be the most sufficient output to return?
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Dec  2 18:54:59 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 2 Dec 2015 09:54:59 -0800
Subject: [R]
	=?utf-8?q?Error_in_model=2Eframe=2Edefault=E2=80=A6_variable_?=
	=?utf-8?q?lengths_differ_=28Without_NA=27s=29?=
In-Reply-To: <CALdB+JHHLxTG+6pi8pBR+Lqcf+frH5igqUe=qh991waW4oORhw@mail.gmail.com>
References: <CALdB+JHHLxTG+6pi8pBR+Lqcf+frH5igqUe=qh991waW4oORhw@mail.gmail.com>
Message-ID: <CAF8bMcZfaVmu+Z28D5Y7tS5ctj_CHOBM+0oE+G=Z5307LP4CrQ@mail.gmail.com>

In your
>  ordinalmodel <- polr(eduattain ~ dadedu, data = workdataset, weights =
> "SPFWT0", Hess = TRUE)
take the quotation marks off of SPFWT0.  Like the subset argument, weights
is a literal expression, evaluated in the context of the data argument, not a
character string naming a column in the data argument.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Dec 2, 2015 at 8:35 AM, Jorge Cimentada <cimentadaj at gmail.com> wrote:
> Hello everyone,
>
> I'm running an ordinal logistic and I keep getting this error:
>
> Error in model.frame.default(formula = eduattain ~ dadedu, data =
> workdataset,  :
>   variable lengths differ (found for '(weights)')
>
> I looked at several similar questions on the internet and ended up deleting
> all the NA's from all the variables in the model prior to running the
> model. I checked this before(where the length differed) and after deleting
> the NA's(where the length was the same)
>
> A similar question
> <http://t.sidekickopen29.com/e1t/c/5/f18dQhb0S7lC8dDMPbW2n0x6l2B9nMJN7t5XX4RYygTW5v0Dpn5wvpr8W4X9Hq256dwC8d2Ntnd02?t=http%3A%2F%2Fstackoverflow.com%2Fquestions%2F29220727%2Ferror-in-model-frame-default-variable-lengths-differ-but-no-nas-in-data&si=4832790679388160&pi=5c2682a6-6cfb-4c8b-98c9-ca8956c3fe4c>
> posed
> a similar problem(same error, no NA's) but no answer was given due to the
> problem not being reproducible.
> ------------------------------
>
> Here is a a reproduction of the problem: here I recoded empty rows into
> NA's thinking that empty rows might have something to do with this:
>
> require(MASS)
>
> spain <- read.csv("http://vs-web-fs-1.oecd.org/piaac/puf-data/CSV/Prgespp1.csv")[,c("B_Q01a","J_Q07b","SPFWT0")]
> workdataset <- spain
> workdataset$eduattain <- workdataset$B_Q01a
> workdataset$dadedu <- workdataset$J_Q07b
> # Model
> ordinalmodel <- polr(eduattain ~ dadedu, data = workdataset, weights =
> "SPFWT0", Hess = TRUE)
> ## Recoding some empty rows into NA's
> workdataset$eduattain[workdataset$B_Q01a == ""] <- NA
> workdataset$dadedu[workdataset$J_Q07b == ""] <- NA
> workdataset <- workdataset[!is.na(workdataset$dadedu) &
> !is.na(workdataset$eduattain),]
> length(workdataset$SPFWT0[!is.na(workdataset$eduattain)])
> length(workdataset$SPFWT0[!is.na(workdataset$dadedu)])
> #Problem persists
> ordinalmodel <- polr(eduattain ~ dadedu, data = workdataset, weights =
> "SPFWT0", Hess = TRUE)
>
> *Note: This study has over 10 datasets for different countries and
> regardless of the dataset I use the problem persists.*
> I've tried all sorts of different things to fix this(I thought it was the
> way I recoded the dependent and independent variables, I thought it was
> because of empty rows). If I exclude the weight variable, the model works
> just fine. I tried looking at the length of the weight when the two
> different variables are not NA's and the length is the same.
>
> Please, any help is appreciated.
>
> *Jorge C.*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ragia11 at hotmail.com  Wed Dec  2 19:09:16 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Wed, 2 Dec 2015 20:09:16 +0200
Subject: [R] extract rows based on column value in a data frame
Message-ID: <DUB125-W865F5C609373C2843F836AB30E0@phx.gbl>

Dear Group,
I have a data frame that such as

v1   v2    v3    v4
1      1      3      6
1      1       5     6
1       1      8     0
1       2      6     1
1       2      4     0
1       3      4      4
1       3      5      4
1       3      6      3
1       3      7      1

2       4      3      7
2       5       5     4
2       5      8     2
2       1      6     1
2       1      4     0
2      1      4      3
2       1      5      2
3       1      6      1
3       6      7      0

3      6       3      6
3      6       5      6
3      6       8      0
3      6       6      1
3      2       4      0
3      2       4      4
3      2       5      4
3      2       6      3
3      2       7      1
3       5       9      5
3       6       9       5   


the result required is each first 3 rows,  from distinct v2 column for each v1 column


v1   v2    v3    v4
1      1      3      6
1       2      6     1
1       3      4      4
2       4      3      7
2       5       5     4
2       1      6     1
3      6       3      6
3      2       4      0
3       5       9      5


thanks in advance 		 	   		  
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec  2 21:26:19 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 2 Dec 2015 12:26:19 -0800
Subject: [R] extract rows based on column value in a data frame
In-Reply-To: <DUB125-W865F5C609373C2843F836AB30E0@phx.gbl>
References: <DUB125-W865F5C609373C2843F836AB30E0@phx.gbl>
Message-ID: <C8715795-BB73-4540-B86C-50109F8E52B2@comcast.net>


> On Dec 2, 2015, at 10:09 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> 
> Dear Group,
> I have a data frame that such as
> 
> v1   v2    v3    v4
> 1      1      3      6
> 1      1       5     6
> 1       1      8     0
> 1       2      6     1
> 1       2      4     0
> 1       3      4      4
> 1       3      5      4
> 1       3      6      3
> 1       3      7      1
> 
> 2       4      3      7
> 2       5       5     4
> 2       5      8     2
> 2       1      6     1
> 2       1      4     0
> 2      1      4      3
> 2       1      5      2
> 3       1      6      1
> 3       6      7      0
> 
> 3      6       3      6
> 3      6       5      6
> 3      6       8      0
> 3      6       6      1
> 3      2       4      0
> 3      2       4      4
> 3      2       5      4
> 3      2       6      3
> 3      2       7      1
> 3       5       9      5
> 3       6       9       5   
> 
> 
> the result required is each first 3 rows,  from distinct v2 column for each v1 column
> 
> 
> v1   v2    v3    v4
> 1      1      3      6
> 1       2      6     1
> 1       3      4      4
> 2       4      3      7
> 2       5       5     4
> 2       1      6     1
> 3      6       3      6
> 3      2       4      0
> 3       5       9      5


Probably something along the lines of 

dfrm[ ave(dfrm$v1, dfrm$v1, FUN=seq_along) %in% 1:3 , ] 


> 
> 
> thanks in advance 		 	   		  
> 	[[alternative HTML version deleted]]

Future postings should be in plain text.

? 

David Winsemius
Alameda, CA, USA


From 538280 at gmail.com  Wed Dec  2 21:28:38 2015
From: 538280 at gmail.com (Greg Snow)
Date: Wed, 2 Dec 2015 13:28:38 -0700
Subject: [R] fill the area outside a polygon
In-Reply-To: <CAJ=0CtC5oyd=dzvqYrTiPEee=xEdVByQx3V0BO9E=hM8dcKahw@mail.gmail.com>
References: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E6177@mb02.ads.tamu.edu>
	<CAJ=0CtC5oyd=dzvqYrTiPEee=xEdVByQx3V0BO9E=hM8dcKahw@mail.gmail.com>
Message-ID: <CAFEqCdzmpy_r3NTok521nZk2RXO0OLjudvvJttg_3Ty0L8Fb9A@mail.gmail.com>

Adrian,

Draw the polygon once without the border and the whole in it, then go
back and draw the border around the outer polygon without any fill.

On Wed, Dec 2, 2015 at 9:31 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> On Wed, Dec 2, 2015 at 5:19 PM, David L Carlson <dcarlson at tamu.edu> wrote:
>>
>> Using only base graphics, one solution would be to embed the inner
> polygon in the outer one and turn off the border:
>>
>> par(mai=c(0, 0, 0, 0))
>> plot(1:100, type="n")
>> polygon(c(0, 100, 100, 0, 0, 20, 80, 80, 20, 20, 0),
>>      c(0, 0, 100, 100, 0, 20, 20, 80, 80, 20, 0), col="lightblue",
> border=NA)
>
>
> I see what you did, interesting.
> I do need the border though, although if this is the only solution, perhaps
> I could live without it.
>
> It's working on a Windows machine (which I presume you are using), but
> under MacOS I only get  the entire big polygon filled, including the hole
> in the middle.
>
> Thanks,
> Adrian
>
> --
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From mdsumner at gmail.com  Wed Dec  2 21:38:44 2015
From: mdsumner at gmail.com (Michael Sumner)
Date: Wed, 02 Dec 2015 20:38:44 +0000
Subject: [R] fill the area outside a polygon
In-Reply-To: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
References: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
Message-ID: <CAAcGz99Kt8PUaXeFJVmmpH9tt96a+-6ukiB3SeW1ojJP8Gx0NA@mail.gmail.com>

On Wed, 2 Dec 2015 at 23:10 Adrian Du?a <dusa.adrian at unibuc.ro> wrote:

> Dear All,
>
> I know how to fill a polygon, using a basic R graphics device:
>
> par(mai=c(0, 0, 0, 0))
> plot(1:100, type="n")
> polygon(c(20, 80, 80, 20), c(20, 20, 80, 80), col="lightblue")
>
>
> But let's say I have an outer polygon like this:
> polygon(c(0,100,100,0), c(0,0,100,100))
>
> Is it possible to fill the area between the two polygons?
>
>
Try polypath with the evenodd rule (vs. the winding rule):


p1 <- list(x = c(20, 80, 80, 20), y= c(20, 20, 80, 80))
p2 <- list(x = c(0,100,100,0), y = c(0,0,100,100))
plot(p2, type="n")
polypath(x = c(p1$x, NA_real_, p2$x), y = c(p1$y, NA_real_, p2$y), col =
"lightblue", rule = "evenodd")

Cheers, Mike.


> I realise there is a quick and dirty solution to fill the outer polygon,
> then draw the inner polygon colored with white, but I would like to
> actually "clip" the outer polygon to the boundaries of the inner one.
>
> Of course, there are various packages which draw maps (i.e. maptools), but
> I need this in base R graphics. In other words, is it possible to define a
> polygon with a hole inside, in base R?
>
> Thank you,
> Adrian
>
> --
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Dec  2 23:10:09 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 2 Dec 2015 14:10:09 -0800
Subject: [R] extract rows based on column value in a data frame
In-Reply-To: <C8715795-BB73-4540-B86C-50109F8E52B2@comcast.net>
References: <DUB125-W865F5C609373C2843F836AB30E0@phx.gbl>
	<C8715795-BB73-4540-B86C-50109F8E52B2@comcast.net>
Message-ID: <CAGxFJbRD8Jht+r9WtReeNJZn1MFokrbLDH2HLjp5xAhnMN=U2A@mail.gmail.com>

... or perhaps using rep() to do the indexing directly instead of matching:

dfrm[ ave(dfrm$v1, dfrm$v1, FUN =
function(x)rep(c(TRUE,FALSE),c(3,length(x)-3))), ]

There's probably another 6 dozen ways to do it, especially if you
access packages like data.table, plyr, etc.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Dec 2, 2015 at 12:26 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 2, 2015, at 10:09 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>>
>> Dear Group,
>> I have a data frame that such as
>>
>> v1   v2    v3    v4
>> 1      1      3      6
>> 1      1       5     6
>> 1       1      8     0
>> 1       2      6     1
>> 1       2      4     0
>> 1       3      4      4
>> 1       3      5      4
>> 1       3      6      3
>> 1       3      7      1
>>
>> 2       4      3      7
>> 2       5       5     4
>> 2       5      8     2
>> 2       1      6     1
>> 2       1      4     0
>> 2      1      4      3
>> 2       1      5      2
>> 3       1      6      1
>> 3       6      7      0
>>
>> 3      6       3      6
>> 3      6       5      6
>> 3      6       8      0
>> 3      6       6      1
>> 3      2       4      0
>> 3      2       4      4
>> 3      2       5      4
>> 3      2       6      3
>> 3      2       7      1
>> 3       5       9      5
>> 3       6       9       5
>>
>>
>> the result required is each first 3 rows,  from distinct v2 column for each v1 column
>>
>>
>> v1   v2    v3    v4
>> 1      1      3      6
>> 1       2      6     1
>> 1       3      4      4
>> 2       4      3      7
>> 2       5       5     4
>> 2       1      6     1
>> 3      6       3      6
>> 3      2       4      0
>> 3       5       9      5
>
>
> Probably something along the lines of
>
> dfrm[ ave(dfrm$v1, dfrm$v1, FUN=seq_along) %in% 1:3 , ]
>
>
>>
>>
>> thanks in advance
>>       [[alternative HTML version deleted]]
>
> Future postings should be in plain text.
>
> ?
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Wed Dec  2 23:12:46 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 2 Dec 2015 14:12:46 -0800
Subject: [R] extract rows based on column value in a data frame
In-Reply-To: <CAGxFJbRD8Jht+r9WtReeNJZn1MFokrbLDH2HLjp5xAhnMN=U2A@mail.gmail.com>
References: <DUB125-W865F5C609373C2843F836AB30E0@phx.gbl>
	<C8715795-BB73-4540-B86C-50109F8E52B2@comcast.net>
	<CAGxFJbRD8Jht+r9WtReeNJZn1MFokrbLDH2HLjp5xAhnMN=U2A@mail.gmail.com>
Message-ID: <CAGxFJbTgtUvsgXZm9p+b_4xx0pLcQX1FxrrxukikHSu0FrrG_g@mail.gmail.com>

...
Perhaps also worth mentioning -- David's solution works even if there
are less than 3 rows per group, whereas mine will fail.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Wed, Dec 2, 2015 at 2:10 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> ... or perhaps using rep() to do the indexing directly instead of matching:
>
> dfrm[ ave(dfrm$v1, dfrm$v1, FUN =
> function(x)rep(c(TRUE,FALSE),c(3,length(x)-3))), ]
>
> There's probably another 6 dozen ways to do it, especially if you
> access packages like data.table, plyr, etc.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Wed, Dec 2, 2015 at 12:26 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>
>>> On Dec 2, 2015, at 10:09 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>>>
>>> Dear Group,
>>> I have a data frame that such as
>>>
>>> v1   v2    v3    v4
>>> 1      1      3      6
>>> 1      1       5     6
>>> 1       1      8     0
>>> 1       2      6     1
>>> 1       2      4     0
>>> 1       3      4      4
>>> 1       3      5      4
>>> 1       3      6      3
>>> 1       3      7      1
>>>
>>> 2       4      3      7
>>> 2       5       5     4
>>> 2       5      8     2
>>> 2       1      6     1
>>> 2       1      4     0
>>> 2      1      4      3
>>> 2       1      5      2
>>> 3       1      6      1
>>> 3       6      7      0
>>>
>>> 3      6       3      6
>>> 3      6       5      6
>>> 3      6       8      0
>>> 3      6       6      1
>>> 3      2       4      0
>>> 3      2       4      4
>>> 3      2       5      4
>>> 3      2       6      3
>>> 3      2       7      1
>>> 3       5       9      5
>>> 3       6       9       5
>>>
>>>
>>> the result required is each first 3 rows,  from distinct v2 column for each v1 column
>>>
>>>
>>> v1   v2    v3    v4
>>> 1      1      3      6
>>> 1       2      6     1
>>> 1       3      4      4
>>> 2       4      3      7
>>> 2       5       5     4
>>> 2       1      6     1
>>> 3      6       3      6
>>> 3      2       4      0
>>> 3       5       9      5
>>
>>
>> Probably something along the lines of
>>
>> dfrm[ ave(dfrm$v1, dfrm$v1, FUN=seq_along) %in% 1:3 , ]
>>
>>
>>>
>>>
>>> thanks in advance
>>>       [[alternative HTML version deleted]]
>>
>> Future postings should be in plain text.
>>
>> ?
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From teotjunk at hotmail.com  Thu Dec  3 03:41:36 2015
From: teotjunk at hotmail.com (TJUN KIAT TEO)
Date: Thu, 3 Dec 2015 10:41:36 +0800
Subject: [R] Caret
Message-ID: <SNT152-W8858459842AE1E42DC01F2DF0D0@phx.gbl>

I tried using Caret with no cross validation, training on the whole data set. Here is my code

Fit <- train(Result ~ ., data = FitData,
             method = method,
             #trControl = FitControl,
             trControl=trainControl(method="none"),
             verbose = FALSE)
             #tuneGrid = Grid)



Error in train.default(x, y, weights = w, ...) : 
  Only one model should be specified in tuneGrid with no resampling


Wonder if anyone can help. 

Thanks

Tjun Kiat

 		 	   		  
	[[alternative HTML version deleted]]


From ragia11 at hotmail.com  Thu Dec  3 05:54:20 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Thu, 3 Dec 2015 06:54:20 +0200
Subject: [R] extract rows based on column value in a data frame
In-Reply-To: <CAGxFJbTgtUvsgXZm9p+b_4xx0pLcQX1FxrrxukikHSu0FrrG_g@mail.gmail.com>
References: <DUB125-W865F5C609373C2843F836AB30E0@phx.gbl>,
	<C8715795-BB73-4540-B86C-50109F8E52B2@comcast.net>,
	<CAGxFJbRD8Jht+r9WtReeNJZn1MFokrbLDH2HLjp5xAhnMN=U2A@mail.gmail.com>,
	<CAGxFJbTgtUvsgXZm9p+b_4xx0pLcQX1FxrrxukikHSu0FrrG_g@mail.gmail.com>
Message-ID: <DUB125-W2ACB054C5D14D8A85B2CAB30D0@phx.gbl>

lots of thanks?
Ragia

----------------------------------------
> Date: Wed, 2 Dec 2015 14:12:46 -0800
> Subject: Re: [R] extract rows based on column value in a data frame
> From: bgunter.4567 at gmail.com
> To: dwinsemius at comcast.net
> CC: ragia11 at hotmail.com; r-help at r-project.org
>
> ...
> Perhaps also worth mentioning -- David's solution works even if there
> are less than 3 rows per group, whereas mine will fail.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
> -- Clifford Stoll
>
>
> On Wed, Dec 2, 2015 at 2:10 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>> ... or perhaps using rep() to do the indexing directly instead of matching:
>>
>> dfrm[ ave(dfrm$v1, dfrm$v1, FUN =
>> function(x)rep(c(TRUE,FALSE),c(3,length(x)-3))), ]
>>
>> There's probably another 6 dozen ways to do it, especially if you
>> access packages like data.table, plyr, etc.
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>> -- Clifford Stoll
>>
>>
>> On Wed, Dec 2, 2015 at 12:26 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>
>>>> On Dec 2, 2015, at 10:09 AM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>>>>
>>>> Dear Group,
>>>> I have a data frame that such as
>>>>
>>>> v1 v2 v3 v4
>>>> 1 1 3 6
>>>> 1 1 5 6
>>>> 1 1 8 0
>>>> 1 2 6 1
>>>> 1 2 4 0
>>>> 1 3 4 4
>>>> 1 3 5 4
>>>> 1 3 6 3
>>>> 1 3 7 1
>>>>
>>>> 2 4 3 7
>>>> 2 5 5 4
>>>> 2 5 8 2
>>>> 2 1 6 1
>>>> 2 1 4 0
>>>> 2 1 4 3
>>>> 2 1 5 2
>>>> 3 1 6 1
>>>> 3 6 7 0
>>>>
>>>> 3 6 3 6
>>>> 3 6 5 6
>>>> 3 6 8 0
>>>> 3 6 6 1
>>>> 3 2 4 0
>>>> 3 2 4 4
>>>> 3 2 5 4
>>>> 3 2 6 3
>>>> 3 2 7 1
>>>> 3 5 9 5
>>>> 3 6 9 5
>>>>
>>>>
>>>> the result required is each first 3 rows, from distinct v2 column for each v1 column
>>>>
>>>>
>>>> v1 v2 v3 v4
>>>> 1 1 3 6
>>>> 1 2 6 1
>>>> 1 3 4 4
>>>> 2 4 3 7
>>>> 2 5 5 4
>>>> 2 1 6 1
>>>> 3 6 3 6
>>>> 3 2 4 0
>>>> 3 5 9 5
>>>
>>>
>>> Probably something along the lines of
>>>
>>> dfrm[ ave(dfrm$v1, dfrm$v1, FUN=seq_along) %in% 1:3 , ]
>>>
>>>
>>>>
>>>>
>>>> thanks in advance
>>>> [[alternative HTML version deleted]]
>>>
>>> Future postings should be in plain text.
>>>
>>> ?
>>>
>>> David Winsemius
>>> Alameda, CA, USA
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  

From dusa.adrian at unibuc.ro  Thu Dec  3 10:54:45 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Thu, 3 Dec 2015 11:54:45 +0200
Subject: [R] fill the area outside a polygon
In-Reply-To: <CAFEqCdzmpy_r3NTok521nZk2RXO0OLjudvvJttg_3Ty0L8Fb9A@mail.gmail.com>
References: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E6177@mb02.ads.tamu.edu>
	<CAJ=0CtC5oyd=dzvqYrTiPEee=xEdVByQx3V0BO9E=hM8dcKahw@mail.gmail.com>
	<CAFEqCdzmpy_r3NTok521nZk2RXO0OLjudvvJttg_3Ty0L8Fb9A@mail.gmail.com>
Message-ID: <CAJ=0CtA-wsnFp4WHWguG9rZ_eLqhE8jojg4VvzQr59Dhz1WUGQ@mail.gmail.com>

Hi Greg,

On Wed, Dec 2, 2015 at 10:28 PM, Greg Snow <538280 at gmail.com> wrote:
>
> Adrian,
>
> Draw the polygon once without the border and the whole in it, then go
> back and draw the border around the outer polygon without any fill.

I thought about it too, but this only works on a Windows machine.
On a MacOS, the polygon() function doesn't help very much, apparently.

Best wishes,
Adrian

--
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Thu Dec  3 10:56:17 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Thu, 3 Dec 2015 11:56:17 +0200
Subject: [R] fill the area outside a polygon
In-Reply-To: <CAAcGz99Kt8PUaXeFJVmmpH9tt96a+-6ukiB3SeW1ojJP8Gx0NA@mail.gmail.com>
References: <CAJ=0CtAa567e1ES2ftaXS7pQ3qk3_U-xWEXBrTUAEHzG1hsOsg@mail.gmail.com>
	<CAAcGz99Kt8PUaXeFJVmmpH9tt96a+-6ukiB3SeW1ojJP8Gx0NA@mail.gmail.com>
Message-ID: <CAJ=0CtDURJnU5Qhkg=Kp_25uZO_xe1rPBuQjfX-qezu2uicqnA@mail.gmail.com>

On Wed, Dec 2, 2015 at 10:38 PM, Michael Sumner <mdsumner at gmail.com> wrote:

>
> On Wed, 2 Dec 2015 at 23:10 Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
>
>> Dear All,
>>
>> I know how to fill a polygon, using a basic R graphics device:
>>
>> par(mai=c(0, 0, 0, 0))
>> plot(1:100, type="n")
>> polygon(c(20, 80, 80, 20), c(20, 20, 80, 80), col="lightblue")
>>
>>
>> But let's say I have an outer polygon like this:
>> polygon(c(0,100,100,0), c(0,0,100,100))
>>
>> Is it possible to fill the area between the two polygons?
>>
>>
> Try polypath with the evenodd rule (vs. the winding rule):
>

Oh, that is sweet!
I didn't even know about the polypath() function. Time to study now...

Thanks a lot!
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From Ramgad82 at gmx.net  Thu Dec  3 11:51:56 2015
From: Ramgad82 at gmx.net (Dagmar)
Date: Thu, 3 Dec 2015 11:51:56 +0100
Subject: [R] expand dataframe but time gap is not the same
Message-ID: <56601ECC.9040001@gmx.net>

Hello,
I hope someone can help me with my problem:
I have a dataframe like this:

datframe <- data.frame(dates=c("02.08.2013","03.08.2013","03.08.2013"), 
times =c("22:10","4:04", "10:18"), tide =c("NW","HW", "NW"))
datframe
Timestamp <- paste(datframe$dates, datframe$times)
myframe2 <- cbind( Timestamp,datframe)
head(myframe2)
myframe2$dates <- NULL
myframe2$times <- NULL
myframe2$Timestamp <- as.POSIXct (strptime(myframe2$Timestamp, "%d.%m.%Y 
%H:%M"), tz="GMT")
head(myframe2)
str(myframe2)

# In the end I want a frame like this:
datframeres <- data.frame(Timestamp=c("2013-08-02 22:10:00", "2013-08-02 
23:09:00","2013-08-03 00:08:00","2013-08-03 01:07:00", "2013-08-03 
02:06:00", "2013-08-03 03:05:00",
"2013-08-03 04:04:00","2013-08-03 05:06:20","2013-08-03 06:08:40", 
"2013-08-03 07:11:00", "2013-08-03 08:13:20", "2013-08-03 09:15:40", 
"2013-08-03 10:18:00")
, tidalclass =c("NW", "HW-5","HW-4", "HW-3", "HW-2", "HW-1", "HW", 
"HW+1","HW+2","HW+3", "HW+4","HW+5", "NW"))

datframeres

# That means: I want to expand to 13 classes instead of the two classes 
"HW" and "NW": "HW", "HW+1", "HW+2" and so on.
# The time gap between HW and NW is not always quite the same (always 
around 6 hours). So I would divide the time gap by 6 and add this number 
(0:59 and 1:02:20 respectively) to the timestamp before.
# I do not know how to do this. Does anyone know how to do this? Many 
thanks in advance!
Tagmarie


From nickdoban at gmail.com  Thu Dec  3 12:59:52 2015
From: nickdoban at gmail.com (Nicolae Doban)
Date: Thu, 3 Dec 2015 11:59:52 +0000
Subject: [R] Strange error
Message-ID: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>

Hi,

recently I received a strange error after running my code in chunks. But, I
don't get any errors when I run it line by line.

Also, what is strange is that the  error message is misspelled

This is the error messages I get

*> setwd("H:/XX/XXX")*
*"rror: unexpected input in "setwd("H:/XX/XXX")*
*> *
*"rror: unexpected input in "*
*> GWS <- read.csv("X.csv", header = TRUE, sep = ",", stringsAsFactors =
FALSE, na.strings=c("","","NA"))*
*"rror: unexpected input in "GWS <- read.csv("X.csv", header = TRUE, sep =
",", stringsAsFactors = FALSE, na.strings=c("","","NA"))*
*> GWS <- GWS[with(GWS, order(d)), ]; row.names(GWS) <- NULL*
*Error: object 'GWS' not found*

As you can see even changing the directory results in an error message but,
it changes the directory to the correct folder.

What is interesting is that I was not getting this error yesterday (Dec.,
2nd 2015)

Could you please shed some light on this issue? Is it something related to
R or Rstudio or the packages?

Thank you in advance very much,
nick

	[[alternative HTML version deleted]]


From wrightkevin3000 at gmail.com  Thu Dec  3 12:42:26 2015
From: wrightkevin3000 at gmail.com (Kevin Wright)
Date: Thu, 3 Dec 2015 11:42:26 +0000
Subject: [R] Problems trying to generate a prime factor vector
Message-ID: <CABn9+JWUmQ-8ou8y6owciSofJPUo_qbGdLwB7h4USn40xoZBnQ@mail.gmail.com>

Hi,

I am very new to 'R' and am trying to write an R function which returns the
prime factors of a given number(n)

Unfortunately, the function only works for very small numbers, if for
example I pass 18 to the function
a mysteriously long vector is returned. I have not been able to find where
or why this is happening.
I know I've done something wrong. I've tried using debugging statements.
Sometimes the
currentPrime variable seems to become some sort of array?!


can you help?


library(gmp)


#passing 18 returns: 2 1 0 0 0 1 0 0 0 1 0 0 0 3 0 0 0 1 0 0 0 1 0 0 0 1 0
0 0 3 0 0 0
# expected: 2 3 3
getPrimeFactors = function(n){
   primeList <- c()

   if( isprime(n) ){
      primeList <- append(primeList,n)
      return (primeList)
   }

   currentPrime <- 2

   while(TRUE){
      # Check if input is divisible by the current prime
      if(n %% currentPrime == 0){
         cat(sprintf("the number %f is divisible by %f\n", n,
currentPrime))
         n = n%/%currentPrime

         cat(sprintf("value of n is %f\n", n))

         cat(sprintf("current prime :%f\n", currentPrime))
         primeList = append(primeList,currentPrime)
        # print(c("list contents:", primeList))


         currentPrime = 2

         if( isprime(n)){

              primeList = append(primeList, n)
              return (primeList)

         }

      }
      else{
         cat(sprintf("the number %f is NOT divisible by %f\n", n,
currentPrime))
         #cat(sprintf("current prime before is: %f\n", currentPrime))
         #print(c("current prime before:", currentPrime))
         currentPrime = nextprime(currentPrime)
         #cat(sprintf("current prime after is: %f\n", currentPrime))
         #print(c("current prime after:", currentPrime))

      }



   }




}






}

	[[alternative HTML version deleted]]


From dario.romare at accenture.com  Thu Dec  3 11:03:32 2015
From: dario.romare at accenture.com (dario.romare at accenture.com)
Date: Thu, 3 Dec 2015 10:03:32 +0000
Subject: [R] Bug in truncdist package
Message-ID: <7c8f397e4cfa437fb91d1983dec8352e@CO2PR4203MB0296.048d.mgd.msft.net>


library(truncdist)

When running the following code, the output from extrunc() and vartrunc() is correct:

# Mean and variance of Gamma distribution
m <- 40; v <- 9

# Derive shape and scale parameters
a <- m^2/v; s <- v/m
# Checks: mean=shape*scale, variance=mean*scale^2
a*s; a*s^2
# Generate 1M random samples from Gamma distribution
x <- rgamma(10^6,shape=a,rate=1/s)
# Checks
mean(x); var(x)
# Mean and variance of corresponding theoretical Gamma
extrunc("gamma",a=0,b=Inf,shape=a,rate=1/s)
vartrunc("gamma",a=0,b=Inf,shape=a,rate=1/s)

However, changing the mean from 40 to 44:
m <- 44; v <- 9

and running the code again, the output is as follows:

extrunc("gamma",a=0,b=Inf,shape=a,rate=1/s)  # = 3.8101e-06
vartrunc("gamma",a=0,b=Inf,shape=a,rate=1/s) # = 1945


________________________________

This message is for the designated recipient only and ma...{{dropped:16}}


From bretschr at xs4all.nl  Thu Dec  3 14:29:46 2015
From: bretschr at xs4all.nl (Franklin Bretschneider)
Date: Thu, 3 Dec 2015 14:29:46 +0100
Subject: [R] Problems trying to generate a prime factor vector
In-Reply-To: <CABn9+JWUmQ-8ou8y6owciSofJPUo_qbGdLwB7h4USn40xoZBnQ@mail.gmail.com>
References: <CABn9+JWUmQ-8ou8y6owciSofJPUo_qbGdLwB7h4USn40xoZBnQ@mail.gmail.com>
Message-ID: <DA9D44DE-1643-44E2-9D3C-E6C03DB66CDC@xs4all.nl>

Hi Kevin Wright,



Re:
> 
> I am very new to 'R' and am trying to write an R function which returns the
> prime factors of a given number(n)
> 
> Unfortunately, the function only works for very small numbers, if for
> example I pass 18 to the function
> a mysteriously long vector is returned. I have not been able to find where
> or why this is happening.
> I know I've done something wrong. I've tried using debugging statements.
> Sometimes the
> currentPrime variable seems to become some sort of array?!
> 
> 
> can you help?
> 
> 
> library(gmp)
> 


But why writing one? Such a function is already in gmp: factorize(n), and
it works with fairly large numbers:

>library(gmp)
>factorize(12345678987654321)
Big Integer ('bigz') object of length 13:
 [1] 2   2   2   2   5   7   11  73  101 109 109 137 167

Success,

Frank
------




Franklin Bretschneider
Dept of Biology
Utrecht University
bretschr at xs4all.nl


From petr.pikal at precheza.cz  Thu Dec  3 14:51:00 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 3 Dec 2015 13:51:00 +0000
Subject: [R] Strange error
In-Reply-To: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
References: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5003350@SRVEXCHMBX.precheza.cz>

Hi

You are probably not telling a whole story and what is worse, you are telling the story in HTML, which is hard to read.

I get these sorts of errors

> setwd("U:/data")
Error in setwd("U:/data") : cannot change working directory
>

> setwd(D:/xxx/xxx)
Error: unexpected '/' in "setwd(D:/"

> setwd(D:\xxx\xxx)
Error: unexpected input in "setwd(D:\"
>

So you probably try enter directory name without quotes and with backslashes.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Nicolae
> Doban
> Sent: Thursday, December 03, 2015 1:00 PM
> To: r-help at r-project.org
> Subject: [R] Strange error
>
> Hi,
>
> recently I received a strange error after running my code in chunks.
> But, I
> don't get any errors when I run it line by line.
>
> Also, what is strange is that the  error message is misspelled
>
> This is the error messages I get
>
> *> setwd("H:/XX/XXX")*
> *"rror: unexpected input in "setwd("H:/XX/XXX")*
> *> *
> *"rror: unexpected input in "*
> *> GWS <- read.csv("X.csv", header = TRUE, sep = ",", stringsAsFactors
> =
> FALSE, na.strings=c("","","NA"))*
> *"rror: unexpected input in "GWS <- read.csv("X.csv", header = TRUE,
> sep =
> ",", stringsAsFactors = FALSE, na.strings=c("","","NA"))*
> *> GWS <- GWS[with(GWS, order(d)), ]; row.names(GWS) <- NULL*
> *Error: object 'GWS' not found*
>
> As you can see even changing the directory results in an error message
> but,
> it changes the directory to the correct folder.
>
> What is interesting is that I was not getting this error yesterday
> (Dec.,
> 2nd 2015)
>
> Could you please shed some light on this issue? Is it something related
> to
> R or Rstudio or the packages?
>
> Thank you in advance very much,
> nick
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From jdnewmil at dcn.davis.ca.us  Thu Dec  3 14:51:47 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Thu, 03 Dec 2015 05:51:47 -0800
Subject: [R] Strange error
In-Reply-To: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
References: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
Message-ID: <5C4C15A8-E82F-4AA9-AE72-6D19A407E34B@dcn.davis.ca.us>

You are posting in html format on a plain text mailing list,  so we are not seeing what you sent. If you are being similarly careless with your data files then you may have corrupted the data.  Do not use a word processor with R code or data files. 
-- 
Sent from my phone. Please excuse my brevity.

On December 3, 2015 3:59:52 AM PST, Nicolae Doban <nickdoban at gmail.com> wrote:
>Hi,
>
>recently I received a strange error after running my code in chunks.
>But, I
>don't get any errors when I run it line by line.
>
>Also, what is strange is that the  error message is misspelled
>
>This is the error messages I get
>
>*> setwd("H:/XX/XXX")*
>*"rror: unexpected input in "setwd("H:/XX/XXX")*
>*> *
>*"rror: unexpected input in "*
>*> GWS <- read.csv("X.csv", header = TRUE, sep = ",", stringsAsFactors
>=
>FALSE, na.strings=c("","","NA"))*
>*"rror: unexpected input in "GWS <- read.csv("X.csv", header = TRUE,
>sep =
>",", stringsAsFactors = FALSE, na.strings=c("","","NA"))*
>*> GWS <- GWS[with(GWS, order(d)), ]; row.names(GWS) <- NULL*
>*Error: object 'GWS' not found*
>
>As you can see even changing the directory results in an error message
>but,
>it changes the directory to the correct folder.
>
>What is interesting is that I was not getting this error yesterday
>(Dec.,
>2nd 2015)
>
>Could you please shed some light on this issue? Is it something related
>to
>R or Rstudio or the packages?
>
>Thank you in advance very much,
>nick
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Dec  3 15:30:21 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 3 Dec 2015 14:30:21 +0000
Subject: [R] expand dataframe but time gap is not the same
In-Reply-To: <56601ECC.9040001@gmx.net>
References: <56601ECC.9040001@gmx.net>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5003383@SRVEXCHMBX.precheza.cz>

Hi

I am not completely sure what you want.

Maybe something like

datframe$datum<-strptime(paste(datframe[,1], datframe[,2]), format="%d.%m.%Y %H:%M")
datframe$datum <- strptime(format(datframe$datum, "%d.%m.%Y %H"), format="%d.%m.%Y %H")
datframe$datum<-as.POSIXct(datframe$datum)

dat<-seq(min(datframe$datum), by="hour", length.out=50)
temp<-data.frame(dat=dat)
myframe <- merge(datframe, temp, by.x="datum", by.y="dat", all=TRUE)

can help you a bit.

You can fill missing tides by e.g. na.locf from zoo package.

I know that it is not exactly what you wanted, however it seems to me that having data frame with equally spaced time can be better approach than dividing unequal intervals to 6 portions.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dagmar
> Sent: Thursday, December 03, 2015 11:52 AM
> To: R help
> Subject: [R] expand dataframe but time gap is not the same
>
> Hello,
> I hope someone can help me with my problem:
> I have a dataframe like this:
>
> datframe <- data.frame(dates=c("02.08.2013","03.08.2013","03.08.2013"),
> times =c("22:10","4:04", "10:18"), tide =c("NW","HW", "NW"))
> datframe
> Timestamp <- paste(datframe$dates, datframe$times)
> myframe2 <- cbind( Timestamp,datframe)
> head(myframe2)
> myframe2$dates <- NULL
> myframe2$times <- NULL
> myframe2$Timestamp <- as.POSIXct (strptime(myframe2$Timestamp,
> "%d.%m.%Y
> %H:%M"), tz="GMT")
> head(myframe2)
> str(myframe2)
>
> # In the end I want a frame like this:
> datframeres <- data.frame(Timestamp=c("2013-08-02 22:10:00", "2013-08-
> 02
> 23:09:00","2013-08-03 00:08:00","2013-08-03 01:07:00", "2013-08-03
> 02:06:00", "2013-08-03 03:05:00",
> "2013-08-03 04:04:00","2013-08-03 05:06:20","2013-08-03 06:08:40",
> "2013-08-03 07:11:00", "2013-08-03 08:13:20", "2013-08-03 09:15:40",
> "2013-08-03 10:18:00")
> , tidalclass =c("NW", "HW-5","HW-4", "HW-3", "HW-2", "HW-1", "HW",
> "HW+1","HW+2","HW+3", "HW+4","HW+5", "NW"))
>
> datframeres
>
> # That means: I want to expand to 13 classes instead of the two classes
> "HW" and "NW": "HW", "HW+1", "HW+2" and so on.
> # The time gap between HW and NW is not always quite the same (always
> around 6 hours). So I would divide the time gap by 6 and add this
> number
> (0:59 and 1:02:20 respectively) to the timestamp before.
> # I do not know how to do this. Does anyone know how to do this? Many
> thanks in advance!
> Tagmarie
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From ragland.debra at yahoo.com  Thu Dec  3 16:28:58 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Thu, 3 Dec 2015 15:28:58 +0000 (UTC)
Subject: [R] looping through rows of a logical matrix for combination with
 and testing of a numeric vector
References: <652307040.14653972.1449156538575.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <652307040.14653972.1449156538575.JavaMail.yahoo@mail.yahoo.com>

I have read in a sequence alignment and have done the necessary steps to separate and store the elements of the original input into a new list of character vectors. I have compared the sequence list to a "standard" vector, such that the return is a matrix of logical values indicating TRUE if there is a match to the standard and FALSE where there is no match.

An example;

mylist=c("AAEBCC", "AABDCC", "AABBCD")
list.2 <- strsplit(mylist, split=NULL)
# setting a standard for comparison
std.string <- "AABBCC"
standard <- unlist(strsplit(std.string, split=NULL))
#create a logical matrix 
mat<-sapply(list.2, function(x) x==standard)
>mat


[,1]  [,2]  [,3]
[1,]  TRUE  TRUE  TRUE
[2,]  TRUE  TRUE  TRUE
[3,] FALSE  TRUE  TRUE
[4,]  TRUE FALSE  TRUE
[5,]  TRUE  TRUE  TRUE
[6,]  TRUE  TRUE FALSE

Where the number of columns is the same length as the original input strings I compared (15) and the number of rows corresponds is the same as the number of strings from the input (99).

I also have a named numeric vector(of length 15)--where the "names" of the the values match those of the columns of the logical matrix. For the example

x2 = runif(3, 5.0, 7.5)
names(x2) = 1:3
> x2 
1        2        3 
5.352611 7.058169 6.993105

For each row in the in the logical matrix I want to combine the logical values with the values from the numeric vector so that I can run a wilcox.test using those values that are "TRUE" against those that are "FALSE".

For instance if each row&vector pairing was a mini data.frame it would look like

df=data.frame(x2, mat[3,])
>df
1 5.352611 FALSE
2 7.058169 TRUE
3 6.993105 TRUE
wilcox.test(df) #based on all true values vs. all false values

How can this be achieved?


From jlorenz at uni-goettingen.de  Thu Dec  3 16:03:19 2015
From: jlorenz at uni-goettingen.de (Lorenz, Jennifer)
Date: Thu, 3 Dec 2015 15:03:19 +0000
Subject: [R] sem-package for categorical and multiply imputed data
Message-ID: <51661F79C9BE0341B082F375A44402FA573732A0@UM-EXCDAG-A03.um.gwdg.de>

Hi,

can anyone recommend a package for structural euqation models with binary outcomes that is able to pool for multiply imputed datasets and calculate cluster-robust standard errors? I tried lavaan.survey from the package lavaan.survey, but unfortunately it does not support categorical data with multiply imputed datasets (yet). I also tried run.MI / sem.mi from the package semTools, but there doesn't seem to be an option for cluster-robust standard errors...
I would really appreciate any suggestions!

Best
Jen


	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Thu Dec  3 16:48:55 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 3 Dec 2015 10:48:55 -0500
Subject: [R] Problems trying to generate a prime factor vector
In-Reply-To: <CABn9+JWUmQ-8ou8y6owciSofJPUo_qbGdLwB7h4USn40xoZBnQ@mail.gmail.com>
References: <CABn9+JWUmQ-8ou8y6owciSofJPUo_qbGdLwB7h4USn40xoZBnQ@mail.gmail.com>
Message-ID: <1BBF2C3F-F93F-4EF0-9133-248FD17C0FBB@utoronto.ca>

It's very commendable that you try writing your own functions to learn R. Of course the function factorize() is in gmp, but that's beside the point.

Your code looks messy - before asking your question you could have removed all these print statements. It gets further messed up because you posted in HTML. The logic is awkward. Instead of returning from the inside of an infinite loop in spaghetti code fashion, write your loop to test for your termination condition. Use "<-" and "=" consistently! Don't write return (), but keep parentheses attached to their functions. "if( isprime(n) ){ return(n)}" is enough for your first test. DRY (Do not repeat yourself). Whenever you find yourself writing essentially the same code in different places of your function, your logic is almost always wrong (you have three different places where you append to your prime list).

But, funny enough, that's not actually the source of your problem. The problem is: you are using the gmp function nextprime(), which - although it doesn't say so in it's help page - returns a big integer of class bigz.

  str(nextprime(2))

The problem is you can't mix integers and big integers in the same list.

  c(nextprime(2), nextprime(3))
  c(27, nextprime(2), nextprime(3))

So keep it all as plain integers:
  currentPrime = as.integer(nextprime(currentPrime))

... and it will work as expected.
But do clean it up.


B.

PS:
factorize(18)
Big Integer ('bigz') object of length 3:
[1] 2 3 3

... 


On Dec 3, 2015, at 6:42 AM, Kevin Wright <wrightkevin3000 at gmail.com> wrote:

> Hi,
> 
> I am very new to 'R' and am trying to write an R function which returns the
> prime factors of a given number(n)
> 
> Unfortunately, the function only works for very small numbers, if for
> example I pass 18 to the function
> a mysteriously long vector is returned. I have not been able to find where
> or why this is happening.
> I know I've done something wrong. I've tried using debugging statements.
> Sometimes the
> currentPrime variable seems to become some sort of array?!
> 
> 
> can you help?
> 
> 
> library(gmp)
> 
> 
> #passing 18 returns: 2 1 0 0 0 1 0 0 0 1 0 0 0 3 0 0 0 1 0 0 0 1 0 0 0 1 0
> 0 0 3 0 0 0
> # expected: 2 3 3
> getPrimeFactors = function(n){
>   primeList <- c()
> 
>   if( isprime(n) ){
>      primeList <- append(primeList,n)
>      return (primeList)
>   }
> 
>   currentPrime <- 2
> 
>   while(TRUE){
>      # Check if input is divisible by the current prime
>      if(n %% currentPrime == 0){
>         cat(sprintf("the number %f is divisible by %f\n", n,
> currentPrime))
>         n = n%/%currentPrime
> 
>         cat(sprintf("value of n is %f\n", n))
> 
>         cat(sprintf("current prime :%f\n", currentPrime))
>         primeList = append(primeList,currentPrime)
>        # print(c("list contents:", primeList))
> 
> 
>         currentPrime = 2
> 
>         if( isprime(n)){
> 
>              primeList = append(primeList, n)
>              return (primeList)
> 
>         }
> 
>      }
>      else{
>         cat(sprintf("the number %f is NOT divisible by %f\n", n,
> currentPrime))
>         #cat(sprintf("current prime before is: %f\n", currentPrime))
>         #print(c("current prime before:", currentPrime))
>         currentPrime = nextprime(currentPrime)
>         #cat(sprintf("current prime after is: %f\n", currentPrime))
>         #print(c("current prime after:", currentPrime))
> 
>      }
> 
> 
> 
>   }
> 
> 
> 
> 
> }
> 
> 
> 
> 
> 
> 
> }
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Thu Dec  3 16:58:00 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 3 Dec 2015 07:58:00 -0800
Subject: [R] looping through rows of a logical matrix for combination
 with and testing of a numeric vector
In-Reply-To: <CAGxFJbQCATcrup=7BDGrDqaWQcz7ncigVoOY31MP0Gvsy4AD7Q@mail.gmail.com>
References: <652307040.14653972.1449156538575.JavaMail.yahoo.ref@mail.yahoo.com>
	<652307040.14653972.1449156538575.JavaMail.yahoo@mail.yahoo.com>
	<CAGxFJbQCATcrup=7BDGrDqaWQcz7ncigVoOY31MP0Gvsy4AD7Q@mail.gmail.com>
Message-ID: <CAGxFJbRGMsfU2fMg2aHdYR=2dr=H5GCoNsW9+rDd8aPDW32K7g@mail.gmail.com>

I should have added -- is this homework? There is a no homework policy
on this list.

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Thu, Dec 3, 2015 at 7:54 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Have you spent any time with an R tutorial or two? I ask, because you
> do not seem to have much knowledge of the language and its features.
> Have you made any effort to figure this out yourself? -- if so, show
> us your code and where/how it goes wrong. I ask, because you seem to
> be asking this list to do your work for you, which is not its purpose.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Thu, Dec 3, 2015 at 7:28 AM, debra ragland via R-help
> <r-help at r-project.org> wrote:
>> I have read in a sequence alignment and have done the necessary steps to separate and store the elements of the original input into a new list of character vectors. I have compared the sequence list to a "standard" vector, such that the return is a matrix of logical values indicating TRUE if there is a match to the standard and FALSE where there is no match.
>>
>> An example;
>>
>> mylist=c("AAEBCC", "AABDCC", "AABBCD")
>> list.2 <- strsplit(mylist, split=NULL)
>> # setting a standard for comparison
>> std.string <- "AABBCC"
>> standard <- unlist(strsplit(std.string, split=NULL))
>> #create a logical matrix
>> mat<-sapply(list.2, function(x) x==standard)
>>>mat
>>
>>
>> [,1]  [,2]  [,3]
>> [1,]  TRUE  TRUE  TRUE
>> [2,]  TRUE  TRUE  TRUE
>> [3,] FALSE  TRUE  TRUE
>> [4,]  TRUE FALSE  TRUE
>> [5,]  TRUE  TRUE  TRUE
>> [6,]  TRUE  TRUE FALSE
>>
>> Where the number of columns is the same length as the original input strings I compared (15) and the number of rows corresponds is the same as the number of strings from the input (99).
>>
>> I also have a named numeric vector(of length 15)--where the "names" of the the values match those of the columns of the logical matrix. For the example
>>
>> x2 = runif(3, 5.0, 7.5)
>> names(x2) = 1:3
>>> x2
>> 1        2        3
>> 5.352611 7.058169 6.993105
>>
>> For each row in the in the logical matrix I want to combine the logical values with the values from the numeric vector so that I can run a wilcox.test using those values that are "TRUE" against those that are "FALSE".
>>
>> For instance if each row&vector pairing was a mini data.frame it would look like
>>
>> df=data.frame(x2, mat[3,])
>>>df
>> 1 5.352611 FALSE
>> 2 7.058169 TRUE
>> 3 6.993105 TRUE
>> wilcox.test(df) #based on all true values vs. all false values
>>
>> How can this be achieved?
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From boris.steipe at utoronto.ca  Thu Dec  3 17:04:00 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 3 Dec 2015 11:04:00 -0500
Subject: [R] looping through rows of a logical matrix for combination
	with and testing of a numeric vector
In-Reply-To: <652307040.14653972.1449156538575.JavaMail.yahoo@mail.yahoo.com>
References: <652307040.14653972.1449156538575.JavaMail.yahoo.ref@mail.yahoo.com>
	<652307040.14653972.1449156538575.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <FD2CB8AF-BC44-4F72-8F66-52C53423897F@utoronto.ca>

Use your logical vector to extract the x, y values for the test from the rows of the matrix:
  x <- mat[3, x2]
  y <- mat[3, !x2]

Or: use the formula version of wilcox.test as explained in ?wilcox.test


B.


On Dec 3, 2015, at 10:28 AM, debra ragland via R-help <r-help at r-project.org> wrote:

> I have read in a sequence alignment and have done the necessary steps to separate and store the elements of the original input into a new list of character vectors. I have compared the sequence list to a "standard" vector, such that the return is a matrix of logical values indicating TRUE if there is a match to the standard and FALSE where there is no match.
> 
> An example;
> 
> mylist=c("AAEBCC", "AABDCC", "AABBCD")
> list.2 <- strsplit(mylist, split=NULL)
> # setting a standard for comparison
> std.string <- "AABBCC"
> standard <- unlist(strsplit(std.string, split=NULL))
> #create a logical matrix 
> mat<-sapply(list.2, function(x) x==standard)
>> mat
> 
> 
> [,1]  [,2]  [,3]
> [1,]  TRUE  TRUE  TRUE
> [2,]  TRUE  TRUE  TRUE
> [3,] FALSE  TRUE  TRUE
> [4,]  TRUE FALSE  TRUE
> [5,]  TRUE  TRUE  TRUE
> [6,]  TRUE  TRUE FALSE
> 
> Where the number of columns is the same length as the original input strings I compared (15) and the number of rows corresponds is the same as the number of strings from the input (99).
> 
> I also have a named numeric vector(of length 15)--where the "names" of the the values match those of the columns of the logical matrix. For the example
> 
> x2 = runif(3, 5.0, 7.5)
> names(x2) = 1:3
>> x2 
> 1        2        3 
> 5.352611 7.058169 6.993105
> 
> For each row in the in the logical matrix I want to combine the logical values with the values from the numeric vector so that I can run a wilcox.test using those values that are "TRUE" against those that are "FALSE".
> 
> For instance if each row&vector pairing was a mini data.frame it would look like
> 
> df=data.frame(x2, mat[3,])
>> df
> 1 5.352611 FALSE
> 2 7.058169 TRUE
> 3 6.993105 TRUE
> wilcox.test(df) #based on all true values vs. all false values
> 
> How can this be achieved?
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From boris.steipe at utoronto.ca  Thu Dec  3 17:06:00 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 3 Dec 2015 11:06:00 -0500
Subject: [R] Problems trying to generate a prime factor vector
In-Reply-To: <1BBF2C3F-F93F-4EF0-9133-248FD17C0FBB@utoronto.ca>
References: <CABn9+JWUmQ-8ou8y6owciSofJPUo_qbGdLwB7h4USn40xoZBnQ@mail.gmail.com>
	<1BBF2C3F-F93F-4EF0-9133-248FD17C0FBB@utoronto.ca>
Message-ID: <05BD1584-4796-46DA-801F-7E7679CF0DD0@utoronto.ca>

* ... in the same vector.

On Dec 3, 2015, at 10:48 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote:

> It's very commendable that you try writing your own functions to learn R. Of course the function factorize() is in gmp, but that's beside the point.
> 
> Your code looks messy - before asking your question you could have removed all these print statements. It gets further messed up because you posted in HTML. The logic is awkward. Instead of returning from the inside of an infinite loop in spaghetti code fashion, write your loop to test for your termination condition. Use "<-" and "=" consistently! Don't write return (), but keep parentheses attached to their functions. "if( isprime(n) ){ return(n)}" is enough for your first test. DRY (Do not repeat yourself). Whenever you find yourself writing essentially the same code in different places of your function, your logic is almost always wrong (you have three different places where you append to your prime list).
> 
> But, funny enough, that's not actually the source of your problem. The problem is: you are using the gmp function nextprime(), which - although it doesn't say so in it's help page - returns a big integer of class bigz.
> 
>  str(nextprime(2))
> 
> The problem is you can't mix integers and big integers in the same list.
> 
>  c(nextprime(2), nextprime(3))
>  c(27, nextprime(2), nextprime(3))
> 
> So keep it all as plain integers:
>  currentPrime = as.integer(nextprime(currentPrime))
> 
> ... and it will work as expected.
> But do clean it up.
> 
> 
> B.
> 
> PS:
> factorize(18)
> Big Integer ('bigz') object of length 3:
> [1] 2 3 3
> 
> ... 
> 
> 
> On Dec 3, 2015, at 6:42 AM, Kevin Wright <wrightkevin3000 at gmail.com> wrote:
> 
>> Hi,
>> 
>> I am very new to 'R' and am trying to write an R function which returns the
>> prime factors of a given number(n)
>> 
>> Unfortunately, the function only works for very small numbers, if for
>> example I pass 18 to the function
>> a mysteriously long vector is returned. I have not been able to find where
>> or why this is happening.
>> I know I've done something wrong. I've tried using debugging statements.
>> Sometimes the
>> currentPrime variable seems to become some sort of array?!
>> 
>> 
>> can you help?
>> 
>> 
>> library(gmp)
>> 
>> 
>> #passing 18 returns: 2 1 0 0 0 1 0 0 0 1 0 0 0 3 0 0 0 1 0 0 0 1 0 0 0 1 0
>> 0 0 3 0 0 0
>> # expected: 2 3 3
>> getPrimeFactors = function(n){
>>  primeList <- c()
>> 
>>  if( isprime(n) ){
>>     primeList <- append(primeList,n)
>>     return (primeList)
>>  }
>> 
>>  currentPrime <- 2
>> 
>>  while(TRUE){
>>     # Check if input is divisible by the current prime
>>     if(n %% currentPrime == 0){
>>        cat(sprintf("the number %f is divisible by %f\n", n,
>> currentPrime))
>>        n = n%/%currentPrime
>> 
>>        cat(sprintf("value of n is %f\n", n))
>> 
>>        cat(sprintf("current prime :%f\n", currentPrime))
>>        primeList = append(primeList,currentPrime)
>>       # print(c("list contents:", primeList))
>> 
>> 
>>        currentPrime = 2
>> 
>>        if( isprime(n)){
>> 
>>             primeList = append(primeList, n)
>>             return (primeList)
>> 
>>        }
>> 
>>     }
>>     else{
>>        cat(sprintf("the number %f is NOT divisible by %f\n", n,
>> currentPrime))
>>        #cat(sprintf("current prime before is: %f\n", currentPrime))
>>        #print(c("current prime before:", currentPrime))
>>        currentPrime = nextprime(currentPrime)
>>        #cat(sprintf("current prime after is: %f\n", currentPrime))
>>        #print(c("current prime after:", currentPrime))
>> 
>>     }
>> 
>> 
>> 
>>  }
>> 
>> 
>> 
>> 
>> }
>> 
>> 
>> 
>> 
>> 
>> 
>> }
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ragland.debra at yahoo.com  Thu Dec  3 17:43:52 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Thu, 3 Dec 2015 16:43:52 +0000 (UTC)
Subject: [R] looping through rows of a logical matrix for combination
 with and testing of a numeric vector
In-Reply-To: <FD2CB8AF-BC44-4F72-8F66-52C53423897F@utoronto.ca>
References: <FD2CB8AF-BC44-4F72-8F66-52C53423897F@utoronto.ca>
Message-ID: <1821531643.14820404.1449161032098.JavaMail.yahoo@mail.yahoo.com>

Thanks again!

And no Bert, this is not homework. I have a very minimal background in R and struggle with putting concepts together. 

But thanks anyway. 



On Thursday, December 3, 2015 11:04 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote:
Use your logical vector to extract the x, y values for the test from the rows of the matrix:
  x <- mat[3, x2]
  y <- mat[3, !x2]

Or: use the formula version of wilcox.test as explained in ?wilcox.test


B.



On Dec 3, 2015, at 10:28 AM, debra ragland via R-help <r-help at r-project.org> wrote:

> I have read in a sequence alignment and have done the necessary steps to separate and store the elements of the original input into a new list of character vectors. I have compared the sequence list to a "standard" vector, such that the return is a matrix of logical values indicating TRUE if there is a match to the standard and FALSE where there is no match.
> 
> An example;
> 
> mylist=c("AAEBCC", "AABDCC", "AABBCD")
> list.2 <- strsplit(mylist, split=NULL)
> # setting a standard for comparison
> std.string <- "AABBCC"
> standard <- unlist(strsplit(std.string, split=NULL))
> #create a logical matrix 
> mat<-sapply(list.2, function(x) x==standard)
>> mat
> 
> 
> [,1]  [,2]  [,3]
> [1,]  TRUE  TRUE  TRUE
> [2,]  TRUE  TRUE  TRUE
> [3,] FALSE  TRUE  TRUE
> [4,]  TRUE FALSE  TRUE
> [5,]  TRUE  TRUE  TRUE
> [6,]  TRUE  TRUE FALSE
> 
> Where the number of columns is the same length as the original input strings I compared (15) and the number of rows corresponds is the same as the number of strings from the input (99).
> 
> I also have a named numeric vector(of length 15)--where the "names" of the the values match those of the columns of the logical matrix. For the example
> 
> x2 = runif(3, 5.0, 7.5)
> names(x2) = 1:3
>> x2 
> 1        2        3 
> 5.352611 7.058169 6.993105
> 
> For each row in the in the logical matrix I want to combine the logical values with the values from the numeric vector so that I can run a wilcox.test using those values that are "TRUE" against those that are "FALSE".
> 
> For instance if each row&vector pairing was a mini data.frame it would look like
> 
> df=data.frame(x2, mat[3,])
>> df
> 1 5.352611 FALSE
> 2 7.058169 TRUE
> 3 6.993105 TRUE
> wilcox.test(df) #based on all true values vs. all false values
> 
> How can this be achieved?
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ragland.debra at yahoo.com  Thu Dec  3 17:47:05 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Thu, 3 Dec 2015 16:47:05 +0000 (UTC)
Subject: [R] looping through rows of a logical matrix for combination
 with and testing of a numeric vector
In-Reply-To: <1821531643.14820404.1449161032098.JavaMail.yahoo@mail.yahoo.com>
References: <FD2CB8AF-BC44-4F72-8F66-52C53423897F@utoronto.ca>
	<1821531643.14820404.1449161032098.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <564097237.14865561.1449161225849.JavaMail.yahoo@mail.yahoo.com>

Or sorry, I should clarify, I struggle with putting components together when it comes to looping. 



On Thursday, December 3, 2015 11:43 AM, debra ragland <ragland.debra at yahoo.com> wrote:
Thanks again!

And no Bert, this is not homework. I have a very minimal background in R and struggle with putting concepts together. 

But thanks anyway. 




On Thursday, December 3, 2015 11:04 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote:
Use your logical vector to extract the x, y values for the test from the rows of the matrix:
  x <- mat[3, x2]
  y <- mat[3, !x2]

Or: use the formula version of wilcox.test as explained in ?wilcox.test


B.



On Dec 3, 2015, at 10:28 AM, debra ragland via R-help <r-help at r-project.org> wrote:

> I have read in a sequence alignment and have done the necessary steps to separate and store the elements of the original input into a new list of character vectors. I have compared the sequence list to a "standard" vector, such that the return is a matrix of logical values indicating TRUE if there is a match to the standard and FALSE where there is no match.
> 
> An example;
> 
> mylist=c("AAEBCC", "AABDCC", "AABBCD")
> list.2 <- strsplit(mylist, split=NULL)
> # setting a standard for comparison
> std.string <- "AABBCC"
> standard <- unlist(strsplit(std.string, split=NULL))
> #create a logical matrix 
> mat<-sapply(list.2, function(x) x==standard)
>> mat
> 
> 
> [,1]  [,2]  [,3]
> [1,]  TRUE  TRUE  TRUE
> [2,]  TRUE  TRUE  TRUE
> [3,] FALSE  TRUE  TRUE
> [4,]  TRUE FALSE  TRUE
> [5,]  TRUE  TRUE  TRUE
> [6,]  TRUE  TRUE FALSE
> 
> Where the number of columns is the same length as the original input strings I compared (15) and the number of rows corresponds is the same as the number of strings from the input (99).
> 
> I also have a named numeric vector(of length 15)--where the "names" of the the values match those of the columns of the logical matrix. For the example
> 
> x2 = runif(3, 5.0, 7.5)
> names(x2) = 1:3
>> x2 
> 1        2        3 
> 5.352611 7.058169 6.993105
> 
> For each row in the in the logical matrix I want to combine the logical values with the values from the numeric vector so that I can run a wilcox.test using those values that are "TRUE" against those that are "FALSE".
> 
> For instance if each row&vector pairing was a mini data.frame it would look like
> 
> df=data.frame(x2, mat[3,])
>> df
> 1 5.352611 FALSE
> 2 7.058169 TRUE
> 3 6.993105 TRUE
> wilcox.test(df) #based on all true values vs. all false values
> 
> How can this be achieved?
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec  3 18:21:41 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 3 Dec 2015 09:21:41 -0800
Subject: [R] Bug in truncdist package
In-Reply-To: <7c8f397e4cfa437fb91d1983dec8352e@CO2PR4203MB0296.048d.mgd.msft.net>
References: <7c8f397e4cfa437fb91d1983dec8352e@CO2PR4203MB0296.048d.mgd.msft.net>
Message-ID: <997973E3-07D8-4377-93E5-BAF0DE795AFC@comcast.net>


> On Dec 3, 2015, at 2:03 AM, <dario.romare at accenture.com> <dario.romare at accenture.com> wrote:
> 
> 
> library(truncdist)
> 
> When running the following code, the output from extrunc() and vartrunc() is correct:
> 
snipped

> 
> However, changing the mean from 40 to 44:
> m <- 44; v <- 9
> 
> and running the code again, the output is as follows:
> 
> extrunc("gamma",a=0,b=Inf,shape=a,rate=1/s)  # = 3.8101e-06
> vartrunc("gamma",a=0,b=Inf,shape=a,rate=1/s) # = 1945

You are sending this report to the wrong address. There is no assurance that the author of this package will ever see it. You should instead do this (with the package loaded).

maintainer(?truncdist?)

And send that code to whatever address that might return.


> ________________________________
> 
> This message is for the designated recipient only and ma...{{dropped:16}}


David Winsemius
Alameda, CA, USA


From hamoen.m at gmail.com  Thu Dec  3 16:45:25 2015
From: hamoen.m at gmail.com (Marleen Hamoen)
Date: Thu, 3 Dec 2015 16:45:25 +0100
Subject: [R] Survival analysis: ERROR: Time and status are different lengths
Message-ID: <CANVDHz3P-WiYC=_0uo=yH3-UE8yVbRjuG9wrVfLaW16wpApoew@mail.gmail.com>

Hi,

I am fitting an AFT model assuming a Weibull distribution and I would like
to check the residuals compared to the Kaplan Meier residuals, but when I
try to create the Kaplan Meier residuals I get an error: Time and status
are different lengths.

I am using the following script:

# Fitting the AFT model
fit.weib <- survreg(Surv(TimeDeath, event) ~ age + sex + mutation +
ventilation + BM1 + BM2, data = DF, dist = "weibull")
fits <- fit.weib$linear.predictors
resids <- (fit.weib$y[, 1] - fits)/fit.weib$scale
resKM <- survfit(Surv(resids, event) ~ 1, data = DF)

I get the error from the last line of the script.

I tried some things that didn't work and I was wondering if anyone could
help me.
If you need more information please let me know.

Thanks in advance,

M

	[[alternative HTML version deleted]]


From sabasehrish at yahoo.com  Thu Dec  3 22:51:54 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Thu, 3 Dec 2015 21:51:54 +0000 (UTC)
Subject: [R] R-help mailing list
References: <193234011.12139901.1449179514819.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <193234011.12139901.1449179514819.JavaMail.yahoo@mail.yahoo.com>

Hi


?
I am a PhD student and I want to learn how to run Linear regression with Lag-5 on R through "For Loop". Please find the details below:


?
1- ? ? ? ? ? ?I need guidance about Coding/ Programming for Simple Linear Regression with Lag-5 on R.

2-???????????I have time series data of ?Daily Returns? of 15 stocks and I want to see how each stock?sreturn is connected to all other stocks? returns. This means, I have to runregression as follows:


?
? ? ? ? ? ? ? ? ? ? a) Impact of Stock 1?s return on return of Stock 2. Impact of Stock 1?s return onreturn of Stock 3. Impact of Stock 1?s return on return of Stock 4 ??? tillreturn of Stock 15.? ? ? ? ? ? ? ? ? ? b) Then, Impact of Stock 2?s return on return of Stock 1. Impact of Stock 2?sreturn on return of Stock 3. Impact of Stock 2?s return on return of Stock 4??? till return of Stock 15. And this will continue till Stock 15, one after another.? ? ? ? ? ? ? ? ? ? c) ?As the the process will have to be repeated, therefore instead of manual coding everytime, ?For Loop? is required.?



?
I shall bereally grateful for a detailed reply.


?
Thanks.


?
Regards

Saba Sehrish


	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Dec  3 23:33:05 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 3 Dec 2015 17:33:05 -0500
Subject: [R] R-help mailing list
In-Reply-To: <193234011.12139901.1449179514819.JavaMail.yahoo@mail.yahoo.com>
References: <193234011.12139901.1449179514819.JavaMail.yahoo.ref@mail.yahoo.com>
	<193234011.12139901.1449179514819.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAM_vjumvdgEkaVm77wySi+dxzBnmi9MPHfQeS7GnncKiLbAvZQ@mail.gmail.com>

You need to start with a basic understanding of how R works. Here are
a couple of sources that might help you get started:
http://www.burns-stat.com/documents/tutorials/impatient-r/
http://cyclismo.org/tutorial/R/

Sarah

On Thu, Dec 3, 2015 at 4:51 PM, Saba Sehrish via R-help
<r-help at r-project.org> wrote:
> Hi
>
>
>
> I am a PhD student and I want to learn how to run Linear regression with Lag-5 on R through "For Loop". Please find the details below:
>
>
>
> 1-            I need guidance about Coding/ Programming for Simple Linear Regression with Lag-5 on R.
>
> 2-           I have time series data of ?Daily Returns? of 15 stocks and I want to see how each stock?sreturn is connected to all other stocks? returns. This means, I have to runregression as follows:
>
>
>
>                     a) Impact of Stock 1?s return on return of Stock 2. Impact of Stock 1?s return onreturn of Stock 3. Impact of Stock 1?s return on return of Stock 4 ??? tillreturn of Stock 15.                    b) Then, Impact of Stock 2?s return on return of Stock 1. Impact of Stock 2?sreturn on return of Stock 3. Impact of Stock 2?s return on return of Stock 4??? till return of Stock 15. And this will continue till Stock 15, one after another.                    c)  As the the process will have to be repeated, therefore instead of manual coding everytime, ?For Loop? is required.
>
>
>
>
> I shall bereally grateful for a detailed reply.
>
>
>
> Thanks.
>
>
>
> Regards
>
> Saba Sehrish
>


From mmuurr at gmail.com  Fri Dec  4 02:28:58 2015
From: mmuurr at gmail.com (Murat Tasan)
Date: Thu, 3 Dec 2015 18:28:58 -0700
Subject: [R] system.file(...) self-referencing the containing package
Message-ID: <CA+YV+Hxh+emgx0uyLo1r--BiMYOArHqkGVz90K0hAfvN8Vvasg@mail.gmail.com>

In a package I'm writing, I'm placing all SQL code here:
/inst/sql/

And so when referring to these blocks of code from the package's R
code, I do something like so:

system.file("sql", "my_example_file.sql", package = "ThisPackage",
mustWork = TRUE)

But, referring to the package itself with the string "ThisPackage" is
annoying and somewhat brittle... if the package were ever to change
names (e.g. during development), I'll have to replace all such calls
with the new package name.
And generally referring to the package containing the code itself by
name seems inelegant.

Anyone know of a safe way to reference additional files in a package,
from within that package's code, that doesn't require specifying the
name of the package as a string?

Cheers,

-Murat


From klebyn at yahoo.com.br  Fri Dec  4 02:31:40 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Thu, 3 Dec 2015 23:31:40 -0200
Subject: [R] Tktable do not expand inside a Tkframe
Message-ID: <5660ECFC.1070009@yahoo.com.br>



hello all,

I'm trying to build a table within a frame. I would like the table to
expand when the entire window is expanded.
I could not find examples of how to solve this problem.
I thank you in advance for help.

Cleber
#######################################
# piece of the code

library( tcltk )
tclRequire( "Tktable" )
dataorgW <- tktoplevel()
fmMenu <- tkframe( dataorgW, borderwidth=10, relief="groove", bg='red' )
tkpack( fmMenu, expand=1, fill='both' )

scrX <- tkscrollbar( fmMenu, command=function(...) tkxview( tableData,
... ), orient="horizontal" )
scrY <- tkscrollbar( fmMenu, command=function(...) tkyview( tableData,
... ), orient="vertical" )

height <- -1; width <- -1

tableData <- tkwidget( fmMenu, "table", rows=50, cols=50,
                      titlerows=1, titlecols=1,
height=height+1,width=width+1,
                      xscrollcommand=function(...) tkset( scrX, ... ),
                      yscrollcommand=function(...) tkset( scrY, ... ),
                      rowstretch='all', colstretch='all'
                      )

tkconfigure( tableData, multiline=0 )
tkgrid( tableData, scrY )
tkgrid.configure( scrY, sticky="nsw")
tkgrid( scrX, sticky="new" )

tclarray <- tclArray()
tkconfigure( tableData, variable=tclarray, selectmode="extended",
background="white" )






---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Dec  4 02:42:15 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 3 Dec 2015 17:42:15 -0800
Subject: [R] system.file(...) self-referencing the containing package
In-Reply-To: <CA+YV+Hxh+emgx0uyLo1r--BiMYOArHqkGVz90K0hAfvN8Vvasg@mail.gmail.com>
References: <CA+YV+Hxh+emgx0uyLo1r--BiMYOArHqkGVz90K0hAfvN8Vvasg@mail.gmail.com>
Message-ID: <CAF8bMca3o4tqvQjryw6OAe4Ew4a0sghgZVT=cbfA9jnk=ybLqA@mail.gmail.com>

Every package has in it, after it is installed, a character object
called ".packageName" containing its name.  It is not exported from
the package.  Functions in your package can refer to it as just
.packageName.
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Dec 3, 2015 at 5:28 PM, Murat Tasan <mmuurr at gmail.com> wrote:
> In a package I'm writing, I'm placing all SQL code here:
> /inst/sql/
>
> And so when referring to these blocks of code from the package's R
> code, I do something like so:
>
> system.file("sql", "my_example_file.sql", package = "ThisPackage",
> mustWork = TRUE)
>
> But, referring to the package itself with the string "ThisPackage" is
> annoying and somewhat brittle... if the package were ever to change
> names (e.g. during development), I'll have to replace all such calls
> with the new package name.
> And generally referring to the package containing the code itself by
> name seems inelegant.
>
> Anyone know of a safe way to reference additional files in a package,
> from within that package's code, that doesn't require specifying the
> name of the package as a string?
>
> Cheers,
>
> -Murat
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From RYAN.VERMILIO at ucdenver.edu  Thu Dec  3 23:42:35 2015
From: RYAN.VERMILIO at ucdenver.edu (Vermilio, Ryan)
Date: Thu, 3 Dec 2015 22:42:35 +0000
Subject: [R] Question Regarding a nested for loop in R
Message-ID: <SN2PR0501MB105596D648EAA2DCCF45245E9D0D0@SN2PR0501MB1055.namprd05.prod.outlook.com>

Hello there,

I'm an R novice and am trying to figure out how to create an external for loop.  My current loop iterates 15 times and stores the resulting values in 3 separate vectors.  What I need is to create an outside loop that will run internal loop 4 times, sum the resulting vectors for each, and then store the sum of each in a vector of length 4 for each investment.

Right now, the target vector has the same value in each position, which is what I'm trying to fix.

Here's what I have at this point:

Inv1Returns <- c(0, 1000, -500, 500)
Inv2Returns <- c(0, -9000, 30000, 10000)
Inv3Returns <- c(0, 4000, -1000, -2000)

random = runif(15, 0, 1)

Inv1Outcome = NULL
Inv2Outcome = NULL
Inv3Outcome = NULL

Inv1Total = NULL
Inv2Total = NULL
Inv3Total = NULL

for (j in 1:4)

{

for (i in 1:15 )

{

  Inv1Outcome[i] = if (random[i] <= .25){Inv1Returns[1]}
  else if (random[i] > .25 & random[i] <= .50){Inv1Returns[2]}
  else if (random[i] > .50 & random[i] <= .75){Inv1Returns[3]}
  else {Inv1Returns[4]}

  Inv2Outcome[i] = if (random[i] <= .20){Inv2Returns[1]}
  else if (random[i] > .20 & random[i] <= .30){Inv2Returns[2]}
  else if (random[i] > .30 & random[i] <= .70){Inv2Returns[3]}
  else {Inv2Returns[4]}

  Inv3Outcome[i] = if (random[i] <= .50){Inv3Returns[1]}
  else if (random[i] > .50 & random[i] <= .70){Inv3Returns[2]}
  else if (random[i] > .70 & random[i] <= .90){Inv3Returns[3]}
  else {Inv3Returns[4]}

}

Inv1Total = append(Inv1Total, sum(Inv1Outcome))
Inv2Total = append(Inv2Total, sum(Inv2Outcome))
Inv3Total = append(Inv3Total, sum(Inv3Outcome))

}

Inv1Total
Inv2Total
Inv3Total


Sincerely,

Ryan

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Dec  4 06:59:51 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 3 Dec 2015 21:59:51 -0800
Subject: [R] Question Regarding a nested for loop in R
In-Reply-To: <SN2PR0501MB105596D648EAA2DCCF45245E9D0D0@SN2PR0501MB1055.namprd05.prod.outlook.com>
References: <SN2PR0501MB105596D648EAA2DCCF45245E9D0D0@SN2PR0501MB1055.namprd05.prod.outlook.com>
Message-ID: <B746BDF9-D0EE-4621-851C-536EAFC62DE2@comcast.net>


> On Dec 3, 2015, at 2:42 PM, Vermilio, Ryan <RYAN.VERMILIO at ucdenver.edu> wrote:
> 
> Hello there,
> 
> I'm an R novice and am trying to figure out how to create an external for loop.  My current loop iterates 15 times and stores the resulting values in 3 separate vectors.  What I need is to create an outside loop that will run internal loop 4 times, sum the resulting vectors for each, and then store the sum of each in a vector of length 4 for each investment.
> 
> Right now, the target vector has the same value in each position, which is what I'm trying to fix.
> 
> Here's what I have at this point:
> 
> Inv1Returns <- c(0, 1000, -500, 500)
> Inv2Returns <- c(0, -9000, 30000, 10000)
> Inv3Returns <- c(0, 4000, -1000, -2000)
> 
> random = runif(15, 0, 1)
> 
> Inv1Outcome = NULL
> Inv2Outcome = NULL
> Inv3Outcome = NULL
> 
> Inv1Total = NULL
> Inv2Total = NULL
> Inv3Total = NULL
> 

?? initialize a list

for( k in  1:4){ 

> for (j in 1:4)
> 
> {
> 
> for (i in 1:15 )
> 
> {
> 
>  Inv1Outcome[i] = if (random[i] <= .25){Inv1Returns[1]}
>  else if (random[i] > .25 & random[i] <= .50){Inv1Returns[2]}
>  else if (random[i] > .50 & random[i] <= .75){Inv1Returns[3]}
>  else {Inv1Returns[4]}
> 
>  Inv2Outcome[i] = if (random[i] <= .20){Inv2Returns[1]}
>  else if (random[i] > .20 & random[i] <= .30){Inv2Returns[2]}
>  else if (random[i] > .30 & random[i] <= .70){Inv2Returns[3]}
>  else {Inv2Returns[4]}
> 
>  Inv3Outcome[i] = if (random[i] <= .50){Inv3Returns[1]}
>  else if (random[i] > .50 & random[i] <= .70){Inv3Returns[2]}
>  else if (random[i] > .70 & random[i] <= .90){Inv3Returns[3]}
>  else {Inv3Returns[4]}
> 
> }
> 
> Inv1Total = append(Inv1Total, sum(Inv1Outcome))
> Inv2Total = append(Inv2Total, sum(Inv2Outcome))
> Inv3Total = append(Inv3Total, sum(Inv3Outcome))
> 
> }

 ---Do the requested operations
 ---Store in list

}


> 
> Inv1Total
> Inv2Total
> Inv3Total

I suspect I could write that operation in about 3 lines of code (and it actually took one line of code), but it really isn?t what you are requesting. There is a function designed for repeating operations is `replicate`. See:

?replicate



> 
> Sincerely,
> 
> Ryan
> 
> 	[[alternative HTML version deleted]]

Sincerely, Ryan, please do read the posting guide and post in plain text.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From phgrosjean at sciviews.org  Fri Dec  4 07:45:42 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Fri, 4 Dec 2015 07:45:42 +0100
Subject: [R] Tktable do not expand inside a Tkframe
In-Reply-To: <5660ECFC.1070009@yahoo.com.br>
References: <5660ECFC.1070009@yahoo.com.br>
Message-ID: <BBAE647B-3BED-41C2-A000-C1C1C0C7265B@sciviews.org>

Instead of using tkgrid(), you can do this with tkpack().

Replace:

> tkgrid( tableData, scrY )
> tkgrid.configure( scrY, sticky="nsw")
> tkgrid( scrX, sticky="new" )


by:

scrYwidth <- as.integer(tkcget(scrY, width = NULL))
tkpack(scrY, side = "right", fill = "y", expand = FALSE,
  pady = c(0, scrYwidth + 2))
tkpack(scrX, side = "bottom", fill = "x", expand = FALSE)
tkpack(tableData, fill = "both", expand = TRUE)

in your code.

Best,

Philippe Grosjean

> On 04 Dec 2015, at 02:31, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> 
> 
> hello all,
> 
> I'm trying to build a table within a frame. I would like the table to
> expand when the entire window is expanded.
> I could not find examples of how to solve this problem.
> I thank you in advance for help.
> 
> Cleber
> #######################################
> # piece of the code
> 
> library( tcltk )
> tclRequire( "Tktable" )
> dataorgW <- tktoplevel()
> fmMenu <- tkframe( dataorgW, borderwidth=10, relief="groove", bg='red' )
> tkpack( fmMenu, expand=1, fill='both' )
> 
> scrX <- tkscrollbar( fmMenu, command=function(...) tkxview( tableData,
> ... ), orient="horizontal" )
> scrY <- tkscrollbar( fmMenu, command=function(...) tkyview( tableData,
> ... ), orient="vertical" )
> 
> height <- -1; width <- -1
> 
> tableData <- tkwidget( fmMenu, "table", rows=50, cols=50,
>                      titlerows=1, titlecols=1,
> height=height+1,width=width+1,
>                      xscrollcommand=function(...) tkset( scrX, ... ),
>                      yscrollcommand=function(...) tkset( scrY, ... ),
>                      rowstretch='all', colstretch='all'
>                      )
> 
> tkconfigure( tableData, multiline=0 )
> tkgrid( tableData, scrY )
> tkgrid.configure( scrY, sticky="nsw")
> tkgrid( scrX, sticky="new" )
> 
> tclarray <- tclArray()
> tkconfigure( tableData, variable=tclarray, selectmode="extended",
> background="white" )
> 
> 
> 
> 
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Fri Dec  4 08:34:31 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 4 Dec 2015 18:34:31 +1100
Subject: [R] Strange error
In-Reply-To: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
References: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
Message-ID: <CA+8X3fVOZE+7VBLthjnaxVzO9CB8uTzd+5SWVMA4PcXVcSGm1Q@mail.gmail.com>

Hi Nick,
I think that Jeff may be correct in that the code was cut and pasted from a
non-text application. In particular, the error message about "*" is
suspicious. What may be happening is that when you select a single line, it
only picks up the text, but when you select multiple lines, the garbage
bytes come along for the ride. Since you seem to be using Windows, try
Notepad (text editor) for an external editor as it is usually better
behaved. The final error is the result of not having read the data into GWS.

Jim

On Thu, Dec 3, 2015 at 10:59 PM, Nicolae Doban <nickdoban at gmail.com> wrote:

> Hi,
>
> recently I received a strange error after running my code in chunks. But, I
> don't get any errors when I run it line by line.
>
> Also, what is strange is that the  error message is misspelled
>
> This is the error messages I get
>
> *> setwd("H:/XX/XXX")*
> *"rror: unexpected input in "setwd("H:/XX/XXX")*
> *> *
> *"rror: unexpected input in "*
> *> GWS <- read.csv("X.csv", header = TRUE, sep = ",", stringsAsFactors =
> FALSE, na.strings=c("","","NA"))*
> *"rror: unexpected input in "GWS <- read.csv("X.csv", header = TRUE, sep =
> ",", stringsAsFactors = FALSE, na.strings=c("","","NA"))*
> *> GWS <- GWS[with(GWS, order(d)), ]; row.names(GWS) <- NULL*
> *Error: object 'GWS' not found*
>
> As you can see even changing the directory results in an error message but,
> it changes the directory to the correct folder.
>
> What is interesting is that I was not getting this error yesterday (Dec.,
> 2nd 2015)
>
> Could you please shed some light on this issue? Is it something related to
> R or Rstudio or the packages?
>
> Thank you in advance very much,
> nick
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From thierry.onkelinx at inbo.be  Fri Dec  4 14:45:58 2015
From: thierry.onkelinx at inbo.be (Thierry Onkelinx)
Date: Fri, 4 Dec 2015 14:45:58 +0100
Subject: [R] queries return different output when sourced
Message-ID: <CAJuCY5wEvvRrCpHkTcg3vRh7_F4snR3CJmqe+DGcK_zEcXnqEQ@mail.gmail.com>

Dear all,

We need to run several queries in an R Markdown file. The queries have quit
elaborate sql statements. We try to recude the amount of code in the
markdown file by moving the query functions in a seperate R script which is
sourced by the markdown file.

The queries work fine if we place the code directly in the markdown file
but fail when sourced. They return an empty data.frame with the correct
colnames. We have attached a minimal example.

Can someone tell us why the sourced functions give the a different output?

Best regards,

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and
Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium

To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
------------- volgend deel ------------
Een niet-tekst bijlage is gescrubt...
Naam: QueriesTest.pdf
Type: application/pdf
Grootte: 103077 bytes
Omschrijving: niet beschikbaar
URL : <https://stat.ethz.ch/pipermail/r-help/attachments/20151204/f1bd4bcc/attachment.pdf>

From therneau at mayo.edu  Fri Dec  4 15:02:14 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Fri, 04 Dec 2015 08:02:14 -0600
Subject: [R] Survival analysis: ERROR: Time and status are different
	lengths
In-Reply-To: <mailman.3.1449226801.3956.r-help@r-project.org>
References: <mailman.3.1449226801.3956.r-help@r-project.org>
Message-ID: <c10f8b$206cfv@ironport10.mayo.edu>

I expect that reading the result of print(fit.weib) will answer your question.  If there 
were any missing values in the data set, then the fit.weib$linear.predictors will be 
shorter than the original data set,
and the printout will have a note about "...deleted due to missing".

The simplest solution to this is to set
   options(na.action="na.exclude")
before doing the fit.  Then predict(fit) and resid(fit) will return vectors of the same 
length as the input data, containing NA in the appropriate positions.  The default 
na.action of "na.omit" leaves missing out of both the fit and the residuals.

(Unfortunately, only a few modeling functions in R pay attention to the difference between 
these two na.action options.)

Terry Therneau


On 12/04/2015 05:00 AM, r-help-request at r-project.org wrote:
> Hi,
>
> I am fitting an AFT model assuming a Weibull distribution and I would like
> to check the residuals compared to the Kaplan Meier residuals, but when I
> try to create the Kaplan Meier residuals I get an error: Time and status
> are different lengths.
>
> I am using the following script:
>
> # Fitting the AFT model
> fit.weib <- survreg(Surv(TimeDeath, event) ~ age + sex + mutation +
> ventilation + BM1 + BM2, data = DF, dist = "weibull")
> fits <- fit.weib$linear.predictors
> resids <- (fit.weib$y[, 1] - fits)/fit.weib$scale
> resKM <- survfit(Surv(resids, event) ~ 1, data = DF)
>
> I get the error from the last line of the script.
>
> I tried some things that didn't work and I was wondering if anyone could
> help me.
> If you need more information please let me know.
>
> Thanks in advance,


From 081024015 at fudan.edu.cn  Fri Dec  4 13:38:43 2015
From: 081024015 at fudan.edu.cn (=?UTF-8?B?5p2O55Cl?=)
Date: Fri, 4 Dec 2015 20:38:43 +0800 (GMT+08:00)
Subject: [R] Can we use all the functions of R in visual studio? The
 Rmath.dll and R.dll seems only contain a part of R functions
Message-ID: <d4613ae.124eb.1516d006e48.Coremail.081024015@fudan.edu.cn>

Can we use all the functions of R in visual studio? The Rmath.dll and R.dll seems only contain a part of R functions

    Hi, I 'm going to use R functions in visual studio. I have investigated this problem for several days. There're guide about
using R in embedded way,which use R.dll. But it seems a lot of functions are not included in the dll, such as t.test().

    So, is there a way that we can use all the R functions in visual studio? Or is there a way we can use all the R functions in other 
compilers with c++? Or can we integrate R into c++ in linux platform?
    Thanks a lot.
   
     Best Regards.
                                  Eric.Li
                                   2015.12.4


From jklene000 at gmail.com  Fri Dec  4 10:15:58 2015
From: jklene000 at gmail.com (Johannes Klene)
Date: Fri, 4 Dec 2015 10:15:58 +0100
Subject: [R] Random forest regression: feedback on general approach and
	possible issues
Message-ID: <CACFq9+eTSbcadMSa8EJvWc5A2fWYPyFuhcrKCUDB=Np5Gf8ctQ@mail.gmail.com>

Hi all,
I'd like to use random forest regression to say something about the
importance of a set of genes (binary) for schizophrenia-related behavior
(continuous measure). I am still reading up on this technique, but would
already really appreciate any feedback on whether my approach is valid.
So...using the randomForest package, is it a good approach to enter a few
dozen binary predictors to assess their importance (as a set, and
individually) for a continuous measure with a sample size of ~1000 people?
More specific questions:
- I have an additional interest in interactions (though perhaps not the
best word in this context), does it make any sense to say something about
the influence one predictor has over others by looking at the change in
estimated importance of the others when that predictor is removed from the
model?
- I have a few siblings in the data, i.e. non-independence, is this a
problem and if so, is there anything I can do about it?
- The few papers I have seen so far on using this technique in a similar
situation do not include any 'standard' covariates such as age and gender,
should I?
Any and all feedback is greatly appreciated!! Kind regards, Johannes

p.s. Hope I've come to the right place despite this being a more general
question, if not please let me know of a forum where this is more suited
for.

	[[alternative HTML version deleted]]


From rmcgu at doh.health.nsw.gov.au  Fri Dec  4 06:49:58 2015
From: rmcgu at doh.health.nsw.gov.au (MCGUIRE, Rhydwyn)
Date: Fri, 4 Dec 2015 05:49:58 +0000
Subject: [R] Question Regarding a nested for loop in R
In-Reply-To: <SN2PR0501MB105596D648EAA2DCCF45245E9D0D0@SN2PR0501MB1055.namprd05.prod.outlook.com>
References: <SN2PR0501MB105596D648EAA2DCCF45245E9D0D0@SN2PR0501MB1055.namprd05.prod.outlook.com>
Message-ID: <AF36C32BE015CB48883C4A9F73CC8C32017488CCD0@DOHNSMXDB03.doh.health.nsw.gov.au>

Hi Ryan, 

I don't think you need the outer loop, loops are worth avoiding if you can possibly can because they are inefficient, have a look at this dplyr tutorial (https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) which should be able to achieve what your outer loop is currently doing. Vectorisation (applying a calculation to a whole array) should be able to process your innerloop. 

Good luck

Rhydwyn 


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Vermilio, Ryan
Sent: Friday, 4 December 2015 9:43 AM
To: r-help at r-project.org
Subject: [R] Question Regarding a nested for loop in R


Hello there,

I'm an R novice and am trying to figure out how to create an external for loop.  My current loop iterates 15 times and stores the resulting values in 3 separate vectors.  What I need is to create an outside loop that will run internal loop 4 times, sum the resulting vectors for each, and then store the sum of each in a vector of length 4 for each investment.

Right now, the target vector has the same value in each position, which is what I'm trying to fix.

Here's what I have at this point:

Inv1Returns <- c(0, 1000, -500, 500)
Inv2Returns <- c(0, -9000, 30000, 10000) Inv3Returns <- c(0, 4000, -1000, -2000)

random = runif(15, 0, 1)

Inv1Outcome = NULL
Inv2Outcome = NULL
Inv3Outcome = NULL

Inv1Total = NULL
Inv2Total = NULL
Inv3Total = NULL

for (j in 1:4)

{

for (i in 1:15 )

{

  Inv1Outcome[i] = if (random[i] <= .25){Inv1Returns[1]}
  else if (random[i] > .25 & random[i] <= .50){Inv1Returns[2]}
  else if (random[i] > .50 & random[i] <= .75){Inv1Returns[3]}
  else {Inv1Returns[4]}

  Inv2Outcome[i] = if (random[i] <= .20){Inv2Returns[1]}
  else if (random[i] > .20 & random[i] <= .30){Inv2Returns[2]}
  else if (random[i] > .30 & random[i] <= .70){Inv2Returns[3]}
  else {Inv2Returns[4]}

  Inv3Outcome[i] = if (random[i] <= .50){Inv3Returns[1]}
  else if (random[i] > .50 & random[i] <= .70){Inv3Returns[2]}
  else if (random[i] > .70 & random[i] <= .90){Inv3Returns[3]}
  else {Inv3Returns[4]}

}

Inv1Total = append(Inv1Total, sum(Inv1Outcome)) Inv2Total = append(Inv2Total, sum(Inv2Outcome)) Inv3Total = append(Inv3Total, sum(Inv3Outcome))

}

Inv1Total
Inv2Total
Inv3Total


Sincerely,

Ryan

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
__________________________________________________________________________________________________________
This email has been scanned for the NSW Ministry of Health by the Websense Hosted Email Security System.
Emails and attachments are monitored to ensure compliance with the NSW Ministry of health's Electronic Messaging Policy.
__________________________________________________________________________________________________________
_______________________________________________________________________________________________________
Disclaimer: This message is intended for the addressee named and may contain confidential information.
If you are not the intended recipient, please delete it and notify the sender.
Views expressed in this message are those of the individual sender, and are not necessarily the views of the NSW Ministry of Health.
_______________________________________________________________________________________________________
This email has been scanned for the NSW Ministry of Health by the Websense Hosted Email Security System.
Emails and attachments are monitored to ensure compliance with the NSW Ministry of Health's Electronic Messaging Policy.


From nickdoban at gmail.com  Fri Dec  4 10:25:32 2015
From: nickdoban at gmail.com (Nicolae Doban)
Date: Fri, 4 Dec 2015 09:25:32 +0000
Subject: [R] Strange error
In-Reply-To: <CA+8X3fVOZE+7VBLthjnaxVzO9CB8uTzd+5SWVMA4PcXVcSGm1Q@mail.gmail.com>
References: <CADPWiKhQMxyHGNCBPVAUdu0jy0s9Jfm_3z3JFufqm+oqg1nCMw@mail.gmail.com>
	<CA+8X3fVOZE+7VBLthjnaxVzO9CB8uTzd+5SWVMA4PcXVcSGm1Q@mail.gmail.com>
Message-ID: <CADPWiKji9hN5eS=+itDc5N24C54R3NJKqDrxXD5g6_YHw8uydQ@mail.gmail.com>

Hi again,

i found the error:
the problem was that I have created a template of rmarkdown myself inspired
from Tufte one and I guess I didn't build it correctly..when i tried
running it in a regular rmarkdown template it worked.

sorry for inconvenience
nick

Nicolae (Nick) Doban | Masters student
EIT KIC Energy for Smart Cities Master of Science Programme
Tel.: +32 489 11 81 73
LinkedIn: https://be.linkedin.com/in/nicolaedoban
Skype ID: doban.nicolae
Twitter: https://twitter.com/NicolaeDoban
GitHub: https://github.com/nickdoban


On Fri, Dec 4, 2015 at 7:34 AM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Nick,
> I think that Jeff may be correct in that the code was cut and pasted from
> a non-text application. In particular, the error message about "*" is
> suspicious. What may be happening is that when you select a single line, it
> only picks up the text, but when you select multiple lines, the garbage
> bytes come along for the ride. Since you seem to be using Windows, try
> Notepad (text editor) for an external editor as it is usually better
> behaved. The final error is the result of not having read the data into GWS.
>
> Jim
>
> On Thu, Dec 3, 2015 at 10:59 PM, Nicolae Doban <nickdoban at gmail.com>
> wrote:
>
>> Hi,
>>
>> recently I received a strange error after running my code in chunks. But,
>> I
>> don't get any errors when I run it line by line.
>>
>> Also, what is strange is that the  error message is misspelled
>>
>> This is the error messages I get
>>
>> *> setwd("H:/XX/XXX")*
>> *"rror: unexpected input in "setwd("H:/XX/XXX")*
>> *> *
>> *"rror: unexpected input in "*
>> *> GWS <- read.csv("X.csv", header = TRUE, sep = ",", stringsAsFactors =
>> FALSE, na.strings=c("","","NA"))*
>> *"rror: unexpected input in "GWS <- read.csv("X.csv", header = TRUE, sep =
>> ",", stringsAsFactors = FALSE, na.strings=c("","","NA"))*
>> *> GWS <- GWS[with(GWS, order(d)), ]; row.names(GWS) <- NULL*
>> *Error: object 'GWS' not found*
>>
>> As you can see even changing the directory results in an error message
>> but,
>> it changes the directory to the correct folder.
>>
>> What is interesting is that I was not getting this error yesterday (Dec.,
>> 2nd 2015)
>>
>> Could you please shed some light on this issue? Is it something related to
>> R or Rstudio or the packages?
>>
>> Thank you in advance very much,
>> nick
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From sabasehrish at yahoo.com  Fri Dec  4 11:20:36 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Fri, 4 Dec 2015 02:20:36 -0800
Subject: [R] For loop coding
Message-ID: <1449224436.74197.YahooMailAndroidMobile@web121806.mail.ne1.yahoo.com>

Hi

I will be grateful if someone please tell me the programming to run regression on time series data through "For Loop".

Regards.
Saba

Sent from Yahoo Mail on Android


	[[alternative HTML version deleted]]


From Marios.BARLAS at cea.fr  Fri Dec  4 11:51:42 2015
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Fri, 4 Dec 2015 10:51:42 +0000
Subject: [R] Ordering Filenames stored in list or vector
Message-ID: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>

Hello everyone,

I am an R rookie and I'm learning as I program.

I am working on a script to process a large amount of data: I read a pattern of filenames in the folder I want and import their data

filenames = list.files(path, pattern="*Q_Read_prist*")

myfiles = lapply(filenames, function(x) read.xlsx2(file=x, sheetName="Data", header=TRUE, FILENAMEVAR=x))

The problem is that R recognizes the files in a 'non human' order.

Q_Read_prist#1 at 1.xls   Q_Read_prist#1 at 1.xls
Q_Read_prist#1 at 10.xls Q_Read_prist#1 at 10.xls
Q_Read_prist#1 at 11.xls Q_Read_prist#1 at 11.xls
Q_Read_prist#1 at 12.xls Q_Read_prist#1 at 12.xls
Q_Read_prist#1 at 13.xls Q_Read_prist#1 at 13.xls
Q_Read_prist#1 at 14.xls Q_Read_prist#1 at 14.xls
Q_Read_prist#1 at 15.xls Q_Read_prist#1 at 15.xls
Q_Read_prist#1 at 16.xls Q_Read_prist#1 at 16.xls
Q_Read_prist#1 at 17.xls Q_Read_prist#1 at 17.xls
Q_Read_prist#1 at 18.xls Q_Read_prist#1 at 18.xls
Q_Read_prist#1 at 19.xls Q_Read_prist#1 at 19.xls
Q_Read_prist#1 at 2.xls   Q_Read_prist#1 at 2.xls
Q_Read_prist#1 at 3.xls   Q_Read_prist#1 at 3.xls
Q_Read_prist#1 at 4.xls   Q_Read_prist#1 at 4.xls
Q_Read_prist#1 at 5.xls   Q_Read_prist#1 at 5.xls
Q_Read_prist#1 at 6.xls   Q_Read_prist#1 at 6.xls
Q_Read_prist#1 at 7.xls   Q_Read_prist#1 at 7.xls
Q_Read_prist#1 at 8.xls   Q_Read_prist#1 at 8.xls
Q_Read_prist#1 at 9.xls   Q_Read_prist#1 at 9.xls

I tried to order them using order or sort but it doesn' seem to work. I have had the same issue in matlab but there I have a function to re-define the order in a "correct" way.

Anyone knows of a smart way to sort these guys from 1 to 19 ascending or descending?

Thanks in advance,
Mario

	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Fri Dec  4 16:53:56 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Fri, 4 Dec 2015 10:53:56 -0500
Subject: [R] Ordering Filenames stored in list or vector
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>
Message-ID: <3BB9484C-1716-4A25-B5F7-6A30808A5F43@utoronto.ca>

The thread below has a number of solutions. I personally like the one with sprintf().
   https://stat.ethz.ch/pipermail/r-help/2010-July/246059.html


B.

On Dec 4, 2015, at 5:51 AM, BARLAS Marios 247554 <Marios.BARLAS at cea.fr> wrote:

> Hello everyone,
> 
> I am an R rookie and I'm learning as I program.
> 
> I am working on a script to process a large amount of data: I read a pattern of filenames in the folder I want and import their data
> 
> filenames = list.files(path, pattern="*Q_Read_prist*")
> 
> myfiles = lapply(filenames, function(x) read.xlsx2(file=x, sheetName="Data", header=TRUE, FILENAMEVAR=x))
> 
> The problem is that R recognizes the files in a 'non human' order.
> 
> Q_Read_prist#1 at 1.xls   Q_Read_prist#1 at 1.xls
> Q_Read_prist#1 at 10.xls Q_Read_prist#1 at 10.xls
> Q_Read_prist#1 at 11.xls Q_Read_prist#1 at 11.xls
> Q_Read_prist#1 at 12.xls Q_Read_prist#1 at 12.xls
> Q_Read_prist#1 at 13.xls Q_Read_prist#1 at 13.xls
> Q_Read_prist#1 at 14.xls Q_Read_prist#1 at 14.xls
> Q_Read_prist#1 at 15.xls Q_Read_prist#1 at 15.xls
> Q_Read_prist#1 at 16.xls Q_Read_prist#1 at 16.xls
> Q_Read_prist#1 at 17.xls Q_Read_prist#1 at 17.xls
> Q_Read_prist#1 at 18.xls Q_Read_prist#1 at 18.xls
> Q_Read_prist#1 at 19.xls Q_Read_prist#1 at 19.xls
> Q_Read_prist#1 at 2.xls   Q_Read_prist#1 at 2.xls
> Q_Read_prist#1 at 3.xls   Q_Read_prist#1 at 3.xls
> Q_Read_prist#1 at 4.xls   Q_Read_prist#1 at 4.xls
> Q_Read_prist#1 at 5.xls   Q_Read_prist#1 at 5.xls
> Q_Read_prist#1 at 6.xls   Q_Read_prist#1 at 6.xls
> Q_Read_prist#1 at 7.xls   Q_Read_prist#1 at 7.xls
> Q_Read_prist#1 at 8.xls   Q_Read_prist#1 at 8.xls
> Q_Read_prist#1 at 9.xls   Q_Read_prist#1 at 9.xls
> 
> I tried to order them using order or sort but it doesn' seem to work. I have had the same issue in matlab but there I have a function to re-define the order in a "correct" way.
> 
> Anyone knows of a smart way to sort these guys from 1 to 19 ascending or descending?
> 
> Thanks in advance,
> Mario
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Fri Dec  4 17:02:31 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Dec 2015 08:02:31 -0800
Subject: [R] Random forest regression: feedback on general approach and
 possible issues
In-Reply-To: <CACFq9+eTSbcadMSa8EJvWc5A2fWYPyFuhcrKCUDB=Np5Gf8ctQ@mail.gmail.com>
References: <CACFq9+eTSbcadMSa8EJvWc5A2fWYPyFuhcrKCUDB=Np5Gf8ctQ@mail.gmail.com>
Message-ID: <CAGxFJbRuf7E_iYzG224EByOi3MO1kQeaaa-eYvuVyHR1qa_1oQ@mail.gmail.com>

I would suggest that you post instead on stats.stackexchange.com  .
This forum is mostly about R programming issues, not statistics
(admittedly, the intersection is nonempty, but ...) That stackexchange
forum is more about statistics.

You might also consider a bioconductor forum, as this appears to be a
bioinformatics type of issue.

Cheers,

Bert

P.S. Both of these could be found with suitable internet searches.
Don't neglect search engines for these types of queries. I have found
them to be very helpful.
Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Dec 4, 2015 at 1:15 AM, Johannes Klene <jklene000 at gmail.com> wrote:
> Hi all,
> I'd like to use random forest regression to say something about the
> importance of a set of genes (binary) for schizophrenia-related behavior
> (continuous measure). I am still reading up on this technique, but would
> already really appreciate any feedback on whether my approach is valid.
> So...using the randomForest package, is it a good approach to enter a few
> dozen binary predictors to assess their importance (as a set, and
> individually) for a continuous measure with a sample size of ~1000 people?
> More specific questions:
> - I have an additional interest in interactions (though perhaps not the
> best word in this context), does it make any sense to say something about
> the influence one predictor has over others by looking at the change in
> estimated importance of the others when that predictor is removed from the
> model?
> - I have a few siblings in the data, i.e. non-independence, is this a
> problem and if so, is there anything I can do about it?
> - The few papers I have seen so far on using this technique in a similar
> situation do not include any 'standard' covariates such as age and gender,
> should I?
> Any and all feedback is greatly appreciated!! Kind regards, Johannes
>
> p.s. Hope I've come to the right place despite this being a more general
> question, if not please let me know of a forum where this is more suited
> for.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tring at gvdnet.dk  Fri Dec  4 17:03:00 2015
From: tring at gvdnet.dk (Troels Ring)
Date: Fri, 4 Dec 2015 17:03:00 +0100
Subject: [R] dataframe rbind
Message-ID: <5661B934.8000403@gvdnet.dk>

Dear friends - I have a very simple question -
I generate a number of dataframes with identical names and want to 
combine them into one large dataframe with the same names -
here is an example

DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
for (i in 1:10){
DF <- DF+rbind(data.frame(a=rnorm(10),b=runif(10),ID=i))}

- the dataframe do not grow as I thought it would.

How would I do this?

All best wishes
Troels Ring
Nephrology
Aalborg
Denmark


From petr.pikal at precheza.cz  Fri Dec  4 17:09:41 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 4 Dec 2015 16:09:41 +0000
Subject: [R] dataframe rbind
In-Reply-To: <5661B934.8000403@gvdnet.dk>
References: <5661B934.8000403@gvdnet.dk>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500368A@SRVEXCHMBX.precheza.cz>

Hi

Maybe little bit of studying how functions work can be useful

rbind uses to bind two objects together, however you give it only one.

Use

DF <- rbind(DF, data.frame(a=rnorm(10),b=runif(10),ID=i))

instead.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels
> Ring
> Sent: Friday, December 04, 2015 5:03 PM
> To: r-help at r-project.org
> Subject: [R] dataframe rbind
>
> Dear friends - I have a very simple question - I generate a number of
> dataframes with identical names and want to combine them into one large
> dataframe with the same names - here is an example
>
> DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
> for (i in 1:10){
> DF <- DF+rbind(data.frame(a=rnorm(10),b=runif(10),ID=i))}
>
> - the dataframe do not grow as I thought it would.
>
> How would I do this?
>
> All best wishes
> Troels Ring
> Nephrology
> Aalborg
> Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From dcarlson at tamu.edu  Fri Dec  4 17:10:19 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Fri, 4 Dec 2015 16:10:19 +0000
Subject: [R] dataframe rbind
In-Reply-To: <5661B934.8000403@gvdnet.dk>
References: <5661B934.8000403@gvdnet.dk>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E6B82@mb02.ads.tamu.edu>

This will work, although depending on what you are trying to do, there may be a better way:

> DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
> for (i in 1:10){
+ DF <- rbind(DF, data.frame(a=rnorm(10),b=runif(10),ID=i))}
> str(DF)
'data.frame':   110 obs. of  3 variables:
 $ a : num  0.792 0.141 -1.091 -0.918 1.265 ...
 $ b : num  0.5935 0.695 0.075 0.0827 0.852 ...
 $ ID: num  0 0 0 0 0 0 0 0 0 0 ...

For example:

> DF <- data.frame(a=rnorm(110), b=rnorm(110), ID=rep(0:10, each=10))
> str(DF)
'data.frame':   110 obs. of  3 variables:
 $ a : num  -1.91926 0.00791 0.65523 0.95019 1.23822 ...
 $ b : num  0.344 1.281 2.057 -1.69 -0.268 ...
 $ ID: int  0 0 0 0 0 0 0 0 0 0 ...

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels Ring
Sent: Friday, December 4, 2015 10:03 AM
To: r-help at r-project.org
Subject: [R] dataframe rbind

Dear friends - I have a very simple question -
I generate a number of dataframes with identical names and want to 
combine them into one large dataframe with the same names -
here is an example

DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
for (i in 1:10){
DF <- DF+rbind(data.frame(a=rnorm(10),b=runif(10),ID=i))}

- the dataframe do not grow as I thought it would.

How would I do this?

All best wishes
Troels Ring
Nephrology
Aalborg
Denmark

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Fri Dec  4 17:11:58 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 4 Dec 2015 16:11:58 +0000
Subject: [R] For loop coding
In-Reply-To: <1449224436.74197.YahooMailAndroidMobile@web121806.mail.ne1.yahoo.com>
References: <1449224436.74197.YahooMailAndroidMobile@web121806.mail.ne1.yahoo.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500369C@SRVEXCHMBX.precheza.cz>

Hi

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Saba
> Sehrish via R-help
> Sent: Friday, December 04, 2015 11:21 AM
> To: r-help at r-project.org
> Subject: [R] For loop coding
>
> Hi
>
> I will be grateful if someone please tell me the programming to run
> regression on time series data through "For Loop".

for ( i in seq) {

lll[[i]] <- lm(whatever)

}

Cheers
Petr

>
> Regards.
> Saba
>
> Sent from Yahoo Mail on Android
>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From bgunter.4567 at gmail.com  Fri Dec  4 17:12:08 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 4 Dec 2015 08:12:08 -0800
Subject: [R] dataframe rbind
In-Reply-To: <5661B934.8000403@gvdnet.dk>
References: <5661B934.8000403@gvdnet.dk>
Message-ID: <CAGxFJbR8WqCLwiy2s4VhrGkyeRk415NTJrPzdJQm6=wr0FiZ1g@mail.gmail.com>

Try reading and following the Help file, ?rbind.data.frame.

You are inventing your own syntax, not using R's.

Incidentally, growing the frames as you do is generally a bad idea.
Search r-help archives for why.

Cheers,
Bert

Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Fri, Dec 4, 2015 at 8:03 AM, Troels Ring <tring at gvdnet.dk> wrote:
> Dear friends - I have a very simple question -
> I generate a number of dataframes with identical names and want to combine
> them into one large dataframe with the same names -
> here is an example
>
> DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
> for (i in 1:10){
> DF <- DF+rbind(data.frame(a=rnorm(10),b=runif(10),ID=i))}
>
> - the dataframe do not grow as I thought it would.
>
> How would I do this?
>
> All best wishes
> Troels Ring
> Nephrology
> Aalborg
> Denmark
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Fri Dec  4 17:13:35 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 4 Dec 2015 17:13:35 +0100
Subject: [R] dataframe rbind
In-Reply-To: <5661B934.8000403@gvdnet.dk>
References: <5661B934.8000403@gvdnet.dk>
Message-ID: <EFBE58A0-58A1-4181-945D-FF5281E14285@gmail.com>


On 04 Dec 2015, at 17:03 , Troels Ring <tring at gvdnet.dk> wrote:

> Dear friends - I have a very simple question -
> I generate a number of dataframes with identical names and want to combine them into one large dataframe with the same names -
> here is an example
> 
> DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
> for (i in 1:10){
> DF <- DF+rbind(data.frame(a=rnorm(10),b=runif(10),ID=i))}
> 
> - the dataframe do not grow as I thought it would.
> 

Don't you mean DF <- rbind(DF, data.frame(...... ?

(Or, but a different discussion, do.call("rbind", lapply(1:10, function(i) data.frame(.....,ID=i)))

-pd


> How would I do this?
> 
> All best wishes
> Troels Ring
> Nephrology
> Aalborg
> Denmark
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From henrik.bengtsson at gmail.com  Fri Dec  4 17:14:35 2015
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Fri, 4 Dec 2015 08:14:35 -0800
Subject: [R] Ordering Filenames stored in list or vector
In-Reply-To: <3BB9484C-1716-4A25-B5F7-6A30808A5F43@utoronto.ca>
References: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>
	<3BB9484C-1716-4A25-B5F7-6A30808A5F43@utoronto.ca>
Message-ID: <CAFDcVCRAFFzsV3=XkSqhdixDVRfT17BdReLzdXae8=C3pk981w@mail.gmail.com>

> filenames <- c("Q_Read_prist#1 at 1.xls", "Q_Read_prist#1 at 10.xls", "Q_Read_prist#1 at 2.xls")
> filenames <- gtools::mixedsort(filenames, numeric.type="decimal")
> filenames
[1] "Q_Read_prist#1 at 1.xls"  "Q_Read_prist#1 at 2.xls"  "Q_Read_prist#1 at 10.xls"

/Henrik

On Fri, Dec 4, 2015 at 7:53 AM, Boris Steipe <boris.steipe at utoronto.ca> wrote:
> The thread below has a number of solutions. I personally like the one with sprintf().
>    https://stat.ethz.ch/pipermail/r-help/2010-July/246059.html
>
>
> B.
>
> On Dec 4, 2015, at 5:51 AM, BARLAS Marios 247554 <Marios.BARLAS at cea.fr> wrote:
>
>> Hello everyone,
>>
>> I am an R rookie and I'm learning as I program.
>>
>> I am working on a script to process a large amount of data: I read a pattern of filenames in the folder I want and import their data
>>
>> filenames = list.files(path, pattern="*Q_Read_prist*")
>>
>> myfiles = lapply(filenames, function(x) read.xlsx2(file=x, sheetName="Data", header=TRUE, FILENAMEVAR=x))
>>
>> The problem is that R recognizes the files in a 'non human' order.
>>
>> Q_Read_prist#1 at 1.xls   Q_Read_prist#1 at 1.xls
>> Q_Read_prist#1 at 10.xls Q_Read_prist#1 at 10.xls
>> Q_Read_prist#1 at 11.xls Q_Read_prist#1 at 11.xls
>> Q_Read_prist#1 at 12.xls Q_Read_prist#1 at 12.xls
>> Q_Read_prist#1 at 13.xls Q_Read_prist#1 at 13.xls
>> Q_Read_prist#1 at 14.xls Q_Read_prist#1 at 14.xls
>> Q_Read_prist#1 at 15.xls Q_Read_prist#1 at 15.xls
>> Q_Read_prist#1 at 16.xls Q_Read_prist#1 at 16.xls
>> Q_Read_prist#1 at 17.xls Q_Read_prist#1 at 17.xls
>> Q_Read_prist#1 at 18.xls Q_Read_prist#1 at 18.xls
>> Q_Read_prist#1 at 19.xls Q_Read_prist#1 at 19.xls
>> Q_Read_prist#1 at 2.xls   Q_Read_prist#1 at 2.xls
>> Q_Read_prist#1 at 3.xls   Q_Read_prist#1 at 3.xls
>> Q_Read_prist#1 at 4.xls   Q_Read_prist#1 at 4.xls
>> Q_Read_prist#1 at 5.xls   Q_Read_prist#1 at 5.xls
>> Q_Read_prist#1 at 6.xls   Q_Read_prist#1 at 6.xls
>> Q_Read_prist#1 at 7.xls   Q_Read_prist#1 at 7.xls
>> Q_Read_prist#1 at 8.xls   Q_Read_prist#1 at 8.xls
>> Q_Read_prist#1 at 9.xls   Q_Read_prist#1 at 9.xls
>>
>> I tried to order them using order or sort but it doesn' seem to work. I have had the same issue in matlab but there I have a function to re-define the order in a "correct" way.
>>
>> Anyone knows of a smart way to sort these guys from 1 to 19 ascending or descending?
>>
>> Thanks in advance,
>> Mario
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From tring at gvdnet.dk  Fri Dec  4 17:17:01 2015
From: tring at gvdnet.dk (Troels Ring)
Date: Fri, 4 Dec 2015 17:17:01 +0100
Subject: [R] dataframe rbind
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C500368A@SRVEXCHMBX.precheza.cz>
References: <5661B934.8000403@gvdnet.dk>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C500368A@SRVEXCHMBX.precheza.cz>
Message-ID: <5661BC7D.3030800@gvdnet.dk>

Thanks a lot - I should have seen that
Best wishes
Troels

Den 04-12-2015 kl. 17:09 skrev PIKAL Petr:
> Hi
>
> Maybe little bit of studying how functions work can be useful
>
> rbind uses to bind two objects together, however you give it only one.
>
> Use
>
> DF <- rbind(DF, data.frame(a=rnorm(10),b=runif(10),ID=i))
>
> instead.
>
> Cheers
> Petr
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Troels
>> Ring
>> Sent: Friday, December 04, 2015 5:03 PM
>> To: r-help at r-project.org
>> Subject: [R] dataframe rbind
>>
>> Dear friends - I have a very simple question - I generate a number of
>> dataframes with identical names and want to combine them into one large
>> dataframe with the same names - here is an example
>>
>> DF <- data.frame(a=rnorm(10),b=runif(10),ID=0)
>> for (i in 1:10){
>> DF <- DF+rbind(data.frame(a=rnorm(10),b=runif(10),ID=i))}
>>
>> - the dataframe do not grow as I thought it would.
>>
>> How would I do this?
>>
>> All best wishes
>> Troels Ring
>> Nephrology
>> Aalborg
>> Denmark
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Fri Dec  4 17:20:08 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Fri, 4 Dec 2015 16:20:08 +0000
Subject: [R] Question Regarding a nested for loop in R
Message-ID: <D286FA4D.1429C3%macqueen1@llnl.gov>

I'll hope this isn't homework (R-help has a "don't do people's homework
for them" convention), and make a few suggestions.

The inner loop is not needed. For example, you can replace

for (i in 1:15 ) {


Inv1Outcome[i] = if (random[i] <= .25){Inv1Returns[1]}
  else if (random[i] > .25 & random[i] <= .50){Inv1Returns[2]}
  else if (random[i] > .50 & random[i] <= .75){Inv1Returns[3]}
  else {Inv1Returns[4]}

}


with

 
Inv1Outcome <- rep(Inv1Returns[1],15]

Inv1Outcome[random > 0.25 & random <= 0.50] <- Inv1Returns[2]
Inv1Outcome[random > 0.50 & random <= 0.75] <- Inv1Returns[3]
Inv1Outcome[random > 0.75]                  <- Inv1Returns[4]

And similarly for the other two outcomes.

For that matter, if I interpret correctly what's going on, this might do
it:

Inv1Outcome <- sample(Inv1Returns, 15, replace=TRUE, prob=c(0.25, 0.25,
0.25, 0.25)



I don't think I understand what you're trying to do with the outer loop,
but it does occur to me that with 'random' defined outside both loops, the
inner loop will have exactly the same results all four times, and does
that make any sense? Try putting
  random = runif(15, 0, 1)
inside the outer loop but outside the inner loop.

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 12/3/15, 2:42 PM, "R-help on behalf of Vermilio, Ryan"
<r-help-bounces at r-project.org on behalf of RYAN.VERMILIO at ucdenver.edu>
wrote:

>Hello there,
>
>I'm an R novice and am trying to figure out how to create an external for
>loop.  My current loop iterates 15 times and stores the resulting values
>in 3 separate vectors.  What I need is to create an outside loop that
>will run internal loop 4 times, sum the resulting vectors for each, and
>then store the sum of each in a vector of length 4 for each investment.
>
>Right now, the target vector has the same value in each position, which
>is what I'm trying to fix.
>
>Here's what I have at this point:
>
>Inv1Returns <- c(0, 1000, -500, 500)
>Inv2Returns <- c(0, -9000, 30000, 10000)
>Inv3Returns <- c(0, 4000, -1000, -2000)
>
>random = runif(15, 0, 1)
>
>Inv1Outcome = NULL
>Inv2Outcome = NULL
>Inv3Outcome = NULL
>
>Inv1Total = NULL
>Inv2Total = NULL
>Inv3Total = NULL
>
>for (j in 1:4)
>
>{
>
>for (i in 1:15 )
>
>{
>
>  Inv1Outcome[i] = if (random[i] <= .25){Inv1Returns[1]}
>  else if (random[i] > .25 & random[i] <= .50){Inv1Returns[2]}
>  else if (random[i] > .50 & random[i] <= .75){Inv1Returns[3]}
>  else {Inv1Returns[4]}
>
>  Inv2Outcome[i] = if (random[i] <= .20){Inv2Returns[1]}
>  else if (random[i] > .20 & random[i] <= .30){Inv2Returns[2]}
>  else if (random[i] > .30 & random[i] <= .70){Inv2Returns[3]}
>  else {Inv2Returns[4]}
>
>  Inv3Outcome[i] = if (random[i] <= .50){Inv3Returns[1]}
>  else if (random[i] > .50 & random[i] <= .70){Inv3Returns[2]}
>  else if (random[i] > .70 & random[i] <= .90){Inv3Returns[3]}
>  else {Inv3Returns[4]}
>
>}
>
>Inv1Total = append(Inv1Total, sum(Inv1Outcome))
>Inv2Total = append(Inv2Total, sum(Inv2Outcome))
>Inv3Total = append(Inv3Total, sum(Inv3Outcome))
>
>}
>
>Inv1Total
>Inv2Total
>Inv3Total
>
>
>Sincerely,
>
>Ryan
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From msharp at txbiomed.org  Fri Dec  4 17:25:04 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Fri, 4 Dec 2015 16:25:04 +0000
Subject: [R] Ordering Filenames stored in list or vector
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>
Message-ID: <EEC69174-3996-4831-BE73-C363C71CFBA4@txbiomed.org>

Mario,

I am certain there are more elegant solutions. This is an effort to make the process clear by dividing out each transformation used into separate lines.

## Start of code
library(stringi) # This is written in C and C++ (ICU library), is fast, and is well documented.
filenames <- c("Q_Read_prist#1 at 1.xls", "Q_Read_prist#1 at 10.xls", 
           "Q_Read_prist#1 at 11.xls", "Q_Read_prist#1 at 12.xls", 
           "Q_Read_prist#1 at 13.xls", "Q_Read_prist#1 at 14.xls", 
           "Q_Read_prist#1 at 15.xls", "Q_Read_prist#1 at 16.xls", 
           "Q_Read_prist#1 at 17.xls", "Q_Read_prist#1 at 18.xls", 
           "Q_Read_prist#1 at 19.xls", "Q_Read_prist#1 at 2.xls", 
           "Q_Read_prist#1 at 3.xls", "Q_Read_prist#1 at 4.xls", 
           "Q_Read_prist#1 at 5.xls", "Q_Read_prist#1 at 6.xls", 
           "Q_Read_prist#1 at 7.xls", "Q_Read_prist#1 at 8.xls", 
           "Q_Read_prist#1 at 9.xls")
indx_list <- stri_split_regex(filenames, pattern = "[@.]")
indx <- sapply(indx_list, function(x) {x[[2]]})
filenames_df <- data.frame(file_name = filenames, indx = indx, 
                            stringsAsFactors = FALSE)
filenames_ordered <- filenames_df[order(as.numeric(filenames_df$indx)), 
                                    "file_name"]
filenames_ordered
## end of code
Mark


R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org







> On Dec 4, 2015, at 4:51 AM, BARLAS Marios 247554 <Marios.BARLAS at cea.fr> wrote:
> 
> Hello everyone,
> 
> I am an R rookie and I'm learning as I program.
> 
> I am working on a script to process a large amount of data: I read a pattern of filenames in the folder I want and import their data
> 
> filenames = list.files(path, pattern="*Q_Read_prist*")
> 
> myfiles = lapply(filenames, function(x) read.xlsx2(file=x, sheetName="Data", header=TRUE, FILENAMEVAR=x))
> 
> The problem is that R recognizes the files in a 'non human' order.
> 
> Q_Read_prist#1 at 1.xls   Q_Read_prist#1 at 1.xls
> Q_Read_prist#1 at 10.xls Q_Read_prist#1 at 10.xls
> Q_Read_prist#1 at 11.xls Q_Read_prist#1 at 11.xls
> Q_Read_prist#1 at 12.xls Q_Read_prist#1 at 12.xls
> Q_Read_prist#1 at 13.xls Q_Read_prist#1 at 13.xls
> Q_Read_prist#1 at 14.xls Q_Read_prist#1 at 14.xls
> Q_Read_prist#1 at 15.xls Q_Read_prist#1 at 15.xls
> Q_Read_prist#1 at 16.xls Q_Read_prist#1 at 16.xls
> Q_Read_prist#1 at 17.xls Q_Read_prist#1 at 17.xls
> Q_Read_prist#1 at 18.xls Q_Read_prist#1 at 18.xls
> Q_Read_prist#1 at 19.xls Q_Read_prist#1 at 19.xls
> Q_Read_prist#1 at 2.xls   Q_Read_prist#1 at 2.xls
> Q_Read_prist#1 at 3.xls   Q_Read_prist#1 at 3.xls
> Q_Read_prist#1 at 4.xls   Q_Read_prist#1 at 4.xls
> Q_Read_prist#1 at 5.xls   Q_Read_prist#1 at 5.xls
> Q_Read_prist#1 at 6.xls   Q_Read_prist#1 at 6.xls
> Q_Read_prist#1 at 7.xls   Q_Read_prist#1 at 7.xls
> Q_Read_prist#1 at 8.xls   Q_Read_prist#1 at 8.xls
> Q_Read_prist#1 at 9.xls   Q_Read_prist#1 at 9.xls
> 
> I tried to order them using order or sort but it doesn' seem to work. I have had the same issue in matlab but there I have a function to re-define the order in a "correct" way.
> 
> Anyone knows of a smart way to sort these guys from 1 to 19 ascending or descending?
> 
> Thanks in advance,
> Mario
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From mmuurr at gmail.com  Fri Dec  4 17:28:51 2015
From: mmuurr at gmail.com (Murat Tasan)
Date: Fri, 4 Dec 2015 09:28:51 -0700
Subject: [R] system.file(...) self-referencing the containing package
In-Reply-To: <CAF8bMca3o4tqvQjryw6OAe4Ew4a0sghgZVT=cbfA9jnk=ybLqA@mail.gmail.com>
References: <CA+YV+Hxh+emgx0uyLo1r--BiMYOArHqkGVz90K0hAfvN8Vvasg@mail.gmail.com>
	<CAF8bMca3o4tqvQjryw6OAe4Ew4a0sghgZVT=cbfA9jnk=ybLqA@mail.gmail.com>
Message-ID: <CA+YV+HyAOt87Byf8znsaJ5yO1BQUpdPeaQJOujdSrBv4VzHEBQ@mail.gmail.com>

Perfect, thanks!

(Any idea if/where this is documented? I checked through the "Writing
R Extensions" doc and couldn't find any mention of it.)

Thanks much again,

-Murat


On Thu, Dec 3, 2015 at 6:42 PM, William Dunlap <wdunlap at tibco.com> wrote:
> Every package has in it, after it is installed, a character object
> called ".packageName" containing its name.  It is not exported from
> the package.  Functions in your package can refer to it as just
> .packageName.
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Thu, Dec 3, 2015 at 5:28 PM, Murat Tasan <mmuurr at gmail.com> wrote:
>> In a package I'm writing, I'm placing all SQL code here:
>> /inst/sql/
>>
>> And so when referring to these blocks of code from the package's R
>> code, I do something like so:
>>
>> system.file("sql", "my_example_file.sql", package = "ThisPackage",
>> mustWork = TRUE)
>>
>> But, referring to the package itself with the string "ThisPackage" is
>> annoying and somewhat brittle... if the package were ever to change
>> names (e.g. during development), I'll have to replace all such calls
>> with the new package name.
>> And generally referring to the package containing the code itself by
>> name seems inelegant.
>>
>> Anyone know of a safe way to reference additional files in a package,
>> from within that package's code, that doesn't require specifying the
>> name of the package as a string?
>>
>> Cheers,
>>
>> -Murat
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From mmuurr at gmail.com  Fri Dec  4 17:52:57 2015
From: mmuurr at gmail.com (Murat Tasan)
Date: Fri, 4 Dec 2015 09:52:57 -0700
Subject: [R] RPostgreSQL (or even ANSI DBI) parameterized query with IN (...)
Message-ID: <CA+YV+HxWNS8GBFej2iQ632=yffCYNGOuic1FBJGJXOPaHwTDtA@mail.gmail.com>

Using PostgreSQL's parameterized query form, this works:

R> dbSendQuery(CONN, "SELECT * FROM foo WHERE val = $1 OR val = $2",
list("bar", "baz"))

... and becomes: SELECT * FROM foo WHERE val = 'bar' OR val = 'baz';

I cannot figure out, however, if something like this is possible with
RPostgreSQL:

R> dbSendQuery(CONN, "SELECT * FROM foo WHERE val IN ($1)",
list(c("bar", "baz")))

... which becomes: SELECT * FROM foo WHERE val IN ('bar', 'baz');

(To be clear, the dbSendQuery attempt above does _not_ work.)

Anyone know if this is doable with RPostgreSQL?
I can construct the statement with sprintf/paste, but I'd prefer to
use properly-parameterized queries if possible (relying on PostgreSQL
to do the type conversions safely).

I've also, BTW, tried using DBI's basic ANSI functionality, like so:

R> sqlInterpolate(ANSI(), "SELECT * FROM foo WHERE name IN (?names)",
names = c("foo", "bar"))

... but this also doesn't work :-/

-Murat


From erich.neuwirth at univie.ac.at  Fri Dec  4 18:12:06 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 4 Dec 2015 18:12:06 +0100
Subject: [R] Can we use all the functions of R in visual studio? The
	Rmath.dll and R.dll seems only contain a part of R functions
In-Reply-To: <d4613ae.124eb.1516d006e48.Coremail.081024015@fudan.edu.cn>
References: <d4613ae.124eb.1516d006e48.Coremail.081024015@fudan.edu.cn>
Message-ID: <22BF7DCE-7B2F-4AA9-A9EF-BD3784809EE7@univie.ac.at>

There is statconnDCOM, whoch gives full R as a COM server which can be used from all he Visual languages.
It is not free. There are cost free licenses for student and home use, but essentially it is a commercial product.

Mor information can be found at
www.statconn.com <http://www.statconn.com/>



> On 04 Dec 2015, at 13:38, ?? <081024015 at fudan.edu.cn> wrote:
> 
> Can we use all the functions of R in visual studio? The Rmath.dll and R.dll seems only contain a part of R functions

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 670 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151204/c7427426/attachment.bin>

From murdoch.duncan at gmail.com  Fri Dec  4 18:24:21 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 4 Dec 2015 12:24:21 -0500
Subject: [R] system.file(...) self-referencing the containing package
In-Reply-To: <CA+YV+HyAOt87Byf8znsaJ5yO1BQUpdPeaQJOujdSrBv4VzHEBQ@mail.gmail.com>
References: <CA+YV+Hxh+emgx0uyLo1r--BiMYOArHqkGVz90K0hAfvN8Vvasg@mail.gmail.com>
	<CAF8bMca3o4tqvQjryw6OAe4Ew4a0sghgZVT=cbfA9jnk=ybLqA@mail.gmail.com>
	<CA+YV+HyAOt87Byf8znsaJ5yO1BQUpdPeaQJOujdSrBv4VzHEBQ@mail.gmail.com>
Message-ID: <5661CC45.9010704@gmail.com>

On 04/12/2015 11:28 AM, Murat Tasan wrote:
> Perfect, thanks!
>
> (Any idea if/where this is documented? I checked through the "Writing
> R Extensions" doc and couldn't find any mention of it.)

It is mentioned in ?getPackageName:

" (Currently, the name is stored as the object |.packageName| but don't 
trust this for the future.)"

The more trustworthy approach is to use packageName() or getPackageName().

Duncan Murdoch
>
> Thanks much again,
>
> -Murat
>
>
> On Thu, Dec 3, 2015 at 6:42 PM, William Dunlap <wdunlap at tibco.com> wrote:
> > Every package has in it, after it is installed, a character object
> > called ".packageName" containing its name.  It is not exported from
> > the package.  Functions in your package can refer to it as just
> > .packageName.
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >
> > On Thu, Dec 3, 2015 at 5:28 PM, Murat Tasan <mmuurr at gmail.com> wrote:
> >> In a package I'm writing, I'm placing all SQL code here:
> >> /inst/sql/
> >>
> >> And so when referring to these blocks of code from the package's R
> >> code, I do something like so:
> >>
> >> system.file("sql", "my_example_file.sql", package = "ThisPackage",
> >> mustWork = TRUE)
> >>
> >> But, referring to the package itself with the string "ThisPackage" is
> >> annoying and somewhat brittle... if the package were ever to change
> >> names (e.g. during development), I'll have to replace all such calls
> >> with the new package name.
> >> And generally referring to the package containing the code itself by
> >> name seems inelegant.
> >>
> >> Anyone know of a safe way to reference additional files in a package,
> >> from within that package's code, that doesn't require specifying the
> >> name of the package as a string?
> >>
> >> Cheers,
> >>
> >> -Murat
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From hamoen.m at gmail.com  Fri Dec  4 20:09:59 2015
From: hamoen.m at gmail.com (Marleen Hamoen)
Date: Fri, 4 Dec 2015 20:09:59 +0100
Subject: [R] Survival analysis: ERROR: Time and status are different
	lengths
In-Reply-To: <c10f8b$206cfu@ironport10.mayo.edu>
References: <mailman.3.1449226801.3956.r-help@r-project.org>
	<c10f8b$206cfu@ironport10.mayo.edu>
Message-ID: <CANVDHz3+TS9=V580iPRCW2CUaosLh9TTtVGecMSUzdS3xqmFBA@mail.gmail.com>

Hi Terry,

I already suspected it had something to do with missing values in one of
the covariates. I couldn't get the na.action="na.exclude" to work (perhaps
this is because I am relatively unexperienced with R), but I managed to
solve the problem by using the following command with "complete.cases":

DF <- with(DF,DF[complete.cases(TimeDeath, event, age, sex, mutation,
ventilation, BM1, BM2), ])

Thank you very much for your help!


2015-12-04 15:02 GMT+01:00 Therneau, Terry M., Ph.D. <therneau at mayo.edu>:

> I expect that reading the result of print(fit.weib) will answer your
> question.  If there were any missing values in the data set, then the
> fit.weib$linear.predictors will be shorter than the original data set,
> and the printout will have a note about "...deleted due to missing".
>
> The simplest solution to this is to set
>   options(na.action="na.exclude")
> before doing the fit.  Then predict(fit) and resid(fit) will return
> vectors of the same length as the input data, containing NA in the
> appropriate positions.  The default na.action of "na.omit" leaves missing
> out of both the fit and the residuals.
>
> (Unfortunately, only a few modeling functions in R pay attention to the
> difference between these two na.action options.)
>
> Terry Therneau
>
>
>
> On 12/04/2015 05:00 AM, r-help-request at r-project.org wrote:
>
>> Hi,
>>
>> I am fitting an AFT model assuming a Weibull distribution and I would like
>> to check the residuals compared to the Kaplan Meier residuals, but when I
>> try to create the Kaplan Meier residuals I get an error: Time and status
>> are different lengths.
>>
>> I am using the following script:
>>
>> # Fitting the AFT model
>> fit.weib <- survreg(Surv(TimeDeath, event) ~ age + sex + mutation +
>> ventilation + BM1 + BM2, data = DF, dist = "weibull")
>> fits <- fit.weib$linear.predictors
>> resids <- (fit.weib$y[, 1] - fits)/fit.weib$scale
>> resKM <- survfit(Surv(resids, event) ~ 1, data = DF)
>>
>> I get the error from the last line of the script.
>>
>> I tried some things that didn't work and I was wondering if anyone could
>> help me.
>> If you need more information please let me know.
>>
>> Thanks in advance,
>>
>

	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Fri Dec  4 21:53:49 2015
From: f_j_rod at hotmail.com (Frank S.)
Date: Fri, 4 Dec 2015 21:53:49 +0100
Subject: [R] Keep only first date from consecutive dates
In-Reply-To: <BAY168-W44A193BD2CF88170D96BE5BAB30@phx.gbl>
References: <CAH-FEnjfDBZF2-1HWR0_ULd71Kfrd3z+ES13ZdXaj4wGD1OVOw@mail.gmail.com>,
	,
	<CAH-FEnh+1DDFOsWNq5GF7p-iLpBOOUEaKst+NZGm1Pc-f+arvA@mail.gmail.com>,
	, <26C7B3B6-6596-418C-9CC4-C65558E54F53@dcn.davis.CA.us>,
	<000f01cff518$92e14210$b8a3c630$@mcmaster.ca>, ,
	<alpine.BSF.2.00.1410310822530.42077@pedal.dcn.davis.ca.us>,
	<BAY168-W110DBCDE35CFDCF099B3682BAB40@phx.gbl>, ,
	<CAF8bMcafucUX2C4Gsdiu=y+nFiQuz8TtSYWM31MRp8i_ii9kHw@mail.gmail.com>,
	<BAY168-W44A193BD2CF88170D96BE5BAB30@phx.gbl>
Message-ID: <BAY168-W1378704ED614CEA5A482652BA0C0@phx.gbl>

Dear R users,
 
I usually work with data.table package, but I'm sure that muy question can also be answered working with R data frame.
Working with grouped data (by "id"),  I wonder if it is possible to keep in a R data.frame (or R data.table):
a) Only the first row if there is a row which belongs to a a group of rows (from same "id") that have consecutive dates.
b) All the rows which do not belong to the above groups.
 
As an example, I have "uci" data.frame:
 
uci <- data.table(id=c(rep(1,6),2),
                date = as.Date(c("2005-10-28","2005-10-29","2005-10-30","2005-11-07","2007-03-19","2007-03-20","2004-06-02")),
                value = c(1, 2, 1, 3, 1, 2, 2))
 
   id              date   value
    1  2005-10-28        1
    1  2005-10-29        2
    1  2005-10-30        1
    1  2005-11-07        3
    1  2007-03-19        1
    1  2007-03-20        2
    2  2004-06-02        2
 
And the desired output would be:
 
   id              date   value
    1  2005-10-28        1
    1  2005-11-07        3
    1  2007-03-19        1
    2  2004-06-02        2
 
# From the following link, I have tried:
http://stackoverflow.com/questions/32308636/r-how-to-sum-values-from-rows-only-if-the-key-value-is-the-same-and-also-if-the
 
setDT(uci)[ ,list(date=date[1L], value = value[1L]),  by = .(ind=rleid(date), id)][, ind:=NULL][]
 
But I get the same data frame, and I do not know the reason.
 
Thank you very much for any help!!
 
Frank S.
 
 
 
 
 		 	   		  
	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Dec  4 22:10:14 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 4 Dec 2015 13:10:14 -0800
Subject: [R] Keep only first date from consecutive dates
In-Reply-To: <BAY168-W1378704ED614CEA5A482652BA0C0@phx.gbl>
References: <CAH-FEnjfDBZF2-1HWR0_ULd71Kfrd3z+ES13ZdXaj4wGD1OVOw@mail.gmail.com>
	<CAH-FEnh+1DDFOsWNq5GF7p-iLpBOOUEaKst+NZGm1Pc-f+arvA@mail.gmail.com>
	<26C7B3B6-6596-418C-9CC4-C65558E54F53@dcn.davis.CA.us>
	<000f01cff518$92e14210$b8a3c630$@mcmaster.ca>
	<alpine.BSF.2.00.1410310822530.42077@pedal.dcn.davis.ca.us>
	<BAY168-W110DBCDE35CFDCF099B3682BAB40@phx.gbl>
	<CAF8bMcafucUX2C4Gsdiu=y+nFiQuz8TtSYWM31MRp8i_ii9kHw@mail.gmail.com>
	<BAY168-W44A193BD2CF88170D96BE5BAB30@phx.gbl>
	<BAY168-W1378704ED614CEA5A482652BA0C0@phx.gbl>
Message-ID: <CAF8bMcZPRWPO4dOnpk2xnzrGgOauBL05BgTJopF9w_8AF3KdfQ@mail.gmail.com>

With a data.frame sorted by id, with ties broken by date, as in
your example, you can select rows that are either the start
of a new id group or the start of run of consecutive dates with:

> w <- c(TRUE, diff(uci$date)>1) | c(TRUE, diff(uci$id)!=0)
> which(w)
[1] 1 4 5 7
> uci[w,]
  id       date value
1  1 2005-10-28     1
4  1 2005-11-07     3
5  1 2007-03-19     1
7  2 2004-06-02     2

I'll leave it to you to translate that R syntax into data.table syntax -
it just involves comparing the current row with the previous row.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Fri, Dec 4, 2015 at 12:53 PM, Frank S. <f_j_rod at hotmail.com> wrote:
> Dear R users,
>
> I usually work with data.table package, but I'm sure that muy question can also be answered working with R data frame.
> Working with grouped data (by "id"),  I wonder if it is possible to keep in a R data.frame (or R data.table):
> a) Only the first row if there is a row which belongs to a a group of rows (from same "id") that have consecutive dates.
> b) All the rows which do not belong to the above groups.
>
> As an example, I have "uci" data.frame:
>
> uci <- data.table(id=c(rep(1,6),2),
>                 date = as.Date(c("2005-10-28","2005-10-29","2005-10-30","2005-11-07","2007-03-19","2007-03-20","2004-06-02")),
>                 value = c(1, 2, 1, 3, 1, 2, 2))
>
>    id              date   value
>     1  2005-10-28        1
>     1  2005-10-29        2
>     1  2005-10-30        1
>     1  2005-11-07        3
>     1  2007-03-19        1
>     1  2007-03-20        2
>     2  2004-06-02        2
>
> And the desired output would be:
>
>    id              date   value
>     1  2005-10-28        1
>     1  2005-11-07        3
>     1  2007-03-19        1
>     2  2004-06-02        2
>
> # From the following link, I have tried:
> http://stackoverflow.com/questions/32308636/r-how-to-sum-values-from-rows-only-if-the-key-value-is-the-same-and-also-if-the
>
> setDT(uci)[ ,list(date=date[1L], value = value[1L]),  by = .(ind=rleid(date), id)][, ind:=NULL][]
>
> But I get the same data frame, and I do not know the reason.
>
> Thank you very much for any help!!
>
> Frank S.
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Sat Dec  5 01:34:38 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 4 Dec 2015 16:34:38 -0800
Subject: [R] Keep only first date from consecutive dates
In-Reply-To: <CAF8bMcZPRWPO4dOnpk2xnzrGgOauBL05BgTJopF9w_8AF3KdfQ@mail.gmail.com>
References: <CAH-FEnjfDBZF2-1HWR0_ULd71Kfrd3z+ES13ZdXaj4wGD1OVOw@mail.gmail.com>
	<CAH-FEnh+1DDFOsWNq5GF7p-iLpBOOUEaKst+NZGm1Pc-f+arvA@mail.gmail.com>
	<26C7B3B6-6596-418C-9CC4-C65558E54F53@dcn.davis.CA.us>
	<000f01cff518$92e14210$b8a3c630$@mcmaster.ca>
	<alpine.BSF.2.00.1410310822530.42077@pedal.dcn.davis.ca.us>
	<BAY168-W110DBCDE35CFDCF099B3682BAB40@phx.gbl>
	<CAF8bMcafucUX2C4Gsdiu=y+nFiQuz8TtSYWM31MRp8i_ii9kHw@mail.gmail.com>
	<BAY168-W44A193BD2CF88170D96BE5BAB30@phx.gbl>
	<BAY168-W1378704ED614CEA5A482652BA0C0@phx.gbl>
	<CAF8bMcZPRWPO4dOnpk2xnzrGgOauBL05BgTJopF9w_8AF3KdfQ@mail.gmail.com>
Message-ID: <44A993D0-DCED-44CE-A1B3-F1F34D9EE56A@comcast.net>


> On Dec 4, 2015, at 1:10 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
> With a data.frame sorted by id, with ties broken by date, as in
> your example, you can select rows that are either the start
> of a new id group or the start of run of consecutive dates with:
> 
>> w <- c(TRUE, diff(uci$date)>1) | c(TRUE, diff(uci$id)!=0)
>> which(w)
> [1] 1 4 5 7
>> uci[w,]
>  id       date value
> 1  1 2005-10-28     1
> 4  1 2005-11-07     3
> 5  1 2007-03-19     1
> 7  2 2004-06-02     2
> 
> I'll leave it to you to translate that R syntax into data.table syntax -
> it just involves comparing the current row with the previous row.
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> 
> On Fri, Dec 4, 2015 at 12:53 PM, Frank S. <f_j_rod at hotmail.com> wrote:
>> Dear R users,
>> 
>> I usually work with data.table package, but I'm sure that muy question can also be answered working with R data frame.
>> Working with grouped data (by "id"),  I wonder if it is possible to keep in a R data.frame (or R data.table):
>> a) Only the first row if there is a row which belongs to a a group of rows (from same "id") that have consecutive dates.
>> b) All the rows which do not belong to the above groups.
>> 
>> As an example, I have "uci" data.frame:
>> 
>> uci <- data.table(id=c(rep(1,6),2),
>>                date = as.Date(c("2005-10-28","2005-10-29","2005-10-30","2005-11-07","2007-03-19","2007-03-20","2004-06-02")),
>>                value = c(1, 2, 1, 3, 1, 2, 2))
>> 
>>   id              date   value
>>    1  2005-10-28        1
>>    1  2005-10-29        2
>>    1  2005-10-30        1
>>    1  2005-11-07        3
>>    1  2007-03-19        1
>>    1  2007-03-20        2
>>    2  2004-06-02        2
>> 
>> And the desired output would be:
>> 
>>   id              date   value
>>    1  2005-10-28        1
>>    1  2005-11-07        3
>>    1  2007-03-19        1
>>    2  2004-06-02        2

The syntax of `[.data.table` is a bit odd; You can refer to columns by name; I never trust my intuition, though.

Selection is usually done with a logical vector in the ?i?-position. The diff operator does succeed in the ?i? position with the obvious need to prepend with a starting value..

> uci[ c(0,diff(date))!=1, ]
   id       date value
1:  1 2005-10-28     1
2:  1 2005-11-07     3
3:  1 2007-03-19     1
4:  2 2004-06-02     2

The other cases are handle with the converse-expression

> uci[c(0,diff(date)) == 1, ]
   id       date value
1:  1 2005-10-29     2
2:  1 2005-10-30     1
3:  1 2007-03-20     2


>> 
>> # From the following link, I have tried:
>> http://stackoverflow.com/questions/32308636/r-how-to-sum-values-from-rows-only-if-the-key-value-is-the-same-and-also-if-the
>> 
>> setDT(uci)[ ,list(date=date[1L], value = value[1L]),  by = .(ind=rleid(date), id)][, ind:=NULL][]
>> 
>> But I get the same data frame, and I do not know the reason.
>> 
>> Thank you very much for any help!!
>> 
>> Frank S.
>> 
>> 
>> 
>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From patrick-breheny at uiowa.edu  Fri Dec  4 21:58:40 2015
From: patrick-breheny at uiowa.edu (Patrick Breheny)
Date: Fri, 4 Dec 2015 14:58:40 -0600
Subject: [R] John M. Chambers Statistical Software Award
Message-ID: <5661FE80.4060605@uiowa.edu>

John M. Chambers Statistical Software Award 2016

This is a second announcement for the Chambers Award and reminder that 
all application materials must be submitted by December 15. The John M. 
Chambers Statistical Software Award is an annual award given by the 
Statistical Computing Section of the American Statistical Association to 
recognize student contributions to the development of statistical software.

Dr. John Chambers, developer of the S statistical computing language, 
generously donated to the Statistical Computing Section to endow an 
annual prize for statistical software written by, or in collaboration 
with, an undergraduate or graduate student. The prize carries with it a 
cash award of $1,000.

Teams of up to 3 people can participate in the competition. To be 
eligible, the team must have designed and implemented a piece of 
statistical software. At least one individual within the team must have 
begun the development while a student and must either currently be a 
student, or have completed all requirements for her/his last degree 
after January 1, 2015. The award will be given to the student, or split 
between student team members if the team consists of multiple students.

To apply for the award, teams must provide the following materials:

* Current CV's of all team members.

* A letter from a faculty mentor at the academic institution of the 
individual indicated to receive the travel award. The letter should 
confirm that the individual had substantial participation in the 
development of the software, certify her/his student status when the 
software began to be developed (and either the current student status or 
the date of degree completion), and briefly discuss the importance of 
the software to statistical practice.

* A brief, one to two page description of the software, summarizing what 
it does, how it does it, and why it is an important contribution. If any 
student team member has continued developing the software after 
finishing her/his studies, the description should indicate what was 
developed when the individual was a student and what has been added since.

* An installable software package with its source code for use by the 
award committee. It should be accompanied by enough information to allow 
the judges to effectively use and evaluate the software (including its 
design considerations). This information can be provided in a variety of 
ways, including but not limited to a user manual, a manuscript, a URL, 
and online help to the system.

All materials must be in English. We prefer that electronic text be 
submitted in Postscript or PDF. The entries will be judged on a variety 
of dimensions, including the importance and relevance for statistical 
practice of the tasks performed by the software, ease of use, clarity of 
description, elegance and availability for use by the statistical 
community. Preference will be given to those entries that are grounded 
in software design rather than calculation. The decision of the award 
committee is final.

All application materials must be received by Tuesday, December 15, 2015 
and should be sent to the e-mail address below.  The winner will be 
announced by January 15.  The award will be presented at the 2016 Joint 
Statistical Meetings, and the winner(s) will be given an opportunity to 
present their work in a topic-contributed session at the meetings.

Patrick Breheny
Department of Biostatistics
University of Iowa
patrick-breheny at uiowa.edu

-- 
Patrick Breheny
Assistant Professor
Department of Biostatistics
University of Iowa
N336 College of Public Health Building
319-384-1584


From klebyn at yahoo.com.br  Sat Dec  5 12:41:56 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sat, 5 Dec 2015 09:41:56 -0200
Subject: [R] Tk table: how to get information about configurations ?
Message-ID: <5662CD84.8000005@yahoo.com.br>

Hello
I would like to know the correct way of getting information about the 
values of the parameters of a Tktable.
some details I even managed to extract but quite archaic form ...
I thank you for advance for any help.

cleber
###################
# example

library( tcltk ) ; tclRequire( 'Tktable' )

tt <- tktoplevel() ; fm <- ttkframe( tt )

tkpack( fm, expand=T, fill='both')

# set info (geometry) of toplevel
tkwm.geometry( tt, "600x400+400+200" )

# get info (geometry) about toplevel
tcl( 'wm', 'geometry', wDataOrg )

table <- tkwidget( fm, 'table', background='white', selectmode="extended" )

tkpack( table, expand=T, fill='both' )

# how get informations about this table?????????????
tkconfigure( table )  #  ?????????

tcl( table, 'xxxxx' ) # ???????????

tcl( table, 'curselection' )




---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From phgrosjean at sciviews.org  Sat Dec  5 14:40:10 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Sat, 5 Dec 2015 14:40:10 +0100
Subject: [R] Tk table: how to get information about configurations ?
In-Reply-To: <5662CD84.8000005@yahoo.com.br>
References: <5662CD84.8000005@yahoo.com.br>
Message-ID: <659BFC35-30BE-40BE-B36A-A2A1F038F1C9@sciviews.org>


It depends what property you are looking for? Have you tried tkcget(table, ?)?

Philippe Grosjean
> On 05 Dec 2015, at 12:41, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> Hello
> I would like to know the correct way of getting information about the values of the parameters of a Tktable.
> some details I even managed to extract but quite archaic form ...
> I thank you for advance for any help.
> 
> cleber
> ###################
> # example
> 
> library( tcltk ) ; tclRequire( 'Tktable' )
> 
> tt <- tktoplevel() ; fm <- ttkframe( tt )
> 
> tkpack( fm, expand=T, fill='both')
> 
> # set info (geometry) of toplevel
> tkwm.geometry( tt, "600x400+400+200" )
> 
> # get info (geometry) about toplevel
> tcl( 'wm', 'geometry', wDataOrg )
> 
> table <- tkwidget( fm, 'table', background='white', selectmode="extended" )
> 
> tkpack( table, expand=T, fill='both' )
> 
> # how get informations about this table?????????????
> tkconfigure( table )  #  ?????????
> 
> tcl( table, 'xxxxx' ) # ???????????
> 
> tcl( table, 'curselection' )
> 
> 
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From klebyn at yahoo.com.br  Sat Dec  5 15:24:28 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sat, 5 Dec 2015 12:24:28 -0200
Subject: [R] Tk table: how to get information about configurations ?
In-Reply-To: <659BFC35-30BE-40BE-B36A-A2A1F038F1C9@sciviews.org>
References: <5662CD84.8000005@yahoo.com.br>
	<659BFC35-30BE-40BE-B36A-A2A1F038F1C9@sciviews.org>
Message-ID: <5662F39C.9060402@yahoo.com.br>

I understood that the configuration is done by the "tk configure"
but I can not get the value of the properties back.
like this:

tkconfigure (table, background = 'red')
tkcget (table, 'background') # give me an error

thanks for you help
cleber
##############################

 > tkconfigure (table, background = 'red', titlecol=1, multiline=0 )
<Tcl>
 >
 > tkcget (table, 'background') # give me an error
Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
   [tcl] invalid command name ".1.1.1".
 > tkcget (table, 'multiline') # give me an error
Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
   [tcl] invalid command name ".1.1.1".
 > tkcget (table, 'titlecol') # give me an error
Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
   [tcl] invalid command name ".1.1.1".
 >




Em 05/12/2015 11:40, phgrosjean at sciviews.org escreveu:
> It depends what property you are looking for? Have you tried tkcget(table, ?)?
>
> Philippe Grosjean

>>
>> Hello
>> I would like to know the correct way of getting information about the values of the parameters of a Tktable.
>> some details I even managed to extract but quite archaic form ...
>> I thank you for advance for any help.
>>
>> cleber
>> ###################
>> # example
>>
>> library( tcltk ) ; tclRequire( 'Tktable' )
>>
>> tt <- tktoplevel() ; fm <- ttkframe( tt )
>>
>> tkpack( fm, expand=T, fill='both')
>>
>> # set info (geometry) of toplevel
>> tkwm.geometry( tt, "600x400+400+200" )
>>
>> # get info (geometry) about toplevel
>> tcl( 'wm', 'geometry', wDataOrg )
>>
>> table <- tkwidget( fm, 'table', background='white', selectmode="extended" )
>>
>> tkpack( table, expand=T, fill='both' )
>>
>> # how get informations about this table?????????????
>> tkconfigure( table )  #  ?????????
>>
>> tcl( table, 'xxxxx' ) # ???????????
>>
>> tcl( table, 'curselection' )
>>
>>
>>
>>
>> ---
>> Este email foi escaneado pelo Avast antiv?rus.
>> https://www.avast.com/antivirus
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From pdalgd at gmail.com  Sat Dec  5 16:28:02 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 5 Dec 2015 16:28:02 +0100
Subject: [R] Tk table: how to get information about configurations ?
In-Reply-To: <5662F39C.9060402@yahoo.com.br>
References: <5662CD84.8000005@yahoo.com.br>
	<659BFC35-30BE-40BE-B36A-A2A1F038F1C9@sciviews.org>
	<5662F39C.9060402@yahoo.com.br>
Message-ID: <FD07BDBC-B1AF-4D9C-942F-9B78E4889BD8@gmail.com>


> On 05 Dec 2015, at 15:24 , Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> I understood that the configuration is done by the "tk configure"
> but I can not get the value of the properties back.
> like this:
> 
> tkconfigure (table, background = 'red')
> tkcget (table, 'background') # give me an error

This corresponds to an argumentless option in Tcl, i.e.

.foo.tb cget -background

(compare .foo.tb configure -background red)

In the R/Tcl interface, this is accomplished done by passing NULL:

tkcget(table, background=NULL)
 

-pd

> 
> thanks for you help
> cleber
> ##############################
> 
> > tkconfigure (table, background = 'red', titlecol=1, multiline=0 )
> <Tcl>
> >
> > tkcget (table, 'background') # give me an error
> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>  [tcl] invalid command name ".1.1.1".
> > tkcget (table, 'multiline') # give me an error
> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>  [tcl] invalid command name ".1.1.1".
> > tkcget (table, 'titlecol') # give me an error
> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>  [tcl] invalid command name ".1.1.1".
> >
> 
> 
> 
> 
> Em 05/12/2015 11:40, phgrosjean at sciviews.org escreveu:
>> It depends what property you are looking for? Have you tried tkcget(table, ?)?
>> 
>> Philippe Grosjean
> 
>>> 
>>> Hello
>>> I would like to know the correct way of getting information about the values of the parameters of a Tktable.
>>> some details I even managed to extract but quite archaic form ...
>>> I thank you for advance for any help.
>>> 
>>> cleber
>>> ###################
>>> # example
>>> 
>>> library( tcltk ) ; tclRequire( 'Tktable' )
>>> 
>>> tt <- tktoplevel() ; fm <- ttkframe( tt )
>>> 
>>> tkpack( fm, expand=T, fill='both')
>>> 
>>> # set info (geometry) of toplevel
>>> tkwm.geometry( tt, "600x400+400+200" )
>>> 
>>> # get info (geometry) about toplevel
>>> tcl( 'wm', 'geometry', wDataOrg )
>>> 
>>> table <- tkwidget( fm, 'table', background='white', selectmode="extended" )
>>> 
>>> tkpack( table, expand=T, fill='both' )
>>> 
>>> # how get informations about this table?????????????
>>> tkconfigure( table )  #  ?????????
>>> 
>>> tcl( table, 'xxxxx' ) # ???????????
>>> 
>>> tcl( table, 'curselection' )
>>> 
>>> 
>>> 
>>> 
>>> ---
>>> Este email foi escaneado pelo Avast antiv?rus.
>>> https://www.avast.com/antivirus
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From klebyn at yahoo.com.br  Sat Dec  5 20:15:54 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sat, 5 Dec 2015 17:15:54 -0200
Subject: [R] tcltk TK table : Is it possible to block the cell? [Question in
 2003, but TK changes the interfaces functions]
Message-ID: <566337EA.7030007@yahoo.com.br>

in 2003 [1] someone asked for:
"Is it possible to" block "the cell?"
but the solution involves the "tkcmd" that no longer exists ...

tkcmd (.Tk.ID (table1), "tag", "celltag" "ZeroOne", "0.1")

I tried to adapt to:

tktag.add (table1, "celltag" "ZeroOne", "0.1")

but I get error. How to adapt it?

Where it could have informations about the tcltk package as it does not 
have the detailed help?

thanks in advanced for help
cleber

[1] - http://grokbase.com/p/r/r-help/037xs650d8/r-tktable-disable-cell


############
 > tclRequire("Tktable")
<Tcl> 2.9
 > tt <- tktoplevel()
 > table1 <- tkwidget(tt,"table",bg="white")
 > tkpack(table1)
<Tcl>
 > tkcmd(.Tk.ID(table1),"tag","celltag","ZeroOne","0,1")
Error: could not find function "tkcmd"
 >
 > tktag.add( table1,"tag","celltag","ZeroOne","0,1")
Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
   [tcl] bad tag option "add": must be celltag, cget, coltag, configure, 
delete, exists, includes, lower, names, raise, or rowtag.
 >
 >
 > tkcmd(.Tk.ID(table1),"tag","celltag","ZeroTwo","0,2")
Error: could not find function "tkcmd"
 > 
tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",state="disabled",bg="gray")
Error: could not find function "tkcmd"
 > 
tkcmd(.Tk.ID(table1),"tag","configure","ZeroTwo",state="normal",bg="white")
Error: could not find function "tkcmd"
 >
 > .Tcl("set tclarray(0,0) Normal")
<Tcl> Normal
 > .Tcl("set tclarray(0,1) Disabled")
<Tcl> Disabled
 > .Tcl("set tclarray(0,2) Normal")
<Tcl> Normal
 > tkconfigure(table1,variable="tclarray")
<Tcl>


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From pdalgd at gmail.com  Sat Dec  5 21:57:34 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 5 Dec 2015 21:57:34 +0100
Subject: [R] tcltk TK table : Is it possible to block the cell?
	[Question in 2003, but TK changes the interfaces functions]
In-Reply-To: <566337EA.7030007@yahoo.com.br>
References: <566337EA.7030007@yahoo.com.br>
Message-ID: <5FCDDA24-0986-493E-9852-7AB0F2116BBC@gmail.com>


> On 05 Dec 2015, at 20:15 , Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> in 2003 [1] someone asked for:
> "Is it possible to" block "the cell?"
> but the solution involves the "tkcmd" that no longer exists ...
> 
> tkcmd (.Tk.ID (table1), "tag", "celltag" "ZeroOne", "0.1")

tcl() should be a drop-in replacement. The .Tk.ID() business should no longer be necessary, though. tcl(table1, "tag", "celltag",....)


> 
> I tried to adapt to:
> 
> tktag.add (table1, "celltag" "ZeroOne", "0.1")
> 
> but I get error. How to adapt it?

Stop guessing and start thinking? 

That would be equivalent to

tcl(table1, "tag", "add", "celltag" "ZeroOne", "0.1")

and as the error message *clearly* says, there is no tag option "add"!

-pd

> 
> Where it could have informations about the tcltk package as it does not have the detailed help?
> 
> thanks in advanced for help
> cleber
> 
> [1] - http://grokbase.com/p/r/r-help/037xs650d8/r-tktable-disable-cell
> 
> 
> ############
> > tclRequire("Tktable")
> <Tcl> 2.9
> > tt <- tktoplevel()
> > table1 <- tkwidget(tt,"table",bg="white")
> > tkpack(table1)
> <Tcl>
> > tkcmd(.Tk.ID(table1),"tag","celltag","ZeroOne","0,1")
> Error: could not find function "tkcmd"
> >
> > tktag.add( table1,"tag","celltag","ZeroOne","0,1")
> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>  [tcl] bad tag option "add": must be celltag, cget, coltag, configure, delete, exists, includes, lower, names, raise, or rowtag.
> >
> >
> > tkcmd(.Tk.ID(table1),"tag","celltag","ZeroTwo","0,2")
> Error: could not find function "tkcmd"
> > tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",state="disabled",bg="gray")
> Error: could not find function "tkcmd"
> > tkcmd(.Tk.ID(table1),"tag","configure","ZeroTwo",state="normal",bg="white")
> Error: could not find function "tkcmd"
> >
> > .Tcl("set tclarray(0,0) Normal")
> <Tcl> Normal
> > .Tcl("set tclarray(0,1) Disabled")
> <Tcl> Disabled
> > .Tcl("set tclarray(0,2) Normal")
> <Tcl> Normal
> > tkconfigure(table1,variable="tclarray")
> <Tcl>
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From glennmschultz at me.com  Sun Dec  6 01:49:40 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Sun, 06 Dec 2015 00:49:40 +0000 (GMT)
Subject: [R] help using SED
Message-ID: <bcfe31ef-9b2c-4f69-bacd-706a82c01a41@me.com>

All,

I have not used SED in the past so I am continuing to read its documentation but I need some help with R

Here is my SED command which works:
? system("sed -i '' ?'/^[\r#]/d; /AGENCY/d' /Library/Frameworks/R.framework/Versions/3.2/Resources/library/BondLab/Temp_CashFlow/blx_test.cfm")

Here are my R commands which do not work. ?
ReadCF <- paste("system(sed -i '' '/^[\r#]/d; /AGENCY/d' ",?
system.file(package = "BondLab"),"/Temp_CashFlow/",?
CashFlowData, ".cfm)", sep ="")

eval(parse(text = ReadCF))
I have managed to determine that the problem is the missing quotes but I do not know how to get those included in the below R command.

Any help is appreciated

Glenn




From dwinsemius at comcast.net  Sun Dec  6 04:53:38 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 5 Dec 2015 19:53:38 -0800
Subject: [R] help using SED
In-Reply-To: <bcfe31ef-9b2c-4f69-bacd-706a82c01a41@me.com>
References: <bcfe31ef-9b2c-4f69-bacd-706a82c01a41@me.com>
Message-ID: <7D68580E-DF51-41FB-B617-CA1D90876D09@comcast.net>


> On Dec 5, 2015, at 4:49 PM, Glenn Schultz <glennmschultz at me.com> wrote:
> 
> All,
> 
> I have not used SED in the past so I am continuing to read its documentation but I need some help with R
> 
> Here is my SED command which works:
>   system("sed -i ''  '/^[\r#]/d; /AGENCY/d' /Library/Frameworks/R.framework/Versions/3.2/Resources/library/BondLab/Temp_CashFlow/blx_test.cfm")
> 
> Here are my R commands which do not work.  
> ReadCF <- paste("system(sed -i '' '/^[\r#]/d; /AGENCY/d' ", 
> system.file(package = "BondLab"),"/Temp_CashFlow/", 
> CashFlowData, ".cfm)", sep ="")
> 
> eval(parse(text = ReadCF))
> I have managed to determine that the problem is the missing quotes but I do not know how to get those included in the below R command.

I see no explanation of what you are attempting, but the method of including an actual quote in an R character vector element is to escape it:

> cat("\"")
"
> nchar("\"")
[1] 1


? 


David Winsemius
Alameda, CA, USA


From pdalgd at gmail.com  Sun Dec  6 10:28:26 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 6 Dec 2015 10:28:26 +0100
Subject: [R] help using SED
In-Reply-To: <7D68580E-DF51-41FB-B617-CA1D90876D09@comcast.net>
References: <bcfe31ef-9b2c-4f69-bacd-706a82c01a41@me.com>
	<7D68580E-DF51-41FB-B617-CA1D90876D09@comcast.net>
Message-ID: <E1385924-48B6-4B0A-9FFD-91E58AA1E2F8@gmail.com>


> On 06 Dec 2015, at 04:53 , David Winsemius <dwinsemius at comcast.net> wrote:
> 
>> 
>> On Dec 5, 2015, at 4:49 PM, Glenn Schultz <glennmschultz at me.com> wrote:
>> 
>> All,
>> 
>> I have not used SED in the past so I am continuing to read its documentation but I need some help with R
>> 
>> Here is my SED command which works:
>>  system("sed -i ''  '/^[\r#]/d; /AGENCY/d' /Library/Frameworks/R.framework/Versions/3.2/Resources/library/BondLab/Temp_CashFlow/blx_test.cfm")
>> 
>> Here are my R commands which do not work.  
>> ReadCF <- paste("system(sed -i '' '/^[\r#]/d; /AGENCY/d' ", 
>> system.file(package = "BondLab"),"/Temp_CashFlow/", 
>> CashFlowData, ".cfm)", sep ="")
>> 
>> eval(parse(text = ReadCF))
>> I have managed to determine that the problem is the missing quotes but I do not know how to get those included in the below R command.
> 
> I see no explanation of what you are attempting, but the method of including an actual quote in an R character vector element is to escape it:
> 
>> cat("\"")
> "
>> nchar("\"")
> [1] 1
> 

At any rate, eval(parse(... is just silly (fortune(181); fortune(106)). 

Just use paste()  or --- probably better ---sprintf() to construct the command, and then do

cmd <- ....
system(cmd)

A further advantage is that you can cat(cmd, \n)  before running it and see whether it contains what you intended.

I think (but you check) that this works:

fmt <- "sed -i ''  '/^[\r#]/d; /AGENCY/d' %s/Temp_CashFlow/%s.cfm"
cmd <- sprintf(fmt, system.file(package = "BondLab"), CashFlowData)
system(cmd)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ragia11 at hotmail.com  Sun Dec  6 10:55:44 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Sun, 6 Dec 2015 11:55:44 +0200
Subject: [R] splitting data frame into fixed rows depending on column rep
Message-ID: <DUB125-W891C37C432ECA7D48EF72EB30A0@phx.gbl>

Dear group,

I have the following data frame

? ? ? ? ? ? v1 v2 ?v3 v4 v5
1 ? ? ? ? ? 1 ?3 1 ? 3.5 ? ?1
2 ? ? ? ? ? 1 ?4 3 ? 3.5 ? ?1
3 ? ? ? ? ? 1 ?8 3 ? 3.5 ? ?1
4 ? ? ? ? ? 2 ?9 8 ? 2.5 ? ?1
5 ? ? ? ? ? 2 10 9 ? 2.5 ? ?1
6 ? ? ? ? ? 2 ?6 3 ? 1.5 ? ?1
7 ? ? ? ? ? 3 ?4 3 ? 2.0 ? ?1
8 ? ? ? ? ? 3 ?5 3 ? 1.0 ? ?1
9 ? ? ? ? ? 3 ?8 3 ? 1.0 ? ?1
10 ? ? ? ? ?4 ?9 8 ? 2.0 ? ?1
11 ? ? ? ? ?4 ?6 5 ? 1.0 ? ?1
12 ? ? ? ? ?4 ?8 4 ? 1.0 ? ?1
13 ? ? ? ? ?5 ?4 3 ? 2.0 ? ?1
14 ? ? ? ? ?5 ?5 3 ? 2.0 ? ?1
15 ? ? ? ? ?5 ?6 5 ? 2.0 ? ?1

I'm trying to split it into data frames where each equal v1 so all v1==1 in data frame and all v1==2 in separate data frame and so on.
so it would be

? ? ? ? v1 v2 ?v3 v4 v5
1 ? ? ? ? ? 1 ?3 1 ? 3.5 ? ?1
2 ? ? ? ? ? 1 ?4 3 ? 3.5 ? ?1
3 ? ? ? ? ? 1 ?8 3 ? 3.5 ? ?1
then
? ? ? ? ? ? v1 v2 ?v3 v4 v5
4 ? ? ? ? ? 2 ?9 8 ? 2.5 ? ?1
5 ? ? ? ? ? 2 10 9 ? 2.5 ? ?1
6 ? ? ? ? ? 2 ?6 3 ? 1.5 ? ?1
.......
...
and so on

I tried?
a<- split(df, df$v1==1)?
but the resulting class is a list and when I try to access for example a$v2 it gave me NULL ??

how can I split it an get each vector of v2 for each group,?
lots of thanks
Ragia 		 	   		  

From drjimlemon at gmail.com  Sun Dec  6 11:10:15 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sun, 6 Dec 2015 21:10:15 +1100
Subject: [R] splitting data frame into fixed rows depending on column rep
In-Reply-To: <DUB125-W891C37C432ECA7D48EF72EB30A0@phx.gbl>
References: <DUB125-W891C37C432ECA7D48EF72EB30A0@phx.gbl>
Message-ID: <CA+8X3fVKnUVvjq712qW3KxVbQYZieMPVNY7wey6HWKpM__E2jQ@mail.gmail.com>

Hi Ragia,
Perhaps the easiest way is to split the data frame into a list by the
values of v1:

sdf<-split(df,df$v1)

Then rename the elements of sdf for convenience:

names(sdf)<-paste("v1",1:5,sep="_")

Now you can extract whatever you like"

sdf$v1_1$v2
[1] 3 4 8

Of course if you only want the values of v2:

split(df$v2,df$v1)
$`1`
[1] 3 4 8

$`2`
[1]  9 10  6

$`3`
[1] 4 5 8

$`4`
[1] 9 6 8

$`5`
[1] 4 5 6

Jim


On Sun, Dec 6, 2015 at 8:55 PM, Ragia Ibrahim <ragia11 at hotmail.com> wrote:

> Dear group,
>
> I have the following data frame
>
>             v1 v2  v3 v4 v5
> 1           1  3 1   3.5    1
> 2           1  4 3   3.5    1
> 3           1  8 3   3.5    1
> 4           2  9 8   2.5    1
> 5           2 10 9   2.5    1
> 6           2  6 3   1.5    1
> 7           3  4 3   2.0    1
> 8           3  5 3   1.0    1
> 9           3  8 3   1.0    1
> 10          4  9 8   2.0    1
> 11          4  6 5   1.0    1
> 12          4  8 4   1.0    1
> 13          5  4 3   2.0    1
> 14          5  5 3   2.0    1
> 15          5  6 5   2.0    1
>
> I'm trying to split it into data frames where each equal v1 so all v1==1
> in data frame and all v1==2 in separate data frame and so on.
> so it would be
>
>         v1 v2  v3 v4 v5
> 1           1  3 1   3.5    1
> 2           1  4 3   3.5    1
> 3           1  8 3   3.5    1
> then
>             v1 v2  v3 v4 v5
> 4           2  9 8   2.5    1
> 5           2 10 9   2.5    1
> 6           2  6 3   1.5    1
> .......
> ...
> and so on
>
> I tried
> a<- split(df, df$v1==1)
> but the resulting class is a list and when I try to access for example
> a$v2 it gave me NULL ??
>
> how can I split it an get each vector of v2 for each group,
> lots of thanks
> Ragia
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From ragia11 at hotmail.com  Sun Dec  6 11:41:16 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Sun, 6 Dec 2015 12:41:16 +0200
Subject: [R] splitting data frame into fixed rows depending on column rep
In-Reply-To: <CA+8X3fVKnUVvjq712qW3KxVbQYZieMPVNY7wey6HWKpM__E2jQ@mail.gmail.com>
References: <DUB125-W891C37C432ECA7D48EF72EB30A0@phx.gbl>,
	<CA+8X3fVKnUVvjq712qW3KxVbQYZieMPVNY7wey6HWKpM__E2jQ@mail.gmail.com>
Message-ID: <DUB125-W6919358203D2A7781C8BCDB30A0@phx.gbl>


LOTS OF THANKS

________________________________
> Date: Sun, 6 Dec 2015 21:10:15 +1100 
> Subject: Re: [R] splitting data frame into fixed rows depending on column rep 
> From: drjimlemon at gmail.com 
> To: ragia11 at hotmail.com 
> CC: r-help at r-project.org 
> 
> Hi Ragia, 
> Perhaps the easiest way is to split the data frame into a list by the 
> values of v1: 
> 
> sdf<-split(df,df$v1) 
> 
> Then rename the elements of sdf for convenience: 
> 
> names(sdf)<-paste("v1",1:5,sep="_") 
> 
> Now you can extract whatever you like" 
> 
> sdf$v1_1$v2 
> [1] 3 4 8 
> 
> Of course if you only want the values of v2: 
> 
> split(df$v2,df$v1) 
> $`1` 
> [1] 3 4 8 
> 
> $`2` 
> [1] 9 10 6 
> 
> $`3` 
> [1] 4 5 8 
> 
> $`4` 
> [1] 9 6 8 
> 
> $`5` 
> [1] 4 5 6 
> 
> Jim 
> 
> 
> On Sun, Dec 6, 2015 at 8:55 PM, Ragia Ibrahim 
> <ragia11 at hotmail.com<mailto:ragia11 at hotmail.com>> wrote: 
> Dear group, 
> 
> I have the following data frame 
> 
> v1 v2 v3 v4 v5 
> 1 1 3 1 3.5 1 
> 2 1 4 3 3.5 1 
> 3 1 8 3 3.5 1 
> 4 2 9 8 2.5 1 
> 5 2 10 9 2.5 1 
> 6 2 6 3 1.5 1 
> 7 3 4 3 2.0 1 
> 8 3 5 3 1.0 1 
> 9 3 8 3 1.0 1 
> 10 4 9 8 2.0 1 
> 11 4 6 5 1.0 1 
> 12 4 8 4 1.0 1 
> 13 5 4 3 2.0 1 
> 14 5 5 3 2.0 1 
> 15 5 6 5 2.0 1 
> 
> I'm trying to split it into data frames where each equal v1 so all 
> v1==1 in data frame and all v1==2 in separate data frame and so on. 
> so it would be 
> 
> v1 v2 v3 v4 v5 
> 1 1 3 1 3.5 1 
> 2 1 4 3 3.5 1 
> 3 1 8 3 3.5 1 
> then 
> v1 v2 v3 v4 v5 
> 4 2 9 8 2.5 1 
> 5 2 10 9 2.5 1 
> 6 2 6 3 1.5 1 
> ....... 
> ... 
> and so on 
> 
> I tried 
> a<- split(df, df$v1==1) 
> but the resulting class is a list and when I try to access for example 
> a$v2 it gave me NULL ?? 
> 
> how can I split it an get each vector of v2 for each group, 
> lots of thanks 
> Ragia 
> ______________________________________________ 
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
> UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html 
> and provide commented, minimal, self-contained, reproducible code. 
> 
 		 	   		  

From klebyn at yahoo.com.br  Sun Dec  6 11:57:59 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Sun, 6 Dec 2015 08:57:59 -0200
Subject: [R] tcltk TK table : Is it possible to block the cell?
 [Question in 2003, but TK changes the interfaces functions]
In-Reply-To: <5FCDDA24-0986-493E-9852-7AB0F2116BBC@gmail.com>
References: <566337EA.7030007@yahoo.com.br>
	<5FCDDA24-0986-493E-9852-7AB0F2116BBC@gmail.com>
Message-ID: <566414B7.7090202@yahoo.com.br>

thanks by the help
but I do not understand how to set a tag in cell table.

cleber


Em 05/12/2015 18:57, peter dalgaard escreveu:
>> On 05 Dec 2015, at 20:15 , Cleber N.Borges <klebyn at yahoo.com.br> wrote:
>>
>> in 2003 [1] someone asked for:
>> "Is it possible to" block "the cell?"
>> but the solution involves the "tkcmd" that no longer exists ...
>>
>> tkcmd (.Tk.ID (table1), "tag", "celltag" "ZeroOne", "0.1")
> tcl() should be a drop-in replacement. The .Tk.ID() business should no longer be necessary, though. tcl(table1, "tag", "celltag",....)
>
>
>> I tried to adapt to:
>>
>> tktag.add (table1, "celltag" "ZeroOne", "0.1")
>>
>> but I get error. How to adapt it?
> Stop guessing and start thinking?
>
> That would be equivalent to
>
> tcl(table1, "tag", "add", "celltag" "ZeroOne", "0.1")
>
> and as the error message *clearly* says, there is no tag option "add"!
>
> -pd
>
>> Where it could have informations about the tcltk package as it does not have the detailed help?
>>
>> thanks in advanced for help
>> cleber
>>
>> [1] - http://grokbase.com/p/r/r-help/037xs650d8/r-tktable-disable-cell
>>
>>
>> ############
>>> tclRequire("Tktable")
>> <Tcl> 2.9
>>> tt <- tktoplevel()
>>> table1 <- tkwidget(tt,"table",bg="white")
>>> tkpack(table1)
>> <Tcl>
>>> tkcmd(.Tk.ID(table1),"tag","celltag","ZeroOne","0,1")
>> Error: could not find function "tkcmd"
>>> tktag.add( table1,"tag","celltag","ZeroOne","0,1")
>> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>>   [tcl] bad tag option "add": must be celltag, cget, coltag, configure, delete, exists, includes, lower, names, raise, or rowtag.
>>>
>>> tkcmd(.Tk.ID(table1),"tag","celltag","ZeroTwo","0,2")
>> Error: could not find function "tkcmd"
>>> tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",state="disabled",bg="gray")
>> Error: could not find function "tkcmd"
>>> tkcmd(.Tk.ID(table1),"tag","configure","ZeroTwo",state="normal",bg="white")
>> Error: could not find function "tkcmd"
>>> .Tcl("set tclarray(0,0) Normal")
>> <Tcl> Normal
>>> .Tcl("set tclarray(0,1) Disabled")
>> <Tcl> Disabled
>>> .Tcl("set tclarray(0,2) Normal")
>> <Tcl> Normal
>>> tkconfigure(table1,variable="tclarray")
>> <Tcl>
>>
>>
>> ---
>> Este email foi escaneado pelo Avast antiv?rus.
>> https://www.avast.com/antivirus
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From pdalgd at gmail.com  Sun Dec  6 20:28:14 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 6 Dec 2015 20:28:14 +0100
Subject: [R] tcltk TK table : Is it possible to block the cell?
	[Question in 2003, but TK changes the interfaces functions]
In-Reply-To: <566414B7.7090202@yahoo.com.br>
References: <566337EA.7030007@yahoo.com.br>
	<5FCDDA24-0986-493E-9852-7AB0F2116BBC@gmail.com>
	<566414B7.7090202@yahoo.com.br>
Message-ID: <E361FF38-AA10-47EA-991C-A90232B98239@gmail.com>

Well, if it was _my_ problem (and it isn't...), I'd get hold of the documentation for TkTable and figure out how you are supposed to do it with Tcl/Tk, then figure out how to do the same thing(s) from R. You should have the building blocks by now.

-pd


> On 06 Dec 2015, at 11:57 , Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> thanks by the help
> but I do not understand how to set a tag in cell table.
> 
> cleber
> 
> 
> Em 05/12/2015 18:57, peter dalgaard escreveu:
>>> On 05 Dec 2015, at 20:15 , Cleber N.Borges <klebyn at yahoo.com.br> wrote:
>>> 
>>> in 2003 [1] someone asked for:
>>> "Is it possible to" block "the cell?"
>>> but the solution involves the "tkcmd" that no longer exists ...
>>> 
>>> tkcmd (.Tk.ID (table1), "tag", "celltag" "ZeroOne", "0.1")
>> tcl() should be a drop-in replacement. The .Tk.ID() business should no longer be necessary, though. tcl(table1, "tag", "celltag",....)
>> 
>> 
>>> I tried to adapt to:
>>> 
>>> tktag.add (table1, "celltag" "ZeroOne", "0.1")
>>> 
>>> but I get error. How to adapt it?
>> Stop guessing and start thinking?
>> 
>> That would be equivalent to
>> 
>> tcl(table1, "tag", "add", "celltag" "ZeroOne", "0.1")
>> 
>> and as the error message *clearly* says, there is no tag option "add"!
>> 
>> -pd
>> 
>>> Where it could have informations about the tcltk package as it does not have the detailed help?
>>> 
>>> thanks in advanced for help
>>> cleber
>>> 
>>> [1] - http://grokbase.com/p/r/r-help/037xs650d8/r-tktable-disable-cell
>>> 
>>> 
>>> ############
>>>> tclRequire("Tktable")
>>> <Tcl> 2.9
>>>> tt <- tktoplevel()
>>>> table1 <- tkwidget(tt,"table",bg="white")
>>>> tkpack(table1)
>>> <Tcl>
>>>> tkcmd(.Tk.ID(table1),"tag","celltag","ZeroOne","0,1")
>>> Error: could not find function "tkcmd"
>>>> tktag.add( table1,"tag","celltag","ZeroOne","0,1")
>>> Error in structure(.External(.C_dotTclObjv, objv), class = "tclObj") :
>>>  [tcl] bad tag option "add": must be celltag, cget, coltag, configure, delete, exists, includes, lower, names, raise, or rowtag.
>>>> 
>>>> tkcmd(.Tk.ID(table1),"tag","celltag","ZeroTwo","0,2")
>>> Error: could not find function "tkcmd"
>>>> tkcmd(.Tk.ID(table1),"tag","configure","ZeroOne",state="disabled",bg="gray")
>>> Error: could not find function "tkcmd"
>>>> tkcmd(.Tk.ID(table1),"tag","configure","ZeroTwo",state="normal",bg="white")
>>> Error: could not find function "tkcmd"
>>>> .Tcl("set tclarray(0,0) Normal")
>>> <Tcl> Normal
>>>> .Tcl("set tclarray(0,1) Disabled")
>>> <Tcl> Disabled
>>>> .Tcl("set tclarray(0,2) Normal")
>>> <Tcl> Normal
>>>> tkconfigure(table1,variable="tclarray")
>>> <Tcl>
>>> 
>>> 
>>> ---
>>> Este email foi escaneado pelo Avast antiv?rus.
>>> https://www.avast.com/antivirus
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From btyner at gmail.com  Sun Dec  6 21:03:32 2015
From: btyner at gmail.com (Benjamin Tyner)
Date: Sun, 6 Dec 2015 15:03:32 -0500
Subject: [R] choropleth packages (US)
Message-ID: <56649494.3050206@gmail.com>

Hi

I wish to draw a basic choropleth (US, by state) and am wondering if 
anyone has any recommendations? I've tried the following thus far:

1. choroplethr: this works, but required installation of 30+ 
dependencies. I would prefer something with fewer dependencies.
2. tmap: this also seems promising, but most of the examples I saw were 
specific to European maps. Can it be adapted for US?
3. statebins: doesn't draw true choropleths, but I liked that it doesn't 
have many dependencies.

Regards
Ben


From rhelp10 at gmail.com  Sun Dec  6 22:18:18 2015
From: rhelp10 at gmail.com (DJ L)
Date: Sun, 6 Dec 2015 16:18:18 -0500
Subject: [R] Dense time series
Message-ID: <CAGphrqpc3wCBr5s51XVS_+oKzz6r-fPcnThTo9i5jBvb7ujR+Q@mail.gmail.com>

Hello R users!

Any idea why this looks so dense? Should be line graphs. Looks fine in
excel.  The csv file is four columns, first date, second well number, 3
well location (ditch or interior), and then the last column is hydraulic
head. Thank you!  I have attached a photo and the R code I am using.

setwd("c:/users/dot/desktop/r")

TS<-read.csv("TS_EAV1_SANDY.csv", header=TRUE,
sep=",",stringsAsFactors=FALSE)

# CHECK

head(TS)

str(TS)

#FORMAT DATE AND TIME

TS$Date <- as.POSIXct(TS$Date, tz = "", origin = "2012/10/22 0:00")

# CHECK
str(TS)

library(ggplot2)
library(scales)

#PLOT

xytheme <- theme(panel.background = element_blank(),
                 panel.grid.major.y = element_line(colour = "grey"),
                 panel.grid.minor.y = element_blank(),
                 panel.grid.major.x = element_line(colour = "grey"),
                 panel.grid.minor.x = element_blank(),
                 panel.border = element_rect(color = "black", fill = NA),
                 axis.text = element_text(color = "black", size = rel(1)),
                legend.title = element_blank())

###THIS ONE WORKS###

ggplot(TS, aes(Date, HYDRAULIC_HEAD.cm., group = Well)) +
  facet_grid(LOCATION~.) +
  geom_line(size=1) + xytheme +
  labs(x = "EAV1",y = "Water Levels, cm")+
scale_x_datetime(breaks = date_breaks("2 days"), labels =
date_format("%m/%d/%y"))

From wdunlap at tibco.com  Mon Dec  7 01:27:34 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 6 Dec 2015 16:27:34 -0800
Subject: [R] Dense time series
In-Reply-To: <CAGphrqpc3wCBr5s51XVS_+oKzz6r-fPcnThTo9i5jBvb7ujR+Q@mail.gmail.com>
References: <CAGphrqpc3wCBr5s51XVS_+oKzz6r-fPcnThTo9i5jBvb7ujR+Q@mail.gmail.com>
Message-ID: <CAF8bMcY4N6NRKSrYTZNas0h4aGmKxp7LenZsDktgieq-mHYcEA@mail.gmail.com>

I cannot see either your data or your picture.  Does the following dataset
along with your ggplot command give a similar picture?

TS <- data.frame(Well=paste0("Well",rep(1:21,each=3)),
LOCATION=paste0("Loc",rep(LETTERS[1:7],each=9)),
Date=as.POSIXct(paste(sep="-", 2015, 10, rep(c(11,18,25),21))),
HYDRAULIC_HEAD.cm.=100*(log2(1:63)+sin(1:63)+1))

If so, then describe what you mean by 'dense'.  I prefer finer lines,
like those produced by geom_line(size=0), but I don't know if that
is what you mean by dense..

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sun, Dec 6, 2015 at 1:18 PM, DJ L <rhelp10 at gmail.com> wrote:
> Hello R users!
>
> Any idea why this looks so dense? Should be line graphs. Looks fine in
> excel.  The csv file is four columns, first date, second well number, 3
> well location (ditch or interior), and then the last column is hydraulic
> head. Thank you!  I have attached a photo and the R code I am using.
>
> setwd("c:/users/dot/desktop/r")
>
> TS<-read.csv("TS_EAV1_SANDY.csv", header=TRUE,
> sep=",",stringsAsFactors=FALSE)
>
> # CHECK
>
> head(TS)
>
> str(TS)
>
> #FORMAT DATE AND TIME
>
> TS$Date <- as.POSIXct(TS$Date, tz = "", origin = "2012/10/22 0:00")
>
> # CHECK
> str(TS)
>
> library(ggplot2)
> library(scales)
>
> #PLOT
>
> xytheme <- theme(panel.background = element_blank(),
>                  panel.grid.major.y = element_line(colour = "grey"),
>                  panel.grid.minor.y = element_blank(),
>                  panel.grid.major.x = element_line(colour = "grey"),
>                  panel.grid.minor.x = element_blank(),
>                  panel.border = element_rect(color = "black", fill = NA),
>                  axis.text = element_text(color = "black", size = rel(1)),
>                 legend.title = element_blank())
>
> ###THIS ONE WORKS###
>
> ggplot(TS, aes(Date, HYDRAULIC_HEAD.cm., group = Well)) +
>   facet_grid(LOCATION~.) +
>   geom_line(size=1) + xytheme +
>   labs(x = "EAV1",y = "Water Levels, cm")+
> scale_x_datetime(breaks = date_breaks("2 days"), labels =
> date_format("%m/%d/%y"))
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From john.peterson.can at gmail.com  Mon Dec  7 01:39:27 2015
From: john.peterson.can at gmail.com (John Peterson)
Date: Sun, 6 Dec 2015 19:39:27 -0500
Subject: [R] metafor package
Message-ID: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>

Hi Everyone,

I am conducting a meta-analysis using the metafor package. I am interested
in obtaining an estimate by subgroup only without showing an overall
effect. This is directly from the metafor website. How would i modify this
code to only show subgroup effects? Further, I want to show weights by
subgroup. The option showweights=TRUE does not display weights by subgroup
but by the weight of each study in comparison to all studies (and not the
subgroup). You help would be appreciated.

library <http://stat.ethz.ch/R-manual/R-devel/library/base/html/library.html>(metafor)
 ### to save as png filepng
<http://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/png.html>(filename="forest_plot_with_subgroups.png",
    res=95, width=680, height=680, type="cairo")
 ### decrease margins so the full space is usedpar
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/par.html>(mar=c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(4,4,1,2))
 ### load BCG vaccine datadata
<http://stat.ethz.ch/R-manual/R-devel/library/utils/html/data.html>(dat.bcg)
 ### fit random-effects model (use slab argument to define study labels)
res <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
<http://stat.ethz.ch/R-manual/R-devel/library/utils/html/data.html>=dat.bcg,
measure="RR",
           slab=paste
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/paste.html>(author,
year, sep=", "), method="REML")
 ### set up forest plot (with 2x2 table counts added; rows argument is
used### to specify exactly in which rows the outcomes will be plotted)
forest(res, xlim=c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-16,
6), at=log <http://stat.ethz.ch/R-manual/R-devel/library/base/html/log.html>(c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(.05,
.25, 1, 4)), atransf=exp
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
       ilab=cbind
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/cbind.html>(dat.bcg$tpos,
dat.bcg$tneg, dat.bcg$cpos, dat.bcg$cneg),
       ilab.xpos=c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-9.5,-8,-6,-4.5),
cex=.75, ylim=c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-1,
27),
       order <http://stat.ethz.ch/R-manual/R-devel/library/base/html/order.html>=order
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/order.html>(dat.bcg$alloc),
rows=c <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(3:4,9:15,20:23),
       xlab="Relative Risk", mlab="RE Model for All Studies", psize=1)
 ### set font expansion factor (as in forest() above) and use bold
italic### font and save original settings in object 'op'
op <- par <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/par.html>(cex=.75,
font=4)
 ### add text for the subgroupstext
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(-16,
c <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(24,16,5),
pos=4, c <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>("Systematic
Allocation",
                               "Random Allocation",
                               "Alternate Allocation"))
 ### switch to bold fontpar
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/par.html>(font=2)
 ### add column headings to the plottext
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-9.5,-8,-6,-4.5),
26, c <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>("TB+",
"TB-", "TB+", "TB-"))text
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(c
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-8.75,-5.25),
    27, c <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>("Vaccinated",
"Control"))text
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(-16,
               26, "Author(s) and Year",     pos=4)text
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(6,
                 26, "Relative Risk [95% CI]", pos=2)
 ### set par back to the original settingspar
<http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/par.html>(op)
 ### fit random-effects model in the three subgroups
res.s <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
<http://stat.ethz.ch/R-manual/R-devel/library/utils/html/data.html>=dat.bcg,
measure="RR",
             subset
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/subset.html>=(alloc=="systematic"),
method="REML")
res.r <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
<http://stat.ethz.ch/R-manual/R-devel/library/utils/html/data.html>=dat.bcg,
measure="RR",
             subset
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/subset.html>=(alloc=="random"),
method="REML")
res.a <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
<http://stat.ethz.ch/R-manual/R-devel/library/utils/html/data.html>=dat.bcg,
measure="RR",
             subset
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/subset.html>=(alloc=="alternate"),
method="REML")
 ### add summary polygons for the three subgroups
addpoly(res.s, row
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/row.html>=18.5,
cex=.75, atransf=exp
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
mlab="RE Model for Subgroup")
addpoly(res.r, row
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/row.html>=
7.5, cex=.75, atransf=exp
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
mlab="RE Model for Subgroup")
addpoly(res.a, row
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/row.html>=
1.5, cex=.75, atransf=exp
<http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
mlab="RE Model for Subgroup")
 dev.off <http://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/dev.off.html>()

	[[alternative HTML version deleted]]


From rhelp10 at gmail.com  Mon Dec  7 01:49:55 2015
From: rhelp10 at gmail.com (DJ L)
Date: Sun, 6 Dec 2015 19:49:55 -0500
Subject: [R] Dense time series
In-Reply-To: <CAF8bMcY4N6NRKSrYTZNas0h4aGmKxp7LenZsDktgieq-mHYcEA@mail.gmail.com>
References: <CAGphrqpc3wCBr5s51XVS_+oKzz6r-fPcnThTo9i5jBvb7ujR+Q@mail.gmail.com>
	<CAF8bMcY4N6NRKSrYTZNas0h4aGmKxp7LenZsDktgieq-mHYcEA@mail.gmail.com>
Message-ID: <CAGphrqpR3z+WM9ET1sNHoJPF0ZmoTk7bK4i=3nqfrSRZhvJAng@mail.gmail.com>

Hello,

Thank you for the reply.

I ran the code with your added code. The code itself works however I am
unable to see how the graphs actually plot because the plot area is so
small and it produces lots of separate graphs.

What I mean by dense is that some lines plot well, then there is a group
that looks like one clump, however the data is not clumped. I changed the
size to 0 but a few are still to dense.

You mentioned you can't see the photo I attached, should I resend?



On Sun, Dec 6, 2015 at 7:27 PM, William Dunlap <wdunlap at tibco.com> wrote:

> I cannot see either your data or your picture.  Does the following dataset
> along with your ggplot command give a similar picture?
>
> TS <- data.frame(Well=paste0("Well",rep(1:21,each=3)),
> LOCATION=paste0("Loc",rep(LETTERS[1:7],each=9)),
> Date=as.POSIXct(paste(sep="-", 2015, 10, rep(c(11,18,25),21))),
> HYDRAULIC_HEAD.cm.=100*(log2(1:63)+sin(1:63)+1))
>
> If so, then describe what you mean by 'dense'.  I prefer finer lines,
> like those produced by geom_line(size=0), but I don't know if that
> is what you mean by dense..
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Sun, Dec 6, 2015 at 1:18 PM, DJ L <rhelp10 at gmail.com> wrote:
> > Hello R users!
> >
> > Any idea why this looks so dense? Should be line graphs. Looks fine in
> > excel.  The csv file is four columns, first date, second well number, 3
> > well location (ditch or interior), and then the last column is hydraulic
> > head. Thank you!  I have attached a photo and the R code I am using.
> >
> > setwd("c:/users/dot/desktop/r")
> >
> > TS<-read.csv("TS_EAV1_SANDY.csv", header=TRUE,
> > sep=",",stringsAsFactors=FALSE)
> >
> > # CHECK
> >
> > head(TS)
> >
> > str(TS)
> >
> > #FORMAT DATE AND TIME
> >
> > TS$Date <- as.POSIXct(TS$Date, tz = "", origin = "2012/10/22 0:00")
> >
> > # CHECK
> > str(TS)
> >
> > library(ggplot2)
> > library(scales)
> >
> > #PLOT
> >
> > xytheme <- theme(panel.background = element_blank(),
> >                  panel.grid.major.y = element_line(colour = "grey"),
> >                  panel.grid.minor.y = element_blank(),
> >                  panel.grid.major.x = element_line(colour = "grey"),
> >                  panel.grid.minor.x = element_blank(),
> >                  panel.border = element_rect(color = "black", fill = NA),
> >                  axis.text = element_text(color = "black", size =
> rel(1)),
> >                 legend.title = element_blank())
> >
> > ###THIS ONE WORKS###
> >
> > ggplot(TS, aes(Date, HYDRAULIC_HEAD.cm., group = Well)) +
> >   facet_grid(LOCATION~.) +
> >   geom_line(size=1) + xytheme +
> >   labs(x = "EAV1",y = "Water Levels, cm")+
> > scale_x_datetime(breaks = date_breaks("2 days"), labels =
> > date_format("%m/%d/%y"))
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From djv5030 at gmail.com  Mon Dec  7 02:47:12 2015
From: djv5030 at gmail.com (Dan Vecellio)
Date: Sun, 6 Dec 2015 19:47:12 -0600
Subject: [R] Simple DLNM in R
Message-ID: <CAPWPvYgwF2aBYAB5dievuyhQO1mu73EVt-EEKfy=KEYjS64ApA@mail.gmail.com>

Hello, first time poster so forgive any mistakes.

I have limited familiarity with R, but am working on a project to find the
relative risk of mortality due to changes in diurnal temperature range.
What I am trying to do is find the relative risk of mortality at the 10th,
50th and 90th percentiles of diurnal temperature range and its additive
effects at lags of 0, 1, 3 and 5 days. I'm doing this for a subset of
months May-Sept (I call the subset here for mortality, temperature is
already subsetted when read in). I have a code that works below, but no
matter what city and what lag I introduce, I get a RR of essentially 1.0,
so I believe that something is off or I am missing an argument somewhere.
If anyone has more experience with these problems than I, your help would
be greatly appreciated. Code is below:

library('dlnm')
library('splines')

mortdata <- read.table('STLmort.txt', sep="\t", header=T)
morts <- subset(mortdata, Month %in% 5:9)
deaths <- morts$AllMort
tempdata <- read.csv('STLRanges.csv',sep=',',header=T)
temp <- tempdata$Trange
HI <- tempdata$HIrange
#basis.var <- onebasis(1:5, knots=3)#mklagbasis(maxlag=5, type="poly",
degree=3)
basis.temp <- crossbasis(temp,vardegree=3,lag=5)
summary(basis.temp)
model <- glm (deaths ~ basis.temp, family=quasipoisson())
pred.temp <- crosspred(basis.temp, model,
at=quantile(temp,c(.10,.50,.90),na.rm=TRUE) , cumul=T)
plot(pred.temp, "slices", var=c(quantile(temp, c(.10, .50,
.90),na.rm=TRUE)) ,lag=c(0,1,5))


-- 
Daniel J. Vecellio

PhD Student, Department of Geography
Texas A&M University

	[[alternative HTML version deleted]]


From jvadams at usgs.gov  Fri Dec  4 22:18:50 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Fri, 4 Dec 2015 15:18:50 -0600
Subject: [R] =?utf-8?q?=5BR-pkgs=5D_LW1949_=E2=80=93_new_package_for_evalu?=
	=?utf-8?q?ating_dose-effect_experiments?=
Message-ID: <CAN5YmCHCCUjX6xLPFfENTj6ne-cOpHNAUR2tYiigv0WCpbeorw@mail.gmail.com>

LW1949 1.0.0 is now available on CRAN.



LW1949 automates the steps taken in Litchfield and Wilcoxon?s (1949) manual
approach to evaluating dose-effect experiments. Letting the computer do the
work saves time and yields the best fit possible using the Litchfield
Wilcoxon approach (by minimizing the chi-squared statistic).



A brief demonstration of LW1949 is given in this web app,

     https://jvadams.shinyapps.io/LW1949demo



An example of how to use the functions in LW1949 is given in this vignette,

     https://rawgit.com/JVAdams/LW1949/master/vignettes/Intro.html



Litchfield, JT Jr. and F Wilcoxon. 1949. A simplified method of evaluating
dose-effect experiments. Journal of Pharmacology and Experimental
Therapeutics 96(2):99-113.

     http://jpet.aspetjournals.org/content/96/2/99.abstract


Jean??


`?.,,  ><(((?>   `?.,,  ><(((?>   `?.,,  ><(((?>

Jean V. Adams??
Statistician
U.S. Geological?? Survey ??
Great Lakes Science Center
223 East Steinfest Road ??
Antigo, WI 54409  USA ??
http://www.glsc.usgs.gov
http://profile.usgs.gov/jvadams

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages

From erich.neuwirth at univie.ac.at  Mon Dec  7 09:34:07 2015
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 7 Dec 2015 09:34:07 +0100
Subject: [R] choropleth packages (US)
In-Reply-To: <56649494.3050206@gmail.com>
References: <56649494.3050206@gmail.com>
Message-ID: <B4FD6FEF-D170-45B4-B51D-CB8D8070020B@univie.ac.at>

ggplot2 also can do this with
fortify
geom_polygon

Von meinem iPad gesendet

> Am 06.12.2015 um 21:03 schrieb Benjamin Tyner <btyner at gmail.com>:
> 
> Hi
> 
> I wish to draw a basic choropleth (US, by state) and am wondering if anyone has any recommendations? I've tried the following thus far:
> 
> 1. choroplethr: this works, but required installation of 30+ dependencies. I would prefer something with fewer dependencies.
> 2. tmap: this also seems promising, but most of the examples I saw were specific to European maps. Can it be adapted for US?
> 3. statebins: doesn't draw true choropleths, but I liked that it doesn't have many dependencies.
> 
> Regards
> Ben
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wolfgang.viechtbauer at maastrichtuniversity.nl  Mon Dec  7 10:02:48 2015
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Mon, 7 Dec 2015 10:02:48 +0100
Subject: [R] metafor package
In-Reply-To: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>
References: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730F24C463BF@UM-MAIL4112.unimaas.nl>

The code you posted is totally mangled up, but it's just what can be found here:

http://www.metafor-project.org/doku.php/plots:forest_plot_with_subgroups

If you don't want an overall estimate, just pass the estimates and corresponding sampling variances to the forest() function (and not the model object). Use the 'rows' argument to specify where the estimates will be placed and adjust 'ylim' so give you enough space to leave gaps for headings and the subgroup estimates. Then fit models within the subgroups (the 'subset' argument is useful here) and use addpoly() to add the subgroup estimates in the appropriate rows. With text(), you can add headings as needed.

If you use weights() on each subgroup model object, you can get the subgroup weights (that add up to 100% within each subgroup). It's probably easiest to just add those values with text() in an appropriate place to the plot.

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John
> Peterson
> Sent: Monday, December 07, 2015 01:39
> To: r-help at r-project.org
> Subject: [R] metafor package
> 
> Hi Everyone,
> 
> I am conducting a meta-analysis using the metafor package. I am
> interested
> in obtaining an estimate by subgroup only without showing an overall
> effect. This is directly from the metafor website. How would i modify
> this
> code to only show subgroup effects? Further, I want to show weights by
> subgroup. The option showweights=TRUE does not display weights by
> subgroup
> but by the weight of each study in comparison to all studies (and not the
> subgroup). You help would be appreciated.
> 
> library <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/library.html>(metafor)
>  ### to save as png filepng
> <http://stat.ethz.ch/R-manual/R-
> devel/library/grDevices/html/png.html>(filename="forest_plot_with_subgrou
> ps.png",
>     res=95, width=680, height=680, type="cairo")
>  ### decrease margins so the full space is usedpar
> <http://stat.ethz.ch/R-manual/R-
> devel/library/graphics/html/par.html>(mar=c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(4,4,1,2))
>  ### load BCG vaccine datadata
> <http://stat.ethz.ch/R-manual/R-
> devel/library/utils/html/data.html>(dat.bcg)
>  ### fit random-effects model (use slab argument to define study labels)
> res <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
> <http://stat.ethz.ch/R-manual/R-
> devel/library/utils/html/data.html>=dat.bcg,
> measure="RR",
>            slab=paste
> <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/paste.html>(author,
> year, sep=", "), method="REML")
>  ### set up forest plot (with 2x2 table counts added; rows argument is
> used### to specify exactly in which rows the outcomes will be plotted)
> forest(res, xlim=c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-16,
> 6), at=log <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/log.html>(c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(.05,
> .25, 1, 4)), atransf=exp
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
>        ilab=cbind
> <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/cbind.html>(dat.bcg$tpos,
> dat.bcg$tneg, dat.bcg$cpos, dat.bcg$cneg),
>        ilab.xpos=c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-9.5,-8,-
> 6,-4.5),
> cex=.75, ylim=c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-1,
> 27),
>        order <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/order.html>=order
> <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/order.html>(dat.bcg$alloc),
> rows=c <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/c.html>(3:4,9:15,20:23),
>        xlab="Relative Risk", mlab="RE Model for All Studies", psize=1)
>  ### set font expansion factor (as in forest() above) and use bold
> italic### font and save original settings in object 'op'
> op <- par <http://stat.ethz.ch/R-manual/R-
> devel/library/graphics/html/par.html>(cex=.75,
> font=4)
>  ### add text for the subgroupstext
> <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(-
> 16,
> c <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/c.html>(24,16,5),
> pos=4, c <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/c.html>("Systematic
> Allocation",
>                                "Random Allocation",
>                                "Alternate Allocation"))
>  ### switch to bold fontpar
> <http://stat.ethz.ch/R-manual/R-
> devel/library/graphics/html/par.html>(font=2)
>  ### add column headings to the plottext
> <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-9.5,-8,-
> 6,-4.5),
> 26, c <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/c.html>("TB+",
> "TB-", "TB+", "TB-"))text
> <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(c
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/c.html>(-8.75,-
> 5.25),
>     27, c <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/c.html>("Vaccinated",
> "Control"))text
> <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(-
> 16,
>                26, "Author(s) and Year",     pos=4)text
> <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/text.html>(6,
>                  26, "Relative Risk [95% CI]", pos=2)
>  ### set par back to the original settingspar
> <http://stat.ethz.ch/R-manual/R-devel/library/graphics/html/par.html>(op)
>  ### fit random-effects model in the three subgroups
> res.s <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
> <http://stat.ethz.ch/R-manual/R-
> devel/library/utils/html/data.html>=dat.bcg,
> measure="RR",
>              subset
> <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/subset.html>=(alloc=="systematic"),
> method="REML")
> res.r <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
> <http://stat.ethz.ch/R-manual/R-
> devel/library/utils/html/data.html>=dat.bcg,
> measure="RR",
>              subset
> <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/subset.html>=(alloc=="random"),
> method="REML")
> res.a <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data
> <http://stat.ethz.ch/R-manual/R-
> devel/library/utils/html/data.html>=dat.bcg,
> measure="RR",
>              subset
> <http://stat.ethz.ch/R-manual/R-
> devel/library/base/html/subset.html>=(alloc=="alternate"),
> method="REML")
>  ### add summary polygons for the three subgroups
> addpoly(res.s, row
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/row.html>=18.5,
> cex=.75, atransf=exp
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
> mlab="RE Model for Subgroup")
> addpoly(res.r, row
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/row.html>=
> 7.5, cex=.75, atransf=exp
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
> mlab="RE Model for Subgroup")
> addpoly(res.a, row
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/row.html>=
> 1.5, cex=.75, atransf=exp
> <http://stat.ethz.ch/R-manual/R-devel/library/base/html/exp.html>,
> mlab="RE Model for Subgroup")
>  dev.off <http://stat.ethz.ch/R-manual/R-
> devel/library/grDevices/html/dev.off.html>()
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Mon Dec  7 13:16:52 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Dec 2015 04:16:52 -0800
Subject: [R] Dense time series
In-Reply-To: <CAGphrqpR3z+WM9ET1sNHoJPF0ZmoTk7bK4i=3nqfrSRZhvJAng@mail.gmail.com>
References: <cagphrqpc3wcbr5s51xvs_+okzz6r-fpcnthto9i5jbvb7ujr+q@mail.gmail.com>
	<caf8bmcy4n6nrksrytznas0h4agmkxp7lenzsdktgieq-mhycea@mail.gmail.com>
Message-ID: <EBD3C46E38D.0000019Bjrkrideau@inbox.com>

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

Probably the image and your code did not come through because R-help is very picky about what types of files it allows as a security mesure.  Code probably would make it if it sent as a .txt file not a .r file. For an image, try, IIRC, png or a pdf file.

Generally speaking,  it is best to provide code and data in the email itself.

If sending sample data, and it is a good idea to do so, use dput() as described in the links above or have a look at ?dput.

Using dput ensures that the R-help reader is seeing the data as you see it.  

BTW should not "Looks fine in excel" actually read, " Looks vaguely acceptable in Excel, given the limitations of a spreadsheet graphs"? :)
John Kane
Kingston ON Canada


> -----Original Message-----
> From: rhelp10 at gmail.com
> Sent: Sun, 6 Dec 2015 19:49:55 -0500
> To: wdunlap at tibco.com
> Subject: Re: [R] Dense time series
> 
> Hello,
> 
> Thank you for the reply.
> 
> I ran the code with your added code. The code itself works however I am
> unable to see how the graphs actually plot because the plot area is so
> small and it produces lots of separate graphs.
> 
> What I mean by dense is that some lines plot well, then there is a group
> that looks like one clump, however the data is not clumped. I changed the
> size to 0 but a few are still to dense.
> 
> You mentioned you can't see the photo I attached, should I resend?
> 
> 
> 
> On Sun, Dec 6, 2015 at 7:27 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
>> I cannot see either your data or your picture.  Does the following
>> dataset
>> along with your ggplot command give a similar picture?
>> 
>> TS <- data.frame(Well=paste0("Well",rep(1:21,each=3)),
>> LOCATION=paste0("Loc",rep(LETTERS[1:7],each=9)),
>> Date=as.POSIXct(paste(sep="-", 2015, 10, rep(c(11,18,25),21))),
>> HYDRAULIC_HEAD.cm.=100*(log2(1:63)+sin(1:63)+1))
>> 
>> If so, then describe what you mean by 'dense'.  I prefer finer lines,
>> like those produced by geom_line(size=0), but I don't know if that
>> is what you mean by dense..
>> 
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>> 
>> 
>> On Sun, Dec 6, 2015 at 1:18 PM, DJ L <rhelp10 at gmail.com> wrote:
>>> Hello R users!
>>> 
>>> Any idea why this looks so dense? Should be line graphs. Looks fine in
>>> excel.  The csv file is four columns, first date, second well number, 3
>>> well location (ditch or interior), and then the last column is
>>> hydraulic
>>> head. Thank you!  I have attached a photo and the R code I am using.
>>> 
>>> setwd("c:/users/dot/desktop/r")
>>> 
>>> TS<-read.csv("TS_EAV1_SANDY.csv", header=TRUE,
>>> sep=",",stringsAsFactors=FALSE)
>>> 
>>> # CHECK
>>> 
>>> head(TS)
>>> 
>>> str(TS)
>>> 
>>> #FORMAT DATE AND TIME
>>> 
>>> TS$Date <- as.POSIXct(TS$Date, tz = "", origin = "2012/10/22 0:00")
>>> 
>>> # CHECK
>>> str(TS)
>>> 
>>> library(ggplot2)
>>> library(scales)
>>> 
>>> #PLOT
>>> 
>>> xytheme <- theme(panel.background = element_blank(),
>>>                  panel.grid.major.y = element_line(colour = "grey"),
>>>                  panel.grid.minor.y = element_blank(),
>>>                  panel.grid.major.x = element_line(colour = "grey"),
>>>                  panel.grid.minor.x = element_blank(),
>>>                  panel.border = element_rect(color = "black", fill =
>>> NA),
>>>                  axis.text = element_text(color = "black", size =
>> rel(1)),
>>>                 legend.title = element_blank())
>>> 
>>> ###THIS ONE WORKS###
>>> 
>>> ggplot(TS, aes(Date, HYDRAULIC_HEAD.cm., group = Well)) +
>>>   facet_grid(LOCATION~.) +
>>>   geom_line(size=1) + xytheme +
>>>   labs(x = "EAV1",y = "Water Levels, cm")+
>>> scale_x_datetime(breaks = date_breaks("2 days"), labels =
>>> date_format("%m/%d/%y"))
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From jrkrideau at inbox.com  Mon Dec  7 13:22:27 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Dec 2015 04:22:27 -0800
Subject: [R] Simple DLNM in R
In-Reply-To: <CAPWPvYgwF2aBYAB5dievuyhQO1mu73EVt-EEKfy=KEYjS64ApA@mail.gmail.com>
Message-ID: <EBE03C188A5.000001A1jrkrideau@inbox.com>

Welcome to the list. 

You have provided a nic clear question but I think the one thing missing in dealing with it is  some sample data. 

Have a look at ?dput or see the dput() discussions in one of these links : http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

Someone may easily find the problem without sample data but it usually is best to troubleshoot with the "real" data.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: djv5030 at gmail.com
> Sent: Sun, 6 Dec 2015 19:47:12 -0600
> To: r-help at r-project.org
> Subject: [R] Simple DLNM in R
> 
> Hello, first time poster so forgive any mistakes.
> 
> I have limited familiarity with R, but am working on a project to find
> the
> relative risk of mortality due to changes in diurnal temperature range.
> What I am trying to do is find the relative risk of mortality at the
> 10th,
> 50th and 90th percentiles of diurnal temperature range and its additive
> effects at lags of 0, 1, 3 and 5 days. I'm doing this for a subset of
> months May-Sept (I call the subset here for mortality, temperature is
> already subsetted when read in). I have a code that works below, but no
> matter what city and what lag I introduce, I get a RR of essentially 1.0,
> so I believe that something is off or I am missing an argument somewhere.
> If anyone has more experience with these problems than I, your help would
> be greatly appreciated. Code is below:
> 
> library('dlnm')
> library('splines')
> 
> mortdata <- read.table('STLmort.txt', sep="\t", header=T)
> morts <- subset(mortdata, Month %in% 5:9)
> deaths <- morts$AllMort
> tempdata <- read.csv('STLRanges.csv',sep=',',header=T)
> temp <- tempdata$Trange
> HI <- tempdata$HIrange
> #basis.var <- onebasis(1:5, knots=3)#mklagbasis(maxlag=5, type="poly",
> degree=3)
> basis.temp <- crossbasis(temp,vardegree=3,lag=5)
> summary(basis.temp)
> model <- glm (deaths ~ basis.temp, family=quasipoisson())
> pred.temp <- crosspred(basis.temp, model,
> at=quantile(temp,c(.10,.50,.90),na.rm=TRUE) , cumul=T)
> plot(pred.temp, "slices", var=c(quantile(temp, c(.10, .50,
> .90),na.rm=TRUE)) ,lag=c(0,1,5))
> 
> 
> --
> Daniel J. Vecellio
> 
> PhD Student, Department of Geography
> Texas A&M University
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From roypeter77 at hotmail.com  Mon Dec  7 11:06:36 2015
From: roypeter77 at hotmail.com (Peter Karpestam)
Date: Mon, 7 Dec 2015 10:06:36 +0000
Subject: [R] Spatial Panel Model by Maximum Likelihood: error message
Message-ID: <DUB123-W45248EAA34EAF09A5CFF1DDD090@phx.gbl>

Dear R-users!

Beforehand I do apologize for this fundamental type of question. But I am new to R, and I have indeed spent several hours online trying to avoid spamming this forum with basic questions.
I am still not able to find out what's wrong.

I attach my code! I nead to estimate a spatial panel model. In doing so, I have installed and loaded the "splm package".

> b<- read.dta13('C:/Users/Peter/Documents/R/win-library/3.2/ENHR/tillvaxttest2.dta')
> mydata1=read.table('C:/Users/Peter/Documents/R/win-library/3.2/ENHR/avstand6.txt', header=FALSE)
> 
> e=matrix(mydata, nrow=286, ncol=286)
> tillvaxt<-pdata.frame(b)
> 
> fm<-dlnypc~Totinfly+totutfly
> 
> fespaterr <- spml(fm, data = tillvaxt,listw =e,
+ model="within", spatial.error="b", Hess = FALSE)
Error in listw2mat(x) : non-positive number of entities

The error message is found in the last line. I am checking my "weighting matrix" and according to my understanding, the matrix fulfills the requirement (i.e. the diagonal elements are zero, whereas remaining elements are positive). It is a symmetric 286X286 matrix.

Obviuosly, there is something wrong with my e-matrix, but I can't figure out how. There are in fact no negative elements in the matrix.

Thank you in advance!

Peter 
 		 	   		  
	[[alternative HTML version deleted]]


From Marios.BARLAS at cea.fr  Mon Dec  7 11:59:58 2015
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Mon, 7 Dec 2015 10:59:58 +0000
Subject: [R] Ordering Filenames stored in list or vector
In-Reply-To: <EEC69174-3996-4831-BE73-C363C71CFBA4@txbiomed.org>
References: <01BFC0B2B4ABFC4CB432008F852D76616CB628@EXDAG0-A1.intra.cea.fr>,
	<EEC69174-3996-4831-BE73-C363C71CFBA4@txbiomed.org>
Message-ID: <01BFC0B2B4ABFC4CB432008F852D76616CB7BC@EXDAG0-A1.intra.cea.fr>

Thanks a lot for the clarifying code Mark!

Actually, I took a lazy option and after some digging around I found out the package called "naturalsort" which provides a pretty compact solution!

As a rookie, I have another question. My main interest in R is that I hope to integrate in a scripting fashion good amount of data crunching coming from electrical measurements and at the same time give me a nice visualization option all in the same tool.

Is it a proper tool for such use in your experience? So far I was using Matlab + OriginPro for treatment and visualization but now, starting my PhD I feel like I want something more "integrated"

Thanks,
Mario
________________________________________
From: Mark Sharp [msharp at TxBiomed.org]
Sent: Friday, December 04, 2015 5:25 PM
To: BARLAS Marios 247554
Cc: r-help at r-project.org
Subject: Re: [R] Ordering Filenames stored in list or vector

Mario,

I am certain there are more elegant solutions. This is an effort to make the process clear by dividing out each transformation used into separate lines.

## Start of code
library(stringi) # This is written in C and C++ (ICU library), is fast, and is well documented.
filenames <- c("Q_Read_prist#1 at 1.xls", "Q_Read_prist#1 at 10.xls",
           "Q_Read_prist#1 at 11.xls", "Q_Read_prist#1 at 12.xls",
           "Q_Read_prist#1 at 13.xls", "Q_Read_prist#1 at 14.xls",
           "Q_Read_prist#1 at 15.xls", "Q_Read_prist#1 at 16.xls",
           "Q_Read_prist#1 at 17.xls", "Q_Read_prist#1 at 18.xls",
           "Q_Read_prist#1 at 19.xls", "Q_Read_prist#1 at 2.xls",
           "Q_Read_prist#1 at 3.xls", "Q_Read_prist#1 at 4.xls",
           "Q_Read_prist#1 at 5.xls", "Q_Read_prist#1 at 6.xls",
           "Q_Read_prist#1 at 7.xls", "Q_Read_prist#1 at 8.xls",
           "Q_Read_prist#1 at 9.xls")
indx_list <- stri_split_regex(filenames, pattern = "[@.]")
indx <- sapply(indx_list, function(x) {x[[2]]})
filenames_df <- data.frame(file_name = filenames, indx = indx,
                            stringsAsFactors = FALSE)
filenames_ordered <- filenames_df[order(as.numeric(filenames_df$indx)),
                                    "file_name"]
filenames_ordered
## end of code
Mark


R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org







> On Dec 4, 2015, at 4:51 AM, BARLAS Marios 247554 <Marios.BARLAS at cea.fr> wrote:
>
> Hello everyone,
>
> I am an R rookie and I'm learning as I program.
>
> I am working on a script to process a large amount of data: I read a pattern of filenames in the folder I want and import their data
>
> filenames = list.files(path, pattern="*Q_Read_prist*")
>
> myfiles = lapply(filenames, function(x) read.xlsx2(file=x, sheetName="Data", header=TRUE, FILENAMEVAR=x))
>
> The problem is that R recognizes the files in a 'non human' order.
>
> Q_Read_prist#1 at 1.xls   Q_Read_prist#1 at 1.xls
> Q_Read_prist#1 at 10.xls Q_Read_prist#1 at 10.xls
> Q_Read_prist#1 at 11.xls Q_Read_prist#1 at 11.xls
> Q_Read_prist#1 at 12.xls Q_Read_prist#1 at 12.xls
> Q_Read_prist#1 at 13.xls Q_Read_prist#1 at 13.xls
> Q_Read_prist#1 at 14.xls Q_Read_prist#1 at 14.xls
> Q_Read_prist#1 at 15.xls Q_Read_prist#1 at 15.xls
> Q_Read_prist#1 at 16.xls Q_Read_prist#1 at 16.xls
> Q_Read_prist#1 at 17.xls Q_Read_prist#1 at 17.xls
> Q_Read_prist#1 at 18.xls Q_Read_prist#1 at 18.xls
> Q_Read_prist#1 at 19.xls Q_Read_prist#1 at 19.xls
> Q_Read_prist#1 at 2.xls   Q_Read_prist#1 at 2.xls
> Q_Read_prist#1 at 3.xls   Q_Read_prist#1 at 3.xls
> Q_Read_prist#1 at 4.xls   Q_Read_prist#1 at 4.xls
> Q_Read_prist#1 at 5.xls   Q_Read_prist#1 at 5.xls
> Q_Read_prist#1 at 6.xls   Q_Read_prist#1 at 6.xls
> Q_Read_prist#1 at 7.xls   Q_Read_prist#1 at 7.xls
> Q_Read_prist#1 at 8.xls   Q_Read_prist#1 at 8.xls
> Q_Read_prist#1 at 9.xls   Q_Read_prist#1 at 9.xls
>
> I tried to order them using order or sort but it doesn' seem to work. I have had the same issue in matlab but there I have a function to re-define the order in a "correct" way.
>
> Anyone knows of a smart way to sort these guys from 1 to 19 ascending or descending?
>
> Thanks in advance,
> Mario
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jrkrideau at inbox.com  Mon Dec  7 16:01:35 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Dec 2015 07:01:35 -0800
Subject: [R] Ordering Filenames stored in list or vector
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CB7BC@EXDAG0-A1.intra.cea.fr>
References: <01bfc0b2b4abfc4cb432008f852d76616cb628@exdag0-a1.intra.cea.fr>
	<eec69174-3996-4831-be73-c363c71cfba4@txbiomed.org>
Message-ID: <ED43EEB74E0.00000361jrkrideau@inbox.com>

Hi Marios,

>"My main interest in R is that I hope to integrate in a scripting fashion good amount of data crunching
> coming from electrical measurements and at the same time give me a nice
> visualization option all in the same tool.

I have never used Matlab, etc, so I have no idea how these integrate but it should be possible to go from raw data to written paper if you want to in R.

See http://yihui.name/knitr/demo/showcase/ for some examples. 

It 

John Kane
Kingston ON Canada


> -----Original Message-----
> From: marios.barlas at cea.fr
> Sent: Mon, 7 Dec 2015 10:59:58 +0000
> To: msharp at txbiomed.org
> Subject: Re: [R] Ordering Filenames stored in list or vector
> 
> Thanks a lot for the clarifying code Mark!
> 
> Actually, I took a lazy option and after some digging around I found out
> the package called "naturalsort" which provides a pretty compact
> solution!
> 
> As a rookie, I have another question. 
> 
> Is it a proper tool for such use in your experience? So far I was using
> Matlab + OriginPro for treatment and visualization but now, starting my
> PhD I feel like I want something more "integrated"
> 
> Thanks,
> Mario
> ________________________________________
> From: Mark Sharp [msharp at TxBiomed.org]
> Sent: Friday, December 04, 2015 5:25 PM
> To: BARLAS Marios 247554
> Cc: r-help at r-project.org
> Subject: Re: [R] Ordering Filenames stored in list or vector
> 
> Mario,
> 
> I am certain there are more elegant solutions. This is an effort to make
> the process clear by dividing out each transformation used into separate
> lines.
> 
> ## Start of code
> library(stringi) # This is written in C and C++ (ICU library), is fast,
> and is well documented.
> filenames <- c("Q_Read_prist#1 at 1.xls", "Q_Read_prist#1 at 10.xls",
>            "Q_Read_prist#1 at 11.xls", "Q_Read_prist#1 at 12.xls",
>            "Q_Read_prist#1 at 13.xls", "Q_Read_prist#1 at 14.xls",
>            "Q_Read_prist#1 at 15.xls", "Q_Read_prist#1 at 16.xls",
>            "Q_Read_prist#1 at 17.xls", "Q_Read_prist#1 at 18.xls",
>            "Q_Read_prist#1 at 19.xls", "Q_Read_prist#1 at 2.xls",
>            "Q_Read_prist#1 at 3.xls", "Q_Read_prist#1 at 4.xls",
>            "Q_Read_prist#1 at 5.xls", "Q_Read_prist#1 at 6.xls",
>            "Q_Read_prist#1 at 7.xls", "Q_Read_prist#1 at 8.xls",
>            "Q_Read_prist#1 at 9.xls")
> indx_list <- stri_split_regex(filenames, pattern = "[@.]")
> indx <- sapply(indx_list, function(x) {x[[2]]})
> filenames_df <- data.frame(file_name = filenames, indx = indx,
>                             stringsAsFactors = FALSE)
> filenames_ordered <- filenames_df[order(as.numeric(filenames_df$indx)),
>                                     "file_name"]
> filenames_ordered
> ## end of code
> Mark
> 
> 
> R. Mark Sharp, Ph.D.
> Director of Primate Records Database
> Southwest National Primate Research Center
> Texas Biomedical Research Institute
> P.O. Box 760549
> San Antonio, TX 78245-0549
> Telephone: (210)258-9476
> e-mail: msharp at TxBiomed.org
> 
> 
> 
> 
> 
> 
> 
>> On Dec 4, 2015, at 4:51 AM, BARLAS Marios 247554 <Marios.BARLAS at cea.fr>
>> wrote:
>> 
>> Hello everyone,
>> 
>> I am an R rookie and I'm learning as I program.
>> 
>> I am working on a script to process a large amount of data: I read a
>> pattern of filenames in the folder I want and import their data
>> 
>> filenames = list.files(path, pattern="*Q_Read_prist*")
>> 
>> myfiles = lapply(filenames, function(x) read.xlsx2(file=x,
>> sheetName="Data", header=TRUE, FILENAMEVAR=x))
>> 
>> The problem is that R recognizes the files in a 'non human' order.
>> 
>> Q_Read_prist#1 at 1.xls   Q_Read_prist#1 at 1.xls
>> Q_Read_prist#1 at 10.xls Q_Read_prist#1 at 10.xls
>> Q_Read_prist#1 at 11.xls Q_Read_prist#1 at 11.xls
>> Q_Read_prist#1 at 12.xls Q_Read_prist#1 at 12.xls
>> Q_Read_prist#1 at 13.xls Q_Read_prist#1 at 13.xls
>> Q_Read_prist#1 at 14.xls Q_Read_prist#1 at 14.xls
>> Q_Read_prist#1 at 15.xls Q_Read_prist#1 at 15.xls
>> Q_Read_prist#1 at 16.xls Q_Read_prist#1 at 16.xls
>> Q_Read_prist#1 at 17.xls Q_Read_prist#1 at 17.xls
>> Q_Read_prist#1 at 18.xls Q_Read_prist#1 at 18.xls
>> Q_Read_prist#1 at 19.xls Q_Read_prist#1 at 19.xls
>> Q_Read_prist#1 at 2.xls   Q_Read_prist#1 at 2.xls
>> Q_Read_prist#1 at 3.xls   Q_Read_prist#1 at 3.xls
>> Q_Read_prist#1 at 4.xls   Q_Read_prist#1 at 4.xls
>> Q_Read_prist#1 at 5.xls   Q_Read_prist#1 at 5.xls
>> Q_Read_prist#1 at 6.xls   Q_Read_prist#1 at 6.xls
>> Q_Read_prist#1 at 7.xls   Q_Read_prist#1 at 7.xls
>> Q_Read_prist#1 at 8.xls   Q_Read_prist#1 at 8.xls
>> Q_Read_prist#1 at 9.xls   Q_Read_prist#1 at 9.xls
>> 
>> I tried to order them using order or sort but it doesn' seem to work. I
>> have had the same issue in matlab but there I have a function to
>> re-define the order in a "correct" way.
>> 
>> Anyone knows of a smart way to sort these guys from 1 to 19 ascending or
>> descending?
>> 
>> Thanks in advance,
>> Mario
>> 
>>       [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From holtermann at hwwi.org  Mon Dec  7 16:06:05 2015
From: holtermann at hwwi.org (Linus Holtermann)
Date: Mon, 7 Dec 2015 15:06:05 +0000
Subject: [R] Spatial Panel Model by Maximum Likelihood: error message
In-Reply-To: <DUB123-W45248EAA34EAF09A5CFF1DDD090@phx.gbl>
References: <DUB123-W45248EAA34EAF09A5CFF1DDD090@phx.gbl>
Message-ID: <da0875503afd4de58b443e022cc6a347@winhexbeeu15.win.mail>

Maybe "e" has the wrong class. Probably it is a matrix. Try listw = mat2listw(e)


Mit freundlichen Gr??en


Linus Holtermann
Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
Heimhuder Stra?e 71
20148 Hamburg
Tel +49-(0)40-340576-336
Fax+49-(0)40-340576-776
Internet: www.hwwi.org
Email: holtermann at hwwi.org
?
Amtsgericht Hamburg HRB 94303
Gesch?ftsf?hrer: Prof. Dr. Henning V?pel
Prokura: Dipl. Kauffrau Alexis Malchin
Umsatzsteuer-ID: DE 241849425


-----Urspr?ngliche Nachricht-----
Von: R-help [mailto:r-help-bounces at r-project.org] Im Auftrag von Peter Karpestam
Gesendet: Montag, 7. Dezember 2015 11:07
An: r-help at r-project.org
Betreff: [R] Spatial Panel Model by Maximum Likelihood: error message

Dear R-users!

Beforehand I do apologize for this fundamental type of question. But I am new to R, and I have indeed spent several hours online trying to avoid spamming this forum with basic questions.
I am still not able to find out what's wrong.

I attach my code! I nead to estimate a spatial panel model. In doing so, I have installed and loaded the "splm package".

> b<- 
> read.dta13('C:/Users/Peter/Documents/R/win-library/3.2/ENHR/tillvaxtte
> st2.dta') 
> mydata1=read.table('C:/Users/Peter/Documents/R/win-library/3.2/ENHR/av
> stand6.txt', header=FALSE)
> 
> e=matrix(mydata, nrow=286, ncol=286)
> tillvaxt<-pdata.frame(b)
> 
> fm<-dlnypc~Totinfly+totutfly
> 
> fespaterr <- spml(fm, data = tillvaxt,listw =e,
+ model="within", spatial.error="b", Hess = FALSE)
Error in listw2mat(x) : non-positive number of entities

The error message is found in the last line. I am checking my "weighting matrix" and according to my understanding, the matrix fulfills the requirement (i.e. the diagonal elements are zero, whereas remaining elements are positive). It is a symmetric 286X286 matrix.

Obviuosly, there is something wrong with my e-matrix, but I can't figure out how. There are in fact no negative elements in the matrix.

Thank you in advance!

Peter 
 		 	   		  
	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From bsmith030465 at gmail.com  Mon Dec  7 16:10:14 2015
From: bsmith030465 at gmail.com (Brian Smith)
Date: Mon, 7 Dec 2015 10:10:14 -0500
Subject: [R] ggplot2: remove axis ticks
Message-ID: <CAEQKoCFJQ8FRzhegoCtCkeYZBzu-quJXjD-TC+ZU9RsycD8=0g@mail.gmail.com>

Hi,

I was trying to remove the axis tick marks and their values using theme()
but haven't had much success. Here is sample code:

rx <- sample(1:100,10)
ry <- sample(1:100,10)
rz <- sample(letters[1:3],10,replace=T)
rdf <- data.frame(rx,ry,rz)

p <- ggplot(rdf,aes(x=rx,y=ry))
p1 <- p + geom_point(aes(shape=factor(rz),colour=factor(rz)),size=6) +
    theme(axis.ticks = element_blank(), axis.text.x =
element_blank(),axis.text.y = element_blank()) +
    scale_shape_manual(values=rz)  + theme_bw() +
    labs(colour='rz',shape='rz')
p1


My session info:

> sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.5 (Yosemite)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
 [1] grid      stats4    parallel  stats     graphics  grDevices utils
datasets  methods   base

other attached packages:
 [1] IlluminaHumanMethylation450kmanifest_0.4.0 biomaRt_2.26.1

 [3] data.table_1.9.6                           foreign_0.8-65

 [5] preprocessCore_1.32.0                      gtools_3.5.0

 [7] BiocInstaller_1.20.1                       ggdendro_0.1-17

 [9] reshape_0.8.5                              RnBeads_1.2.0

[11] plyr_1.8.3                                 methylumi_2.16.0

[13] minfi_1.16.0                               bumphunter_1.10.0

[15] locfit_1.5-9.1                             iterators_1.0.8

[17] foreach_1.4.3                              Biostrings_2.38.2

[19] XVector_0.10.0                             SummarizedExperiment_1.0.1

[21] lattice_0.20-33
 FDb.InfiniumMethylation.hg19_2.2.0
[23] org.Hs.eg.db_3.2.3                         RSQLite_1.0.0

[25] DBI_0.3.1
 TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2
[27] GenomicFeatures_1.22.5                     AnnotationDbi_1.32.0

[29] reshape2_1.4.1                             scales_0.3.0

[31] Biobase_2.30.0                             illuminaio_0.12.0

[33] matrixStats_0.15.0                         limma_3.26.3

[35] gridExtra_2.0.0                            gplots_2.17.0

[37] ggplot2_1.0.1                              fields_8.3-5

[39] maps_3.0.0-2                               spam_1.3-0

[41] ff_2.2-13                                  bit_1.1-12

[43] cluster_2.0.3                              RColorBrewer_1.1-2

[45] MASS_7.3-43                                GenomicRanges_1.22.1

[47] GenomeInfoDb_1.6.1                         IRanges_2.4.4

[49] S4Vectors_0.8.3                            BiocGenerics_0.16.1


loaded via a namespace (and not attached):
 [1] nlme_3.1-121            bitops_1.0-6            tools_3.2.2
  doRNG_1.6
 [5] nor1mix_1.2-1           KernSmooth_2.23-15      colorspace_1.2-6
 base64_1.1
 [9] chron_2.3-47            pkgmaker_0.22           labeling_0.3
 rtracklayer_1.30.1
[13] caTools_1.17.1          genefilter_1.52.0       quadprog_1.5-5
 stringr_1.0.0
[17] digest_0.6.8            Rsamtools_1.22.0        siggenes_1.44.0
  GEOquery_2.36.0
[21] mclust_5.1              BiocParallel_1.4.0      RCurl_1.95-4.7
 magrittr_1.5
[25] futile.logger_1.4.1     Rcpp_0.12.2             munsell_0.4.2
  proto_0.3-10
[29] stringi_1.0-1           zlibbioc_1.16.0         gdata_2.17.0
 splines_3.2.2
[33] multtest_2.26.0         annotate_1.48.0         beanplot_1.2
 igraph_1.0.1
[37] corpcor_1.6.8           rngtools_1.2.4          codetools_0.2-14
 mixOmics_5.2.0
[41] futile.options_1.0.0    XML_3.98-1.3            lambda.r_1.1.7
 gtable_0.1.2
[45] xtable_1.8-0            survival_2.38-3         ellipse_0.3-8
  GenomicAlignments_1.6.1
[49] registry_0.3            rgl_0.95.1201


Am I setting the arguments for theme() incorrectly?

many thanks,

	[[alternative HTML version deleted]]


From djv5030 at gmail.com  Mon Dec  7 16:01:52 2015
From: djv5030 at gmail.com (Dan Vecellio)
Date: Mon, 7 Dec 2015 09:01:52 -0600
Subject: [R] Simple DLNM in R
In-Reply-To: <EBE03C188A5.000001A1jrkrideau@inbox.com>
References: <CAPWPvYgwF2aBYAB5dievuyhQO1mu73EVt-EEKfy=KEYjS64ApA@mail.gmail.com>
	<EBE03C188A5.000001A1jrkrideau@inbox.com>
Message-ID: <CAPWPvYj2mjXn3Pr+nAy8DJKoe2c4=eA8Pq6hU8Tq5M9VPy5sUg@mail.gmail.com>

Thanks for the suggestion, John. These are very long but below is the dput
for "morts" and "tempdata".

structure(list(Year = c(1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L), Month = c(5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L,
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L,
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L,
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L,
9L, 9L, 9L, 9L, 9L, 9L), Day = c(1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,
10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L,
23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L,
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L,
29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L,
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,
18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L,
31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,
14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L,
27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L,
24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L,
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,
20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L,
26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,
9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L,
22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 1L, 2L, 3L, 4L,
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,
19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L,
1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,
15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L,
28L, 29L, 30L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L,
25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,
8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L,
21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 1L, 2L,
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L,
30L), AllMort = c(61L, 59L, 67L, 58L, 80L, 57L, 60L, 61L, 52L,
63L, 66L, 66L, 69L, 59L, 63L, 67L, 59L, 56L, 51L, 53L, 68L, 53L,
63L, 57L, 60L, 64L, 60L, 47L, 57L, 69L, 76L, 68L, 60L, 51L, 66L,
66L, 59L, 70L, 59L, 74L, 60L, 69L, 90L, 63L, 47L, 77L, 63L, 63L,
65L, 68L, 57L, 66L, 63L, 62L, 68L, 70L, 59L, 66L, 80L, 58L, 65L,
70L, 57L, 66L, 55L, 67L, 63L, 64L, 69L, 60L, 76L, 61L, 74L, 56L,
68L, 50L, 47L, 62L, 62L, 60L, 66L, 67L, 71L, 57L, 56L, 45L, 67L,
67L, 76L, 72L, 42L, 49L, 65L, 51L, 68L, 44L, 67L, 58L, 57L, 58L,
57L, 57L, 75L, 62L, 65L, 60L, 57L, 59L, 68L, 56L, 60L, 61L, 70L,
52L, 60L, 59L, 60L, 46L, 57L, 52L, 77L, 60L, 68L, 56L, 70L, 54L,
64L, 55L, 70L, 62L, 68L, 59L, 61L, 66L, 60L, 60L, 66L, 57L, 64L,
62L, 65L, 73L, 72L, 55L, 58L, 71L, 55L, 64L, 52L, 73L, 74L, 61L,
75L, 54L, 51L, 63L, 72L, 45L, 71L, 61L, 82L, 57L, 62L, 67L, 60L,
69L, 53L, 60L, 66L, 55L, 59L, 76L, 69L, 42L, 64L, 54L, 54L, 75L,
63L, 62L, 60L, 61L, 79L, 54L, 68L, 57L, 58L, 61L, 64L, 68L, 63L,
58L, 60L, 76L, 67L, 74L, 65L, 65L, 55L, 56L, 54L, 49L, 64L, 50L,
58L, 73L, 44L, 52L, 62L, 76L, 61L, 67L, 68L, 59L, 55L, 67L, 63L,
53L, 51L, 57L, 77L, 49L, 66L, 56L, 43L, 56L, 68L, 77L, 76L, 62L,
68L, 79L, 66L, 66L, 79L, 75L, 78L, 83L, 62L, 59L, 74L, 55L, 53L,
60L, 62L, 56L, 59L, 64L, 71L, 65L, 45L, 70L, 51L, 61L, 51L, 76L,
48L, 53L, 66L, 69L, 71L, 59L, 49L, 67L, 68L, 45L, 69L, 61L, 64L,
64L, 58L, 63L, 56L, 51L, 53L, 49L, 74L, 70L, 56L, 69L, 66L, 61L,
57L, 49L, 61L, 58L, 49L, 41L, 63L, 69L, 69L, 52L, 61L, 80L, 53L,
69L, 63L, 59L, 57L, 59L, 67L, 69L, 71L, 52L, 58L, 62L, 65L, 72L,
64L, 65L, 59L, 72L, 52L, 49L, 64L, 64L, 56L, 66L, 82L, 80L, 89L,
59L, 64L, 56L, 74L, 59L, 84L, 59L, 59L, 50L, 54L, 59L, 57L, 54L,
71L, 58L, 59L, 60L, 58L, 54L, 57L, 62L, 71L, 58L, 59L, 45L, 67L,
74L, 69L, 52L, 65L, 53L, 60L, 59L, 57L, 68L, 69L, 54L, 47L, 93L,
62L, 66L, 59L, 56L, 51L, 60L, 63L, 69L, 56L, 57L, 59L, 60L, 69L,
92L, 86L, 69L, 54L, 58L, 78L, 66L, 59L, 83L, 88L, 96L, 79L, 71L,
75L, 76L, 54L, 67L, 69L, 59L, 59L, 57L, 56L, 50L, 63L, 71L, 60L,
67L, 64L, 71L, 57L, 74L, 64L, 62L, 57L, 60L, 55L, 56L, 59L, 62L,
57L, 67L, 63L, 61L, 58L, 62L, 59L, 82L, 61L, 50L, 49L, 59L, 65L,
68L, 63L, 64L, 50L, 67L, 69L, 44L, 66L, 60L, 80L, 66L, 60L, 56L,
53L, 50L, 76L, 68L, 47L, 49L, 75L, 54L, 56L, 59L, 69L, 49L, 66L,
73L, 62L, 65L, 70L, 57L, 75L, 71L, 59L, 47L, 60L, 74L, 55L, 50L,
65L, 55L, 70L, 50L, 61L, 63L, 71L, 69L, 64L, 68L, 74L, 66L, 52L,
61L, 65L, 50L, 63L, 75L, 60L, 59L, 69L, 61L, 59L, 60L, 70L, 66L,
58L, 53L, 62L, 67L, 62L, 69L, 56L, 52L, 53L, 52L, 74L, 75L, 61L,
60L, 75L, 64L, 69L, 73L, 57L, 72L, 56L, 58L, 60L, 66L, 74L, 69L,
62L, 70L, 79L, 66L, 74L, 83L, 80L, 53L, 69L, 61L, 71L, 66L, 55L,
54L, 66L, 73L, 69L, 60L, 61L, 65L, 62L, 73L, 56L, 74L, 60L, 54L,
71L, 67L, 53L, 46L, 61L, 52L, 53L, 78L, 50L, 46L, 64L, 62L, 48L,
68L, 60L, 55L, 58L, 70L, 66L, 58L, 50L, 60L, 66L, 58L, 71L, 59L,
56L, 58L, 61L, 56L, 50L, 70L, 56L, 66L, 69L, 57L, 64L, 61L, 54L,
60L, 61L, 66L, 52L, 52L, 70L, 71L, 53L, 64L, 72L, 60L, 73L, 63L,
71L, 59L, 53L, 57L, 46L, 70L, 56L, 63L, 53L, 73L, 75L, 56L, 48L,
64L, 63L, 53L, 61L, 75L, 58L, 67L, 77L, 67L, 64L, 58L, 70L, 86L,
66L, 64L, 70L, 63L, 77L, 63L, 68L, 56L, 78L, 62L, 78L, 61L, 69L,
51L, 68L, 51L, 46L, 60L, 60L, 70L, 63L, 75L, 64L, 65L, 66L, 69L,
70L, 63L, 71L, 49L, 75L, 75L, 64L, 72L, 73L, 55L, 54L, 55L, 65L,
72L, 66L, 50L, 61L, 52L, 60L, 72L, 77L, 55L, 73L, 69L, 76L, 44L,
58L, 57L, 47L, 54L, 64L, 69L, 56L, 49L, 87L, 85L, 61L, 49L, 65L,
87L, 69L, 62L, 59L, 63L, 63L, 54L, 60L, 66L, 82L, 55L, 52L, 66L,
61L, 65L, 50L, 45L, 55L, 75L, 48L, 58L, 73L, 64L, 62L, 61L, 59L,
65L, 70L, 61L, 61L, 65L, 52L, 63L, 65L, 57L, 61L, 65L, 61L, 67L,
60L, 69L, 66L, 56L, 79L, 81L, 54L, 73L, 69L, 63L, 67L, 67L, 60L,
57L, 67L, 62L, 69L, 57L, 57L, 70L, 53L, 69L, 65L, 59L, 57L, 51L,
64L, 48L, 66L, 69L, 73L, 54L, 54L, 60L, 49L, 55L, 63L, 65L, 69L,
65L, 54L, 72L, 64L, 83L, 71L, 65L, 61L, 64L, 67L, 68L, 72L, 81L,
51L, 60L, 71L, 46L, 59L, 60L, 52L, 67L, 70L, 70L, 63L, 56L, 57L,
66L, 72L, 61L, 64L, 62L, 56L, 76L, 51L, 66L, 68L, 51L, 71L, 65L,
74L, 59L, 48L, 58L, 69L, 47L, 60L, 65L, 68L, 58L, 65L, 44L, 57L,
53L, 57L, 61L, 60L, 78L, 63L, 54L, 66L, 63L, 59L, 71L, 62L, 54L,
70L, 84L, 81L, 71L, 66L, 67L, 107L, 88L, 97L, 95L, 111L, 99L,
87L, 97L, 92L, 81L, 66L, 73L, 66L, 59L, 53L, 64L, 57L, 64L, 75L,
65L, 58L, 66L, 59L, 57L, 64L, 54L, 61L, 74L, 59L, 59L, 67L, 73L,
70L, 66L, 52L, 64L, 74L, 61L, 69L, 64L, 72L, 55L, 70L, 55L, 68L,
73L, 58L, 62L, 47L, 65L, 56L, 63L, 61L, 71L, 64L, 50L, 66L, 59L,
59L, 74L, 61L, 51L, 73L, 55L, 73L, 57L, 66L, 67L, 73L, 52L, 55L,
57L, 60L, 69L, 78L, 85L, 38L, 61L, 57L, 74L, 67L, 77L, 69L, 73L,
53L, 64L, 66L, 43L, 59L, 62L, 56L, 69L, 65L, 55L, 50L, 61L, 72L,
67L, 66L, 87L, 67L, 55L, 60L, 75L, 62L, 77L, 63L, 58L, 63L, 73L,
58L, 70L, 62L, 82L, 55L, 59L, 64L, 63L, 59L, 59L, 54L, 65L, 71L,
65L, 63L, 60L, 65L, 60L, 78L, 60L, 54L, 53L, 66L, 72L, 77L, 67L,
54L, 68L, 65L, 59L, 68L, 57L, 63L, 75L, 49L, 58L, 59L, 58L, 74L,
74L, 64L, 59L, 53L, 51L, 62L, 47L, 55L, 65L, 60L, 71L, 58L, 73L,
53L, 68L, 69L, 42L, 47L, 54L, 60L, 65L, 69L, 56L, 65L, 51L, 65L,
46L, 83L, 45L, 76L, 60L, 62L, 68L, 70L, 64L, 64L, 62L, 70L, 63L,
45L, 68L, 57L, 60L, 67L, 50L, 43L, 66L, 49L, 53L, 47L, 59L, 61L,
60L, 54L, 58L, 48L, 65L, 77L, 52L, 63L, 74L, 67L, 53L, 81L, 58L,
63L, 52L, 61L, 66L, 55L, 55L, 58L, 58L, 58L, 56L, 63L, 55L, 71L,
77L, 74L, 60L, 66L, 60L, 93L, 71L, 55L, 79L, 62L, 61L, 62L, 58L,
64L, 71L, 61L, 66L, 81L, 77L, 65L, 67L, 52L, 77L, 53L, 63L, 59L,
58L, 55L, 58L, 71L, 64L, 63L, 66L, 69L, 71L, 78L, 73L, 62L, 70L,
45L, 49L, 66L, 57L, 68L, 67L, 65L, 69L, 67L, 75L, 58L, 72L, 64L,
42L, 55L, 53L, 69L, 53L, 49L, 63L, 62L, 55L, 59L, 62L, 51L, 69L,
64L, 72L, 65L, 61L, 64L, 52L, 72L, 53L, 62L, 64L, 63L, 64L, 62L,
70L, 64L, 61L, 62L, 55L, 53L, 62L, 51L, 53L, 72L, 66L, 72L, 63L,
55L, 60L, 66L, 68L, 70L, 64L, 62L, 50L, 42L, 68L, 58L, 56L, 68L,
57L, 54L, 58L, 63L, 71L, 50L, 69L, 54L, 61L, 68L, 67L, 71L, 47L,
61L, 51L, 66L, 68L, 60L, 58L, 75L, 43L, 59L, 51L, 53L, 73L, 58L,
66L, 47L, 61L, 67L, 60L, 67L, 78L, 71L, 64L, 80L, 59L, 85L, 74L,
52L, 56L, 68L, 58L, 55L, 68L, 57L, 64L, 51L, 56L, 54L, 61L, 53L,
62L, 67L, 71L, 74L, 53L, 58L, 53L, 64L, 58L, 74L, 61L, 65L, 62L,
62L, 68L, 72L, 76L, 84L, 75L, 69L, 71L, 66L, 63L, 51L, 60L, 65L,
71L, 80L, 58L, 74L, 68L, 60L, 58L, 55L, 60L, 61L, 73L, 54L, 57L,
60L, 58L, 65L, 71L, 52L, 76L, 61L, 74L, 80L, 62L, 73L, 72L, 69L,
72L, 71L, 69L, 72L, 60L, 62L, 72L, 62L, 69L, 58L, 63L, 66L, 64L,
76L, 55L, 57L, 55L, 64L, 48L, 68L, 65L, 63L, 65L, 77L, 62L, 54L,
57L, 63L, 64L, 57L, 55L, 59L, 69L, 81L, 64L, 83L, 71L, 73L, 89L,
77L, 83L, 68L, 49L, 58L, 57L, 67L, 50L, 57L, 69L, 50L, 63L, 68L,
71L, 73L, 66L, 74L, 59L, 67L, 59L, 46L, 58L, 63L, 63L, 54L, 57L,
52L, 70L, 66L, 74L, 69L, 62L, 72L, 50L, 64L, 77L, 55L, 63L, 75L,
56L, 51L, 75L, 61L, 46L, 69L, 70L, 65L, 54L, 49L, 76L, 55L, 54L,
52L, 66L, 60L, 64L, 56L, 65L, 63L, 65L, 67L, 51L, 76L, 70L, 61L,
68L, 64L, 57L, 79L, 75L, 69L, 62L, 65L, 58L, 65L, 73L, 67L, 46L,
59L, 85L, 72L, 58L, 62L, 52L, 61L, 59L, 71L, 73L, 56L, 67L, 54L,
64L, 66L, 55L, 62L, 71L, 49L, 52L, 59L, 78L, 55L, 83L, 69L, 64L,
64L, 59L, 58L, 51L, 66L, 68L, 76L, 80L, 67L, 61L, 62L, 65L, 49L,
71L, 60L, 59L, 52L, 62L, 58L, 54L, 64L, 62L, 61L, 74L, 61L, 60L,
54L, 66L, 59L, 79L, 64L, 58L, 62L, 54L, 62L, 51L, 65L, 59L, 84L,
60L, 56L, 72L, 63L, 60L, 67L, 65L, 68L, 52L, 47L, 53L, 53L, 74L,
46L, 53L, 59L, 78L, 50L, 60L, 61L, 69L, 61L, 66L, 66L, 60L, 50L,
59L, 67L, 64L, 53L, 63L, 65L, 70L, 63L, 70L, 57L, 51L, 56L, 74L,
49L, 73L, 60L, 56L, 64L, 77L, 55L, 62L, 66L, 65L, 55L, 69L, 46L,
56L, 57L, 59L, 62L, 53L, 55L, 70L, 57L, 75L, 52L, 56L, 52L, 61L,
81L, 57L, 73L, 68L, 59L, 61L, 78L, 68L, 48L, 66L, 64L, 65L, 68L,
71L, 49L, 52L, 66L, 69L, 75L, 60L, 53L, 55L, 53L, 69L, 75L, 70L,
65L, 73L, 68L, 58L, 64L, 78L, 61L, 75L, 62L, 67L, 75L, 66L, 66L,
76L, 45L, 62L, 64L, 74L, 81L, 75L, 72L, 58L, 57L, 45L, 63L, 61L,
70L, 52L, 63L, 59L, 80L, 51L, 72L, 64L, 59L, 57L, 64L, 41L, 60L,
54L, 59L, 72L, 60L, 47L, 55L, 61L, 62L, 60L, 63L, 60L, 67L, 78L,
69L, 59L, 74L, 63L, 78L, 62L, 69L, 73L, 66L, 75L, 61L, 67L, 65L,
58L, 57L, 66L, 52L, 56L, 69L, 67L, 62L, 53L, 67L, 51L, 62L, 61L,
69L, 68L, 66L, 59L, 60L, 64L, 58L, 65L, 67L, 70L, 43L, 49L, 68L,
65L, 53L, 50L, 74L, 54L, 49L, 64L, 89L, 74L, 65L, 60L, 61L, 61L,
62L, 66L, 59L, 60L, 57L, 52L, 61L, 61L, 56L, 52L, 60L, 55L, 66L,
60L, 59L, 55L, 54L, 69L, 59L, 54L, 69L, 73L, 50L, 65L, 67L, 78L,
71L, 59L, 66L, 48L, 51L, 68L, 59L, 73L, 68L, 66L, 64L, 57L, 62L,
66L, 52L, 78L, 73L, 63L, 65L, 77L, 73L, 74L, 66L, 70L, 71L, 64L,
84L, 75L, 54L, 64L, 59L, 76L, 60L, 59L, 70L, 72L, 56L, 74L, 62L,
63L, 66L, 69L, 64L, 62L, 73L, 62L, 71L, 68L, 53L, 61L, 57L, 53L,
57L, 57L, 53L, 72L, 78L, 59L, 61L, 54L, 72L, 64L, 72L, 67L, 61L,
67L, 52L, 70L, 68L, 61L, 68L, 40L, 69L, 53L, 56L, 74L, 64L, 67L,
76L, 77L, 66L, 72L, 82L, 81L, 54L, 52L, 57L, 55L, 73L, 67L, 62L,
70L, 73L, 59L, 52L, 60L, 68L, 64L, 76L, 72L, 69L, 67L, 54L, 68L,
62L, 63L, 53L, 55L, 61L, 59L, 70L, 62L, 76L, 58L, 57L, 72L, 71L,
68L, 76L, 67L, 59L, 54L, 67L, 65L, 73L, 84L, 52L, 58L, 73L, 62L,
69L, 70L, 62L, 64L, 79L, 73L, 65L, 52L, 67L, 66L, 62L, 50L, 62L,
60L, 64L, 78L, 70L, 69L, 58L, 72L, 57L, 53L, 64L, 64L, 54L, 70L,
63L, 63L, 77L, 68L, 52L, 73L, 59L, 57L, 71L, 77L, 66L, 80L, 63L,
67L, 69L, 61L, 58L, 68L, 61L, 49L, 68L, 65L, 46L, 57L, 82L, 67L,
79L, 53L, 57L, 73L, 66L, 73L, 60L, 54L, 68L, 76L, 60L, 70L, 61L,
69L, 56L, 64L, 82L, 53L, 58L, 67L, 72L, 70L, 77L, 70L, 63L, 66L,
51L, 63L, 59L, 63L, 66L, 66L, 63L, 64L, 61L, 73L, 66L, 58L, 75L,
54L, 53L, 68L, 63L, 58L, 57L, 62L, 57L, 76L, 76L, 70L, 68L, 67L,
63L, 68L, 65L, 63L, 75L, 58L, 58L, 54L, 67L, 64L, 64L, 58L, 57L,
53L, 83L, 66L, 69L, 54L, 62L, 45L, 56L, 71L, 61L, 60L, 58L, 77L,
63L, 55L, 68L, 64L, 53L, 59L, 61L, 47L, 65L, 67L, 52L, 59L, 62L,
50L, 66L, 56L, 69L, 70L, 77L, 64L, 63L, 69L, 56L, 51L, 60L, 67L,
59L, 72L, 56L, 77L, 76L, 72L, 72L, 66L, 56L, 68L, 57L, 49L, 69L,
57L, 56L, 69L, 63L, 65L, 75L, 53L, 73L, 66L, 63L, 53L, 77L, 64L,
60L, 77L, 62L, 61L, 71L, 61L, 63L, 59L, 57L, 64L, 69L, 75L, 72L,
57L, 61L, 65L, 91L, 60L, 81L, 83L, 57L, 52L, 65L, 56L, 78L, 62L,
56L, 68L, 67L, 72L, 44L, 62L, 61L, 68L, 72L, 76L, 70L, 69L, 63L,
65L, 67L, 68L, 61L, 74L, 58L, 61L, 75L, 68L, 54L, 58L, 48L, 63L,
71L, 63L, 63L, 71L, 55L, 59L, 77L, 64L, 68L, 75L, 61L, 69L, 66L,
58L, 63L, 53L, 54L, 58L, 73L, 50L, 78L, 64L, 63L, 59L, 56L, 47L,
65L, 63L, 80L, 56L, 55L, 66L, 48L, 59L, 54L, 56L, 55L, 64L, 61L,
61L, 58L, 74L, 51L, 60L, 61L, 56L, 64L, 54L, 54L, 52L, 61L, 51L,
58L, 50L, 56L, 48L, 67L, 73L, 42L, 79L, 68L, 72L, 59L, 75L, 68L,
53L, 55L, 63L, 59L, 66L, 64L, 55L, 59L, 64L, 65L, 60L, 65L, 70L,
69L, 37L, 57L, 66L, 63L, 70L, 76L, 74L, 78L, 59L, 67L, 60L, 57L,
58L, 73L, 65L, 75L, 55L, 63L, 84L, 48L, 60L, 41L, 69L, 57L, 59L,
72L, 65L, 63L, 55L, 70L, 69L, 65L, 62L, 59L, 63L, 65L, 63L, 73L,
58L, 66L, 59L, 71L, 60L, 76L, 63L, 62L, 69L, 59L, 65L, 87L, 66L,
63L, 60L, 62L, 58L, 62L, 50L, 59L, 58L, 72L, 65L, 67L, 63L, 64L,
56L, 59L, 70L, 62L, 55L, 69L, 62L, 52L, 64L, 64L, 69L, 68L, 73L,
55L, 63L, 57L, 60L, 66L, 68L, 73L, 74L, 66L, 59L, 64L, 62L, 62L,
65L, 56L, 53L, 73L, 55L, 58L, 66L, 78L, 62L, 53L, 47L, 67L, 56L,
66L, 61L, 63L, 66L, 64L, 59L, 70L, 55L, 60L, 57L, 43L, 56L, 60L,
59L, 60L, 57L, 63L, 55L, 76L, 59L, 80L, 53L, 69L, 81L, 50L, 64L,
58L, 74L, 64L, 74L, 41L, 61L, 63L, 48L, 44L, 50L, 56L, 66L, 60L,
72L, 57L, 70L, 80L, 78L, 68L, 57L, 62L, 69L, 66L, 58L, 51L, 62L,
54L, 62L, 70L, 53L, 52L, 65L, 67L, 38L, 56L, 61L, 78L, 62L, 75L,
65L, 66L, 53L, 65L, 55L, 53L, 66L, 66L, 65L, 71L, 67L, 79L, 73L,
64L, 66L, 67L, 63L, 75L, 66L, 65L, 57L, 66L, 62L, 72L, 63L, 63L,
61L, 61L, 56L, 72L, 66L, 75L, 60L, 78L, 63L, 57L, 85L, 76L, 70L,
53L, 57L, 77L, 51L, 68L, 70L, 72L, 60L, 64L, 70L, 56L, 62L, 68L,
75L, 75L, 70L, 57L, 75L, 73L, 66L, 67L, 56L, 56L, 68L, 73L, 68L,
63L, 77L, 67L, 50L, 60L, 71L, 60L, 58L, 68L, 63L, 71L, 69L, 62L,
69L, 62L, 55L, 63L, 67L, 61L, 61L, 71L, 54L, 47L, 59L, 57L, 82L,
61L, 66L, 74L, 50L, 58L, 65L, 76L, 58L, 59L, 63L, 66L, 65L, 46L,
59L, 50L, 70L, 59L, 47L, 61L, 60L, 72L, 71L, 62L, 61L, 53L, 64L,
60L, 56L, 58L, 67L, 58L, 82L, 68L, 56L, 55L, 45L, 52L, 74L, 65L,
64L, 66L, 60L, 55L, 68L, 64L, 59L, 41L, 56L, 76L, 81L, 71L, 51L,
71L, 57L, 68L, 53L, 74L, 66L, 62L, 65L, 52L, 69L, 55L, 57L, 36L,
61L, 57L, 66L, 65L, 68L, 52L, 57L, 71L, 68L, 62L, 61L, 64L, 68L,
61L, 64L, 67L, 51L, 59L, 63L, 68L, 75L, 73L, 74L, 71L, 73L, 53L,
63L, 51L, 75L, 55L, 61L, 73L, 74L, 53L, 73L, 68L, 60L, 78L, 73L,
59L, 56L, 67L, 63L, 81L, 59L, 70L, 58L, 69L, 62L, 52L, 57L, 63L,
73L, 78L, 57L, 64L, 59L, 71L, 80L, 65L, 66L, 65L, 60L, 56L, 61L,
66L, 41L, 52L, 64L, 74L, 71L, 63L, 56L, 66L, 64L, 77L, 87L, 60L,
62L, 70L, 67L, 70L, 72L, 49L, 78L, 64L, 55L, 74L, 64L, 57L, 62L,
60L, 74L, 58L, 66L, 66L, 59L, 61L, 58L, 64L, 58L, 61L, 70L, 82L,
56L, 68L, 60L, 72L, 66L, 56L, 60L, 53L, 52L, 57L, 58L, 69L, 59L,
61L, 85L, 64L, 57L, 70L, 82L, 57L, 68L, 54L, 57L, 60L, 53L, 67L,
73L, 56L, 64L, 66L, 66L, 56L, 51L, 65L, 86L, 66L, 61L, 62L, 41L,
59L, 52L, 66L, 60L, 61L, 52L, 61L, 65L, 74L, 65L, 57L, 60L, 70L,
66L, 54L, 60L, 70L, 81L, 75L, 70L, 51L, 67L, 58L, 64L, 73L, 60L,
62L, 64L, 67L, 79L, 60L, 68L, 60L, 75L, 69L, 71L, 67L, 79L, 57L,
65L, 67L, 68L, 55L, 79L, 70L, 77L, 63L, 61L, 68L, 63L, 62L, 76L,
52L, 64L, 70L, 62L, 63L, 72L, 62L, 71L, 75L, 83L, 70L, 57L, 67L,
83L, 53L, 58L, 69L, 63L, 64L, 50L, 70L, 65L, 62L, 66L, 67L, 59L,
62L, 71L, 57L, 40L, 77L, 70L, 61L, 64L, 57L, 75L, 62L, 62L, 77L,
67L, 76L, 68L, 69L, 52L, 62L, 63L, 67L, 51L, 69L, 57L, 64L, 55L,
64L, 74L, 64L, 61L, 73L, 66L, 62L, 62L, 64L, 74L, 79L, 58L, 73L,
70L, 66L, 68L, 82L, 62L, 62L, 42L, 52L, 60L, 61L, 71L, 84L, 64L,
54L, 76L, 62L, 65L, 62L, 67L, 63L, 63L, 53L, 60L, 63L, 67L, 64L,
65L, 66L, 61L, 65L, 58L, 55L, 56L, 60L, 61L, 51L, 61L, 59L, 60L,
60L, 60L, 67L, 61L, 64L, 48L, 65L, 61L, 49L, 62L, 58L, 74L, 60L,
70L, 65L, 60L, 80L, 65L, 65L, 61L, 75L, 69L, 68L, 67L, 73L, 61L,
57L, 64L, 67L, 70L, 72L, 73L, 61L, 61L, 62L, 69L, 57L, 70L, 65L,
65L, 79L, 74L, 64L, 64L, 70L, 76L, 67L, 81L, 69L, 68L, 74L, 63L,
62L, 65L, 61L, 64L, 58L, 52L, 74L, 74L, 61L, 60L, 60L, 75L, 73L,
63L, 69L, 61L, 65L, 73L, 59L, 53L, 57L, 64L, 63L, 61L, 67L, 72L,
57L, 76L, 72L, 72L, 72L, 48L, 70L, 70L, 52L, 71L, 58L, 86L, 59L,
62L, 67L, 63L, 65L, 78L, 65L, 58L, 65L, 81L, 74L, 60L, 87L, 58L,
69L, 49L, 59L, 82L, 70L, 61L, 64L, 63L, 55L, 64L, 66L, 63L, 78L,
47L, 75L, 68L, 65L, 75L, 63L, 71L, 54L, 51L, 64L, 55L, 56L, 51L,
52L, 63L, 62L, 59L, 67L, 76L, 58L, 75L, 69L, 67L, 64L, 68L, 79L,
57L, 62L, 58L, 63L, 70L, 84L, 73L, 69L, 74L, 61L, 56L, 68L, 87L,
65L, 62L, 73L, 78L, 65L, 63L, 64L, 62L, 75L, 71L, 55L, 64L, 82L,
69L, 73L, 69L, 78L, 73L, 62L, 76L, 65L, 68L, 76L, 80L, 57L, 66L,
62L, 70L, 84L, 71L, 65L, 45L, 59L, 54L, 75L, 71L, 61L, 76L, 51L,
63L, 75L, 82L, 57L, 62L, 73L, 54L, 57L, 67L, 57L, 66L, 73L, 65L,
64L, 67L, 70L, 75L, 72L, 64L, 62L, 77L, 69L, 64L, 64L, 65L, 69L,
61L, 64L, 59L, 84L, 65L, 60L, 69L, 59L, 61L, 66L, 68L, 63L, 66L,
86L, 71L, 71L, 53L, 60L, 67L, 70L, 68L, 67L, 84L, 66L, 58L, 85L,
58L, 63L, 68L, 67L, 71L, 67L, 59L, 77L, 62L, 83L, 69L, 67L, 58L,
69L, 85L, 74L, 69L, 63L, 61L, 65L, 59L, 73L, 66L, 67L, 75L, 56L,
78L, 68L, 66L, 59L, 55L, 73L, 61L, 69L, 67L, 71L, 59L, 56L, 73L,
62L, 60L, 62L, 74L, 62L, 81L, 80L, 76L, 70L, 60L, 49L, 69L, 64L,
63L, 75L, 61L, 63L, 70L, 69L, 47L, 73L, 67L, 63L, 76L, 65L, 79L,
64L, 62L, 69L, 78L, 74L, 74L, 67L, 88L, 66L, 65L, 74L, 69L, 64L,
82L, 61L, 62L, 71L, 75L, 57L, 74L, 54L, 74L, 70L, 71L, 59L, 56L,
63L, 71L, 74L, 71L, 60L, 77L, 61L, 66L, 63L, 58L, 60L, 59L, 74L,
69L, 82L, 81L, 82L, 68L, 62L, 73L, 63L, 69L, 65L, 69L, 61L, 39L,
59L, 59L, 79L, 67L, 70L, 70L, 64L, 79L, 61L, 66L, 60L, 63L, 77L,
60L, 54L, 62L, 62L, 63L, 65L, 62L, 68L, 63L, 65L, 72L, 68L, 74L,
56L, 72L, 68L, 76L, 69L, 77L, 73L, 85L, 75L, 76L, 64L, 72L, 59L,
63L, 60L, 74L, 67L, 66L, 71L, 66L, 61L, 62L, 72L, 73L, 63L, 69L,
67L, 78L, 71L, 84L, 71L, 55L, 64L, 80L, 69L, 72L, 61L, 66L, 70L,
61L, 65L, 70L, 70L, 80L, 71L, 73L, 57L, 69L, 72L, 52L, 75L, 57L,
69L, 54L, 69L, 58L, 68L, 61L, 62L, 72L, 83L, 76L, 73L, 71L, 62L,
74L, 69L, 68L, 70L, 59L, 67L, 60L, 55L, 71L, 65L, 68L, 55L, 67L,
71L, 83L, 81L, 63L, 77L, 78L, 71L, 60L, 59L, 70L, 56L, 62L, 69L,
82L, 65L, 82L, 63L, 56L, 63L, 77L, 54L, 46L, 77L, 82L, 58L, 61L,
89L, 50L, 73L, 67L, 76L, 57L, 74L, 60L, 63L, 65L, 59L, 74L, 60L,
57L, 80L, 66L, 73L, 60L, 79L, 64L, 76L, 50L, 84L, 62L, 72L, 79L,
70L, 65L, 61L, 73L, 68L, 69L, 68L, 55L, 65L, 56L, 68L, 60L, 58L,
51L, 79L, 61L, 73L, 81L, 67L, 63L, 68L, 55L, 53L, 55L, 73L, 66L,
61L, 66L, 81L, 69L, 61L, 65L, 70L, 65L, 71L, 49L, 55L, 64L, 66L,
56L, 77L, 70L, 65L, 65L, 64L, 38L, 64L, 79L, 53L, 71L, 63L, 70L,
55L, 54L, 75L, 59L, 69L, 61L, 71L, 56L, 64L, 72L, 72L, 53L, 65L,
70L, 75L, 57L, 66L, 56L, 76L, 69L, 49L, 54L, 60L, 69L, 54L, 68L,
61L, 67L, 76L, 54L, 56L, 65L, 57L, 65L, 65L, 63L, 69L, 63L, 63L,
63L, 80L, 66L, 67L, 66L, 62L, 68L, 76L, 68L, 73L, 59L, 77L, 62L,
56L, 44L, 61L, 57L, 60L, 75L, 62L, 66L, 74L, 76L, 81L, 83L, 68L,
59L, 73L, 67L, 73L, 65L, 65L, 79L, 61L, 73L, 71L, 63L, 54L, 61L,
74L, 91L, 64L, 77L, 76L, 63L, 64L, 59L, 68L, 75L, 76L, 75L, 60L,
68L, 67L, 86L, 76L, 64L, 68L, 81L, 75L, 69L, 71L, 64L, 73L, 75L,
70L, 65L, 65L, 72L, 61L, 77L, 49L, 79L, 58L, 66L, 69L, 63L, 60L,
69L, 63L, 70L, 67L, 64L, 65L, 67L, 81L, 84L, 65L, 63L, 74L, 67L,
55L, 62L, 55L, 71L, 68L, 71L, 61L, 72L, 70L, 59L, 77L, 65L, 70L,
82L, 77L, 69L, 52L, 65L, 60L, 76L, 79L, 67L, 41L, 73L, 71L, 64L,
92L, 51L, 60L, 58L, 67L, 68L, 50L, 61L, 67L, 84L, 63L, 76L, 60L,
80L, 58L, 68L, 68L, 62L, 62L, 68L, 59L, 66L, 70L, 48L, 63L, 64L,
65L, 59L, 67L, 59L, 73L, 56L, 71L, 64L, 69L, 58L, 70L, 81L, 73L,
72L, 76L, 58L, 54L, 62L, 70L, 78L, 74L, 53L, 55L, 64L, 65L, 73L,
59L, 76L, 68L, 60L, 89L, 61L, 70L, 62L, 85L, 66L, 65L, 59L, 63L,
71L, 63L, 85L, 66L, 60L, 62L, 73L, 65L, 71L, 74L, 72L, 70L, 70L,
82L, 73L, 60L, 62L, 78L, 66L, 66L, 60L, 66L, 72L, 74L, 73L, 77L,
67L, 61L, 72L, 77L, 65L, 69L, 76L, 64L, 70L, 74L, 65L, 54L, 66L,
74L, 62L, 67L, 65L, 54L, 61L, 58L, 60L, 65L, 66L, 79L, 74L, 67L,
59L, 53L, 60L, 63L, 58L, 57L, 63L, 86L, 60L, 69L, 82L, 55L, 69L,
54L, 93L, 68L, 65L, 57L, 76L, 54L, 66L, 63L, 57L, 68L, 68L, 71L,
75L, 66L, 63L, 54L, 74L, 72L, 89L, 55L, 63L, 72L, 63L, 66L, 63L,
76L, 71L, 68L, 84L, 64L, 58L, 87L, 76L, 61L, 80L, 81L, 62L, 85L,
58L, 72L, 67L, 66L, 70L, 62L, 63L, 79L, 93L, 47L, 84L, 60L, 60L,
75L, 72L, 74L, 75L, 66L, 84L, 70L, 70L, 63L, 61L, 64L, 55L, 74L,
72L, 72L, 79L, 81L, 76L, 80L, 72L, 63L, 65L, 62L, 73L, 74L, 64L,
56L, 68L, 76L, 69L, 91L, 72L, 57L, 70L, 59L, 68L, 52L, 62L, 81L,
59L, 64L, 69L, 61L, 78L, 82L, 75L, 84L, 73L, 67L, 60L, 68L, 75L,
79L, 79L, 58L, 81L, 66L, 59L, 56L, 75L, 50L, 72L, 64L, 67L, 77L,
52L, 70L, 62L, 69L, 72L, 67L, 75L, 77L, 74L, 60L, 70L, 68L, 74L,
80L, 61L, 66L, 58L, 70L, 59L, 61L, 60L, 64L, 71L, 59L, 65L, 52L,
68L, 69L, 73L, 63L, 62L, 76L, 67L, 60L, 54L, 72L, 67L, 71L, 72L,
69L, 73L, 69L, 78L, 71L, 73L, 77L, 75L, 64L, 70L, 81L, 68L, 53L,
76L, 61L, 68L, 64L, 74L, 56L, 54L, 70L, 59L, 70L, 71L, 66L, 58L,
60L, 71L, 68L, 56L, 71L, 56L, 66L, 70L, 62L, 75L, 61L, 66L, 59L,
63L, 71L, 62L, 59L, 61L, 64L, 74L, 73L, 62L, 74L, 68L, 72L, 44L,
65L, 55L, 56L, 82L, 71L, 70L, 68L, 56L, 75L, 81L, 67L, 59L, 72L,
66L, 78L, 75L, 63L, 54L, 59L, 69L, 60L, 80L, 48L, 67L, 78L, 56L,
56L, 58L, 72L, 76L, 62L, 70L, 59L, 65L, 63L, 67L, 74L, 72L, 60L,
81L, 63L, 68L, 68L, 76L, 65L, 69L, 74L, 58L, 75L, 67L, 63L, 66L,
61L, 62L, 71L, 74L, 70L, 67L, 62L, 72L, 54L, 66L, 49L, 63L, 60L,
69L, 63L, 74L, 48L, 57L, 53L, 51L, 70L, 71L, 58L, 67L, 50L, 85L,
55L, 66L, 58L, 63L, 70L, 62L, 54L, 63L, 58L, 80L, 63L, 60L, 72L,
70L, 69L, 75L, 59L, 69L, 64L, 65L, 63L, 59L, 68L, 62L, 61L, 64L,
67L, 69L, 65L, 67L, 57L, 63L, 59L, 73L, 57L, 69L, 58L, 71L, 76L,
74L, 56L, 74L, 76L, 66L, 69L, 56L, 64L, 64L, 66L, 81L, 55L, 84L,
59L, 73L, 75L, 70L, 72L, 73L, 70L, 74L, 74L, 73L, 65L, 72L, 63L,
63L, 76L, 70L, 54L, 60L, 57L, 61L, 70L, 70L, 97L, 73L, 74L, 78L,
59L, 78L, 68L, 83L, 67L, 58L, 89L, 55L, 71L, 66L, 77L, 76L, 63L,
65L, 73L, 65L, 70L, 85L, 68L, 69L, 56L, 71L, 77L, 60L, 71L, 67L,
68L, 86L, 62L, 68L, 56L, 69L, 80L, 60L, 61L, 69L, 76L, 79L, 68L,
59L, 76L, 70L, 58L, 76L, 77L, 82L, 91L, 80L, 85L, 80L, 60L, 62L,
81L, 81L, 78L, 67L, 70L, 57L, 60L, 75L, 72L, 66L, 63L, 82L, 60L,
68L, 81L, 80L, 67L, 82L, 58L, 73L, 72L, 77L, 73L, 74L, 65L, 58L,
74L, 68L, 77L, 81L, 57L, 62L, 70L, 90L, 67L, 74L, 69L, 71L, 66L,
105L, 84L, 56L, 69L, 53L, 63L, 66L, 65L, 66L, 72L, 72L, 50L,
68L, 68L, 66L, 63L, 70L, 54L, 52L, 59L, 64L, 60L, 73L, 52L, 65L,
76L, 62L, 55L, 71L, 74L, 68L, 64L, 67L, 51L, 91L, 70L, 68L, 68L,
66L, 61L, 68L, 63L, 73L, 69L, 57L, 69L, 59L, 54L, 63L, 72L, 83L,
63L, 62L, 73L, 73L, 65L, 57L, 91L, 67L, 81L, 61L, 70L, 53L, 65L,
77L, 67L, 67L, 47L, 78L, 68L, 69L, 67L, 73L, 65L, 75L, 77L, 75L,
56L, 66L, 58L, 62L, 77L, 74L, 81L, 71L, 70L, 74L, 80L, 57L, 75L,
76L, 70L, 50L, 69L, 65L, 84L, 65L, 76L, 72L, 83L, 74L, 74L, 75L,
85L, 58L, 72L, 74L, 70L, 73L, 81L, 59L, 65L, 58L, 75L, 68L, 62L,
69L, 50L, 58L, 66L, 89L, 68L, 58L, 72L, 70L, 79L, 62L, 75L, 68L,
59L, 57L, 61L, 62L, 77L, 70L, 78L, 70L, 71L, 67L, 56L, 55L, 72L,
64L, 69L, 76L, 70L, 54L, 73L, 83L, 76L, 65L, 62L, 80L, 68L, 61L,
77L, 67L, 75L, 63L, 49L, 77L, 79L, 96L, 57L, 62L, 69L, 82L, 64L,
65L, 68L, 64L, 71L, 71L, 74L, 70L, 79L, 54L, 79L, 74L, 74L, 69L,
65L, 82L, 86L, 74L, 60L, 70L, 82L, 83L, 84L, 57L, 77L, 65L, 83L,
76L, 79L, 57L, 65L, 66L, 71L, 84L, 67L, 65L, 82L, 78L, 60L, 71L,
64L, 84L, 74L, 56L, 67L, 66L, 72L, 76L, 64L, 77L, 64L, 59L, 54L,
82L, 72L, 64L, 69L, 60L, 68L, 73L, 77L, 68L, 65L, 68L, 72L, 68L,
64L, 67L, 62L, 65L, 73L, 73L, 64L, 58L, 77L, 62L, 68L, 71L, 63L,
70L, 66L, 89L, 63L, 67L, 70L, 84L, 67L, 58L, 71L, 66L, 74L, 65L,
76L, 67L, 57L, 99L, 66L, 66L, 74L, 63L, 78L, 78L, 70L, 71L, 72L,
72L, 72L, 74L, 68L, 65L, 67L, 71L, 63L, 70L, 81L, 73L, 86L, 69L,
90L, 56L, 67L, 59L, 70L, 82L, 64L, 78L, 72L, 54L, 54L, 85L, 64L,
73L, 69L, 79L, 70L, 74L, 73L, 75L, 63L, 64L, 57L, 57L, 72L, 64L,
59L, 67L, 79L, 79L, 58L, 59L, 79L, 73L, 62L, 67L, 62L, 65L, 97L,
63L, 82L, 70L, 68L, 62L, 68L, 68L, 66L, 76L, 58L, 77L, 56L, 71L,
61L, 57L, 65L, 65L, 64L, 69L, 61L, 82L, 72L, 67L, 71L, 66L, 78L,
62L, 72L, 63L, 63L, 74L, 75L, 51L, 57L, 75L, 69L, 77L, 66L, 74L,
65L, 74L, 68L, 68L, 67L, 80L, 46L, 63L, 60L, 63L, 71L, 76L, 64L,
81L, 70L, 70L, 60L, 73L, 71L, 68L, 65L, 73L, 62L, 61L, 55L, 78L,
72L, 67L, 79L, 76L, 55L, 55L, 75L, 66L, 67L, 65L, 56L, 74L, 67L,
81L, 75L, 69L, 67L, 70L, 60L, 68L, 68L, 72L, 58L, 60L, 73L, 72L,
62L, 64L, 70L, 65L, 39L, 70L, 63L, 73L, 59L, 74L, 62L, 64L, 71L,
61L, 61L, 70L, 69L, 75L, 75L, 76L, 68L, 79L, 68L, 60L, 64L, 73L,
81L, 48L, 66L, 75L, 45L, 87L, 71L, 60L, 59L, 69L, 68L, 78L, 53L,
64L, 77L, 71L, 62L, 66L, 58L, 64L, 66L, 60L, 67L, 64L, 62L, 57L,
74L, 58L, 59L, 60L, 72L, 59L, 58L, 80L, 71L, 63L, 63L, 69L, 68L,
66L, 67L, 68L, 66L, 67L, 61L, 59L, 61L, 65L, 61L, 78L, 68L, 69L,
74L, 75L, 83L, 62L, 51L, 70L, 71L, 57L, 76L, 57L, 49L, 60L, 62L,
55L, 61L, 81L, 70L, 69L, 63L, 67L, 62L, 57L, 72L, 52L, 69L, 68L,
65L, 68L, 76L, 71L, 69L, 63L, 66L, 64L, 70L, 82L, 72L, 71L, 60L,
67L, 86L, 65L, 53L, 67L, 78L, 69L), OldMort = c(37L, 42L, 37L,
35L, 50L, 37L, 42L, 34L, 32L, 34L, 45L, 42L, 41L, 39L, 44L, 49L,
40L, 38L, 31L, 35L, 46L, 33L, 48L, 34L, 41L, 39L, 36L, 32L, 41L,
43L, 40L, 41L, 39L, 31L, 36L, 39L, 36L, 39L, 36L, 57L, 41L, 50L,
51L, 42L, 25L, 44L, 44L, 39L, 39L, 45L, 38L, 44L, 37L, 37L, 41L,
44L, 30L, 41L, 45L, 39L, 39L, 49L, 33L, 40L, 35L, 43L, 41L, 37L,
40L, 37L, 55L, 37L, 50L, 36L, 41L, 38L, 29L, 40L, 37L, 42L, 40L,
43L, 39L, 34L, 38L, 23L, 39L, 40L, 42L, 47L, 26L, 37L, 39L, 27L,
38L, 28L, 41L, 44L, 37L, 39L, 32L, 33L, 41L, 42L, 48L, 33L, 36L,
39L, 44L, 33L, 39L, 38L, 38L, 29L, 37L, 43L, 35L, 32L, 38L, 35L,
40L, 36L, 37L, 41L, 53L, 29L, 41L, 39L, 44L, 42L, 41L, 36L, 44L,
45L, 36L, 42L, 37L, 32L, 48L, 41L, 44L, 50L, 52L, 31L, 44L, 46L,
40L, 37L, 35L, 48L, 44L, 42L, 48L, 39L, 37L, 39L, 46L, 32L, 46L,
35L, 54L, 41L, 40L, 49L, 36L, 43L, 37L, 32L, 43L, 36L, 37L, 52L,
45L, 26L, 35L, 35L, 29L, 55L, 44L, 41L, 44L, 39L, 48L, 39L, 40L,
29L, 43L, 36L, 44L, 38L, 43L, 33L, 33L, 52L, 45L, 37L, 42L, 45L,
35L, 39L, 33L, 35L, 46L, 34L, 37L, 39L, 31L, 32L, 44L, 50L, 46L,
44L, 47L, 42L, 35L, 45L, 40L, 37L, 36L, 44L, 52L, 32L, 47L, 39L,
27L, 39L, 42L, 49L, 49L, 43L, 39L, 51L, 40L, 44L, 48L, 47L, 43L,
56L, 31L, 37L, 52L, 39L, 37L, 44L, 47L, 38L, 35L, 41L, 45L, 40L,
25L, 35L, 31L, 46L, 28L, 52L, 30L, 36L, 43L, 40L, 49L, 36L, 37L,
45L, 36L, 25L, 37L, 43L, 40L, 36L, 40L, 37L, 37L, 31L, 36L, 37L,
48L, 46L, 33L, 42L, 35L, 37L, 37L, 35L, 43L, 36L, 29L, 27L, 44L,
47L, 46L, 38L, 40L, 49L, 35L, 41L, 40L, 42L, 41L, 39L, 45L, 44L,
51L, 35L, 38L, 43L, 41L, 44L, 40L, 37L, 40L, 48L, 32L, 33L, 38L,
41L, 41L, 42L, 58L, 55L, 48L, 35L, 43L, 38L, 46L, 38L, 58L, 31L,
36L, 32L, 32L, 36L, 28L, 33L, 42L, 38L, 33L, 40L, 35L, 37L, 24L,
35L, 39L, 35L, 41L, 22L, 43L, 50L, 51L, 35L, 40L, 35L, 36L, 40L,
40L, 44L, 42L, 45L, 29L, 68L, 38L, 43L, 37L, 40L, 29L, 32L, 40L,
46L, 31L, 31L, 40L, 40L, 43L, 63L, 50L, 46L, 31L, 40L, 55L, 43L,
38L, 49L, 51L, 64L, 47L, 48L, 56L, 45L, 35L, 46L, 41L, 37L, 40L,
39L, 36L, 39L, 36L, 40L, 39L, 46L, 45L, 46L, 42L, 44L, 44L, 34L,
38L, 39L, 34L, 34L, 34L, 36L, 41L, 41L, 48L, 37L, 33L, 38L, 42L,
51L, 41L, 30L, 33L, 39L, 41L, 51L, 44L, 45L, 35L, 50L, 45L, 26L,
43L, 38L, 48L, 42L, 39L, 39L, 33L, 29L, 49L, 44L, 29L, 32L, 51L,
39L, 40L, 37L, 40L, 30L, 41L, 47L, 45L, 45L, 42L, 38L, 49L, 46L,
37L, 32L, 50L, 45L, 39L, 31L, 47L, 38L, 43L, 33L, 38L, 46L, 55L,
55L, 43L, 44L, 49L, 45L, 36L, 48L, 35L, 39L, 48L, 54L, 38L, 38L,
44L, 50L, 39L, 24L, 47L, 40L, 37L, 29L, 42L, 47L, 44L, 50L, 32L,
33L, 35L, 30L, 50L, 52L, 44L, 41L, 50L, 45L, 45L, 39L, 40L, 43L,
35L, 38L, 34L, 39L, 44L, 46L, 39L, 51L, 54L, 39L, 47L, 54L, 61L,
36L, 53L, 41L, 51L, 44L, 32L, 39L, 44L, 54L, 41L, 35L, 40L, 46L,
42L, 49L, 36L, 51L, 34L, 29L, 44L, 42L, 36L, 31L, 30L, 35L, 34L,
55L, 33L, 31L, 51L, 36L, 34L, 50L, 39L, 34L, 43L, 46L, 45L, 38L,
29L, 37L, 47L, 39L, 46L, 40L, 32L, 29L, 38L, 35L, 31L, 49L, 40L,
43L, 49L, 32L, 44L, 38L, 38L, 42L, 39L, 46L, 34L, 30L, 47L, 51L,
35L, 43L, 49L, 40L, 53L, 43L, 53L, 41L, 34L, 35L, 33L, 49L, 32L,
39L, 36L, 49L, 50L, 38L, 28L, 47L, 42L, 30L, 38L, 42L, 41L, 39L,
53L, 49L, 48L, 41L, 45L, 65L, 39L, 39L, 54L, 40L, 49L, 48L, 38L,
31L, 46L, 44L, 40L, 34L, 46L, 33L, 51L, 36L, 26L, 41L, 36L, 40L,
37L, 47L, 44L, 42L, 49L, 48L, 48L, 42L, 41L, 34L, 50L, 51L, 40L,
52L, 48L, 30L, 44L, 39L, 44L, 45L, 49L, 34L, 44L, 38L, 42L, 48L,
54L, 32L, 44L, 45L, 62L, 29L, 36L, 38L, 32L, 46L, 40L, 49L, 40L,
30L, 57L, 58L, 45L, 34L, 36L, 58L, 44L, 43L, 33L, 35L, 41L, 32L,
34L, 44L, 60L, 37L, 39L, 48L, 37L, 37L, 30L, 29L, 35L, 49L, 28L,
40L, 47L, 41L, 41L, 40L, 40L, 45L, 42L, 36L, 42L, 41L, 39L, 39L,
51L, 30L, 42L, 43L, 30L, 42L, 43L, 46L, 51L, 34L, 51L, 49L, 38L,
49L, 40L, 42L, 48L, 47L, 37L, 33L, 38L, 43L, 45L, 37L, 35L, 41L,
34L, 46L, 45L, 38L, 33L, 34L, 43L, 30L, 46L, 46L, 52L, 35L, 36L,
38L, 33L, 37L, 40L, 45L, 46L, 40L, 35L, 44L, 42L, 45L, 41L, 52L,
41L, 49L, 44L, 45L, 48L, 58L, 33L, 43L, 53L, 32L, 39L, 36L, 29L,
43L, 40L, 44L, 36L, 43L, 38L, 47L, 47L, 27L, 41L, 38L, 34L, 50L,
40L, 42L, 41L, 36L, 55L, 55L, 48L, 40L, 33L, 40L, 54L, 32L, 33L,
48L, 45L, 36L, 53L, 37L, 39L, 35L, 40L, 35L, 41L, 55L, 43L, 31L,
48L, 43L, 30L, 49L, 51L, 31L, 50L, 58L, 45L, 45L, 44L, 43L, 71L,
62L, 61L, 69L, 73L, 69L, 58L, 73L, 65L, 59L, 37L, 47L, 45L, 45L,
35L, 35L, 42L, 47L, 51L, 38L, 39L, 49L, 39L, 38L, 41L, 36L, 35L,
52L, 42L, 42L, 39L, 49L, 48L, 44L, 40L, 43L, 45L, 46L, 47L, 46L,
55L, 29L, 54L, 33L, 43L, 48L, 40L, 39L, 35L, 36L, 33L, 46L, 46L,
48L, 47L, 33L, 43L, 44L, 32L, 47L, 43L, 32L, 52L, 32L, 50L, 40L,
43L, 49L, 43L, 34L, 41L, 40L, 47L, 45L, 51L, 52L, 26L, 37L, 42L,
52L, 40L, 53L, 51L, 47L, 40L, 44L, 34L, 25L, 38L, 43L, 36L, 48L,
40L, 33L, 31L, 38L, 58L, 49L, 50L, 50L, 39L, 43L, 45L, 42L, 43L,
52L, 44L, 39L, 45L, 38L, 39L, 37L, 41L, 44L, 36L, 49L, 42L, 42L,
36L, 38L, 34L, 38L, 48L, 48L, 48L, 43L, 51L, 42L, 54L, 43L, 34L,
39L, 41L, 43L, 44L, 40L, 33L, 45L, 44L, 38L, 42L, 42L, 47L, 48L,
34L, 39L, 35L, 34L, 46L, 48L, 43L, 34L, 39L, 38L, 36L, 30L, 35L,
36L, 51L, 53L, 41L, 44L, 34L, 39L, 40L, 27L, 37L, 37L, 42L, 45L,
39L, 36L, 45L, 34L, 45L, 28L, 52L, 38L, 48L, 42L, 40L, 50L, 41L,
41L, 33L, 41L, 38L, 49L, 31L, 48L, 34L, 43L, 44L, 37L, 30L, 47L,
37L, 29L, 31L, 33L, 48L, 39L, 39L, 39L, 30L, 42L, 51L, 33L, 45L,
59L, 44L, 39L, 48L, 33L, 42L, 34L, 39L, 46L, 33L, 39L, 39L, 42L,
43L, 41L, 39L, 38L, 51L, 43L, 52L, 41L, 47L, 47L, 59L, 50L, 34L,
58L, 44L, 42L, 48L, 36L, 45L, 52L, 38L, 50L, 46L, 52L, 35L, 51L,
31L, 51L, 29L, 37L, 41L, 37L, 43L, 40L, 45L, 47L, 50L, 48L, 50L,
50L, 54L, 50L, 45L, 55L, 27L, 36L, 49L, 36L, 44L, 45L, 45L, 49L,
52L, 51L, 40L, 51L, 36L, 26L, 39L, 35L, 48L, 37L, 39L, 35L, 47L,
35L, 37L, 44L, 34L, 44L, 41L, 48L, 43L, 41L, 47L, 36L, 52L, 28L,
40L, 47L, 42L, 45L, 45L, 43L, 42L, 46L, 46L, 34L, 33L, 48L, 37L,
34L, 48L, 49L, 47L, 42L, 39L, 46L, 47L, 40L, 42L, 38L, 46L, 30L,
28L, 52L, 31L, 35L, 45L, 34L, 37L, 36L, 47L, 42L, 24L, 54L, 38L,
41L, 50L, 45L, 48L, 32L, 37L, 36L, 44L, 42L, 40L, 36L, 60L, 33L,
41L, 43L, 38L, 46L, 46L, 34L, 34L, 41L, 46L, 38L, 37L, 53L, 52L,
41L, 58L, 43L, 64L, 50L, 35L, 41L, 56L, 43L, 41L, 51L, 39L, 46L,
36L, 44L, 38L, 44L, 37L, 46L, 42L, 47L, 55L, 36L, 36L, 35L, 48L,
50L, 45L, 46L, 51L, 43L, 43L, 49L, 51L, 56L, 62L, 53L, 47L, 50L,
46L, 51L, 31L, 44L, 49L, 46L, 61L, 43L, 45L, 51L, 49L, 42L, 38L,
42L, 39L, 57L, 40L, 40L, 40L, 42L, 50L, 52L, 35L, 59L, 40L, 44L,
62L, 41L, 50L, 46L, 54L, 55L, 49L, 41L, 56L, 41L, 39L, 53L, 41L,
53L, 42L, 38L, 42L, 49L, 49L, 41L, 42L, 33L, 41L, 30L, 38L, 40L,
41L, 48L, 53L, 42L, 42L, 38L, 47L, 39L, 35L, 41L, 36L, 44L, 48L,
43L, 64L, 49L, 46L, 62L, 50L, 58L, 47L, 36L, 40L, 36L, 47L, 32L,
32L, 43L, 32L, 46L, 47L, 49L, 51L, 45L, 46L, 44L, 50L, 40L, 35L,
36L, 49L, 47L, 36L, 44L, 41L, 49L, 40L, 58L, 53L, 43L, 42L, 36L,
47L, 58L, 35L, 45L, 52L, 34L, 36L, 42L, 45L, 29L, 48L, 40L, 41L,
35L, 37L, 55L, 33L, 34L, 39L, 47L, 42L, 48L, 43L, 52L, 45L, 38L,
52L, 31L, 55L, 46L, 43L, 46L, 50L, 38L, 50L, 51L, 45L, 45L, 46L,
39L, 51L, 49L, 43L, 34L, 35L, 59L, 59L, 42L, 45L, 40L, 48L, 45L,
46L, 57L, 41L, 49L, 34L, 48L, 48L, 39L, 45L, 53L, 34L, 42L, 42L,
55L, 38L, 59L, 42L, 47L, 46L, 47L, 40L, 36L, 38L, 44L, 51L, 49L,
48L, 43L, 44L, 42L, 36L, 43L, 43L, 41L, 35L, 51L, 35L, 41L, 46L,
47L, 46L, 51L, 41L, 41L, 38L, 43L, 34L, 59L, 44L, 38L, 43L, 37L,
43L, 33L, 43L, 41L, 62L, 44L, 36L, 49L, 43L, 43L, 48L, 43L, 44L,
33L, 31L, 37L, 34L, 49L, 32L, 40L, 39L, 60L, 36L, 41L, 44L, 51L,
42L, 46L, 42L, 40L, 30L, 40L, 46L, 44L, 34L, 43L, 45L, 56L, 41L,
41L, 37L, 37L, 36L, 49L, 35L, 45L, 38L, 35L, 39L, 46L, 39L, 44L,
50L, 43L, 37L, 44L, 27L, 42L, 41L, 45L, 39L, 39L, 41L, 48L, 42L,
45L, 32L, 38L, 29L, 42L, 58L, 37L, 46L, 46L, 34L, 38L, 54L, 41L,
37L, 43L, 47L, 47L, 48L, 51L, 32L, 36L, 47L, 50L, 54L, 38L, 39L,
38L, 32L, 46L, 50L, 50L, 41L, 53L, 46L, 37L, 37L, 56L, 44L, 51L,
42L, 45L, 46L, 50L, 49L, 55L, 32L, 44L, 47L, 54L, 50L, 51L, 56L,
44L, 36L, 32L, 40L, 37L, 50L, 38L, 41L, 37L, 58L, 34L, 46L, 44L,
39L, 40L, 46L, 28L, 42L, 40L, 35L, 52L, 43L, 31L, 40L, 43L, 39L,
42L, 49L, 44L, 53L, 52L, 41L, 39L, 44L, 45L, 52L, 45L, 53L, 53L,
49L, 50L, 38L, 43L, 48L, 40L, 38L, 48L, 32L, 41L, 54L, 48L, 42L,
39L, 48L, 39L, 42L, 41L, 52L, 44L, 40L, 41L, 37L, 48L, 44L, 44L,
41L, 44L, 37L, 37L, 46L, 50L, 35L, 36L, 50L, 34L, 36L, 44L, 63L,
50L, 46L, 41L, 41L, 40L, 42L, 46L, 39L, 47L, 40L, 37L, 36L, 47L,
39L, 34L, 42L, 32L, 43L, 48L, 41L, 33L, 41L, 53L, 42L, 44L, 53L,
57L, 29L, 47L, 53L, 62L, 52L, 39L, 45L, 31L, 39L, 51L, 39L, 51L,
43L, 48L, 42L, 46L, 47L, 45L, 38L, 52L, 52L, 49L, 53L, 52L, 54L,
52L, 38L, 43L, 48L, 46L, 59L, 55L, 29L, 47L, 38L, 53L, 42L, 44L,
51L, 52L, 38L, 53L, 41L, 48L, 50L, 54L, 45L, 46L, 48L, 41L, 52L,
48L, 39L, 43L, 37L, 37L, 47L, 46L, 41L, 51L, 58L, 43L, 39L, 38L,
50L, 48L, 45L, 45L, 38L, 42L, 34L, 50L, 49L, 47L, 49L, 25L, 49L,
38L, 40L, 55L, 40L, 49L, 60L, 47L, 47L, 47L, 55L, 53L, 37L, 34L,
39L, 40L, 57L, 52L, 49L, 57L, 46L, 42L, 32L, 36L, 41L, 39L, 53L,
54L, 44L, 45L, 41L, 50L, 39L, 41L, 40L, 34L, 49L, 41L, 48L, 42L,
47L, 42L, 41L, 47L, 53L, 49L, 49L, 45L, 42L, 36L, 49L, 47L, 46L,
54L, 36L, 49L, 57L, 40L, 47L, 52L, 41L, 47L, 56L, 49L, 41L, 34L,
45L, 42L, 36L, 36L, 51L, 43L, 43L, 58L, 47L, 53L, 33L, 42L, 43L,
36L, 44L, 42L, 36L, 44L, 47L, 48L, 50L, 46L, 34L, 49L, 36L, 39L,
49L, 55L, 42L, 58L, 48L, 45L, 55L, 41L, 40L, 41L, 41L, 36L, 51L,
44L, 26L, 34L, 58L, 49L, 55L, 36L, 49L, 56L, 54L, 52L, 44L, 40L,
49L, 53L, 46L, 46L, 44L, 45L, 43L, 42L, 53L, 37L, 45L, 49L, 47L,
53L, 59L, 44L, 43L, 47L, 38L, 45L, 42L, 42L, 44L, 43L, 33L, 51L,
38L, 50L, 49L, 43L, 49L, 39L, 39L, 50L, 43L, 41L, 31L, 51L, 36L,
51L, 53L, 56L, 45L, 46L, 44L, 48L, 40L, 40L, 48L, 36L, 43L, 39L,
50L, 50L, 42L, 35L, 39L, 39L, 63L, 42L, 50L, 35L, 44L, 31L, 37L,
56L, 40L, 41L, 41L, 57L, 46L, 35L, 52L, 46L, 35L, 45L, 41L, 34L,
39L, 49L, 42L, 41L, 43L, 41L, 52L, 44L, 46L, 54L, 46L, 45L, 47L,
46L, 40L, 37L, 45L, 49L, 47L, 50L, 45L, 55L, 45L, 52L, 54L, 44L,
36L, 46L, 39L, 36L, 50L, 42L, 47L, 47L, 44L, 55L, 58L, 34L, 54L,
51L, 45L, 36L, 61L, 50L, 42L, 55L, 42L, 45L, 50L, 51L, 49L, 43L,
36L, 51L, 44L, 50L, 56L, 44L, 48L, 52L, 67L, 46L, 56L, 65L, 46L,
33L, 48L, 44L, 54L, 50L, 39L, 49L, 46L, 54L, 25L, 46L, 43L, 50L,
53L, 53L, 51L, 43L, 49L, 37L, 51L, 38L, 41L, 54L, 44L, 47L, 59L,
50L, 41L, 42L, 28L, 40L, 53L, 48L, 53L, 49L, 40L, 43L, 50L, 51L,
45L, 49L, 38L, 48L, 51L, 34L, 46L, 37L, 42L, 39L, 47L, 35L, 51L,
45L, 44L, 41L, 42L, 30L, 46L, 48L, 65L, 42L, 36L, 49L, 35L, 44L,
34L, 40L, 42L, 45L, 40L, 39L, 43L, 52L, 28L, 44L, 41L, 39L, 48L,
37L, 34L, 37L, 40L, 34L, 42L, 32L, 44L, 31L, 44L, 45L, 31L, 61L,
54L, 50L, 45L, 51L, 43L, 39L, 34L, 46L, 39L, 46L, 44L, 45L, 40L,
43L, 48L, 45L, 46L, 54L, 47L, 23L, 41L, 49L, 43L, 53L, 57L, 49L,
52L, 41L, 53L, 46L, 38L, 42L, 53L, 43L, 53L, 41L, 42L, 62L, 31L,
43L, 28L, 48L, 43L, 38L, 51L, 49L, 45L, 38L, 53L, 55L, 46L, 51L,
41L, 56L, 47L, 45L, 51L, 41L, 44L, 44L, 52L, 41L, 54L, 40L, 48L,
50L, 45L, 50L, 59L, 40L, 44L, 45L, 43L, 42L, 50L, 31L, 41L, 44L,
53L, 44L, 45L, 44L, 51L, 32L, 40L, 51L, 41L, 44L, 48L, 39L, 34L,
51L, 41L, 57L, 50L, 52L, 44L, 48L, 39L, 43L, 47L, 52L, 49L, 53L,
54L, 43L, 51L, 46L, 44L, 50L, 46L, 39L, 53L, 44L, 40L, 45L, 55L,
44L, 37L, 32L, 46L, 43L, 47L, 45L, 51L, 46L, 48L, 40L, 48L, 42L,
43L, 41L, 32L, 43L, 34L, 40L, 45L, 40L, 44L, 42L, 54L, 38L, 55L,
31L, 44L, 61L, 32L, 44L, 43L, 51L, 47L, 48L, 29L, 42L, 49L, 33L,
28L, 39L, 44L, 50L, 46L, 49L, 44L, 51L, 46L, 56L, 49L, 42L, 44L,
50L, 47L, 43L, 31L, 44L, 41L, 50L, 48L, 38L, 31L, 45L, 45L, 32L,
40L, 49L, 63L, 45L, 55L, 42L, 47L, 39L, 46L, 41L, 36L, 46L, 46L,
43L, 48L, 48L, 52L, 56L, 47L, 50L, 49L, 39L, 51L, 50L, 45L, 44L,
47L, 47L, 52L, 44L, 50L, 45L, 40L, 41L, 50L, 45L, 49L, 39L, 58L,
43L, 41L, 56L, 48L, 47L, 39L, 47L, 60L, 39L, 52L, 56L, 51L, 44L,
51L, 52L, 41L, 39L, 42L, 56L, 54L, 54L, 40L, 55L, 52L, 50L, 44L,
44L, 39L, 43L, 47L, 51L, 46L, 57L, 50L, 29L, 43L, 46L, 43L, 42L,
49L, 48L, 55L, 47L, 45L, 51L, 50L, 36L, 46L, 51L, 41L, 46L, 54L,
34L, 33L, 46L, 46L, 56L, 46L, 42L, 49L, 41L, 34L, 43L, 59L, 39L,
45L, 39L, 49L, 43L, 33L, 40L, 35L, 49L, 37L, 34L, 40L, 39L, 53L,
46L, 42L, 44L, 40L, 50L, 43L, 37L, 42L, 50L, 44L, 57L, 53L, 33L,
37L, 36L, 38L, 53L, 49L, 38L, 51L, 44L, 42L, 44L, 43L, 44L, 30L,
38L, 56L, 59L, 53L, 32L, 49L, 34L, 50L, 41L, 55L, 49L, 45L, 43L,
39L, 50L, 45L, 40L, 24L, 44L, 38L, 40L, 47L, 51L, 38L, 43L, 52L,
43L, 48L, 49L, 47L, 49L, 41L, 42L, 50L, 36L, 39L, 42L, 47L, 53L,
55L, 58L, 52L, 52L, 37L, 48L, 37L, 58L, 41L, 43L, 52L, 53L, 37L,
47L, 48L, 44L, 58L, 47L, 35L, 35L, 43L, 48L, 51L, 46L, 47L, 38L,
50L, 41L, 44L, 40L, 53L, 50L, 54L, 40L, 50L, 41L, 51L, 57L, 40L,
45L, 45L, 38L, 40L, 40L, 46L, 33L, 37L, 46L, 49L, 46L, 51L, 40L,
46L, 47L, 60L, 60L, 46L, 46L, 55L, 41L, 40L, 50L, 35L, 62L, 42L,
36L, 52L, 38L, 50L, 41L, 44L, 51L, 44L, 48L, 43L, 40L, 43L, 45L,
43L, 44L, 43L, 44L, 54L, 40L, 46L, 47L, 55L, 48L, 43L, 42L, 38L,
39L, 40L, 49L, 50L, 47L, 43L, 59L, 49L, 48L, 52L, 55L, 41L, 43L,
35L, 44L, 46L, 39L, 50L, 54L, 40L, 44L, 50L, 47L, 47L, 39L, 45L,
59L, 49L, 42L, 45L, 26L, 41L, 43L, 47L, 36L, 41L, 34L, 39L, 53L,
56L, 45L, 38L, 42L, 56L, 44L, 33L, 39L, 45L, 58L, 55L, 44L, 36L,
48L, 41L, 48L, 50L, 40L, 47L, 52L, 42L, 67L, 42L, 48L, 41L, 46L,
46L, 52L, 50L, 61L, 40L, 49L, 50L, 47L, 41L, 56L, 54L, 56L, 41L,
48L, 48L, 49L, 43L, 57L, 39L, 48L, 45L, 46L, 43L, 55L, 49L, 59L,
46L, 58L, 47L, 45L, 49L, 67L, 40L, 47L, 50L, 49L, 43L, 33L, 51L,
46L, 40L, 45L, 53L, 43L, 48L, 48L, 43L, 33L, 56L, 46L, 41L, 42L,
39L, 46L, 37L, 49L, 50L, 50L, 54L, 51L, 50L, 38L, 43L, 46L, 46L,
36L, 48L, 38L, 50L, 42L, 49L, 48L, 39L, 40L, 48L, 46L, 46L, 43L,
44L, 55L, 59L, 40L, 59L, 47L, 45L, 41L, 57L, 46L, 47L, 33L, 39L,
41L, 44L, 53L, 64L, 46L, 38L, 47L, 41L, 47L, 45L, 46L, 46L, 43L,
34L, 38L, 43L, 47L, 46L, 49L, 44L, 42L, 46L, 42L, 43L, 37L, 48L,
45L, 45L, 45L, 46L, 43L, 44L, 40L, 48L, 45L, 49L, 37L, 51L, 43L,
30L, 44L, 47L, 49L, 44L, 49L, 41L, 46L, 51L, 54L, 45L, 49L, 57L,
56L, 56L, 53L, 49L, 51L, 46L, 50L, 46L, 56L, 49L, 53L, 48L, 51L,
45L, 52L, 38L, 55L, 49L, 49L, 63L, 58L, 51L, 51L, 48L, 56L, 54L,
59L, 48L, 51L, 51L, 41L, 44L, 49L, 37L, 46L, 43L, 39L, 54L, 61L,
47L, 36L, 46L, 52L, 51L, 46L, 54L, 45L, 59L, 55L, 42L, 41L, 40L,
47L, 42L, 41L, 50L, 55L, 41L, 55L, 51L, 53L, 53L, 39L, 57L, 51L,
37L, 50L, 36L, 61L, 44L, 49L, 45L, 45L, 46L, 58L, 47L, 38L, 52L,
55L, 48L, 41L, 66L, 41L, 51L, 36L, 43L, 56L, 49L, 43L, 46L, 44L,
45L, 45L, 45L, 48L, 60L, 32L, 44L, 42L, 44L, 58L, 48L, 51L, 36L,
34L, 45L, 36L, 35L, 32L, 39L, 44L, 49L, 41L, 51L, 55L, 42L, 53L,
50L, 45L, 42L, 54L, 50L, 47L, 45L, 40L, 44L, 52L, 57L, 52L, 53L,
49L, 47L, 46L, 49L, 66L, 51L, 43L, 55L, 54L, 51L, 47L, 45L, 45L,
55L, 54L, 37L, 44L, 63L, 55L, 53L, 56L, 58L, 54L, 46L, 59L, 56L,
54L, 59L, 54L, 43L, 40L, 41L, 52L, 61L, 52L, 48L, 36L, 43L, 40L,
56L, 50L, 41L, 58L, 34L, 45L, 57L, 58L, 41L, 49L, 55L, 38L, 46L,
51L, 38L, 44L, 52L, 44L, 47L, 50L, 50L, 47L, 50L, 51L, 47L, 58L,
46L, 46L, 51L, 50L, 51L, 46L, 50L, 45L, 65L, 52L, 48L, 52L, 39L,
50L, 45L, 47L, 44L, 43L, 64L, 45L, 44L, 37L, 46L, 50L, 56L, 45L,
42L, 64L, 51L, 41L, 67L, 43L, 45L, 47L, 52L, 55L, 51L, 41L, 57L,
39L, 59L, 48L, 46L, 43L, 51L, 59L, 45L, 48L, 40L, 48L, 52L, 45L,
57L, 49L, 48L, 57L, 41L, 63L, 49L, 43L, 46L, 46L, 53L, 48L, 47L,
40L, 49L, 45L, 44L, 51L, 43L, 42L, 48L, 54L, 43L, 56L, 56L, 55L,
50L, 41L, 37L, 41L, 48L, 52L, 52L, 43L, 45L, 49L, 50L, 35L, 52L,
46L, 41L, 55L, 42L, 53L, 47L, 49L, 51L, 51L, 56L, 52L, 45L, 62L,
49L, 48L, 54L, 47L, 53L, 57L, 46L, 37L, 52L, 56L, 45L, 57L, 44L,
63L, 44L, 59L, 43L, 40L, 46L, 56L, 55L, 49L, 40L, 57L, 40L, 46L,
42L, 39L, 42L, 39L, 54L, 58L, 59L, 63L, 59L, 47L, 52L, 53L, 49L,
51L, 52L, 48L, 47L, 26L, 42L, 43L, 58L, 53L, 50L, 56L, 41L, 55L,
47L, 48L, 47L, 37L, 58L, 39L, 42L, 47L, 42L, 54L, 50L, 40L, 50L,
46L, 50L, 47L, 45L, 59L, 42L, 46L, 48L, 56L, 52L, 57L, 55L, 56L,
49L, 57L, 45L, 49L, 43L, 48L, 41L, 50L, 50L, 58L, 58L, 50L, 40L,
49L, 54L, 57L, 46L, 50L, 51L, 56L, 55L, 60L, 58L, 44L, 52L, 57L,
48L, 50L, 43L, 52L, 51L, 45L, 45L, 52L, 53L, 56L, 63L, 52L, 35L,
49L, 62L, 36L, 52L, 38L, 51L, 41L, 43L, 38L, 50L, 47L, 43L, 56L,
63L, 51L, 58L, 49L, 46L, 52L, 42L, 52L, 54L, 39L, 50L, 44L, 37L,
46L, 43L, 53L, 47L, 52L, 52L, 61L, 62L, 46L, 58L, 54L, 52L, 43L,
42L, 59L, 38L, 50L, 52L, 54L, 48L, 70L, 44L, 39L, 41L, 57L, 41L,
31L, 52L, 62L, 47L, 41L, 67L, 35L, 56L, 48L, 52L, 45L, 56L, 44L,
49L, 45L, 38L, 49L, 41L, 45L, 60L, 54L, 53L, 43L, 59L, 44L, 52L,
36L, 64L, 47L, 47L, 64L, 52L, 49L, 45L, 43L, 52L, 47L, 43L, 38L,
45L, 46L, 49L, 47L, 41L, 43L, 59L, 48L, 55L, 64L, 45L, 39L, 51L,
42L, 34L, 43L, 56L, 52L, 45L, 55L, 63L, 54L, 41L, 44L, 45L, 47L,
58L, 31L, 41L, 49L, 46L, 42L, 57L, 51L, 56L, 39L, 43L, 33L, 46L,
48L, 34L, 47L, 39L, 52L, 36L, 43L, 55L, 47L, 52L, 45L, 55L, 45L,
45L, 56L, 50L, 34L, 50L, 48L, 55L, 40L, 52L, 38L, 49L, 54L, 42L,
42L, 43L, 57L, 42L, 50L, 41L, 54L, 55L, 41L, 41L, 48L, 35L, 50L,
42L, 45L, 48L, 50L, 45L, 51L, 54L, 47L, 51L, 53L, 49L, 48L, 56L,
52L, 47L, 40L, 59L, 51L, 33L, 34L, 35L, 50L, 41L, 58L, 45L, 49L,
58L, 58L, 60L, 52L, 53L, 38L, 57L, 53L, 57L, 51L, 54L, 57L, 42L,
60L, 59L, 43L, 38L, 44L, 55L, 71L, 53L, 54L, 58L, 44L, 48L, 46L,
49L, 52L, 56L, 54L, 43L, 52L, 51L, 62L, 63L, 47L, 54L, 64L, 57L,
53L, 53L, 50L, 53L, 58L, 54L, 42L, 48L, 51L, 49L, 55L, 40L, 61L,
43L, 46L, 54L, 48L, 45L, 53L, 46L, 51L, 43L, 46L, 50L, 53L, 59L,
66L, 46L, 46L, 59L, 50L, 38L, 49L, 35L, 53L, 53L, 56L, 48L, 46L,
54L, 49L, 55L, 49L, 52L, 56L, 56L, 59L, 35L, 49L, 41L, 46L, 61L,
48L, 23L, 52L, 55L, 47L, 63L, 36L, 47L, 40L, 53L, 45L, 44L, 45L,
44L, 60L, 44L, 59L, 38L, 56L, 45L, 52L, 51L, 43L, 44L, 55L, 48L,
49L, 49L, 40L, 44L, 47L, 48L, 44L, 45L, 48L, 60L, 39L, 52L, 50L,
51L, 42L, 46L, 61L, 52L, 51L, 55L, 48L, 41L, 45L, 50L, 53L, 59L,
43L, 39L, 53L, 51L, 44L, 47L, 58L, 53L, 46L, 56L, 49L, 47L, 46L,
61L, 53L, 48L, 44L, 50L, 53L, 49L, 68L, 50L, 41L, 44L, 54L, 44L,
57L, 58L, 52L, 51L, 57L, 59L, 59L, 48L, 45L, 55L, 54L, 51L, 40L,
55L, 50L, 56L, 48L, 52L, 49L, 49L, 51L, 54L, 45L, 53L, 56L, 52L,
52L, 52L, 50L, 38L, 48L, 56L, 50L, 51L, 45L, 41L, 39L, 45L, 42L,
47L, 52L, 56L, 59L, 51L, 41L, 40L, 44L, 46L, 38L, 38L, 51L, 65L,
42L, 51L, 56L, 41L, 50L, 43L, 57L, 52L, 49L, 40L, 56L, 41L, 50L,
39L, 39L, 57L, 55L, 52L, 60L, 47L, 43L, 45L, 51L, 56L, 66L, 41L,
51L, 50L, 44L, 50L, 48L, 57L, 52L, 43L, 60L, 50L, 46L, 60L, 54L,
43L, 63L, 61L, 44L, 61L, 47L, 55L, 48L, 47L, 52L, 46L, 37L, 60L,
71L, 40L, 62L, 45L, 44L, 54L, 58L, 56L, 58L, 49L, 62L, 48L, 51L,
48L, 44L, 41L, 43L, 60L, 55L, 60L, 61L, 59L, 61L, 59L, 55L, 47L,
47L, 42L, 53L, 59L, 47L, 45L, 51L, 56L, 49L, 77L, 54L, 43L, 52L,
42L, 59L, 42L, 42L, 62L, 45L, 49L, 56L, 40L, 61L, 64L, 55L, 67L,
58L, 51L, 41L, 48L, 61L, 59L, 70L, 47L, 65L, 47L, 42L, 43L, 55L,
31L, 51L, 50L, 46L, 67L, 39L, 54L, 45L, 54L, 53L, 50L, 57L, 63L,
54L, 40L, 54L, 49L, 57L, 60L, 45L, 50L, 44L, 53L, 39L, 44L, 42L,
53L, 50L, 40L, 48L, 39L, 52L, 48L, 58L, 46L, 44L, 54L, 40L, 51L,
43L, 51L, 53L, 51L, 57L, 45L, 64L, 50L, 53L, 50L, 50L, 60L, 58L,
45L, 49L, 57L, 48L, 43L, 55L, 51L, 56L, 45L, 57L, 32L, 33L, 52L,
43L, 51L, 53L, 46L, 42L, 48L, 48L, 47L, 46L, 46L, 45L, 49L, 55L,
48L, 57L, 45L, 48L, 44L, 38L, 51L, 46L, 37L, 45L, 49L, 53L, 59L,
43L, 59L, 44L, 56L, 36L, 50L, 41L, 43L, 54L, 59L, 54L, 50L, 39L,
58L, 59L, 47L, 49L, 50L, 50L, 62L, 62L, 44L, 43L, 42L, 52L, 47L,
58L, 33L, 52L, 60L, 39L, 39L, 42L, 59L, 55L, 50L, 56L, 42L, 46L,
46L, 51L, 52L, 60L, 45L, 60L, 50L, 55L, 54L, 58L, 52L, 56L, 54L,
38L, 53L, 50L, 45L, 54L, 49L, 44L, 53L, 53L, 51L, 50L, 53L, 46L,
41L, 50L, 33L, 43L, 47L, 50L, 44L, 49L, 34L, 45L, 38L, 36L, 54L,
51L, 46L, 55L, 35L, 62L, 40L, 46L, 40L, 49L, 52L, 47L, 44L, 49L,
46L, 56L, 41L, 47L, 57L, 48L, 49L, 56L, 43L, 52L, 57L, 49L, 49L,
43L, 52L, 48L, 43L, 42L, 50L, 51L, 48L, 44L, 42L, 47L, 45L, 50L,
42L, 53L, 37L, 56L, 58L, 55L, 37L, 58L, 61L, 46L, 45L, 37L, 49L,
52L, 51L, 67L, 39L, 62L, 45L, 55L, 49L, 49L, 58L, 60L, 57L, 49L,
56L, 46L, 47L, 55L, 42L, 44L, 59L, 50L, 39L, 47L, 43L, 49L, 55L,
51L, 73L, 59L, 54L, 53L, 44L, 63L, 52L, 59L, 57L, 40L, 63L, 44L,
51L, 53L, 63L, 55L, 43L, 48L, 55L, 44L, 50L, 66L, 48L, 46L, 39L,
46L, 57L, 47L, 51L, 49L, 52L, 70L, 42L, 46L, 43L, 47L, 55L, 51L,
45L, 45L, 54L, 59L, 49L, 43L, 52L, 55L, 43L, 56L, 59L, 64L, 66L,
61L, 67L, 57L, 37L, 46L, 59L, 60L, 50L, 55L, 53L, 38L, 41L, 62L,
54L, 42L, 51L, 57L, 38L, 51L, 68L, 56L, 52L, 64L, 48L, 52L, 49L,
49L, 59L, 49L, 51L, 44L, 55L, 49L, 50L, 57L, 40L, 43L, 54L, 64L,
53L, 61L, 48L, 54L, 47L, 69L, 57L, 44L, 49L, 36L, 52L, 56L, 49L,
53L, 47L, 57L, 38L, 47L, 49L, 50L, 44L, 48L, 41L, 36L, 41L, 52L,
51L, 51L, 41L, 50L, 59L, 42L, 39L, 52L, 55L, 48L, 50L, 40L, 36L,
61L, 46L, 49L, 51L, 44L, 50L, 53L, 50L, 56L, 47L, 41L, 59L, 49L,
37L, 49L, 46L, 57L, 43L, 47L, 56L, 57L, 44L, 43L, 65L, 51L, 57L,
44L, 48L, 42L, 46L, 59L, 48L, 46L, 34L, 57L, 50L, 54L, 50L, 49L,
46L, 51L, 53L, 56L, 42L, 45L, 41L, 48L, 58L, 52L, 59L, 52L, 51L,
54L, 65L, 49L, 54L, 55L, 60L, 35L, 49L, 44L, 59L, 53L, 54L, 48L,
57L, 54L, 57L, 59L, 67L, 43L, 55L, 48L, 56L, 60L, 63L, 40L, 51L,
38L, 58L, 54L, 51L, 48L, 39L, 45L, 48L, 61L, 48L, 48L, 57L, 55L,
60L, 49L, 58L, 50L, 47L, 41L, 43L, 44L, 62L, 49L, 57L, 57L, 58L,
52L, 44L, 36L, 58L, 43L, 47L, 56L, 49L, 44L, 51L, 59L, 57L, 51L,
44L, 66L, 43L, 48L, 61L, 54L, 60L, 50L, 37L, 53L, 58L, 63L, 42L,
50L, 56L, 58L, 46L, 44L, 49L, 44L, 49L, 56L, 54L, 53L, 53L, 33L,
61L, 54L, 49L, 52L, 48L, 58L, 67L, 52L, 42L, 46L, 64L, 61L, 63L,
44L, 54L, 46L, 63L, 51L, 56L, 45L, 50L, 54L, 45L, 60L, 49L, 49L,
55L, 56L, 41L, 50L, 46L, 60L, 51L, 45L, 51L, 46L, 50L, 54L, 49L,
54L, 47L, 46L, 39L, 60L, 49L, 54L, 43L, 48L, 48L, 55L, 62L, 57L,
52L, 52L, 53L, 52L, 50L, 46L, 47L, 46L, 49L, 51L, 50L, 37L, 51L,
45L, 45L, 54L, 44L, 56L, 47L, 61L, 49L, 51L, 48L, 61L, 50L, 42L,
52L, 51L, 55L, 44L, 58L, 46L, 38L, 67L, 50L, 46L, 59L, 52L, 56L,
63L, 53L, 49L, 53L, 58L, 57L, 50L, 52L, 48L, 55L, 54L, 49L, 53L,
65L, 57L, 60L, 51L, 65L, 41L, 52L, 44L, 45L, 61L, 48L, 61L, 55L,
45L, 41L, 66L, 42L, 51L, 46L, 56L, 50L, 59L, 56L, 51L, 38L, 48L,
39L, 40L, 50L, 41L, 44L, 50L, 46L, 56L, 43L, 43L, 47L, 59L, 43L,
52L, 46L, 48L, 72L, 45L, 63L, 54L, 47L, 44L, 51L, 45L, 43L, 55L,
40L, 54L, 46L, 46L, 40L, 42L, 41L, 53L, 43L, 42L, 44L, 63L, 55L,
45L, 54L, 53L, 64L, 41L, 50L, 44L, 46L, 48L, 59L, 39L, 44L, 55L,
46L, 60L, 40L, 58L, 50L, 53L, 48L, 53L, 53L, 55L, 28L, 45L, 46L,
43L, 53L, 58L, 47L, 60L, 51L, 51L, 40L, 54L, 56L, 55L, 50L, 51L,
50L, 39L, 48L, 55L, 53L, 49L, 59L, 63L, 42L, 37L, 57L, 44L, 48L,
46L, 46L, 48L, 57L, 58L, 51L, 60L, 53L, 52L, 42L, 56L, 50L, 57L,
47L, 43L, 40L, 49L, 45L, 42L, 51L, 46L, 29L, 50L, 47L, 47L, 41L,
48L, 46L, 49L, 49L, 43L, 40L, 47L, 50L, 50L, 61L, 62L, 48L, 50L,
51L, 45L, 37L, 51L, 60L, 33L, 52L, 54L, 36L, 61L, 55L, 39L, 40L,
53L, 48L, 53L, 39L, 45L, 53L, 40L, 47L, 46L, 41L, 43L, 51L, 42L,
45L, 45L, 40L, 47L, 54L, 44L, 46L, 46L, 54L, 44L, 43L, 50L, 54L,
42L, 42L, 48L, 57L, 49L, 49L, 51L, 52L, 48L, 40L, 39L, 49L, 45L,
38L, 57L, 49L, 52L, 43L, 64L, 58L, 48L, 35L, 57L, 49L, 42L, 52L,
38L, 38L, 48L, 48L, 39L, 45L, 57L, 46L, 42L, 44L, 47L, 42L, 52L,
53L, 41L, 43L, 50L, 48L, 47L, 61L, 53L, 50L, 51L, 39L, 51L, 51L,
63L, 49L, 56L, 44L, 44L, 65L, 47L, 43L, 55L, 56L, 48L)), .Names = c("Year",
"Month", "Day", "AllMort", "OldMort"), row.names = c(121L, 122L,
123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L,
134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L,
145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L,
156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L,
167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L,
178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L,
189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L,
200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L,
211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L,
222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L,
233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L,
244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L,
255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L,
266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 487L, 488L, 489L,
490L, 491L, 492L, 493L, 494L, 495L, 496L, 497L, 498L, 499L, 500L,
501L, 502L, 503L, 504L, 505L, 506L, 507L, 508L, 509L, 510L, 511L,
512L, 513L, 514L, 515L, 516L, 517L, 518L, 519L, 520L, 521L, 522L,
523L, 524L, 525L, 526L, 527L, 528L, 529L, 530L, 531L, 532L, 533L,
534L, 535L, 536L, 537L, 538L, 539L, 540L, 541L, 542L, 543L, 544L,
545L, 546L, 547L, 548L, 549L, 550L, 551L, 552L, 553L, 554L, 555L,
556L, 557L, 558L, 559L, 560L, 561L, 562L, 563L, 564L, 565L, 566L,
567L, 568L, 569L, 570L, 571L, 572L, 573L, 574L, 575L, 576L, 577L,
578L, 579L, 580L, 581L, 582L, 583L, 584L, 585L, 586L, 587L, 588L,
589L, 590L, 591L, 592L, 593L, 594L, 595L, 596L, 597L, 598L, 599L,
600L, 601L, 602L, 603L, 604L, 605L, 606L, 607L, 608L, 609L, 610L,
611L, 612L, 613L, 614L, 615L, 616L, 617L, 618L, 619L, 620L, 621L,
622L, 623L, 624L, 625L, 626L, 627L, 628L, 629L, 630L, 631L, 632L,
633L, 634L, 635L, 636L, 637L, 638L, 639L, 852L, 853L, 854L, 855L,
856L, 857L, 858L, 859L, 860L, 861L, 862L, 863L, 864L, 865L, 866L,
867L, 868L, 869L, 870L, 871L, 872L, 873L, 874L, 875L, 876L, 877L,
878L, 879L, 880L, 881L, 882L, 883L, 884L, 885L, 886L, 887L, 888L,
889L, 890L, 891L, 892L, 893L, 894L, 895L, 896L, 897L, 898L, 899L,
900L, 901L, 902L, 903L, 904L, 905L, 906L, 907L, 908L, 909L, 910L,
911L, 912L, 913L, 914L, 915L, 916L, 917L, 918L, 919L, 920L, 921L,
922L, 923L, 924L, 925L, 926L, 927L, 928L, 929L, 930L, 931L, 932L,
933L, 934L, 935L, 936L, 937L, 938L, 939L, 940L, 941L, 942L, 943L,
944L, 945L, 946L, 947L, 948L, 949L, 950L, 951L, 952L, 953L, 954L,
955L, 956L, 957L, 958L, 959L, 960L, 961L, 962L, 963L, 964L, 965L,
966L, 967L, 968L, 969L, 970L, 971L, 972L, 973L, 974L, 975L, 976L,
977L, 978L, 979L, 980L, 981L, 982L, 983L, 984L, 985L, 986L, 987L,
988L, 989L, 990L, 991L, 992L, 993L, 994L, 995L, 996L, 997L, 998L,
999L, 1000L, 1001L, 1002L, 1003L, 1004L, 1217L, 1218L, 1219L,
1220L, 1221L, 1222L, 1223L, 1224L, 1225L, 1226L, 1227L, 1228L,
1229L, 1230L, 1231L, 1232L, 1233L, 1234L, 1235L, 1236L, 1237L,
1238L, 1239L, 1240L, 1241L, 1242L, 1243L, 1244L, 1245L, 1246L,
1247L, 1248L, 1249L, 1250L, 1251L, 1252L, 1253L, 1254L, 1255L,
1256L, 1257L, 1258L, 1259L, 1260L, 1261L, 1262L, 1263L, 1264L,
1265L, 1266L, 1267L, 1268L, 1269L, 1270L, 1271L, 1272L, 1273L,
1274L, 1275L, 1276L, 1277L, 1278L, 1279L, 1280L, 1281L, 1282L,
1283L, 1284L, 1285L, 1286L, 1287L, 1288L, 1289L, 1290L, 1291L,
1292L, 1293L, 1294L, 1295L, 1296L, 1297L, 1298L, 1299L, 1300L,
1301L, 1302L, 1303L, 1304L, 1305L, 1306L, 1307L, 1308L, 1309L,
1310L, 1311L, 1312L, 1313L, 1314L, 1315L, 1316L, 1317L, 1318L,
1319L, 1320L, 1321L, 1322L, 1323L, 1324L, 1325L, 1326L, 1327L,
1328L, 1329L, 1330L, 1331L, 1332L, 1333L, 1334L, 1335L, 1336L,
1337L, 1338L, 1339L, 1340L, 1341L, 1342L, 1343L, 1344L, 1345L,
1346L, 1347L, 1348L, 1349L, 1350L, 1351L, 1352L, 1353L, 1354L,
1355L, 1356L, 1357L, 1358L, 1359L, 1360L, 1361L, 1362L, 1363L,
1364L, 1365L, 1366L, 1367L, 1368L, 1369L, 1582L, 1583L, 1584L,
1585L, 1586L, 1587L, 1588L, 1589L, 1590L, 1591L, 1592L, 1593L,
1594L, 1595L, 1596L, 1597L, 1598L, 1599L, 1600L, 1601L, 1602L,
1603L, 1604L, 1605L, 1606L, 1607L, 1608L, 1609L, 1610L, 1611L,
1612L, 1613L, 1614L, 1615L, 1616L, 1617L, 1618L, 1619L, 1620L,
1621L, 1622L, 1623L, 1624L, 1625L, 1626L, 1627L, 1628L, 1629L,
1630L, 1631L, 1632L, 1633L, 1634L, 1635L, 1636L, 1637L, 1638L,
1639L, 1640L, 1641L, 1642L, 1643L, 1644L, 1645L, 1646L, 1647L,
1648L, 1649L, 1650L, 1651L, 1652L, 1653L, 1654L, 1655L, 1656L,
1657L, 1658L, 1659L, 1660L, 1661L, 1662L, 1663L, 1664L, 1665L,
1666L, 1667L, 1668L, 1669L, 1670L, 1671L, 1672L, 1673L, 1674L,
1675L, 1676L, 1677L, 1678L, 1679L, 1680L, 1681L, 1682L, 1683L,
1684L, 1685L, 1686L, 1687L, 1688L, 1689L, 1690L, 1691L, 1692L,
1693L, 1694L, 1695L, 1696L, 1697L, 1698L, 1699L, 1700L, 1701L,
1702L, 1703L, 1704L, 1705L, 1706L, 1707L, 1708L, 1709L, 1710L,
1711L, 1712L, 1713L, 1714L, 1715L, 1716L, 1717L, 1718L, 1719L,
1720L, 1721L, 1722L, 1723L, 1724L, 1725L, 1726L, 1727L, 1728L,
1729L, 1730L, 1731L, 1732L, 1733L, 1734L, 1948L, 1949L, 1950L,
1951L, 1952L, 1953L, 1954L, 1955L, 1956L, 1957L, 1958L, 1959L,
1960L, 1961L, 1962L, 1963L, 1964L, 1965L, 1966L, 1967L, 1968L,
1969L, 1970L, 1971L, 1972L, 1973L, 1974L, 1975L, 1976L, 1977L,
1978L, 1979L, 1980L, 1981L, 1982L, 1983L, 1984L, 1985L, 1986L,
1987L, 1988L, 1989L, 1990L, 1991L, 1992L, 1993L, 1994L, 1995L,
1996L, 1997L, 1998L, 1999L, 2000L, 2001L, 2002L, 2003L, 2004L,
2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L,
2014L, 2015L, 2016L, 2017L, 2018L, 2019L, 2020L, 2021L, 2022L,
2023L, 2024L, 2025L, 2026L, 2027L, 2028L, 2029L, 2030L, 2031L,
2032L, 2033L, 2034L, 2035L, 2036L, 2037L, 2038L, 2039L, 2040L,
2041L, 2042L, 2043L, 2044L, 2045L, 2046L, 2047L, 2048L, 2049L,
2050L, 2051L, 2052L, 2053L, 2054L, 2055L, 2056L, 2057L, 2058L,
2059L, 2060L, 2061L, 2062L, 2063L, 2064L, 2065L, 2066L, 2067L,
2068L, 2069L, 2070L, 2071L, 2072L, 2073L, 2074L, 2075L, 2076L,
2077L, 2078L, 2079L, 2080L, 2081L, 2082L, 2083L, 2084L, 2085L,
2086L, 2087L, 2088L, 2089L, 2090L, 2091L, 2092L, 2093L, 2094L,
2095L, 2096L, 2097L, 2098L, 2099L, 2100L, 2313L, 2314L, 2315L,
2316L, 2317L, 2318L, 2319L, 2320L, 2321L, 2322L, 2323L, 2324L,
2325L, 2326L, 2327L, 2328L, 2329L, 2330L, 2331L, 2332L, 2333L,
2334L, 2335L, 2336L, 2337L, 2338L, 2339L, 2340L, 2341L, 2342L,
2343L, 2344L, 2345L, 2346L, 2347L, 2348L, 2349L, 2350L, 2351L,
2352L, 2353L, 2354L, 2355L, 2356L, 2357L, 2358L, 2359L, 2360L,
2361L, 2362L, 2363L, 2364L, 2365L, 2366L, 2367L, 2368L, 2369L,
2370L, 2371L, 2372L, 2373L, 2374L, 2375L, 2376L, 2377L, 2378L,
2379L, 2380L, 2381L, 2382L, 2383L, 2384L, 2385L, 2386L, 2387L,
2388L, 2389L, 2390L, 2391L, 2392L, 2393L, 2394L, 2395L, 2396L,
2397L, 2398L, 2399L, 2400L, 2401L, 2402L, 2403L, 2404L, 2405L,
2406L, 2407L, 2408L, 2409L, 2410L, 2411L, 2412L, 2413L, 2414L,
2415L, 2416L, 2417L, 2418L, 2419L, 2420L, 2421L, 2422L, 2423L,
2424L, 2425L, 2426L, 2427L, 2428L, 2429L, 2430L, 2431L, 2432L,
2433L, 2434L, 2435L, 2436L, 2437L, 2438L, 2439L, 2440L, 2441L,
2442L, 2443L, 2444L, 2445L, 2446L, 2447L, 2448L, 2449L, 2450L,
2451L, 2452L, 2453L, 2454L, 2455L, 2456L, 2457L, 2458L, 2459L,
2460L, 2461L, 2462L, 2463L, 2464L, 2465L, 2678L, 2679L, 2680L,
2681L, 2682L, 2683L, 2684L, 2685L, 2686L, 2687L, 2688L, 2689L,
2690L, 2691L, 2692L, 2693L, 2694L, 2695L, 2696L, 2697L, 2698L,
2699L, 2700L, 2701L, 2702L, 2703L, 2704L, 2705L, 2706L, 2707L,
2708L, 2709L, 2710L, 2711L, 2712L, 2713L, 2714L, 2715L, 2716L,
2717L, 2718L, 2719L, 2720L, 2721L, 2722L, 2723L, 2724L, 2725L,
2726L, 2727L, 2728L, 2729L, 2730L, 2731L, 2732L, 2733L, 2734L,
2735L, 2736L, 2737L, 2738L, 2739L, 2740L, 2741L, 2742L, 2743L,
2744L, 2745L, 2746L, 2747L, 2748L, 2749L, 2750L, 2751L, 2752L,
2753L, 2754L, 2755L, 2756L, 2757L, 2758L, 2759L, 2760L, 2761L,
2762L, 2763L, 2764L, 2765L, 2766L, 2767L, 2768L, 2769L, 2770L,
2771L, 2772L, 2773L, 2774L, 2775L, 2776L, 2777L, 2778L, 2779L,
2780L, 2781L, 2782L, 2783L, 2784L, 2785L, 2786L, 2787L, 2788L,
2789L, 2790L, 2791L, 2792L, 2793L, 2794L, 2795L, 2796L, 2797L,
2798L, 2799L, 2800L, 2801L, 2802L, 2803L, 2804L, 2805L, 2806L,
2807L, 2808L, 2809L, 2810L, 2811L, 2812L, 2813L, 2814L, 2815L,
2816L, 2817L, 2818L, 2819L, 2820L, 2821L, 2822L, 2823L, 2824L,
2825L, 2826L, 2827L, 2828L, 2829L, 2830L, 3043L, 3044L, 3045L,
3046L, 3047L, 3048L, 3049L, 3050L, 3051L, 3052L, 3053L, 3054L,
3055L, 3056L, 3057L, 3058L, 3059L, 3060L, 3061L, 3062L, 3063L,
3064L, 3065L, 3066L, 3067L, 3068L, 3069L, 3070L, 3071L, 3072L,
3073L, 3074L, 3075L, 3076L, 3077L, 3078L, 3079L, 3080L, 3081L,
3082L, 3083L, 3084L, 3085L, 3086L, 3087L, 3088L, 3089L, 3090L,
3091L, 3092L, 3093L, 3094L, 3095L, 3096L, 3097L, 3098L, 3099L,
3100L, 3101L, 3102L, 3103L, 3104L, 3105L, 3106L, 3107L, 3108L,
3109L, 3110L, 3111L, 3112L, 3113L, 3114L, 3115L, 3116L, 3117L,
3118L, 3119L, 3120L, 3121L, 3122L, 3123L, 3124L, 3125L, 3126L,
3127L, 3128L, 3129L, 3130L, 3131L, 3132L, 3133L, 3134L, 3135L,
3136L, 3137L, 3138L, 3139L, 3140L, 3141L, 3142L, 3143L, 3144L,
3145L, 3146L, 3147L, 3148L, 3149L, 3150L, 3151L, 3152L, 3153L,
3154L, 3155L, 3156L, 3157L, 3158L, 3159L, 3160L, 3161L, 3162L,
3163L, 3164L, 3165L, 3166L, 3167L, 3168L, 3169L, 3170L, 3171L,
3172L, 3173L, 3174L, 3175L, 3176L, 3177L, 3178L, 3179L, 3180L,
3181L, 3182L, 3183L, 3184L, 3185L, 3186L, 3187L, 3188L, 3189L,
3190L, 3191L, 3192L, 3193L, 3194L, 3195L, 3409L, 3410L, 3411L,
3412L, 3413L, 3414L, 3415L, 3416L, 3417L, 3418L, 3419L, 3420L,
3421L, 3422L, 3423L, 3424L, 3425L, 3426L, 3427L, 3428L, 3429L,
3430L, 3431L, 3432L, 3433L, 3434L, 3435L, 3436L, 3437L, 3438L,
3439L, 3440L, 3441L, 3442L, 3443L, 3444L, 3445L, 3446L, 3447L,
3448L, 3449L, 3450L, 3451L, 3452L, 3453L, 3454L, 3455L, 3456L,
3457L, 3458L, 3459L, 3460L, 3461L, 3462L, 3463L, 3464L, 3465L,
3466L, 3467L, 3468L, 3469L, 3470L, 3471L, 3472L, 3473L, 3474L,
3475L, 3476L, 3477L, 3478L, 3479L, 3480L, 3481L, 3482L, 3483L,
3484L, 3485L, 3486L, 3487L, 3488L, 3489L, 3490L, 3491L, 3492L,
3493L, 3494L, 3495L, 3496L, 3497L, 3498L, 3499L, 3500L, 3501L,
3502L, 3503L, 3504L, 3505L, 3506L, 3507L, 3508L, 3509L, 3510L,
3511L, 3512L, 3513L, 3514L, 3515L, 3516L, 3517L, 3518L, 3519L,
3520L, 3521L, 3522L, 3523L, 3524L, 3525L, 3526L, 3527L, 3528L,
3529L, 3530L, 3531L, 3532L, 3533L, 3534L, 3535L, 3536L, 3537L,
3538L, 3539L, 3540L, 3541L, 3542L, 3543L, 3544L, 3545L, 3546L,
3547L, 3548L, 3549L, 3550L, 3551L, 3552L, 3553L, 3554L, 3555L,
3556L, 3557L, 3558L, 3559L, 3560L, 3561L, 3774L, 3775L, 3776L,
3777L, 3778L, 3779L, 3780L, 3781L, 3782L, 3783L, 3784L, 3785L,
3786L, 3787L, 3788L, 3789L, 3790L, 3791L, 3792L, 3793L, 3794L,
3795L, 3796L, 3797L, 3798L, 3799L, 3800L, 3801L, 3802L, 3803L,
3804L, 3805L, 3806L, 3807L, 3808L, 3809L, 3810L, 3811L, 3812L,
3813L, 3814L, 3815L, 3816L, 3817L, 3818L, 3819L, 3820L, 3821L,
3822L, 3823L, 3824L, 3825L, 3826L, 3827L, 3828L, 3829L, 3830L,
3831L, 3832L, 3833L, 3834L, 3835L, 3836L, 3837L, 3838L, 3839L,
3840L, 3841L, 3842L, 3843L, 3844L, 3845L, 3846L, 3847L, 3848L,
3849L, 3850L, 3851L, 3852L, 3853L, 3854L, 3855L, 3856L, 3857L,
3858L, 3859L, 3860L, 3861L, 3862L, 3863L, 3864L, 3865L, 3866L,
3867L, 3868L, 3869L, 3870L, 3871L, 3872L, 3873L, 3874L, 3875L,
3876L, 3877L, 3878L, 3879L, 3880L, 3881L, 3882L, 3883L, 3884L,
3885L, 3886L, 3887L, 3888L, 3889L, 3890L, 3891L, 3892L, 3893L,
3894L, 3895L, 3896L, 3897L, 3898L, 3899L, 3900L, 3901L, 3902L,
3903L, 3904L, 3905L, 3906L, 3907L, 3908L, 3909L, 3910L, 3911L,
3912L, 3913L, 3914L, 3915L, 3916L, 3917L, 3918L, 3919L, 3920L,
3921L, 3922L, 3923L, 3924L, 3925L, 3926L, 4139L, 4140L, 4141L,
4142L, 4143L, 4144L, 4145L, 4146L, 4147L, 4148L, 4149L, 4150L,
4151L, 4152L, 4153L, 4154L, 4155L, 4156L, 4157L, 4158L, 4159L,
4160L, 4161L, 4162L, 4163L, 4164L, 4165L, 4166L, 4167L, 4168L,
4169L, 4170L, 4171L, 4172L, 4173L, 4174L, 4175L, 4176L, 4177L,
4178L, 4179L, 4180L, 4181L, 4182L, 4183L, 4184L, 4185L, 4186L,
4187L, 4188L, 4189L, 4190L, 4191L, 4192L, 4193L, 4194L, 4195L,
4196L, 4197L, 4198L, 4199L, 4200L, 4201L, 4202L, 4203L, 4204L,
4205L, 4206L, 4207L, 4208L, 4209L, 4210L, 4211L, 4212L, 4213L,
4214L, 4215L, 4216L, 4217L, 4218L, 4219L, 4220L, 4221L, 4222L,
4223L, 4224L, 4225L, 4226L, 4227L, 4228L, 4229L, 4230L, 4231L,
4232L, 4233L, 4234L, 4235L, 4236L, 4237L, 4238L, 4239L, 4240L,
4241L, 4242L, 4243L, 4244L, 4245L, 4246L, 4247L, 4248L, 4249L,
4250L, 4251L, 4252L, 4253L, 4254L, 4255L, 4256L, 4257L, 4258L,
4259L, 4260L, 4261L, 4262L, 4263L, 4264L, 4265L, 4266L, 4267L,
4268L, 4269L, 4270L, 4271L, 4272L, 4273L, 4274L, 4275L, 4276L,
4277L, 4278L, 4279L, 4280L, 4281L, 4282L, 4283L, 4284L, 4285L,
4286L, 4287L, 4288L, 4289L, 4290L, 4291L, 4504L, 4505L, 4506L,
4507L, 4508L, 4509L, 4510L, 4511L, 4512L, 4513L, 4514L, 4515L,
4516L, 4517L, 4518L, 4519L, 4520L, 4521L, 4522L, 4523L, 4524L,
4525L, 4526L, 4527L, 4528L, 4529L, 4530L, 4531L, 4532L, 4533L,
4534L, 4535L, 4536L, 4537L, 4538L, 4539L, 4540L, 4541L, 4542L,
4543L, 4544L, 4545L, 4546L, 4547L, 4548L, 4549L, 4550L, 4551L,
4552L, 4553L, 4554L, 4555L, 4556L, 4557L, 4558L, 4559L, 4560L,
4561L, 4562L, 4563L, 4564L, 4565L, 4566L, 4567L, 4568L, 4569L,
4570L, 4571L, 4572L, 4573L, 4574L, 4575L, 4576L, 4577L, 4578L,
4579L, 4580L, 4581L, 4582L, 4583L, 4584L, 4585L, 4586L, 4587L,
4588L, 4589L, 4590L, 4591L, 4592L, 4593L, 4594L, 4595L, 4596L,
4597L, 4598L, 4599L, 4600L, 4601L, 4602L, 4603L, 4604L, 4605L,
4606L, 4607L, 4608L, 4609L, 4610L, 4611L, 4612L, 4613L, 4614L,
4615L, 4616L, 4617L, 4618L, 4619L, 4620L, 4621L, 4622L, 4623L,
4624L, 4625L, 4626L, 4627L, 4628L, 4629L, 4630L, 4631L, 4632L,
4633L, 4634L, 4635L, 4636L, 4637L, 4638L, 4639L, 4640L, 4641L,
4642L, 4643L, 4644L, 4645L, 4646L, 4647L, 4648L, 4649L, 4650L,
4651L, 4652L, 4653L, 4654L, 4655L, 4656L, 4870L, 4871L, 4872L,
4873L, 4874L, 4875L, 4876L, 4877L, 4878L, 4879L, 4880L, 4881L,
4882L, 4883L, 4884L, 4885L, 4886L, 4887L, 4888L, 4889L, 4890L,
4891L, 4892L, 4893L, 4894L, 4895L, 4896L, 4897L, 4898L, 4899L,
4900L, 4901L, 4902L, 4903L, 4904L, 4905L, 4906L, 4907L, 4908L,
4909L, 4910L, 4911L, 4912L, 4913L, 4914L, 4915L, 4916L, 4917L,
4918L, 4919L, 4920L, 4921L, 4922L, 4923L, 4924L, 4925L, 4926L,
4927L, 4928L, 4929L, 4930L, 4931L, 4932L, 4933L, 4934L, 4935L,
4936L, 4937L, 4938L, 4939L, 4940L, 4941L, 4942L, 4943L, 4944L,
4945L, 4946L, 4947L, 4948L, 4949L, 4950L, 4951L, 4952L, 4953L,
4954L, 4955L, 4956L, 4957L, 4958L, 4959L, 4960L, 4961L, 4962L,
4963L, 4964L, 4965L, 4966L, 4967L, 4968L, 4969L, 4970L, 4971L,
4972L, 4973L, 4974L, 4975L, 4976L, 4977L, 4978L, 4979L, 4980L,
4981L, 4982L, 4983L, 4984L, 4985L, 4986L, 4987L, 4988L, 4989L,
4990L, 4991L, 4992L, 4993L, 4994L, 4995L, 4996L, 4997L, 4998L,
4999L, 5000L, 5001L, 5002L, 5003L, 5004L, 5005L, 5006L, 5007L,
5008L, 5009L, 5010L, 5011L, 5012L, 5013L, 5014L, 5015L, 5016L,
5017L, 5018L, 5019L, 5020L, 5021L, 5022L, 5235L, 5236L, 5237L,
5238L, 5239L, 5240L, 5241L, 5242L, 5243L, 5244L, 5245L, 5246L,
5247L, 5248L, 5249L, 5250L, 5251L, 5252L, 5253L, 5254L, 5255L,
5256L, 5257L, 5258L, 5259L, 5260L, 5261L, 5262L, 5263L, 5264L,
5265L, 5266L, 5267L, 5268L, 5269L, 5270L, 5271L, 5272L, 5273L,
5274L, 5275L, 5276L, 5277L, 5278L, 5279L, 5280L, 5281L, 5282L,
5283L, 5284L, 5285L, 5286L, 5287L, 5288L, 5289L, 5290L, 5291L,
5292L, 5293L, 5294L, 5295L, 5296L, 5297L, 5298L, 5299L, 5300L,
5301L, 5302L, 5303L, 5304L, 5305L, 5306L, 5307L, 5308L, 5309L,
5310L, 5311L, 5312L, 5313L, 5314L, 5315L, 5316L, 5317L, 5318L,
5319L, 5320L, 5321L, 5322L, 5323L, 5324L, 5325L, 5326L, 5327L,
5328L, 5329L, 5330L, 5331L, 5332L, 5333L, 5334L, 5335L, 5336L,
5337L, 5338L, 5339L, 5340L, 5341L, 5342L, 5343L, 5344L, 5345L,
5346L, 5347L, 5348L, 5349L, 5350L, 5351L, 5352L, 5353L, 5354L,
5355L, 5356L, 5357L, 5358L, 5359L, 5360L, 5361L, 5362L, 5363L,
5364L, 5365L, 5366L, 5367L, 5368L, 5369L, 5370L, 5371L, 5372L,
5373L, 5374L, 5375L, 5376L, 5377L, 5378L, 5379L, 5380L, 5381L,
5382L, 5383L, 5384L, 5385L, 5386L, 5387L, 5600L, 5601L, 5602L,
5603L, 5604L, 5605L, 5606L, 5607L, 5608L, 5609L, 5610L, 5611L,
5612L, 5613L, 5614L, 5615L, 5616L, 5617L, 5618L, 5619L, 5620L,
5621L, 5622L, 5623L, 5624L, 5625L, 5626L, 5627L, 5628L, 5629L,
5630L, 5631L, 5632L, 5633L, 5634L, 5635L, 5636L, 5637L, 5638L,
5639L, 5640L, 5641L, 5642L, 5643L, 5644L, 5645L, 5646L, 5647L,
5648L, 5649L, 5650L, 5651L, 5652L, 5653L, 5654L, 5655L, 5656L,
5657L, 5658L, 5659L, 5660L, 5661L, 5662L, 5663L, 5664L, 5665L,
5666L, 5667L, 5668L, 5669L, 5670L, 5671L, 5672L, 5673L, 5674L,
5675L, 5676L, 5677L, 5678L, 5679L, 5680L, 5681L, 5682L, 5683L,
5684L, 5685L, 5686L, 5687L, 5688L, 5689L, 5690L, 5691L, 5692L,
5693L, 5694L, 5695L, 5696L, 5697L, 5698L, 5699L, 5700L, 5701L,
5702L, 5703L, 5704L, 5705L, 5706L, 5707L, 5708L, 5709L, 5710L,
5711L, 5712L, 5713L, 5714L, 5715L, 5716L, 5717L, 5718L, 5719L,
5720L, 5721L, 5722L, 5723L, 5724L, 5725L, 5726L, 5727L, 5728L,
5729L, 5730L, 5731L, 5732L, 5733L, 5734L, 5735L, 5736L, 5737L,
5738L, 5739L, 5740L, 5741L, 5742L, 5743L, 5744L, 5745L, 5746L,
5747L, 5748L, 5749L, 5750L, 5751L, 5752L, 5965L, 5966L, 5967L,
5968L, 5969L, 5970L, 5971L, 5972L, 5973L, 5974L, 5975L, 5976L,
5977L, 5978L, 5979L, 5980L, 5981L, 5982L, 5983L, 5984L, 5985L,
5986L, 5987L, 5988L, 5989L, 5990L, 5991L, 5992L, 5993L, 5994L,
5995L, 5996L, 5997L, 5998L, 5999L, 6000L, 6001L, 6002L, 6003L,
6004L, 6005L, 6006L, 6007L, 6008L, 6009L, 6010L, 6011L, 6012L,
6013L, 6014L, 6015L, 6016L, 6017L, 6018L, 6019L, 6020L, 6021L,
6022L, 6023L, 6024L, 6025L, 6026L, 6027L, 6028L, 6029L, 6030L,
6031L, 6032L, 6033L, 6034L, 6035L, 6036L, 6037L, 6038L, 6039L,
6040L, 6041L, 6042L, 6043L, 6044L, 6045L, 6046L, 6047L, 6048L,
6049L, 6050L, 6051L, 6052L, 6053L, 6054L, 6055L, 6056L, 6057L,
6058L, 6059L, 6060L, 6061L, 6062L, 6063L, 6064L, 6065L, 6066L,
6067L, 6068L, 6069L, 6070L, 6071L, 6072L, 6073L, 6074L, 6075L,
6076L, 6077L, 6078L, 6079L, 6080L, 6081L, 6082L, 6083L, 6084L,
6085L, 6086L, 6087L, 6088L, 6089L, 6090L, 6091L, 6092L, 6093L,
6094L, 6095L, 6096L, 6097L, 6098L, 6099L, 6100L, 6101L, 6102L,
6103L, 6104L, 6105L, 6106L, 6107L, 6108L, 6109L, 6110L, 6111L,
6112L, 6113L, 6114L, 6115L, 6116L, 6117L, 6331L, 6332L, 6333L,
6334L, 6335L, 6336L, 6337L, 6338L, 6339L, 6340L, 6341L, 6342L,
6343L, 6344L, 6345L, 6346L, 6347L, 6348L, 6349L, 6350L, 6351L,
6352L, 6353L, 6354L, 6355L, 6356L, 6357L, 6358L, 6359L, 6360L,
6361L, 6362L, 6363L, 6364L, 6365L, 6366L, 6367L, 6368L, 6369L,
6370L, 6371L, 6372L, 6373L, 6374L, 6375L, 6376L, 6377L, 6378L,
6379L, 6380L, 6381L, 6382L, 6383L, 6384L, 6385L, 6386L, 6387L,
6388L, 6389L, 6390L, 6391L, 6392L, 6393L, 6394L, 6395L, 6396L,
6397L, 6398L, 6399L, 6400L, 6401L, 6402L, 6403L, 6404L, 6405L,
6406L, 6407L, 6408L, 6409L, 6410L, 6411L, 6412L, 6413L, 6414L,
6415L, 6416L, 6417L, 6418L, 6419L, 6420L, 6421L, 6422L, 6423L,
6424L, 6425L, 6426L, 6427L, 6428L, 6429L, 6430L, 6431L, 6432L,
6433L, 6434L, 6435L, 6436L, 6437L, 6438L, 6439L, 6440L, 6441L,
6442L, 6443L, 6444L, 6445L, 6446L, 6447L, 6448L, 6449L, 6450L,
6451L, 6452L, 6453L, 6454L, 6455L, 6456L, 6457L, 6458L, 6459L,
6460L, 6461L, 6462L, 6463L, 6464L, 6465L, 6466L, 6467L, 6468L,
6469L, 6470L, 6471L, 6472L, 6473L, 6474L, 6475L, 6476L, 6477L,
6478L, 6479L, 6480L, 6481L, 6482L, 6483L, 6696L, 6697L, 6698L,
6699L, 6700L, 6701L, 6702L, 6703L, 6704L, 6705L, 6706L, 6707L,
6708L, 6709L, 6710L, 6711L, 6712L, 6713L, 6714L, 6715L, 6716L,
6717L, 6718L, 6719L, 6720L, 6721L, 6722L, 6723L, 6724L, 6725L,
6726L, 6727L, 6728L, 6729L, 6730L, 6731L, 6732L, 6733L, 6734L,
6735L, 6736L, 6737L, 6738L, 6739L, 6740L, 6741L, 6742L, 6743L,
6744L, 6745L, 6746L, 6747L, 6748L, 6749L, 6750L, 6751L, 6752L,
6753L, 6754L, 6755L, 6756L, 6757L, 6758L, 6759L, 6760L, 6761L,
6762L, 6763L, 6764L, 6765L, 6766L, 6767L, 6768L, 6769L, 6770L,
6771L, 6772L, 6773L, 6774L, 6775L, 6776L, 6777L, 6778L, 6779L,
6780L, 6781L, 6782L, 6783L, 6784L, 6785L, 6786L, 6787L, 6788L,
6789L, 6790L, 6791L, 6792L, 6793L, 6794L, 6795L, 6796L, 6797L,
6798L, 6799L, 6800L, 6801L, 6802L, 6803L, 6804L, 6805L, 6806L,
6807L, 6808L, 6809L, 6810L, 6811L, 6812L, 6813L, 6814L, 6815L,
6816L, 6817L, 6818L, 6819L, 6820L, 6821L, 6822L, 6823L, 6824L,
6825L, 6826L, 6827L, 6828L, 6829L, 6830L, 6831L, 6832L, 6833L,
6834L, 6835L, 6836L, 6837L, 6838L, 6839L, 6840L, 6841L, 6842L,
6843L, 6844L, 6845L, 6846L, 6847L, 6848L, 7061L, 7062L, 7063L,
7064L, 7065L, 7066L, 7067L, 7068L, 7069L, 7070L, 7071L, 7072L,
7073L, 7074L, 7075L, 7076L, 7077L, 7078L, 7079L, 7080L, 7081L,
7082L, 7083L, 7084L, 7085L, 7086L, 7087L, 7088L, 7089L, 7090L,
7091L, 7092L, 7093L, 7094L, 7095L, 7096L, 7097L, 7098L, 7099L,
7100L, 7101L, 7102L, 7103L, 7104L, 7105L, 7106L, 7107L, 7108L,
7109L, 7110L, 7111L, 7112L, 7113L, 7114L, 7115L, 7116L, 7117L,
7118L, 7119L, 7120L, 7121L, 7122L, 7123L, 7124L, 7125L, 7126L,
7127L, 7128L, 7129L, 7130L, 7131L, 7132L, 7133L, 7134L, 7135L,
7136L, 7137L, 7138L, 7139L, 7140L, 7141L, 7142L, 7143L, 7144L,
7145L, 7146L, 7147L, 7148L, 7149L, 7150L, 7151L, 7152L, 7153L,
7154L, 7155L, 7156L, 7157L, 7158L, 7159L, 7160L, 7161L, 7162L,
7163L, 7164L, 7165L, 7166L, 7167L, 7168L, 7169L, 7170L, 7171L,
7172L, 7173L, 7174L, 7175L, 7176L, 7177L, 7178L, 7179L, 7180L,
7181L, 7182L, 7183L, 7184L, 7185L, 7186L, 7187L, 7188L, 7189L,
7190L, 7191L, 7192L, 7193L, 7194L, 7195L, 7196L, 7197L, 7198L,
7199L, 7200L, 7201L, 7202L, 7203L, 7204L, 7205L, 7206L, 7207L,
7208L, 7209L, 7210L, 7211L, 7212L, 7213L, 7426L, 7427L, 7428L,
7429L, 7430L, 7431L, 7432L, 7433L, 7434L, 7435L, 7436L, 7437L,
7438L, 7439L, 7440L, 7441L, 7442L, 7443L, 7444L, 7445L, 7446L,
7447L, 7448L, 7449L, 7450L, 7451L, 7452L, 7453L, 7454L, 7455L,
7456L, 7457L, 7458L, 7459L, 7460L, 7461L, 7462L, 7463L, 7464L,
7465L, 7466L, 7467L, 7468L, 7469L, 7470L, 7471L, 7472L, 7473L,
7474L, 7475L, 7476L, 7477L, 7478L, 7479L, 7480L, 7481L, 7482L,
7483L, 7484L, 7485L, 7486L, 7487L, 7488L, 7489L, 7490L, 7491L,
7492L, 7493L, 7494L, 7495L, 7496L, 7497L, 7498L, 7499L, 7500L,
7501L, 7502L, 7503L, 7504L, 7505L, 7506L, 7507L, 7508L, 7509L,
7510L, 7511L, 7512L, 7513L, 7514L, 7515L, 7516L, 7517L, 7518L,
7519L, 7520L, 7521L, 7522L, 7523L, 7524L, 7525L, 7526L, 7527L,
7528L, 7529L, 7530L, 7531L, 7532L, 7533L, 7534L, 7535L, 7536L,
7537L, 7538L, 7539L, 7540L, 7541L, 7542L, 7543L, 7544L, 7545L,
7546L, 7547L, 7548L, 7549L, 7550L, 7551L, 7552L, 7553L, 7554L,
7555L, 7556L, 7557L, 7558L, 7559L, 7560L, 7561L, 7562L, 7563L,
7564L, 7565L, 7566L, 7567L, 7568L, 7569L, 7570L, 7571L, 7572L,
7573L, 7574L, 7575L, 7576L, 7577L, 7578L, 7792L, 7793L, 7794L,
7795L, 7796L, 7797L, 7798L, 7799L, 7800L, 7801L, 7802L, 7803L,
7804L, 7805L, 7806L, 7807L, 7808L, 7809L, 7810L, 7811L, 7812L,
7813L, 7814L, 7815L, 7816L, 7817L, 7818L, 7819L, 7820L, 7821L,
7822L, 7823L, 7824L, 7825L, 7826L, 7827L, 7828L, 7829L, 7830L,
7831L, 7832L, 7833L, 7834L, 7835L, 7836L, 7837L, 7838L, 7839L,
7840L, 7841L, 7842L, 7843L, 7844L, 7845L, 7846L, 7847L, 7848L,
7849L, 7850L, 7851L, 7852L, 7853L, 7854L, 7855L, 7856L, 7857L,
7858L, 7859L, 7860L, 7861L, 7862L, 7863L, 7864L, 7865L, 7866L,
7867L, 7868L, 7869L, 7870L, 7871L, 7872L, 7873L, 7874L, 7875L,
7876L, 7877L, 7878L, 7879L, 7880L, 7881L, 7882L, 7883L, 7884L,
7885L, 7886L, 7887L, 7888L, 7889L, 7890L, 7891L, 7892L, 7893L,
7894L, 7895L, 7896L, 7897L, 7898L, 7899L, 7900L, 7901L, 7902L,
7903L, 7904L, 7905L, 7906L, 7907L, 7908L, 7909L, 7910L, 7911L,
7912L, 7913L, 7914L, 7915L, 7916L, 7917L, 7918L, 7919L, 7920L,
7921L, 7922L, 7923L, 7924L, 7925L, 7926L, 7927L, 7928L, 7929L,
7930L, 7931L, 7932L, 7933L, 7934L, 7935L, 7936L, 7937L, 7938L,
7939L, 7940L, 7941L, 7942L, 7943L, 7944L, 8157L, 8158L, 8159L,
8160L, 8161L, 8162L, 8163L, 8164L, 8165L, 8166L, 8167L, 8168L,
8169L, 8170L, 8171L, 8172L, 8173L, 8174L, 8175L, 8176L, 8177L,
8178L, 8179L, 8180L, 8181L, 8182L, 8183L, 8184L, 8185L, 8186L,
8187L, 8188L, 8189L, 8190L, 8191L, 8192L, 8193L, 8194L, 8195L,
8196L, 8197L, 8198L, 8199L, 8200L, 8201L, 8202L, 8203L, 8204L,
8205L, 8206L, 8207L, 8208L, 8209L, 8210L, 8211L, 8212L, 8213L,
8214L, 8215L, 8216L, 8217L, 8218L, 8219L, 8220L, 8221L, 8222L,
8223L, 8224L, 8225L, 8226L, 8227L, 8228L, 8229L, 8230L, 8231L,
8232L, 8233L, 8234L, 8235L, 8236L, 8237L, 8238L, 8239L, 8240L,
8241L, 8242L, 8243L, 8244L, 8245L, 8246L, 8247L, 8248L, 8249L,
8250L, 8251L, 8252L, 8253L, 8254L, 8255L, 8256L, 8257L, 8258L,
8259L, 8260L, 8261L, 8262L, 8263L, 8264L, 8265L, 8266L, 8267L,
8268L, 8269L, 8270L, 8271L, 8272L, 8273L, 8274L, 8275L, 8276L,
8277L, 8278L, 8279L, 8280L, 8281L, 8282L, 8283L, 8284L, 8285L,
8286L, 8287L, 8288L, 8289L, 8290L, 8291L, 8292L, 8293L, 8294L,
8295L, 8296L, 8297L, 8298L, 8299L, 8300L, 8301L, 8302L, 8303L,
8304L, 8305L, 8306L, 8307L, 8308L, 8309L, 8522L, 8523L, 8524L,
8525L, 8526L, 8527L, 8528L, 8529L, 8530L, 8531L, 8532L, 8533L,
8534L, 8535L, 8536L, 8537L, 8538L, 8539L, 8540L, 8541L, 8542L,
8543L, 8544L, 8545L, 8546L, 8547L, 8548L, 8549L, 8550L, 8551L,
8552L, 8553L, 8554L, 8555L, 8556L, 8557L, 8558L, 8559L, 8560L,
8561L, 8562L, 8563L, 8564L, 8565L, 8566L, 8567L, 8568L, 8569L,
8570L, 8571L, 8572L, 8573L, 8574L, 8575L, 8576L, 8577L, 8578L,
8579L, 8580L, 8581L, 8582L, 8583L, 8584L, 8585L, 8586L, 8587L,
8588L, 8589L, 8590L, 8591L, 8592L, 8593L, 8594L, 8595L, 8596L,
8597L, 8598L, 8599L, 8600L, 8601L, 8602L, 8603L, 8604L, 8605L,
8606L, 8607L, 8608L, 8609L, 8610L, 8611L, 8612L, 8613L, 8614L,
8615L, 8616L, 8617L, 8618L, 8619L, 8620L, 8621L, 8622L, 8623L,
8624L, 8625L, 8626L, 8627L, 8628L, 8629L, 8630L, 8631L, 8632L,
8633L, 8634L, 8635L, 8636L, 8637L, 8638L, 8639L, 8640L, 8641L,
8642L, 8643L, 8644L, 8645L, 8646L, 8647L, 8648L, 8649L, 8650L,
8651L, 8652L, 8653L, 8654L, 8655L, 8656L, 8657L, 8658L, 8659L,
8660L, 8661L, 8662L, 8663L, 8664L, 8665L, 8666L, 8667L, 8668L,
8669L, 8670L, 8671L, 8672L, 8673L, 8674L, 8887L, 8888L, 8889L,
8890L, 8891L, 8892L, 8893L, 8894L, 8895L, 8896L, 8897L, 8898L,
8899L, 8900L, 8901L, 8902L, 8903L, 8904L, 8905L, 8906L, 8907L,
8908L, 8909L, 8910L, 8911L, 8912L, 8913L, 8914L, 8915L, 8916L,
8917L, 8918L, 8919L, 8920L, 8921L, 8922L, 8923L, 8924L, 8925L,
8926L, 8927L, 8928L, 8929L, 8930L, 8931L, 8932L, 8933L, 8934L,
8935L, 8936L, 8937L, 8938L, 8939L, 8940L, 8941L, 8942L, 8943L,
8944L, 8945L, 8946L, 8947L, 8948L, 8949L, 8950L, 8951L, 8952L,
8953L, 8954L, 8955L, 8956L, 8957L, 8958L, 8959L, 8960L, 8961L,
8962L, 8963L, 8964L, 8965L, 8966L, 8967L, 8968L, 8969L, 8970L,
8971L, 8972L, 8973L, 8974L, 8975L, 8976L, 8977L, 8978L, 8979L,
8980L, 8981L, 8982L, 8983L, 8984L, 8985L, 8986L, 8987L, 8988L,
8989L, 8990L, 8991L, 8992L, 8993L, 8994L, 8995L, 8996L, 8997L,
8998L, 8999L, 9000L, 9001L, 9002L, 9003L, 9004L, 9005L, 9006L,
9007L, 9008L, 9009L, 9010L, 9011L, 9012L, 9013L, 9014L, 9015L,
9016L, 9017L, 9018L, 9019L, 9020L, 9021L, 9022L, 9023L, 9024L,
9025L, 9026L, 9027L, 9028L, 9029L, 9030L, 9031L, 9032L, 9033L,
9034L, 9035L, 9036L, 9037L, 9038L, 9039L, 9253L, 9254L, 9255L,
9256L, 9257L, 9258L, 9259L, 9260L, 9261L, 9262L, 9263L, 9264L,
9265L, 9266L, 9267L, 9268L, 9269L, 9270L, 9271L, 9272L, 9273L,
9274L, 9275L, 9276L, 9277L, 9278L, 9279L, 9280L, 9281L, 9282L,
9283L, 9284L, 9285L, 9286L, 9287L, 9288L, 9289L, 9290L, 9291L,
9292L, 9293L, 9294L, 9295L, 9296L, 9297L, 9298L, 9299L, 9300L,
9301L, 9302L, 9303L, 9304L, 9305L, 9306L, 9307L, 9308L, 9309L,
9310L, 9311L, 9312L, 9313L, 9314L, 9315L, 9316L, 9317L, 9318L,
9319L, 9320L, 9321L, 9322L, 9323L, 9324L, 9325L, 9326L, 9327L,
9328L, 9329L, 9330L, 9331L, 9332L, 9333L, 9334L, 9335L, 9336L,
9337L, 9338L, 9339L, 9340L, 9341L, 9342L, 9343L, 9344L, 9345L,
9346L, 9347L, 9348L, 9349L, 9350L, 9351L, 9352L, 9353L, 9354L,
9355L, 9356L, 9357L, 9358L, 9359L, 9360L, 9361L, 9362L, 9363L,
9364L, 9365L, 9366L, 9367L, 9368L, 9369L, 9370L, 9371L, 9372L,
9373L, 9374L, 9375L, 9376L, 9377L, 9378L, 9379L, 9380L, 9381L,
9382L, 9383L, 9384L, 9385L, 9386L, 9387L, 9388L, 9389L, 9390L,
9391L, 9392L, 9393L, 9394L, 9395L, 9396L, 9397L, 9398L, 9399L,
9400L, 9401L, 9402L, 9403L, 9404L, 9405L, 9618L, 9619L, 9620L,
9621L, 9622L, 9623L, 9624L, 9625L, 9626L, 9627L, 9628L, 9629L,
9630L, 9631L, 9632L, 9633L, 9634L, 9635L, 9636L, 9637L, 9638L,
9639L, 9640L, 9641L, 9642L, 9643L, 9644L, 9645L, 9646L, 9647L,
9648L, 9649L, 9650L, 9651L, 9652L, 9653L, 9654L, 9655L, 9656L,
9657L, 9658L, 9659L, 9660L, 9661L, 9662L, 9663L, 9664L, 9665L,
9666L, 9667L, 9668L, 9669L, 9670L, 9671L, 9672L, 9673L, 9674L,
9675L, 9676L, 9677L, 9678L, 9679L, 9680L, 9681L, 9682L, 9683L,
9684L, 9685L, 9686L, 9687L, 9688L, 9689L, 9690L, 9691L, 9692L,
9693L, 9694L, 9695L, 9696L, 9697L, 9698L, 9699L, 9700L, 9701L,
9702L, 9703L, 9704L, 9705L, 9706L, 9707L, 9708L, 9709L, 9710L,
9711L, 9712L, 9713L, 9714L, 9715L, 9716L, 9717L, 9718L, 9719L,
9720L, 9721L, 9722L, 9723L, 9724L, 9725L, 9726L, 9727L, 9728L,
9729L, 9730L, 9731L, 9732L, 9733L, 9734L, 9735L, 9736L, 9737L,
9738L, 9739L, 9740L, 9741L, 9742L, 9743L, 9744L, 9745L, 9746L,
9747L, 9748L, 9749L, 9750L, 9751L, 9752L, 9753L, 9754L, 9755L,
9756L, 9757L, 9758L, 9759L, 9760L, 9761L, 9762L, 9763L, 9764L,
9765L, 9766L, 9767L, 9768L, 9769L, 9770L, 9983L, 9984L, 9985L,
9986L, 9987L, 9988L, 9989L, 9990L, 9991L, 9992L, 9993L, 9994L,
9995L, 9996L, 9997L, 9998L, 9999L, 10000L, 10001L, 10002L, 10003L,
10004L, 10005L, 10006L, 10007L, 10008L, 10009L, 10010L, 10011L,
10012L, 10013L, 10014L, 10015L, 10016L, 10017L, 10018L, 10019L,
10020L, 10021L, 10022L, 10023L, 10024L, 10025L, 10026L, 10027L,
10028L, 10029L, 10030L, 10031L, 10032L, 10033L, 10034L, 10035L,
10036L, 10037L, 10038L, 10039L, 10040L, 10041L, 10042L, 10043L,
10044L, 10045L, 10046L, 10047L, 10048L, 10049L, 10050L, 10051L,
10052L, 10053L, 10054L, 10055L, 10056L, 10057L, 10058L, 10059L,
10060L, 10061L, 10062L, 10063L, 10064L, 10065L, 10066L, 10067L,
10068L, 10069L, 10070L, 10071L, 10072L, 10073L, 10074L, 10075L,
10076L, 10077L, 10078L, 10079L, 10080L, 10081L, 10082L, 10083L,
10084L, 10085L, 10086L, 10087L, 10088L, 10089L, 10090L, 10091L,
10092L, 10093L, 10094L, 10095L, 10096L, 10097L, 10098L, 10099L,
10100L, 10101L, 10102L, 10103L, 10104L, 10105L, 10106L, 10107L,
10108L, 10109L, 10110L, 10111L, 10112L, 10113L, 10114L, 10115L,
10116L, 10117L, 10118L, 10119L, 10120L, 10121L, 10122L, 10123L,
10124L, 10125L, 10126L, 10127L, 10128L, 10129L, 10130L, 10131L,
10132L, 10133L, 10134L, 10135L, 10348L, 10349L, 10350L, 10351L,
10352L, 10353L, 10354L, 10355L, 10356L, 10357L, 10358L, 10359L,
10360L, 10361L, 10362L, 10363L, 10364L, 10365L, 10366L, 10367L,
10368L, 10369L, 10370L, 10371L, 10372L, 10373L, 10374L, 10375L,
10376L, 10377L, 10378L, 10379L, 10380L, 10381L, 10382L, 10383L,
10384L, 10385L, 10386L, 10387L, 10388L, 10389L, 10390L, 10391L,
10392L, 10393L, 10394L, 10395L, 10396L, 10397L, 10398L, 10399L,
10400L, 10401L, 10402L, 10403L, 10404L, 10405L, 10406L, 10407L,
10408L, 10409L, 10410L, 10411L, 10412L, 10413L, 10414L, 10415L,
10416L, 10417L, 10418L, 10419L, 10420L, 10421L, 10422L, 10423L,
10424L, 10425L, 10426L, 10427L, 10428L, 10429L, 10430L, 10431L,
10432L, 10433L, 10434L, 10435L, 10436L, 10437L, 10438L, 10439L,
10440L, 10441L, 10442L, 10443L, 10444L, 10445L, 10446L, 10447L,
10448L, 10449L, 10450L, 10451L, 10452L, 10453L, 10454L, 10455L,
10456L, 10457L, 10458L, 10459L, 10460L, 10461L, 10462L, 10463L,
10464L, 10465L, 10466L, 10467L, 10468L, 10469L, 10470L, 10471L,
10472L, 10473L, 10474L, 10475L, 10476L, 10477L, 10478L, 10479L,
10480L, 10481L, 10482L, 10483L, 10484L, 10485L, 10486L, 10487L,
10488L, 10489L, 10490L, 10491L, 10492L, 10493L, 10494L, 10495L,
10496L, 10497L, 10498L, 10499L, 10500L, 10714L, 10715L, 10716L,
10717L, 10718L, 10719L, 10720L, 10721L, 10722L, 10723L, 10724L,
10725L, 10726L, 10727L, 10728L, 10729L, 10730L, 10731L, 10732L,
10733L, 10734L, 10735L, 10736L, 10737L, 10738L, 10739L, 10740L,
10741L, 10742L, 10743L, 10744L, 10745L, 10746L, 10747L, 10748L,
10749L, 10750L, 10751L, 10752L, 10753L, 10754L, 10755L, 10756L,
10757L, 10758L, 10759L, 10760L, 10761L, 10762L, 10763L, 10764L,
10765L, 10766L, 10767L, 10768L, 10769L, 10770L, 10771L, 10772L,
10773L, 10774L, 10775L, 10776L, 10777L, 10778L, 10779L, 10780L,
10781L, 10782L, 10783L, 10784L, 10785L, 10786L, 10787L, 10788L,
10789L, 10790L, 10791L, 10792L, 10793L, 10794L, 10795L, 10796L,
10797L, 10798L, 10799L, 10800L, 10801L, 10802L, 10803L, 10804L,
10805L, 10806L, 10807L, 10808L, 10809L, 10810L, 10811L, 10812L,
10813L, 10814L, 10815L, 10816L, 10817L, 10818L, 10819L, 10820L,
10821L, 10822L, 10823L, 10824L, 10825L, 10826L, 10827L, 10828L,
10829L, 10830L, 10831L, 10832L, 10833L, 10834L, 10835L, 10836L,
10837L, 10838L, 10839L, 10840L, 10841L, 10842L, 10843L, 10844L,
10845L, 10846L, 10847L, 10848L, 10849L, 10850L, 10851L, 10852L,
10853L, 10854L, 10855L, 10856L, 10857L, 10858L, 10859L, 10860L,
10861L, 10862L, 10863L, 10864L, 10865L, 10866L), class = "data.frame")

And for temp:

structure(list(X = 0:4589, Year = c(1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L, 1975L,
1975L, 1975L, 1975L, 1975L, 1975L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L, 1976L,
1976L, 1976L, 1976L, 1976L, 1976L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L, 1977L,
1977L, 1977L, 1977L, 1977L, 1977L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L, 1978L,
1978L, 1978L, 1978L, 1978L, 1978L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L, 1979L,
1979L, 1979L, 1979L, 1979L, 1979L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L, 1980L,
1980L, 1980L, 1980L, 1980L, 1980L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L, 1981L,
1981L, 1981L, 1981L, 1981L, 1981L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L, 1982L,
1982L, 1982L, 1982L, 1982L, 1982L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L, 1983L,
1983L, 1983L, 1983L, 1983L, 1983L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L, 1984L,
1984L, 1984L, 1984L, 1984L, 1984L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L, 1985L,
1985L, 1985L, 1985L, 1985L, 1985L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L, 1986L,
1986L, 1986L, 1986L, 1986L, 1986L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L, 1987L,
1987L, 1987L, 1987L, 1987L, 1987L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L, 1988L,
1988L, 1988L, 1988L, 1988L, 1988L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L, 1989L,
1989L, 1989L, 1989L, 1989L, 1989L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L, 1990L,
1990L, 1990L, 1990L, 1990L, 1990L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L, 1991L,
1991L, 1991L, 1991L, 1991L, 1991L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L, 1992L,
1992L, 1992L, 1992L, 1992L, 1992L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L, 1993L,
1993L, 1993L, 1993L, 1993L, 1993L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L, 1994L,
1994L, 1994L, 1994L, 1994L, 1994L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L, 1995L,
1995L, 1995L, 1995L, 1995L, 1995L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L, 1996L,
1996L, 1996L, 1996L, 1996L, 1996L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L, 1997L,
1997L, 1997L, 1997L, 1997L, 1997L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L, 1998L,
1998L, 1998L, 1998L, 1998L, 1998L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L, 1999L,
1999L, 1999L, 1999L, 1999L, 1999L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L, 2000L,
2000L, 2000L, 2000L, 2000L, 2000L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L, 2001L,
2001L, 2001L, 2001L, 2001L, 2001L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L, 2002L,
2002L, 2002L, 2002L, 2002L, 2002L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L, 2003L,
2003L, 2003L, 2003L, 2003L, 2003L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L, 2004L,
2004L, 2004L, 2004L, 2004L, 2004L), Julian = c(121L, 122L, 123L,
124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L,
135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L,
146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L,
157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L,
168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L,
179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L,
190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L,
201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L,
212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L,
223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L,
234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L,
245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L,
256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L,
267L, 268L, 269L, 270L, 271L, 272L, 273L, 122L, 123L, 124L, 125L,
126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L,
137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L,
148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L,
159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L,
170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L,
181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L,
192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L,
203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L,
214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L,
225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L,
236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L,
247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L,
258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L,
269L, 270L, 271L, 272L, 273L, 274L, 121L, 122L, 123L, 124L, 125L,
126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L,
137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L,
148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L,
159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L,
170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L,
181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L,
192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L,
203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L,
214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L,
225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L,
236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L,
247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L,
258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L,
269L, 270L, 271L, 272L, 273L, 121L, 122L, 123L, 124L, 125L, 126L,
127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L,
138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L,
149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L,
160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L,
171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L,
182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L,
193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L,
204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L,
215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L,
226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L,
237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L,
248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L,
259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L,
270L, 271L, 272L, 273L, 121L, 122L, 123L, 124L, 125L, 126L, 127L,
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L,
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L,
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L,
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L,
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L,
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L,
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L,
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L,
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L,
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L,
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L,
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L,
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L,
271L, 272L, 273L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L,
130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L,
141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L,
152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L,
163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L,
174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L,
185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L,
196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L,
207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L,
218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L,
229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L,
240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L,
251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L,
262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L,
273L, 274L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L,
130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L,
141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L,
152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L,
163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L,
174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L,
185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L,
196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L,
207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L,
218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L,
229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L,
240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L,
251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L,
262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L,
273L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L,
131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L,
142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L,
153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L,
164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L,
175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L,
186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L,
197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L,
208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L,
219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L,
230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L,
241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L,
252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L,
263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L,
121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L,
132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L,
143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L,
154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L,
165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L,
176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L,
187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L,
198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L,
209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L,
220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L,
231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L,
242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L,
253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L,
264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 122L,
123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L,
134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L,
145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L,
156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L,
167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L,
178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L,
189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L,
200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L,
211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L,
222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L,
233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L,
244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L,
255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L,
266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 274L, 121L, 122L,
123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L,
134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L,
145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L,
156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L,
167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L,
178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L,
189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L,
200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L,
211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L,
222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L,
233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L,
244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L,
255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L,
266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 121L, 122L, 123L,
124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L,
135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L,
146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L,
157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L,
168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L,
179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L,
190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L,
201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L,
212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L,
223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L,
234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L,
245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L,
256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L,
267L, 268L, 269L, 270L, 271L, 272L, 273L, 121L, 122L, 123L, 124L,
125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L,
136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L,
147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L,
158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L,
169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L,
180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L,
191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L,
202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L,
213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L,
224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L,
235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L,
246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L,
257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L,
268L, 269L, 270L, 271L, 272L, 273L, 122L, 123L, 124L, 125L, 126L,
127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L,
138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L,
149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L,
160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L,
171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L,
182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L,
193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L,
204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L,
215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L,
226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L,
237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L,
248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L,
259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L,
270L, 271L, 272L, 273L, 274L, 121L, 122L, 123L, 124L, 125L, 126L,
127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L,
138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L,
149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L,
160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L,
171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L,
182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L,
193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L,
204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L,
215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L,
226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L,
237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L,
248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L,
259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L,
270L, 271L, 272L, 273L, 121L, 122L, 123L, 124L, 125L, 126L, 127L,
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L,
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L,
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L,
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L,
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L,
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L,
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L,
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L,
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L,
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L,
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L,
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L,
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L,
271L, 272L, 273L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L,
140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L,
151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L,
162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L,
173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L,
184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L,
195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L,
206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L,
217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L,
228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L,
239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L,
250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L,
261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L,
272L, 273L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L,
131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L,
142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L,
153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L,
164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L,
175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L,
186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L,
197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L,
208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L,
219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L,
230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L,
241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L,
252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L,
263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L,
274L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L,
131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L,
142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L,
153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L,
164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L,
175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L,
186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L,
197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L,
208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L,
219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L,
230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L,
241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L,
252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L,
263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L,
121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L,
132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L,
143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L,
154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L,
165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L,
176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L,
187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L,
198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L,
209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L,
220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L,
231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L,
242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L,
253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L,
264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 121L,
122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L,
133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L,
144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L,
155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L,
166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L,
177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L,
188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L,
199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L,
210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L,
221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L,
232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L,
243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L,
254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L,
265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 122L, 123L,
124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L,
135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L,
146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L,
157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L,
168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L,
179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L,
190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L,
201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L,
212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L,
223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L,
234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L,
245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L,
256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L,
267L, 268L, 269L, 270L, 271L, 272L, 273L, 274L, 121L, 122L, 123L,
124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L,
135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L,
146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L,
157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L,
168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L,
179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L,
190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L,
201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L,
212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L,
223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L,
234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L,
245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L,
256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L,
267L, 268L, 269L, 270L, 271L, 272L, 273L, 121L, 122L, 123L, 124L,
125L, 126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L,
136L, 137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L,
147L, 148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L,
158L, 159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L,
169L, 170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L,
180L, 181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L,
191L, 192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L,
202L, 203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L,
213L, 214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L,
224L, 225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L,
235L, 236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L,
246L, 247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L,
257L, 258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L,
268L, 269L, 270L, 271L, 272L, 273L, 121L, 122L, 123L, 124L, 125L,
126L, 127L, 128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L,
137L, 138L, 139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L,
148L, 149L, 150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L,
159L, 160L, 161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L,
170L, 171L, 172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L,
181L, 182L, 183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L,
192L, 193L, 194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L,
203L, 204L, 205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L,
214L, 215L, 216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L,
225L, 226L, 227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L,
236L, 237L, 238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L,
247L, 248L, 249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L,
258L, 259L, 260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L,
269L, 270L, 271L, 272L, 273L, 122L, 123L, 124L, 125L, 126L, 127L,
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L,
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L,
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L,
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L,
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L,
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L,
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L,
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L,
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L,
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L,
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L,
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L,
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L,
271L, 272L, 273L, 274L, 121L, 122L, 123L, 124L, 125L, 126L, 127L,
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L,
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L,
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L,
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L,
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L,
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L,
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L,
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L,
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L,
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L,
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L,
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L,
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L,
271L, 272L, 273L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L,
129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L,
140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L,
151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L,
162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L,
173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L,
184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L,
195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L,
206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L,
217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L,
228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L,
239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L,
250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L,
261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L,
272L, 273L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L,
130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L,
141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L,
152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L,
163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L,
174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L,
185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L,
196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L,
207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L,
218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L,
229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L,
240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L,
251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L,
262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L,
273L, 122L, 123L, 124L, 125L, 126L, 127L, 128L, 129L, 130L, 131L,
132L, 133L, 134L, 135L, 136L, 137L, 138L, 139L, 140L, 141L, 142L,
143L, 144L, 145L, 146L, 147L, 148L, 149L, 150L, 151L, 152L, 153L,
154L, 155L, 156L, 157L, 158L, 159L, 160L, 161L, 162L, 163L, 164L,
165L, 166L, 167L, 168L, 169L, 170L, 171L, 172L, 173L, 174L, 175L,
176L, 177L, 178L, 179L, 180L, 181L, 182L, 183L, 184L, 185L, 186L,
187L, 188L, 189L, 190L, 191L, 192L, 193L, 194L, 195L, 196L, 197L,
198L, 199L, 200L, 201L, 202L, 203L, 204L, 205L, 206L, 207L, 208L,
209L, 210L, 211L, 212L, 213L, 214L, 215L, 216L, 217L, 218L, 219L,
220L, 221L, 222L, 223L, 224L, 225L, 226L, 227L, 228L, 229L, 230L,
231L, 232L, 233L, 234L, 235L, 236L, 237L, 238L, 239L, 240L, 241L,
242L, 243L, 244L, 245L, 246L, 247L, 248L, 249L, 250L, 251L, 252L,
253L, 254L, 255L, 256L, 257L, 258L, 259L, 260L, 261L, 262L, 263L,
264L, 265L, 266L, 267L, 268L, 269L, 270L, 271L, 272L, 273L, 274L
), Trange = c(26, 22, 18, 28.9999999999, 25.9999999999, 16, 16.9999999999,
19.0000000001, 16, 23, 16.9999999999, 14.9999999999, 22, 23,
14, 25, 28.0000000001, 27.9999999999, 26.0000000001, 16, 22.0000000001,
25, 22, 18, 11.0000000002, 15.0000000001, 19.0000000001, 25,
12.0000000001, 19.0000000001, 11, 23.9999999999, 20.9999999999,
18, 14.9999999999, 16, 23, 17.0000000001, 20, 11, 14.9999999999,
11, 22, 22, 20, 11.9999999999, 25.9999999999, 13.9999999999,
19.0000000001, 20.9999999999, 18, 25.9999999999, 13.0000000001,
14, 18.9999999999, 13.9999999999, 20, 18.9999999999, 17.0000000001,
16.9999999999, 21.0000000001, 20.9999999999, 20, 20.9999999999,
22.9999999999, 20, 21.0000000001, 21.0000000001, 18, 11.9999999999,
16, 18, 17.0000000001, 21.0000000001, 29.9999999999, 21.0000000001,
19.0000000001, 18.9999999999, 16, 17.0000000001, 18, 18, 22.0000000001,
16, 14, 18.9999999999, 20, 23, 20, 23, 19.0000000001, 18, 9,
13, 14, 21.0000000001, 23.9999999999, 17.0000000001, 23, 27,
23.9999999999, 20, 25, 23.9999999999, 12.0000000001, 12.0000000001,
10.0000000001, 17.0000000001, 14.9999999999, 23, 20.9999999999,
22.9999999999, 20, 18.9999999999, 18, 20, 20, 16, 19.0000000001,
15.0000000001, 20, 13.0000000001, 16, 27, 20.9999999999, 20,
12.0000000001, 18, 22, 29.9999999999, 25.9999999999, 21.0000000001,
21.0000000001, 10.0000000001, 13, 26, 18.9999999999, 11.9999999999,
10.0000000001, 10.0000000001, 20, 18, 13.9999999999, 10.9999999999,
23, 21, 11.9999999999, 20, 25.9999999999, 28, 24.0000000001,
16, 22, 16.9999999999, 28.0000000001, 25, 36, 23.9999999999,
27, 27.0000000001, 27, 29.0000000001, 18, 21.0000000001, 28.9999999999,
10.0000000001, 7.99999999992, 17.0000000001, 25.9999999999, 12.0000000001,
22, 34, 24.0000000001, 23.9999999999, 25.9999999999, 23.9999999999,
21.0000000001, 27.0000000001, 29, 14.9999999999, 9, 23, 14.9999999999,
12.0000000001, 16, 18, 22.9999999999, 12.0000000001, 17.0000000001,
23.9999999999, 27, 24.9999999998, 21.0000000001, 18, 20.9999999999,
20, 16, 22, 16.9999999999, 22.0000000001, 29, 20, 18, 25, 20.9999999999,
21.0000000001, 14, 18.9999999999, 16, 25, 20.9999999999, 9.9999999999,
18, 18, 25, 11, 8.0000000001, 20, 27, 23.9999999999, 20.9999999999,
21.0000000001, 20, 23, 23, 18, 24.9999999998, 23, 17.0000000001,
13, 23, 30.0000000001, 23, 25, 23.9999999999, 24.0000000001,
23, 18.9999999999, 23.9999999999, 30.0000000001, 15.0000000001,
23, 16, 24.9999999998, 20, 19.0000000001, 16, 23, 23.9999999999,
21.0000000001, 13.0000000001, 18, 26.0000000001, 26.0000000001,
27, 18, 16, 23, 16.9999999999, 8.0000000001, 19.0000000001, 22,
23.9999999999, 25.9999999999, 25.9999999999, 25, 22.9999999999,
23.9999999999, 23.9999999999, 21.0000000001, 26.0000000001, 20.9999999999,
18, 24.0000000001, 25.9999999999, 27, 15.0000000001, 20.9999999999,
20, 22, 30.0000000001, 31, 33.0000000001, 23, 22, 29.9999999999,
32.9999999999, 25.9999999999, 35.0000000001, 24.0000000001, 19.0000000001,
23, 27, 32.9999999999, 25, 14, 28, 36.9999999999, 11.9999999999,
22.9999999999, 20.0000000001, 18, 11, 20, 20, 25, 13.0000000001,
14.9999999999, 12.0000000001, 25, 19.0000000001, 12.0000000001,
11, 22, 20, 28.9999999999, 29.0000000001, 32.0000000001, 32,
24.9999999998, 25, 20, 16.9999999999, 20, 21.0000000001, 18,
23.9999999999, 20, 18, 17.0000000001, 27, 23, 25, 17.0000000001,
18, 25, 20, 25, 24.0000000001, 29, 14.9999999999, 25, 18, 23,
16, 25, 28.0000000001, 30.0000000001, 16, 13, 19.0000000001,
25, 18.9999999999, 14.9999999999, 14, 13, 16.9999999999, 20.9999999999,
20, 11, 14.9999999999, 16, 13.9999999999, 16, 10.0000000001,
19.0000000001, 18.9999999999, 22.9999999999, 23, 20, 19.0000000001,
21.0000000001, 20, 18.9999999999, 16, 16, 18, 15.9999999998,
23, 18.9999999999, 18, 14, 19.0000000001, 16.9999999999, 18,
16.9999999999, 14.9999999999, 14, 13, 22.0000000001, 20, 16.9999999999,
20.0000000002, 22.0000000001, 23.9999999999, 18, 20, 18.9999999999,
18.9999999999, 15.0000000001, 20, 22, 16, 17.0000000001, 20,
14, 14.9999999999, 15.0000000001, 13.0000000001, 16, 13.9999999999,
18, 16.9999999999, 10.0000000001, 13.0000000001, 18, 21.0000000001,
20, 16.9999999999, 18.9999999999, 10.0000000001, 9.9999999999,
23, 24.0000000001, 13.0000000001, 16.9999999999, 11, 17.0000000001,
16.9999999999, 18, 20, 16.9999999999, 20, 18.9999999999, 16.9999999999,
20.0000000002, 18.9999999999, 18, 22.0000000001, 23.9999999999,
16.9999999999, 13, 9, 16.9999999999, 20, 21.0000000001, 14.9999999999,
9.9999999999, 22, 11, 27, 18, 14.9999999999, 23.9999999999, 23,
16.9999999999, 20, 14, 14, 18, 26, 17.9999999999, 4, 9, 21, 7.99999999992,
18, 13, 29, 9, 24.0000000001, 13, 19.0000000001, 11.0000000001,
9.9999999999, 20, 27, 25, 18, 15.0000000001, 9, 16, 22, 22.0000000001,
23.9999999999, 20, 15.9999999998, 21.0000000001, 16, 27.9999999999,
20, 13, 18, 25, 20, 14, 15.0000000001, 11.9999999999, 27.9999999999,
23.9999999999, 25.9999999999, 16.9999999999, 20.0000000002, 29,
27, 25.9999999999, 18, 18, 23.9999999999, 12.0000000001, 16,
25, 15.0000000001, 23.9999999999, 18.9999999999, 14.9999999999,
16.9999999999, 21.0000000001, 18.9999999999, 18.9999999999, 20.0000000002,
17.0000000001, 14, 16.9999999999, 22, 25, 19.0000000001, 16.9999999999,
15.0000000001, 10.0000000001, 16.9999999999, 15.0000000001, 16,
21.0000000001, 20, 20, 20, 20, 14.9999999999, 16.9999999999,
18, 16, 12.0000000001, 5.99999999994, 22, 25.9999999999, 18.9999999999,
22, 20.9999999999, 10.0000000001, 22, 14, 13, 14.9999999999,
14.9999999999, 13, 18, 27, 24.0000000001, 21.0000000001, 22.0000000001,
20, 19.0000000001, 24.0000000001, 20.9999999999, 18, 18.9999999999,
25, 18.9999999999, 19.0000000001, 27, 29, 31, 29.9999999999,
21.0000000001, 25, 23.9999999999, 16, 11.9999999999, 10.0000000001,
16, 23.9999999999, 28.0000000001, 29, 23.9999999999, 21.0000000001,
28.0000000001, 29, 27, 27.9999999999, 18.9999999999, 25, 20,
10.0000000001, 18, 22, 25.9999999999, 16.9999999999, 17.0000000001,
16.9999999999, 18, 20.9999999999, 12.0000000001, 22.0000000001,
25, 29, 20, 29, 27, 23.0000000001, 37.9999999999, 15.0000000001,
26.0000000001, 23.9999999999, 11.9999999999, 10, 30.0000000001,
23.9999999999, 29, 18, 16.9999999999, 18, 25, 16, 27, 29.9999999999,
23.9999999999, 22.9999999999, 24.0000000001, 23, 9, 14.9999999999,
14.9999999999, 26.9999999999, 20, 20, 28.0000000001, 30.0000000001,
19.0000000001, 29, 25, 18.9999999999, 16, 17.0000000001, 13.9999999999,
29, 22.9999999999, 25, 14.9999999999, 13, 12.0000000001, 18,
22, 27, 25, 14.9999999999, 21.0000000001, 17.0000000001, 19.0000000001,
20, 16.9999999999, 20, 22.0000000001, 22.0000000001, 16, 20,
23, 21.0000000001, 25.9999999999, 19.0000000001, 9, 21.0000000001,
23, 21.0000000001, 14.9999999999, 17.0000000001, 18.9999999999,
11.9999999999, 18, 13, 14.9999999999, 19.0000000001, 20, 18,
18, 22.0000000001, 22, 16, 16, 16.9999999999, 23, 19.0000000001,
20, 24.0000000001, 23, 18, 16, 13, 9, 5.00000000004, 18.9999999999,
16, 20, 7.00000000002, 9, 19.0000000001, 18, 20, 19.0000000001,
18.9999999999, 19.0000000001, 18, 15.9999999998, 16, 13, 19.0000000001,
22.9999999999, 15.9999999998, 11, 20.0000000002, 26.0000000001,
20, 16.9999999999, 20, 20, 19.0000000001, 16, 14, 16.9999999999,
13, 18, 17.0000000001, 16, 8.0000000001, 16, 13.9999999999, 18,
19.0000000001, 20.0000000002, 18.9999999999, 20, 16, 20, 24.0000000001,
27.9999999999, 25, 19.0000000001, 11, 18, 26, 25.9999999999,
28.0000000001, 23.9999999999, 23, 16.9999999999, 8.0000000001,
23.9999999999, 26, 30.0000000001, 27, 22.9999999999, 25, 29.0000000002,
22.0000000001, 27, 25, 25.9999999999, 29, 30.0000000001, 29.0000000002,
22, 13, 19, 30.0000000001, 27.9999999999, 11.9999999999, 16,
22, 22.0000000001, 25.9999999999, 11, 19.0000000001, 11, 14.9999999999,
21.0000000001, 20, 22.0000000001, 11, 16, 22, 19.0000000001,
25.9999999999, 23, 17.0000000001, 17.0000000001, 20.0000000002,
18, 14, 12.0000000001, 16.9999999999, 25, 18, 17.0000000001,
13.9999999999, 27, 23.9999999999, 25, 28.0000000001, 28.0000000001,
23, 24.0000000001, 17.0000000001, 24.0000000001, 28.0000000001,
23, 25, 20, 28.0000000001, 9.9999999999, 14, 18.9999999999, 25,
18, 23, 18.9999999999, 23, 30.0000000001, 14.9999999999, 14.9999999999,
20.0000000002, 23, 14, 20.9999999999, 20, 21.0000000001, 21.0000000001,
22.9999999999, 17.0000000001, 18, 22, 23.9999999999, 15.0000000001,
25, 22.0000000001, 24.0000000001, 22, 20, 16, 18, 22.0000000001,
27, 9.9999999999, 10.0000000001, 23.9999999999, 22.9999999999,
31, 17.0000000001, 25.9999999999, 13.9999999999, 20, 28.0000000001,
16, 17.0000000001, 18, 20, 23.9999999999, 25, 20, 21.0000000001,
20.9999999999, 10.0000000001, 10.0000000001, 20.9999999999, 19.0000000001,
13, 20.0000000002, 16, 16, 23, 21.0000000001, 24.0000000001,
25.9999999999, 28.0000000001, 24.0000000001, 18, 11, 21.0000000001,
16, 12.0000000001, 14.9999999999, 23, 17.0000000001, 17.0000000001,
24.0000000001, 25, 22, 21.0000000001, 17.0000000001, 25.9999999999,
26.0000000001, 14, 16, 18.9999999999, 27, 13, 25.9999999999,
27, 22, 16.9999999999, 23.9999999999, 14, 24.9999999999, 21.0000000001,
20.9999999999, 22.0000000001, 12.0000000001, 27, 27, 13, 24.0000000001,
25.9999999999, 13, 9, 18, 23, 22.9999999999, 14, 21, 18, 19,
5.00000000004, 13.0000000001, 33, 19.0000000001, 6.00000000012,
6.99999999995, 19, 28.0000000001, 31.9999999999, 28.0000000001,
16, 13, 25, 7.00000000002, 22, 22, 11, 10.0000000001, 22.9999999999,
13, 18, 14, 16.9999999999, 9, 9, 25.9999999999, 19.0000000001,
17.0000000001, 19.0000000001, 9.9999999999, 20, 7.99999999992,
9.9999999999, 16.9999999999, 12.0000000001, 25, 22.9999999999,
9, 16, 9.9999999999, 7.99999999992, 18.9999999999, 18, 15.9999999998,
22, 22, 16.9999999999, 16.9999999999, 11, 13, 13, 13.9999999999,
16, 5.99999999994, 14.9999999999, 16, 16, 15.0000000001, 12.0000000001,
24.9999999998, 16, 14.9999999999, 16, 16, 11, 19.0000000001,
11, 14.9999999999, 22.9999999999, 14, 13.9999999999, 10.0000000001,
11.0000000002, 17.0000000001, 16, 15.0000000001, 9.9999999999,
16, 19.0000000001, 20, 23, 14, 14.9999999999, 16.9999999999,
11, 12.0000000001, 11.9999999999, 14, 20, 11, 16, 23, 16, 20,
16, 12.0000000001, 14.9999999999, 19.0000000001, 20, 22, 25,
22, 24.0000000001, 15.0000000001, 20, 11.0000000002, 9, 14.9999999999,
20, 16.9999999999, 16.9999999999, 5.99999999994, 18, 15.0000000001,
16, 18.9999999999, 20, 14.9999999999, 20, 23, 25, 19.0000000001,
20, 10.0000000001, 9, 14.9999999999, 14, 17, 23, 25.9999999999,
25, 23.9999999999, 13.0000000001, 21.0000000001, 11, 23.9999999999,
18, 25, 24, 28.0000000001, 20, 25, 25, 23, 20, 20, 23, 22, 27,
29.9999999999, 26.0000000001, 25.9999999999, 20, 18, 9.9999999999,
23.9999999999, 23, 25, 14, 23.9999999999, 18, 20, 10.0000000001,
10.0000000001, 18, 13, 16, 17.0000000001, 22, 16.9999999999,
20, 13, 19.0000000001, 18, 15.0000000001, 9, 17.0000000001, 25.9999999999,
18, 16.9999999999, 11, 16.9999999999, 16, 9, 22, 24.0000000001,
18, 9.9999999999, 20, 16.9999999999, 14, 23, 18.9999999999, 16,
21.0000000001, 23, 17.0000000001, 16, 14.9999999999, 16.9999999999,
24.0000000001, 13, 18, 25, 21.0000000001, 21.0000000001, 19.0000000001,
16, 11, 14, 16, 14.9999999999, 12.0000000001, 23, 13, 13.9999999999,
18, 14, 17.0000000001, 11, 11, 16, 14.9999999999, 11, 18, 20.0000000002,
20, 14.9999999999, 15.0000000001, 11.9999999999, 20, 16, 20,
25.9999999999, 23.9999999999, 22.0000000001, 16, 22.0000000001,
12.0000000001, 11, 14.9999999999, 18, 9, 14.9999999999, 21.0000000001,
9.9999999999, 22, 7.99999999992, 13, 18.9999999999, 20, 23, 17.0000000001,
18.9999999999, 25, 10.0000000001, 12.0000000001, 21.0000000001,
14, 5.00000000004, 13.9999999999, 25.9999999999, 13, 16.9999999999,
11, 21.0000000001, 21.0000000001, 23, 20, 22, 8.0000000001, 15.0000000001,
21.0000000001, 16.9999999999, 14, 11.9999999999, 7.00000000002,
14.9999999999, 13, 18, 23, 13, 18, 9, 19, 24, 23, 16.9999999999,
5.00000000004, 5.00000000004, 18.9999999999, 23, 25, 23.9999999999,
13.9999999999, 12.0000000001, 6.99999999995, 29, 30, 22, 20.9999999999,
15.0000000001, 28, 32, 27, 13.0000000001, 15.0000000001, 16,
9.00000000002, 25, 26.0000000001, 9.9999999999, 14, 22, 13.9999999999,
14.9999999999, 17.0000000001, 25.9999999999, 14.9999999999, 20.9999999999,
11, 22.9999999999, 22, 17, 17.9999999999, 22, 4.99999999986,
16, 18, 22, 16, 25, 22.0000000001, 18, 20.9999999999, 14.9999999999,
14.9999999999, 17.0000000001, 13, 24.9999999998, 23, 19.0000000001,
21.0000000001, 9.9999999999, 15.0000000001, 20.9999999999, 20,
20, 18.9999999999, 19.0000000001, 14.9999999999, 16.9999999999,
16, 16.9999999999, 21.0000000001, 20, 15.9999999998, 20, 13,
18, 18, 28.0000000001, 30.0000000001, 25, 25, 21.0000000001,
20, 18, 14, 14, 16.9999999999, 21.0000000001, 20, 20, 20, 21.0000000001,
21.0000000001, 22.9999999999, 20.0000000002, 13.9999999999, 20,
20, 20, 19.0000000001, 11, 21.0000000001, 18, 23.9999999999,
27, 16, 20, 16, 20.9999999999, 21.0000000001, 23, 27.9999999999,
16, 16, 23, 25, 24.9999999998, 23, 18.9999999999, 13, 20, 20.9999999999,
20, 24.0000000001, 18.9999999999, 18, 20, 19.0000000001, 20,
15.0000000001, 22.9999999999, 9, 16.9999999999, 18, 22, 23.9999999999,
23, 21.0000000001, 17.0000000001, 14.9999999999, 24.9999999998,
20, 20, 12.0000000001, 10.0000000001, 20.0000000002, 20.9999999999,
15.0000000001, 22.9999999999, 36, 16.9999999999, 16.9999999999,
32.0000000001, 22.0000000001, 16.9999999999, 26, 28, 3.00000000006,
18.9999999999, 27, 22, 26.0000000001, 27, 27, 15.0000000001,
20.9999999999, 15, 16.9999999999, 11, 11, 20.0000000001, 21.9999999999,
41, 20, 23, 20, 16, 24, 24, 31.0000000001, 27, 21.0000000001,
9, 12.0000000001, 20, 19.0000000001, 27.9999999999, 22, 16.9999999999,
5.99999999994, 17, 23, 26.0000000001, 28.0000000001, 27, 18,
22, 20, 20, 15.0000000001, 14, 8.0000000001, 14, 10.0000000001,
19.0000000001, 16, 14.9999999999, 18, 18, 19.0000000001, 16,
14.9999999999, 18.9999999999, 13, 10.0000000001, 21.0000000001,
18, 16.9999999999, 23.9999999999, 11.9999999999, 13.9999999999,
21.0000000001, 18.9999999999, 16, 22, 22.0000000001, 18.9999999999,
10.0000000001, 19.0000000001, 12.0000000001, 22, 21.0000000001,
25, 14, 13, 22.0000000001, 22, 18.9999999999, 18.9999999999,
16, 22, 22, 31, 14, 21.0000000001, 20.9999999999, 18.9999999999,
18.9999999999, 18, 18.9999999999, 20.9999999999, 16, 23.9999999999,
25, 19.0000000001, 23, 20, 16, 16, 20, 22, 24.0000000001, 16.9999999999,
19.0000000001, 18, 20, 19.0000000001, 18, 23, 21.0000000001,
21.0000000001, 11, 12.0000000001, 16.9999999999, 20.9999999999,
25.9999999999, 14.9999999999, 16.9999999999, 18, 25.9999999999,
23.9999999999, 23.9999999999, 23, 23.9999999999, 18.9999999999,
29, 27.9999999999, 26.0000000001, 11, 23, 20, 19.0000000001,
18, 16, 10.0000000001, 16.9999999999, 18, 20, 18, 15.0000000001,
16, 19.9999999999, 16.9999999999, 26.9999999999, 27, 23.9999999999,
25, 7.99999999992, 16, 12.0000000001, 29.9999999999, 16, 8.99999999995,
18.0000000001, 12.0000000001, 23, 13.0000000001, 18, 25.0000000001,
20.9999999999, 27, 13, 16.9999999999, 27, 25, 14.9999999999,
13.9999999999, 16.9999999999, 15.9999999998, 20.9999999999, 14.9999999999,
11, 9, 27, 25, 16.9999999999, 16, 12.0000000001, 26.9999999999,
27.9999999999, 19.0000000001, 24.0000000001, 12.0000000001, 13.9999999999,
21.0000000001, 16.9999999999, 18, 16.9999999999, 20.9999999999,
7.00000000002, 11.9999999999, 2.99999999988, 5.99999999994, 9.9999999999,
27, 18, 18.9999999999, 16.9999999999, 9, 23, 17.0000000001, 18.9999999999,
23, 11, 16.9999999999, 19.0000000001, 22, 16, 16.9999999999,
20, 16, 20, 18, 14, 16, 18.9999999999, 18.9999999999, 20, 23,
16.9999999999, 22, 17.0000000001, 19.0000000001, 22, 19.0000000001,
14.9999999999, 12.0000000001, 20, 14, 16, 16, 14.9999999999,
22.0000000001, 21.0000000001, 21.0000000001, 21.0000000001, 14.9999999999,
11, 14.9999999999, 21.0000000001, 22, 14.9999999999, 13, 19.0000000001,
18.9999999999, 20, 15.0000000001, 20, 10.0000000001, 19.0000000001,
17.0000000001, 6.00000000012, 19.0000000001, 17.0000000001, 18.9999999999,
20, 20, 12.0000000001, 20.9999999999, 22, 16, 16, 7.99999999992,
17.0000000001, 20, 11.9999999999, 11.9999999999, 17.0000000001,
18, 7.00000000002, 5.00000000004, 16, 10.0000000001, 11, 23.9999999999,
21.0000000001, 20, 18, 18, 22, 14.9999999999, 13, 7.00000000002,
18, 18, 14.9999999999, 18.9999999999, 18, 12.0000000001, 15.0000000001,
18.9999999999, 13.0000000001, 16, 21.0000000001, 25, 22, 20,
19.0000000001, 21.0000000001, 23.9999999999, 17.0000000001, 25,
28.0000000001, 8.0000000001, 10, 28.0000000001, 25, 22.9999999999,
18, 14, 17, 20.0000000001, 22, 28.0000000001, 13, 20, 21.0000000001,
18, 9.9999999999, 12.0000000001, 25.9999999999, 14, 27.9999999999,
14, 18, 21.0000000001, 7.00000000002, 16.9999999999, 19.9999999999,
24.0000000001, 25, 12.0000000001, 11.9999999999, 13.9999999999,
9, 20, 11, 16, 18, 21.0000000001, 16.9999999999, 19.0000000001,
23, 13, 14, 13, 11, 18.9999999999, 9, 16, 14.9999999999, 9, 23,
20, 21.0000000001, 16, 16, 21.0000000001, 22, 22, 20, 13, 11.9999999999,
13, 23.9999999999, 22, 20.9999999999, 18, 22.0000000001, 20,
13, 16, 18.9999999999, 20, 17.0000000001, 19.0000000001, 16.9999999999,
17.0000000001, 13.9999999999, 20, 18, 11, 16.9999999999, 10.0000000001,
20, 16.9999999999, 18.9999999999, 20, 20.9999999999, 14, 19.0000000001,
18, 20.9999999999, 20, 18, 17.0000000001, 21.0000000001, 23,
22, 18, 16, 15.9999999998, 19.0000000001, 22, 22, 18, 21.0000000001,
13, 16.9999999999, 20.0000000002, 14, 18, 22, 20, 22, 10.0000000001,
11.9999999999, 21.0000000001, 19.0000000001, 21.0000000001, 22,
21.0000000001, 25, 18, 22, 23, 23.9999999999, 16, 20.0000000001,
24, 27, 24.0000000001, 16, 16, 14.9999999999, 19.0000000001,
25, 21.0000000001, 16, 30.9999999999, 27, 19.0000000001, 16.9999999999,
17.0000000001, 27, 23.9999999999, 14, 19.0000000001, 12.0000000001,
7.00000000002, 23, 16, 16, 14, 10.0000000001, 17.0000000001,
14, 13.9999999999, 21.0000000001, 16.9999999999, 13.9999999999,
9, 34, 21.0000000001, 10.0000000001, 13.0000000001, 21.0000000001,
22, 25.9999999999, 25, 22, 19.0000000001, 20, 16, 25, 18, 16,
29.9999999999, 21.0000000001, 14.9999999999, 22, 19.0000000001,
19.0000000001, 11.9999999999, 20, 19.0000000001, 13.9999999999,
19.0000000001, 16.9999999999, 15.0000000001, 16, 9.9999999999,
21.0000000001, 20, 23.9999999999, 13.0000000001, 20, 25.9999999999,
25, 23, 21.0000000001, 16, 16, 14, 18, 23, 25, 14.9999999999,
21.0000000001, 20, 19.0000000001, 13, 16, 18, 18.9999999999,
16.9999999999, 18, 13, 18, 23, 21.0000000001, 18.9999999999,
14.9999999999, 11, 18, 16, 18, 11.9999999999, 21.0000000001,
21.0000000001, 20, 14.9999999999, 14.9999999999, 16, 10.0000000001,
13.0000000001, 16.9999999999, 19.0000000001, 20, 18, 16, 13.9999999999,
16, 20, 18, 15.0000000001, 15.0000000001, 18, 18, 20, 18.9999999999,
18, 18, 20, 20, 21.0000000001, 25, 9, 19.0000000001, 18, 22,
22, 10.0000000001, 18, 13.9999999999, 18, 17.0000000001, 11,
13, 18, 16, 13.9999999999, 19.0000000001, 23, 29, 22.0000000001,
10.0000000001, 12.0000000001, 17.0000000001, 22, 9, 13.9999999999,
23, 19.0000000001, 14, 23, 19.0000000001, 23.9999999999, 28.0000000001,
27.9999999999, 24.0000000001, 11, 17.0000000001, 18, 20.9999999999,
20, 18, 25, 21.0000000001, 9, 16.9999999999, 11.0000000002, 13,
22, 20.9999999999, 22, 20, 23, 29.9999999999, 23, 23, 22, 16,
12.0000000001, 23.9999999999, 31.0000000001, 23.9999999999, 12.0000000001,
16.9999999999, 28.9999999999, 29, 20.9999999999, 20, 14, 23,
28.0000000001, 20.9999999999, 17.0000000001, 31.0000000001, 16,
20, 23.9999999999, 25.9999999999, 27, 27, 20, 7.00000000002,
9, 16.9999999999, 24.0000000001, 30.0000000001, 23.9999999999,
25, 20, 23.9999999999, 22, 23, 20, 16, 22, 29, 27, 27.9999999999,
24.0000000001, 16.9999999999, 25, 29.9999999999, 23, 23, 18.9999999999,
17.0000000001, 14.9999999999, 28.0000000001, 20.9999999999, 23.9999999999,
18, 22, 22.9999999999, 18, 23.9999999999, 18, 14.9999999999,
23.9999999999, 29, 13, 13, 15.9999999998, 18, 25, 28.0000000001,
21.0000000001, 17.0000000001, 19.0000000001, 23.9999999999, 22,
9.9999999999, 10.0000000001, 14, 19.0000000001, 22, 20, 17.0000000001,
14.9999999999, 12.0000000001, 16, 9.9999999999, 16, 18.9999999999,
24.0000000001, 16, 20, 18, 23, 20.9999999999, 12.0000000001,
14, 21.0000000001, 20, 18.9999999999, 21.0000000001, 17.0000000001,
9, 18.9999999999, 22.0000000001, 20, 16.9999999999, 21.0000000001,
18.9999999999, 14.9999999999, 14.9999999999, 16.9999999999, 21.0000000001,
18.9999999999, 22, 20.9999999999, 14, 13.0000000001, 18, 16,
14, 23, 27, 16, 7.00000000002, 11, 24.0000000001, 25, 22, 25,
9, 16.9999999999, 16, 12.0000000001, 24.9999999999, 22, 27.9999999999,
22, 29, 18, 18.9999999999, 16.9999999999, 22, 22, 22, 26.0000000001,
7.00000000002, 18.9999999999, 26.0000000001, 29.9999999999, 20.9999999999,
18, 14.9999999999, 28.0000000001, 27, 23, 18.9999999999, 12.0000000001,
12.0000000001, 20, 22, 24, 18.9999999999, 14.9999999999, 13,
26, 16, 9, 27, 23, 31, 25.9999999999, 15.0000000001, 28, 23.9999999999,
18, 12.0000000001, 16.9999999999, 19.0000000001, 26.0000000001,
7.00000000002, 28.0000000001, 17.0000000001, 15.0000000001, 16.9999999999,
18, 8.0000000001, 31.9999999999, 19.0000000001, 16, 14, 19.0000000001,
10.0000000001, 12.0000000001, 18, 23, 18, 14.9999999999, 12.0000000001,
22.0000000001, 12.0000000001, 12.0000000001, 12.0000000001, 17.0000000001,
12.0000000001, 22.9999999999, 20.0000000002, 13.0000000001, 21.0000000001,
21.0000000001, 18.9999999999, 18, 15.0000000001, 19.0000000001,
20, 15.9999999998, 16.9999999999, 18, 20, 21.0000000001, 13,
9.9999999999, 14, 23, 22, 19.0000000001, 23, 18.9999999999, 19.0000000001,
18, 16, 14, 14, 17.0000000001, 11.0000000002, 14.9999999999,
16.9999999999, 9.9999999999, 9, 5.00000000004, 17.0000000001,
16.9999999999, 14.9999999999, 15.0000000001, 14, 16.9999999999,
17.0000000001, 20.0000000002, 18.9999999999, 9.9999999999, 14,
14.9999999999, 22, 17.0000000001, 16, 14.9999999999, 13.9999999999,
14.9999999999, 21.0000000001, 20.9999999999, 19.0000000001, 19.0000000001,
20, 18, 18, 14.9999999999, 10.0000000001, 20, 18.9999999999,
13.0000000001, 13, 7.00000000002, 20, 13, 8.0000000001, 11, 16,
19.0000000001, 21.0000000001, 9.9999999999, 13.9999999999, 18,
18, 14, 20, 16.9999999999, 18, 13.0000000001, 18, 18.9999999999,
12.0000000001, 11, 11.9999999999, 13, 4.99999999986, 7.00000000002,
3.00000000006, 18.9999999999, 21.0000000001, 18.9999999999, 21.0000000001,
20, 18, 20, 19, 24.9999999999, 28, 16.9999999999, 24.0000000001,
25.9999999999, 20, 22, 13, 11.9999999999, 7.99999999992, 20.9999999999,
21, 20, 27, 16, 22.9999999999, 15, 22, 15.9999999998, 10.0000000001,
23.9999999999, 7.00000000002, 11, 22.9999999999, 24.0000000001,
9, 13, 3.00000000006, 13, 20, 16, 20.9999999999, 11, 10.0000000001,
14.9999999999, 24.0000000001, 12.0000000001, 20, 19.0000000001,
15.9999999998, 27, 11, 22, 9.9999999999, 12.0000000001, 14.9999999999,
19.0000000001, 20, 19.0000000001, 21.0000000001, 21.0000000001,
9, 16, 18.9999999999, 19.0000000001, 18.9999999999, 18, 14.9999999999,
22, 14, 16.9999999999, 22, 16.9999999999, 17.0000000001, 20,
18, 11.9999999999, 13, 18, 22, 23, 20, 16, 18, 23.9999999999,
16, 18, 17.0000000001, 16.9999999999, 11, 14, 9, 20.9999999999,
21.0000000001, 21.0000000001, 18, 20, 24.9999999998, 16, 17.0000000001,
18, 18.9999999999, 16.9999999999, 17.0000000001, 18, 17.0000000001,
11.9999999999, 14, 16, 19.0000000001, 20.0000000002, 14.9999999999,
10.0000000001, 13, 14.9999999999, 18.9999999999, 16.9999999999,
22, 18, 19.0000000001, 11.9999999999, 13, 18.9999999999, 6.99999999984,
3.99999999996, 22, 15.9999999998, 16.9999999999, 19.0000000001,
5.00000000004, 6.99999999984, 13, 18, 17.0000000001, 20, 20,
20.9999999999, 13, 18, 22.0000000001, 20.9999999999, 21.0000000001,
20.9999999999, 23, 21.0000000001, 20, 14.9999999999, 10.0000000001,
15.0000000001, 16, 14.9999999999, 9.9999999999, 16.9999999999,
16.9999999999, 25, 14.9999999999, 18.9999999999, 9, 7.99999999992,
16, 10.0000000001, 20, 15.0000000001, 25, 27, 28.0000000001,
27, 14, 10.0000000001, 14.9999999999, 16, 25.9999999999, 7.00000000002,
18, 23, 16, 30.0000000001, 16, 9.9999999999, 16, 13.0000000001,
20, 14.9999999999, 20, 15.0000000001, 14, 22, 22, 5.99999999994,
20, 13, 11, 17.0000000001, 11, 12.0000000001, 14.9999999999,
25, 20, 14.9999999999, 13.0000000001, 15.0000000001, 16, 18.9999999999,
20, 16, 16, 20, 23, 23, 20, 13, 13, 22, 15.0000000001, 21.0000000001,
23.9999999999, 12.0000000001, 24.0000000001, 21.0000000001, 21.0000000001,
18.9999999999, 16, 12.0000000001, 12.0000000001, 13, 16.9999999999,
16, 16.9999999999, 16, 20, 20, 18.9999999999, 17.0000000001,
21.0000000001, 23, 20, 21.0000000001, 20, 13, 16, 15.9999999998,
22, 20.9999999999, 12.0000000001, 14.9999999999, 19.0000000001,
18, 20.9999999999, 20, 16, 17.0000000001, 18, 17.0000000001,
14, 7.99999999992, 25, 18, 20.9999999999, 12.0000000001, 7.99999999992,
23, 24.0000000001, 22, 26.0000000001, 22, 9, 14, 14, 23, 23,
5.99999999994, 20.9999999999, 22, 21.0000000001, 20.9999999999,
23.9999999999, 21.0000000001, 19.0000000001, 20, 21.0000000001,
15.0000000001, 22, 23, 22.9999999999, 23, 20, 20, 16, 16.9999999999,
12.0000000001, 12.0000000001, 18.9999999999, 18, 18.9999999999,
16, 7.99999999992, 13, 20, 21.0000000001, 22, 14, 20, 13, 18,
18, 15.9999999998, 13.9999999999, 18, 14.9999999999, 14, 17.0000000001,
16, 20.0000000001, 26.0000000001, 5.00000000004, 22.0000000001,
14.9999999999, 20.9999999999, 16, 24.9999999999, 23.9999999999,
20, 18.9999999999, 22.0000000001, 18, 22, 13, 13, 24, 29.0000000001,
28.9999999999, 30.0000000001, 20.9999999999, 18, 12.0000000001,
13, 7.00000000002, 25.9999999999, 21.0000000001, 7.99999999992,
13, 17.0000000001, 16, 11.9999999999, 13.0000000001, 21.0000000001,
16, 17, 11, 14, 20.0000000001, 11, 27, 25, 13, 16, 5.99999999994,
18, 20, 13.0000000001, 18, 20, 20, 9, 17.0000000001, 25.9999999999,
23.9999999999, 21.0000000001, 16, 18, 14, 19.0000000001, 20.0000000002,
17.0000000001, 20, 21.0000000001, 22.0000000001, 18.9999999999,
22.0000000001, 14, 21.0000000001, 20.9999999999, 15.9999999998,
14.9999999999, 15.0000000001, 31, 13.0000000001, 22, 15.0000000001,
18, 18, 14.9999999999, 14.9999999999, 14.9999999999, 14, 20.9999999999,
17.0000000001, 14, 20, 13.9999999999, 20.0000000002, 19.0000000001,
18.9999999999, 16, 7.99999999992, 16.9999999999, 14, 15.0000000001,
14, 10.0000000001, 13, 21.0000000001, 16.9999999999, 11, 6.00000000012,
21.0000000001, 20, 12.0000000001, 16, 18, 18, 10.0000000001,
16.9999999999, 13.9999999999, 14, 16, 18.9999999999, 9, 16, 18.9999999999,
20, 20, 22.9999999999, 14, 18, 22.9999999999, 20, 18, 17.0000000001,
20, 20, 11, 18, 27, 7.00000000002, 9, 16.9999999999, 14.9999999999,
19.0000000001, 22, 16.9999999999, 16, 23, 9, 26.0000000001, 16.9999999999,
19.0000000001, 21.0000000001, 23, 23, 17.0000000001, 16, 18,
16, 18, 12.0000000001, 9, 14, 18.9999999999, 24.0000000001, 7.00000000002,
18, 20, 14, 23.9999999999, 27.0000000001, 5.99999999994, 3.99999999996,
11.9999999999, 7.99999999992, 28.0000000001, 18, 14, 18, 16,
18, 7.00000000002, 3.99999999996, 20, 29.0000000001, 20, 14,
24, 15.0000000001, 14, 11, 21.0000000001, 18, 13.9999999999,
13.0000000001, 14, 23.9999999999, 22, 22, 20.0000000002, 17.0000000001,
12.0000000001, 21.0000000001, 16.9999999999, 13, 17.0000000001,
22, 7.99999999992, 28.0000000001, 19.0000000001, 14, 14.9999999999,
6.99999999984, 14, 18, 14, 17.0000000001, 23, 20, 19.0000000001,
14.9999999999, 13, 16, 16.9999999999, 18, 14.9999999999, 12.0000000001,
22.0000000001, 18.9999999999, 16, 23, 23.9999999999, 12.0000000001,
11, 17.0000000001, 16, 10.0000000001, 14, 19.0000000001, 20,
14, 16, 16, 11, 18, 10.0000000001, 9, 18, 16.9999999999, 14.9999999999,
14, 17.0000000001, 13, 12.0000000001, 12.0000000001, 20, 14.9999999999,
12.0000000001, 16.9999999999, 14.9999999999, 16, 13, 18, 20,
16.9999999999, 14.9999999999, 14, 11, 14.9999999999, 16.9999999999,
20, 18, 14, 14.9999999999, 16.9999999999, 16, 15.0000000001,
16.9999999999, 14.9999999999, 14.9999999999, 14.9999999999, 21.0000000001,
12.0000000001, 13.9999999999, 16, 22, 16, 16.9999999999, 18,
16, 15.0000000001, 14.9999999999, 14.9999999999, 6.99999999984,
13.0000000001, 16, 10.0000000001, 16.9999999999, 20.9999999999,
11, 14, 10.0000000001, 14.9999999999, 16, 20, 21.0000000001,
7.00000000002, 25, 5.99999999994, 9.9999999999, 18, 18, 16.9999999999,
16, 20.0000000002, 11, 9.9999999999, 7.99999999992, 14, 13.0000000001,
15, 22, 13, 17.9999999999, 12.0000000001, 15, 15.9999999998,
18, 23, 11, 9.9999999999, 22.9999999999, 23, 25, 25, 18, 10.0000000001,
11.9999999999, 16, 22.0000000001, 20.9999999999, 25.9999999999,
25, 23.9999999999, 23, 24.9999999998, 20, 16, 20, 9, 21, 23,
21.0000000001, 16, 16.9999999999, 13.9999999999, 13.0000000001,
18, 21.0000000001, 20, 17.0000000001, 18.9999999999, 14, 12.0000000001,
14, 22, 16.9999999999, 14, 19.0000000001, 16.9999999999, 13.9999999999,
20, 19.0000000001, 18, 16, 20, 14.9999999999, 20, 16.9999999999,
22.0000000001, 14, 22, 20, 16.9999999999, 21.0000000001, 18,
14, 18, 18, 16, 13.9999999999, 15.0000000001, 15.9999999998,
18.9999999999, 18, 19.0000000001, 18, 15.0000000001, 16, 14.9999999999,
14, 16, 18, 19.0000000001, 22.9999999999, 14, 16.9999999999,
18.9999999999, 15.0000000001, 16, 14, 18, 20.9999999999, 19.0000000001,
18.9999999999, 16, 20, 16.9999999999, 18, 13.9999999999, 11,
21.0000000001, 22, 22, 11, 23.9999999999, 21.0000000001, 28.0000000001,
18, 13.0000000001, 21.0000000001, 21.0000000001, 22.0000000001,
20, 17.0000000001, 17.0000000001, 16.9999999999, 18, 16.9999999999,
20.9999999999, 17.0000000001, 16, 21.0000000001, 26.0000000001,
14, 13.9999999999, 14, 7.99999999992, 18.9999999999, 16, 13,
16.9999999999, 13, 20, 22.9999999999, 22.0000000001, 20.0000000002,
21.0000000001, 20.0000000002, 20, 19.0000000001, 15.0000000001,
17.0000000001, 17.0000000001, 22, 25, 22, 18.9999999999, 19.0000000001,
15.9999999998, 6.00000000012, 9, 9.9999999999, 13.9999999999,
18.9999999999, 29, 31.9999999999, 5.00000000008, 21.0000000001,
11.0000000001, 6.00000000005, 21.0000000001, 17.0000000001, 20,
11, 22, 10.0000000001, 9, 16, 27, 13.0000000001, 21.0000000001,
18, 3.99999999996, 11.9999999999, 22, 21.0000000001, 13, 18,
18, 12.0000000001, 5.00000000004, 5.99999999994, 11.9999999999,
14.9999999999, 12.0000000001, 23, 9, 14, 18, 16.9999999999, 16,
20, 13.9999999999, 16, 12.0000000001, 20, 12.0000000001, 10.0000000001,
21.0000000001, 23, 17.0000000001, 20, 18, 18, 16.9999999999,
18.9999999999, 20.9999999999, 18, 20, 16, 21.0000000001, 14.9999999999,
12.0000000001, 14, 15.9999999998, 18, 11, 13.9999999999, 18,
7.99999999992, 16, 13.9999999999, 20.0000000002, 21.0000000001,
13, 21.0000000001, 18, 20, 22, 18, 17.0000000001, 14, 22.0000000001,
18, 22, 20.9999999999, 11, 19.0000000001, 13, 10.0000000001,
18.9999999999, 20, 21.0000000001, 18, 12.0000000001, 18.9999999999,
16.9999999999, 14.9999999999, 14.9999999999, 13, 12.0000000001,
12.0000000001, 16, 18, 18, 16, 18, 18, 20, 17.0000000001, 16,
16, 14.9999999999, 16, 19.0000000001, 18, 21.0000000001, 11,
15.0000000001, 20, 18.9999999999, 21.0000000001, 19.0000000001,
17.0000000001, 16.9999999999, 18, 17.0000000001, 18, 15.0000000001,
13.9999999999, 19.0000000001, 20, 21.0000000001, 18, 19.0000000001,
14, 7.00000000002, 13, 21.0000000001, 21.0000000001, 10.0000000001,
19.0000000001, 17.0000000001, 18.9999999999, 10.0000000001, 13,
22.9999999999, 11, 5.99999999994, 12, 17, 22.0000000001, 22.9999999999,
21.0000000001, 23.9999999999, 23, 22.0000000001, 21.0000000001,
16.9999999999, 21, 22, 13, 14.9999999999, 7.00000000002, 5.00000000004,
15.0000000001, 18, 21.0000000001, 20, 10.0000000001, 11.0000000001,
11.0000000001, 14, 25.9999999999, 17.0000000001, 22, 18.9999999999,
20, 26.0000000001, 20, 22.9999999999, 20.9999999999, 23.9999999999,
26.0000000001, 16, 14, 11, 9, 18.9999999999, 22, 9.9999999999,
23.9999999999, 13, 15.0000000001, 23.9999999999, 14, 17.0000000001,
7.99999999992, 5.99999999994, 11, 16, 21.0000000001, 20, 20,
16, 18.9999999999, 13, 16.9999999999, 19.0000000001, 18, 14,
13.9999999999, 16.9999999999, 13.9999999999, 16, 22, 23, 23,
19.0000000001, 18.9999999999, 14, 20, 18, 16.9999999999, 22.9999999999,
22.0000000001, 16, 14.9999999999, 16.9999999999, 18, 19.0000000001,
16.9999999999, 18, 17.0000000001, 22.9999999999, 20, 18, 16.9999999999,
20.9999999999, 11, 7.99999999992, 9, 19.0000000001, 20, 16, 19.0000000001,
7.00000000002, 20.9999999999, 11, 14, 18, 16, 13.0000000001,
19.0000000001, 16.9999999999, 21.0000000001, 16.9999999999, 18.9999999999,
13, 22, 16.9999999999, 11.9999999999, 18.9999999999, 20, 22.0000000001,
14.9999999999, 19.0000000001, 13, 12.0000000001, 20, 18, 16.9999999999,
18, 20, 16, 16, 16, 14.9999999999, 17.0000000001, 16, 16, 21.0000000001,
18, 19.0000000001, 16.9999999999, 19.0000000001, 20, 21.0000000001,
20.0000000002, 11, 12.0000000001, 20, 24.0000000001, 14.9999999999,
16.9999999999, 25.0000000001, 12.0000000001, 8.0000000001, 25,
18, 21.0000000001, 14, 22.0000000001, 29, 6.99999999984, 12.0000000001,
21.0000000001, 6.00000000012, 7.00000000002, 23.0000000001, 26.9999999999,
23, 23, 16, 10, 26, 20, 21.0000000001, 23, 15.9999999998, 16.9999999999,
26.0000000001, 28.0000000001, 15, 18, 18, 15.0000000001, 32.9999999999,
26.0000000001, 23.9999999999, 21.0000000001, 23.0000000001, 18,
22.0000000001, 27, 9.9999999999, 14, 9.9999999999, 11, 9, 16,
5.00000000004, 13, 12.0000000001, 13.0000000001, 10.0000000001,
7.99999999992, 11, 19.0000000001, 18, 10.0000000001, 12.0000000001,
13.9999999999, 19.0000000001, 14.9999999999, 9, 12.0000000001,
22, 14.9999999999, 12.0000000001, 23, 18.9999999999, 14, 18,
11, 22, 17.0000000001, 18, 16, 20.0000000002, 16, 17.0000000001,
14, 16.9999999999, 18.9999999999, 17.0000000001, 7.99999999992,
23, 20, 21.0000000001, 20, 13.9999999999, 16.9999999999, 20.0000000002,
19.0000000001, 18, 18, 20, 23, 23, 18.9999999999, 22, 18.9999999999,
16.9999999999, 14, 14, 16, 18.9999999999, 18.9999999999, 16.9999999999,
13, 11.9999999999, 19.0000000001, 25, 25, 16.9999999999, 18.9999999999,
18, 13.0000000001, 18, 23, 22, 7.99999999992, 14.9999999999,
16, 14, 7.00000000002, 14, 23, 14.9999999999, 13, 12.0000000001,
4.00000000014, 14, 18, 18, 21.0000000001, 22, 14.9999999999,
15.0000000001, 13.0000000001, 18, 13, 14.9999999999, 18, 18,
18.9999999999, 12.0000000001, 18.9999999999, 26.0000000001, 24.9999999998,
18, 14, 14, 14, 20.9999999999, 23, 13, 14.9999999999, 14.9999999999,
22, 13, 21.0000000001, 16.9999999999, 19.0000000001, 15.0000000001,
18, 8.0000000001, 14, 24.9999999998, 23.9999999999, 23, 10.0000000001,
27, 20.0000000002, 13.9999999999, 20, 16.9999999999, 17.0000000001,
23.9999999999, 9, 8.0000000001, 11, 14, 18.9999999999, 23, 22,
18, 22, 13, 20.9999999999, 27, 23, 21.0000000001, 13, 14, 9.9999999999,
14, 9.9999999999, 12.0000000001, 9.9999999999, 14, 18, 16.9999999999,
14.9999999999, 22, 19.0000000001, 16.9999999999, 13, 5.00000000004,
8.0000000001, 21.0000000001, 25, 5.99999999994, 23, 22.0000000001,
13, 18.9999999999, 16, 18, 7.99999999992, 16, 25, 15.0000000001,
22, 6.00000000012, 20, 14, 18.9999999999, 16.9999999999, 17.0000000001,
13.9999999999, 15.9999999998, 15.9999999998, 21.0000000001, 16,
15.9999999998, 18, 18, 12.0000000001, 14.9999999999, 18, 16,
14.9999999999, 16.9999999999, 9, 9.9999999999, 17.0000000001,
16.9999999999, 5.00000000004, 8.0000000001, 19.0000000001, 13.9999999999,
22.0000000001, 21.0000000001, 17.0000000001, 16, 22.0000000001,
11, 5.99999999994, 14, 5.99999999994, 12.0000000001, 23, 12.0000000001,
11, 20, 14.9999999999, 17.0000000001, 17.0000000001, 17.0000000001,
13, 11.9999999999, 9, 9, 18.9999999999, 19.0000000001, 10.0000000001,
16, 15.0000000001, 18, 18.9999999999, 14.9999999999, 17.0000000001,
20, 13.0000000001, 19.0000000001, 21.0000000001, 23, 17.0000000001,
19.0000000001, 14, 19.0000000001, 16, 9, 18, 21.0000000001, 13.9999999999,
14, 18, 23.9999999999, 15.0000000001, 23, 21.0000000001, 18.9999999999,
14.9999999999, 21.0000000001, 23, 29, 27, 16, 8.0000000001, 12.0000000001,
9, 8.0000000001, 19.0000000001, 20.0000000002, 17.0000000001,
16, 13, 18, 9.9999999999, 20, 18, 17.0000000001, 17.0000000001,
20, 10.0000000001, 23.9999999999, 27.9999999999, 14, 17.0000000001,
19.0000000001, 21.0000000001, 10.0000000001, 21.0000000001, 24.9999999998,
25, 16, 14.9999999999, 5.00000000004, 9, 16.9999999999, 18, 16,
15.0000000001, 25, 23, 13, 16, 19.0000000001, 16, 22, 23, 27.9999999999,
25, 25.9999999999, 18, 12.0000000001, 16, 7.00000000002, 16,
23, 22.9999999999, 18.9999999999, 18, 20.9999999999, 22, 16,
14.9999999999, 13.9999999999, 11.9999999999, 14, 15.0000000001,
11, 20, 20, 16, 18.9999999999, 16.9999999999, 14, 7.00000000002,
14, 20, 5.00000000004, 20, 13.9999999999, 13.9999999999, 14,
9.9999999999, 16, 16.9999999999, 14.9999999999, 14.9999999999,
18, 19.0000000001, 22.0000000001, 20, 11, 17.0000000001, 18.9999999999,
19.0000000001, 20, 19.0000000001, 18, 13, 20, 16, 18, 14, 18,
18, 16.9999999999, 20.9999999999, 20.9999999999, 11, 16.9999999999,
22, 19.0000000001, 11, 14, 14.9999999999, 11, 18, 18.9999999999,
21.0000000001, 11, 9.9999999999, 17.0000000001, 23, 16, 16.9999999999,
14, 13.0000000001, 21.0000000001, 29, 18, 15.9999999998, 11,
18, 21.0000000001, 24.0000000001, 18, 18, 16, 21.0000000001,
20, 22, 21.0000000001, 18.9999999999, 22.9999999999, 31.9999999999,
19.0000000001, 18, 27, 18, 21.0000000001, 29, 18, 27, 28.0000000001,
22.0000000001, 18.9999999999, 17.0000000001, 29, 17.0000000001,
24, 24, 27.9999999999, 7.00000000002, 13, 19, 29, 29.9999999999,
22, 34, 23.9999999999, 16.9999999999, 18, 12.0000000001, 29,
13, 15.9999999998, 14.9999999999, 18, 16, 9.9999999999, 11, 14.9999999999,
22, 25, 25, 23, 13.9999999999, 25, 18, 20.9999999999, 18, 9,
16, 15.9999999999, 18.9999999999, 25, 20.9999999999, 21.0000000001,
14, 11.9999999999, 16, 5.00000000004, 16, 25, 20, 17.0000000001,
14.9999999999, 17.0000000001, 23, 11, 20.9999999999, 25, 22,
21.0000000001, 13, 12.0000000001, 13, 18, 13, 21.0000000001,
15.0000000001, 7.00000000002, 11, 20.9999999999, 19.0000000001,
14, 20, 18, 13, 19.0000000001, 9.9999999999, 11, 15.0000000001,
17.0000000001, 13, 18, 16.9999999999, 14.9999999999, 14.9999999999,
14.9999999999, 16.9999999999, 14, 19.0000000001, 22, 14.9999999999,
18, 14.9999999999, 12.0000000001, 19.0000000001, 13, 14.9999999999,
6.00000000012, 17.0000000001, 12.0000000001, 13, 13.0000000001,
14.9999999999, 18, 18, 22.9999999999, 19.0000000001, 16, 9, 7.99999999992,
14, 11, 20, 17.0000000001, 11.9999999999, 16.9999999999, 16,
17.0000000001, 16, 18, 15.9999999998, 11.9999999999, 16.9999999999,
18, 16, 23, 16, 11.9999999999, 27.9999999999, 12.0000000001,
11.9999999999, 11, 16.9999999999, 18.9999999999, 15.9999999998,
16.9999999999, 14, 13, 16.9999999999, 20, 20, 16.9999999999,
16.9999999999, 18.9999999999, 20, 20, 18, 20, 25, 16.9999999999,
13, 11.9999999999, 14, 27, 13, 26.0000000001, 16, 16, 22, 29,
27, 29, 23.9999999999, 21, 31, 18, 3.99999999996, 14.9999999999,
25.9999999999, 28, 22, 23.9999999999, 25, 14.9999999999, 18.9999999999,
20.9999999999, 20, 14.9999999999, 13, 16, 27, 23.9999999999,
23.9999999999, 13, 15.0000000001, 20, 25, 20, 20, 18, 10.0000000001,
18, 14, 14, 20, 21.0000000001, 13, 10.9999999999, 22, 18.9999999999,
23, 23, 5.99999999994, 20, 20.9999999999, 12.0000000001, 9, 7.99999999992,
26.0000000001, 13, 13.0000000001, 14.9999999999, 18.9999999999,
22, 20, 16.9999999999, 16.9999999999, 22.9999999999, 13.9999999999,
26.0000000001, 23.9999999999, 20.9999999999, 17.0000000001, 10.0000000001,
11, 16.9999999999, 20, 22, 22, 20, 16.9999999999, 18, 19.0000000001,
13.9999999999, 20, 23, 22.9999999999, 18, 15.9999999998, 21.0000000001,
20, 16.9999999999, 11.9999999999, 13.9999999999, 12.0000000001,
8.0000000001, 22, 20, 24.0000000001, 22, 19.0000000001, 16, 8.0000000001,
11, 16.9999999999, 14.9999999999, 20, 16, 9.9999999999, 7.99999999992,
9, 14, 16.9999999999, 16.9999999999, 16, 19.0000000001, 21.0000000001,
14, 18.9999999999, 18, 21.0000000001, 16, 13.9999999999, 22,
11.9999999999, 11.9999999999, 14.9999999999, 18.9999999999, 16,
25, 13, 23, 25, 17.0000000001, 22, 23, 20, 9, 11, 14, 11.9999999999,
20, 19.0000000001, 23, 19.0000000001, 12.0000000001, 20.9999999999,
22, 21.0000000001, 20, 21.0000000001, 18.9999999999, 16.9999999999,
18.9999999999, 11.9999999999, 22, 25.9999999999, 23.9999999999,
23.9999999999, 16, 20.9999999999, 22, 14.9999999999, 7.00000000002,
11, 27, 16, 23, 18, 12.0000000001, 20.0000000001, 28.0000000001,
29.9999999999, 16, 21.0000000001, 25, 9.9999999999, 11.9999999999,
27, 23, 23, 11.9999999999, 11, 19.0000000001, 13, 22, 23, 20,
14, 20, 25, 5.99999999994, 21.9999999999, 23.0000000001, 19,
19, 18, 22.0000000001, 24.0000000001, 7.00000000002, 14, 23.9999999999,
20, 16, 13, 16.9999999999, 20.9999999999, 18.9999999999, 14.9999999999,
16, 14.9999999999, 10.0000000001, 15.9999999998, 22, 22.9999999999,
20, 9, 19.0000000001, 19.0000000001, 9.9999999999, 9.9999999999,
23, 14, 25, 16.9999999999, 14.9999999999, 16, 18, 21.0000000001,
18, 16.9999999999, 13, 19.0000000001, 16, 18, 16.9999999999,
14, 18.9999999999, 18, 16, 21.0000000001, 15.9999999998, 16,
18.9999999999, 19.0000000001, 18.9999999999, 9.9999999999, 9.9999999999,
7.99999999992, 17.0000000001, 23, NA, 18, 18, 13, 20, 21.0000000001,
18, 23, 15.0000000001, 18.9999999999, 19.0000000001, 16.9999999999,
16.9999999999, 16.9999999999, 9, 23, 18, 22, 14, 21.0000000001,
20, 16, 14.9999999999, 19.0000000001, 21.0000000001, 23, 22.9999999999,
11.9999999999, 18.9999999999, 9, 10.0000000001, 18.9999999999,
14, 16.9999999999, 13, 18, 16, 20, 16.9999999999, 11, 13.9999999999,
14, 16, 17.0000000001, 19.0000000001, 18.9999999999, 18.9999999999,
18, 20, 18.9999999999, 12.0000000001, 16, 23.9999999999, 27.9999999999,
27.9999999999, 20, 20.9999999999, 21.0000000001, 14, 20, 23,
25, 9, 14.9999999999, 6.00000000012, 14.9999999999, 7.99999999992,
11, 23, 17.0000000001, 25, 20, 25.9999999999, 22, 22, 23.9999999999,
19.0000000001, 22, 9.9999999999, 7.99999999992, 26, 9, 23.9999999999,
19.0000000001, 12.0000000001, 16, 19.0000000001, 19.0000000001,
11, 19.0000000001, 16.9999999999, 13, 14.9999999999, 15.0000000001,
9.9999999999, 11, 18, 16.9999999999, 14.9999999999, 18, 18, 12.0000000001,
18, 21.0000000001, 23.9999999999, 21.0000000001, 22, 25, 16,
18, 9, 4.99999999986, 16, 22, 5.99999999994, NA, 18, 22, 16.9999999999,
9.9999999999, 14, 9, 14.9999999999, 18, 18, 19.0000000001, 19.0000000001,
13, 25, 22, 22, 16, 16.9999999999, 22, NA, 23.9999999999, 19.0000000001,
18, 13, 16, 17.0000000001, 19.0000000001, 17.0000000001, 14.9999999999,
14.9999999999, 15.9999999998, 14.9999999999, 18.9999999999, 18,
16.9999999999, 14, 18, 23, 14, 18.9999999999, 18.9999999999,
10.0000000001, 12.0000000001, 20.9999999999, 16, 13, 14.9999999999,
19.0000000001, 21.0000000001, 20.9999999999, 19.0000000001, 11.9999999999,
14.9999999999, 18, 23, 14, 19.0000000001, 18, 16.9999999999,
14.9999999999, 11.9999999999, 16, 14.9999999999, 14.9999999999,
14.9999999999, 21.0000000001, 12.0000000001, 16.9999999999, 16.9999999999,
18, 21.0000000001, 20, 14.9999999999, 18, 21.0000000001, 22.9999999999,
14.9999999999, 23, 22, 25.9999999999, 25, 10.0000000001, 20,
5.00000000004, 11.9999999999, 14.9999999999, 11, 9.9999999999,
16.9999999999, 18, 21.0000000001, 23, 23, 21.0000000001, 18,
20.9999999999, 22, 5.99999999994, 4.99999999986, 13, 25.9999999999,
22.9999999999, 24.0000000001, 22, 11.9999999999, 23, 25, 17.0000000001,
23.9999999999, 24.0000000001, 16, 29, 18.9999999999, 14.0000000001,
20.9999999999, 11, 12, 15, 16, 24, 29, 25, 19.0000000001, 20.0000000002,
16.9999999999, 13, 16.9999999999, 7.99999999992, 7.99999999992,
12.0000000001, 16, 20, 16.9999999999, 14.9999999999, 16, 14.9999999999,
15.0000000001, 14.9999999999, 11, 22, 11, 7.00000000002, 23,
18, 15.0000000001, 18.9999999999, 16.9999999999, 22, 14.9999999999,
20, 21.0000000001, 16, 14, 16, 18.9999999999, 3.99999999996,
14.9999999999, 14.9999999999, 15.0000000001, 17.0000000001, 19.0000000001,
7.00000000002, 13, 14.9999999999, 16, 12.0000000001, 18.9999999999,
14, 9.9999999999, 24.0000000001, 23, 14, 23, 18.9999999999, 13,
22.9999999999, 21.0000000001, 13.9999999999, 14, 5.99999999994,
20, 18.9999999999, 13.9999999999, 15.0000000001, 17.0000000001,
16, 16, 19.0000000001, 16, 17.0000000001, 13, 16.9999999999,
16, 13, 16, 19.0000000001, 18, 17.0000000001, 16.9999999999,
9.9999999999, 11, 5.00000000004, 14, 19.0000000001, 21.0000000001,
9.9999999999, 9, 20, 20.9999999999, 14, 16, 14.9999999999, 14.9999999999,
18, 18.9999999999, 21.0000000001, 18, 15.0000000001, 14, 13,
22.9999999999, 20, 23.9999999999, 23.9999999999, 18, 22, 11.9999999999,
6.00000000012, 19.0000000001, 20.9999999999, 14.9999999999, 15.9999999998,
18, 22, 16, 13, 7.00000000002, 22.0000000001, 20, 22, 18.9999999999,
18, 18, 15.0000000001, 13, 17.0000000001, 23.9999999999, 23.9999999999,
23.9999999999, 22.9999999999, 22, 15.9999999998, 14.9999999999,
13.9999999999, 13.9999999999, 21.0000000001, 20, 22, 27.9999999999,
28.0000000001, 28.0000000001, 23.9999999999, 12.0000000001, 23.9999999999,
21.0000000001, 23.9999999999, 20, 23.0000000001, 25), HIrange =
c(25.9386252175,
21.8138971228, 17.883743264, 29.2264621572, 26.4841922119, 16.3687180728,
17.3464370753, 19.2193281222, 15.9080472422, 22.9354269507, 17.4323660952,
14.8774278633, 22.0760659814, 23.2762247181, 13.6447858658, 24.8830147391,
27.9526870284, 27.3891804709, 29.9563115113, 20.7866389566, 26.78841008,
27.5461667305, 22.9652796392, 20.7976883947, 12.1123018726, 16.9758555747,
18.9423552214, 27.0328922476, 11.5249310646, 21.2979770044, 10.9585995331,
23.804710975, 20.8342466575, 20.1235158717, 17.4708005731, 13.9867646874,
21.9517752428, 17.4557168786, 19.9750485168, 11.635155445, 15.3776136866,
11.1024567562, 21.6209291622, 23.2559584776, 21.7527604535, 11.2531604146,
26.5317983088, 16.5101525194, 25.9915653846, 26.2141135525, 22.3744410374,
30.9091402341, 17.0597204746, 20.5655672315, 23.3137574884, 18.0298987356,
25.2336428067, 22.3026213065, 22.2489463003, 22.7642379977, 22.0409305009,
21.7620511712, 20.9353089691, 23.8047225822, 30.1256477468, 28.3540535076,
30.0009560084, 23.919764699, 20.3020615874, 10.4509861076, 16.8653049101,
17.7724084034, 16.593511007, 20.8137691352, 29.9939558116, 22.1532868697,
19.1380990137, 25.0080325347, 21.8123892322, 27.5160275826, 29.6828164017,
18.4638050052, 25.4280244475, 26.2700830374, 20.2171841781, 19.4515338841,
19.0832326091, 27.3147116758, 26.007083648, 27.4236536446, 26.334818853,
20.5027675641, 8.8316937576, 17.8517479482, 15.7877432195, 20.9914237252,
25.0082317371, 17.9195571598, 23.5982476853, 25.6529188566, 27.4551408449,
22.5020615874, 26.4916246783, 27.2070558602, 16.6132795717, 16.0256312763,
15.692895487, 19.8102088876, 18.5462251118, 26.5280244475, 24.5587010198,
29.506409164, 25.6513099596, 25.5704436194, 23.5996736927, 25.495971234,
26.1053766047, 17.5249450764, 23.3694525696, 20.0943654715, 27.9925594665,
16.032147468, 17.0397450425, 40.525302314, 35.717390658, 29.811922793,
14.5596213917, 20.5411386413, 21.7712190739, 30.4929767498, 27.3884332289,
22.5397450425, 25.2490892759, 11.3112672379, 14.7757907785, 25.8847793213,
18.7560415064, 12.3267749788, 10.1168736888, 10.5456135388, 22.1804743895,
19.6762663703, 13.6501115695, 10.7807959846, 23.162742021, 20.9493151968,
11.3764334079, 20.0332583912, 25.7885493145, 28.1611261562, 24.0129957314,
15.994042077, 22.1218990344, 15.5017442842, 30.1327398326, 22.0717240754,
34.1595180697, 23.2242676951, 28.6657699072, 24.4788158929, 25.5662466725,
29.43357719, 19.3548692952, 21.5272550541, 30.7227636947, 8.3316422763,
8.6374020239, 17.2426317445, 26.4100652012, 12.3131014458, 21.223258413,
35.2812629684, 23.8368512942, 23.5001308014, 27.1372868115, 24.8678962697,
21.619529256, 27.6925930813, 29.0395068414, 14.2842792564, 9.4326781353,
24.125291574, 14.7153107503, 11.8416537043, 17.309607356, 17.0770677423,
23.5945244388, 13.0230091328, 18.9939154784, 23.3463345715, 26.2783854924,
24.2540424996, 19.923655443, 18.0026096532, 23.572791409, 19.0824154614,
20.8703883599, 23.0164764716, 21.9573145728, 22.1325333782, 28.7046530572,
22.6427627968, 17.4721742798, 24.9477452922, 20.914250106, 21.1704590032,
13.8158407309, 21.7546593239, 14.5631056993, 25.7263047564, 23.1917786334,
12.9793188898, 21.5370002794, 17.6234065971, 24.0942182209, 11.0099977332,
7.7998982568, 19.5116270311, 25.3857196777, 22.8763393461, 20.7525069633,
23.2967683473, 22.7324443893, 27.069042053, 26.263501921, 29.6819326931,
29.8421005618, 25.0425616301, 20.5615507812, 13.5260295876, 22.7219540156,
27.6399670107, 23.9579186806, 26.4367906588, 26.018050001, 22.5135891489,
25.6503703174, 29.9317378932, 25.0692961849, 35.0545844385, 20.9902649764,
28.5710855782, 20.4870592513, 30.045871602, 26.9131998015, 18.7998745845,
15.6875765979, 22.5900516362, 22.8855289251, 22.6360861688, 14.6848796296,
17.5342466575, 25.5697550397, 25.7527672575, 28.3766682372, 20.9296717038,
19.0718761361, 24.7581284081, 20.4256312763, 7.8073142388, 20.28352922,
22.6568384948, 22.8694443362, 25.0763393461, 25.387861642, 23.1516626947,
22.4204860191, 25.0638050052, 25.4450775343, 20.4024250399, 26.654276844,
22.7324443893, 16.3185609452, 22.7915985282, 25.7212718348, 25.0916883784,
14.5294176526, 21.0679348445, 20.1941221893, 25.0425355117, 28.0201396013,
29.344491297, 31.6409952887, 24.6810286796, 22.9531576419, 30.21957321,
32.3955528594, 25.2284472845, 33.4260033763, 23.1586304603, 20.2543427809,
22.8049178224, 26.6987817248, 32.9278656513, 24.7851156378, 15.2693621786,
27.8373236466, 37.0661837607, 13.3727317473, 24.0940812728, 20.5555835265,
18.5598547224, 10.4774278633, 19.6930719038, 19.8826895864, 24.046329634,
13.0418588236, 14.7153107503, 11.8222850866, 28.2555009737, 22.5664645596,
11.8511972018, 10.9776136866, 21.9274451354, 20.3473968979, 27.0811971393,
28.8467559931, 31.8806583589, 32.0410352282, 25.4413328687, 22.9117611335,
21.0672299236, 17.9054558167, 20.649377123, 19.7923417401, 17.9452559335,
23.0215832245, 17.5800638298, 19.1249226329, 17.8741200116, 26.8431880878,
23.0721347467, 23.4867322114, 18.4089860839, 19.2801174392, 27.350418548,
18.0864912664, 23.7950051588, 22.5319282601, 29.6555825471, 16.9769084398,
30.0052663921, 22.5222425825, 22.6238627412, 16.8578706844, 24.4193127483,
28.2254582185, 33.9189785232, 19.9508005713, 13.426901287, 20.4508714872,
27.33599854, 21.603415224, 19.1267223992, 15.8423791496, 15.9934489019,
19.9199917308, 21.258490564, 24.234828214, 12.7190841925, 21.1972330043,
21.0377433018, 20.0972330043, 23.0856455399, 16.4657728863, 18.5167627472,
28.051105169, 22.7929767498, 22.012362004, 30.7401446425, 25.8814343707,
31.0620323778, 29.2729781055, 30.6202902767, 26.997934796, 19.2230241889,
25.5056835216, 23.2085330175, 34.452097555, 35.32310623, 25.0934965971,
16.583795741, 29.669222355, 24.2344322363, 24.4454938754, 20.3360512829,
20.713188069, 21.6286635164, 16.2404983877, 25.8870560884, 30.633759486,
26.2269419584, 20.1911412636, 22.8172190787, 25.8106381535, 22.411910332,
23.2677843192, 21.7948658012, 18.0421912376, 16.4751925832, 19.0053813876,
28.0473955008, 22.2492313068, 25.7379563573, 27.7156645992, 22.6058845587,
24.1327193074, 24.2467775296, 16.7136178395, 17.6200413079, 18.7664835614,
25.3294263116, 24.2477620302, 19.2442177138, 13.2442817092, 17.7659545309,
21.7204941789, 22.6151311241, 19.8877678522, 18.5548941128, 11.8087287405,
10.0395310496, 25.2321891125, 33.785705588, 19.4807750008, 23.7155607939,
10.7511972018, 22.1006220263, 22.6082595187, 22.5840381158, 25.2236536446,
22.8053766047, 25.758002771, 22.819764699, 21.7565765566, 19.3528635669,
21.3058622424, 21.5953067983, 21.4660895155, 23.9963792658, 18.4336063571,
13.4400187161, 8.3906432017, 17.2371846118, 21.9249450764, 26.3579408461,
19.1108018929, 11.6286267293, 22.1024078632, 11.2100964862, 30.0877432195,
20.5555009737, 15.090749674, 28.2501620441, 22.1400023453, 17.0862808591,
19.7903931964, 14.5452754029, 14.6645264296, 16.2090710879, 23.191160284,
19.6663597319, 4.0589858453, 9.0208831012, 19.7420663487, 8.3191937126,
17.3821461283, 13.3408248483, 30.1395352562, 9.3854397454, 25.9492471341,
12.7689099466, 18.6063992842, 11.4477491118, 10.0812816556, 19.8768945104,
27.2712190739, 26.2974746467, 19.8845217626, 14.8082372387, 9.4530199774,
16.8073722887, 26.4360659373, 25.0170684397, 24.2204227492, 20.0044633817,
18.4675748594, 21.089744698, 17.0219956677, 25.9424936009, 22.0292912568,
13.1590816682, 17.9053619682, 25.0659844375, 21.6204464415, 15.0268098654,
17.1409188011, 13.2364113783, 28.2406997764, 25.331129776, 29.0365302872,
18.8193794063, 20.3281174428, 29.3757147266, 26.2191749535, 24.5898678603,
20.3514489421, 19.3536302633, 22.8694443362, 12.9567264482, 15.850409343,
25.0151168393, 14.8249310646, 28.5038504836, 26.0081283259, 20.428659417,
24.4752874234, 25.9962052982, 23.6795071692, 24.1757067401, 25.8057062483,
22.1515696744, 17.0545277353, 18.8484400527, 23.7192357956, 30.3735025401,
25.0449806842, 17.855459466, 19.0680563616, 11.6051266845, 17.2465838503,
17.4159855669, 24.7215765819, 25.7172808229, 24.2334849968, 22.5878893176,
22.3102834181, 23.9743818718, 20.3364644388, 21.3465647543, 21.7112826805,
20.666232325, 14.5579812399, 6.451917658, 25.269400429, 32.3224902034,
20.4074062242, 23.1503505919, 28.8648387406, 11.4552756739, 23.2844163619,
17.3442759766, 17.3664929929, 17.0937731602, 15.0049141236, 12.7141722985,
17.616748292, 26.4022456173, 25.4964241045, 24.5811564705, 23.7198720298,
23.6347424361, 20.3083581659, 27.0013909922, 26.6335606494, 23.6073361038,
19.8420137569, 31.5695005076, 22.0620328264, 22.9931220899, 25.9552105599,
27.4519155078, 28.3799591278, 28.3674570396, 26.3880579508, 32.5404490819,
26.767421792, 21.5696144236, 15.6064329757, 9.8300647295, 15.2670341112,
22.1118192729, 26.8582980809, 28.2033624201, 24.0038956321, 20.4936399227,
25.9738905712, 27.7392868662, 23.6411563023, 27.9181317994, 20.186267146,
24.6185077139, 21.2907773594, 13.1775118975, 18.4946161729, 20.8959132196,
27.955299219, 19.3795123008, 20.3073166757, 17.3289833867, 20.1500867042,
21.7389160025, 13.3764334363, 21.6865643937, 24.5726746987, 29.3893881046,
20.1848806469, 29.370309125, 27.327179984, 22.7880111258, 37.9191398589,
15.1173838995, 26.3044042335, 24.3819134434, 13.2162668754, 9.9772148819,
29.1773808376, 24.5983944143, 28.4979428933, 18.7692923055, 18.5966519644,
19.5108338308, 26.8358689501, 15.6030749225, 27.161714569, 30.5729955394,
23.3666809053, 23.8943214932, 24.9347797975, 22.9052558618, 9.6118202001,
14.4763332023, 15.0296093227, 28.0526132526, 21.019797123, 19.8003462519,
27.3438335358, 31.4988780322, 18.4571676196, 28.5910568043, 24.5639821921,
19.8940544996, 17.8412463147, 16.8763950693, 13.6283899989, 27.2050412068,
21.7278769513, 25.2794570243, 15.0156683036, 14.3276133199, 17.9168366959,
21.7112826805, 25.299621601, 25.4911378554, 23.6736847163, 15.4132613523,
20.2265390524, 15.8654378561, 18.8854618877, 20.1256289227, 18.6804647137,
22.8391886733, 25.8226109532, 19.3185629828, 15.7111147844, 21.6722427496,
23.432416932, 20.677472566, 24.1288927916, 18.3238135614, 9.90687425,
20.9423597361, 22.2010436068, 20.8219169615, 18.8380649621, 18.5902879315,
28.6625204855, 11.8763255281, 17.7766784748, 13.4198216529, 15.119887286,
18.0826297279, 19.7496957682, 18.3106082107, 17.4720941868, 26.248746474,
27.3565068476, 16.6125241593, 15.1038390196, 16.7926992221, 21.767749745,
19.3802570257, 18.682612971, 23.275882465, 22.2097921067, 20.4210714823,
18.5823187226, 14.794728818, 13.2089988621, 7.4216668656, 22.5285349309,
24.5266188161, 27.5409658492, 8.9034589562, 10.5385023734, 18.3297923621,
19.5388737726, 22.5338956942, 20.8445162416, 21.6912176644, 23.0625239449,
21.6912176644, 18.5447673702, 20.6374729213, 13.0808050947, 18.7962105705,
23.0425405204, 15.8764976809, 10.7663159391, 19.9769344977, 30.2319589386,
25.6697772552, 21.3465647543, 22.3799818814, 21.2084493065, 20.7089855955,
18.7667617728, 13.526197977, 16.8681937422, 14.6672189567, 18.218788167,
17.5304251744, 18.040118958, 10.4578198188, 19.2230241889, 17.6364892912,
20.2295378626, 19.6755608239, 19.5775447684, 18.1937260647, 20.346008933,
15.6331763377, 19.5077259068, 24.6549771623, 25.9169288879, 24.0977799426,
19.2161016716, 11.7336813643, 17.3040832286, 25.9686644185, 25.8508103617,
29.7645561706, 21.3036145935, 23.1566687941, 17.6643202674, 8.9605545868,
23.6925006378, 26.4010242099, 30.3250574512, 26.1415060668, 21.6672002002,
23.0556523967, 27.3913062639, 19.9136462323, 26.1601362818, 24.7847793213,
25.9866074995, 28.8044334502, 28.1760639178, 27.1903060287, 21.5064089433,
13.946351329, 19.4464089027, 30.4969567497, 28.8063513424, 11.7260186887,
16.6441302709, 23.0219948536, 21.3058526012, 26.0723224038, 10.008598493,
19.4467425674, 11.0105587352, 14.8031773861, 20.9896358649, 19.1913679924,
22.348416056, 10.6178554141, 15.9395704558, 22.5540443529, 19.2161016716,
25.7041330162, 21.693778737, 18.3089212862, 18.3310495583, 18.9814882251,
20.8316201964, 15.616009126, 12.5695328256, 16.9790277265, 27.7360834442,
21.7874806694, 23.6980458155, 15.4856292772, 26.7446051857, 24.4117910469,
24.8013051045, 27.714864289, 29.1189845364, 27.262792339, 29.2143221025,
16.6588231868, 24.0974783272, 27.2620973858, 27.5179252114, 25.1425136088,
18.3435982748, 28.1488763076, 12.008562435, 16.6155414682, 22.819764699,
29.4355281504, 28.7742951172, 39.1676435006, 18.8501068395, 21.9051950077,
40.6504002036, 27.1758116028, 22.4925123409, 36.963746259, 34.633183469,
29.955162233, 33.5308994793, 28.4627850599, 26.6260658655, 35.1865401991,
33.1189333252, 25.3052088644, 23.0767241725, 31.618915358, 27.9600239147,
23.8914194684, 23.2048593252, 23.3998154181, 27.9490799292, 23.5380641185,
27.9925594665, 16.5463547203, 16.7118217397, 22.3115337786, 25.8075811389,
10.6478781549, 9.6970854296, 23.187861642, 28.587046974, 38.081541693,
22.1588927164, 23.8570941185, 13.4790769513, 18.5472624207, 35.5808022006,
18.7859417433, 19.5173240594, 19.6777489893, 21.4179759548, 23.3520442146,
28.9710363318, 23.9179671618, 19.3380346905, 24.7759943182, 14.4584597009,
15.3306094805, 29.826293763, 27.242619231, 19.6841449642, 27.4353948858,
20.64681812, 17.9462185048, 20.8667666762, 19.6101515998, 23.1726662118,
24.4162327919, 27.7802940789, 24.7643901502, 17.7449040784, 14.3952861225,
23.6963577093, 24.9999209774, 17.0526482001, 17.4976883947, 28.6195046487,
20.4143293056, 19.6022440197, 31.068989328, 30.0216569803, 24.9363403196,
28.3924580375, 16.6826971226, 25.6082021351, 29.2606726382, 19.8983405335,
16.2157495511, 18.7331774117, 31.0694525696, 12.1560415064, 26.1712190739,
26.3362170077, 25.1502322181, 22.7751340415, 32.9469763143, 13.2400646418,
25.9988107018, 21.7472768003, 20.7124338772, 21.6514750449, 11.7056784338,
26.9770677423, 26.8527672575, 12.8423265032, 23.6773808376, 26.8058429225,
14.3726724727, 9.9, 19.1078115612, 23.0416535779, 23.4094770361,
14.7207168217, 23.1, 18.8860277684, 19.2492219614, 5.9613827522,
14.8052895094, 33.3261943924, 19.3788530201, 5.6857794519, 7.7,
18.6348958584, 28.6394547168, 32.1839921311, 28.8140997799, 17.4714304542,
13.100819015, 25.0851787303, 7.4422301156, 22.518620331, 22.3941221893,
13.0276588039, 10.5847584471, 22.704710975, 12.8371846118, 19.7194386005,
15.6681067133, 20.3347424361, 12.4431123458, 9.5368717463, 26.4914237252,
22.4292571864, 34.029235233, 30.7331096622, 10.6222691813, 27.5477620302,
15.6881292, 15.398763436, 21.6728306235, 12.2226858012, 24.7996324339,
23.8653902873, 9.777039909, 22.8026203356, 12.6448108029, 6.9144546593,
18.8745149814, 28.357072779, 15.3840386584, 20.9910027007, 22.2006772418,
19.0073120047, 18.9718541164, 15.959150791, 17.5579408461, 16.0913646058,
16.5549582243, 21.9306094805, 6.2975163399, 21.1972330043, 21.2317714235,
24.148608081, 28.144938265, 21.1242473184, 36.1586052354, 30.4097913949,
24.305010225, 23.911363222, 27.627163969, 15.3306094805, 22.2859459389,
19.888058033, 29.826325462, 31.0485962867, 14.6655905874, 17.0016007808,
11.0123018726, 12.4014561939, 24.4778761236, 23.1079071555, 26.0532735216,
11.8791299742, 15.4063069322, 19.6767250517, 20.107366257, 23.2370909077,
20.3784753177, 21.496457819, 23.9514865554, 19.237297854, 17.549488144,
13.4424762391, 14.7278232738, 20.7141592703, 13.9891323264, 15.1089129864,
24.5614651569, 16.2354740465, 27.0644406123, 25.5466645682, 15.8576602802,
14.8283048683, 18.8037969433, 21.7169080643, 22.0882215263, 23.5582980809,
21.1569301839, 24.7889027325, 16.7792320572, 24.7304370816, 12.3387870922,
9.3122124501, 17.4329014983, 21.3788621561, 24.0675862627, 27.734609467,
6.6074679767, 19.1913214815, 14.5229438931, 16.2157495511, 19.725291574,
19.5495406389, 14.8137002213, 19.6547748378, 22.5060110087, 26.017535175,
19.9384233806, 24.1111507587, 14.254485849, 11.6246059175, 14.5296870003,
14.4337909062, 16.2559519986, 22.4245648812, 26.5891285906, 25.6736393924,
25.3161555515, 12.9539362206, 20.6102585188, 11.5789819593, 24.2301950182,
21.2575514048, 24.3103144834, 23.9891342922, 31.3036921714, 20.5173655777,
24.7331574492, 24.5346006996, 23.9642980021, 20.0692244807, 18.9061923613,
22.6194596999, 21.5568642318, 27.8299940409, 29.3066770686, 24.953234484,
25.4411203654, 19.4603748652, 18.1283685589, 9.3827489401, 23.7308723466,
24.0953614335, 24.7356841039, 15.113601756, 23.8170150529, 19.6122784347,
20.2212742934, 10.2187546399, 9.9882128001, 17.7954037142, 13.4858628877,
15.7941221893, 16.5366491303, 24.0030103028, 19.2237798811, 23.3822784665,
14.0588342003, 18.6833705701, 17.6758147287, 15.2712057754, 9.2788752536,
16.8022466568, 27.6844163619, 26.8892911817, 20.0822784665, 13.8201488284,
17.1009472852, 15.6742935734, 9.1762084932, 21.7495406389, 25.3010703419,
23.2171777982, 9.7210877372, 20.6184977911, 17.4897375575, 14.5485442382,
24.7918963798, 18.5013033231, 17.1798547187, 21.2342640485, 22.5401118023,
17.4192364006, 18.2309676427, 18.1018088127, 19.4351529642, 27.7726350167,
14.6757177065, 20.5070084553, 29.6895317399, 31.0499432163, 33.4383344044,
24.3196437298, 21.6836349285, 15.5776542605, 13.6854736522, 19.115005715,
19.1311621662, 12.6750933439, 23.4145729933, 16.5620439448, 17.6364892912,
23.393358156, 22.2525407504, 25.7379563573, 21.2447777363, 19.0076417589,
25.9783913492, 26.5311227823, 12.9546593239, 18.5859008139, 21.4090220316,
26.0661810101, 21.5679026789, 22.4332148492, 12.5068619342, 21.2557550936,
17.7286844066, 18.6369081647, 26.1438245229, 26.8702153163, 32.0138416842,
19.8128761596, 31.928056748, 19.4986743331, 16.6085330175, 21.7294420868,
18.028510851, 8.6813555304, 14.4977270977, 21.2597301607, 10.3487309131,
23.2844163619, 8.4962975413, 17.8517479482, 20.7564609124, 20.2094617651,
23.4052124414, 25.2161926169, 18.1128713242, 30.4259401135, 12.732147468,
16.6006220263, 21.3880775735, 14.0395601397, 5.3593338366, 14.3478556533,
29.0363065031, 16.534828214, 27.734609467, 19.7327193074, 26.5467560227,
20.1061537441, 23.0156065105, 20.7780677689, 23.1527931248, 8.1711784371,
14.9319289654, 23.0249450764, 18.8701812102, 17.5490892759, 19.1159358603,
8.2350938794, 21.9482973155, 13.8474183393, 18.6937614261, 27.0079684627,
12.0830835991, 17.4200183577, 9.1376545991, 17.9944026056, 23.8255012692,
23.3420240225, 16.8083633485, 5.0925390537, 4.8067512017, 18.8721025905,
24.0821458992, 25.3513057299, 24.1814283502, 14.5334956885, 12.2081126089,
7.0012101318, 29.361714569, 30.0261943924, 20.6024637444, 21.5905517138,
15.470569016, 27.5992711703, 32.2913591444, 28.1763419959, 12.8422344858,
16.9567273519, 17.1898994236, 9.0456396472, 25.81457903, 25.6779837699,
11.2765353601, 13.7449259101, 22.2785752333, 14.400736083, 14.4089300456,
16.6361898963, 26.0622568723, 14.3814032004, 20.6438221061, 12.6500246577,
23.8400669993, 22.5256967636, 16.8782549051, 17.8286335815, 21.3328041669,
4.4, 17.0397450425, 17.0521968417, 22.7355721336, 15.1287883281,
25.2718005482, 22.373540043, 18.5664597044, 19.953128956, 16.4359112434,
15.2322235019, 18.3089212862, 15.0316780217, 23.0413458012, 22.042935148,
18.5777651558, 20.8219169615, 12.3184694398, 19.8140277653, 23.6461453944,
19.8901295447, 23.7559557898, 20.607907952, 21.7634751304, 19.7221408022,
21.6654056138, 20.9065692452, 18.4942734276, 23.1245993038, 25.4114229508,
17.660187375, 26.5056241452, 17.1666810965, 17.274124405, 17.4733403586,
27.5018558029, 27.3424582578, 23.6120491505, 23.7296229091, 23.2677951904,
23.3541424707, 19.9179025319, 15.7247108503, 15.3325549017, 21.151386781,
22.5267424326, 26.3722253461, 25.83607441, 23.6625595296, 25.1301060836,
22.301914708, 23.5242279651, 27.969431225, 15.413730742, 19.9115089888,
25.5543926235, 31.8938629115, 24.7733805956, 16.4147223933, 27.3466780078,
17.0614252919, 24.6455692738, 30.7887841141, 23.0690802879, 25.2652545125,
27.9320925193, 21.1371691474, 23.2535929886, 26.9269163597, 32.7905135264,
15.2122828795, 16.3878342324, 21.9629443961, 24.0901685339, 26.4704591835,
29.0979425522, 22.3076054742, 19.5687227653, 23.735799505, 22.7260693321,
15.5243215275, 35.286167353, 28.509502607, 27.626293763, 29.570265414,
24.8574034266, 28.4627850599, 22.0377873922, 22.254276844, 10.4289284721,
17.4230301822, 16.9538566294, 20.3274529775, 21.7966727684, 22.6242178127,
21.1121298532, 19.7004709434, 14.3424024534, 31.3131475132, 20.893295474,
21.124828935, 13.9542174602, 11.5407200569, 19.2697980623, 20.4995992696,
15.3616766357, 22.0172336057, 38.5530818768, 17.711489429, 19.1936667684,
36.4522525993, 21.4192123671, 16.9874128021, 23.3502964609, 28.6177107077,
5.1495379256, 20.1829655531, 26.9289319158, 21.6929767498, 24.0214521636,
25.0464203948, 27.6677338724, 16.3214006613, 21.3548524408, 14.7724558696,
16.9501115695, 11.6250419053, 11.6062163886, 20.4904487526, 22.8980495961,
39.6428682021, 19.7728475059, 22.531452113, 21.4451622048, 16.1902185979,
23.7757399165, 25.4354348674, 31.639588586, 26.3525857282, 22.9862549675,
8.9793043539, 11.8559342169, 21.7295519106, 18.3153560094, 28.2288784729,
26.312377052, 16.3200183577, 6.7643455939, 18.4083505796, 22.8631666881,
25.5997799682, 28.6870422723, 27.5909321297, 20.4917902765, 21.048232741,
21.0134205049, 22.4897913627, 16.5831862912, 18.1401574622, 12.8999839997,
14.5697588802, 15.3890709987, 23.873105644, 20.0477081511, 18.9109579352,
20.1870904645, 21.7668786191, 27.331202775, 25.2991141729, 21.4306683402,
20.2575919884, 15.3910608385, 13.7253252383, 27.91512077, 24.8416784283,
15.5782233922, 23.850666551, 13.6695014365, 12.9957026667, 19.8939428649,
17.0656682019, 16.2517127209, 21.6190695735, 20.4988946172, 22.9363807363,
11.4916448249, 18.8313716988, 12.9631445062, 20.8721173774, 25.7301811456,
32.7019861781, 15.4804297658, 19.2751691337, 21.0826656756, 21.011561329,
23.8013621282, 27.450515657, 13.2566176398, 21.7877266014, 20.9141914196,
29.7960998122, 14.5401858563, 21.8925933373, 21.6417193335, 18.8003261904,
20.6486219255, 21.021934957, 24.7496534893, 20.4297710812, 14.8055419356,
22.1456877444, 22.878276839, 19.7103179775, 24.281408524, 20.5647300476,
18.8375351578, 17.1590282186, 22.5338956942, 25.2986297051, 28.8456987359,
26.8711496666, 24.0451411272, 19.2870518337, 18.2711779361, 21.1886411661,
18.8302236286, 22.0015330144, 20.6263624138, 22.4454762982, 12.7782093229,
15.8247912952, 15.6380519631, 20.0009241453, 28.4241686011, 12.1818304922,
16.7405270461, 16.9663969599, 24.7182823844, 23.6213099111, 28.3769973748,
27.4501199142, 25.740942628, 17.1643703595, 25.6412845842, 27.2907592631,
27.7455252841, 11.0189803133, 22.2363995412, 20.4527431765, 19.7943564225,
17.2671997512, 16.6844163619, 10.2645264296, 21.7565765566, 24.0646135568,
26.514897374, 24.8887540331, 17.6799458389, 15.5098596952, 19.4595213221,
17.0165108428, 26.9552001914, 27.5371140988, 24.9357816231, 24.9357452973,
6.9976233966, 20.3478191224, 18.9469063199, 36.8928049595, 16.2968353205,
9.0425965538, 17.8073804058, 11.3618264626, 20.5024143782, 13.8827485174,
17.4433465548, 26.0163113671, 21.477424754, 26.3282257724, 14.2572801101,
16.4185819001, 26.9035380401, 25.0608552927, 15.6153953714, 14.2536691743,
15.713927442, 16.1321108098, 22.8585554602, 15.2480932295, 10.6486374635,
8.2592973251, 27.1212127609, 25.6843739179, 18.0679426917, 15.4181908935,
13.6983066559, 27.5156849723, 27.1205904261, 18.7946281218, 24.4465727824,
13.0528536977, 13.7219626785, 21.9970172686, 20.3616485253, 15.6139056464,
16.8180900484, 22.7657511627, 6.8574016337, 12.4673535104, 3.1400991822,
6.7639720602, 10.1334956885, 30.2020615874, 17.8127711925, 20.1463337249,
18.4883617485, 7.6348958584, 22.7999904172, 17.1504499909, 20.6030694652,
23.1359788346, 10.9290634444, 16.828677239, 19.1142060832, 21.9173089828,
18.6285309603, 16.5092779881, 28.5248175805, 21.2898446817, 27.9364533114,
25.5623519955, 20.7419839588, 15.7150822842, 19.0473569921, 19.7465186228,
20.6798010637, 24.281408524, 16.4327732593, 25.8165632474, 16.4250745291,
17.9876630608, 22.4910197818, 22.3750799193, 21.6917570677, 13.8055785867,
24.040141241, 22.183828548, 22.1836489048, 20.2362247275, 18.1212087751,
19.5261305622, 19.4760636522, 20.5828862176, 24.8403746289, 20.8590469115,
14.2636903323, 15.3790086612, 26.3125778312, 28.0145766949, 25.9889959282,
14.8757462183, 18.745127704, 18.8745149814, 21.83599854, 23.0105226132,
30.393875374, 11.1612409889, 19.3543678774, 18.6075289163, 7.3093589347,
25.6317714235, 28.6973216864, 25.1181665235, 19.550010635, 26.8133053562,
16.602404392, 20.5746235024, 33.1610957961, 25.7507698718, 23.1477620302,
7.9432537698, 22.2517479482, 24.852182784, 13.9903030068, 12.3608262478,
16.5366491303, 18.9076616145, 7.0137641636, 5.1900929952, 18.251591616,
10.1134943864, 10.8418588236, 24.9452557949, 21.2941221893, 27.0038901205,
24.3112301841, 21.9579408461, 37.273592533, 17.3431043092, 18.0123273073,
10.3189835147, 30.6768455453, 28.4291331468, 22.7084948675, 29.8153314922,
28.650077249, 11.6158407309, 15.1929807939, 18.6999651868, 13.0850773306,
15.9455739901, 21.355746648, 24.8741998982, 22.8079468052, 21.6827953598,
18.5279988099, 22.0512411625, 23.2838372892, 17.216185068, 26.2568571549,
28.3144821431, 6.6694441106, 9.8197934687, 27.9104777556, 25.4345887188,
23.2753955072, 20.1471409561, 13.3834835932, 17.0993899494, 20.1862411856,
22.9248953837, 28.3721605832, 14.5321861547, 21.4136539007, 21.8017780627,
18.8483931199, 11.0581943332, 11.8896109255, 25.3002071442, 14.2871881675,
27.3079207307, 13.8819260983, 20.8159003128, 22.6684759199, 6.2117020596,
16.894171261, 20.6908315556, 23.7181060777, 25.9948577778, 12.1227770188,
11.7831454615, 13.829921713, 8.8813363876, 19.5027703094, 11.5032977539,
17.0397450425, 19.1828400231, 21.1526647141, 19.2041960751, 20.1705776556,
25.3634082557, 14.7828656023, 16.9370219163, 16.6063591055, 14.0816653569,
22.5059979245, 11.4164609123, 23.1079071555, 16.9258104087, 8.7022501866,
24.3869642947, 20.74225446, 23.7279375987, 22.7715460972, 15.4297764979,
20.1871204089, 21.1742652871, 21.5774778183, 20.9120702105, 18.3833918945,
13.249214009, 11.7739836992, 23.8018967908, 26.5264400341, 23.907962818,
25.1694016765, 24.7656199002, 26.9447256573, 18.3097759321, 17.9711667464,
17.6461081571, 23.1018616195, 20.2944035182, 25.5313973393, 19.0464274766,
18.812212795, 20.8291745525, 23.6697732571, 23.205299942, 15.1117125361,
22.408704072, 15.3529091613, 24.9984532627, 23.3654300394, 21.3647896214,
26.8129800212, 24.5654333995, 17.9299753591, 19.7352135013, 17.9346027158,
22.46329379, 25.9495407197, 25.6876046278, 22.7568437229, 30.4272692733,
32.9904140483, 23.6055973387, 25.6856062458, 18.6669240571, 15.1729939027,
19.1393691805, 21.7999454521, 21.2228531133, 16.6393347624, 24.1551408449,
15.9772540852, 17.6470104167, 19.281227906, 16.0982837003, 17.4684081953,
21.7758143661, 19.6339790886, 23.8272220917, 10.7074936569, 14.7395482961,
22.9230301822, 20.3083581659, 23.375809277, 21.3067753991, 19.785570029,
26.5774456385, 21.4823655592, 21.7614400581, 29.9057214379, 24.516400008,
17.3121040255, 19.7054742411, 24.1934486971, 27.5514801089, 23.9602867197,
17.3731660851, 16.1241173464, 16.6575796104, 20.1491212138, 22.9927335731,
21.7718572269, 15.2112512318, 31.0913302592, 28.6325182999, 21.3675622074,
19.8683168127, 16.7434591095, 25.7889716249, 23.1176480413, 14.6582482046,
19.4972338798, 12.7874192932, 6.7703964531, 27.4707322955, 19.2028385813,
18.1642498664, 17.7357049974, 10.0497394249, 22.914204646, 19.869125618,
16.9071172598, 25.685453022, 21.4008909069, 16.9994352498, 9.6505187569,
34.0348271283, 20.7460585938, 9.5063782929, 12.9554704872, 22.3102282738,
21.5104451006, 26.1206157222, 25.7825904103, 21.8947020568, 19.028744168,
20.6887816895, 17.0192075667, 25.1009879791, 21.1478022592, 15.8987547886,
30.5331218571, 22.5262381083, 17.077173912, 28.2674964708, 22.7769491966,
22.3251673425, 12.573975068, 20.261223036, 19.9705123046, 14.9461289789,
20.6126925222, 16.3374364513, 16.0991419747, 16.5283460577, 11.0635838975,
22.7684364859, 20.4821937873, 25.7660843164, 11.9033647565, 19.5368225691,
25.1064148923, 24.6135501757, 22.681591812, 20.6841778169, 21.436021435,
18.915269096, 15.4304562428, 24.9618677307, 26.7913857131, 27.991148714,
18.164435713, 27.710156923, 24.3076464091, 21.0631047828, 17.2238390308,
18.1781037302, 24.8001836822, 23.6635266953, 22.0321839367, 21.2527130894,
17.1666810965, 16.9215585769, 22.7648383998, 25.0934355263, 19.2764949804,
19.714498048, 12.7153045397, 19.3067382234, 19.6848235428, 22.411910332,
16.8429518157, 30.6986951876, 29.5900109054, 27.149227557, 19.9240178944,
22.7517434012, 22.6831780712, 17.9815194722, 15.530387942, 16.4302975578,
20.5016104647, 21.5103365143, 21.3499847496, 21.0573897707, 22.9125890182,
19.2574045601, 21.9918379841, 21.7522291275, 20.4563459235, 21.5379334157,
27.9312223133, 28.0118970805, 32.1760897068, 27.2394840801, 27.3398851049,
30.0989817709, 29.9597389129, 27.8536272545, 22.8100344786, 33.4419593327,
14.6797315711, 19.3822303233, 17.9160558155, 23.8612924883, 33.7414426047,
12.5101295329, 18.2815664576, 16.2249236795, 23.2539296391, 27.08208951,
17.994527649, 25.5377103023, 23.412853705, 15.4333721953, 16.8624188354,
17.842685223, 26.5596678488, 34.538480212, 29.1096622348, 9.7070409495,
12.359288914, 20.6653295107, 26.4319931202, 9.4648769397, 13.7319444526,
22.4366822506, 19.0151596015, 14.2842991032, 22.6594644548, 19.2216387593,
24.2779491954, 26.3381100707, 26.3529617292, 21.9590361582, 11.9301442468,
16.210772305, 17.6697726564, 21.635310126, 19.8507124603, 19.1098905841,
25.0804398035, 20.5828862176, 10.4492137197, 20.3899873627, 10.7342676078,
12.4616991579, 21.2303941815, 21.347151889, 21.9247709604, 19.8706104289,
22.990356186, 29.1415595466, 23.3553083833, 23.664489518, 20.9333468403,
17.099028971, 11.7629299489, 24.7924792845, 31.712609711, 25.187813728,
11.828163035, 16.2750055528, 29.9881061103, 29.0697256432, 20.9565656167,
21.6020608117, 14.3409483904, 23.7122690934, 27.6216150929, 20.0052583731,
18.1252254544, 31.9040896672, 15.8116024156, 19.6990489782, 23.7958354192,
26.0454222053, 25.5784156654, 25.380909777, 18.3169367646, 6.6807876054,
8.6072270322, 19.0619237585, 25.3829240155, 30.3174159645, 24.0268900599,
24.2925989723, 18.9848567722, 22.7168611646, 19.885891299, 21.9134123218,
19.4786562161, 15.5085262471, 22.6114716821, 27.9356971111, 24.8900523113,
26.8724883014, 22.9840310111, 16.5746320249, 26.0844653197, 29.2371776832,
21.984930991, 21.5911887118, 18.5444380284, 19.4966372502, 14.1925039487,
25.4450675284, 18.8371662574, 23.5801348974, 18.9066025347, 21.0230823245,
23.1408755903, 26.6223699779, 28.6527564976, 20.6196669766, 19.2271383613,
23.0981946116, 26.5110627894, 11.3815921688, 13.0239248362, 16.5571435754,
16.6337259611, 22.6329069225, 26.0025215562, 23.4073418993, 18.4679109883,
18.5403307775, 24.8667529785, 21.9868053013, 10.093343142, 12.385483159,
16.9645953488, 21.4916781729, 27.717020278, 22.037322826, 15.2875833732,
17.1695359375, 15.8247912952, 16.6608133978, 12.4298537234, 14.7166581677,
18.0725455771, 21.7522053427, 19.0205038733, 25.4330953717, 17.2934482469,
21.8017780627, 20.340115797, 17.3244451576, 18.7850421749, 27.8106981439,
21.3813895, 21.9327793229, 25.3900834467, 19.3297380105, 14.0146870763,
20.3302547394, 23.0455558423, 26.946895476, 25.0135184346, 25.0177349264,
19.2423572096, 24.0046985709, 19.6567287154, 25.4235341683, 26.7392247475,
24.424218982, 23.661404648, 28.2815295221, 20.0212367082, 18.1903249625,
19.3291945703, 19.8924748417, 16.6661873666, 22.7914028806, 26.0794788736,
16.1898907758, 8.3841116997, 11.2685262029, 23.4935825504, 24.6250055706,
21.2779498938, 26.026094551, 11.7125818915, 17.9539956599, 15.9246730346,
11.2099041609, 25.0037692678, 22.2118916875, 26.9423506359, 21.4379833957,
30.5744433634, 21.8500384048, 22.1960909197, 17.4821911812, 21.1124608765,
20.1789499819, 24.9258785512, 30.1548468057, 8.8923540327, 27.170295425,
25.4906878416, 29.7421029898, 24.7759943182, 22.2956551045, 14.926971741,
27.6176686037, 26.5158695068, 22.8798576724, 20.1623876917, 11.6319289654,
13.077039909, 20.6518884615, 20.5912033924, 23.9684619785, 19.2088036667,
15.2714649326, 11.743717055, 26.6438803995, 18.0527181333, 10.5038369838,
26.8228210512, 23.6566369131, 31.8413659065, 26.2287584585, 14.8520361488,
28.0549650565, 24.2422058371, 18.0485237745, 14.1419521339, 17.07939606,
17.1014932672, 25.2855804408, 6.0624519132, 28.3172643523, 22.136954991,
17.2743308707, 16.1271744746, 18.2795369828, 8.6187703383, 33.8887211807,
20.7308927258, 17.9400820461, 16.5910134897, 19.2161016716, 9.9318530273,
12.0186546601, 17.322934332, 21.6412126849, 17.2939795418, 14.6296146042,
12.12997252, 22.98812751, 12.3325975498, 13.2786755526, 11.4754078023,
17.1888991441, 12.0839710659, 22.9661939725, 20.8525542703, 12.6823969735,
22.2060367477, 21.4489163716, 18.2785100801, 19.5445047314, 18.2156063816,
21.1256310827, 22.4834658863, 17.9759605313, 19.2650991945, 18.8644336121,
18.3460050397, 21.1296937014, 15.0854911956, 11.3902170964, 15.7867604652,
24.6654163156, 24.8325393688, 19.548984198, 22.2935292638, 21.8621157841,
23.6602657672, 22.932887748, 17.2665802318, 18.9717396738, 12.963261477,
16.5557777665, 11.2452433199, 13.6262059045, 16.2754909933, 11.7001234389,
9.0018163639, 5.2124060767, 18.3023430186, 17.7844163619, 18.4484049918,
20.0943654715, 19.7336428067, 21.1112533264, 21.1469247377, 24.6305589409,
24.6144058736, 11.6706139053, 19.1451461446, 15.8256714921, 26.7154399005,
24.5838224303, 24.343032421, 20.555441828, 22.6144909405, 14.7835047156,
21.1159102342, 20.4864759681, 18.1185186725, 18.1185186725, 18.8202199583,
17.2953297007, 18.1680358221, 16.1648562166, 10.1367660628, 20.0586763173,
19.2775900744, 14.4382317611, 18.8597630659, 9.0443830236, 27.6019400208,
17.470677301, 10.6887696377, 16.2539811146, 21.51398125, 26.929526207,
30.145154655, 12.0322292248, 13.9943392348, 26.7963988217, 28.650077249,
14.2878447028, 20.1756520887, 18.5651408085, 21.5709427319, 14.9895344937,
22.6421669769, 25.5607956252, 14.1417951224, 10.5549086939, 11.9319869365,
13.7747764678, 5.2001686953, 6.9610232456, 2.8720904369, 18.8679393864,
20.7715465099, 18.8056944716, 21.22735421, 20.1272167327, 18.0068370898,
20.4527431765, 21.7273842109, 25.4697863121, 28.3837274492, 16.5356476753,
23.972579296, 26.6066108635, 19.5352386927, 23.7821700438, 13.4151291078,
12.8016286186, 8.8111838177, 22.0318741837, 20.6607787394, 19.160865139,
26.7001690706, 16.413154017, 24.3848697088, 14.4714592473, 22.5095453082,
17.1839283213, 9.8815114012, 24.9095022083, 7.0113168677, 11.8235059732,
22.2106684954, 25.0864564618, 9.119423716, 13.2027381535, 3.5896737794,
12.4356110617, 20.4012158069, 16.189911551, 25.7389054879, 11.0298960095,
9.502599071, 14.4707638152, 23.8565159875, 13.1721447751, 20.5526546316,
20.9449125044, 21.2691103714, 26.871950632, 11.4741700849, 23.0947888192,
9.8070058939, 13.5668984609, 19.9030201705, 17.4901998433, 19.2310883858,
18.1185186725, 23.119702281, 30.2438822193, 14.6697696245, 19.1682368715,
28.4122562361, 29.3042419812, 16.0628771786, 18.5596044661, 18.1959987643,
20.1364851556, 14.5953517788, 17.5044208368, 20.3413149874, 18.3178277463,
20.9318638027, 23.8622496485, 22.7954162953, 18.3351429489, 17.2605967138,
25.8900729561, 22.4431015073, 34.8696026284, 28.8310673783, 27.1846432099,
18.7070368596, 29.892424521, 15.2395410295, 20.7954285518, 18.1173135775,
21.7403440864, 10.4846409768, 13.5896737647, 8.8258487384, 20.3679162184,
21.975424016, 21.5150660478, 22.1727367599, 23.8343976601, 28.4965909259,
18.8130623363, 17.7433383064, 16.6218192307, 18.1340709796, 16.9919484657,
16.9289180836, 20.170887578, 21.9832410917, 15.8295047367, 14.5891033733,
15.9147289627, 18.7414788975, 20.8675154646, 20.7626464343, 12.0641521343,
13.2129350833, 14.7783856633, 18.7914618057, 17.0792419928, 21.2370129773,
18.8028166463, 18.7527266673, 14.2119291535, 13.6963383419, 19.1361428638,
7.8551909713, 4.1175194209, 30.9462226096, 19.248914978, 21.3727550322,
26.2974349755, 6.247775443, 6.8982261233, 14.7558478028, 21.426687184,
23.331101484, 29.5150675876, 29.3741658494, 27.003664468, 17.4455218823,
17.216185068, 22.3660169996, 21.4600428031, 21.898334314, 27.4541949144,
28.6190604933, 23.862371409, 21.8146899707, 16.8463991472, 12.2117028487,
16.2053230888, 20.392863412, 16.3067022757, 12.524152561, 18.8484400527,
15.3567241219, 24.0658926774, 14.7891169208, 18.9212703893, 9.7705651596,
8.2117838034, 15.9069360027, 11.5012365005, 20.2471891187, 14.8291370851,
25.1984343985, 27.1232319936, 24.94327195, 26.1984480901, 15.5806799997,
10.5919062348, 14.3523670905, 15.4298707701, 26.7043776858, 6.4558287104,
18.2319289654, 25.0822634635, 16.503972237, 31.0611057222, 16.1303344056,
11.0535907622, 17.5917044891, 14.2194386005, 21.4106381535, 18.6076419439,
20.0607800931, 18.8192921774, 15.4920899303, 23.5803689153, 24.7684602185,
6.4669968085, 22.5258090386, 14.491025752, 13.1654079785, 17.7471689754,
14.7311263581, 14.7191240486, 18.8787927574, 29.6637555506, 24.877646269,
19.1267223992, 17.0372028808, 20.0848292736, 21.1613446835, 20.3456370473,
23.2677843192, 19.6978169765, 16.2968322189, 19.496706708, 21.9074782213,
21.9148261572, 20.2268516815, 12.748560717, 13.3235909302, 22.1722190845,
19.1048708309, 24.7725846086, 30.2071416694, 13.0620419226, 23.2046879059,
21.4795516301, 20.2713176895, 20.7712164166, 18.6991020107, 15.1753259236,
13.6226396275, 14.7828656023, 18.7753080268, 18.7937785761, 17.6253360198,
16.3255709836, 25.6697772552, 24.1596826027, 22.1618396705, 21.1663653519,
18.6461549394, 21.632170153, 23.4310794133, 21.09729479, 22.379139806,
14.9165058202, 18.6854961464, 20.0785689186, 32.1206730424, 21.7843732956,
12.0566012921, 14.9533954506, 17.3707829118, 17.1430708939, 20.2129106533,
23.3170996468, 18.3630223049, 21.3690621951, 21.7863249204, 19.2548481465,
16.4944491713, 7.9598645761, 23.4999501716, 17.5819238943, 19.8474294023,
12.9504068118, 7.9577983012, 22.7318348262, 24.126028381, 20.8282160437,
27.8424377934, 26.5527124395, 11.7438103041, 16.4918763256, 17.6631743699,
30.2863830726, 27.533808534, 5.4356111974, 20.3178290464, 20.3730402542,
20.2752775453, 19.6024557167, 22.4072780422, 19.598554398, 19.4121701666,
20.5647300476, 21.0890630921, 14.9574587599, 22.2187424996, 20.3456441418,
24.9031994619, 26.0596952019, 18.8773661559, 19.7549983274, 17.4511626094,
18.6833566289, 15.8468041029, 16.4004007831, 22.5059979245, 22.3104012306,
24.7496534893, 20.0848292736, 11.5672079837, 13.607345147, 19.3414195218,
19.7923417401, 19.8791997992, 17.8781921951, 23.0547382346, 18.0837846225,
23.8901579561, 18.9954254595, 16.9487777003, 15.7812863862, 18.9202964431,
14.0558363966, 14.1536069367, 18.3993635269, 15.9152179843, 19.94247098,
26.1792717959, 6.1362616143, 21.1949625777, 14.2712183684, 20.6624721303,
15.8104944666, 26.1363967085, 25.0381355864, 19.0468679928, 18.4680512066,
20.4507027225, 19.2271907195, 22.4173563643, 12.500568014, 11.5858251663,
24.7129710714, 29.0558429623, 29.3072797292, 29.7776039488, 20.5602009738,
19.5047092285, 11.6606376292, 13.1752571151, 5.7074385199, 27.3480156173,
22.1809997172, 7.9410195547, 12.4489539435, 18.031754053, 16.9224085255,
13.9298193303, 15.5145135241, 22.4232382795, 16.6204948581, 17.2793335003,
10.3842580045, 13.7765438297, 20.3005563085, 11.3814529047, 26.8533388277,
25.0509223957, 13.284352338, 15.9613801659, 5.4188195222, 17.8166775922,
18.9555560998, 14.6712166236, 17.1462717609, 18.9870363455, 18.8339885003,
9.3312558327, 16.9062910482, 24.7475783542, 23.7835158903, 19.682577635,
19.582695357, 25.3431702761, 14.9913744386, 15.1643540982, 18.0313559552,
18.225908892, 20.3046439888, 21.4798856371, 22.4830869795, 17.5131625106,
21.0985862917, 14.101199583, 21.3841174558, 20.2145029843, 15.1699274052,
15.4238358443, 17.3985810246, 41.460885885, 12.5985242321, 24.6546516433,
16.3484241514, 18.9840017495, 21.0844618877, 24.7640217154, 17.1254849876,
18.0079180024, 18.0995323973, 27.1930116573, 16.6619564798, 17.245682458,
23.3714420832, 14.5782447772, 18.0199917606, 17.767796712, 19.9631403802,
19.1682368715, 8.0227114655, 21.6311651866, 19.3662892963, 22.1047700877,
22.8549184723, 15.7831911412, 11.7680775868, 21.066616261, 18.4306270716,
12.2191033308, 5.7527122887, 21.109088469, 22.1854598905, 11.9738590934,
15.6408977678, 18.0618393767, 18.3474519421, 12.2721474143, 25.4768712324,
19.6961069755, 21.1505053883, 15.9737836955, 17.7563304321, 9.2327106597,
15.7792857305, 18.5472625669, 19.5721949399, 20.4723516705, 21.9561570249,
13.3027822491, 17.9213842116, 22.1577445016, 21.6331631433, 18.9236698753,
19.453549585, 20.7897504144, 24.3226049896, 12.2699095748, 17.6009303986,
26.8966110516, 6.8541268278, 8.7279366574, 19.0790141108, 17.1769313603,
20.6575807786, 24.7157089074, 18.8701812102, 14.8012869464, 26.1611522467,
9.0244054175, 30.4418489181, 16.3936109212, 18.8508849766, 21.084418787,
23.1798589618, 26.2802538376, 19.9403780807, 15.6505586719, 20.6549888946,
19.5030936153, 18.0218754382, 13.7827125894, 10.8216497615, 15.7124223653,
18.9309811753, 24.5820977491, 8.0869435168, 18.3227514832, 19.5857195008,
14.2324181745, 23.8520497731, 27.180235224, 5.7653659912, 4.1409544902,
11.8629973668, 7.6562730467, 28.9182761378, 18.6446152977, 15.8218894279,
19.513572722, 17.2095718034, 18.9076616145, 7.355773933, 4.4077743594,
21.1699339965, 29.2771310881, 20.2066756741, 12.9488924919, 24.0788748749,
14.6195564966, 13.5858591354, 10.3996655554, 21.268492258, 18.8157979057,
14.6517736918, 13.4590376673, 13.9424356757, 23.7958354192, 22.5979359966,
20.4758512206, 21.0275482924, 17.436791284, 14.1656868617, 20.9248787307,
17.5851325862, 12.8342245074, 18.6508016901, 21.825793476, 7.4279342024,
31.7789574037, 22.8720747688, 13.5551668746, 16.1933251068, 7.0474972276,
16.3147379258, 20.1767293187, 13.7834930542, 16.9611785051, 23.0746029881,
25.9065152793, 23.8413148811, 18.1212087751, 14.9976502243, 17.8088588301,
17.6125818191, 20.2630063539, 19.3692309589, 11.1005656746, 20.801740534,
22.6733299943, 22.3094970227, 30.7025041127, 33.1082574758, 19.5688141762,
14.6075522489, 19.9186823037, 20.6176621939, 11.805399906, 16.2476080896,
25.6972141151, 25.4130455979, 21.2130429911, 19.9186823037, 20.5853717709,
14.3294497901, 23.6463292146, 14.4203386303, 13.6080248691, 23.3298156368,
21.6338111135, 19.980008879, 20.5825588163, 22.914204646, 17.955963447,
16.4581488441, 20.6930610007, 30.016890732, 25.5105421789, 14.3178547985,
16.5824804445, 18.2137280107, 16.8020949838, 14.6506767608, 24.052377211,
23.2213339123, 15.1934382702, 14.7814833491, 13.5027666344, 10.5707590257,
14.8468227799, 16.6006785678, 20.2094617651, 20.9893206065, 16.6440719337,
21.6478821828, 24.7057900141, 19.8284158596, 19.5075215096, 28.2043103769,
19.8012751735, 21.74454376, 26.8725328775, 28.3961012638, 12.0150622387,
14.2056177742, 23.2709608476, 29.0243919026, 21.0492232073, 23.3571965568,
24.5664275966, 18.9549821175, 21.8415630438, 17.8550268101, 19.189982947,
7.3327260675, 14.2014255493, 21.3109849003, 9.1252264166, 16.4477732861,
20.994165911, 11.3865256888, 13.4873398948, 9.79337837, 16.3597602738,
15.2028439984, 19.9582019056, 23.6684997236, 9.2128193896, 26.7251199208,
6.1372318789, 10.1101990914, 17.9237369165, 18.1846303776, 16.9790277265,
17.4643534095, 19.9886010451, 11.0255384905, 10.8534951676, 8.1198984292,
15.4341163516, 13.7244587876, 14.6832566079, 21.5811327292, 13.0142534433,
18.5269774859, 10.7915244252, 14.6886947151, 16.2690292405, 17.961871635,
23.2148431009, 10.8486670641, 12.0871426111, 22.6649335116, 23.0128859747,
25.1727890356, 27.2216262767, 16.8854428472, 10.2782672517, 13.7373746477,
15.1262430262, 21.7752361763, 22.102397498, 27.7654877499, 26.1371195729,
24.5729964163, 23.1373394756, 23.8714957877, 19.4603748652, 17.1270608601,
19.2659900517, 9.40707963, 20.9313293436, 23.6330836741, 22.2580873573,
17.53497358, 18.7662945359, 12.0622236089, 12.3115675486, 18.1224346336,
21.7964966761, 19.841988645, 19.8014484778, 21.6268415357, 14.5912255906,
12.4874119946, 13.3736500365, 20.1360017407, 20.7315644901, 22.3815665978,
24.0260950713, 20.4653047864, 18.5447673702, 21.9651750088, 23.1266102363,
22.0368544676, 21.454385869, 22.6671966016, 18.9109579352, 28.5511593477,
17.564169552, 21.9545118025, 15.264591785, 22.1645654734, 19.4068041946,
15.9960599334, 19.1019359123, 21.831550474, 18.0524111886, 21.3379314523,
28.4972660758, 18.684551649, 14.8522314179, 20.3234040435, 18.3183037551,
17.8796069198, 16.8831970303, 19.8195469598, 16.7098038711, 16.5817730114,
20.0848292736, 13.6850409937, 17.5857828804, 17.1590282186, 20.2305231511,
27.7261774552, 31.2195559468, 15.3845947842, 15.949574259, 18.8832317941,
16.3484241514, 14.70133073, 15.2560761795, 18.1764037621, 20.5192436595,
17.890512304, 18.356494982, 15.1405316799, 21.7401507743, 18.1049803772,
20.4210714823, 16.4132365117, 10.5253481153, 21.074834273, 23.2839735551,
23.0704221098, 15.9277988653, 30.2665506426, 27.9008940442, 36.3960598419,
22.9304029672, 13.2485871817, 21.3060612714, 20.7059293116, 22.4496191926,
20.9353089691, 19.1584733895, 17.3097764106, 16.6011730163, 17.9047216496,
17.5056259562, 22.038669802, 20.7795000926, 21.4522845272, 23.8550156655,
34.5842975077, 13.7030721144, 15.4335298107, 14.4928807571, 7.3877623243,
19.146346527, 16.0961208998, 13.9772157225, 21.7228325258, 12.0457475489,
19.3512584643, 22.6679278731, 20.8120131488, 19.1329505622, 20.9595411947,
20.7949636585, 21.7401507743, 22.7697217052, 17.918347724, 19.7754390033,
17.8292532632, 21.5563037894, 24.3596051898, 21.086233433, 19.5559696292,
19.6874566905, 15.5151206015, 6.6186229926, 9.3734225031, 10.6463525552,
13.867652823, 18.939680307, 29.6918245716, 33.8887211807, 5.0605039181,
20.789401718, 10.7808734027, 6.2981405461, 20.6445099868, 16.8637501279,
20.7436735333, 10.8388316173, 21.9149041667, 10.1520793505, 9.166766436,
15.7067793055, 33.4619788826, 13.7577102919, 20.8193406579, 21.1287944525,
4.4026971583, 13.8218977191, 22.0905577154, 22.0105932445, 12.7537268596,
18.7786292259, 18.8551711975, 12.4941795312, 5.5067553466, 6.4872040284,
12.6821100952, 14.5761033521, 12.4868119427, 22.3435796461, 8.9958897222,
13.4166775922, 18.0691867169, 17.2777059673, 16.7854930328, 20.7896513702,
15.3418300562, 23.9036827763, 13.8689953977, 24.6300213882, 14.899502618,
9.9832741092, 20.6034816028, 23.2346993554, 17.5820860426, 19.4603748652,
17.3496664147, 16.4592595841, 16.7901546241, 17.5079044959, 22.1874346155,
20.9038084425, 22.0061820054, 18.8375351578, 22.3048651775, 17.2001847322,
13.5668984609, 15.1101084393, 18.1644394302, 18.8484400527, 11.098689635,
14.1255278192, 17.9166997923, 7.9041688195, 21.7377063244, 15.4092949647,
19.4316620828, 19.7923417401, 14.3144159819, 25.216685338, 18.354503324,
26.492355156, 36.4715195803, 29.6361383318, 21.0914841706, 16.397606641,
26.090678588, 21.8534083738, 21.0857194785, 20.0817367516, 10.887429597,
23.2596205575, 19.6228370611, 14.0515215811, 24.5780401058, 30.1336966058,
23.9896406861, 25.3047478733, 10.7222994024, 24.1757067401, 22.7837587839,
17.444944026, 22.5021484914, 21.6478821828, 19.8343706371, 18.5217227861,
19.8284158596, 24.7865272842, 25.5466645682, 25.061474392, 28.1379996264,
25.9646539911, 24.2877693004, 24.332555472, 22.3196399656, 23.9220016469,
25.4722658378, 24.2468508464, 32.655124237, 24.5221892485, 29.6338240123,
19.7328669239, 17.3985810246, 20.4243639899, 18.0323088611, 21.5855730897,
22.1584351302, 20.1678953037, 19.7661419559, 21.9472343896, 20.4363425231,
18.585005842, 21.0058473002, 13.0093904761, 18.6820384068, 21.2594241069,
22.786633917, 17.7803406585, 16.7562321609, 15.4828110903, 7.5571964846,
12.6795791748, 21.3645432492, 21.2336329901, 10.8244210433, 21.03675048,
17.505445904, 19.9566841494, 9.9592572231, 12.5706711871, 22.8245251941,
11.6697057928, 6.7645320585, 13.3852419901, 15.7138941068, 22.4401830711,
22.5372321389, 21.3443029598, 23.5382301347, 23.3693197323, 22.3373411606,
21.1176163042, 18.196657606, 20.0177161169, 22.4461062917, 12.7461746318,
15.22812204, 7.8651757017, 4.9168542475, 15.3436904654, 19.2020198807,
18.673591614, 21.21010336, 9.8815114012, 10.5032315424, 10.6635134449,
13.8522663442, 29.3447106314, 20.2700737577, 22.9552683459, 22.2126345252,
18.4960241687, 26.5204994963, 19.5315975915, 21.3170400792, 24.7827863232,
28.0379395404, 30.1871047039, 19.3079684627, 14.2611482429, 10.9870001193,
9.0855360466, 19.1356661837, 23.2303258184, 10.5638959729, 22.9942182209,
13.5840992785, 14.2677990002, 25.0072792754, 15.4123018726, 16.8022466568,
7.9032443743, 6.2797679825, 10.7800273918, 15.6858710946, 24.3531335158,
22.1412178087, 25.4330953717, 20.7866389566, 24.9655293483, 22.3949709488,
20.0240921435, 26.835797615, 26.5938241013, 23.9567022878, 24.0587657333,
28.0627101858, 18.9405164853, 16.2435614662, 22.4163898337, 23.0287690734,
26.7878966058, 27.4572051664, 28.4122562361, 19.3087832987, 26.5150603246,
17.4672579249, 17.3081903925, 21.5590734698, 27.3618229407, 30.2848962769,
17.5083412592, 17.0333914658, 17.9091399672, 19.1520278621, 18.4434508104,
15.817426421, 18.0821551359, 24.0586277697, 22.7119026072, 26.4749641306,
25.5351358519, 32.571916995, 13.2370663408, 10.6583243751, 9.2212195968,
18.5533490647, 21.3090507258, 15.9774927349, 18.7792478265, 7.0771516892,
25.4199917308, 12.6818614936, 16.279430428, 18.9748048988, 17.4714304542,
14.3793394183, 24.4517479482, 28.0676686331, 24.985705588, 23.0527174642,
27.2521867172, 13.3189614659, 21.1028388468, 17.8410574406, 12.2514658174,
19.4297935438, 21.1920964504, 22.9041930325, 16.4394195729, 22.2288847632,
15.6222827328, 14.2666714091, 23.6825420215, 20.8052815063, 21.151386781,
20.7896565801, 26.0568437229, 17.3951242915, 18.8466217961, 21.8020081969,
18.1777012364, 21.2317434764, 19.8654464825, 15.4981162305, 21.0987510751,
17.368691704, 19.1843893588, 17.6006407136, 18.3412724312, 19.4016143361,
23.3903520469, 21.0327889682, 12.2526183423, 13.2026451962, 19.7858078925,
23.002622521, 15.049841431, 16.4037648301, 24.6628070782, 12.6091336788,
8.1773813771, 24.9933141964, 17.9108094792, 21.0668350878, 13.4950516867,
21.4014340729, 28.7474822141, 6.7813788963, 12.4280195485, 21.1234751225,
7.1766778458, 8.1712637544, 22.739170317, 27.6568682529, 23.1259412033,
23.2620489852, 17.0758801578, 11.2895006351, 26.7481750041, 21.7025589692,
21.0933231897, 23.6961518209, 15.3437592593, 17.1028943206, 26.3950843499,
29.6250422907, 14.5208295037, 17.0790988569, 18.085732297, 14.5393060053,
33.9100117106, 25.9072085709, 22.5408803089, 22.5151842226, 23.0109077552,
18.152268919, 22.2983325071, 27.8852092774, 11.8907371126, 16.0172961705,
11.1698338067, 11.3023039622, 9.7902636241, 15.7067793055, 4.4027655517,
12.8645495514, 11.9071178376, 12.9222850866, 9.9848277448, 7.841473724,
11.156171198, 19.1539325875, 17.6300401864, 9.6703503531, 11.8672240334,
14.3373110468, 19.7149719916, 17.0844079516, 9.0367660628, 13.0481204858,
24.8763931834, 16.7194460151, 13.815964304, 26.4102088876, 20.2305231511,
20.5439620171, 28.6189795242, 14.8682220472, 27.8664557158, 18.8584784436,
24.0169106793, 14.6804597641, 21.1162082702, 19.7961829771, 19.6582634624,
18.5830187423, 25.1469459277, 21.1539334835, 16.6777679217, 7.9136122483,
22.6358437184, 21.0943322887, 22.9309489187, 24.307465398, 16.5307137234,
17.2269319368, 24.3867122044, 26.5035292942, 25.9296512328, 23.295520035,
19.6487091864, 23.6970120811, 26.9782546123, 22.8593302546, 27.998368687,
25.106409164, 25.9296512328, 21.8415630438, 17.5371427111, 22.5965206871,
29.5604095744, 25.3417553338, 23.3871777338, 21.6068193136, 13.6536002036,
18.1843425949, 24.1144062955, 26.4930794172, 20.495243393, 25.2592051712,
22.6125283113, 12.0574519365, 18.0452710525, 22.8393640617, 22.0392557051,
8.0729605961, 16.9058622424, 19.5378399244, 22.4820079723, 6.9834098578,
15.4382808979, 29.8168403864, 24.380757352, 19.6228370611, 13.1033163509,
4.5612409889, 15.270073467, 17.6201383656, 17.8570555439, 21.1372305472,
23.7500772785, 21.8013534223, 21.2317714235, 22.6046306805, 25.7351459532,
17.1666810965, 22.8024193457, 38.5402482564, 25.5778761236, 24.7206940679,
14.2925713197, 18.6078691233, 26.3350838795, 25.926934835, 19.2935806676,
15.9435128272, 13.7249310646, 13.9707430051, 20.8241385832, 23.2800300927,
13.453560274, 16.6172319622, 18.5627377343, 26.3565936608, 14.5373865891,
24.6191240486, 21.6736430126, 23.3320799016, 15.0683171939, 18.3925413884,
7.7244809642, 14.0644702834, 25.4009059137, 23.9186472848, 23.2078382281,
10.5053545145, 26.0708199247, 20.6933320614, 14.4108885852, 19.8772693829,
17.2994842486, 17.0734807079, 23.7054138633, 9.430846476, 7.8556169524,
11.4082626251, 13.9707430051, 19.6102411424, 22.9999056305, 23.3928203047,
19.4618442579, 21.446243453, 18.3604159118, 19.9618247868, 24.5713878854,
23.1670250364, 26.3143728661, 13.9906776172, 16.2918384429, 10.4292708734,
19.0847895876, 11.4301356911, 11.6706452183, 10.2555000581, 15.0003138611,
21.516716568, 19.2829399969, 18.0131688288, 29.1036493835, 20.5134483094,
14.7558544112, 12.7607961728, 4.8944901376, 7.5447882559, 20.4768365485,
25.4117989637, 5.4413887255, 28.4485241399, 27.4205775372, 17.4355382811,
24.26199166, 16.4124959557, 24.1306094805, 8.0505996489, 17.0317619003,
31.8766558743, 24.0830502988, 22.0583188094, 5.9942507102, 24.8896740337,
18.6040209974, 27.5482618352, 24.2010886457, 23.311655064, 20.5529215901,
24.3035348236, 26.7850004394, 34.9020377974, 15.8301627306, 17.4085500226,
19.6066120326, 25.4858242382, 16.3094544094, 20.9519395404, 26.6977506889,
29.8504624883, 21.084129189, 18.2129183406, 11.3856356145, 12.9811628314,
19.3338915021, 19.3964562337, 5.3936779623, 10.8622722836, 20.2627866689,
14.9461289789, 27.8832034285, 31.3319326469, 24.037390059, 21.0891780563,
32.1056065436, 14.3570561524, 6.0155207415, 14.8788439002, 5.6626930263,
13.4117924264, 31.0090030191, 17.0603991088, 12.4124933295, 19.7135126991,
14.7285969457, 16.7495228832, 23.6377632247, 22.1025266318, 16.3437209498,
14.3645663287, 11.36476431, 10.2793315376, 22.5285349309, 21.0667112282,
15.4182522597, 16.4030309342, 15.2576139209, 18.8343937494, 20.581337821,
17.3446494409, 19.8014484778, 27.2243702369, 18.043905294, 19.0047719616,
19.9861392327, 25.7945066568, 25.1238259843, 26.9765957982, 22.5067748559,
20.0813534243, 16.4880512848, 13.4518945831, 16.7899349391, 22.7301840263,
16.8611670043, 14.9358583981, 20.3483120381, 23.3372443306, 22.5464576348,
24.6677111543, 25.1623244324, 28.81503545, 14.9887716743, 20.9919044993,
22.8826075022, 27.9155168333, 26.8031165579, 17.970020953, 9.1900567098,
14.1134224651, 10.263062044, 7.7671046523, 19.0941221893, 20.5413116087,
21.4484799702, 18.8986746597, 13.843792026, 18.1115688204, 11.6371366554,
25.3393458265, 18.6431816708, 22.6828248195, 17.2054352326, 19.2935806676,
10.1511432307, 24.7323646179, 28.2871722354, 14.8404388819, 17.8352040091,
17.54781012, 20.2743314613, 9.8724238634, 21.6154706544, 25.4009059137,
24.7462135739, 17.7581130929, 15.0323331792, 5.5035566567, 8.6000152983,
17.3220114293, 20.7794068507, 17.5476501886, 14.7281957231, 24.9862650083,
23.5036570906, 14.0313095201, 15.9987948507, 20.5015416533, 16.4626663171,
21.813637379, 22.8539444804, 28.0125461295, 25.0845784773, 25.195167055,
19.6697852443, 13.3363620404, 19.0659830386, 7.8618982085, 16.1954787869,
29.8195832334, 28.4922665453, 21.1243135101, 20.6843661858, 24.0783044336,
23.9585869071, 19.8358689539, 20.6589895578, 18.5620183415, 16.0654176659,
14.5338976159, 15.4050672215, 10.3444698937, 19.8772693829, 20.7368580286,
16.7241803011, 20.7137738532, 18.3605577034, 14.6721211049, 9.0658357712,
15.0007909101, 22.9316080893, 5.5089717402, 26.0661810101, 16.7075953995,
13.9255086866, 16.0055881038, 13.962932178, 27.6146689019, 26.0140813129,
22.9473974039, 19.6658864643, 28.405886529, 19.5441964801, 26.1807972024,
28.744931259, 12.7067627661, 17.2966816416, 18.1001051092, 18.499001243,
19.6322682222, 24.1728605068, 21.5595985768, 21.2779073702, 26.9559593183,
21.086156564, 26.7731400584, 19.3374272519, 21.242000049, 23.782806645,
26.8711496666, 25.5696112294, 24.1367552506, 18.215373306, 27.4633113082,
30.53605703, 22.4827414315, 22.892896476, 17.1948327962, 15.0754031693,
11.1483824771, 18.0338473637, 17.7462389976, 20.2265390524, 15.5282708648,
12.2330035915, 18.0327946167, 29.5363141893, 16.7758100998, 34.5640960446,
17.8387836687, 13.0831958968, 21.7146741547, 27.8037345387, 22.6828248195,
17.4896651868, 12.3968582754, 17.843090016, 20.6974301726, 23.4196815612,
18.7914872086, 19.4911132913, 17.2693795947, 20.7691648052, 19.7496957682,
23.8835043445, 24.8119157014, 18.7579914152, 22.1625091595, 33.4694300305,
21.8933547409, 18.0015517268, 28.1087255076, 21.138844844, 19.14257691,
27.5438183522, 19.885420031, 25.8618104065, 27.0864704952, 20.540892499,
16.5929814365, 17.4919888814, 29.2179887477, 17.6459132742, 23.9891342922,
24.0189895315, 27.6288612375, 7.1821411097, 12.6809685803, 18.8244084956,
29.0817672259, 30.1002118628, 21.6666838309, 31.7537683909, 23.3739137567,
17.2138077841, 18.3731912849, 12.5390434763, 29.9628327461, 13.0867321493,
15.5721065324, 15.0898736647, 19.2055020432, 17.2016624055, 10.0463534292,
11.3757408238, 22.8024193457, 23.9840511709, 25.4766979716, 31.5286083122,
31.6064322419, 14.4118813332, 25.5324438184, 18.3558472616, 23.8553875489,
22.7048024296, 11.8875291753, 16.9813354249, 15.9792285065, 19.0432117673,
26.5437086841, 21.1359623413, 20.8424613671, 14.8215396233, 14.0733675687,
18.0872599673, 4.7236464771, 16.6848164799, 31.5756532345, 27.5468117988,
23.5886448065, 18.3351608782, 17.0370139694, 25.0087727983, 12.2724686927,
20.8912175609, 25.5730429774, 21.8827250856, 23.1742577499, 18.5580425893,
15.9484761663, 19.3058841649, 19.4617173728, 16.4696810797, 21.8318074553,
16.8991182919, 6.9801565521, 11.4121680393, 24.273228038, 27.2294420868,
15.5804743895, 21.9965673586, 29.6559627799, 17.3138049922, 22.7962189248,
11.823346043, 11.172370433, 15.784185893, 16.8461153559, 13.1760987089,
22.9419325146, 28.452996316, 26.5311227823, 22.9433468593, 27.9174645309,
30.009664564, 20.0410456495, 23.9760971008, 30.8263955062, 29.478561016,
33.013835352, 27.3227952212, 16.1093102993, 25.7115005557, 16.3437209498,
17.9575532648, 6.9010077395, 26.138126369, 12.5547416094, 12.9991019748,
12.2090180217, 14.5793991356, 19.0195571598, 19.2385828171, 22.9342645039,
23.3386486132, 24.0208607432, 10.723346043, 8.2122124501, 16.4584329284,
12.923346043, 24.8155607939, 24.828813202, 17.3552219652, 22.0112238148,
30.1353343303, 27.1020038078, 23.9036827763, 26.4418835995, 26.9725185624,
13.7919344828, 18.8701812102, 25.4244695388, 21.9687063421, 34.9934610604,
19.6687741809, 21.3407183143, 34.4350053012, 11.8840591584, 11.9720301377,
11.0640405941, 21.5718118994, 28.3835212231, 25.6093045282, 21.4818940823,
20.3929004728, 22.5408297913, 23.2113377143, 29.2816197783, 33.1966422507,
35.972888436, 30.1130898639, 32.1760897068, 28.58895278, 33.3758053753,
23.4369702214, 19.6868172961, 27.508741293, 18.997711157, 15.8279829809,
17.6448595457, 18.8377433018, 33.3558720518, 15.1094047707, 26.0824572318,
17.6766992462, 15.4597921753, 22.015377066, 29.1776330172, 26.4613379868,
27.5695873408, 24.1408642616, 20.6607787394, 34.424768471, 19.9857282944,
4.2586526262, 15.8973111582, 25.8957706577, 28.3259678747, 21.6779616986,
24.2285935001, 25.2589391729, 14.7493453904, 18.7847388748, 20.0073695069,
19.2117668766, 15.5040101215, 14.5398820953, 16.1522432878, 27.0018292268,
24.1799782916, 23.0518123894, 12.7529029848, 14.5065293812, 20.6968301055,
25.6272541135, 21.2651965513, 25.2581812367, 19.623745622, 9.815171169,
20.1927881439, 15.2416600548, 16.4872561933, 19.652961819, 20.9016074692,
12.9412862945, 11.6839463081, 21.6985617388, 19.0997191603, 23.1320027298,
23.3669802915, 5.4914049033, 21.2422458326, 21.899414392, 11.945593975,
8.9666090959, 8.3696434994, 31.8304931988, 14.4933411473, 14.5344454528,
15.9298471988, 18.6112525579, 25.5229215509, 24.1970175131, 22.1274305527,
18.3148041792, 28.8360951573, 13.7502181235, 25.9393726349, 23.8108926335,
21.6870680005, 18.5513128506, 11.6403392596, 11.8394845243, 16.8336200212,
19.6868172961, 22.3518304722, 21.8038908385, 18.4915725774, 16.4191974988,
17.9236547722, 18.858889276, 15.4880127776, 21.8096359158, 23.6435107594,
30.0277404683, 22.323885117, 16.4167929631, 21.8505714645, 28.6549120332,
25.640563979, 21.393333096, 20.1957759385, 12.7569322262, 7.4545306583,
21.5258564428, 17.227316605, 22.5773985452, 23.1370905895, 31.6143094453,
25.6986935591, 10.6887696377, 17.2655672315, 28.4744006551, 24.2468508464,
29.8623134869, 25.4364929808, 13.530617086, 11.2472024674, 12.6690233975,
20.5825588163, 21.3140687844, 27.3268396819, 20.7914982969, 24.4409582932,
27.110448096, 19.4552029672, 23.1611203132, 19.9816189648, 23.5897183299,
23.2868646928, 21.4858855924, 30.9479343159, 16.213538645, 14.2405277504,
16.017718099, 19.8762465179, 16.5993817135, 25.5369543625, 13.1564376363,
22.4080936595, 28.5777542271, 17.1627605801, 21.2238039818, 31.7403223759,
20.881009012, 13.1618642234, 16.9062212499, 18.160360228, 13.7204808921,
20.6638050052, 24.2120476541, 27.2815437616, 24.1529868093, 14.2639287131,
21.1865840018, 22.5625409517, 22.7850662331, 25.5543926235, 27.2934564555,
21.7243759078, 20.0115930359, 18.0323088611, 12.5790004263, 21.6285349431,
25.4956656035, 23.2636076402, 23.0580947465, 16.1572278261, 21.0731395254,
22.3264111832, 16.2097241543, 7.3925971572, 10.8141160623, 27.4272155988,
15.7807514366, 22.8872680869, 18.4478343671, 12.3774298664, 19.3948496633,
28.0766773393, 30.1994324916, 16.1039256949, 20.9554741905, 25.0741716039,
11, 12.0585179904, 27.2474336789, 22.6068525218, 23.7026346071,
12.6099189852, 11.6492126072, 22.6079684627, 13.384292477, 21.8138971228,
26.2968611089, 21.5474183393, 14.2383682149, 19.7837792276, 25.3996663833,
6.7558277626, 24.2575445846, 21.7779466169, 18.9281910168, 18.8035216071,
18.203830176, 23.2617143612, 25.3447752615, 6.5291772228, 15.0667931903,
24.2995814874, 21.6984023432, 17.8383523576, 14.1586433462, 18.398169056,
25.810358164, 22.568499172, 18.6324564446, 19.3826128182, 19.8439957612,
11.6086563102, 15.6430500209, 22.035211571, 22.1600954008, 25.3857476177,
12.0623549239, 27.2819965416, 23.2171777982, 12.2281821516, 10.1951920824,
22.0581168543, 14.1293880599, 24.6391670879, 18.3549144224, 16.7588135101,
19.5965362448, 19.4463813601, 21.8925933373, 18.859382846, 21.021934957,
14.291977019, 23.7137917419, 21.9608411794, 19.3615627265, 27.0393377451,
19.2206238158, 20.0928926779, 23.7489362949, 25.1672559566, 32.4639253077,
20.8428141743, 20.0115930359, 21.0053738895, 23.3078273092, 25.8238914881,
15.1527476727, 11.6411992877, 9.3852554658, 16.2317138767, 22.8102783351,
NA, 20.1396672106, 21.6270214779, 18.3604159118, 23.4935076768,
27.6463148058, 22.7569357448, 32.2893698983, 16.9224170726, 19.4035662484,
19.3953530564, 25.1663951722, 20.1699196685, 19.7919495918, 11.6508535939,
21.9617030415, 23.6073361038, 25.8219028545, 24.4856833799, 26.8066255369,
22.618968017, 19.3374272519, 19.714498048, 18.4509032043, 20.7059293116,
22.1815011978, 25.3781345147, 14.3413358128, 19.7572652979, 10.6666343567,
9.4841680167, 25.2662875026, 18.139998615, 21.2582369448, 15.6222827328,
22.9903019517, 20.5142887538, 24.9984532627, 23.2361765359, 15.5226716996,
17.1920122657, 15.5922891826, 16.0955441432, 16.5264499235, 19.0047719616,
19.5943151534, 19.6533706435, 22.0914524853, 25.4062716444, 28.1332084908,
15.396362823, 14.1977851279, 23.0782350358, 26.0040871704, 31.3349475819,
20.8023472694, 23.4925367877, 22.2098974739, 13.1664397225, 20.3024800629,
21.5524594538, 26.2131283085, 9.0738890484, 15.8913214815, 6.3415311857,
19.0555106611, 10.05657752, 11.7925971572, 22.2713187228, 16.4091564203,
24.5928948847, 20.2308884506, 26.2705611224, 22.8966581371, 23.2974059678,
24.9452557949, 22.9513249593, 23.1503505919, 10.0679501525, 8.1198984292,
26.6357802829, 9.7367171367, 23.3198048037, 21.5434586475, 12.5503923088,
17.6, 27.5900423985, 23.698283974, 11.3115963607, 19.2751149025,
17.4349995679, 13.6545095029, 14.7669122598, 15.7857938733, 10.395879582,
11.2367660628, 20.4264352965, 19.3414268765, 14.9076423516, 17.2418102521,
17.6694168195, 11.8986917325, 17.75951171, 20.9033572552, 23.5743535339,
22.4447918711, 21.9734756878, 28.9137077857, 16.0177126421, 16.7887360567,
8.7267690159, 5.1730540631, 15.0472961246, 22.1727744748, 7.0365305271,
NA, 18.3493281472, 22.4808641524, 19.6453490536, 10.2020858489,
15.8280075311, 8.8683757296, 16.1387097638, 18.749835343, 17.5781471685,
17.8417803752, 19.841988645, 14.1393048513, 26.6294346627, 22.4814553645,
21.2848826245, 17.533800026, 23.1564859731, 30.594537178, NA,
24.2185633326, 19.3027688957, 20.5184670422, 16.4208709037, 17.8088588301,
18.8342006176, 23.9179671618, 27.715130072, 25.061474392, 24.1273439734,
25.5601658576, 18.4267874095, 27.6279392606, 19.638072789, 17.2086205021,
14.4290743493, 18.1355350512, 25.9031913029, 20.6760151897, 21.8297493979,
27.7497942196, 13.5434793746, 15.4628317001, 29.8044441828, 22.1668223196,
13.8097026082, 14.6396290352, 18.372999275, 21.1230212046, 22.8569204511,
29.1821160492, 17.5612932969, 14.4242625894, 20.5463644262, 25.344834864,
18.874174278, 22.7297398683, 19.9497188378, 19.4351529642, 16.9605813304,
13.4212928112, 17.8810581661, 17.5099826604, 15.8981366597, 15.8256714921,
21.6426059947, 13.0284659047, 20.0114086925, 20.3034937625, 22.8156473917,
27.3466780078, 30.0962105392, 23.9560337193, 29.7097868556, 35.0036819309,
25.9250823958, 20.0699460317, 26.409928419, 24.1078580114, 29.8185792772,
34.4839714943, 14.9084253805, 28.8816324886, 11.7290488364, 16.56347936,
22.5601630156, 11, 10.1334956885, 18.9681067133, 17.9405002916,
20.6782103605, 23.3399576755, 23.0243793368, 21.0497799527, 19.163975093,
21.9744929316, 25.3668223139, 6.0122124501, 5.23098858, 13.0683945638,
25.6950080787, 22.3156907904, 23.5527672575, 23.1740449783, 11.751767929,
22.9449822764, 26.6276501568, 15.9938430469, 23.7413106606, 24.7621824197,
16.3907683785, 32.5890603057, 19.2679515523, 13.9634159993, 20.6725483772,
10.5790997309, 13.0749721543, 13.9871856416, 15.3924366531, 25.0650701539,
30.4452557949, 24.7538594194, 20.2984968694, 20.3636873408, 18.6316202128,
14.7670328917, 18.6557461505, 7.9436922922, 8.3836547487, 13.216167007,
16.0874549562, 20.5699042114, 18.0305154413, 18.1959987643, 20.398283974,
23.864899882, 19.8047644447, 19.4198664422, 14.3231383576, 25.3602863708,
11.0217720854, 7.7092853445, 30.5961189271, 18.374124405, 18.3268651829,
27.8418678243, 16.885609969, 21.8849242029, 15.4617246568, 19.6026543251,
21.0872153556, 16.6033443665, 17.2984659488, 22.8024193457, 26.4692188448,
4.1076185924, 23.2854047906, 26.1361819692, 19.977480462, 18.4825763216,
25.5562306133, 12.9050435457, 23.6676686331, 24.5974452413, 20.6355948895,
11.5485698818, 19.6102411424, 17.1613351899, 9.6606624235, 23.9144100394,
26.1880090389, 14.7766034909, 22.4372140735, 19.3567171831, 15.1317653238,
22.3082127167, 23.5521943282, 20.2545028339, 23.6676686331, 8.573457213,
28.394537178, 27.5294263116, 23.0327193074, 15.2340252228, 30.1233284234,
25.606807967, 26.7114762498, 31.757856873, 26.0121656926, 29.8601492374,
21.6604956484, 17.0460625835, 20.1082211042, 14.5978129441, 16.7932622082,
20.0034400812, 31.2296894514, 25.7266712049, 28.2906093305, 12.9582195674,
12.0395492861, 4.637283997, 13.449327587, 19.04748944, 21.4527016357,
11.0537281892, 9.9, 23.736318107, 27.1668367065, 20.3929004728,
32.2301138131, 24.2452811499, 14.5448680249, 18.7691372539, 19.1138054596,
21.0744556176, 20.8304477633, 14.3964922546, 14.6102228627, 12.7108691788,
22.9079571461, 19.5196817035, 23.3437708671, 23.7709874586, 23.9084685727,
26.0647538105, 15.9127804907, 6.2875584873, 19.1880775735, 23.8247759995,
21.6655672315, 23.3671102053, 28.7393834711, 41.513668807, 30.9627854233,
17.6641310218, 7.1163414064, 23.872790732, 19.7057257173, 21.9744929316,
19.9844163619, 21.0492027366, 21.6297398683, 20.0943654715, 14.4933411473,
16.0626708114, 24.1228579985, 25.3587299723, 23.9257191471, 23.3323422857,
21.4289319158, 20.6565765566, 22.1085330175, 19.405088952, 15.27948822,
21.4636146145, 20.0656905602, 20.9847114505, 27.7611579135, 28.443558379,
27.7586763173, 25.4384649918, 12.7989819946, 22.6552105599, 21.8626317887,
23.3367224377, 19.3703671269, 23.1611991468, 25.1311852045)), .Names =
c("X",
"Year", "Julian", "Trange", "HIrange"), class = "data.frame", row.names =
c(NA,
-4590L))


On Mon, Dec 7, 2015 at 6:22 AM, John Kane <jrkrideau at inbox.com> wrote:

> Welcome to the list.
>
> You have provided a nic clear question but I think the one thing missing
> in dealing with it is  some sample data.
>
> Have a look at ?dput or see the dput() discussions in one of these links :
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> and/or http://adv-r.had.co.nz/Reproducibility.html
>
> Someone may easily find the problem without sample data but it usually is
> best to troubleshoot with the "real" data.
>
> John Kane
> Kingston ON Canada
>
>
> > -----Original Message-----
> > From: djv5030 at gmail.com
> > Sent: Sun, 6 Dec 2015 19:47:12 -0600
> > To: r-help at r-project.org
> > Subject: [R] Simple DLNM in R
> >
> > Hello, first time poster so forgive any mistakes.
> >
> > I have limited familiarity with R, but am working on a project to find
> > the
> > relative risk of mortality due to changes in diurnal temperature range.
> > What I am trying to do is find the relative risk of mortality at the
> > 10th,
> > 50th and 90th percentiles of diurnal temperature range and its additive
> > effects at lags of 0, 1, 3 and 5 days. I'm doing this for a subset of
> > months May-Sept (I call the subset here for mortality, temperature is
> > already subsetted when read in). I have a code that works below, but no
> > matter what city and what lag I introduce, I get a RR of essentially 1.0,
> > so I believe that something is off or I am missing an argument somewhere.
> > If anyone has more experience with these problems than I, your help would
> > be greatly appreciated. Code is below:
> >
> > library('dlnm')
> > library('splines')
> >
> > mortdata <- read.table('STLmort.txt', sep="\t", header=T)
> > morts <- subset(mortdata, Month %in% 5:9)
> > deaths <- morts$AllMort
> > tempdata <- read.csv('STLRanges.csv',sep=',',header=T)
> > temp <- tempdata$Trange
> > HI <- tempdata$HIrange
> > #basis.var <- onebasis(1:5, knots=3)#mklagbasis(maxlag=5, type="poly",
> > degree=3)
> > basis.temp <- crossbasis(temp,vardegree=3,lag=5)
> > summary(basis.temp)
> > model <- glm (deaths ~ basis.temp, family=quasipoisson())
> > pred.temp <- crosspred(basis.temp, model,
> > at=quantile(temp,c(.10,.50,.90),na.rm=TRUE) , cumul=T)
> > plot(pred.temp, "slices", var=c(quantile(temp, c(.10, .50,
> > .90),na.rm=TRUE)) ,lag=c(0,1,5))
> >
> >
> > --
> > Daniel J. Vecellio
> >
> > PhD Student, Department of Geography
> > Texas A&M University
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ____________________________________________________________
> FREE ONLINE PHOTOSHARING - Share your photos online with your friends and
> family!
> Visit http://www.inbox.com/photosharing to find out more!
>
>
>


-- 
Daniel J. Vecellio

PhD Student, Department of Geography
Texas A&M University

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Dec  7 16:44:25 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 7 Dec 2015 07:44:25 -0800
Subject: [R] ggplot2: remove axis ticks
In-Reply-To: <CAEQKoCFJQ8FRzhegoCtCkeYZBzu-quJXjD-TC+ZU9RsycD8=0g@mail.gmail.com>
References: <CAEQKoCFJQ8FRzhegoCtCkeYZBzu-quJXjD-TC+ZU9RsycD8=0g@mail.gmail.com>
Message-ID: <CAF8bMcbOd-BS0eHBTdsA0O_vw7i6QdwKn21ma62zQgDacyOrAg@mail.gmail.com>

Change the order of adding the themes from
   theme(axis.ticks=element_blank()) + theme_bw()
to
   theme_bw() + theme(axis.ticks=element_blank())
because theme_bw() adds axis.ticks = element_line(colour = "black").
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Dec 7, 2015 at 7:10 AM, Brian Smith <bsmith030465 at gmail.com> wrote:
> Hi,
>
> I was trying to remove the axis tick marks and their values using theme()
> but haven't had much success. Here is sample code:
>
> rx <- sample(1:100,10)
> ry <- sample(1:100,10)
> rz <- sample(letters[1:3],10,replace=T)
> rdf <- data.frame(rx,ry,rz)
>
> p <- ggplot(rdf,aes(x=rx,y=ry))
> p1 <- p + geom_point(aes(shape=factor(rz),colour=factor(rz)),size=6) +
>     theme(axis.ticks = element_blank(), axis.text.x =
> element_blank(),axis.text.y = element_blank()) +
>     scale_shape_manual(values=rz)  + theme_bw() +
>     labs(colour='rz',shape='rz')
> p1
>
>
> My session info:
>
>> sessionInfo()
> R version 3.2.2 (2015-08-14)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.5 (Yosemite)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
>  [1] grid      stats4    parallel  stats     graphics  grDevices utils
> datasets  methods   base
>
> other attached packages:
>  [1] IlluminaHumanMethylation450kmanifest_0.4.0 biomaRt_2.26.1
>
>  [3] data.table_1.9.6                           foreign_0.8-65
>
>  [5] preprocessCore_1.32.0                      gtools_3.5.0
>
>  [7] BiocInstaller_1.20.1                       ggdendro_0.1-17
>
>  [9] reshape_0.8.5                              RnBeads_1.2.0
>
> [11] plyr_1.8.3                                 methylumi_2.16.0
>
> [13] minfi_1.16.0                               bumphunter_1.10.0
>
> [15] locfit_1.5-9.1                             iterators_1.0.8
>
> [17] foreach_1.4.3                              Biostrings_2.38.2
>
> [19] XVector_0.10.0                             SummarizedExperiment_1.0.1
>
> [21] lattice_0.20-33
>  FDb.InfiniumMethylation.hg19_2.2.0
> [23] org.Hs.eg.db_3.2.3                         RSQLite_1.0.0
>
> [25] DBI_0.3.1
>  TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2
> [27] GenomicFeatures_1.22.5                     AnnotationDbi_1.32.0
>
> [29] reshape2_1.4.1                             scales_0.3.0
>
> [31] Biobase_2.30.0                             illuminaio_0.12.0
>
> [33] matrixStats_0.15.0                         limma_3.26.3
>
> [35] gridExtra_2.0.0                            gplots_2.17.0
>
> [37] ggplot2_1.0.1                              fields_8.3-5
>
> [39] maps_3.0.0-2                               spam_1.3-0
>
> [41] ff_2.2-13                                  bit_1.1-12
>
> [43] cluster_2.0.3                              RColorBrewer_1.1-2
>
> [45] MASS_7.3-43                                GenomicRanges_1.22.1
>
> [47] GenomeInfoDb_1.6.1                         IRanges_2.4.4
>
> [49] S4Vectors_0.8.3                            BiocGenerics_0.16.1
>
>
> loaded via a namespace (and not attached):
>  [1] nlme_3.1-121            bitops_1.0-6            tools_3.2.2
>   doRNG_1.6
>  [5] nor1mix_1.2-1           KernSmooth_2.23-15      colorspace_1.2-6
>  base64_1.1
>  [9] chron_2.3-47            pkgmaker_0.22           labeling_0.3
>  rtracklayer_1.30.1
> [13] caTools_1.17.1          genefilter_1.52.0       quadprog_1.5-5
>  stringr_1.0.0
> [17] digest_0.6.8            Rsamtools_1.22.0        siggenes_1.44.0
>   GEOquery_2.36.0
> [21] mclust_5.1              BiocParallel_1.4.0      RCurl_1.95-4.7
>  magrittr_1.5
> [25] futile.logger_1.4.1     Rcpp_0.12.2             munsell_0.4.2
>   proto_0.3-10
> [29] stringi_1.0-1           zlibbioc_1.16.0         gdata_2.17.0
>  splines_3.2.2
> [33] multtest_2.26.0         annotate_1.48.0         beanplot_1.2
>  igraph_1.0.1
> [37] corpcor_1.6.8           rngtools_1.2.4          codetools_0.2-14
>  mixOmics_5.2.0
> [41] futile.options_1.0.0    XML_3.98-1.3            lambda.r_1.1.7
>  gtable_0.1.2
> [45] xtable_1.8-0            survival_2.38-3         ellipse_0.3-8
>   GenomicAlignments_1.6.1
> [49] registry_0.3            rgl_0.95.1201
>
>
> Am I setting the arguments for theme() incorrectly?
>
> many thanks,
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Mon Dec  7 19:45:10 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 7 Dec 2015 12:45:10 -0600
Subject: [R] [R-pkgs] WriteXLS Version 4.0.0 Released
Message-ID: <A5FB07ED-53B9-4169-9CB0-1B7C56204E98@me.com>

Hi all,

WriteXLS version 4.0.0 has been released. It should start appearing in source tarballs and binaries in due course on CRAN mirrors.

The primary changes in this new version are:

1. The new ability to specify either a single data frame object or a list object that contains one or more data frames. 

Previously one needed to specify a character vector of the quoted names of data frames or the quoted name of a list containing one or more data frames. 

I have had several requests to also allow the objects themselves to be passed and this is now possible, in addition to the original functionality. The use of quoted names has also been one of the more common sources of confusion in the use of the function.

Please see the examples in ?WriteXLS for the differences in calling the function with quoted names versus objects.

It was my goal to not break existing code and this has been tested with the new functionality, so please let me know if any one runs into any issues.


2. A new 'na' argument, defaulting to "" (blank) for backward compatibility, that allows one to specify the character value to be used in the target Excel file, when NA values are encountered in the source data frame. This is a parallel to the same argument in ?write.table.


Regards,

Marc Schwartz

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From roypeter77 at hotmail.com  Mon Dec  7 16:43:55 2015
From: roypeter77 at hotmail.com (Peter Karpestam)
Date: Mon, 7 Dec 2015 15:43:55 +0000
Subject: [R] Spatial Panel Model by Maximum Likelihood: error message
In-Reply-To: <da0875503afd4de58b443e022cc6a347@winhexbeeu15.win.mail>
References: <DUB123-W45248EAA34EAF09A5CFF1DDD090@phx.gbl>,
	<da0875503afd4de58b443e022cc6a347@winhexbeeu15.win.mail>
Message-ID: <DUB123-W18AE476BE2CE0E66C7C6E6DD090@phx.gbl>

Thank you but no! I have tried that as well and I get: 

"Error in mat2listw(e) : (list) object cannot be coerced to type 'double'"

> From: holtermann at hwwi.org
> To: roypeter77 at hotmail.com; r-help at r-project.org
> Subject: AW: [R] Spatial Panel Model by Maximum Likelihood: error message
> Date: Mon, 7 Dec 2015 15:06:05 +0000
> 
> Maybe "e" has the wrong class. Probably it is a matrix. Try listw = mat2listw(e)
> 
> 
> Mit freundlichen Gr??en
> 
> 
> Linus Holtermann
> Hamburgisches WeltWirtschaftsInstitut gemeinn?tzige GmbH (HWWI)
> Heimhuder Stra?e 71
> 20148 Hamburg
> Tel +49-(0)40-340576-336
> Fax+49-(0)40-340576-776
> Internet: www.hwwi.org
> Email: holtermann at hwwi.org
>  
> Amtsgericht Hamburg HRB 94303
> Gesch?ftsf?hrer: Prof. Dr. Henning V?pel
> Prokura: Dipl. Kauffrau Alexis Malchin
> Umsatzsteuer-ID: DE 241849425
> 
> 
> -----Urspr?ngliche Nachricht-----
> Von: R-help [mailto:r-help-bounces at r-project.org] Im Auftrag von Peter Karpestam
> Gesendet: Montag, 7. Dezember 2015 11:07
> An: r-help at r-project.org
> Betreff: [R] Spatial Panel Model by Maximum Likelihood: error message
> 
> Dear R-users!
> 
> Beforehand I do apologize for this fundamental type of question. But I am new to R, and I have indeed spent several hours online trying to avoid spamming this forum with basic questions.
> I am still not able to find out what's wrong.
> 
> I attach my code! I nead to estimate a spatial panel model. In doing so, I have installed and loaded the "splm package".
> 
> > b<- 
> > read.dta13('C:/Users/Peter/Documents/R/win-library/3.2/ENHR/tillvaxtte
> > st2.dta') 
> > mydata1=read.table('C:/Users/Peter/Documents/R/win-library/3.2/ENHR/av
> > stand6.txt', header=FALSE)
> > 
> > e=matrix(mydata, nrow=286, ncol=286)
> > tillvaxt<-pdata.frame(b)
> > 
> > fm<-dlnypc~Totinfly+totutfly
> > 
> > fespaterr <- spml(fm, data = tillvaxt,listw =e,
> + model="within", spatial.error="b", Hess = FALSE)
> Error in listw2mat(x) : non-positive number of entities
> 
> The error message is found in the last line. I am checking my "weighting matrix" and according to my understanding, the matrix fulfills the requirement (i.e. the diagonal elements are zero, whereas remaining elements are positive). It is a symmetric 286X286 matrix.
> 
> Obviuosly, there is something wrong with my e-matrix, but I can't figure out how. There are in fact no negative elements in the matrix.
> 
> Thank you in advance!
> 
> Peter 
>  		 	   		  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  
	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Mon Dec  7 21:05:22 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 7 Dec 2015 12:05:22 -0800
Subject: [R] Simple DLNM in R
In-Reply-To: <CAPWPvYj2mjXn3Pr+nAy8DJKoe2c4=eA8Pq6hU8Tq5M9VPy5sUg@mail.gmail.com>
References: <ebe03c188a5.000001a1jrkrideau@inbox.com>
	<capwpvygwf2abyab5dievuyhqo1mu73evt-eekfy=keyjs64apa@mail.gmail.com>
Message-ID: <EFEAEC65D33.000007C3jrkrideau@inbox.com>

Oops, perhaps I should have said ?a representative sample of the data? but it looks like it has arrived safely. Let me tell you about head() someday. 

I am not sure of the data.frame names since the original code has some sub-setting at the beginning. At the moment I am calling the first data set ?mortdata? and the second one ?tempdata? but is this correct? Your wording seems a bit ambiguous.

It seems to run so I guess I have the data sets named correctly. 

Now we just hope someone with domain knowledge show up to discuss the actual problem. 

I killed the rest of the post as it was a bit too big to keep sending back and forth.

John Kane

Kingston ON

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From hannah.hlx at gmail.com  Mon Dec  7 21:15:48 2015
From: hannah.hlx at gmail.com (li li)
Date: Mon, 7 Dec 2015 15:15:48 -0500
Subject: [R] question with drm function in "drc package"
Message-ID: <CAHLnndZ6oXd48GAtZ_casb3W2XkQ=ZYrSbcZVj5v9oErEkBa2A@mail.gmail.com>

Hi all,
  I am trying to use the drm function in drc package to fit a 4 PL or 3PL
curve for an assay response. Please see the listed data below. When I do
the curve fitting, it returns the following error message. Anyone who
familiar with this have any input on what went wrong?
   Thanks so much in advance!!
      Hanna

> dat
      values   log_dose
1   -68.1125  3.9120230
2   -73.8955  3.9120230
3   -75.1235  3.9120230
4   -47.3205  3.2188758
5   -52.9835  3.2188758
6   -58.0075  3.2188758
7     8.2515  2.5257286
8   -24.5925  2.5257286
9   -18.1425  2.5257286
10   75.3375  1.8325815
11   89.3755  1.8325815
12  115.9685  1.8325815
13  345.7675  1.1394343
14  470.8125  1.1394343
15  514.8985  1.1394343
16 1003.2235  0.4462871
17 1033.0345  0.4462871
18  866.1365  0.4462871
19 1383.6525 -0.2468601
20 1431.3245 -0.2468601
21 1459.8025 -0.2468601
22 1852.5795 -0.9400073
23 1917.8015 -0.9400073
24 1858.0875 -0.9400073
25 1995.1185 -1.6331544
26 2033.9455 -1.6331544
27 1991.0405 -1.6331544
28 2064.5855 -2.3263016
29 2043.5195 -2.3263016
30 2089.8525 -2.3263016
31 2147.8445 -3.0194488
32 2047.7905 -3.0194488
33 2002.5375 -3.0194488
34 2075.2665 -3.7125960
35 2068.3545 -3.7125960
36 2055.9605 -3.7125960
> mod1 <- drm(values~log_dose, fct=LL.4(), data=dat)
Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control
= list(maxit = maxIt,  :
  initial value in 'vmmin' is not finite
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained,
warnVal,  :
  Convergence failed
>

	[[alternative HTML version deleted]]


From bsmith030465 at gmail.com  Mon Dec  7 21:34:51 2015
From: bsmith030465 at gmail.com (Brian Smith)
Date: Mon, 7 Dec 2015 15:34:51 -0500
Subject: [R] ChAMP: champ.runCombat error with methylation 450k data
Message-ID: <CAEQKoCGgwFFuX-eFoe8kMxoBtjL=TmZeg+=8ZTzRV-03xHtK5w@mail.gmail.com>

Hi,

I was trying to run COMBAT on methylation data, but keep on getting an
error:

Error in while (change > conv) { : missing value where TRUE/FALSE needed

The error occurs irrespective of whether I give the entire or reduced
(variation filter keeps only about 140k CpGs) datasets.

Is there any other preprocessing that I should be doing?

thanks!!

my code and sessionInfo():



> betacombat <- champ.runCombat(beta.c = beta3, pd = ss, logitTrans = TRUE)
Preparing files for ComBat
Zeros in your dataset have been replaced with 0.000001
Your data is being logit transformed before batch correction
Beginning batch correction
Found 60 batches
Found 0  categorical covariate(s)
Standardizing Data across genes
Fitting L/S model and finding priors
Finding parametric adjustments
Error in while (change > conv) { : missing value where TRUE/FALSE needed
> traceback()
3: it.sol(s.data[, batches[[i]]], gamma.hat[i, ], delta.hat[i, ],
       gamma.bar[i], t2[i], a.prior[i], b.prior[i])
2: champ.ComBat(dat = log, batch = batch, mod = mod, par.prior = TRUE)
1: champ.runCombat(beta.c = betaASDnorm3, pd = ss_2ASD, logitTrans = TRUE)
> sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.10.5 (Yosemite)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
 [1] grid      stats4    parallel  stats     graphics  grDevices utils
datasets  methods   base

other attached packages:
 [1] ChAMP_1.8.0
 Illumina450ProbeVariants.db_1.6.0
 [3] ChAMPdata_1.8.0
 IlluminaHumanMethylation450kmanifest_0.4.0
 [5] biomaRt_2.26.1                             data.table_1.9.6

 [7] foreign_0.8-65                             preprocessCore_1.32.0

 [9] gtools_3.5.0                               BiocInstaller_1.20.1

[11] ggdendro_0.1-17                            reshape_0.8.5

[13] RnBeads_1.2.0                              plyr_1.8.3

[15] methylumi_2.16.0                           minfi_1.16.0

[17] bumphunter_1.10.0                          locfit_1.5-9.1

[19] iterators_1.0.8                            foreach_1.4.3

[21] Biostrings_2.38.2                          XVector_0.10.0

[23] SummarizedExperiment_1.0.1                 lattice_0.20-33

[25] FDb.InfiniumMethylation.hg19_2.2.0         org.Hs.eg.db_3.2.3

[27] RSQLite_1.0.0                              DBI_0.3.1

[29] TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2    GenomicFeatures_1.22.5

[31] AnnotationDbi_1.32.0                       reshape2_1.4.1

[33] scales_0.3.0                               Biobase_2.30.0

[35] illuminaio_0.12.0                          matrixStats_0.15.0

[37] limma_3.26.3                               gridExtra_2.0.0

[39] gplots_2.17.0                              ggplot2_1.0.1

[41] fields_8.3-5                               maps_3.0.0-2

[43] spam_1.3-0                                 ff_2.2-13

[45] bit_1.1-12                                 cluster_2.0.3

[47] RColorBrewer_1.1-2                         MASS_7.3-43

[49] GenomicRanges_1.22.1                       GenomeInfoDb_1.6.1

[51] IRanges_2.4.4                              S4Vectors_0.8.3

[53] BiocGenerics_0.16.1

loaded via a namespace (and not attached):
 [1] nlme_3.1-121            bitops_1.0-6            tools_3.2.2
  doRNG_1.6
 [5] nor1mix_1.2-1           KernSmooth_2.23-15      mgcv_1.8-7
 colorspace_1.2-6
 [9] DNAcopy_1.44.0          base64_1.1              chron_2.3-47
 wateRmelon_1.10.0
[13] RPMM_1.20               pkgmaker_0.22           labeling_0.3
 rtracklayer_1.30.1
[17] caTools_1.17.1          genefilter_1.52.0       quadprog_1.5-5
 stringr_1.0.0
[21] digest_0.6.8            Rsamtools_1.22.0        siggenes_1.44.0
  GEOquery_2.36.0
[25] impute_1.44.0           mclust_5.1              BiocParallel_1.4.0
 RCurl_1.95-4.7
[29] magrittr_1.5            Matrix_1.2-2            futile.logger_1.4.1
  Rcpp_0.12.2
[33] munsell_0.4.2           proto_0.3-10            stringi_1.0-1
  zlibbioc_1.16.0
[37] gdata_2.17.0            splines_3.2.2           multtest_2.26.0
  annotate_1.48.0
[41] beanplot_1.2            igraph_1.0.1            corpcor_1.6.8
  rngtools_1.2.4
[45] marray_1.48.0           codetools_0.2-14        mixOmics_5.2.0
 futile.options_1.0.0
[49] XML_3.98-1.3            lambda.r_1.1.7          gtable_0.1.2
 xtable_1.8-0
[53] survival_2.38-3         ellipse_0.3-8
GenomicAlignments_1.6.1 registry_0.3
[57] sva_3.18.0              rgl_0.95.1201

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Mon Dec  7 21:40:37 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 7 Dec 2015 12:40:37 -0800
Subject: [R] ChAMP: champ.runCombat error with methylation 450k data
In-Reply-To: <CAEQKoCGgwFFuX-eFoe8kMxoBtjL=TmZeg+=8ZTzRV-03xHtK5w@mail.gmail.com>
References: <CAEQKoCGgwFFuX-eFoe8kMxoBtjL=TmZeg+=8ZTzRV-03xHtK5w@mail.gmail.com>
Message-ID: <CAGxFJbSiupn04QJVawrCnuaaVsDOZsdT00p+QoLeWa7WPTZwQA@mail.gmail.com>

Champ is a BioConductor package. You should post to the Bioconductor
Help, not here.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Mon, Dec 7, 2015 at 12:34 PM, Brian Smith <bsmith030465 at gmail.com> wrote:
> Hi,
>
> I was trying to run COMBAT on methylation data, but keep on getting an
> error:
>
> Error in while (change > conv) { : missing value where TRUE/FALSE needed
>
> The error occurs irrespective of whether I give the entire or reduced
> (variation filter keeps only about 140k CpGs) datasets.
>
> Is there any other preprocessing that I should be doing?
>
> thanks!!
>
> my code and sessionInfo():
>
>
>
>> betacombat <- champ.runCombat(beta.c = beta3, pd = ss, logitTrans = TRUE)
> Preparing files for ComBat
> Zeros in your dataset have been replaced with 0.000001
> Your data is being logit transformed before batch correction
> Beginning batch correction
> Found 60 batches
> Found 0  categorical covariate(s)
> Standardizing Data across genes
> Fitting L/S model and finding priors
> Finding parametric adjustments
> Error in while (change > conv) { : missing value where TRUE/FALSE needed
>> traceback()
> 3: it.sol(s.data[, batches[[i]]], gamma.hat[i, ], delta.hat[i, ],
>        gamma.bar[i], t2[i], a.prior[i], b.prior[i])
> 2: champ.ComBat(dat = log, batch = batch, mod = mod, par.prior = TRUE)
> 1: champ.runCombat(beta.c = betaASDnorm3, pd = ss_2ASD, logitTrans = TRUE)
>> sessionInfo()
> R version 3.2.2 (2015-08-14)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.10.5 (Yosemite)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
>  [1] grid      stats4    parallel  stats     graphics  grDevices utils
> datasets  methods   base
>
> other attached packages:
>  [1] ChAMP_1.8.0
>  Illumina450ProbeVariants.db_1.6.0
>  [3] ChAMPdata_1.8.0
>  IlluminaHumanMethylation450kmanifest_0.4.0
>  [5] biomaRt_2.26.1                             data.table_1.9.6
>
>  [7] foreign_0.8-65                             preprocessCore_1.32.0
>
>  [9] gtools_3.5.0                               BiocInstaller_1.20.1
>
> [11] ggdendro_0.1-17                            reshape_0.8.5
>
> [13] RnBeads_1.2.0                              plyr_1.8.3
>
> [15] methylumi_2.16.0                           minfi_1.16.0
>
> [17] bumphunter_1.10.0                          locfit_1.5-9.1
>
> [19] iterators_1.0.8                            foreach_1.4.3
>
> [21] Biostrings_2.38.2                          XVector_0.10.0
>
> [23] SummarizedExperiment_1.0.1                 lattice_0.20-33
>
> [25] FDb.InfiniumMethylation.hg19_2.2.0         org.Hs.eg.db_3.2.3
>
> [27] RSQLite_1.0.0                              DBI_0.3.1
>
> [29] TxDb.Hsapiens.UCSC.hg19.knownGene_3.2.2    GenomicFeatures_1.22.5
>
> [31] AnnotationDbi_1.32.0                       reshape2_1.4.1
>
> [33] scales_0.3.0                               Biobase_2.30.0
>
> [35] illuminaio_0.12.0                          matrixStats_0.15.0
>
> [37] limma_3.26.3                               gridExtra_2.0.0
>
> [39] gplots_2.17.0                              ggplot2_1.0.1
>
> [41] fields_8.3-5                               maps_3.0.0-2
>
> [43] spam_1.3-0                                 ff_2.2-13
>
> [45] bit_1.1-12                                 cluster_2.0.3
>
> [47] RColorBrewer_1.1-2                         MASS_7.3-43
>
> [49] GenomicRanges_1.22.1                       GenomeInfoDb_1.6.1
>
> [51] IRanges_2.4.4                              S4Vectors_0.8.3
>
> [53] BiocGenerics_0.16.1
>
> loaded via a namespace (and not attached):
>  [1] nlme_3.1-121            bitops_1.0-6            tools_3.2.2
>   doRNG_1.6
>  [5] nor1mix_1.2-1           KernSmooth_2.23-15      mgcv_1.8-7
>  colorspace_1.2-6
>  [9] DNAcopy_1.44.0          base64_1.1              chron_2.3-47
>  wateRmelon_1.10.0
> [13] RPMM_1.20               pkgmaker_0.22           labeling_0.3
>  rtracklayer_1.30.1
> [17] caTools_1.17.1          genefilter_1.52.0       quadprog_1.5-5
>  stringr_1.0.0
> [21] digest_0.6.8            Rsamtools_1.22.0        siggenes_1.44.0
>   GEOquery_2.36.0
> [25] impute_1.44.0           mclust_5.1              BiocParallel_1.4.0
>  RCurl_1.95-4.7
> [29] magrittr_1.5            Matrix_1.2-2            futile.logger_1.4.1
>   Rcpp_0.12.2
> [33] munsell_0.4.2           proto_0.3-10            stringi_1.0-1
>   zlibbioc_1.16.0
> [37] gdata_2.17.0            splines_3.2.2           multtest_2.26.0
>   annotate_1.48.0
> [41] beanplot_1.2            igraph_1.0.1            corpcor_1.6.8
>   rngtools_1.2.4
> [45] marray_1.48.0           codetools_0.2-14        mixOmics_5.2.0
>  futile.options_1.0.0
> [49] XML_3.98-1.3            lambda.r_1.1.7          gtable_0.1.2
>  xtable_1.8-0
> [53] survival_2.38-3         ellipse_0.3-8
> GenomicAlignments_1.6.1 registry_0.3
> [57] sva_3.18.0              rgl_0.95.1201
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Mon Dec  7 22:33:44 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 07 Dec 2015 13:33:44 -0800
Subject: [R] question with drm function in "drc package"
In-Reply-To: <CAHLnnda_GEKfjz_D7t8MfhwO9PV9w1miSMZKkh_kV_Q=iGZpVg@mail.gmail.com>
References: <CAHLnndZ6oXd48GAtZ_casb3W2XkQ=ZYrSbcZVj5v9oErEkBa2A@mail.gmail.com>
	<9E227E5D-AC99-4DDB-8652-A52C5868BC49@dcn.davis.ca.us>
	<CAHLnnda_GEKfjz_D7t8MfhwO9PV9w1miSMZKkh_kV_Q=iGZpVg@mail.gmail.com>
Message-ID: <C258F32C-B131-4053-A1DD-4C8AD7FA16A9@dcn.davis.ca.us>

The fine manual for the drm function mentions a parameter "logDose" ... I expect that since you did not specify it that the drm function is helpfully attempting to take the log of your data for you. 

I highly recommend reading up on making reproducible examples so your helpers can experiment with your problem directly instead of guessing by reading only. Of course,  you would then want to post using plain text as the Posting Guide indicates so the HTML would not corrupt your code.

http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
-- 
Sent from my phone. Please excuse my brevity.

On December 7, 2015 1:05:32 PM PST, li li <hannah.hlx at gmail.com> wrote:
>Hi Jeff,
>  I am not sure where the log of 0 was taken?
>  Thanks.
>    Li
>
>2015-12-07 15:55 GMT-05:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>
>> Log of zero is infinity. Don't do that.
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On December 7, 2015 12:15:48 PM PST, li li <hannah.hlx at gmail.com>
>wrote:
>>
>>> Hi all,
>>>   I am trying to use the drm function in drc package to fit a 4 PL
>or 3PL
>>> curve for an assay response. Please see the listed data below. When
>I do
>>> the curve fitting, it returns the following error message. Anyone
>who
>>> familiar with this have any input on what went wrong?
>>>    Thanks so much in advance!!
>>>       Hanna
>>>
>>>  dat
>>>>
>>>       values   log_dose
>>> 1   -68.1125  3.9120230
>>> 2   -73.8955  3.9120230
>>> 3   -75.1235  3.9120230
>>> 4   -47.3205  3.2188758
>>> 5   -52.9835  3.2188758
>>> 6   -58.0075  3.2188758
>>> 7     8.2515  2.5257286
>>> 8   -24.5925  2.5257286
>>> 9   -18.1425  2.5257286
>>> 10   75.3375  1.8325815
>>> 11   89.3755  1.8325815
>>> 12  115.9685  1.8325815
>>> 13  345.7675  1.1394343
>>> 14  470.8125  1.1394343
>>> 15  514.8985  1.1394343
>>> 16
>>> 1003.2235  0.4462871
>>> 17 1033.0345  0.4462871
>>> 18  866.1365  0.4462871
>>> 19 1383.6525 -0.2468601
>>> 20 1431.3245 -0.2468601
>>> 21 1459.8025 -0.2468601
>>> 22 1852.5795 -0.9400073
>>> 23 1917.8015 -0.9400073
>>> 24 1858.0875 -0.9400073
>>> 25 1995.1185 -1.6331544
>>> 26 2033.9455 -1.6331544
>>> 27 1991.0405 -1.6331544
>>> 28 2064.5855 -2.3263016
>>> 29 2043.5195 -2.3263016
>>> 30 2089.8525 -2.3263016
>>> 31 2147.8445 -3.0194488
>>> 32 2047.7905 -3.0194488
>>> 33 2002.5375 -3.0194488
>>> 34 2075.2665 -3.7125960
>>> 35 2068.3545 -3.7125960
>>> 36 2055.9605 -3.7125960
>>>
>>>>  mod1 <- drm(values~log_dose, fct=LL.4(), data=dat)
>>>>
>>> Error in optim(startVec, opfct, hessian = TRUE, method = optMethod,
>control
>>> = list(maxit = maxIt,  :
>>>   initial value in 'vmmin' is not finite
>>> Error in drmOpt(opfct, opdfct1, startVecSc,
>>> optMethod, constrained,
>>> warnVal,  :
>>>   Convergence failed
>>>
>>>>
>>>>
>>>  [[alternative HTML version deleted]]
>>>
>>> ------------------------------
>>>
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Tue Dec  8 03:11:28 2015
From: hannah.hlx at gmail.com (li li)
Date: Mon, 7 Dec 2015 21:11:28 -0500
Subject: [R] question with drm function in "drc package"
In-Reply-To: <C258F32C-B131-4053-A1DD-4C8AD7FA16A9@dcn.davis.ca.us>
References: <CAHLnndZ6oXd48GAtZ_casb3W2XkQ=ZYrSbcZVj5v9oErEkBa2A@mail.gmail.com>
	<9E227E5D-AC99-4DDB-8652-A52C5868BC49@dcn.davis.ca.us>
	<CAHLnnda_GEKfjz_D7t8MfhwO9PV9w1miSMZKkh_kV_Q=iGZpVg@mail.gmail.com>
	<C258F32C-B131-4053-A1DD-4C8AD7FA16A9@dcn.davis.ca.us>
Message-ID: <CAHLnndaoSZMsYV5ABOCCPRNt+HS0Hmf-pyD4GYdtp56ANru3vQ@mail.gmail.com>

Thanks. I see what went wrong.


2015-12-07 16:33 GMT-05:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:

> The fine manual for the drm function mentions a parameter "logDose" ... I
> expect that since you did not specify it that the drm function is helpfully
> attempting to take the log of your data for you.
>
> I highly recommend reading up on making reproducible examples so your
> helpers can experiment with your problem directly instead of guessing by
> reading only. Of course, you would then want to post using plain text as
> the Posting Guide indicates so the HTML would not corrupt your code.
>
>
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 7, 2015 1:05:32 PM PST, li li <hannah.hlx at gmail.com> wrote:
>>
>> Hi Jeff,
>>   I am not sure where the log of 0 was taken?
>>   Thanks.
>>     Li
>>
>> 2015-12-07 15:55 GMT-05:00 Jeff Newmiller <jdnewmil at dcn.davis.ca.us>:
>>
>>> Log of zero is infinity. Don't do that.
>>> --
>>> Sent from my phone. Please excuse my brevity.
>>>
>>> On December 7, 2015 12:15:48 PM PST, li li <hannah.hlx at gmail.com> wrote:
>>>
>>>> Hi all,
>>>>   I am trying to use the drm function in drc package to fit a 4 PL or 3PL
>>>> curve for an assay response. Please see the listed data below. When I do
>>>> the curve fitting, it returns the following error message. Anyone who
>>>> familiar with this have any input on what went wrong?
>>>>    Thanks so much in advance!!
>>>>       Hanna
>>>>
>>>>  dat
>>>>>
>>>>       values   log_dose
>>>> 1   -68.1125  3.9120230
>>>> 2   -73.8955  3.9120230
>>>> 3   -75.1235  3.9120230
>>>> 4   -47.3205  3.2188758
>>>> 5   -52.9835  3.2188758
>>>> 6   -58.0075  3.2188758
>>>> 7     8.2515  2.5257286
>>>> 8   -24.5925  2.5257286
>>>> 9   -18.1425  2.5257286
>>>> 10   75.3375  1.8325815
>>>> 11   89.3755  1.8325815
>>>> 12  115.9685  1.8325815
>>>> 13  345.7675  1.1394343
>>>> 14  470.8125
>>>> 1.1394343
>>>> 15  514.8985  1.1394343
>>>> 16
>>>> 1003.2235  0.4462871
>>>> 17 1033.0345  0.4462871
>>>> 18  866.1365  0.4462871
>>>> 19 1383.6525 -0.2468601
>>>> 20 1431.3245 -0.2468601
>>>> 21 1459.8025 -0.2468601
>>>> 22 1852.5795 -0.9400073
>>>> 23 1917.8015 -0.9400073
>>>> 24 1858.0875 -0.9400073
>>>> 25 1995.1185 -1.6331544
>>>> 26 2033.9455 -1.6331544
>>>> 27 1991.0405 -1.6331544
>>>> 28 2064.5855 -2.3263016
>>>> 29 2043.5195 -2.3263016
>>>> 30 2089.8525 -2.3263016
>>>> 31 2147.8445 -3.0194488
>>>> 32 2047.7905 -3.0194488
>>>> 33 2002.5375 -3.0194488
>>>> 34 2075.2665 -3.7125960
>>>> 35 2068.3545 -3.7125960
>>>> 36 2055.9605 -3.7125960
>>>>
>>>>>  mod1 <- drm(values~log_dose, fct=LL.4(), data=dat)
>>>>>
>>>> Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control
>>>> = list(maxit = maxIt,  :
>>>>   initial value in 'vmmin' is not
>>>> finite
>>>> Error in drmOpt(opfct, opdfct1, startVecSc,
>>>> optMethod, constrained,
>>>> warnVal,  :
>>>>   Convergence failed
>>>>
>>>>>
>>>>>
>>>>  [[alternative HTML version deleted]]
>>>>
>>>> ------------------------------
>>>>
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Tue Dec  8 04:26:17 2015
From: hannah.hlx at gmail.com (li li)
Date: Mon, 7 Dec 2015 22:26:17 -0500
Subject: [R] Plot in drc package
Message-ID: <CAHLnndYAFKnNuGC9GfK9nv_ACLKEYeXuxk=gYUvFau7gUq5S-w@mail.gmail.com>

Hi all,
  I have the following data and I fit a log logistic model using the drm
function in DRC package.
I saved the fitted model in an object called "mod". (See below)
   I understand that we can use the plot function to plot the regression
curve with the original data points using the code "plot(mod, type="all",
col="blue")". I have two questions regarding the generated plot:

    1. the concentration stop at 10 in the graph. How can I show data for
all concentration values? (in the original data set, the largest
concentration is 20).
    2. The plot looks like on the log (conc) scale (If I simply plot resp
against conc, the shape of the data is not sigmoid). However, the x label
automatically and the scale of the x axis come out as conc. It seems to be
it should be on the log scale instead.

Thanks very much in advance!!
   Hanna

 > dat
   resp         conc
1   -68 20.000000000
2   -74 20.000000000
3   -75 20.000000000
4   -47 10.000000000
5   -53 10.000000000
6   -58 10.000000000
7     8  5.000000000
8   -25  5.000000000
9   -18  5.000000000
10   75  2.500000000
11   89  2.500000000
12  116  2.500000000
13  346  1.250000000
14  471  1.250000000
15  515  1.250000000
16 1003  0.625000000
17 1033  0.625000000
18  866  0.625000000
19 1384  0.312500000
20 1431  0.312500000
21 1460  0.312500000
22 1853  0.156250000
23 1918  0.156250000
24 1858  0.156250000
25 1995  0.078125000
26 2034  0.078125000
27 1991  0.078125000
28 2065  0.039062500
29 2044  0.039062500
30 2090  0.039062500
31 2148  0.019531250
32 2048  0.019531250
33 2003  0.019531250
34 2075  0.009765625
35 2068  0.009765625
36 2056  0.009765625
> mod <- drm(resp ~ conc, fct=LL.4(), data=dat)
> summary(mod)
Model fitted: Log-logistic (ED50 as parameter) (4 parms)
Parameter estimates:
                 Estimate  Std. Error     t-value p-value
b:(Intercept)    1.536783    0.060292   25.489167   0e+00
c:(Intercept)  -86.952455   18.941887   -4.590485   1e-04
d:(Intercept) 2089.228320   16.413634  127.286154   0e+00
e:(Intercept)    0.581979    0.017057   34.119620   0e+00
Residual standard error:
 48.61747 (32 degrees of freedom)

	[[alternative HTML version deleted]]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Dec  8 09:47:43 2015
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 8 Dec 2015 09:47:43 +0100
Subject: [R] metafor package
In-Reply-To: <CAMYNiDo5fyXH+zBMX+Hf+NJgB5QQSQNLtQKhdruwDP8Rg-3BFw@mail.gmail.com>
References: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C463BF@UM-MAIL4112.unimaas.nl>
	<CAMYNiDo5fyXH+zBMX+Hf+NJgB5QQSQNLtQKhdruwDP8Rg-3BFw@mail.gmail.com>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730F24C4663D@UM-MAIL4112.unimaas.nl>

Hi John,

Please keep r-help copied on the reply.

What's the 'previous model'? How do you get estimates within subgroups that 'includes the overall effect'? I really cannot follow you here.

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: John Peterson [mailto:john.peterson.can at gmail.com]
> Sent: Monday, December 07, 2015 22:14
> To: Viechtbauer Wolfgang (STAT)
> Subject: Re: [R] metafor package
> 
> Hi Dr. Viechtbauer,
> Thank you very much for your reply. I tried your advice and was able to
> make a forest plot with only the estimates for each subgroup.For the
> estiamte for each subgroup, similar to the previous model, I random
> effects model within each subgroup. However, I now find the result for
> the estiamte within subgroup to be different thant the result for the
> previous model. I have tried analyzing this in STATA and I get the same
> result as the model which includes the overall effect. Any advice on what
> may be wrong here? Thanks greatly,
> John
> 
> On 7 December 2015 at 04:02, Viechtbauer Wolfgang (STAT)
> <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> The code you posted is totally mangled up, but it's just what can be
> found here:
> 
> http://www.metafor-project.org/doku.php/plots:forest_plot_with_subgroups
> 
> If you don't want an overall estimate, just pass the estimates and
> corresponding sampling variances to the forest() function (and not the
> model object). Use the 'rows' argument to specify where the estimates
> will be placed and adjust 'ylim' so give you enough space to leave gaps
> for headings and the subgroup estimates. Then fit models within the
> subgroups (the 'subset' argument is useful here) and use addpoly() to add
> the subgroup estimates in the appropriate rows. With text(), you can add
> headings as needed.
> 
> If you use weights() on each subgroup model object, you can get the
> subgroup weights (that add up to 100% within each subgroup). It's
> probably easiest to just add those values with text() in an appropriate
> place to the plot.
> 
> Best,
> Wolfgang
> 
> --
> Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John
> > Peterson
> > Sent: Monday, December 07, 2015 01:39
> > To: r-help at r-project.org
> > Subject: [R] metafor package
> >
> > Hi Everyone,
> >
> > I am conducting a meta-analysis using the metafor package. I am
> > interested
> > in obtaining an estimate by subgroup only without showing an overall
> > effect. This is directly from the metafor website. How would i modify
> > this
> > code to only show subgroup effects? Further, I want to show weights by
> > subgroup. The option showweights=TRUE does not display weights by
> > subgroup
> > but by the weight of each study in comparison to all studies (and not
> the
> > subgroup). You help would be appreciated.

[snip garbled code]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Tue Dec  8 14:37:18 2015
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Tue, 8 Dec 2015 14:37:18 +0100
Subject: [R] metafor package
In-Reply-To: <CAMYNiDqL0NeaS2YAfCPvMm5FF0YpDdMECrevncfnLaZrTJQa3w@mail.gmail.com>
References: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C463BF@UM-MAIL4112.unimaas.nl>
	<CAMYNiDo5fyXH+zBMX+Hf+NJgB5QQSQNLtQKhdruwDP8Rg-3BFw@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C4663D@UM-MAIL4112.unimaas.nl>
	<CAMYNiDqL0NeaS2YAfCPvMm5FF0YpDdMECrevncfnLaZrTJQa3w@mail.gmail.com>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730F24C4677C@UM-MAIL4112.unimaas.nl>

The first and second argument of forest() (or more precisely, forest.default()) are for the estimates and the corresponding sampling variances, respectively. So, if you do forest(rr, se, ...), then the function will interpret the standard errors as if they are variances. So, you should do forest(rr, sei=se, ...).

And just in case: In all likelihood, those SEs are for the *log-transformed* risk ratios, so you should also pass log-transformed risk ratios to the function (and then use 'atransf=exp' so results are shown with back-transformed x-axis labels and annotations).

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: John Peterson [mailto:john.peterson.can at gmail.com]
> Sent: Tuesday, December 08, 2015 14:25
> To: Viechtbauer Wolfgang (STAT)
> Cc: R-help at r-project.org
> Subject: Re: [R] metafor package
> 
> Hi Dr. Viechtbauer,
> 
> The code provided in the metafor projects website for subgroup includes
> fitting a random effects model on the entire dataset and fitting a random
> effects model within subgroups. When I exactly follow this code, my
> estimates and confidence intervals for estimate within each subgroup
> matches with what I get in STATA so it seems to be the correct estimate
> (and CI). However, I don't want to present an overall effect and I want
> to present only the effect within each subgroup. In my second attempt, I
> did not run a random effects model within the entire dataset and only ran
> the models within each subgroup. I generated a variable corresponding to
> the estimate(risk ratio) and standard error? which I plugged in the
> forest() function (i.e. forest(rr, se, .....).? When I run this code, the
> estimate I get for each subgroup is slightly different than the estimate
> I get for each subgroup in comparison to when I also included the random
> effects model for the overall effect. (i.e. my first attempt). Is this
> the right approach? I was not clear when you said passing the estimates
> and sampling variances to the forest() function. I created a variable
> corresponding to the estimate and standard error and plugged those in the
> forest() function. I am not sure if this is the right approach. Thanks,
> 
> John
> 
> On 8 December 2015 at 03:47, Viechtbauer Wolfgang (STAT)
> <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> Hi John,
> 
> Please keep r-help copied on the reply.
> 
> What's the 'previous model'? How do you get estimates within subgroups
> that 'includes the overall effect'? I really cannot follow you here.
> 
> Best,
> Wolfgang
> 
> --
> Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
> 
> > -----Original Message-----
> > From: John Peterson [mailto:john.peterson.can at gmail.com]
> > Sent: Monday, December 07, 2015 22:14
> > To: Viechtbauer Wolfgang (STAT)
> > Subject: Re: [R] metafor package
> >
> > Hi Dr. Viechtbauer,
> > Thank you very much for your reply. I tried your advice and was able to
> > make a forest plot with only the estimates for each subgroup.For the
> > estiamte for each subgroup, similar to the previous model, I random
> > effects model within each subgroup. However, I now find the result for
> > the estiamte within subgroup to be different thant the result for the
> > previous model. I have tried analyzing this in STATA and I get the same
> > result as the model which includes the overall effect. Any advice on
> what
> > may be wrong here? Thanks greatly,
> > John
> >
> > On 7 December 2015 at 04:02, Viechtbauer Wolfgang (STAT)
> > <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> > The code you posted is totally mangled up, but it's just what can be
> > found here:
> >
> > http://www.metafor-
> project.org/doku.php/plots:forest_plot_with_subgroups
> >
> > If you don't want an overall estimate, just pass the estimates and
> > corresponding sampling variances to the forest() function (and not the
> > model object). Use the 'rows' argument to specify where the estimates
> > will be placed and adjust 'ylim' so give you enough space to leave gaps
> > for headings and the subgroup estimates. Then fit models within the
> > subgroups (the 'subset' argument is useful here) and use addpoly() to
> add
> > the subgroup estimates in the appropriate rows. With text(), you can
> add
> > headings as needed.
> >
> > If you use weights() on each subgroup model object, you can get the
> > subgroup weights (that add up to 100% within each subgroup). It's
> > probably easiest to just add those values with text() in an appropriate
> > place to the plot.
> >
> > Best,
> > Wolfgang
> >
> > --
> > Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry
> and
> > Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200
> MD
> > Maastricht, The Netherlands | +31 (43) 388-4170 |
> http://www.wvbauer.com
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John
> > > Peterson
> > > Sent: Monday, December 07, 2015 01:39
> > > To: r-help at r-project.org
> > > Subject: [R] metafor package
> > >
> > > Hi Everyone,
> > >
> > > I am conducting a meta-analysis using the metafor package. I am
> > > interested
> > > in obtaining an estimate by subgroup only without showing an overall
> > > effect. This is directly from the metafor website. How would i modify
> > > this
> > > code to only show subgroup effects? Further, I want to show weights
> by
> > > subgroup. The option showweights=TRUE does not display weights by
> > > subgroup
> > > but by the weight of each study in comparison to all studies (and not
> > the
> > > subgroup). You help would be appreciated.
> 
> [snip garbled code]

From Marios.BARLAS at cea.fr  Tue Dec  8 12:29:57 2015
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Tue, 8 Dec 2015 11:29:57 +0000
Subject: [R] Importing data by targeting in filenames inside a nested list
Message-ID: <01BFC0B2B4ABFC4CB432008F852D76616CC1A8@EXDAG0-A1.intra.cea.fr>

Hello everyone,

So, rookie me is trying to write a smart code, so here's what I'm doing:

I have a list of a couple of hundrend files, some of which refer to different experiments. 
The naming of the file refers to the experiment and the serial number to the topological reference on my sample.

Performing my data analysis in 1 file class at a time looks OK, so I'm trying to generalize my code.
I figured out I could group them together by performing a pattern read on a nested list, which works for getting the file names and grouping them but then I'm getting some problems when I try to perform the import from the nested list. My code looks like this:

# Vector containing file name patterns to be read and grouped together
measurement_filenames <- c("*Q_Read_prist*", "*Quasi_Forming*", "*read_set#1*","*Q_Reset_pForm#1*","*read_reset#2*","*quasistatic_set*", "*read_set#2*", "*quasistatic_reset#2*" )

# Create a list of the files to be read in sorted in a natural fashion
electrical_meas_files <- lapply(measurement_filenames, function(x) naturalsort(list.files(path, pattern=x)))
names(electrical_meas_files) <- measurement_filenames

# Perform data import for each element

for(i in 1:length(measurement_filenames))
{
  electrical_meas_raw_data[[i]] <- lapply(electrical_meas_files[[i]], function(x) read.xlsx(file=x, sheetName="Data", header=TRUE, as.data.frame =TRUE,  stringsAsFactors = F) )
}



My idea is to come up with a nested list of the structure 
{list of different experiments}
	{list of all sites where the experiment was run}
		{set of dataframes with all data for each site}


Do I make sense or am I over-complicating the situation ?


Any ideas how I could write this piece of code or improve it ?

Thanks in advance,
Mario


From john.peterson.can at gmail.com  Tue Dec  8 14:24:55 2015
From: john.peterson.can at gmail.com (John Peterson)
Date: Tue, 8 Dec 2015 08:24:55 -0500
Subject: [R] metafor package
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730F24C4663D@UM-MAIL4112.unimaas.nl>
References: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C463BF@UM-MAIL4112.unimaas.nl>
	<CAMYNiDo5fyXH+zBMX+Hf+NJgB5QQSQNLtQKhdruwDP8Rg-3BFw@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C4663D@UM-MAIL4112.unimaas.nl>
Message-ID: <CAMYNiDqL0NeaS2YAfCPvMm5FF0YpDdMECrevncfnLaZrTJQa3w@mail.gmail.com>

Hi Dr. Viechtbauer,

The code provided in the metafor projects website for subgroup includes
fitting a random effects model on the entire dataset and fitting a random
effects model within subgroups. When I exactly follow this code, my
estimates and confidence intervals for estimate within each subgroup
matches with what I get in STATA so it seems to be the correct estimate
(and CI). However, I don't want to present an overall effect and I want to
present only the effect within each subgroup. In my second attempt, I did
not run a random effects model within the entire dataset and only ran the
models within each subgroup. I generated a variable corresponding to the
estimate(risk ratio) and standard error  which I plugged in the forest()
function (i.e. forest(rr, se, .....).  When I run this code, the estimate I
get for each subgroup is slightly different than the estimate I get for
each subgroup in comparison to when I also included the random effects
model for the overall effect. (i.e. my first attempt). Is this the right
approach? I was not clear when you said passing the estimates and sampling
variances to the forest() function. I created a variable corresponding to
the estimate and standard error and plugged those in the forest() function.
I am not sure if this is the right approach. Thanks,

John

On 8 December 2015 at 03:47, Viechtbauer Wolfgang (STAT) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> Hi John,
>
> Please keep r-help copied on the reply.
>
> What's the 'previous model'? How do you get estimates within subgroups
> that 'includes the overall effect'? I really cannot follow you here.
>
> Best,
> Wolfgang
>
> --
> Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
>
> > -----Original Message-----
> > From: John Peterson [mailto:john.peterson.can at gmail.com]
> > Sent: Monday, December 07, 2015 22:14
> > To: Viechtbauer Wolfgang (STAT)
> > Subject: Re: [R] metafor package
> >
> > Hi Dr. Viechtbauer,
> > Thank you very much for your reply. I tried your advice and was able to
> > make a forest plot with only the estimates for each subgroup.For the
> > estiamte for each subgroup, similar to the previous model, I random
> > effects model within each subgroup. However, I now find the result for
> > the estiamte within subgroup to be different thant the result for the
> > previous model. I have tried analyzing this in STATA and I get the same
> > result as the model which includes the overall effect. Any advice on what
> > may be wrong here? Thanks greatly,
> > John
> >
> > On 7 December 2015 at 04:02, Viechtbauer Wolfgang (STAT)
> > <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> > The code you posted is totally mangled up, but it's just what can be
> > found here:
> >
> > http://www.metafor-project.org/doku.php/plots:forest_plot_with_subgroups
> >
> > If you don't want an overall estimate, just pass the estimates and
> > corresponding sampling variances to the forest() function (and not the
> > model object). Use the 'rows' argument to specify where the estimates
> > will be placed and adjust 'ylim' so give you enough space to leave gaps
> > for headings and the subgroup estimates. Then fit models within the
> > subgroups (the 'subset' argument is useful here) and use addpoly() to add
> > the subgroup estimates in the appropriate rows. With text(), you can add
> > headings as needed.
> >
> > If you use weights() on each subgroup model object, you can get the
> > subgroup weights (that add up to 100% within each subgroup). It's
> > probably easiest to just add those values with text() in an appropriate
> > place to the plot.
> >
> > Best,
> > Wolfgang
> >
> > --
> > Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> > Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> > Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
> >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John
> > > Peterson
> > > Sent: Monday, December 07, 2015 01:39
> > > To: r-help at r-project.org
> > > Subject: [R] metafor package
> > >
> > > Hi Everyone,
> > >
> > > I am conducting a meta-analysis using the metafor package. I am
> > > interested
> > > in obtaining an estimate by subgroup only without showing an overall
> > > effect. This is directly from the metafor website. How would i modify
> > > this
> > > code to only show subgroup effects? Further, I want to show weights by
> > > subgroup. The option showweights=TRUE does not display weights by
> > > subgroup
> > > but by the weight of each study in comparison to all studies (and not
> > the
> > > subgroup). You help would be appreciated.
>
> [snip garbled code]
>
>

	[[alternative HTML version deleted]]


From pnsinha68 at gmail.com  Tue Dec  8 15:16:51 2015
From: pnsinha68 at gmail.com (Partha Sinha)
Date: Tue, 8 Dec 2015 19:46:51 +0530
Subject: [R] Error while using e1071 package
Message-ID: <CADcgpJdcZraVCh0UGBzmYqQDXJr5YO5r1JP6PsTaxHYyGMkTVQ@mail.gmail.com>

I am using R 3.2.2 on win-7

while using predict function with e1071 (naive bayes classifier)
I am getting the following error
"Error in object$tables[[v]][, nd] : subscript out of bounds"


pl help.
regards
Parth


From ivan.calandra at univ-reims.fr  Tue Dec  8 15:47:55 2015
From: ivan.calandra at univ-reims.fr (Ivan Calandra)
Date: Tue, 8 Dec 2015 15:47:55 +0100
Subject: [R] Importing data by targeting in filenames inside a nested
	list
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CC1A8@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CC1A8@EXDAG0-A1.intra.cea.fr>
Message-ID: <5666ED9B.5040301@univ-reims.fr>

Hi Mario,

It is at the limit of my skills so I'm not sure this will be a real 
solution. But it might help you anyway (and there will be more competent 
people anyway).

What does not make sense to me is why you set the names of your files 
and then use them to list them with list.files()
What don't you just do
electrical_meas_files <- list.files(path, pattern)
and define a pattern if you don't want to read all of them?

For the next step, I don't think you need to lapply() at all; it is 
already in the loop. I usually prefer loops to lapply() because I find 
it more intuitive. But your lapply() solution would work as well.

So I think that your code is somewhat redundant (but I might have missed 
something). This should do it:

#first define your output data list that will be iteratively filled, 
this will increase speed
electrical_meas_raw_data <- vector(mode="list", 
length=length(electrical_meas_files))

#then read the files (seq_along() is great)
for(i in seq_along(measurement_filenames)){
     electrical_meas_raw_data[[i]] <- 
read.xlsx(file=electrical_meas_files[[i]], sheetName="Data", 
header=TRUE, as.data.frame =TRUE, stringsAsFactors = F)
}

HTH,
Ivan

--
Ivan Calandra, PhD
University of Reims Champagne-Ardenne
GEGENAA - EA 3795
CREA - 2 esplanade Roland Garros
51100 Reims, France
+33(0)3 26 77 36 89
ivan.calandra at univ-reims.fr
https://www.researchgate.net/profile/Ivan_Calandra

Le 08/12/15 12:29, BARLAS Marios 247554 a ?crit :
> Hello everyone,
>
> So, rookie me is trying to write a smart code, so here's what I'm doing:
>
> I have a list of a couple of hundrend files, some of which refer to different experiments.
> The naming of the file refers to the experiment and the serial number to the topological reference on my sample.
>
> Performing my data analysis in 1 file class at a time looks OK, so I'm trying to generalize my code.
> I figured out I could group them together by performing a pattern read on a nested list, which works for getting the file names and grouping them but then I'm getting some problems when I try to perform the import from the nested list. My code looks like this:
>
> # Vector containing file name patterns to be read and grouped together
> measurement_filenames <- c("*Q_Read_prist*", "*Quasi_Forming*", "*read_set#1*","*Q_Reset_pForm#1*","*read_reset#2*","*quasistatic_set*", "*read_set#2*", "*quasistatic_reset#2*" )
>
> # Create a list of the files to be read in sorted in a natural fashion
> electrical_meas_files <- lapply(measurement_filenames, function(x) naturalsort(list.files(path, pattern=x)))
> names(electrical_meas_files) <- measurement_filenames
>
> # Perform data import for each element
>
> for(i in 1:length(measurement_filenames))
> {
>    electrical_meas_raw_data[[i]] <- lapply(electrical_meas_files[[i]], function(x) read.xlsx(file=x, sheetName="Data", header=TRUE, as.data.frame =TRUE,  stringsAsFactors = F) )
> }
>
>
>
> My idea is to come up with a nested list of the structure
> {list of different experiments}
> 	{list of all sites where the experiment was run}
> 		{set of dataframes with all data for each site}
>
>
> Do I make sense or am I over-complicating the situation ?
>
>
> Any ideas how I could write this piece of code or improve it ?
>
> Thanks in advance,
> Mario
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From evan.cooch at gmail.com  Tue Dec  8 15:29:35 2015
From: evan.cooch at gmail.com (Evan Cooch)
Date: Tue, 8 Dec 2015 09:29:35 -0500
Subject: [R] column means dropping column minimum
Message-ID: <5666E94F.2010500@gmail.com>

Suppose I have something like the following dataframe:

samp1 <- c(60,50,20,90)
samp2 <- c(60,60,90,58)
samp3 <- c(25,65,65,90)

test <- data.frame(samp1,samp2,samp3)

I want to calculate column means. Easy enough, if I want to use all the 
data within each column:


print(colMeans(test),na.rm = TRUE)


However, I'm danged if I can figure out how to do the same thing after 
dropping the minimum value for each column. For example, column 1 in the 
dataframe test consists of 60, 50,20,90. I want to calculate the mean 
over (60,50,90), dropping the minimum value (20). Figuring out what the 
minimum value is in a single column is easy, but I can't figure out how 
to arm-twist colMeans into 'applying itself' to the elements of a column 
greater than the minimum, for each column in turn. I've tried 
permutations of select, subset etc., to no avail. Only thing I can think 
of is to (i) find the minimum in a column, (ii) change it to NA, and 
then (iii) tell colMeans to na.rm = TRUE):

test2 <- test

for (i in 1:ncol(test)) { test2[which.min(test[,i]),i]==NA}

print(test2)

print(colMeans(test2),na.rm = TRUE)


While this works, seems awfully 'clunky' -- is there a better way?

Thanks in advance...


From phgrosjean at sciviews.org  Tue Dec  8 19:53:24 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Tue, 8 Dec 2015 19:53:24 +0100
Subject: [R] column means dropping column minimum
In-Reply-To: <5666E94F.2010500@gmail.com>
References: <5666E94F.2010500@gmail.com>
Message-ID: <CF5C3686-E6DE-478A-BA11-E042A158379F@sciviews.org>

There is a bug in your code: it is not

> for (i in 1:ncol(test)) { test2[which.min(test[,i]),i]==NA}

but

for (i in 1:ncos(test)) {
  test2[which.min(test[, i]), i] <- NA
}

Otherwise, a solution would be to create your own function to compute the mean of a vector without the smallest value:

meanNoMin <- function (x, na.rm = FALSE)
  mean(sort(x)[-1], na.rm = na.rm)

? and then, to apply it to all columns of your data frame:

sapply(test, meanNoMin, na.rm = TRUE)

If you need faster code, you may want to look at Rcpp and a C++ version of the previous code, but it is much more work.
Best,

Philippe Grosjean


> On 08 Dec 2015, at 15:29, Evan Cooch <evan.cooch at gmail.com> wrote:
> 
> Suppose I have something like the following dataframe:
> 
> samp1 <- c(60,50,20,90)
> samp2 <- c(60,60,90,58)
> samp3 <- c(25,65,65,90)
> 
> test <- data.frame(samp1,samp2,samp3)
> 
> I want to calculate column means. Easy enough, if I want to use all the data within each column:
> 
> 
> print(colMeans(test),na.rm = TRUE)
> 
> 
> However, I'm danged if I can figure out how to do the same thing after dropping the minimum value for each column. For example, column 1 in the dataframe test consists of 60, 50,20,90. I want to calculate the mean over (60,50,90), dropping the minimum value (20). Figuring out what the minimum value is in a single column is easy, but I can't figure out how to arm-twist colMeans into 'applying itself' to the elements of a column greater than the minimum, for each column in turn. I've tried permutations of select, subset etc., to no avail. Only thing I can think of is to (i) find the minimum in a column, (ii) change it to NA, and then (iii) tell colMeans to na.rm = TRUE):
> 
> test2 <- test
> 
> for (i in 1:ncol(test)) { test2[which.min(test[,i]),i]==NA}
> 
> print(test2)
> 
> print(colMeans(test2),na.rm = TRUE)
> 
> 
> While this works, seems awfully 'clunky' -- is there a better way?
> 
> Thanks in advance...
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Tue Dec  8 19:57:47 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 8 Dec 2015 18:57:47 +0000
Subject: [R] column means dropping column minimum
Message-ID: <248E6FA047A8C746BA491485764190F52663FB2F@ESESSMB207.ericsson.se>

First, your code has flaws in the assignment of NA and in passing na.rm=TRUE to colMeans().

It should be:

test2 <- test
for (i in 1:ncol(test)) { test2[which.min(test[,i]),i]=NA}
print(test2)


samp1 samp2 samp3

1    60    60    NA

2    50    60    65

3    NA    90    65

4    90    NA    90



print(colMeans(test2,na.rm = TRUE))

   samp1    samp2    samp3

66.66667 70.00000 73.33333

For your purpose, I suggest the following:

apply(test, 2, function(x) { mean(x[-which.min(x)])})


GG



	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Dec  8 20:23:22 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 8 Dec 2015 19:23:22 +0000
Subject: [R] column means dropping column minimum
In-Reply-To: <248E6FA047A8C746BA491485764190F52663FB2F@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F52663FB2F@ESESSMB207.ericsson.se>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E778E@mb02.ads.tamu.edu>

The which.min() only gets the first minimum value. If two or more values are tied for the minimum, it will delete only the first one. This would get them all:

> apply(test, 2, function(x) mean(x[-which(x == min(x))]))
   samp1    samp2    samp3 
66.66667 70.00000 73.33333

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Giorgio Garziano
Sent: Tuesday, December 8, 2015 12:58 PM
To: r-help at r-project.org
Subject: Re: [R] column means dropping column minimum

First, your code has flaws in the assignment of NA and in passing na.rm=TRUE to colMeans().

It should be:

test2 <- test
for (i in 1:ncol(test)) { test2[which.min(test[,i]),i]=NA}
print(test2)


samp1 samp2 samp3

1    60    60    NA

2    50    60    65

3    NA    90    65

4    90    NA    90



print(colMeans(test2,na.rm = TRUE))

   samp1    samp2    samp3

66.66667 70.00000 73.33333

For your purpose, I suggest the following:

apply(test, 2, function(x) { mean(x[-which.min(x)])})


GG



	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From hannah.hlx at gmail.com  Tue Dec  8 20:32:06 2015
From: hannah.hlx at gmail.com (li li)
Date: Tue, 8 Dec 2015 14:32:06 -0500
Subject: [R] Error in loading the drc package
Message-ID: <CAHLnndZno9TvV_7n_ziVe6=se5qPiZMPq6Wo4FYruO6tyqK-OA@mail.gmail.com>

Hi all,
  When trying to load the drc package. I got the following error. Any
suggestions?
  Thanks.
     Hanna


> install.packages("drc", dependencies=TRUE)
--- Please select a CRAN mirror for use in this session ---
trying URL 'https://cran.fhcrc.org/bin/windows/contrib/3.1/drc_2.5-12.zip'
Content type 'application/zip' length 502776 bytes (490 Kb)
opened URL
downloaded 490 Kb
package ?drc? successfully unpacked and MD5 sums checked
The downloaded binary packages are in
        C:\Users\Temp\Rtmpysroid\downloaded_packages

> library(drc)
Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck
= vI[[j]]) :
  namespace ?Matrix? 1.1-3 is already loaded, but >= 1.1.5 is required
In addition: Warning message:
package ?drc? was built under R version 3.1.3
Error: package or namespace load failed for ?drc?
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Dec  8 20:44:06 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 9 Dec 2015 08:44:06 +1300
Subject: [R] [FORGED]  Error in loading the drc package
In-Reply-To: <CAHLnndZno9TvV_7n_ziVe6=se5qPiZMPq6Wo4FYruO6tyqK-OA@mail.gmail.com>
References: <CAHLnndZno9TvV_7n_ziVe6=se5qPiZMPq6Wo4FYruO6tyqK-OA@mail.gmail.com>
Message-ID: <56673306.2030909@auckland.ac.nz>

On 09/12/15 08:32, li li wrote:
> Hi all,
>    When trying to load the drc package. I got the following error. Any
> suggestions?

Yes.  Read the error message

cheers,

Rolf Turner

>    Thanks.
>       Hanna
>
>
>> install.packages("drc", dependencies=TRUE)
> --- Please select a CRAN mirror for use in this session ---
> trying URL 'https://cran.fhcrc.org/bin/windows/contrib/3.1/drc_2.5-12.zip'
> Content type 'application/zip' length 502776 bytes (490 Kb)
> opened URL
> downloaded 490 Kb
> package ?drc? successfully unpacked and MD5 sums checked
> The downloaded binary packages are in
>          C:\Users\Temp\Rtmpysroid\downloaded_packages
>
>> library(drc)
> Error in loadNamespace(j <- i[[1L]], c(lib.loc, .libPaths()), versionCheck
> = vI[[j]]) :
>    namespace ?Matrix? 1.1-3 is already loaded, but >= 1.1.5 is required
> In addition: Warning message:
> package ?drc? was built under R version 3.1.3
> Error: package or namespace load failed for ?drc?


From r.turner at auckland.ac.nz  Tue Dec  8 21:20:14 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 9 Dec 2015 09:20:14 +1300
Subject: [R] [FORGED]  Error in loading the drc package
In-Reply-To: <CAHLnndaTh6ODRoHYA-BJ3XNbW1HVBpk6GHScxdSpyy=CDofw3w@mail.gmail.com>
References: <CAHLnndZno9TvV_7n_ziVe6=se5qPiZMPq6Wo4FYruO6tyqK-OA@mail.gmail.com>
	<56673306.2030909@auckland.ac.nz>
	<CAHLnndaTh6ODRoHYA-BJ3XNbW1HVBpk6GHScxdSpyy=CDofw3w@mail.gmail.com>
Message-ID: <56673B7E.9050006@auckland.ac.nz>


Please keep communications on-list.  Others may have relevant comments 
and suggestions to make.


On 09/12/15 09:00, li li wrote:
> Thanks for the reply. So a newer version of R can solve the problem? But
> I was able to successfully load the package yesterday.
> Thanks.

I have no access to your system so I cannot advise.  Apparently 
*something* changed with your system between sometime yesterday and 
whenever you got the error.  Only you are in a position to know what 
changed.

However that is probably irrelevant.  Just

   * update R
   * update (re-install) the drc package

and things should work without errors or warnings.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From hannah.hlx at gmail.com  Tue Dec  8 21:27:18 2015
From: hannah.hlx at gmail.com (li li)
Date: Tue, 8 Dec 2015 15:27:18 -0500
Subject: [R] [FORGED]  Error in loading the drc package
In-Reply-To: <56673B7E.9050006@auckland.ac.nz>
References: <CAHLnndZno9TvV_7n_ziVe6=se5qPiZMPq6Wo4FYruO6tyqK-OA@mail.gmail.com>
	<56673306.2030909@auckland.ac.nz>
	<CAHLnndaTh6ODRoHYA-BJ3XNbW1HVBpk6GHScxdSpyy=CDofw3w@mail.gmail.com>
	<56673B7E.9050006@auckland.ac.nz>
Message-ID: <CAHLnndYpCi8tCfekFmeKTLnZiLVedt_wxem9ty64W6AUz-h8FA@mail.gmail.com>

Thanks.

2015-12-08 15:20 GMT-05:00 Rolf Turner <r.turner at auckland.ac.nz>:

>
> Please keep communications on-list.  Others may have relevant comments and
> suggestions to make.
>
>
> On 09/12/15 09:00, li li wrote:
>
>> Thanks for the reply. So a newer version of R can solve the problem? But
>> I was able to successfully load the package yesterday.
>> Thanks.
>>
>
> I have no access to your system so I cannot advise.  Apparently
> *something* changed with your system between sometime yesterday and
> whenever you got the error.  Only you are in a position to know what
> changed.
>
> However that is probably irrelevant.  Just
>
>   * update R
>   * update (re-install) the drc package
>
> and things should work without errors or warnings.
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Tue Dec  8 22:24:26 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Dec 2015 08:24:26 +1100
Subject: [R] Error while using e1071 package
In-Reply-To: <CADcgpJdcZraVCh0UGBzmYqQDXJr5YO5r1JP6PsTaxHYyGMkTVQ@mail.gmail.com>
References: <CADcgpJdcZraVCh0UGBzmYqQDXJr5YO5r1JP6PsTaxHYyGMkTVQ@mail.gmail.com>
Message-ID: <CA+8X3fX2mE5OYuOXk=6BDo6H--7yue7ei9MYUzJPtTo-dYdbmQ@mail.gmail.com>

Hi Partha,
Probably the first thing to be done is to see what:

object$tables

really is. The error message tells you that either "v" has become larger
than the number of elements in the list/data frame "tables" or that "nd"
has become larger than the number of columns in the element "v" (or both).
Perhaps displaying the successive values as the code is run:

for(v in 1:<some value>) {
 cat("v =",v,"\n")
 for(nd in 1:<some other value>) {
  cat("nd =",nd,"\n")
  ...
 }
}

will tell you where the error is occurring.

Jim




On Wed, Dec 9, 2015 at 1:16 AM, Partha Sinha <pnsinha68 at gmail.com> wrote:

> I am using R 3.2.2 on win-7
>
> while using predict function with e1071 (naive bayes classifier)
> I am getting the following error
> "Error in object$tables[[v]][, nd] : subscript out of bounds"
>
>
> pl help.
> regards
> Parth
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From freddyeggleton at yahoo.co.uk  Tue Dec  8 21:32:46 2015
From: freddyeggleton at yahoo.co.uk (Freddy Eggleton)
Date: Tue, 8 Dec 2015 20:32:46 +0000
Subject: [R] tmap colour scale
Message-ID: <F52DE83E-188C-4146-A7DF-79DC44460BE0@yahoo.co.uk>

Hi,

I am trying to plot Conservative Vote % per borough for London using tmap however the legend colours boroughs with a high Conservative % as light blue and boroughs with a low % of conservative votes as dark blue. How do i reverse this scale so that areas with the most Conservative votes are darkest? 

my code is as follows;

tm_shape(lndbor) + tm_polygons("CON", style = "kmeans", palette = "-Blues", title = "Conservative Vote (%)")

Thanks,
 

From dimitri.liakhovitski at gmail.com  Tue Dec  8 23:30:42 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 8 Dec 2015 17:30:42 -0500
Subject: [R] Why mean is not working in by?
Message-ID: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>

Hello!
Could you please explain why the first 5 lines work but the last 2 lines don't?
Thank you!

by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
by(data = iris[myvars], INDICES = iris["Species"], FUN = min)

by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)

-- 
Dimitri Liakhovitski


From sarah.goslee at gmail.com  Tue Dec  8 23:50:54 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 8 Dec 2015 17:50:54 -0500
Subject: [R] Why mean is not working in by?
In-Reply-To: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
References: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
Message-ID: <CAM_vjukJGK_7qTa=wMVxrhq1uTgduDUhOvP0L8XwZtAr07zPsg@mail.gmail.com>

Hi Dimitri,

I changed this into a reproducible example (we don't know what myvars
is). Assuming length(myvars) > 1, I'm not convinced that your first
five lines "work" either: what do you expect?

I get:

> by(data = iris[, -5], INDICES = iris["Species"], FUN = min)
Species: setosa
[1] 0.1
------------------------------------------------------------------
Species: versicolor
[1] 1
------------------------------------------------------------------
Species: virginica
[1] 1.4

But was expecting:

> aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=min)
     Species Sepal.Length Sepal.Width Petal.Length Petal.Width
1     setosa          4.3         2.3          1.0         0.1
2 versicolor          4.9         2.0          3.0         1.0
3  virginica          4.9         2.2          4.5         1.4



aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=sd)
aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=mean)

provide the answers I would expect. If you want clearer advice, you
need to provide an actually reproducible example, and tell us more
about what you expect to get.

Sarah


On Tue, Dec 8, 2015 at 5:30 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
> Could you please explain why the first 5 lines work but the last 2 lines don't?
> Thank you!
>
> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>
> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>
> --
> Dimitri Liakhovitski
>


From drjimlemon at gmail.com  Wed Dec  9 00:02:13 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Dec 2015 10:02:13 +1100
Subject: [R] tmap colour scale
In-Reply-To: <F52DE83E-188C-4146-A7DF-79DC44460BE0@yahoo.co.uk>
References: <F52DE83E-188C-4146-A7DF-79DC44460BE0@yahoo.co.uk>
Message-ID: <CA+8X3fVUZEL+u4gyBXsdprt7ugj2=GAvFK76xYoB5T+jTsp4hQ@mail.gmail.com>

Hi Freddy,
Have you tried rev() on the palette?

Jim


On Wed, Dec 9, 2015 at 7:32 AM, Freddy Eggleton <freddyeggleton at yahoo.co.uk>
wrote:

> Hi,
>
> I am trying to plot Conservative Vote % per borough for London using tmap
> however the legend colours boroughs with a high Conservative % as light
> blue and boroughs with a low % of conservative votes as dark blue. How do i
> reverse this scale so that areas with the most Conservative votes are
> darkest?
>
> my code is as follows;
>
> tm_shape(lndbor) + tm_polygons("CON", style = "kmeans", palette =
> "-Blues", title = "Conservative Vote (%)")
>
> Thanks,
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Wed Dec  9 00:08:21 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 8 Dec 2015 18:08:21 -0500
Subject: [R] Why mean is not working in by?
In-Reply-To: <CAM_vjukJGK_7qTa=wMVxrhq1uTgduDUhOvP0L8XwZtAr07zPsg@mail.gmail.com>
References: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
	<CAM_vjukJGK_7qTa=wMVxrhq1uTgduDUhOvP0L8XwZtAr07zPsg@mail.gmail.com>
Message-ID: <CAN2xGJZAJCc2_gDFS6+BcVMGjdxWaqk3SVCWi8nX3A20+Sexcg@mail.gmail.com>

Sorry, I omitted the first line:

myvars <- c("Sepal.Length", "Sepal.Width")
by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
by(data = iris[myvars], INDICES = iris["Species"], FUN = min)

by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)

The first lines are doing what I expected them to do: for each level
of the factor "Species" they gave me a summary, a sum, a variance, a
max, a min for each of the 2 variables in question (myvars).
I expected by to generate the sd and the mean for the 2 variables in
question for each level of "Species".

On Tue, Dec 8, 2015 at 5:50 PM, Sarah Goslee <sarah.goslee at gmail.com> wrote:
> Hi Dimitri,
>
> I changed this into a reproducible example (we don't know what myvars
> is). Assuming length(myvars) > 1, I'm not convinced that your first
> five lines "work" either: what do you expect?
>
> I get:
>
>> by(data = iris[, -5], INDICES = iris["Species"], FUN = min)
> Species: setosa
> [1] 0.1
> ------------------------------------------------------------------
> Species: versicolor
> [1] 1
> ------------------------------------------------------------------
> Species: virginica
> [1] 1.4
>
> But was expecting:
>
>> aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=min)
>      Species Sepal.Length Sepal.Width Petal.Length Petal.Width
> 1     setosa          4.3         2.3          1.0         0.1
> 2 versicolor          4.9         2.0          3.0         1.0
> 3  virginica          4.9         2.2          4.5         1.4
>
>
>
> aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=sd)
> aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=mean)
>
> provide the answers I would expect. If you want clearer advice, you
> need to provide an actually reproducible example, and tell us more
> about what you expect to get.
>
> Sarah
>
>
> On Tue, Dec 8, 2015 at 5:30 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Hello!
>> Could you please explain why the first 5 lines work but the last 2 lines don't?
>> Thank you!
>>
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>>
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>>
>> --
>> Dimitri Liakhovitski
>>



-- 
Dimitri Liakhovitski


From bgunter.4567 at gmail.com  Wed Dec  9 00:09:04 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 8 Dec 2015 15:09:04 -0800
Subject: [R] Why mean is not working in by?
In-Reply-To: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
References: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
Message-ID: <CAGxFJbRixw9wWcjTWxMBj+C3LoornnJ=BwBH5gphN97PjZw8zw@mail.gmail.com>

Because you are using by() incorrectly.

"A data frame is split by row into **data frames**  subsetted by the
values of one or more factors, and function FUN is applied to each
subset in turn."

So your FUN is applied to a subset of the data frame (which is also a
list). Note that sum, min, and max have  "..." as their initial
arguments and so use all the columns in the data frame of each subset
for .... var() takes the covariance matrix of the several columns and
summary.data.frame summarizes each column. mean() and sd() must be fed
a numeric vector as their first argument, which a data frame is not --
ergo the error.

Cheers,
Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Dec 8, 2015 at 2:30 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Hello!
> Could you please explain why the first 5 lines work but the last 2 lines don't?
> Thank you!
>
> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>
> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Wed Dec  9 00:17:57 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 8 Dec 2015 15:17:57 -0800
Subject: [R] Why mean is not working in by?
In-Reply-To: <CAN2xGJZAJCc2_gDFS6+BcVMGjdxWaqk3SVCWi8nX3A20+Sexcg@mail.gmail.com>
References: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
	<CAM_vjukJGK_7qTa=wMVxrhq1uTgduDUhOvP0L8XwZtAr07zPsg@mail.gmail.com>
	<CAN2xGJZAJCc2_gDFS6+BcVMGjdxWaqk3SVCWi8nX3A20+Sexcg@mail.gmail.com>
Message-ID: <CAF8bMcZdQnH_a4=x+3N-fynPm3R9cFgC5akahmqz0op4VQ9HdQ@mail.gmail.com>

by() calls FUN with a data.frame as the argument.  summary(), sum(), etc.
have methods that work on data.frames but sd() and mean() do not.

aggregate() calls its FUN with each column of a data.frame as the argument.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Dec 8, 2015 at 3:08 PM, Dimitri Liakhovitski <
dimitri.liakhovitski at gmail.com> wrote:

> Sorry, I omitted the first line:
>
> myvars <- c("Sepal.Length", "Sepal.Width")
> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>
> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>
> The first lines are doing what I expected them to do: for each level
> of the factor "Species" they gave me a summary, a sum, a variance, a
> max, a min for each of the 2 variables in question (myvars).
> I expected by to generate the sd and the mean for the 2 variables in
> question for each level of "Species".
>
> On Tue, Dec 8, 2015 at 5:50 PM, Sarah Goslee <sarah.goslee at gmail.com>
> wrote:
> > Hi Dimitri,
> >
> > I changed this into a reproducible example (we don't know what myvars
> > is). Assuming length(myvars) > 1, I'm not convinced that your first
> > five lines "work" either: what do you expect?
> >
> > I get:
> >
> >> by(data = iris[, -5], INDICES = iris["Species"], FUN = min)
> > Species: setosa
> > [1] 0.1
> > ------------------------------------------------------------------
> > Species: versicolor
> > [1] 1
> > ------------------------------------------------------------------
> > Species: virginica
> > [1] 1.4
> >
> > But was expecting:
> >
> >> aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=min)
> >      Species Sepal.Length Sepal.Width Petal.Length Petal.Width
> > 1     setosa          4.3         2.3          1.0         0.1
> > 2 versicolor          4.9         2.0          3.0         1.0
> > 3  virginica          4.9         2.2          4.5         1.4
> >
> >
> >
> > aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=sd)
> > aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=mean)
> >
> > provide the answers I would expect. If you want clearer advice, you
> > need to provide an actually reproducible example, and tell us more
> > about what you expect to get.
> >
> > Sarah
> >
> >
> > On Tue, Dec 8, 2015 at 5:30 PM, Dimitri Liakhovitski
> > <dimitri.liakhovitski at gmail.com> wrote:
> >> Hello!
> >> Could you please explain why the first 5 lines work but the last 2
> lines don't?
> >> Thank you!
> >>
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
> >>
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
> >>
> >> --
> >> Dimitri Liakhovitski
> >>
>
>
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Wed Dec  9 00:18:54 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 8 Dec 2015 18:18:54 -0500
Subject: [R] Why mean is not working in by?
In-Reply-To: <CAF8bMcZdQnH_a4=x+3N-fynPm3R9cFgC5akahmqz0op4VQ9HdQ@mail.gmail.com>
References: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
	<CAM_vjukJGK_7qTa=wMVxrhq1uTgduDUhOvP0L8XwZtAr07zPsg@mail.gmail.com>
	<CAN2xGJZAJCc2_gDFS6+BcVMGjdxWaqk3SVCWi8nX3A20+Sexcg@mail.gmail.com>
	<CAF8bMcZdQnH_a4=x+3N-fynPm3R9cFgC5akahmqz0op4VQ9HdQ@mail.gmail.com>
Message-ID: <CAN2xGJammEcA2iq0dgCDzZ0KGTL7NAgC+_qFY48syJaMzsfYyA@mail.gmail.com>

Got it - thank you, everybody!
by splits it into data frames.
Lesson: use aggregate.

On Tue, Dec 8, 2015 at 6:17 PM, William Dunlap <wdunlap at tibco.com> wrote:
> by() calls FUN with a data.frame as the argument.  summary(), sum(), etc.
> have methods that work on data.frames but sd() and mean() do not.
>
> aggregate() calls its FUN with each column of a data.frame as the argument.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Tue, Dec 8, 2015 at 3:08 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>>
>> Sorry, I omitted the first line:
>>
>> myvars <- c("Sepal.Length", "Sepal.Width")
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>>
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>>
>> The first lines are doing what I expected them to do: for each level
>> of the factor "Species" they gave me a summary, a sum, a variance, a
>> max, a min for each of the 2 variables in question (myvars).
>> I expected by to generate the sd and the mean for the 2 variables in
>> question for each level of "Species".
>>
>> On Tue, Dec 8, 2015 at 5:50 PM, Sarah Goslee <sarah.goslee at gmail.com>
>> wrote:
>> > Hi Dimitri,
>> >
>> > I changed this into a reproducible example (we don't know what myvars
>> > is). Assuming length(myvars) > 1, I'm not convinced that your first
>> > five lines "work" either: what do you expect?
>> >
>> > I get:
>> >
>> >> by(data = iris[, -5], INDICES = iris["Species"], FUN = min)
>> > Species: setosa
>> > [1] 0.1
>> > ------------------------------------------------------------------
>> > Species: versicolor
>> > [1] 1
>> > ------------------------------------------------------------------
>> > Species: virginica
>> > [1] 1.4
>> >
>> > But was expecting:
>> >
>> >> aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=min)
>> >      Species Sepal.Length Sepal.Width Petal.Length Petal.Width
>> > 1     setosa          4.3         2.3          1.0         0.1
>> > 2 versicolor          4.9         2.0          3.0         1.0
>> > 3  virginica          4.9         2.2          4.5         1.4
>> >
>> >
>> >
>> > aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=sd)
>> > aggregate(iris[,-5], by=iris[,"Species", drop=FALSE], FUN=mean)
>> >
>> > provide the answers I would expect. If you want clearer advice, you
>> > need to provide an actually reproducible example, and tell us more
>> > about what you expect to get.
>> >
>> > Sarah
>> >
>> >
>> > On Tue, Dec 8, 2015 at 5:30 PM, Dimitri Liakhovitski
>> > <dimitri.liakhovitski at gmail.com> wrote:
>> >> Hello!
>> >> Could you please explain why the first 5 lines work but the last 2
>> >> lines don't?
>> >> Thank you!
>> >>
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>> >>
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
>> >> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>> >>
>> >> --
>> >> Dimitri Liakhovitski
>> >>
>>
>>
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Dimitri Liakhovitski


From bgunter.4567 at gmail.com  Wed Dec  9 00:19:02 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 8 Dec 2015 15:19:02 -0800
Subject: [R] Why mean is not working in by?
In-Reply-To: <CAGxFJbRixw9wWcjTWxMBj+C3LoornnJ=BwBH5gphN97PjZw8zw@mail.gmail.com>
References: <CAN2xGJbTdy6Q4hi8LnhPiFrDcbgVQYu72VMg7PBjjJPMSCZ8Sg@mail.gmail.com>
	<CAGxFJbRixw9wWcjTWxMBj+C3LoornnJ=BwBH5gphN97PjZw8zw@mail.gmail.com>
Message-ID: <CAGxFJbSdfbm_V2efar1XgUDMvKfsjkvY2VWGvRju6_UXEXdtVQ@mail.gmail.com>

Sarah:

Note that (as I read them) aggregate() and by() work differently on
data frames. aggregate() computes FUN column by column while by()
feeds the whole (subset) data frame to FUN.

If I am wrong about this, I would greatly appreciate being corrected.

Cheers,
Bert




Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Dec 8, 2015 at 3:09 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Because you are using by() incorrectly.
>
> "A data frame is split by row into **data frames**  subsetted by the
> values of one or more factors, and function FUN is applied to each
> subset in turn."
>
> So your FUN is applied to a subset of the data frame (which is also a
> list). Note that sum, min, and max have  "..." as their initial
> arguments and so use all the columns in the data frame of each subset
> for .... var() takes the covariance matrix of the several columns and
> summary.data.frame summarizes each column. mean() and sd() must be fed
> a numeric vector as their first argument, which a data frame is not --
> ergo the error.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>    -- Clifford Stoll
>
>
> On Tue, Dec 8, 2015 at 2:30 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> Hello!
>> Could you please explain why the first 5 lines work but the last 2 lines don't?
>> Thank you!
>>
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = summary)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = sum)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = var)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = max)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = min)
>>
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = sd)
>> by(data = iris[myvars], INDICES = iris["Species"], FUN = mean)
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From klebyn at yahoo.com.br  Wed Dec  9 01:37:42 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Tue, 8 Dec 2015 22:37:42 -0200
Subject: [R] [tcltk][tktable] How to make an efficient data transfer from R
 to Tcl?
Message-ID: <566777D6.6090501@yahoo.com.br>

hello all,
I intend transfer a big data.frame, more than 1e4 rows, more than 100 
columns...
I found solutions (in internet and help pages) for small data.frame like 
the showed bellow.
Big data.frames is very expensive in computation time in my approach
I would like to optimize this transfer anyway ...
I saw a function named "as.tclObj" that seems to me a promising form but 
hard to code...
Somebody know a way more clever to do it?

thanks for tips
cleber

##########################################
library( tcltk )
nrow <- 10 ; ncol <- 100
# simulated data: data is originally a data frame
DF <- as.data.frame(matrix(round(runif(nrow*ncol),3),nr=nrow,nc=ncol))
######
# DF_tcl <- as.tclObj( as.matrix( DF ) )

tableDataTclArray <- tclArray()
for( i in 1:nrow( DF ) )
     tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "', 
rownames( DF )[ i ] , '"') ) )

for( i in 1:ncol( DF ) )
     tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "', 
colnames( DF )[ i ] , '"') ) )


for( i in 1:nrow( DF ) )
     for( j in 1:ncol( DF ) )
         tableDataTclArray[[ i, j ]] <- DF[i,j]
######





##############################################
##############################################
##############################################
# my full code:
###############
library( tcltk ); tclRequire("Tktable")
wDataOrg <- tktoplevel()
tcl('wm', 'geometry',  wDataOrg, "1024x600+175+80" )
fmTableData <- ttklabelframe( wDataOrg, text='  Data  ', padding=c( 8, 
5, 8, 15 ) )
height <- -1; width <- -1
nrowDefault <- 1e4+1; ncolDefault <- 100+1
tableDataTclArray <- tclArray()
tableData <- tkwidget( fmTableData, "table", rows=nrowDefault, 
cols=ncolDefault,
                      height=height+1,width=width+1,
                      xscrollcommand=function(...) tkset( scrX, ... ),
                      yscrollcommand=function(...) tkset( scrY, ... ),
                      rowstretchmode='unset', colstretchmode='unset', 
multiline=0, cache=1,
                      background="white", selectmode="extended", 
selecttitle=0,
                      font='{Arial} 10', ipadx=5, ipady=5, anchor='e',
                      borderwidth=c( 0, 1, 0, 1 ), drawmode='slow',
                      variable=tableDataTclArray )
tcl( tableData, "tag", "celltag", "ZeroZero", "0,0" )
tcl( tableData, "tag", "rowtag",  "rowtitle", "0" )
tcl( tableData, "tag", "coltag",  "coltitle", "0" )
tcl( tableData, "tag", "configure", "ZeroZero", bg='SystemButtonFace', 
relief="raised", state='disabled' )
tcl( tableData, "tag", "configure", "rowtitle", bg='lightgreen', 
relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='center' )
tcl( tableData, "tag", "configure", "coltitle", bg='lightgreen', 
relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='w' )
tcl( tableData, "tag", "configure", "active", fg='yellow', bg='red' )
tcl( tableData, "width", "0", "20" )
scrX <- tkscrollbar( fmTableData, orient="horizontal", 
command=function(...) tkxview( tableData, ... ) )
scrY <- tkscrollbar( fmTableData, orient="vertical", 
command=function(...) tkyview( tableData, ... ) )
scrYwidth <- as.integer( tkcget( scrY, width = NULL ) )
tcl( "pack", scrY, side = "right",  fill = "y", expand = FALSE, pady = 
c(0, scrYwidth + 2) )
tcl( "pack", scrX, side = "bottom", fill = "x", expand = FALSE )
tcl( "pack", tableData, anchor='n', side="right", expand=TRUE, 
fill="both"  )
tcl( "pack", fmTableData, fill="both", expand = TRUE, padx=10, pady=10 )
##########################################  expensive time computation 
[[elided Yahoo spam]]
DF <- as.data.frame( matrix( round(runif(1e4 * 100),3), nr=1e4, nc=100 ) 
) #### simulated data
for( i in 1:nrow( DF ) ){
     tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "', 
rownames( DF )[ i ] , '"') ) )
     }
for( i in 1:ncol( DF ) ){
     tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "', 
colnames( DF )[ i ] , '"') ) )
     }
for( i in 1:nrow( DF ) ) {
     for( j in 1:ncol( DF ) ){
         tableDataTclArray[[ i, j ]] <- DF[i,j]
     }
}
#


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From bgunter.4567 at gmail.com  Wed Dec  9 01:51:30 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 8 Dec 2015 16:51:30 -0800
Subject: [R] [tcltk][tktable] How to make an efficient data transfer
 from R to Tcl?
In-Reply-To: <566777D6.6090501@yahoo.com.br>
References: <566777D6.6090501@yahoo.com.br>
Message-ID: <CAGxFJbSrG7U9qkhgnANQ5tdviDtRf6q6winypQJ3ETXso4fe_A@mail.gmail.com>

Define: "transfer"

(  save/load should be efficient and fast within R, but you appear to
have something else in mind. What?)

Apologies if it's obvious and I just don't get it.

Cheers,

Bert


Bert Gunter

"Data is not information. Information is not knowledge. And knowledge
is certainly not wisdom."
   -- Clifford Stoll


On Tue, Dec 8, 2015 at 4:37 PM, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> hello all,
> I intend transfer a big data.frame, more than 1e4 rows, more than 100
> columns...
> I found solutions (in internet and help pages) for small data.frame like the
> showed bellow.
> Big data.frames is very expensive in computation time in my approach
> I would like to optimize this transfer anyway ...
> I saw a function named "as.tclObj" that seems to me a promising form but
> hard to code...
> Somebody know a way more clever to do it?
>
> thanks for tips
> cleber
>
> ##########################################
> library( tcltk )
> nrow <- 10 ; ncol <- 100
> # simulated data: data is originally a data frame
> DF <- as.data.frame(matrix(round(runif(nrow*ncol),3),nr=nrow,nc=ncol))
> ######
> # DF_tcl <- as.tclObj( as.matrix( DF ) )
>
> tableDataTclArray <- tclArray()
> for( i in 1:nrow( DF ) )
>     tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "',
> rownames( DF )[ i ] , '"') ) )
>
> for( i in 1:ncol( DF ) )
>     tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "',
> colnames( DF )[ i ] , '"') ) )
>
>
> for( i in 1:nrow( DF ) )
>     for( j in 1:ncol( DF ) )
>         tableDataTclArray[[ i, j ]] <- DF[i,j]
> ######
>
>
>
>
>
> ##############################################
> ##############################################
> ##############################################
> # my full code:
> ###############
> library( tcltk ); tclRequire("Tktable")
> wDataOrg <- tktoplevel()
> tcl('wm', 'geometry',  wDataOrg, "1024x600+175+80" )
> fmTableData <- ttklabelframe( wDataOrg, text='  Data  ', padding=c( 8, 5, 8,
> 15 ) )
> height <- -1; width <- -1
> nrowDefault <- 1e4+1; ncolDefault <- 100+1
> tableDataTclArray <- tclArray()
> tableData <- tkwidget( fmTableData, "table", rows=nrowDefault,
> cols=ncolDefault,
>                      height=height+1,width=width+1,
>                      xscrollcommand=function(...) tkset( scrX, ... ),
>                      yscrollcommand=function(...) tkset( scrY, ... ),
>                      rowstretchmode='unset', colstretchmode='unset',
> multiline=0, cache=1,
>                      background="white", selectmode="extended",
> selecttitle=0,
>                      font='{Arial} 10', ipadx=5, ipady=5, anchor='e',
>                      borderwidth=c( 0, 1, 0, 1 ), drawmode='slow',
>                      variable=tableDataTclArray )
> tcl( tableData, "tag", "celltag", "ZeroZero", "0,0" )
> tcl( tableData, "tag", "rowtag",  "rowtitle", "0" )
> tcl( tableData, "tag", "coltag",  "coltitle", "0" )
> tcl( tableData, "tag", "configure", "ZeroZero", bg='SystemButtonFace',
> relief="raised", state='disabled' )
> tcl( tableData, "tag", "configure", "rowtitle", bg='lightgreen',
> relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='center' )
> tcl( tableData, "tag", "configure", "coltitle", bg='lightgreen',
> relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='w' )
> tcl( tableData, "tag", "configure", "active", fg='yellow', bg='red' )
> tcl( tableData, "width", "0", "20" )
> scrX <- tkscrollbar( fmTableData, orient="horizontal", command=function(...)
> tkxview( tableData, ... ) )
> scrY <- tkscrollbar( fmTableData, orient="vertical", command=function(...)
> tkyview( tableData, ... ) )
> scrYwidth <- as.integer( tkcget( scrY, width = NULL ) )
> tcl( "pack", scrY, side = "right",  fill = "y", expand = FALSE, pady = c(0,
> scrYwidth + 2) )
> tcl( "pack", scrX, side = "bottom", fill = "x", expand = FALSE )
> tcl( "pack", tableData, anchor='n', side="right", expand=TRUE, fill="both"
> )
> tcl( "pack", fmTableData, fill="both", expand = TRUE, padx=10, pady=10 )
> ##########################################  expensive time computation
> [[elided Yahoo spam]]
> DF <- as.data.frame( matrix( round(runif(1e4 * 100),3), nr=1e4, nc=100 ) )
> #### simulated data
> for( i in 1:nrow( DF ) ){
>     tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "',
> rownames( DF )[ i ] , '"') ) )
>     }
> for( i in 1:ncol( DF ) ){
>     tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "',
> colnames( DF )[ i ] , '"') ) )
>     }
> for( i in 1:nrow( DF ) ) {
>     for( j in 1:ncol( DF ) ){
>         tableDataTclArray[[ i, j ]] <- DF[i,j]
>     }
> }
> #
>
>
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From klebyn at yahoo.com.br  Wed Dec  9 03:04:50 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Wed, 9 Dec 2015 00:04:50 -0200
Subject: [R] [tcltk][tktable] How to make an efficient data transfer
 from R to Tcl?
In-Reply-To: <CAGxFJbSrG7U9qkhgnANQ5tdviDtRf6q6winypQJ3ETXso4fe_A@mail.gmail.com>
References: <566777D6.6090501@yahoo.com.br>
	<CAGxFJbSrG7U9qkhgnANQ5tdviDtRf6q6winypQJ3ETXso4fe_A@mail.gmail.com>
Message-ID: <56678C42.20000@yahoo.com.br>

my objective is to show data in screen inside a tktable...
for that, the data must be in a TCL variable and not only in a R variable
like that:

> library( tcltk )
> mtcars_in_TCL <- tclArray()
> for( i in 1:5 ) for( j in 1:5 ) mtcars_in_TCL[[ i,j ]] <-
as.matrix(mtcars)[ i,j ]

i thank by  attention
cleber

Em 08/12/2015 22:51, Bert Gunter escreveu:
> Define: "transfer"
>
> (  save/load should be efficient and fast within R, but you appear to
> have something else in mind. What?)
>
> Apologies if it's obvious and I just don't get it.
>
> Cheers,
>
> Bert
>
>
> Bert Gunter
>
> "Data is not information. Information is not knowledge. And knowledge
> is certainly not wisdom."
>     -- Clifford Stoll
>
>
> On Tue, Dec 8, 2015 at 4:37 PM, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
>> hello all,
>> I intend transfer a big data.frame, more than 1e4 rows, more than 100
>> columns...
>> I found solutions (in internet and help pages) for small data.frame like the
>> showed bellow.
>> Big data.frames is very expensive in computation time in my approach
>> I would like to optimize this transfer anyway ...
>> I saw a function named "as.tclObj" that seems to me a promising form but
>> hard to code...
>> Somebody know a way more clever to do it?
>>
>> thanks for tips
>> cleber
>>
>> ##########################################
>> library( tcltk )
>> nrow <- 10 ; ncol <- 100
>> # simulated data: data is originally a data frame
>> DF <- as.data.frame(matrix(round(runif(nrow*ncol),3),nr=nrow,nc=ncol))
>> ######
>> # DF_tcl <- as.tclObj( as.matrix( DF ) )
>>
>> tableDataTclArray <- tclArray()
>> for( i in 1:nrow( DF ) )
>>      tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "',
>> rownames( DF )[ i ] , '"') ) )
>>
>> for( i in 1:ncol( DF ) )
>>      tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "',
>> colnames( DF )[ i ] , '"') ) )
>>
>>
>> for( i in 1:nrow( DF ) )
>>      for( j in 1:ncol( DF ) )
>>          tableDataTclArray[[ i, j ]] <- DF[i,j]
>> ######
>>
>>
>>
>>
>>
>> ##############################################
>> ##############################################
>> ##############################################
>> # my full code:
>> ###############
>> library( tcltk ); tclRequire("Tktable")
>> wDataOrg <- tktoplevel()
>> tcl('wm', 'geometry',  wDataOrg, "1024x600+175+80" )
>> fmTableData <- ttklabelframe( wDataOrg, text='  Data  ', padding=c( 8, 5, 8,
>> 15 ) )
>> height <- -1; width <- -1
>> nrowDefault <- 1e4+1; ncolDefault <- 100+1
>> tableDataTclArray <- tclArray()
>> tableData <- tkwidget( fmTableData, "table", rows=nrowDefault,
>> cols=ncolDefault,
>>                       height=height+1,width=width+1,
>>                       xscrollcommand=function(...) tkset( scrX, ... ),
>>                       yscrollcommand=function(...) tkset( scrY, ... ),
>>                       rowstretchmode='unset', colstretchmode='unset',
>> multiline=0, cache=1,
>>                       background="white", selectmode="extended",
>> selecttitle=0,
>>                       font='{Arial} 10', ipadx=5, ipady=5, anchor='e',
>>                       borderwidth=c( 0, 1, 0, 1 ), drawmode='slow',
>>                       variable=tableDataTclArray )
>> tcl( tableData, "tag", "celltag", "ZeroZero", "0,0" )
>> tcl( tableData, "tag", "rowtag",  "rowtitle", "0" )
>> tcl( tableData, "tag", "coltag",  "coltitle", "0" )
>> tcl( tableData, "tag", "configure", "ZeroZero", bg='SystemButtonFace',
>> relief="raised", state='disabled' )
>> tcl( tableData, "tag", "configure", "rowtitle", bg='lightgreen',
>> relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='center' )
>> tcl( tableData, "tag", "configure", "coltitle", bg='lightgreen',
>> relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='w' )
>> tcl( tableData, "tag", "configure", "active", fg='yellow', bg='red' )
>> tcl( tableData, "width", "0", "20" )
>> scrX <- tkscrollbar( fmTableData, orient="horizontal", command=function(...)
>> tkxview( tableData, ... ) )
>> scrY <- tkscrollbar( fmTableData, orient="vertical", command=function(...)
>> tkyview( tableData, ... ) )
>> scrYwidth <- as.integer( tkcget( scrY, width = NULL ) )
>> tcl( "pack", scrY, side = "right",  fill = "y", expand = FALSE, pady = c(0,
>> scrYwidth + 2) )
>> tcl( "pack", scrX, side = "bottom", fill = "x", expand = FALSE )
>> tcl( "pack", tableData, anchor='n', side="right", expand=TRUE, fill="both"
>> )
>> tcl( "pack", fmTableData, fill="both", expand = TRUE, padx=10, pady=10 )
>> ##########################################  expensive time computation
>> [[elided Yahoo spam]]
>> DF <- as.data.frame( matrix( round(runif(1e4 * 100),3), nr=1e4, nc=100 ) )
>> #### simulated data
>> for( i in 1:nrow( DF ) ){
>>      tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "',
>> rownames( DF )[ i ] , '"') ) )
>>      }
>> for( i in 1:ncol( DF ) ){
>>      tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "',
>> colnames( DF )[ i ] , '"') ) )
>>      }
>> for( i in 1:nrow( DF ) ) {
>>      for( j in 1:ncol( DF ) ){
>>          tableDataTclArray[[ i, j ]] <- DF[i,j]
>>      }
>> }
>> #
>>
>>
>>


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From drjimlemon at gmail.com  Wed Dec  9 03:09:57 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Dec 2015 13:09:57 +1100
Subject: [R] [tcltk][tktable] How to make an efficient data transfer
 from R to Tcl?
In-Reply-To: <56678C42.20000@yahoo.com.br>
References: <566777D6.6090501@yahoo.com.br>
	<CAGxFJbSrG7U9qkhgnANQ5tdviDtRf6q6winypQJ3ETXso4fe_A@mail.gmail.com>
	<56678C42.20000@yahoo.com.br>
Message-ID: <CA+8X3fUPgSQTV7ywokVoVursv7aiSYy51OTzcYmM-NLx4se2Bw@mail.gmail.com>

Hi Cleber,
have you tried:

edit(mtcars)

Jim


On Wed, Dec 9, 2015 at 1:04 PM, Cleber N.Borges <klebyn at yahoo.com.br> wrote:

> my objective is to show data in screen inside a tktable...
> for that, the data must be in a TCL variable and not only in a R variable
> like that:
>
> library( tcltk )
>> mtcars_in_TCL <- tclArray()
>> for( i in 1:5 ) for( j in 1:5 ) mtcars_in_TCL[[ i,j ]] <-
>>
> as.matrix(mtcars)[ i,j ]
>
> i thank by  attention
> cleber
>
>
> Em 08/12/2015 22:51, Bert Gunter escreveu:
>
>> Define: "transfer"
>>
>> (  save/load should be efficient and fast within R, but you appear to
>> have something else in mind. What?)
>>
>> Apologies if it's obvious and I just don't get it.
>>
>> Cheers,
>>
>> Bert
>>
>>
>> Bert Gunter
>>
>> "Data is not information. Information is not knowledge. And knowledge
>> is certainly not wisdom."
>>     -- Clifford Stoll
>>
>>
>> On Tue, Dec 8, 2015 at 4:37 PM, Cleber N.Borges <klebyn at yahoo.com.br>
>> wrote:
>>
>>> hello all,
>>> I intend transfer a big data.frame, more than 1e4 rows, more than 100
>>> columns...
>>> I found solutions (in internet and help pages) for small data.frame like
>>> the
>>> showed bellow.
>>> Big data.frames is very expensive in computation time in my approach
>>> I would like to optimize this transfer anyway ...
>>> I saw a function named "as.tclObj" that seems to me a promising form but
>>> hard to code...
>>> Somebody know a way more clever to do it?
>>>
>>> thanks for tips
>>> cleber
>>>
>>> ##########################################
>>> library( tcltk )
>>> nrow <- 10 ; ncol <- 100
>>> # simulated data: data is originally a data frame
>>> DF <- as.data.frame(matrix(round(runif(nrow*ncol),3),nr=nrow,nc=ncol))
>>> ######
>>> # DF_tcl <- as.tclObj( as.matrix( DF ) )
>>>
>>> tableDataTclArray <- tclArray()
>>> for( i in 1:nrow( DF ) )
>>>      tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "',
>>> rownames( DF )[ i ] , '"') ) )
>>>
>>> for( i in 1:ncol( DF ) )
>>>      tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "',
>>> colnames( DF )[ i ] , '"') ) )
>>>
>>>
>>> for( i in 1:nrow( DF ) )
>>>      for( j in 1:ncol( DF ) )
>>>          tableDataTclArray[[ i, j ]] <- DF[i,j]
>>> ######
>>>
>>>
>>>
>>>
>>>
>>> ##############################################
>>> ##############################################
>>> ##############################################
>>> # my full code:
>>> ###############
>>> library( tcltk ); tclRequire("Tktable")
>>> wDataOrg <- tktoplevel()
>>> tcl('wm', 'geometry',  wDataOrg, "1024x600+175+80" )
>>> fmTableData <- ttklabelframe( wDataOrg, text='  Data  ', padding=c( 8,
>>> 5, 8,
>>> 15 ) )
>>> height <- -1; width <- -1
>>> nrowDefault <- 1e4+1; ncolDefault <- 100+1
>>> tableDataTclArray <- tclArray()
>>> tableData <- tkwidget( fmTableData, "table", rows=nrowDefault,
>>> cols=ncolDefault,
>>>                       height=height+1,width=width+1,
>>>                       xscrollcommand=function(...) tkset( scrX, ... ),
>>>                       yscrollcommand=function(...) tkset( scrY, ... ),
>>>                       rowstretchmode='unset', colstretchmode='unset',
>>> multiline=0, cache=1,
>>>                       background="white", selectmode="extended",
>>> selecttitle=0,
>>>                       font='{Arial} 10', ipadx=5, ipady=5, anchor='e',
>>>                       borderwidth=c( 0, 1, 0, 1 ), drawmode='slow',
>>>                       variable=tableDataTclArray )
>>> tcl( tableData, "tag", "celltag", "ZeroZero", "0,0" )
>>> tcl( tableData, "tag", "rowtag",  "rowtitle", "0" )
>>> tcl( tableData, "tag", "coltag",  "coltitle", "0" )
>>> tcl( tableData, "tag", "configure", "ZeroZero", bg='SystemButtonFace',
>>> relief="raised", state='disabled' )
>>> tcl( tableData, "tag", "configure", "rowtitle", bg='lightgreen',
>>> relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='center' )
>>> tcl( tableData, "tag", "configure", "coltitle", bg='lightgreen',
>>> relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='w' )
>>> tcl( tableData, "tag", "configure", "active", fg='yellow', bg='red' )
>>> tcl( tableData, "width", "0", "20" )
>>> scrX <- tkscrollbar( fmTableData, orient="horizontal",
>>> command=function(...)
>>> tkxview( tableData, ... ) )
>>> scrY <- tkscrollbar( fmTableData, orient="vertical",
>>> command=function(...)
>>> tkyview( tableData, ... ) )
>>> scrYwidth <- as.integer( tkcget( scrY, width = NULL ) )
>>> tcl( "pack", scrY, side = "right",  fill = "y", expand = FALSE, pady =
>>> c(0,
>>> scrYwidth + 2) )
>>> tcl( "pack", scrX, side = "bottom", fill = "x", expand = FALSE )
>>> tcl( "pack", tableData, anchor='n', side="right", expand=TRUE,
>>> fill="both"
>>> )
>>> tcl( "pack", fmTableData, fill="both", expand = TRUE, padx=10, pady=10 )
>>> ##########################################  expensive time computation
>>> [[elided Yahoo spam]]
>>> DF <- as.data.frame( matrix( round(runif(1e4 * 100),3), nr=1e4, nc=100 )
>>> )
>>> #### simulated data
>>> for( i in 1:nrow( DF ) ){
>>>      tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "',
>>> rownames( DF )[ i ] , '"') ) )
>>>      }
>>> for( i in 1:ncol( DF ) ){
>>>      tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "',
>>> colnames( DF )[ i ] , '"') ) )
>>>      }
>>> for( i in 1:nrow( DF ) ) {
>>>      for( j in 1:ncol( DF ) ){
>>>          tableDataTclArray[[ i, j ]] <- DF[i,j]
>>>      }
>>> }
>>> #
>>>
>>>
>>>
>>>
>
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Wed Dec  9 03:57:49 2015
From: hannah.hlx at gmail.com (li li)
Date: Tue, 8 Dec 2015 21:57:49 -0500
Subject: [R] change the x axis tickmarks when using plot function in drc
	package
Message-ID: <CAHLnndb-SF8EBCh184F4L6Vt69QoyzYaDzOXh6DVjQK0qyah-A@mail.gmail.com>

Hi all,
  When plotting the dose response curve using plot function as in the
example codes below, the scale of the axis should really be on the log
scale of the dose given the shape of the graph. But as you can see, the
tickmarks of the returned graph represent the original scale. How can I
change the tick marks to the scale that the correspond to log(dose)?
  Thanks much in advance.
    Hanna


dose <- rep(50*2^(-(0:11)),3)
dose
d <- 100
c <- 1
b <- 1
e <- 1.6
y <- rnorm(length(dose))+ c+ (d-c)/(1+exp(b*(log(dose)-log(e))))
library(drc)
mod <- drm(y~dose, fct = LL.4())
summary(mod)
plot(mod, type="all")

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Wed Dec  9 04:22:42 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 9 Dec 2015 14:22:42 +1100
Subject: [R] change the x axis tickmarks when using plot function in drc
	package
In-Reply-To: <CAHLnndb-SF8EBCh184F4L6Vt69QoyzYaDzOXh6DVjQK0qyah-A@mail.gmail.com>
References: <CAHLnndb-SF8EBCh184F4L6Vt69QoyzYaDzOXh6DVjQK0qyah-A@mail.gmail.com>
Message-ID: <CA+8X3fUU2vH440XWNcpnaMaVKvcDWvyKXpF340WFuo_Pwg8MqA@mail.gmail.com>

Hi Hannah,
Try this:

plot(mod, type="all",log="xy")

Jim


On Wed, Dec 9, 2015 at 1:57 PM, li li <hannah.hlx at gmail.com> wrote:

> Hi all,
>   When plotting the dose response curve using plot function as in the
> example codes below, the scale of the axis should really be on the log
> scale of the dose given the shape of the graph. But as you can see, the
> tickmarks of the returned graph represent the original scale. How can I
> change the tick marks to the scale that the correspond to log(dose)?
>   Thanks much in advance.
>     Hanna
>
>
> dose <- rep(50*2^(-(0:11)),3)
> dose
> d <- 100
> c <- 1
> b <- 1
> e <- 1.6
> y <- rnorm(length(dose))+ c+ (d-c)/(1+exp(b*(log(dose)-log(e))))
> library(drc)
> mod <- drm(y~dose, fct = LL.4())
> summary(mod)
> plot(mod, type="all")
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From hannah.hlx at gmail.com  Wed Dec  9 04:56:00 2015
From: hannah.hlx at gmail.com (li li)
Date: Tue, 8 Dec 2015 22:56:00 -0500
Subject: [R] change the x axis tickmarks when using plot function in drc
	package
In-Reply-To: <CA+8X3fUU2vH440XWNcpnaMaVKvcDWvyKXpF340WFuo_Pwg8MqA@mail.gmail.com>
References: <CAHLnndb-SF8EBCh184F4L6Vt69QoyzYaDzOXh6DVjQK0qyah-A@mail.gmail.com>
	<CA+8X3fUU2vH440XWNcpnaMaVKvcDWvyKXpF340WFuo_Pwg8MqA@mail.gmail.com>
Message-ID: <CAHLnndZv6=AZjT_Y7M3i=t01qcb0L9iWAfmrjJ4ehvYwUe52aA@mail.gmail.com>

Thanks for the reply but that does not seem to work. With that the plot is
on log scale for both the response and the dose levels, but the tickmarks
are still on the original scale.


2015-12-08 22:22 GMT-05:00 Jim Lemon <drjimlemon at gmail.com>:

> Hi Hannah,
> Try this:
>
> plot(mod, type="all",log="xy")
>
> Jim
>
>
> On Wed, Dec 9, 2015 at 1:57 PM, li li <hannah.hlx at gmail.com> wrote:
>
>> Hi all,
>>   When plotting the dose response curve using plot function as in the
>> example codes below, the scale of the axis should really be on the log
>> scale of the dose given the shape of the graph. But as you can see, the
>> tickmarks of the returned graph represent the original scale. How can I
>> change the tick marks to the scale that the correspond to log(dose)?
>>   Thanks much in advance.
>>     Hanna
>>
>>
>> dose <- rep(50*2^(-(0:11)),3)
>> dose
>> d <- 100
>> c <- 1
>> b <- 1
>> e <- 1.6
>> y <- rnorm(length(dose))+ c+ (d-c)/(1+exp(b*(log(dose)-log(e))))
>> library(drc)
>> mod <- drm(y~dose, fct = LL.4())
>> summary(mod)
>> plot(mod, type="all")
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From phgrosjean at sciviews.org  Wed Dec  9 08:42:22 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Wed, 9 Dec 2015 08:42:22 +0100
Subject: [R] [tcltk][tktable] How to make an efficient data transfer
	from R to Tcl?
In-Reply-To: <566777D6.6090501@yahoo.com.br>
References: <566777D6.6090501@yahoo.com.br>
Message-ID: <BA726B51-811A-455F-BDB4-F4F258E1CD9A@sciviews.org>

Are you sure this is the right way to go for your use case? Even if you got a quick solution to display an 1e4 x 100 table in TkTable, what is the purpose of it? Will the user browse the whole dataset that way? Even if the answer is yes, you would probably need to implement sorting and filtering features to ease access to the data? and you will end up reinventing a database.

Have you considered to put your data in, say SQLite, using RSQLite? Then, you could feed your data page by page (10, 20, 50, or 100 items per page) into your TkTable widget. You still you will got a bunch of programming in tcltk. But it may be a necessity if you have to integrate the table display in a larger Tk GUI.

Another approach, still with the database backend, is to use HTML + Javascript and the numerous solutions that exist to display the content of your database. If you need close interaction with R, you may like the Shiny reactive programming approach. Look at the DT package, although I have not tested it with very large tables.

Best,

Philippe Grosjean


> On 09 Dec 2015, at 01:37, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> hello all,
> I intend transfer a big data.frame, more than 1e4 rows, more than 100 columns...
> I found solutions (in internet and help pages) for small data.frame like the showed bellow.
> Big data.frames is very expensive in computation time in my approach
> I would like to optimize this transfer anyway ...
> I saw a function named "as.tclObj" that seems to me a promising form but hard to code...
> Somebody know a way more clever to do it?
> 
> thanks for tips
> cleber
> 
> ##########################################
> library( tcltk )
> nrow <- 10 ; ncol <- 100
> # simulated data: data is originally a data frame
> DF <- as.data.frame(matrix(round(runif(nrow*ncol),3),nr=nrow,nc=ncol))
> ######
> # DF_tcl <- as.tclObj( as.matrix( DF ) )
> 
> tableDataTclArray <- tclArray()
> for( i in 1:nrow( DF ) )
>    tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "', rownames( DF )[ i ] , '"') ) )
> 
> for( i in 1:ncol( DF ) )
>    tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "', colnames( DF )[ i ] , '"') ) )
> 
> 
> for( i in 1:nrow( DF ) )
>    for( j in 1:ncol( DF ) )
>        tableDataTclArray[[ i, j ]] <- DF[i,j]
> ######
> 
> 
> 
> 
> 
> ##############################################
> ##############################################
> ##############################################
> # my full code:
> ###############
> library( tcltk ); tclRequire("Tktable")
> wDataOrg <- tktoplevel()
> tcl('wm', 'geometry',  wDataOrg, "1024x600+175+80" )
> fmTableData <- ttklabelframe( wDataOrg, text='  Data  ', padding=c( 8, 5, 8, 15 ) )
> height <- -1; width <- -1
> nrowDefault <- 1e4+1; ncolDefault <- 100+1
> tableDataTclArray <- tclArray()
> tableData <- tkwidget( fmTableData, "table", rows=nrowDefault, cols=ncolDefault,
>                     height=height+1,width=width+1,
>                     xscrollcommand=function(...) tkset( scrX, ... ),
>                     yscrollcommand=function(...) tkset( scrY, ... ),
>                     rowstretchmode='unset', colstretchmode='unset', multiline=0, cache=1,
>                     background="white", selectmode="extended", selecttitle=0,
>                     font='{Arial} 10', ipadx=5, ipady=5, anchor='e',
>                     borderwidth=c( 0, 1, 0, 1 ), drawmode='slow',
>                     variable=tableDataTclArray )
> tcl( tableData, "tag", "celltag", "ZeroZero", "0,0" )
> tcl( tableData, "tag", "rowtag",  "rowtitle", "0" )
> tcl( tableData, "tag", "coltag",  "coltitle", "0" )
> tcl( tableData, "tag", "configure", "ZeroZero", bg='SystemButtonFace', relief="raised", state='disabled' )
> tcl( tableData, "tag", "configure", "rowtitle", bg='lightgreen', relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='center' )
> tcl( tableData, "tag", "configure", "coltitle", bg='lightgreen', relief='raised', borderwidth=c( 0, 1, 0, 1 ), anchor='w' )
> tcl( tableData, "tag", "configure", "active", fg='yellow', bg='red' )
> tcl( tableData, "width", "0", "20" )
> scrX <- tkscrollbar( fmTableData, orient="horizontal", command=function(...) tkxview( tableData, ... ) )
> scrY <- tkscrollbar( fmTableData, orient="vertical", command=function(...) tkyview( tableData, ... ) )
> scrYwidth <- as.integer( tkcget( scrY, width = NULL ) )
> tcl( "pack", scrY, side = "right",  fill = "y", expand = FALSE, pady = c(0, scrYwidth + 2) )
> tcl( "pack", scrX, side = "bottom", fill = "x", expand = FALSE )
> tcl( "pack", tableData, anchor='n', side="right", expand=TRUE, fill="both"  )
> tcl( "pack", fmTableData, fill="both", expand = TRUE, padx=10, pady=10 )
> ##########################################  expensive time computation [[elided Yahoo spam]]
> DF <- as.data.frame( matrix( round(runif(1e4 * 100),3), nr=1e4, nc=100 ) ) #### simulated data
> for( i in 1:nrow( DF ) ){
>    tableDataTclArray[[ i, 0 ]] <- .Tcl( noquote( paste('format "', rownames( DF )[ i ] , '"') ) )
>    }
> for( i in 1:ncol( DF ) ){
>    tableDataTclArray[[ 0, i ]] <- .Tcl( noquote( paste('format "', colnames( DF )[ i ] , '"') ) )
>    }
> for( i in 1:nrow( DF ) ) {
>    for( j in 1:ncol( DF ) ){
>        tableDataTclArray[[ i, j ]] <- DF[i,j]
>    }
> }
> #
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From f_j_rod at hotmail.com  Wed Dec  9 10:38:19 2015
From: f_j_rod at hotmail.com (Frank S.)
Date: Wed, 9 Dec 2015 10:38:19 +0100
Subject: [R] Keep only first date from consecutive dates
In-Reply-To: <44A993D0-DCED-44CE-A1B3-F1F34D9EE56A@comcast.net>
References: <CAH-FEnjfDBZF2-1HWR0_ULd71Kfrd3z+ES13ZdXaj4wGD1OVOw@mail.gmail.com>
	<CAH-FEnh+1DDFOsWNq5GF7p-iLpBOOUEaKst+NZGm1Pc-f+arvA@mail.gmail.com>
	<26C7B3B6-6596-418C-9CC4-C65558E54F53@dcn.davis.CA.us>
	<000f01cff518$92e14210$b8a3c630$@mcmaster.ca>
	<alpine.BSF.2.00.1410310822530.42077@pedal.dcn.davis.ca.us>
	<BAY168-W110DBCDE35CFDCF099B3682BAB40@phx.gbl>
	<CAF8bMcafucUX2C4Gsdiu=y+nFiQuz8TtSYWM31MRp8i_ii9kHw@mail.gmail.com>
	<BAY168-W44A193BD2CF88170D96BE5BAB30@phx.gbl>
	<BAY168-W1378704ED614CEA5A482652BA0C0@phx.gbl>
	<CAF8bMcZPRWPO4dOnpk2xnzrGgOauBL05BgTJopF9w_8AF3KdfQ@mail.gmail.com>,
	<44A993D0-DCED-44CE-A1B3-F1F34D9EE56A@comcast.net>
Message-ID: <BAY168-W132432213E5860EFB4451F2BAE80@phx.gbl>

Many thanks to: William Dunlap, Dennis Murphy and David Winsemius for your quick and efficient answers!!
 
Best regards,
 
Frank S.
 
 
> Subject: Re: [R] Keep only first date from consecutive dates
> From: dwinsemius at comcast.net
> Date: Fri, 4 Dec 2015 16:34:38 -0800
> CC: f_j_rod at hotmail.com; r-help at r-project.org
> To: wdunlap at tibco.com
> 
> 
> > On Dec 4, 2015, at 1:10 PM, William Dunlap <wdunlap at tibco.com> wrote:
> > 
> > With a data.frame sorted by id, with ties broken by date, as in
> > your example, you can select rows that are either the start
> > of a new id group or the start of run of consecutive dates with:
> > 
> >> w <- c(TRUE, diff(uci$date)>1) | c(TRUE, diff(uci$id)!=0)
> >> which(w)
> > [1] 1 4 5 7
> >> uci[w,]
> >  id       date value
> > 1  1 2005-10-28     1
> > 4  1 2005-11-07     3
> > 5  1 2007-03-19     1
> > 7  2 2004-06-02     2
> > 
> > I'll leave it to you to translate that R syntax into data.table syntax -
> > it just involves comparing the current row with the previous row.
> > 
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> > 
> > 
> > On Fri, Dec 4, 2015 at 12:53 PM, Frank S. <f_j_rod at hotmail.com> wrote:
> >> Dear R users,
> >> 
> >> I usually work with data.table package, but I'm sure that muy question can also be answered working with R data frame.
> >> Working with grouped data (by "id"),  I wonder if it is possible to keep in a R data.frame (or R data.table):
> >> a) Only the first row if there is a row which belongs to a a group of rows (from same "id") that have consecutive dates.
> >> b) All the rows which do not belong to the above groups.
> >> 
> >> As an example, I have "uci" data.frame:
> >> 
> >> uci <- data.table(id=c(rep(1,6),2),
> >>                date = as.Date(c("2005-10-28","2005-10-29","2005-10-30","2005-11-07","2007-03-19","2007-03-20","2004-06-02")),
> >>                value = c(1, 2, 1, 3, 1, 2, 2))
> >> 
> >>   id              date   value
> >>    1  2005-10-28        1
> >>    1  2005-10-29        2
> >>    1  2005-10-30        1
> >>    1  2005-11-07        3
> >>    1  2007-03-19        1
> >>    1  2007-03-20        2
> >>    2  2004-06-02        2
> >> 
> >> And the desired output would be:
> >> 
> >>   id              date   value
> >>    1  2005-10-28        1
> >>    1  2005-11-07        3
> >>    1  2007-03-19        1
> >>    2  2004-06-02        2
> 
> The syntax of `[.data.table` is a bit odd; You can refer to columns by name; I never trust my intuition, though.
> 
> Selection is usually done with a logical vector in the ?i?-position. The diff operator does succeed in the ?i? position with the obvious need to prepend with a starting value..
> 
> > uci[ c(0,diff(date))!=1, ]
>    id       date value
> 1:  1 2005-10-28     1
> 2:  1 2005-11-07     3
> 3:  1 2007-03-19     1
> 4:  2 2004-06-02     2
> 
> The other cases are handle with the converse-expression
> 
> > uci[c(0,diff(date)) == 1, ]
>    id       date value
> 1:  1 2005-10-29     2
> 2:  1 2005-10-30     1
> 3:  1 2007-03-20     2
> 
> 
> >> 
> >> # From the following link, I have tried:
> >> http://stackoverflow.com/questions/32308636/r-how-to-sum-values-from-rows-only-if-the-key-value-is-the-same-and-also-if-the
> >> 
> >> setDT(uci)[ ,list(date=date[1L], value = value[1L]),  by = .(ind=rleid(date), id)][, ind:=NULL][]
> >> 
> >> But I get the same data frame, and I do not know the reason.
> >> 
> >> Thank you very much for any help!!
> >> 
> >> Frank S.
> >> 
> >> 
> >> 
> >> 
> >> 
> >>        [[alternative HTML version deleted]]
> >> 
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
 		 	   		  
	[[alternative HTML version deleted]]


From ragland.debra at yahoo.com  Wed Dec  9 13:49:37 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Wed, 9 Dec 2015 12:49:37 +0000 (UTC)
Subject: [R] applying wilcox.test to every combination of rows in a matrix
 (pairwise)
References: <1332848668.17996960.1449665377363.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>

Hello All,

I have written the following loop which will apply/split the same vector of numbers (pc1.eigv) to each (logical) row of a matrix and run a wilcox.test on those values that line up with TRUE and those that line up with FALSE. It works fine. However, I am now interested in using the same vector and (logical)matrix run the wilcox.test only this time I would like information about pairs of rows (not just single rows as it already does).

The loop:
n.iteration=dim(as.matrix(p))[1]
n.test= rep(NA, n.iteration )
for( i in 1:n.iteration ){  ## i=1
i_spl<-split(pc1.eigv, p[i,])
if( sum(p[i,])==15 | sum(p[i,])==0) { n.test[i]=NA  }
if( sum(p[i,])!=15 & sum(p[i,])!=0) { 
testout=wilcox.test(i_spl$'TRUE', i_spl$'FALSE')
n.test[i]=testout$p.value     }
}


some sample data
p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)))

pc1.eigv<-runif(4, 1.0, 2.0)

After some searching I thought that perhaps the combn function would help me (i.e. combn(p)) for the same loop but I get an error.

Can anyone help with this?


From S.Ellison at LGCGroup.com  Wed Dec  9 14:45:02 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Wed, 9 Dec 2015 13:45:02 +0000
Subject: [R] applying wilcox.test to every combination of rows in a
 matrix (pairwise)
In-Reply-To: <1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>
References: <1332848668.17996960.1449665377363.JavaMail.yahoo.ref@mail.yahoo.com>
	<1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403C96CD247@GBTEDVPEXCMB04.corp.lgc-group.com>



> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of debra ragland via R-help
> some sample data
> p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)))
i) Something wrong with p, here; it's a single column matrix. did you mean 
p4<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)
? (I changed the name for later)


ii) You don't need split(), just ordinary indexing. For example
wilcox.test(pc1.eigv[ p[1,] ], pc1.eigv[ !p[1,] ] ) 

> I am now interested in using the same vector and (logical)matrix run the wilcox.test 
> only this time I would like information about pairs of rows
ii) 'fraid that's not specific enough. How will you select the pairs (what row indexes will you want) and do you intend to test one row in each pair against the other or concatenate the TRUE and FALSE sets from the rows and then test TRUE vs FALSE?

> After some searching I thought that perhaps the combn function would help me
> (i.e. combn(p)) for the same loop but I get an error.
iv) Did you mean 
rowpairs <- combn(length(p), 2) #or combn(nrow(p), 2) if p is really a matrix
?
If you did, that generates a 2 x p matrix so your row pairs would be accessed via 
rowpairs[, i]

v) You don't need a loop either. Consider 

#Set up a function to do the donkey work on a particular
#pair of row indices:
rptest <- function(rows, p, pc1) {
	#Simplify later extraction by extending pc1.eigv:
	pc2 <- rep(pc1, 2)

	#extract and concatenates the two rows of the TRUE/PALSE matrix p
	select <- as.vector( p[rows,] )
	
	#Combine the two in a wilcox test
	wilcox.test(pc2[ select ], pc2 [ !select ] )
}

rowpairs <- combn(nrow(p4), 2)
apply(rowpairs, 2, rptest, p=p4, pc1=pc1.eigv)

#Returns a list of wilcoxon tests of TRUE vs FALSE on all rows taken 



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From chrysopix at gmail.com  Wed Dec  9 14:52:29 2015
From: chrysopix at gmail.com (=?UTF-8?Q?Ronaldo_Reis_J=c3=banior?=)
Date: Wed, 9 Dec 2015 11:52:29 -0200
Subject: [R] Custom manual legend in ggplot2
Message-ID: <5668321D.2020305@gmail.com>

Hi,

I'm trying to make a ggplot() graph with a custom legend, but without 
success, it is simple, I have somethink like this in traditional plot:

plot(y~x)
curve(equation1,lty=1)
curve(equation2,lty=2)
legend(x,y,legend=c("equation 1","equation 2"),lty=c(1,2),bty="n").

In my ggplot graph I have this sequence:

ggplot(data1, aes(x=x1, y=y1))+
     geom_point()+
     geom_smooth(method="glm", family="gaussian")+
     geom_smooth(aes(x=x1, y=y1,lty=2,data=data2, method="glm", 
family="gaussian")+
     theme(plot.title = element_text(lineheight=.8, face="bold"))+
     theme_bw()

The problem is that legend only work in automatic way, I try some option 
to add a manual legend in ggplot like the traditional legend() command. 
Anybody can help-me?

Thanks
Ronaldo


-- 

> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8192 | ronaldo.reis at unimontes.br
| http://www.ppgcb.unimontes.br/lecc | LinuxUser#: 205366


	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Wed Dec  9 15:10:19 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 9 Dec 2015 14:10:19 +0000
Subject: [R] Custom manual legend in ggplot2
Message-ID: <248E6FA047A8C746BA491485764190F5266402FC@ESESSMB207.ericsson.se>

Try to look at the link:

http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/


Also consider:

help(theme)

legend.background    background of legend (element_rect; inherits from rect)
legend.margin   extra space added around legend (unit)
legend.key background underneath legend keys (element_rect; inherits from rect)
legend.key.size size of legend keys (unit; inherits from legend.key.size)
legend.key.height    key background height (unit; inherits from legend.key.size)
legend.key.width     key background width (unit; inherits from legend.key.size)
legend.text     legend item labels (element_text; inherits from text)
legend.text.align    alignment of legend labels (number from 0 (left) to 1 (right))
legend.title    title of legend (element_text; inherits from title)
legend.title.align   alignment of legend title (number from 0 (left) to 1 (right))
legend.position the position of legends ("none", "left", "right", "bottom", "top", or two-element numeric vector)
legend.direction     layout of items in legends ("horizontal" or "vertical")
legend.justification anchor point for positioning legend inside plot ("center" or two-element numeric vector)
legend.box arrangement of multiple legends ("horizontal" or "vertical")
legend.box.just justification of each legend within the overall bounding box, when there are multiple legends ("top", "bottom", "left", or "right")

--

GG

	[[alternative HTML version deleted]]


From chrysopix at gmail.com  Wed Dec  9 16:11:23 2015
From: chrysopix at gmail.com (=?UTF-8?Q?Ronaldo_Reis_J=c3=banior?=)
Date: Wed, 9 Dec 2015 13:11:23 -0200
Subject: [R] Custom manual legend in ggplot2
In-Reply-To: <248E6FA047A8C746BA491485764190F5266402FC@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F5266402FC@ESESSMB207.ericsson.se>
Message-ID: <5668449B.9080200@gmail.com>

Thanks,

Now its work, but I dont get to change te legend position. Look:


ggplot(data1, aes(x=x1, y=y1))+
     geom_point()+
     geom_smooth(method="glm", family="gaussian",aes(linetype="equation1"))+
     geom_smooth(aes(x=x1, y=y1, linetype="equation2"),data=data2, 
method="glm", family="gaussian")+
     scale_linetype_manual(values = 
c("solid","dashed"),name="Equations",labels=c("Equation 1","Equation 
2"),guide="legend")+
     theme(plot.title = element_text(lineheight=.8, 
face="bold"),legend.position="top")+
     theme_bw()

Now the legend work, but the argument legend.position="top" in theme() 
function dont work. I try to change the order of commands, but without 
effects.

Any idea?

Thanks
Ronaldo

Em 09-12-2015 12:10, Giorgio Garziano escreveu:
> Try to look at the link:
>
> http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/
>
>
> Also consider:
>
> help(theme)
>
> legend.background    background of legend (element_rect; inherits from rect)
> legend.margin   extra space added around legend (unit)
> legend.key background underneath legend keys (element_rect; inherits from rect)
> legend.key.size size of legend keys (unit; inherits from legend.key.size)
> legend.key.height    key background height (unit; inherits from legend.key.size)
> legend.key.width     key background width (unit; inherits from legend.key.size)
> legend.text     legend item labels (element_text; inherits from text)
> legend.text.align    alignment of legend labels (number from 0 (left) to 1 (right))
> legend.title    title of legend (element_text; inherits from title)
> legend.title.align   alignment of legend title (number from 0 (left) to 1 (right))
> legend.position the position of legends ("none", "left", "right", "bottom", "top", or two-element numeric vector)
> legend.direction     layout of items in legends ("horizontal" or "vertical")
> legend.justification anchor point for positioning legend inside plot ("center" or two-element numeric vector)
> legend.box arrangement of multiple legends ("horizontal" or "vertical")
> legend.box.just justification of each legend within the overall bounding box, when there are multiple legends ("top", "bottom", "left", or "right")
>
> --
>
> GG
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
1

> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8192 | ronaldo.reis at unimontes.br
| http://www.ppgcb.unimontes.br/lecc | LinuxUser#: 205366


	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Wed Dec  9 16:20:24 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 9 Dec 2015 15:20:24 +0000
Subject: [R] Custom manual legend in ggplot2
Message-ID: <248E6FA047A8C746BA491485764190F526640524@ESESSMB207.ericsson.se>

Last "+theme_bw()" to be deleted.

Try this:

ggplot(data1, aes(x=x1, y=y1))+
  geom_point()+
  geom_smooth(method="glm", family="gaussian",aes(linetype="equation1"))+
  geom_smooth(aes(x=x1, y=y1, linetype="equation2"),data=data2, method="glm", family="gaussian")+
  scale_linetype_manual(values = c("solid","dashed"),name="Equations",
                        labels = c("Equation 1","Equation 2"),guide="legend")+
  theme(plot.title = element_text(lineheight=.8, face="bold"), legend.position="top")


--

GG

http://around-r.blogspot.it




	[[alternative HTML version deleted]]


From brant.inman at me.com  Tue Dec  8 23:02:15 2015
From: brant.inman at me.com (Brant Inman)
Date: Tue, 08 Dec 2015 22:02:15 +0000 (GMT)
Subject: [R] =?utf-8?q?Testing_a_non-binary_categorical_variable_with_coxp?=
 =?utf-8?q?h_and_robust_variance?=
Message-ID: <734408aa-814f-48b9-b071-6e6305aae2fa@me.com>


From ulrik.stervbo at gmail.com  Wed Dec  9 14:17:29 2015
From: ulrik.stervbo at gmail.com (Ulrik Stervbo)
Date: Wed, 09 Dec 2015 13:17:29 +0000
Subject: [R] applying wilcox.test to every combination of rows in a
	matrix (pairwise)
In-Reply-To: <1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>
References: <1332848668.17996960.1449665377363.JavaMail.yahoo.ref@mail.yahoo.com>
	<1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAKVAULO6bVjvJA5mAxLp-R+neu_QR6679GwWJnab7kfpNgfrnQ@mail.gmail.com>

Could you use expand.grid and loop over each returned row?

Best,
Ulrik

On Wed, 9 Dec 2015 at 13:55 debra ragland via R-help <r-help at r-project.org>
wrote:

> Hello All,
>
> I have written the following loop which will apply/split the same vector
> of numbers (pc1.eigv) to each (logical) row of a matrix and run a
> wilcox.test on those values that line up with TRUE and those that line up
> with FALSE. It works fine. However, I am now interested in using the same
> vector and (logical)matrix run the wilcox.test only this time I would like
> information about pairs of rows (not just single rows as it already does).
>
> The loop:
> n.iteration=dim(as.matrix(p))[1]
> n.test= rep(NA, n.iteration )
> for( i in 1:n.iteration ){  ## i=1
> i_spl<-split(pc1.eigv, p[i,])
> if( sum(p[i,])==15 | sum(p[i,])==0) { n.test[i]=NA  }
> if( sum(p[i,])!=15 & sum(p[i,])!=0) {
> testout=wilcox.test(i_spl$'TRUE', i_spl$'FALSE')
> n.test[i]=testout$p.value     }
> }
>
>
> some sample data
> p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3),
> rep(c(F,F,T),3)))
>
> pc1.eigv<-runif(4, 1.0, 2.0)
>
> After some searching I thought that perhaps the combn function would help
> me (i.e. combn(p)) for the same loop but I get an error.
>
> Can anyone help with this?
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From chrysopix at gmail.com  Wed Dec  9 16:24:47 2015
From: chrysopix at gmail.com (=?UTF-8?Q?Ronaldo_Reis_J=c3=banior?=)
Date: Wed, 9 Dec 2015 13:24:47 -0200
Subject: [R] Custom manual legend in ggplot2
In-Reply-To: <5668449B.9080200@gmail.com>
References: <248E6FA047A8C746BA491485764190F5266402FC@ESESSMB207.ericsson.se>
	<5668449B.9080200@gmail.com>
Message-ID: <566847BF.7030305@gmail.com>

Hi,

Now its work, just invert the theme() and theme_bw() position. The 
theme_bw() was overlapping the legend position.

Thanks
Ronaldo

Em 09-12-2015 13:11, Ronaldo Reis J?nior escreveu:
> Thanks,
>
> Now its work, but I dont get to change te legend position. Look:
>
>
> ggplot(data1, aes(x=x1, y=y1))+
>     geom_point()+
>     geom_smooth(method="glm", 
> family="gaussian",aes(linetype="equation1"))+
>     geom_smooth(aes(x=x1, y=y1, linetype="equation2"),data=data2, 
> method="glm", family="gaussian")+
>     scale_linetype_manual(values = 
> c("solid","dashed"),name="Equations",labels=c("Equation 1","Equation 
> 2"),guide="legend")+
>     theme(plot.title = element_text(lineheight=.8, 
> face="bold"),legend.position="top")+
>     theme_bw()
>
> Now the legend work, but the argument legend.position="top" in theme() 
> function dont work. I try to change the order of commands, but without 
> effects.
>
> Any idea?
>
> Thanks
> Ronaldo
>
> Em 09-12-2015 12:10, Giorgio Garziano escreveu:
>> Try to look at the link:
>>
>> http://www.cookbook-r.com/Graphs/Legends_(ggplot2)/
>>
>>
>> Also consider:
>>
>> help(theme)
>>
>> legend.background    background of legend (element_rect; inherits from rect)
>> legend.margin   extra space added around legend (unit)
>> legend.key background underneath legend keys (element_rect; inherits from rect)
>> legend.key.size size of legend keys (unit; inherits from legend.key.size)
>> legend.key.height    key background height (unit; inherits from legend.key.size)
>> legend.key.width     key background width (unit; inherits from legend.key.size)
>> legend.text     legend item labels (element_text; inherits from text)
>> legend.text.align    alignment of legend labels (number from 0 (left) to 1 (right))
>> legend.title    title of legend (element_text; inherits from title)
>> legend.title.align   alignment of legend title (number from 0 (left) to 1 (right))
>> legend.position the position of legends ("none", "left", "right", "bottom", "top", or two-element numeric vector)
>> legend.direction     layout of items in legends ("horizontal" or "vertical")
>> legend.justification anchor point for positioning legend inside plot ("center" or two-element numeric vector)
>> legend.box arrangement of multiple legends ("horizontal" or "vertical")
>> legend.box.just justification of each legend within the overall bounding box, when there are multiple legends ("top", "bottom", "left", or "right")
>>
>> --
>>
>> GG
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org  mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guidehttp://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> -- 
> 1
>
> > Prof. Ronaldo Reis J?nior
> |  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
> | : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
> | `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
> |   `- Fone: (38) 3229-8192 |ronaldo.reis at unimontes.br  
> |http://www.ppgcb.unimontes.br/lecc  | LinuxUser#: 205366

-- 
1

> Prof. Ronaldo Reis J?nior
|  .''`. UNIMONTES/DBG/Lab. Ecologia Comportamental e Computacional
| : :'  : Campus Universit?rio Prof. Darcy Ribeiro, Vila Mauric?ia
| `. `'` CP: 126, CEP: 39401-089, Montes Claros - MG - Brasil
|   `- Fone: (38) 3229-8192 | ronaldo.reis at unimontes.br
| http://www.ppgcb.unimontes.br/lecc | LinuxUser#: 205366


	[[alternative HTML version deleted]]


From Ramgad82 at gmx.net  Wed Dec  9 16:32:39 2015
From: Ramgad82 at gmx.net (Dagmar)
Date: Wed, 9 Dec 2015 16:32:39 +0100
Subject: [R] expand dataframe but time gap is not the same
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5003383@SRVEXCHMBX.precheza.cz>
References: <56601ECC.9040001@gmx.net>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5003383@SRVEXCHMBX.precheza.cz>
Message-ID: <56684997.4080607@gmx.net>

Hello Petr, hi all,

Thank you very much for trying to help me Petr! Unfortunately it didn't 
bring me further.
I do not want to round on full hours but I want uneven intervals to 6 
portions. That is probably the biggest part of my problem.
Does anyone know a solution?
That would be very, very good

Thank you in advance,
Dagmar


Am 03.12.2015 um 15:30 schrieb PIKAL Petr:
> Hi
>
> I am not completely sure what you want.
>
> Maybe something like
>
> datframe$datum<-strptime(paste(datframe[,1], datframe[,2]), format="%d.%m.%Y %H:%M")
> datframe$datum <- strptime(format(datframe$datum, "%d.%m.%Y %H"), format="%d.%m.%Y %H")
> datframe$datum<-as.POSIXct(datframe$datum)
>
> dat<-seq(min(datframe$datum), by="hour", length.out=50)
> temp<-data.frame(dat=dat)
> myframe <- merge(datframe, temp, by.x="datum", by.y="dat", all=TRUE)
>
> can help you a bit.
>
> You can fill missing tides by e.g. na.locf from zoo package.
>
> I know that it is not exactly what you wanted, however it seems to me that having data frame with equally spaced time can be better approach than dividing unequal intervals to 6 portions.
>
> Cheers
> Petr
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dagmar
>> Sent: Thursday, December 03, 2015 11:52 AM
>> To: R help
>> Subject: [R] expand dataframe but time gap is not the same
>>
>> Hello,
>> I hope someone can help me with my problem:
>> I have a dataframe like this:
>>
>> datframe <- data.frame(dates=c("02.08.2013","03.08.2013","03.08.2013"),
>> times =c("22:10","4:04", "10:18"), tide =c("NW","HW", "NW"))
>> datframe
>> Timestamp <- paste(datframe$dates, datframe$times)
>> myframe2 <- cbind( Timestamp,datframe)
>> head(myframe2)
>> myframe2$dates <- NULL
>> myframe2$times <- NULL
>> myframe2$Timestamp <- as.POSIXct (strptime(myframe2$Timestamp,
>> "%d.%m.%Y
>> %H:%M"), tz="GMT")
>> head(myframe2)
>> str(myframe2)
>>
>> # In the end I want a frame like this:
>> datframeres <- data.frame(Timestamp=c("2013-08-02 22:10:00", "2013-08-
>> 02
>> 23:09:00","2013-08-03 00:08:00","2013-08-03 01:07:00", "2013-08-03
>> 02:06:00", "2013-08-03 03:05:00",
>> "2013-08-03 04:04:00","2013-08-03 05:06:20","2013-08-03 06:08:40",
>> "2013-08-03 07:11:00", "2013-08-03 08:13:20", "2013-08-03 09:15:40",
>> "2013-08-03 10:18:00")
>> , tidalclass =c("NW", "HW-5","HW-4", "HW-3", "HW-2", "HW-1", "HW",
>> "HW+1","HW+2","HW+3", "HW+4","HW+5", "NW"))
>>
>> datframeres
>>
>> # That means: I want to expand to 13 classes instead of the two classes
>> "HW" and "NW": "HW", "HW+1", "HW+2" and so on.
>> # The time gap between HW and NW is not always quite the same (always
>> around 6 hours). So I would divide the time gap by 6 and add this
>> number
>> (0:59 and 1:02:20 respectively) to the timestamp before.
>> # I do not know how to do this. Does anyone know how to do this? Many
>> thanks in advance!
>> Tagmarie
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From ragland.debra at yahoo.com  Wed Dec  9 17:00:21 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Wed, 9 Dec 2015 16:00:21 +0000 (UTC)
Subject: [R] applying wilcox.test to every combination of rows in a
 matrix (pairwise)
In-Reply-To: <1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>
References: <1332848668.17996960.1449665377363.JavaMail.yahoo.ref@mail.yahoo.com>
	<1332848668.17996960.1449665377363.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <447202852.18004550.1449676822517.JavaMail.yahoo@mail.yahoo.com>

Sorry for the repost, but I want to clarify that I am trying to apply the wilcox.test to every pairwise combination of rows i.e. row 1 with row 2, row 1 with row 3, row 1 with row 4 and so on until all row combinations have been achieved.
 I've made some corrections.



On Wednesday, December 9, 2015 7:49 AM, debra ragland <ragland.debra at yahoo.com> wrote:
Hello All,

I have written the following loop which will apply/split the same vector of numbers (pc1.eigv) to each (logical) row of a matrix and run a wilcox.test on those values that line up with TRUE and those that line up with FALSE. It works fine. However, I am now interested in using the same vector and (logical)matrix run the wilcox.test only this time I would like information about pairs of rows (not just single rows as it already does).

The loop:
n.iteration=dim(as.matrix(p))[1]
n.test= rep(NA, n.iteration )
for( i in 1:n.iteration ){  ## i=1
i_spl<-split(pc1.eigv, p[i,])
if( sum(p[i,])==15 | sum(p[i,])==0) { n.test[i]=NA  }
if( sum(p[i,])!=15 & sum(p[i,])!=0) { 
testout=wilcox.test(i_spl$'TRUE', i_spl$'FALSE')
n.test[i]=testout$p.value     }
}


some sample data
p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)

pc1.eigv<-runif(4, 1.0, 2.0)

After some searching I thought that perhaps the combn function would help me (i.e. combn(nrow(p),2) for the same loop but I get an error.

Can anyone help with this?


From giorgio.garziano at ericsson.com  Wed Dec  9 17:46:06 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 9 Dec 2015 16:46:06 +0000
Subject: [R] change the x axis tickmarks when using plot function in drc
 package
Message-ID: <248E6FA047A8C746BA491485764190F52664074E@ESESSMB207.ericsson.se>

Looking at the source code of the package drc, there is something that may somehow explain what
you are experiencing:

file: plot.drc.R, function addAxes(), lines 543-626

ceilingxTicks <- ceiling(log10(xaxisTicks[-1]))
...
xaxisTicks <- c(xaxisTicks[1], 10^(unique(ceilingxTicks)))
....
xLabels <- as.character(xaxisTicks)


I may suggest two options:


1.  provide the x labels at plot() call time:



   plot(mod, type="all", log="x", xtlab = c(-2, -1, 0, 1, 2), xlab="log(dose)")


2.  try to use the option logDose in drm():

      mod <- drm(y~log(dose), fct = LL.4(), logDose=10)
      plot(mod, type="all")

Not sure if that second option fits your needs.


--
GG


	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Dec  9 18:07:24 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 9 Dec 2015 09:07:24 -0800
Subject: [R] expand dataframe but time gap is not the same
In-Reply-To: <56601ECC.9040001@gmx.net>
References: <56601ECC.9040001@gmx.net>
Message-ID: <CAF8bMcZjxobGuC3_xj_b+5CqTFCGrU_kFwr4CX+byMmcnfTRfA@mail.gmail.com>

You can use the approx() function (in that stats package) to put
5 equally spaced times between your high and low water times.
E.g., in the following 'tmp' will be your interpolated times, in seconds
since 1970, which I clumsily convert to POSIX times (I can never
remember how to deal with time zones so I make everything use UTC).

> tmp <- with(myframe2, approx(x=seq(1, by=6, length.out=length(Timestamp)), y=as.numeric(Timestamp), xout=seq(1,by=1,length.out=6*(length(Timestamp)-1)+1))$y)
> as.POSIXct(i, origin="1970-01-01 00:00", tz="UTC")
 [1] "2013-08-02 22:10:00 UTC" "2013-08-02 23:09:00 UTC"
 [3] "2013-08-03 00:08:00 UTC" "2013-08-03 01:07:00 UTC"
 [5] "2013-08-03 02:06:00 UTC" "2013-08-03 03:05:00 UTC"
 [7] "2013-08-03 04:04:00 UTC" "2013-08-03 05:06:20 UTC"
 [9] "2013-08-03 06:08:40 UTC" "2013-08-03 07:11:00 UTC"
[11] "2013-08-03 08:13:20 UTC" "2013-08-03 09:15:40 UTC"
[13] "2013-08-03 10:18:00 UTC"
> diff(.Last.value)
Time differences in mins
 [1] 59.00000 59.00000 59.00000 59.00000 59.00000 59.00000 62.33333 62.33333
 [9] 62.33333 62.33333 62.33333 62.33333

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Thu, Dec 3, 2015 at 2:51 AM, Dagmar <Ramgad82 at gmx.net> wrote:
> Hello,
> I hope someone can help me with my problem:
> I have a dataframe like this:
>
> datframe <- data.frame(dates=c("02.08.2013","03.08.2013","03.08.2013"),
> times =c("22:10","4:04", "10:18"), tide =c("NW","HW", "NW"))
> datframe
> Timestamp <- paste(datframe$dates, datframe$times)
> myframe2 <- cbind( Timestamp,datframe)
> head(myframe2)
> myframe2$dates <- NULL
> myframe2$times <- NULL
> myframe2$Timestamp <- as.POSIXct (strptime(myframe2$Timestamp, "%d.%m.%Y
> %H:%M"), tz="GMT")
> head(myframe2)
> str(myframe2)
>
> # In the end I want a frame like this:
> datframeres <- data.frame(Timestamp=c("2013-08-02 22:10:00", "2013-08-02
> 23:09:00","2013-08-03 00:08:00","2013-08-03 01:07:00", "2013-08-03
> 02:06:00", "2013-08-03 03:05:00",
> "2013-08-03 04:04:00","2013-08-03 05:06:20","2013-08-03 06:08:40",
> "2013-08-03 07:11:00", "2013-08-03 08:13:20", "2013-08-03 09:15:40",
> "2013-08-03 10:18:00")
> , tidalclass =c("NW", "HW-5","HW-4", "HW-3", "HW-2", "HW-1", "HW",
> "HW+1","HW+2","HW+3", "HW+4","HW+5", "NW"))
>
> datframeres
>
> # That means: I want to expand to 13 classes instead of the two classes "HW"
> and "NW": "HW", "HW+1", "HW+2" and so on.
> # The time gap between HW and NW is not always quite the same (always around
> 6 hours). So I would divide the time gap by 6 and add this number (0:59 and
> 1:02:20 respectively) to the timestamp before.
> # I do not know how to do this. Does anyone know how to do this? Many thanks
> in advance!
> Tagmarie
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Wed Dec  9 18:35:56 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 9 Dec 2015 17:35:56 +0000
Subject: [R] applying wilcox.test to every combination of rows in a
 matrix (pairwise)
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E7C9F@mb02.ads.tamu.edu>

If I understand correctly, this should do what you want, but there will be warnings for each test about p-values not being exact because you reuse the pc1.eigv vector for each row so that each value occurs twice:

First we can simplify the original comparisons by using the formula mode for wilcox.test:

> p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3),
+      rep(c(F,F,T),3)), ncol=4)
> set.seed(42)
> pc1.eigv<-runif(4, 1.0, 2.0)
> n.iteration=dim(as.matrix(p))[1]
> n.test <- sapply(seq_len(n.iteration), function(i) 
+      wilcox.test(pc1.eigv~p[i,])$p.value)
> n.test
[1] 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667
[7] 1.0000000 0.6666667 0.6666667

Then we generate the row combinations:

> rows <- expand.grid(i=1:9, j=1:9) # All 81 row combinations
> rows <- rows[rows$j < rows$i, ] # Just the 36 distinct combinations
> vals <- rep(pc1.eigv, 2)  # Double the pc1.eigv vector
> tf <- cbind(p[rows[, 1] ,], p[rows[, 2], ]) # Create the combined row vector
> vals # The same values for all comparisons
[1] 1.914806 1.937075 1.286140 1.830448 1.914806 1.937075 1.286140
[8] 1.830448
> tf[1:2, ] # First row combines rows 2 and 1, second 3 and 1, etc
[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE
[2,] FALSE  TRUE FALSE  TRUE FALSE TRUE TRUE FALSE
> n.iteration <- dim(tf)[1]
> n.test2 <- sapply(seq_len(n.iteration), function(i) 
+      wilcox.test(vals~tf[i,])$p.value)
There were 36 warnings (use warnings() to see them)
> n.test2
 [1] 0.6572552 0.6572552 1.0000000 0.6572552 0.6572552 1.0000000
 [7] 0.6572552 0.6572552 1.0000000 0.6572552 0.3005223 1.0000000
[13] 0.6572552 0.3005223 1.0000000 0.6572552 1.0000000 0.3005223
[19] 0.6572552 1.0000000 0.3005223 0.6572552 0.6572552 1.0000000
[25] 0.6572552 0.6572552 1.0000000 0.6572552 0.3005223 1.0000000
[31] 0.6572552 1.0000000 0.3005223 0.6572552 0.6572552 1.0000000

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of debra ragland via R-help
Sent: Wednesday, December 9, 2015 10:00 AM
To: R-help
Subject: Re: [R] applying wilcox.test to every combination of rows in a matrix (pairwise)

Sorry for the repost, but I want to clarify that I am trying to apply the wilcox.test to every pairwise combination of rows i.e. row 1 with row 2, row 1 with row 3, row 1 with row 4 and so on until all row combinations have been achieved.
 I've made some corrections.



On Wednesday, December 9, 2015 7:49 AM, debra ragland <ragland.debra at yahoo.com> wrote:
Hello All,

I have written the following loop which will apply/split the same vector of numbers (pc1.eigv) to each (logical) row of a matrix and run a wilcox.test on those values that line up with TRUE and those that line up with FALSE. It works fine. However, I am now interested in using the same vector and (logical)matrix run the wilcox.test only this time I would like information about pairs of rows (not just single rows as it already does).

The loop:
n.iteration=dim(as.matrix(p))[1]
n.test= rep(NA, n.iteration )
for( i in 1:n.iteration ){  ## i=1
i_spl<-split(pc1.eigv, p[i,])
if( sum(p[i,])==15 | sum(p[i,])==0) { n.test[i]=NA  }
if( sum(p[i,])!=15 & sum(p[i,])!=0) { 
testout=wilcox.test(i_spl$'TRUE', i_spl$'FALSE')
n.test[i]=testout$p.value     }
}


some sample data
p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)

pc1.eigv<-runif(4, 1.0, 2.0)

After some searching I thought that perhaps the combn function would help me (i.e. combn(nrow(p),2) for the same loop but I get an error.

Can anyone help with this?

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From hannah.hlx at gmail.com  Wed Dec  9 19:24:04 2015
From: hannah.hlx at gmail.com (li li)
Date: Wed, 9 Dec 2015 13:24:04 -0500
Subject: [R] change the x axis tickmarks when using plot function in drc
	package
In-Reply-To: <248E6FA047A8C746BA491485764190F52664074E@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F52664074E@ESESSMB207.ericsson.se>
Message-ID: <CAHLnndbTskOavK7OnySvdhvqLR_aO9-YHC6S+C+bjw5==6yGnw@mail.gmail.com>

Thanks for your help Giorgio! Both options worked.

2015-12-09 11:46 GMT-05:00 Giorgio Garziano <giorgio.garziano at ericsson.com>:

> Looking at the source code of the package drc, there is something that may
> somehow explain what
> you are experiencing:
>
> file: plot.drc.R, function addAxes(), lines 543-626
>
> ceilingxTicks <- ceiling(log10(xaxisTicks[-1]))
> ...
> xaxisTicks <- c(xaxisTicks[1], 10^(unique(ceilingxTicks)))
> ....
> xLabels <- as.character(xaxisTicks)
>
>
> I may suggest two options:
>
>
> 1.  provide the x labels at plot() call time:
>
>
>
>    plot(mod, type="all", log="x", xtlab = c(-2, -1, 0, 1, 2),
> xlab="log(dose)")
>
>
> 2.  try to use the option logDose in drm():
>
>       mod <- drm(y~log(dose), fct = LL.4(), logDose=10)
>       plot(mod, type="all")
>
> Not sure if that second option fits your needs.
>
>
> --
> GG
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From petretta at unina.it  Wed Dec  9 19:48:56 2015
From: petretta at unina.it (Mario Petretta)
Date: Wed, 9 Dec 2015 19:48:56 +0100
Subject: [R] forest plot metafor
Message-ID: <003e01d132b2$424f8450$c6ee8cf0$@it>

Dear all,



I use metafor package to generate a forest plot showing the weight of each study in the plot.



I use the code:

library(metafor)

data(dat.bcg)

res <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, measure="RR",

           slab=paste(author, year, sep=", "), method="REML")

forest(res, showweights=TRUE)



It is possible to order the columns, placing the weights after effect size and CI?



A further query: using escalc (or rma), it is possible to add the weight of each study (other than yi and vi) to the data (in the example: data.bcg)?



Thanks for your attention



________________________



Mario Petretta

Professor of Internal Medicine

Department of Translational Medical Science

Naples University ?Federico II? - Italy





  _____


 <http://www.avast.com/>

Questa e-mail ? priva di virus e malware perch? ? attiva la protezione avast! Antivirus <http://www.avast.com/>  .



Nessun virus nel messaggio.
Controllato da AVG - www.avg.com
Versione: 2014.0.4259 / Database dei virus: 3705/7093 - Data di rilascio: 14/02/2014



---
Questa e-mail ? stata controllata per individuare virus con Avast antivirus.
https://www.avast.com/antivirus

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Wed Dec  9 20:01:05 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 9 Dec 2015 19:01:05 +0000
Subject: [R] applying wilcox.test to every combination of rows in a
 matrix (pairwise)
In-Reply-To: <464635811.17985688.1449683239886.JavaMail.yahoo@mail.yahoo.com>
References: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E7C71@mb02.ads.tamu.edu>
	<464635811.17985688.1449683239886.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E7D05@mb02.ads.tamu.edu>

The error message suggests that you have more than TRUE and FALSE in your logical matrix. What does str(p) show you? Could you have missing values?

David C

-----Original Message-----
From: debra ragland [mailto:ragland.debra at yahoo.com] 
Sent: Wednesday, December 9, 2015 11:47 AM
To: David L Carlson
Subject: Re: [R] applying wilcox.test to every combination of rows in a matrix (pairwise)

Hi David, 

Thank you so much for your input. As with the other suggestions I've gotten, I keep getting errors. In reality, the logical matrix has 15 col and 99 rows. Using your suggestion I get the errors


Error in wilcox.test.formula(pc1.eigv ~ p[i, ]) : 
grouping factor must have exactly 2 levels 
AND
Error in wilcox.test.formula(vals ~ tf[i, ]) : 
grouping factor must have exactly 2 levels 




On Wednesday, December 9, 2015 12:28 PM, David L Carlson <dcarlson at tamu.edu> wrote:
If I understand correctly, this should do what you want, but there will be warnings for each test about p-values not being exact because you reuse the pc1.eigv vector for each row so that each value occurs twice:

First we can simplify the original comparisons by using the formula mode for wilcox.test:

> p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3),
+      rep(c(F,F,T),3)), ncol=4)
> set.seed(42)
> pc1.eigv<-runif(4, 1.0, 2.0)
> n.iteration=dim(as.matrix(p))[1]
> n.test <- sapply(seq_len(n.iteration), function(i) 
+      wilcox.test(pc1.eigv~p[i,])$p.value)
> n.test
[1] 1.0000000 0.6666667 0.6666667 1.0000000 0.6666667 0.6666667
[7] 1.0000000 0.6666667 0.6666667

Then we generate the row combinations:

> rows <- expand.grid(i=1:9, j=1:9) # All 81 row combinations
> rows <- rows[rows$j < rows$i, ] # Just the 36 distinct combinations
> vals <- rep(pc1.eigv, 2)  # Double the pc1.eigv vector
> tf <- cbind(p[rows[, 1] ,], p[rows[, 2], ]) # Create the combined row vector
> vals # The same values for all comparisons
[1] 1.914806 1.937075 1.286140 1.830448 1.914806 1.937075 1.286140
[8] 1.830448
> tf[1:2, ] # First row combines rows 2 and 1, second 3 and 1, etc
[1]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE
[2,] FALSE  TRUE FALSE  TRUE FALSE TRUE TRUE FALSE
> n.iteration <- dim(tf)[1]
> n.test2 <- sapply(seq_len(n.iteration), function(i) 
+      wilcox.test(vals~tf[i,])$p.value)
There were 36 warnings (use warnings() to see them)
> n.test2
[1] 0.6572552 0.6572552 1.0000000 0.6572552 0.6572552 1.0000000
[7] 0.6572552 0.6572552 1.0000000 0.6572552 0.3005223 1.0000000
[13] 0.6572552 0.3005223 1.0000000 0.6572552 1.0000000 0.3005223
[19] 0.6572552 1.0000000 0.3005223 0.6572552 0.6572552 1.0000000
[25] 0.6572552 0.6572552 1.0000000 0.6572552 0.3005223 1.0000000
[31] 0.6572552 1.0000000 0.3005223 0.6572552 0.6572552 1.0000000

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of debra ragland via R-help
Sent: Wednesday, December 9, 2015 10:00 AM
To: R-help
Subject: Re: [R] applying wilcox.test to every combination of rows in a matrix (pairwise)

Sorry for the repost, but I want to clarify that I am trying to apply the wilcox.test to every pairwise combination of rows i.e. row 1 with row 2, row 1 with row 3, row 1 with row 4 and so on until all row combinations have been achieved.
I've made some corrections.



On Wednesday, December 9, 2015 7:49 AM, debra ragland <ragland.debra at yahoo.com> wrote:
Hello All,

I have written the following loop which will apply/split the same vector of numbers (pc1.eigv) to each (logical) row of a matrix and run a wilcox.test on those values that line up with TRUE and those that line up with FALSE. It works fine. However, I am now interested in using the same vector and (logical)matrix run the wilcox.test only this time I would like information about pairs of rows (not just single rows as it already does).

The loop:
n.iteration=dim(as.matrix(p))[1]
n.test= rep(NA, n.iteration )
for( i in 1:n.iteration ){  ## i=1
i_spl<-split(pc1.eigv, p[i,])
if( sum(p[i,])==15 | sum(p[i,])==0) { n.test[i]=NA  }
if( sum(p[i,])!=15 & sum(p[i,])!=0) { 
testout=wilcox.test(i_spl$'TRUE', i_spl$'FALSE')
n.test[i]=testout$p.value     }
}


some sample data
p<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)

pc1.eigv<-runif(4, 1.0, 2.0)

After some searching I thought that perhaps the combn function would help me (i.e. combn(nrow(p),2) for the same loop but I get an error.

Can anyone help with this?

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code. 

From clivelists at googlemail.com  Wed Dec  9 21:20:55 2015
From: clivelists at googlemail.com (Clive Nicholas)
Date: Wed, 9 Dec 2015 20:20:55 +0000
Subject: [R] change the x axis tickmarks when using plot function in
	-drc- package
Message-ID: <CAHs5aTgQFcvWpbskRt1myaF3i6iKZyOTPeBbafeEOsTc75V-WA@mail.gmail.com>

Message: 30
Date: Tue, 8 Dec 2015 22:56:00 -0500
From: li li <hannah.hlx at gmail.com>
To: Jim Lemon <drjimlemon at gmail.com>
Cc: r-help <r-help at r-project.org>
Subject: Re: [R] change the x axis tickmarks when using plot function
        in drc  package
Message-ID:
        <CAHLnndZv6=AZjT_Y7M3i=t01qcb0L9iWAfmrjJ4ehvYwUe52aA at mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"

Thanks for the reply but that does not seem to work. With that the plot is
on log scale for both the response and the dose levels, but the tickmarks
are still on the original scale.

[...]

I'm viewing this on the digest, so I'm behind, but I ran your code plus
Jim's suggestion and it worked for me! You should get something akin to the
image I attach for you.

I hope this helps.

R version 3.2.2 (2015-08-14)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 15.10

locale:
 [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
LC_TIME=en_GB.UTF-8
 [4] LC_COLLATE=en_GB.UTF-8     LC_MONETARY=en_GB.UTF-8
LC_MESSAGES=en_GB.UTF-8
 [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C
LC_ADDRESS=C
[10] LC_TELEPHONE=C             LC_MEASUREMENT=en_GB.UTF-8
LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] drc_2.5-12  MASS_7.3-43

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.2        splines_3.2.2      munsell_0.4.2
colorspace_1.2-6   lattice_0.20-33
 [6] multcomp_1.4-1     minqa_1.2.4        plyr_1.8.3
car_2.1-0          tools_3.2.2
[11] nnet_7.3-10        parallel_3.2.2     pbkrtest_0.4-2
grid_3.2.2         nlme_3.1-122
[16] mgcv_1.8-7         quantreg_5.19      plotrix_3.6
TH.data_1.0-6      htmltools_0.2.6
[21] MatrixModels_0.4-1 gtools_3.5.0       digest_0.6.8
survival_2.38-3    lme4_1.1-10
[26] Matrix_1.2-2       nloptr_1.0.4       codetools_0.2-14
rmarkdown_0.8.1    sandwich_2.3-4
[31] scales_0.3.0       SparseM_1.7        mvtnorm_1.0-3      zoo_1.7-12

-- 
Clive Nicholas

"My colleagues in the social sciences talk a great deal about methodology.
I prefer to call it style." -- Freeman J. Dyson
-------------- next part --------------
A non-text attachment was scrubbed...
Name: drc_log.pdf
Type: application/pdf
Size: 7090 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151209/803843da/attachment.pdf>

From hannah.hlx at gmail.com  Wed Dec  9 21:27:48 2015
From: hannah.hlx at gmail.com (li li)
Date: Wed, 9 Dec 2015 15:27:48 -0500
Subject: [R] Restricted 4-PL curves using drc package
Message-ID: <CAHLnndb92_oqQbcwxkDDTfJ4CXRZhNJMaVUM0CpZtDBY4qHbNw@mail.gmail.com>

Hi all,
  In drc package, is there a function which can be used to fit restricted
4PL curves? For example, we restrict two 4PL curves have the same lower and
upper asymptotes?
  Thanks for the help in advance!
    Hanna

	[[alternative HTML version deleted]]


From micha at arava.co.il  Wed Dec  9 19:58:57 2015
From: micha at arava.co.il (Micha Silver)
Date: Wed, 9 Dec 2015 20:58:57 +0200
Subject: [R] seq.POSIXt creates duplicate entries
Message-ID: <566879F1.4070904@arava.co.il>

Hello all:
I am creating a time series using seq.POSIXt.

ts.full <- as.character(seq.POSIXt(as.POSIXct(start(d.zoo)), 
as.POSIXct(end(d.zoo)), by="hour"))

The period crosses the hour of change from "summer time" to "winter 
time". When checking with:
duplicated(ts.full)
I see that there are some duplicate hours. These fall right at the time 
change. How can I avoid this? (or remove the duplicates)?

Thanks
-- 
Micha Silver
Arava Drainage Authority
+972-523-665918


From bgunter.4567 at gmail.com  Wed Dec  9 22:24:34 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 9 Dec 2015 13:24:34 -0800
Subject: [R] Restricted 4-PL curves using drc package
In-Reply-To: <CAHLnndb92_oqQbcwxkDDTfJ4CXRZhNJMaVUM0CpZtDBY4qHbNw@mail.gmail.com>
References: <CAHLnndb92_oqQbcwxkDDTfJ4CXRZhNJMaVUM0CpZtDBY4qHbNw@mail.gmail.com>
Message-ID: <CAGxFJbTSetUGnaLiG18UKU_H3xtk_G1E=mJNzP52uO60sY0fvg@mail.gmail.com>

1. Don't know, but, assuming I understand you correctly, probably not.

2 Note (subject to my understanding again) that you can rephrase your
query as fitting a 4 parameter logistic to all the data together with
a grouping factor to indicate the separate curves, allowing two of the
parameters (e.g. ED50 and slope) to vary by group. You can then use
standard nonlinear fitting (e.g. via the optimx package) to fit the
model. Note that you will have to specify 2c +2 starting values for c
curves. With c "large" (?whatever that means), this may make
convergence tricky.

If you care to reply, please do so to the list, not to me.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 9, 2015 at 12:27 PM, li li <hannah.hlx at gmail.com> wrote:
> Hi all,
>   In drc package, is there a function which can be used to fit restricted
> 4PL curves? For example, we restrict two 4PL curves have the same lower and
> upper asymptotes?
>   Thanks for the help in advance!
>     Hanna
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec 10 00:33:22 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 9 Dec 2015 15:33:22 -0800
Subject: [R] seq.POSIXt creates duplicate entries
In-Reply-To: <566879F1.4070904@arava.co.il>
References: <566879F1.4070904@arava.co.il>
Message-ID: <9516AD0B-C6EA-49C7-9ADD-659F0F8B2D10@comcast.net>


> On Dec 9, 2015, at 10:58 AM, Micha Silver <micha at arava.co.il> wrote:
> 
> Hello all:
> I am creating a time series using seq.POSIXt.
> 
> ts.full <- as.character(seq.POSIXt(as.POSIXct(start(d.zoo)), as.POSIXct(end(d.zoo)), by="hour"))
> 
> The period crosses the hour of change from "summer time" to "winter time". When checking with:
> duplicated(ts.full)
> I see that there are some duplicate hours. These fall right at the time change. How can I avoid this? (or remove the duplicates)?

If you want us to redefine Daylight Savings time, then you should specify in great detail exactly how this should be done. 

I don?t see any duplicates, anyway, since the DST zones are incuded:

> seq(as.POSIXct("2015-10-31 23:00"), as.POSIXct("2015-11-01 4:00"), by="1 hour")
[1] "2015-10-31 23:00:00 PDT" "2015-11-01 00:00:00 PDT" "2015-11-01 01:00:00 PDT"
[4] "2015-11-01 01:00:00 PST" "2015-11-01 02:00:00 PST" "2015-11-01 03:00:00 PST"
[7] "2015-11-01 04:00:00 PST?

But if you use GMT you do get a sequence with no ?duplicates:

> seq(as.POSIXct("2015-10-31 23:00", tz="GMT"), as.POSIXct("2015-11-01 4:00", tz="GMT"), by="1 hour")
[1] "2015-10-31 23:00:00 GMT" "2015-11-01 00:00:00 GMT" "2015-11-01 01:00:00 GMT"
[4] "2015-11-01 02:00:00 GMT" "2015-11-01 03:00:00 GMT" "2015-11-01 04:00:00 GMT"

> 
> Thanks
> -- 
> Micha Silver
> Arava Drainage Authority
> +972-523-665918
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From klebyn at yahoo.com.br  Thu Dec 10 01:00:08 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Wed, 9 Dec 2015 22:00:08 -0200
Subject: [R] there is a vectorinzing version of "[[<-" for "tclArray" class ?
Message-ID: <5668C088.8040408@yahoo.com.br>

hi all,
there is a vectorinzing version of  "[[<-" for "tclArray" class ?
TIA
cleber
#####
 > library(tcltk);  getS3method("[[<-",'tclArray')
function (x, ..., value)
{
     name <- as.character(x)
     i <- paste(..., sep = ",")
     if (is.null(value))
         .External(.C_RTcl_RemoveArrayElem, name, i)
     else {
         value <- as.tclObj(value)
         .External(.C_RTcl_SetArrayElem, name, i, value)
     }
     x
}
<bytecode: 0x000000000e7ab608>
<environment: namespace:tcltk>
 >


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From kristi.glover at hotmail.com  Thu Dec 10 03:20:42 2015
From: kristi.glover at hotmail.com (Kristi Glover)
Date: Thu, 10 Dec 2015 02:20:42 +0000
Subject: [R] Error in run.mark.model(model, invisible = invisible,
 adjust = adjust, -
Message-ID: <BY2PR13MB0454248E4614FCEE68D10B04FAE90@BY2PR13MB0454.namprd13.prod.outlook.com>

Hi R user,

I tried to run a package (Rmark) in MAC and run some of the analyses using the package, but I could not do any analyses.

It says"


"/bin/sh: mark: command not found

Error in run.mark.model(model, invisible = invisible, adjust = adjust,  : "


I installed the package but I think something wrong. How can I fix the problem?  I am a new user of MAC.



For example,

library(RMark)

data(dipper)

# add a time varying covariate for Phi named to match beginning time of each time interval (default begin.time=1 and time.intervals=1)(r1,r2,..r6)

dipper$r1=rep(1,294)

dipper$r2=rep(10,294)

dipper$r3=rep(8,294)

dipper$r4=rep(15,294)

dipper$r5=rep(3,294)

dipper$r6=rep(6,294)

# process data

dp=process.data(dipper)

# create default design data

ddl=make.design.data(dp)

# add rain environmental covariate to design data for Phi; this matches r1 to r6.

ddl$Phi$rain=1

ddl$Phi$rain[ddl$Phi$time==2]=10

ddl$Phi$rain[ddl$Phi$time==3]=8

ddl$Phi$rain[ddl$Phi$time==4]=15

ddl$Phi$rain[ddl$Phi$time==5]=3

ddl$Phi$rain[ddl$Phi$time==6]=6

	[[alternative HTML version deleted]]


From mapjacques at gmail.com  Thu Dec 10 03:09:31 2015
From: mapjacques at gmail.com (Maria Alice Jacques)
Date: Thu, 10 Dec 2015 00:09:31 -0200
Subject: [R] Use of R for estimation of Discrete Choice Models incorporating
 latent variables
Message-ID: <CAJ3zTk=_VA=v5H3Tb2BEvVDKK0xZ6zzYYJw3V5tNYL_Z2N+wqQ@mail.gmail.com>

Dear all,

I would like to know if it is possible to estimate an integrated choice and
latent variable model with R. If it is so, please send me the name of the
respective package.

Best regards,

Maria Alice P. Jacques
Associated Researcher
University of Brasilia - Brazil

	[[alternative HTML version deleted]]


From shq-777 at hotmail.com  Thu Dec 10 02:00:58 2015
From: shq-777 at hotmail.com (MANAL AL-HARBI)
Date: Thu, 10 Dec 2015 04:00:58 +0300
Subject: [R] Help about reliability and accelerated life testin
Message-ID: <DUB407-EAS1908BAB75211C27E1743D3ABFE90@phx.gbl>

Hello 
Can you help me to do programe or order
For ( estimated of constant stress partially accelerated life testing model parameter? )
1/what is the package ?
2/what is the order ?
Thank you 

Manal Alharbi
	[[alternative HTML version deleted]]


From islandelephants at gmail.com  Thu Dec 10 07:46:57 2015
From: islandelephants at gmail.com (=?UTF-8?Q?Alexander_Mo=C3=9Fbrucker?=)
Date: Thu, 10 Dec 2015 13:46:57 +0700
Subject: [R] problems with plotting/graphics after changing my computer
Message-ID: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>

Dear all,

I have worked on AKDEs using the package ctmm. Because my old computer
often crashed, I bought a new one and reinstalled all the software,
including r.

All seemed to work fine, but when I wanted to plot several results I
encountered error messages.

I tried many different plots, some plots work, others not, I could't find
out why. The error message is always " ..... is not a graphical parameter".
I am using exactly the same r script and packages as before on my old
computer.

Would be great if you could help me with this....., best, Alex

Please find below an example from the ctmm package:

> # Load package and data> library(ctmm)> data(buffalo)> cilla <- buffalo[[1]]> # Fit a continuous-velocity model with tau ~ c(10 days, 1 hour)> # see help(variogram.fit)> m2 <- ctmm(tau=c(10*24*60^2,60^2))> M2 <- ctmm.fit(cilla,m2)> # Compute akde object> KD2 <- akde(cilla,M2)> # Plot data with AKDE contours> plot(cilla,AKDE=KD2)Warning messages:1: In plot.window(...) : "AKDE" is not a graphical parameter2: In plot.xy(xy, type, ...) : "AKDE" is not a graphical parameter3: In axis(side = side, at = at, labels = labels, ...) :
  "AKDE" is not a graphical parameter4: In axis(side = side, at = at,
labels = labels, ...) :
  "AKDE" is not a graphical parameter5: In box(...) : "AKDE" is not a
graphical parameter6: In title(...) : "AKDE" is not a graphical
parameter7: In plot.xy(xy.coords(x, y), type = type, ...) :
  "AKDE" is not a graphical parameter





<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
This
email has been sent from a virus-free computer protected by Avast.
www.avast.com
<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

	[[alternative HTML version deleted]]


From micha at arava.co.il  Thu Dec 10 07:44:29 2015
From: micha at arava.co.il (Micha Silver)
Date: Thu, 10 Dec 2015 08:44:29 +0200
Subject: [R] seq.POSIXt creates duplicate entries
In-Reply-To: <9516AD0B-C6EA-49C7-9ADD-659F0F8B2D10@comcast.net>
References: <566879F1.4070904@arava.co.il>
	<9516AD0B-C6EA-49C7-9ADD-659F0F8B2D10@comcast.net>
Message-ID: <56691F4D.9000805@arava.co.il>

Hello David and David:
Thanks for responding

On 12/10/2015 01:33 AM, David Winsemius wrote:
>> On Dec 9, 2015, at 10:58 AM, Micha Silver <micha at arava.co.il> wrote:
>>
>> Hello all:
>> I am creating a time series using seq.POSIXt.
>>
>> ts.full <- as.character(seq.POSIXt(as.POSIXct(start(d.zoo)), as.POSIXct(end(d.zoo)), by="hour"))
>>
>> The period crosses the hour of change from "summer time" to "winter time". When checking with:
>> duplicated(ts.full)
>> I see that there are some duplicate hours. These fall right at the time change. How can I avoid this? (or remove the duplicates)?
> If you want us to redefine Daylight Savings time, then you should specify in great detail exactly how this should be done.
>
> I don?t see any duplicates, anyway, since the DST zones are incuded:
>
>> seq(as.POSIXct("2015-10-31 23:00"), as.POSIXct("2015-11-01 4:00"), by="1 hour")
> [1] "2015-10-31 23:00:00 PDT" "2015-11-01 00:00:00 PDT" "2015-11-01 01:00:00 PDT"
> [4] "2015-11-01 01:00:00 PST" "2015-11-01 02:00:00 PST" "2015-11-01 03:00:00 PST"
> [7] "2015-11-01 04:00:00 PST?

Here's what I get:
 > Sys.time()
[1] "2015-12-10 08:36:58 IST"

 > as.character(seq.POSIXt(as.POSIXct("2015-10-24 23:00"), 
as.POSIXct("2015-10-25 04:00"), by="hour"))
[1] "2015-10-24 23:00:00" "2015-10-25 00:00:00" "2015-10-25 01:00:00"
[4] "2015-10-25 01:00:00" "2015-10-25 02:00:00" "2015-10-25 03:00:00"
[7] "2015-10-25 04:00:00"

with the dups at 01:00.
But then explicitly adding the timezone gives:

 > as.character(seq.POSIXt(as.POSIXct("2015-10-24 23:00"), 
as.POSIXct("2015-10-25 04:00"), by="hour"), tz="IDT")
[1] "2015-10-24 20:00:00" "2015-10-24 21:00:00" "2015-10-24 22:00:00"
[4] "2015-10-24 23:00:00" "2015-10-25 00:00:00" "2015-10-25 01:00:00"
[7] "2015-10-25 02:00:00"

so I guess problem solved, thanks.

> But if you use GMT you do get a sequence with no ?duplicates:
>
>> seq(as.POSIXct("2015-10-31 23:00", tz="GMT"), as.POSIXct("2015-11-01 4:00", tz="GMT"), by="1 hour")
> [1] "2015-10-31 23:00:00 GMT" "2015-11-01 00:00:00 GMT" "2015-11-01 01:00:00 GMT"
> [4] "2015-11-01 02:00:00 GMT" "2015-11-01 03:00:00 GMT" "2015-11-01 04:00:00 GMT"
>
>> Thanks
>> -- 
>> Micha Silver
>> Arava Drainage Authority
>> +972-523-665918
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> David Winsemius
> Alameda, CA, USA
>
>
> This mail was received via Mail-SeCure System.
>
>


From petr.pikal at precheza.cz  Thu Dec 10 08:23:39 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 10 Dec 2015 07:23:39 +0000
Subject: [R] problems with plotting/graphics after changing my computer
In-Reply-To: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>
References: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004114@SRVEXCHMBX.precheza.cz>

Hi

1. Do not post in HTML. Your mail is corrupted and almost unreadable.
2. As far I I could decipher it, your plot call is

plot(cilla,AKDE=KD2)

According to docs the plot function does not have parameter called AKDE

plot(x, CTMM=NULL, UD=NULL, level.UD=0.95, level=0.95, DF="CDF", col="red",
col.level="black", col.DF="blue", col.grid="grey", fraction=1, add=FALSE, xlim=NULL,
ylim=NULL, ...)

but I believe that akde result shall be transferred to UD parameter.

UD Optional UD object such as from the output of akde or list of such objects

So your plot shall be called

plot(cilla, UD=KD2)

I do not have any experience with ctmm though, so I may completely miss the point. But if I am right, you could find all this in help page the same way as I did and probably quicker.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Alexander Mo?brucker
> Sent: Thursday, December 10, 2015 7:47 AM
> To: r-help at r-project.org
> Subject: [R] problems with plotting/graphics after changing my computer
>
> Dear all,
>
> I have worked on AKDEs using the package ctmm. Because my old computer
> often crashed, I bought a new one and reinstalled all the software,
> including r.
>
> All seemed to work fine, but when I wanted to plot several results I
> encountered error messages.
>
> I tried many different plots, some plots work, others not, I could't
> find
> out why. The error message is always " ..... is not a graphical
> parameter".
> I am using exactly the same r script and packages as before on my old
> computer.
>
> Would be great if you could help me with this....., best, Alex
>
> Please find below an example from the ctmm package:
>
> > # Load package and data> library(ctmm)> data(buffalo)> cilla <-
> buffalo[[1]]> # Fit a continuous-velocity model with tau ~ c(10 days, 1
> hour)> # see help(variogram.fit)> m2 <- ctmm(tau=c(10*24*60^2,60^2))>
> M2 <- ctmm.fit(cilla,m2)> # Compute akde object> KD2 <- akde(cilla,M2)>
> # Plot data with AKDE contours> plot(cilla,AKDE=KD2)Warning messages:1:
> In plot.window(...) : "AKDE" is not a graphical parameter2: In
> plot.xy(xy, type, ...) : "AKDE" is not a graphical parameter3: In
> axis(side = side, at = at, labels = labels, ...) :
>   "AKDE" is not a graphical parameter4: In axis(side = side, at = at,
> labels = labels, ...) :
>   "AKDE" is not a graphical parameter5: In box(...) : "AKDE" is not a
> graphical parameter6: In title(...) : "AKDE" is not a graphical
> parameter7: In plot.xy(xy.coords(x, y), type = type, ...) :
>   "AKDE" is not a graphical parameter
>
>
>
>
>
> <https://www.avast.com/lp-safe-
> emailing?utm_medium=email&utm_source=link&utm_campaign=sig-
> email&utm_content=webmail>
> This
> email has been sent from a virus-free computer protected by Avast.
> www.avast.com
> <https://www.avast.com/lp-safe-
> emailing?utm_medium=email&utm_source=link&utm_campaign=sig-
> email&utm_content=webmail>
> <#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.


From phgrosjean at sciviews.org  Thu Dec 10 08:27:46 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Thu, 10 Dec 2015 08:27:46 +0100
Subject: [R] there is a vectorinzing version of "[[<-" for "tclArray"
	class ?
In-Reply-To: <5668C088.8040408@yahoo.com.br>
References: <5668C088.8040408@yahoo.com.br>
Message-ID: <4BB1E0D6-F240-4495-B0EB-9A33572E5902@sciviews.org>

library(tcltk)
methods(class = "tclArray")

## [1] [[       [[<-     $        $<-      length   length<- names    names<- 
## see '?methods' for accessing help and source code

It should be something like `[<-`. But this one is not defined. Perhaps can you contribute one here?
Best,

Philippe Grosjean
> On 10 Dec 2015, at 01:00, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
> 
> hi all,
> there is a vectorinzing version of  "[[<-" for "tclArray" class ?
> TIA
> cleber
> #####
> > library(tcltk);  getS3method("[[<-",'tclArray')
> function (x, ..., value)
> {
>    name <- as.character(x)
>    i <- paste(..., sep = ",")
>    if (is.null(value))
>        .External(.C_RTcl_RemoveArrayElem, name, i)
>    else {
>        value <- as.tclObj(value)
>        .External(.C_RTcl_SetArrayElem, name, i, value)
>    }
>    x
> }
> <bytecode: 0x000000000e7ab608>
> <environment: namespace:tcltk>
> >
> 
> 
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Thu Dec 10 08:37:09 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 10 Dec 2015 07:37:09 +0000
Subject: [R] Importing data by targeting in filenames inside a nested
	list
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CC1A8@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CC1A8@EXDAG0-A1.intra.cea.fr>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004151@SRVEXCHMBX.precheza.cz>

Hi

I did not see any answer so I give it a try. Your approach seems to be OK. However you probably need to polish your code to get the correct part of nested list.

> lll<-list(a=rnorm(10), b= list(x=1:10, y<-letters))

> lll[[1]]
 [1] -0.1876418  1.5933030 -0.1799642  0.1713959  1.1079227 -0.5885820
 [7] -1.1629393 -1.7157378 -1.5088232 -0.1150207

> lll[[2]]
$x
 [1]  1  2  3  4  5  6  7  8  9 10

[[2]]
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"
[20] "t" "u" "v" "w" "x" "y" "z"

> lll[[2]][[1]]
 [1]  1  2  3  4  5  6  7  8  9 10

> lll[[2]][[2]]
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"
[20] "t" "u" "v" "w" "x" "y" "z"
>

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of BARLAS
> Marios 247554
> Sent: Tuesday, December 08, 2015 12:30 PM
> To: r-help at r-project.org
> Subject: [R] Importing data by targeting in filenames inside a nested
> list
>
> Hello everyone,
>
> So, rookie me is trying to write a smart code, so here's what I'm
> doing:
>
> I have a list of a couple of hundrend files, some of which refer to
> different experiments.
> The naming of the file refers to the experiment and the serial number
> to the topological reference on my sample.
>
> Performing my data analysis in 1 file class at a time looks OK, so I'm
> trying to generalize my code.
> I figured out I could group them together by performing a pattern read
> on a nested list, which works for getting the file names and grouping
> them but then I'm getting some problems when I try to perform the
> import from the nested list. My code looks like this:
>
> # Vector containing file name patterns to be read and grouped together
> measurement_filenames <- c("*Q_Read_prist*", "*Quasi_Forming*",
> "*read_set#1*","*Q_Reset_pForm#1*","*read_reset#2*","*quasistatic_set*"
> , "*read_set#2*", "*quasistatic_reset#2*" )
>
> # Create a list of the files to be read in sorted in a natural fashion
> electrical_meas_files <- lapply(measurement_filenames, function(x)
> naturalsort(list.files(path, pattern=x)))
> names(electrical_meas_files) <- measurement_filenames
>
> # Perform data import for each element
>
> for(i in 1:length(measurement_filenames))
> {
>   electrical_meas_raw_data[[i]] <- lapply(electrical_meas_files[[i]],
> function(x) read.xlsx(file=x, sheetName="Data", header=TRUE,
> as.data.frame =TRUE,  stringsAsFactors = F) )
> }
>
>
>
> My idea is to come up with a nested list of the structure
> {list of different experiments}
>       {list of all sites where the experiment was run}
>               {set of dataframes with all data for each site}
>
>
> Do I make sense or am I over-complicating the situation ?
>
>
> Any ideas how I could write this piece of code or improve it ?
>
> Thanks in advance,
> Mario
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From petr.pikal at precheza.cz  Thu Dec 10 09:04:48 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 10 Dec 2015 08:04:48 +0000
Subject: [R] problems with plotting/graphics after changing my computer
In-Reply-To: <CAM4izHu-aPoJApfBMfjaePRMwM5G_GXRQnOv92H-KrPTasQrTA@mail.gmail.com>
References: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004114@SRVEXCHMBX.precheza.cz>
	<CAM4izHu-aPoJApfBMfjaePRMwM5G_GXRQnOv92H-KrPTasQrTA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004195@SRVEXCHMBX.precheza.cz>

Hi

Reply to the list too, somebody could answer your question better than myself.

You say that it is the same script. OK. But is it also the same version of R and ctmm?

Cheers
Petr

From: Alexander Mo?brucker [mailto:islandelephants at gmail.com]
Sent: Thursday, December 10, 2015 8:50 AM
To: PIKAL Petr
Subject: Re: [R] problems with plotting/graphics after changing my computer

Hi Petr,
thank you for your quick reply. I have the script now attached as word doc, hope that is readable now.
There must be another problem, because I could plot the AKDE very easily on my old computer, using exactly the same script and data and the same commands and all....thus it is very strange if suddenly on my new computer R doesn't recognize the command any more....
I also attached the ctmm PDF, the example is directly taken from this PDF, from the section "akde"

Best, Alex

[https://ipmcdn.avast.com/images/logo-avast-v1.png]<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

This email has been sent from a virus-free computer protected by Avast.
www.avast.com<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>


2015-12-10 14:23 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>>:
Hi

1. Do not post in HTML. Your mail is corrupted and almost unreadable.
2. As far I I could decipher it, your plot call is

plot(cilla,AKDE=KD2)

According to docs the plot function does not have parameter called AKDE

plot(x, CTMM=NULL, UD=NULL, level.UD=0.95, level=0.95, DF="CDF", col="red",
col.level="black", col.DF="blue", col.grid="grey", fraction=1, add=FALSE, xlim=NULL,
ylim=NULL, ...)

but I believe that akde result shall be transferred to UD parameter.

UD Optional UD object such as from the output of akde or list of such objects

So your plot shall be called

plot(cilla, UD=KD2)

I do not have any experience with ctmm though, so I may completely miss the point. But if I am right, you could find all this in help page the same way as I did and probably quicker.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of
> Alexander Mo?brucker
> Sent: Thursday, December 10, 2015 7:47 AM
> To: r-help at r-project.org<mailto:r-help at r-project.org>
> Subject: [R] problems with plotting/graphics after changing my computer
>
> Dear all,
>
> I have worked on AKDEs using the package ctmm. Because my old computer
> often crashed, I bought a new one and reinstalled all the software,
> including r.
>
> All seemed to work fine, but when I wanted to plot several results I
> encountered error messages.
>
> I tried many different plots, some plots work, others not, I could't
> find
> out why. The error message is always " ..... is not a graphical
> parameter".
> I am using exactly the same r script and packages as before on my old
> computer.
>
> Would be great if you could help me with this....., best, Alex
>
> Please find below an example from the ctmm package:
>
> > # Load package and data> library(ctmm)> data(buffalo)> cilla <-
> buffalo[[1]]> # Fit a continuous-velocity model with tau ~ c(10 days, 1
> hour)> # see help(variogram.fit)> m2 <- ctmm(tau=c(10*24*60^2,60^2))>
> M2 <- ctmm.fit(cilla,m2)> # Compute akde object> KD2 <- akde(cilla,M2)>
> # Plot data with AKDE contours> plot(cilla,AKDE=KD2)Warning messages:1:
> In plot.window(...) : "AKDE" is not a graphical parameter2: In
> plot.xy(xy, type, ...) : "AKDE" is not a graphical parameter3: In
> axis(side = side, at = at, labels = labels, ...) :
>   "AKDE" is not a graphical parameter4: In axis(side = side, at = at,
> labels = labels, ...) :
>   "AKDE" is not a graphical parameter5: In box(...) : "AKDE" is not a
> graphical parameter6: In title(...) : "AKDE" is not a graphical
> parameter7: In plot.xy(xy.coords(x, y), type = type, ...) :
>   "AKDE" is not a graphical parameter
>
>
>
>
>



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From islandelephants at gmail.com  Thu Dec 10 09:28:32 2015
From: islandelephants at gmail.com (=?UTF-8?Q?Alexander_Mo=C3=9Fbrucker?=)
Date: Thu, 10 Dec 2015 15:28:32 +0700
Subject: [R] problems with plotting/graphics after changing my computer
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004195@SRVEXCHMBX.precheza.cz>
References: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004114@SRVEXCHMBX.precheza.cz>
	<CAM4izHu-aPoJApfBMfjaePRMwM5G_GXRQnOv92H-KrPTasQrTA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004195@SRVEXCHMBX.precheza.cz>
Message-ID: <CAM4izHudLmPVgKQi9LJzUqy93YpYboWr=gC3vt7J-mb2yd3cBQ@mail.gmail.com>

I am not surve, as I cannot start the crashed computer I used before. I
guess it might be the same versions, I installed them on my crashed
computer in October, and now reinstalled them on my new computer in
December, thus only 2-3 months difference.



<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
This
email has been sent from a virus-free computer protected by Avast.
www.avast.com
<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
<#DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>

2015-12-10 15:04 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz>:

> Hi
>
>
>
> Reply to the list too, somebody could answer your question better than
> myself.
>
>
>
> You say that it is the same script. OK. But is it also the same version of
> R and ctmm?
>
>
>
> Cheers
>
> Petr
>
>
>
> *From:* Alexander Mo?brucker [mailto:islandelephants at gmail.com]
> *Sent:* Thursday, December 10, 2015 8:50 AM
> *To:* PIKAL Petr
> *Subject:* Re: [R] problems with plotting/graphics after changing my
> computer
>
>
>
> Hi Petr,
>
> thank you for your quick reply. I have the script now attached as word
> doc, hope that is readable now.
>
> There must be another problem, because I could plot the AKDE very easily
> on my old computer, using exactly the same script and data and the same
> commands and all....thus it is very strange if suddenly on my new computer
> R doesn't recognize the command any more....
>
> I also attached the ctmm PDF, the example is directly taken from this PDF,
> from the section "akde"
>
>
>
> Best, Alex
>
>
>
>
> <https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
> This email has been sent from a virus-free computer protected by Avast.
> www.avast.com
> <https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>
>
>
> 2015-12-10 14:23 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz>:
>
> Hi
>
> 1. Do not post in HTML. Your mail is corrupted and almost unreadable.
> 2. As far I I could decipher it, your plot call is
>
> plot(cilla,AKDE=KD2)
>
> According to docs the plot function does not have parameter called AKDE
>
> plot(x, CTMM=NULL, UD=NULL, level.UD=0.95, level=0.95, DF="CDF", col="red",
> col.level="black", col.DF="blue", col.grid="grey", fraction=1, add=FALSE,
> xlim=NULL,
> ylim=NULL, ...)
>
> but I believe that akde result shall be transferred to UD parameter.
>
> UD Optional UD object such as from the output of akde or list of such
> objects
>
> So your plot shall be called
>
> plot(cilla, UD=KD2)
>
> I do not have any experience with ctmm though, so I may completely miss
> the point. But if I am right, you could find all this in help page the same
> way as I did and probably quicker.
>
> Cheers
> Petr
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> > Alexander Mo?brucker
> > Sent: Thursday, December 10, 2015 7:47 AM
> > To: r-help at r-project.org
> > Subject: [R] problems with plotting/graphics after changing my computer
> >
> > Dear all,
> >
> > I have worked on AKDEs using the package ctmm. Because my old computer
> > often crashed, I bought a new one and reinstalled all the software,
> > including r.
> >
> > All seemed to work fine, but when I wanted to plot several results I
> > encountered error messages.
> >
> > I tried many different plots, some plots work, others not, I could't
> > find
> > out why. The error message is always " ..... is not a graphical
> > parameter".
> > I am using exactly the same r script and packages as before on my old
> > computer.
> >
> > Would be great if you could help me with this....., best, Alex
> >
> > Please find below an example from the ctmm package:
> >
> > > # Load package and data> library(ctmm)> data(buffalo)> cilla <-
> > buffalo[[1]]> # Fit a continuous-velocity model with tau ~ c(10 days, 1
> > hour)> # see help(variogram.fit)> m2 <- ctmm(tau=c(10*24*60^2,60^2))>
> > M2 <- ctmm.fit(cilla,m2)> # Compute akde object> KD2 <- akde(cilla,M2)>
> > # Plot data with AKDE contours> plot(cilla,AKDE=KD2)Warning messages:1:
> > In plot.window(...) : "AKDE" is not a graphical parameter2: In
> > plot.xy(xy, type, ...) : "AKDE" is not a graphical parameter3: In
> > axis(side = side, at = at, labels = labels, ...) :
> >   "AKDE" is not a graphical parameter4: In axis(side = side, at = at,
> > labels = labels, ...) :
> >   "AKDE" is not a graphical parameter5: In box(...) : "AKDE" is not a
> > graphical parameter6: In title(...) : "AKDE" is not a graphical
> > parameter7: In plot.xy(xy.coords(x, y), type = type, ...) :
> >   "AKDE" is not a graphical parameter
> >
> >
> >
> >
> >
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From islandelephants at gmail.com  Thu Dec 10 09:31:18 2015
From: islandelephants at gmail.com (=?UTF-8?Q?Alexander_Mo=C3=9Fbrucker?=)
Date: Thu, 10 Dec 2015 15:31:18 +0700
Subject: [R] problems with plotting/graphics after changing my computer
In-Reply-To: <CAM4izHudLmPVgKQi9LJzUqy93YpYboWr=gC3vt7J-mb2yd3cBQ@mail.gmail.com>
References: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004114@SRVEXCHMBX.precheza.cz>
	<CAM4izHu-aPoJApfBMfjaePRMwM5G_GXRQnOv92H-KrPTasQrTA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004195@SRVEXCHMBX.precheza.cz>
	<CAM4izHudLmPVgKQi9LJzUqy93YpYboWr=gC3vt7J-mb2yd3cBQ@mail.gmail.com>
Message-ID: <CAM4izHvXYjh5zAbbfv5HhTxLwCSuxkp6bKop0c7T8oe7MEhx8A@mail.gmail.com>

Just curious: do you get the same error message, or is it a problem with my
specific version/computer???

2015-12-10 15:28 GMT+07:00 Alexander Mo?brucker <islandelephants at gmail.com>:

> I am not surve, as I cannot start the crashed computer I used before. I
> guess it might be the same versions, I installed them on my crashed
> computer in October, and now reinstalled them on my new computer in
> December, thus only 2-3 months difference.
>
>
>
>
> <https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> This
> email has been sent from a virus-free computer protected by Avast.
> www.avast.com
> <https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
> <#1518b0187e13409b_DDB4FAA8-2DD7-40BB-A1B8-4E2AA1F9FDF2>
>
> 2015-12-10 15:04 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz>:
>
>> Hi
>>
>>
>>
>> Reply to the list too, somebody could answer your question better than
>> myself.
>>
>>
>>
>> You say that it is the same script. OK. But is it also the same version
>> of R and ctmm?
>>
>>
>>
>> Cheers
>>
>> Petr
>>
>>
>>
>> *From:* Alexander Mo?brucker [mailto:islandelephants at gmail.com]
>> *Sent:* Thursday, December 10, 2015 8:50 AM
>> *To:* PIKAL Petr
>> *Subject:* Re: [R] problems with plotting/graphics after changing my
>> computer
>>
>>
>>
>> Hi Petr,
>>
>> thank you for your quick reply. I have the script now attached as word
>> doc, hope that is readable now.
>>
>> There must be another problem, because I could plot the AKDE very easily
>> on my old computer, using exactly the same script and data and the same
>> commands and all....thus it is very strange if suddenly on my new computer
>> R doesn't recognize the command any more....
>>
>> I also attached the ctmm PDF, the example is directly taken from this
>> PDF, from the section "akde"
>>
>>
>>
>> Best, Alex
>>
>>
>>
>>
>> <https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>>
>> This email has been sent from a virus-free computer protected by Avast.
>> www.avast.com
>> <https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>
>>
>>
>>
>> 2015-12-10 14:23 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz>:
>>
>> Hi
>>
>> 1. Do not post in HTML. Your mail is corrupted and almost unreadable.
>> 2. As far I I could decipher it, your plot call is
>>
>> plot(cilla,AKDE=KD2)
>>
>> According to docs the plot function does not have parameter called AKDE
>>
>> plot(x, CTMM=NULL, UD=NULL, level.UD=0.95, level=0.95, DF="CDF",
>> col="red",
>> col.level="black", col.DF="blue", col.grid="grey", fraction=1, add=FALSE,
>> xlim=NULL,
>> ylim=NULL, ...)
>>
>> but I believe that akde result shall be transferred to UD parameter.
>>
>> UD Optional UD object such as from the output of akde or list of such
>> objects
>>
>> So your plot shall be called
>>
>> plot(cilla, UD=KD2)
>>
>> I do not have any experience with ctmm though, so I may completely miss
>> the point. But if I am right, you could find all this in help page the same
>> way as I did and probably quicker.
>>
>> Cheers
>> Petr
>>
>>
>> > -----Original Message-----
>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> > Alexander Mo?brucker
>> > Sent: Thursday, December 10, 2015 7:47 AM
>> > To: r-help at r-project.org
>> > Subject: [R] problems with plotting/graphics after changing my computer
>> >
>> > Dear all,
>> >
>> > I have worked on AKDEs using the package ctmm. Because my old computer
>> > often crashed, I bought a new one and reinstalled all the software,
>> > including r.
>> >
>> > All seemed to work fine, but when I wanted to plot several results I
>> > encountered error messages.
>> >
>> > I tried many different plots, some plots work, others not, I could't
>> > find
>> > out why. The error message is always " ..... is not a graphical
>> > parameter".
>> > I am using exactly the same r script and packages as before on my old
>> > computer.
>> >
>> > Would be great if you could help me with this....., best, Alex
>> >
>> > Please find below an example from the ctmm package:
>> >
>> > > # Load package and data> library(ctmm)> data(buffalo)> cilla <-
>> > buffalo[[1]]> # Fit a continuous-velocity model with tau ~ c(10 days, 1
>> > hour)> # see help(variogram.fit)> m2 <- ctmm(tau=c(10*24*60^2,60^2))>
>> > M2 <- ctmm.fit(cilla,m2)> # Compute akde object> KD2 <- akde(cilla,M2)>
>> > # Plot data with AKDE contours> plot(cilla,AKDE=KD2)Warning messages:1:
>> > In plot.window(...) : "AKDE" is not a graphical parameter2: In
>> > plot.xy(xy, type, ...) : "AKDE" is not a graphical parameter3: In
>> > axis(side = side, at = at, labels = labels, ...) :
>> >   "AKDE" is not a graphical parameter4: In axis(side = side, at = at,
>> > labels = labels, ...) :
>> >   "AKDE" is not a graphical parameter5: In box(...) : "AKDE" is not a
>> > graphical parameter6: In title(...) : "AKDE" is not a graphical
>> > parameter7: In plot.xy(xy.coords(x, y), type = type, ...) :
>> >   "AKDE" is not a graphical parameter
>> >
>> >
>> >
>> >
>> >
>>
>>
>>
>> ------------------------------
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>> vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in which
>> he/she is expressly authorized to do so in writing, and such authorization
>> or power of attorney is submitted to the recipient or the person
>> represented by the recipient, or the existence of such authorization is
>> known to the recipient of the person represented by the recipient.
>>
>
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Thu Dec 10 10:26:17 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Thu, 10 Dec 2015 09:26:17 +0000
Subject: [R] problems with plotting/graphics after changing my computer
In-Reply-To: <CAM4izHvXYjh5zAbbfv5HhTxLwCSuxkp6bKop0c7T8oe7MEhx8A@mail.gmail.com>
References: <CAM4izHtU9oT+hUOcxrYD8J1dbecoeie2oyH=ORTStupKR+DAVg@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004114@SRVEXCHMBX.precheza.cz>
	<CAM4izHu-aPoJApfBMfjaePRMwM5G_GXRQnOv92H-KrPTasQrTA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004195@SRVEXCHMBX.precheza.cz>
	<CAM4izHudLmPVgKQi9LJzUqy93YpYboWr=gC3vt7J-mb2yd3cBQ@mail.gmail.com>
	<CAM4izHvXYjh5zAbbfv5HhTxLwCSuxkp6bKop0c7T8oe7MEhx8A@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50041CE@SRVEXCHMBX.precheza.cz>

Hi

I do not use ctmm and did not install it. I just opened recent docs for stmm and there is no AKDE parameter for plot. ctmm pdf you sent me has 0.2.8 version and was published August 26, 2015 but recently available version has number 0.3.0 and was published November 28, 2015

So if you did the same as myself you would have found it too.

As I wrote in my first mail, use

plot(cilla, UD=KD2)

and see what happens. It is far better than speculating about versions your poor old comp used.

Cheers
Petr

From: Alexander Mo?brucker [mailto:islandelephants at gmail.com]
Sent: Thursday, December 10, 2015 9:31 AM
To: PIKAL Petr
Cc: r-help at r-project.org
Subject: Re: [R] problems with plotting/graphics after changing my computer

Just curious: do you get the same error message, or is it a problem with my specific version/computer???

2015-12-10 15:28 GMT+07:00 Alexander Mo?brucker <islandelephants at gmail.com<mailto:islandelephants at gmail.com>>:
I am not surve, as I cannot start the crashed computer I used before. I guess it might be the same versions, I installed them on my crashed computer in October, and now reinstalled them on my new computer in December, thus only 2-3 months difference.


[https://ipmcdn.avast.com/images/logo-avast-v1.png]<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

This email has been sent from a virus-free computer protected by Avast.
www.avast.com<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>


2015-12-10 15:04 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>>:
Hi

Reply to the list too, somebody could answer your question better than myself.

You say that it is the same script. OK. But is it also the same version of R and ctmm?

Cheers
Petr

From: Alexander Mo?brucker [mailto:islandelephants at gmail.com<mailto:islandelephants at gmail.com>]
Sent: Thursday, December 10, 2015 8:50 AM
To: PIKAL Petr
Subject: Re: [R] problems with plotting/graphics after changing my computer

Hi Petr,
thank you for your quick reply. I have the script now attached as word doc, hope that is readable now.
There must be another problem, because I could plot the AKDE very easily on my old computer, using exactly the same script and data and the same commands and all....thus it is very strange if suddenly on my new computer R doesn't recognize the command any more....
I also attached the ctmm PDF, the example is directly taken from this PDF, from the section "akde"

Best, Alex

[https://ipmcdn.avast.com/images/logo-avast-v1.png]<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>

This email has been sent from a virus-free computer protected by Avast.
www.avast.com<https://www.avast.com/lp-safe-emailing?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail>


2015-12-10 14:23 GMT+07:00 PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>>:
Hi

1. Do not post in HTML. Your mail is corrupted and almost unreadable.
2. As far I I could decipher it, your plot call is

plot(cilla,AKDE=KD2)

According to docs the plot function does not have parameter called AKDE

plot(x, CTMM=NULL, UD=NULL, level.UD=0.95, level=0.95, DF="CDF", col="red",
col.level="black", col.DF="blue", col.grid="grey", fraction=1, add=FALSE, xlim=NULL,
ylim=NULL, ...)

but I believe that akde result shall be transferred to UD parameter.

UD Optional UD object such as from the output of akde or list of such objects

So your plot shall be called

plot(cilla, UD=KD2)

I do not have any experience with ctmm though, so I may completely miss the point. But if I am right, you could find all this in help page the same way as I did and probably quicker.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of
> Alexander Mo?brucker
> Sent: Thursday, December 10, 2015 7:47 AM
> To: r-help at r-project.org<mailto:r-help at r-project.org>
> Subject: [R] problems with plotting/graphics after changing my computer
>
> Dear all,
>
> I have worked on AKDEs using the package ctmm. Because my old computer
> often crashed, I bought a new one and reinstalled all the software,
> including r.
>
> All seemed to work fine, but when I wanted to plot several results I
> encountered error messages.
>
> I tried many different plots, some plots work, others not, I could't
> find
> out why. The error message is always " ..... is not a graphical
> parameter".
> I am using exactly the same r script and packages as before on my old
> computer.
>
> Would be great if you could help me with this....., best, Alex
>
> Please find below an example from the ctmm package:
>
> > # Load package and data> library(ctmm)> data(buffalo)> cilla <-
> buffalo[[1]]> # Fit a continuous-velocity model with tau ~ c(10 days, 1
> hour)> # see help(variogram.fit)> m2 <- ctmm(tau=c(10*24*60^2,60^2))>
> M2 <- ctmm.fit(cilla,m2)> # Compute akde object> KD2 <- akde(cilla,M2)>
> # Plot data with AKDE contours> plot(cilla,AKDE=KD2)Warning messages:1:
> In plot.window(...) : "AKDE" is not a graphical parameter2: In
> plot.xy(xy, type, ...) : "AKDE" is not a graphical parameter3: In
> axis(side = side, at = at, labels = labels, ...) :
>   "AKDE" is not a graphical parameter4: In axis(side = side, at = at,
> labels = labels, ...) :
>   "AKDE" is not a graphical parameter5: In box(...) : "AKDE" is not a
> graphical parameter6: In title(...) : "AKDE" is not a graphical
> parameter7: In plot.xy(xy.coords(x, y), type = type, ...) :
>   "AKDE" is not a graphical parameter
>
>
>
>
>


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From stefano.sofia at regione.marche.it  Thu Dec 10 11:08:26 2015
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Thu, 10 Dec 2015 10:08:26 +0000
Subject: [R] matrix which results singular but at the same time positive
	definite
Message-ID: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>

Dear list users,
through the "matrixcalc" package I am performing some checks of variance matrices (which must be positive definite).
In this example, it happens that the matrix A here reported is singular but positive definite. Is it possible?

              [,1]          [,2]          [,3]          [,4]
[1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12
[2,] -1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12
[3,] -8.238960e-13  1.364242e-12  4.809988e+00  7.742369e-01
[4,] -1.240294e-12  1.818989e-12  7.742369e-01  1.090411e+00

print(is.non.singular.matrix(A, tol = 1e-18))
FALSE
print(is.positive.definite(A, tol=1e-18))
TRUE

Is there something wrong with this matrix?
Any comment will be appreciated.
Stefano


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Thu Dec 10 11:20:44 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Thu, 10 Dec 2015 10:20:44 +0000
Subject: [R] Use of R for estimation of Discrete Choice Models
 incorporating latent variables
Message-ID: <248E6FA047A8C746BA491485764190F5371205E2@ESESSMB210.ericsson.se>

My educated guess:

package: RSGHB

https://cran.r-project.org/web/packages/RSGHB/RSGHB.pdf

http://www.inside-r.org/packages/cran/RSGHB/docs/doHB


Wondering if also package ltm may help.

There is available R package search at:

http://rseek.org/


My apologies if abovementioned packages do not fit well your needs.

--
GG



	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Thu Dec 10 11:35:22 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Thu, 10 Dec 2015 10:35:22 +0000
Subject: [R] matrix which results singular but at the same time positive
 definite
Message-ID: <248E6FA047A8C746BA491485764190F53712060F@ESESSMB210.ericsson.se>

Decrease the "tol" parameter specified into the "is.non.singular.matrix() call,
for example as:

m <- matrix(c( 1.904255e-12, -1.904255e-12, -8.238960e-13, -1.240294e-12,
               -1.904255e-12,  3.637979e-12,  1.364242e-12,  1.818989e-12,
               -8.238960e-13,  1.364242e-12,  4.809988e+00,  7.742369e-01,
               -1.240294e-12,  1.818989e-12,  7.742369e-01,  1.090411e+00),
                nrow=4, ncol=4)


> m

              [,1]          [,2]          [,3]          [,4]

[1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12

[2,] -1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12

[3,] -8.238960e-13  1.364242e-12  4.809988e+00  7.742369e-01

[4,] -1.240294e-12  1.818989e-12  7.742369e-01  1.090411e+00


> print(is.non.singular.matrix(m, tol = 1e-24))
[1] TRUE

> print(is.positive.definite(m, tol=1e-18))
[1] TRUE


--

GG

http://around-r.blogspot.it





	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Thu Dec 10 11:38:22 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 10 Dec 2015 23:38:22 +1300
Subject: [R] matrix which results singular but at the same time positive
 definite
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>
Message-ID: <5669561E.7030800@auckland.ac.nz>

On 10/12/15 23:08, Stefano Sofia wrote:
> Dear list users,
> through the "matrixcalc" package I am performing some checks of variance matrices (which must be positive definite).
> In this example, it happens that the matrix A here reported is singular but positive definite. Is it possible?
>
>                [,1]          [,2]          [,3]          [,4]
> [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12
> [2,] -1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12
> [3,] -8.238960e-13  1.364242e-12  4.809988e+00  7.742369e-01
> [4,] -1.240294e-12  1.818989e-12  7.742369e-01  1.090411e+00
>
> print(is.non.singular.matrix(A, tol = 1e-18))
> FALSE
> print(is.positive.definite(A, tol=1e-18))
> TRUE
>
> Is there something wrong with this matrix?
> Any comment will be appreciated.

There is nothing wrong with A (at least nothing that either a nice bowl 
of chicken soup or a bloody good swim wouldn't cure).

Look at the code for the two functions.  The tests use the tolerance in 
very different ways.

My initial reaction is that the code for these functions is rather naive.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From arnaud.gaboury at gmail.com  Thu Dec 10 12:12:09 2015
From: arnaud.gaboury at gmail.com (arnaud gaboury)
Date: Thu, 10 Dec 2015 12:12:09 +0100
Subject: [R] change col types of a df/tbl_df
Message-ID: <CAK1hC9vqOQNPcxw57iYRUy5oSUWTxYuBZK_PXSgC6Oy8DAungg@mail.gmail.com>

Here is a sample of my data frame, obtained with read_csv2 from readr package.

myDf <- structure(list(X15 = c("30.09.2015", "05.10.2015", "30.09.2015",

"29.09.2015", "10.10.2015"), X16 = c("02.10.2015", "06.10.2015",
"01.10.2015", "01.10.2015", "13.10.2015"), X17 = c("Grains",
"Grains", "Grains", "Grains", "Grains"), X18 = c("Soyabeans",
"Soyabeans", "Soyabeans", "Soyabeans", "Soyabeans"), X19 = c("20,000",
"20,000", "20,000", "29,930", "26,000")), .Names = c("X15", "X16",
"X17", "X18", "X19"), class = c("tbl_df", "data.frame"), row.names = c(NA,
-5L))

gabx at hortensia [R] str(myDf)
Classes ?tbl_df? and 'data.frame': 5 obs. of  5 variables:
 $ X15: chr  "30.09.2015" "05.10.2015" "30.09.2015" "29.09.2015" ...
 $ X16: chr  "02.10.2015" "06.10.2015" "01.10.2015" "01.10.2015" ...
 $ X17: chr  "Grains" "Grains" "Grains" "Grains" ...
 $ X18: chr  "Soyabeans" "Soyabeans" "Soyabeans" "Soyabeans" ...
 $ X19: chr  "20,000" "20,000" "20,000" "29,930" ...

I want to change date to date class and numbers (X19) to numeric, and
keep the class of my object.

This code works:

myDf$X19 <- as.numeric(gsub(",", "", myDf$X19))
myDf$X15 <- as.Date(myDf$X15, format = "%d.%m.%Y"))
myDf$X16 <- as.Date(myDf$X16, format = "%d.%m.%Y"))

Now, as I have more than 5 columns, this can be fastidious and slowing
code (?), even if I can group by type. Columns are only types of char,
num and Date, so it could be OK.

I tried with lapply for the Date columns. It works BUT will place NA
in any columns with numbers as characters.
The reuslt will be this for X19:  num NA NA NA NA NA NA NA NA NA NA ..

How can I target my goal with something else than lapply or writing a
line for each type ?

Thank you for hints.


-- 

google.com/+arnaudgabourygabx


From murdoch.duncan at gmail.com  Thu Dec 10 12:54:10 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 10 Dec 2015 06:54:10 -0500
Subject: [R] change col types of a df/tbl_df
In-Reply-To: <CAK1hC9vqOQNPcxw57iYRUy5oSUWTxYuBZK_PXSgC6Oy8DAungg@mail.gmail.com>
References: <CAK1hC9vqOQNPcxw57iYRUy5oSUWTxYuBZK_PXSgC6Oy8DAungg@mail.gmail.com>
Message-ID: <566967E2.60009@gmail.com>

On 10/12/2015 6:12 AM, arnaud gaboury wrote:
> Here is a sample of my data frame, obtained with read_csv2 from readr package.
>
> myDf <- structure(list(X15 = c("30.09.2015", "05.10.2015", "30.09.2015",
>
> "29.09.2015", "10.10.2015"), X16 = c("02.10.2015", "06.10.2015",
> "01.10.2015", "01.10.2015", "13.10.2015"), X17 = c("Grains",
> "Grains", "Grains", "Grains", "Grains"), X18 = c("Soyabeans",
> "Soyabeans", "Soyabeans", "Soyabeans", "Soyabeans"), X19 = c("20,000",
> "20,000", "20,000", "29,930", "26,000")), .Names = c("X15", "X16",
> "X17", "X18", "X19"), class = c("tbl_df", "data.frame"), row.names = c(NA,
> -5L))
>
> gabx at hortensia [R] str(myDf)
> Classes ?tbl_df? and 'data.frame': 5 obs. of  5 variables:
>   $ X15: chr  "30.09.2015" "05.10.2015" "30.09.2015" "29.09.2015" ...
>   $ X16: chr  "02.10.2015" "06.10.2015" "01.10.2015" "01.10.2015" ...
>   $ X17: chr  "Grains" "Grains" "Grains" "Grains" ...
>   $ X18: chr  "Soyabeans" "Soyabeans" "Soyabeans" "Soyabeans" ...
>   $ X19: chr  "20,000" "20,000" "20,000" "29,930" ...
>
> I want to change date to date class and numbers (X19) to numeric, and
> keep the class of my object.
>
> This code works:
>
> myDf$X19 <- as.numeric(gsub(",", "", myDf$X19))
> myDf$X15 <- as.Date(myDf$X15, format = "%d.%m.%Y"))
> myDf$X16 <- as.Date(myDf$X16, format = "%d.%m.%Y"))
>
> Now, as I have more than 5 columns, this can be fastidious and slowing
> code (?), even if I can group by type. Columns are only types of char,
> num and Date, so it could be OK.
>
> I tried with lapply for the Date columns. It works BUT will place NA
> in any columns with numbers as characters.
> The reuslt will be this for X19:  num NA NA NA NA NA NA NA NA NA NA ..
>
> How can I target my goal with something else than lapply or writing a
> line for each type ?

I don't see how a function could reliably detect the types, but it might 
be good enough to use a regular expression, possibly just on the first 
line of the result.  Once you've identified columns, e.g.

  numcols <- 19
  datecols <- c(15:16)

etc, you can use lapply:

myDf[,numcols] <- lapply(myDf[, numcools, drop=FALSE], function(x) 
as.numeric(gsub(",", "", x)))

You can simplify myDf[,numcols] to myDf[numcols] if you want, but I 
think it makes it less clear.

Duncan Murdoch


From arnaud.gaboury at gmail.com  Thu Dec 10 13:10:35 2015
From: arnaud.gaboury at gmail.com (arnaud gaboury)
Date: Thu, 10 Dec 2015 13:10:35 +0100
Subject: [R] change col types of a df/tbl_df
In-Reply-To: <566967E2.60009@gmail.com>
References: <CAK1hC9vqOQNPcxw57iYRUy5oSUWTxYuBZK_PXSgC6Oy8DAungg@mail.gmail.com>
	<566967E2.60009@gmail.com>
Message-ID: <CAK1hC9smw=fM7r2FQdETCa1EukHPWDk2f0RmynrOwKr8hrWe6Q@mail.gmail.com>

On Thu, Dec 10, 2015 at 12:54 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 10/12/2015 6:12 AM, arnaud gaboury wrote:
>
>> Here is a sample of my data frame, obtained with read_csv2 from readr
>> package.
>>
>> myDf <- structure(list(X15 = c("30.09.2015", "05.10.2015", "30.09.2015",
>>
>> "29.09.2015", "10.10.2015"), X16 = c("02.10.2015", "06.10.2015",
>> "01.10.2015", "01.10.2015", "13.10.2015"), X17 = c("Grains",
>> "Grains", "Grains", "Grains", "Grains"), X18 = c("Soyabeans",
>> "Soyabeans", "Soyabeans", "Soyabeans", "Soyabeans"), X19 = c("20,000",
>> "20,000", "20,000", "29,930", "26,000")), .Names = c("X15", "X16",
>> "X17", "X18", "X19"), class = c("tbl_df", "data.frame"), row.names = c(NA,
>> -5L))
>>
>> gabx at hortensia [R] str(myDf)
>> Classes ?tbl_df? and 'data.frame': 5 obs. of  5 variables:
>>   $ X15: chr  "30.09.2015" "05.10.2015" "30.09.2015" "29.09.2015" ...
>>   $ X16: chr  "02.10.2015" "06.10.2015" "01.10.2015" "01.10.2015" ...
>>   $ X17: chr  "Grains" "Grains" "Grains" "Grains" ...
>>   $ X18: chr  "Soyabeans" "Soyabeans" "Soyabeans" "Soyabeans" ...
>>   $ X19: chr  "20,000" "20,000" "20,000" "29,930" ...
>>
>> I want to change date to date class and numbers (X19) to numeric, and
>> keep the class of my object.
>>
>> This code works:
>>
>> myDf$X19 <- as.numeric(gsub(",", "", myDf$X19))
>> myDf$X15 <- as.Date(myDf$X15, format = "%d.%m.%Y"))
>> myDf$X16 <- as.Date(myDf$X16, format = "%d.%m.%Y"))
>>
>> Now, as I have more than 5 columns, this can be fastidious and slowing
>> code (?), even if I can group by type. Columns are only types of char,
>> num and Date, so it could be OK.
>>
>> I tried with lapply for the Date columns. It works BUT will place NA
>> in any columns with numbers as characters.
>> The reuslt will be this for X19:  num NA NA NA NA NA NA NA NA NA NA ..
>>
>> How can I target my goal with something else than lapply or writing a
>> line for each type ?
>>
>
> I don't see how a function could reliably detect the types,

In fact, I only have 25 columns, so it is not difficult to list them in the
3 types: char, num and Date. No need of a function thus.


> but it might be good enough to use a regular expression, possibly just on
> the first line of the result.  Once you've identified columns, e.g.
>
>  numcols <- 19
>  datecols <- c(15:16)
>
> etc, you can use lapply:
>
> myDf[,numcols] <- lapply(myDf[, numcools, drop=FALSE], function(x)
> as.numeric(gsub(",", "", x)))
>
> You can simplify myDf[,numcols] to myDf[numcols] if you want, but I think
> it makes it less clear.


Thank you.

>
>
> Duncan Murdoch
>
>


-- 

google.com/+arnaudgabourygabx
<https://plus.google.com/_/notifications/emlink?emr=05814804238976922326&emid=CKiv-v6PvboCFcfoQgod6msAAA&path=%2F116159236040461325607%2Fop%2Fu&dt=1383086841306&ub=50>

	[[alternative HTML version deleted]]


From pd.mes at cbs.dk  Thu Dec 10 13:07:57 2015
From: pd.mes at cbs.dk (Peter Dalgaard)
Date: Thu, 10 Dec 2015 13:07:57 +0100
Subject: [R] R 3.2.3 is released
Message-ID: <090FD1B2-509D-49B3-945A-822AE6C5E5CD@cbs.dk>

The build system rolled up  R-3.2.3.tar.gz (codename "Wooden Christmas-Tree") this morning.

The list below details the changes in this release.

You can get the source code from

http://cran.r-project.org/src/base/R-3/R-3.2.3.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you.

Binaries for various platforms will appear in due course.


For the R Core Team,

Peter Dalgaard


These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

MD5 (AUTHORS) = eb97a5cd38acb1cfc6408988bffef765
MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
MD5 (FAQ) = 3a15bad19f5031d05d1a4893f0fadb81
MD5 (INSTALL) = 3964b9119adeaab9ceb633773fc94aac
MD5 (NEWS) = e09aa00e906c9feb3f63838e4b4919c9
MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
MD5 (NEWS.2) = 8e2f4d1d5228663ae598a09bf1e2bc6b
MD5 (R-latest.tar.gz) = 1ba3dac113efab69e706902810cc2970
MD5 (README) = aece1dfbd18c1760128c3787f5456af6
MD5 (RESOURCES) = 529223fd3ffef95731d0a87353108435
MD5 (THANKS) = ba00f6cc68a823e1741cfa6011f40ccb
MD5 (VERSION-INFO.dcf) = a042818b70e88f7c5253ee533f45cd2f
MD5 (R-3/R-3.2.3.tar.gz) = 1ba3dac113efab69e706902810cc2970

This is the relevant part of the NEWS file

CHANGES IN R 3.2.3:

  NEW FEATURES:

    * Some recently-added Windows time zone names have been added to
      the conversion table used to convert these to Olson names.
      (Including those relating to changes for Russia in Oct 2014, as
      in PR#16503.)

    * (Windows) Compatibility information has been added to the
      manifests for Rgui.exe, Rterm.exe and Rscript.exe.  This should
      allow win.version() and Sys.info() to report the actual Windows
      version up to Windows 10.

    * Windows "wininet" FTP first tries EPSV / PASV mode rather than
      only using active mode (reported by Dan Tenenbaum).

    * which.min(x) and which.max(x) may be much faster for logical and
      integer x and now also work for long vectors.

    * The 'emulation' part of tools::texi2dvi() has been somewhat
      enhanced, including supporting quiet = TRUE.  It can be selected
      by texi2dvi = "emulation".

      (Windows) MiKTeX removed its texi2dvi.exe command in Sept 2015:
      tools::texi2dvi() tries texify.exe if it is not found.

    * (Windows only) Shortcuts for printing and saving have been added
      to menus in Rgui.exe.  (Request of PR#16572.)

    * loess(..., iterTrace=TRUE) now provides diagnostics for
      robustness iterations, and the print() method for
      summary(<loess>) shows slightly more.

    * The included version of PCRE has been updated to 8.38, a bug-fix
      release.

    * View() now displays nested data frames in a more friendly way.
      (Request with patch in PR#15915.)

  INSTALLATION and INCLUDED SOFTWARE:

    * The included configuration code for libintl has been updated to
      that from gettext version 0.19.5.1 - this should only affect how
      an external library is detected (and the only known instance is
      under OpenBSD).  (Wish of PR#16464.)

    * configure has a new argument --disable-java to disable the checks
      for Java.

    * The configure default for MAIN_LDFLAGS has been changed for the
      FreeBSD, NetBSD and Hurd OSes to one more likely to work with
      compilers other than gcc (FreeBSD 10 defaults to clang).

    * configure now supports the OpenMP flags -fopenmp=libomp (clang)
      and -qopenmp (Intel C).

    * Various macros can be set to override the default behaviour of
      configure when detecting OpenMP: see file config.site.

    * Source installation on Windows has been modified to allow for
      MiKTeX installations without texi2dvi.exe.  See file
      MkRules.dist.

  BUG FIXES:

    * regexpr(pat, x, perl = TRUE) with Python-style named capture did
      not work correctly when x contained NA strings.  (PR#16484)

    * The description of dataset ToothGrowth has been
      improved/corrected.  (PR#15953)

    * model.tables(type = "means") and hence TukeyHSD() now support
      "aov" fits without an intercept term.  (PR#16437)

    * close() now reports the status of a pipe() connection opened with
      an explicit open argument.  (PR#16481)

    * Coercing a list without names to a data frame is faster if the
      elements are very long. (PR#16467)

    * (Unix-only) Under some rare circumstances piping the output from
      Rscript or R -f could result in attempting to close the input
      file twice, possibly crashing the process.  (PR#16500)

    * (Windows) Sys.info() was out of step with win.version() and did
      not report Windows 8.

    * topenv(baseenv()) returns baseenv() again as in R 3.1.0 and
      earlier.  This also fixes compilerJIT(3) when used in .Rprofile.

    * detach()ing the methods package keeps .isMethodsDispatchOn()
      true, as long as the methods namespace is not unloaded.

    * Removed some spurious warnings from configure about the
      preprocessor not finding header files.  (PR#15989)

    * rchisq(*, df=0, ncp=0) now returns 0 instead of NaN, and
      dchisq(*, df=0, ncp=*) also no longer returns NaN in limit cases
      (where the limit is unique).  (PR#16521)

    * pchisq(*, df=0, ncp > 0, log.p=TRUE) no longer underflows (for
      ncp > ~60).

    * nchar(x, "w") returned -1 for characters it did not know about
      (e.g. zero-width spaces): it now assumes 1.  It now knows about
      most zero-width characters and a few more double-width
      characters.

    * Help for which.min() is now more precise about behavior with
      logical arguments.  (PR#16532)

    * The print width of character strings marked as "latin1" or
      "bytes" was in some cases computed incorrectly.

    * abbreviate() did not give names to the return value if minlength
      was zero, unlike when it was positive.

    * (Windows only) dir.create() did not always warn when it failed to
      create a directory.  (PR#16537)

    * When operating in a non-UTF-8 multibyte locale (e.g. an East
      Asian locale on Windows), grep() and related functions did not
      handle UTF-8 strings properly.  (PR#16264)

    * read.dcf() sometimes misread lines longer than 8191 characters.
      (Reported by Herv'e Pag`es with a patch.)

    * within(df, ..) no longer drops columns whose name start with a
      ".".

    * The built-in HTTP server converted entire Content-Type to
      lowercase including parameters which can cause issues for
      multi-part form boundaries (PR#16541).

    * Modifying slots of S4 objects could fail when the methods package
      was not attached. (PR#16545)

    * splineDesign(*, outer.ok=TRUE) (splines) is better now
      (PR#16549), and interpSpline() now allows sparse=TRUE for speedup
      with non-small sizes.

    * If the expression in the traceback was too long, traceback() did
      not report the source line number.  (Patch by Kirill M"uller.)

    * The browser did not truncate the display of the function when
      exiting with options("deparse.max.lines") set.  (PR#16581)

    * When bs(*, Boundary.knots=) had boundary knots inside the data
      range, extrapolation was somewhat off.  (Patch by Trevor Hastie.)

    * var() and hence sd() warn about factor arguments which are
      deprecated now. (PR#16564)

    * loess(*, weights = *) stored wrong weights and hence gave
      slightly wrong predictions for newdata.  (PR#16587)

    * aperm(a, *) now preserves names(dim(a)).

    * poly(x, ..) now works when either raw=TRUE or coef is specified.
      (PR#16597)

    * data(package=*) is more careful in determining the path.

    * prettyNum(*, decimal.mark, big.mark): fixed bug introduced when
      fixing PR#16411.

_______________________________________________
R-announce at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce


From pdalgd at gmail.com  Thu Dec 10 13:36:30 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 10 Dec 2015 13:36:30 +0100
Subject: [R] there is a vectorinzing version of "[[<-" for "tclArray"
	class ?
In-Reply-To: <4BB1E0D6-F240-4495-B0EB-9A33572E5902@sciviews.org>
References: <5668C088.8040408@yahoo.com.br>
	<4BB1E0D6-F240-4495-B0EB-9A33572E5902@sciviews.org>
Message-ID: <27130D6C-3680-4594-A1AF-5698D083C521@gmail.com>


On 10 Dec 2015, at 08:27 , phgrosjean at sciviews.org wrote:

> library(tcltk)
> methods(class = "tclArray")
> 
> ## [1] [[       [[<-     $        $<-      length   length<- names    names<- 
> ## see '?methods' for accessing help and source code
> 
> It should be something like `[<-`. But this one is not defined. Perhaps can you contribute one here?

However, first ensure that you understand the section about Tcl arrays on the help page, and its implications. In particular, logical indexing and multidimensional indexing would never work.

-pd


> Best,
> 
> Philippe Grosjean
>> On 10 Dec 2015, at 01:00, Cleber N.Borges <klebyn at yahoo.com.br> wrote:
>> 
>> hi all,
>> there is a vectorinzing version of  "[[<-" for "tclArray" class ?
>> TIA
>> cleber
>> #####
>>> library(tcltk);  getS3method("[[<-",'tclArray')
>> function (x, ..., value)
>> {
>>   name <- as.character(x)
>>   i <- paste(..., sep = ",")
>>   if (is.null(value))
>>       .External(.C_RTcl_RemoveArrayElem, name, i)
>>   else {
>>       value <- as.tclObj(value)
>>       .External(.C_RTcl_SetArrayElem, name, i, value)
>>   }
>>   x
>> }
>> <bytecode: 0x000000000e7ab608>
>> <environment: namespace:tcltk>
>>> 
>> 
>> 
>> ---
>> Este email foi escaneado pelo Avast antiv?rus.
>> https://www.avast.com/antivirus
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From giorgio.garziano at ericsson.com  Thu Dec 10 13:47:35 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Thu, 10 Dec 2015 12:47:35 +0000
Subject: [R] change col types of a df/tbl_df
Message-ID: <248E6FA047A8C746BA491485764190F5371207B3@ESESSMB210.ericsson.se>

my_convert <- function(col) {
  v <- grep("[0-9]{2}.[0-9]{2}.[0-9]{4}", col);
  w <- grep("[0-9]+,[0-9]+", col)
  col2 <- col
  if (length(v) == length(col)){
    col2 <- as.Date(col, format="%d.%m.%y")
  } else if (length(w) == length(col)) {
    col2 <- as.numeric(gsub(",", "", col))
  }
  col2
}

myDf <- as.data.frame(lapply(myDf, my_convert), stringsAsFactors = FALSE)


--
GG


	[[alternative HTML version deleted]]


From arnaud.gaboury at gmail.com  Thu Dec 10 13:52:23 2015
From: arnaud.gaboury at gmail.com (arnaud gaboury)
Date: Thu, 10 Dec 2015 13:52:23 +0100
Subject: [R] change col types of a df/tbl_df
In-Reply-To: <248E6FA047A8C746BA491485764190F5371207B3@ESESSMB210.ericsson.se>
References: <248E6FA047A8C746BA491485764190F5371207B3@ESESSMB210.ericsson.se>
Message-ID: <CAK1hC9s6EHQ=N=gjXVnVgLqV6NJeZT4i7BMOBo5RBdpevomnjQ@mail.gmail.com>

On Thu, Dec 10, 2015 at 1:47 PM, Giorgio Garziano <
giorgio.garziano at ericsson.com> wrote:

> my_convert <- function(col) {
>   v <- grep("[0-9]{2}.[0-9]{2}.[0-9]{4}", col);
>   w <- grep("[0-9]+,[0-9]+", col)
>   col2 <- col
>   if (length(v) == length(col)){
>     col2 <- as.Date(col, format="%d.%m.%y")
>   } else if (length(w) == length(col)) {
>     col2 <- as.numeric(gsub(",", "", col))
>   }
>   col2
> }
>
> myDf <- as.data.frame(lapply(myDf, my_convert), stringsAsFactors = FALSE)
>
> Sounds good. I will give a try,
Thank you

> --
> GG
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 

google.com/+arnaudgabourygabx
<https://plus.google.com/_/notifications/emlink?emr=05814804238976922326&emid=CKiv-v6PvboCFcfoQgod6msAAA&path=%2F116159236040461325607%2Fop%2Fu&dt=1383086841306&ub=50>

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Thu Dec 10 14:41:44 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 10 Dec 2015 13:41:44 +0000
Subject: [R] matrix which results singular but at the same time
	positive	definite
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>

Dear Stefano,

You've already had a couple of informative responses directly addressing your question, but are you aware how ill-conditioned the matrix is (one of the responses alluded to this)?

> kappa(X, exact=TRUE)
[1] 7.313338e+12

> eigen(X)$values
[1] 4.964711e+00 9.356881e-01 4.863392e-12 6.788344e-13

Two of the variables have variances around 10^0 and the other two around 10^-12. Of course, you haven't said anything about the context, but does it really make sense to analyze the data on these scales?

Best,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox




> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Stefano Sofia
> Sent: December 10, 2015 5:08 AM
> To: r-help at r-project.org
> Subject: [R] matrix which results singular but at the same time positive definite
> 
> Dear list users,
> through the "matrixcalc" package I am performing some checks of variance
> matrices (which must be positive definite).
> In this example, it happens that the matrix A here reported is singular but
> positive definite. Is it possible?
> 
>               [,1]          [,2]          [,3]          [,4]
> [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12 [2,] -
> 1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12 [3,] -8.238960e-13
> 1.364242e-12  4.809988e+00  7.742369e-01 [4,] -1.240294e-12  1.818989e-12
> 7.742369e-01  1.090411e+00
> 
> print(is.non.singular.matrix(A, tol = 1e-18)) FALSE print(is.positive.definite(A,
> tol=1e-18)) TRUE
> 
> Is there something wrong with this matrix?
> Any comment will be appreciated.
> Stefano
> 
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla
> ricezione. I messaggi di posta elettronica per i client di Regione Marche
> possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il
> destinatario specificato, non leggere, copiare, inoltrare o archiviare questo
> messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente
> ed eliminarlo completamente dal sistema del proprio computer. Ai sensi
> dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed
> urgenza, la risposta al presente messaggio di posta elettronica pu? essere
> visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by
> persons entitled to receive the confidential information it may contain. E-mail
> messages to clients of Regione Marche may contain information that is
> confidential and legally privileged. Please do not read, copy, forward, or store
> this message unless you are an intended recipient of it. If you have received
> this message in error, please forward it to the sender and delete it completely
> from your computer system.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

From tmrsg11 at gmail.com  Thu Dec 10 16:17:47 2015
From: tmrsg11 at gmail.com (C W)
Date: Thu, 10 Dec 2015 10:17:47 -0500
Subject: [R] If I have 3 parameters,
	is optim() doing the same thing as Gibbs sampling?
Message-ID: <CAE2FW2=bbjCaPMNttRcwoWX6+6zMdPdCOmS1rDACNP9GPRhu6A@mail.gmail.com>

Hi R list,

I am using optim() to optimize a function with 3 parameters.

#My 1-d toy example: loglikelihood of normal with x=c(2,5,3,7,-3,-2,0),
find MLE of mean.

p1 <- function(theta){
    sum(log(dnorm(c(2,5,3,7,-3,-2,0), mean = theta, sd = 1))) +log(dnorm(
theta, mean = 0.8, sd = 2))
}
optimize(p1, c(-3, 5), maximum = TRUE)


My question:

If function p1 has 3 parameters, is it doing the same thing as Gibbs
sampling?

In Gibbs, we optimize parameter 1 while fixing parameter 2 and 3.  Then
optimize 2, fixing 1 and 3. Repeat until convergence.

How does optim() choose random numbers?  I am using the default,
Nelder-Mead.

Is optim() drawing numbers from uniform distribution?  Is it picking from
[-Inf, Inf]?  What if I want draw from a prior N(3, 1) instead?

Thanks so much!

Mike

	[[alternative HTML version deleted]]


From rik.verdonck at bio.kuleuven.be  Thu Dec 10 16:21:07 2015
From: rik.verdonck at bio.kuleuven.be (Rik Verdonck)
Date: Thu, 10 Dec 2015 15:21:07 +0000
Subject: [R] package MASS - MLE of negative binomial distributions
Message-ID: <FCEEA3779CE37C4691254848B5FCEF7B441A98E7@ICTS-S-MBX1.luna.kuleuven.be>

Dear list,



I have a question about the exact estimate of the maximum likelihood for a negative binomial fit. I'm trying to approach this in two different ways: the first one is a fit using the glm.nb method, and the second one is a fit using the fitdistr function for each condition separately, where I add up all log likelihoods. These two methods do not yield the same values for the log likelihood of the fit. They do yield the same log likelihood if all data are one group (no summation), so I assume I'm doing something wrong when I sum up log likelihoods. Am I not "allowed" to do this?


Example code:
library(MASS)
x<-c(601,619,637,609,594,499,494,507,477,450,400,367,428,359,400,276,260,262,304,342,216,189,152,231,200,104,85,85,85,112)
groups<-as.factor(c(rep("dist1",5),rep("dist2",5),rep("dist3",5),rep("dist4",5),rep("dist5",5),rep("dist6",5)))

glm.nb(x~groups)$twologlik

logliks<-NULL
for(group in levels(groups))
{
	NBfit<-fitdistr(x[groups==group],"Negative Binomial")
	logliks<-c(logliks,NBfit$loglik)
	rm(NBfit)
}

sum(logliks)*2


Many thanks!
Rik



From S.Ellison at LGCGroup.com  Thu Dec 10 16:28:52 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Thu, 10 Dec 2015 15:28:52 +0000
Subject: [R] If I have 3 parameters,
 is optim() doing the same thing as Gibbs sampling?
In-Reply-To: <CAE2FW2=bbjCaPMNttRcwoWX6+6zMdPdCOmS1rDACNP9GPRhu6A@mail.gmail.com>
References: <CAE2FW2=bbjCaPMNttRcwoWX6+6zMdPdCOmS1rDACNP9GPRhu6A@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403C97B0E51@GBTEDVPEXCMB04.corp.lgc-group.com>

> If function p1 has 3 parameters, is it doing the same thing as Gibbs sampling?
No.

> How does optim() choose random numbers?  
It doesn't, for Nelder-Mead. Nelder-Mead is a deterministic algorithm that does not need random numbers.

Read up on Nelder-Mead; it - and everything else in optim() - is pretty much completely different from MCMC using a Gibbs sampling algorithm

S Ellison




*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From stefano.sofia at regione.marche.it  Thu Dec 10 16:41:52 2015
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Thu, 10 Dec 2015 15:41:52 +0000
Subject: [R] matrix which results singular but at the same time
	positive	definite
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>,
	<ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>

Dear John,
thank you for your considerations.
This matrix (which is a variance matrix) is part of an algorithm for forward-filtering and backward-sampling of Dynamic Linear Models (West and Harrison, 1997), applied to DLM representation of ARIMA processes (Petris, Petrone, Campagnoli).  It is therefore very difficult to explain why this variance matrix becomes so ill conditioned. This already happens at the first iteration of the algorithm. I will try to work on initial conditions and some fixed parameters.

Thank you again
Stefano


________________________________________
Da: Fox, John [jfox at mcmaster.ca]
Inviato: gioved? 10 dicembre 2015 14.41
A: Stefano Sofia; r-help at r-project.org
Oggetto: RE: matrix which results singular but at the same time positive        definite

Dear Stefano,

You've already had a couple of informative responses directly addressing your question, but are you aware how ill-conditioned the matrix is (one of the responses alluded to this)?

> kappa(X, exact=TRUE)
[1] 7.313338e+12

> eigen(X)$values
[1] 4.964711e+00 9.356881e-01 4.863392e-12 6.788344e-13

Two of the variables have variances around 10^0 and the other two around 10^-12. Of course, you haven't said anything about the context, but does it really make sense to analyze the data on these scales?

Best,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox




> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Stefano Sofia
> Sent: December 10, 2015 5:08 AM
> To: r-help at r-project.org
> Subject: [R] matrix which results singular but at the same time positive definite
>
> Dear list users,
> through the "matrixcalc" package I am performing some checks of variance
> matrices (which must be positive definite).
> In this example, it happens that the matrix A here reported is singular but
> positive definite. Is it possible?
>
>               [,1]          [,2]          [,3]          [,4]
> [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12 [2,] -
> 1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12 [3,] -8.238960e-13
> 1.364242e-12  4.809988e+00  7.742369e-01 [4,] -1.240294e-12  1.818989e-12
> 7.742369e-01  1.090411e+00
>
> print(is.non.singular.matrix(A, tol = 1e-18)) FALSE print(is.positive.definite(A,
> tol=1e-18)) TRUE
>
> Is there something wrong with this matrix?
> Any comment will be appreciated.
> Stefano
>
>
> ________________________________
>
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla
> ricezione. I messaggi di posta elettronica per i client di Regione Marche
> possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il
> destinatario specificato, non leggere, copiare, inoltrare o archiviare questo
> messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente
> ed eliminarlo completamente dal sistema del proprio computer. Ai sensi
> dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed
> urgenza, la risposta al presente messaggio di posta elettronica pu? essere
> visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by
> persons entitled to receive the confidential information it may contain. E-mail
> messages to clients of Regione Marche may contain information that is
> confidential and legally privileged. Please do not read, copy, forward, or store
> this message unless you are an intended recipient of it. If you have received
> this message in error, please forward it to the sender and delete it completely
> from your computer system.
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.


From jfox at mcmaster.ca  Thu Dec 10 16:52:01 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 10 Dec 2015 15:52:01 +0000
Subject: [R] matrix which results singular but at the same time
	positive	definite
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>,
	<ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>
	<8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F41C69@FHSDB2D11-2.csu.mcmaster.ca>

Dear Stefano,

I don't really know anything about your application, but my point is about the scaling of the variables. Can't you rescale some or all of the variables so their variances aren't so different? For example, the correlation matrix among these four variables isn't ill-conditioned.

Best,
 John

> -----Original Message-----
> From: Stefano Sofia [mailto:stefano.sofia at regione.marche.it]
> Sent: Thursday, December 10, 2015 10:42 AM
> To: Fox, John; r-help at r-project.org
> Subject: RE: matrix which results singular but at the same time positive
> definite
> 
> Dear John,
> thank you for your considerations.
> This matrix (which is a variance matrix) is part of an algorithm for
> forward-filtering and backward-sampling of Dynamic Linear Models (West
> and Harrison, 1997), applied to DLM representation of ARIMA processes
> (Petris, Petrone, Campagnoli).  It is therefore very difficult to
> explain why this variance matrix becomes so ill conditioned. This
> already happens at the first iteration of the algorithm. I will try to
> work on initial conditions and some fixed parameters.
> 
> Thank you again
> Stefano
> 
> 
> ________________________________________
> Da: Fox, John [jfox at mcmaster.ca]
> Inviato: gioved? 10 dicembre 2015 14.41
> A: Stefano Sofia; r-help at r-project.org
> Oggetto: RE: matrix which results singular but at the same time positive
> definite
> 
> Dear Stefano,
> 
> You've already had a couple of informative responses directly addressing
> your question, but are you aware how ill-conditioned the matrix is (one
> of the responses alluded to this)?
> 
> > kappa(X, exact=TRUE)
> [1] 7.313338e+12
> 
> > eigen(X)$values
> [1] 4.964711e+00 9.356881e-01 4.863392e-12 6.788344e-13
> 
> Two of the variables have variances around 10^0 and the other two around
> 10^-12. Of course, you haven't said anything about the context, but does
> it really make sense to analyze the data on these scales?
> 
> Best,
>  John
> 
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
> 
> 
> 
> 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Stefano Sofia
> > Sent: December 10, 2015 5:08 AM
> > To: r-help at r-project.org
> > Subject: [R] matrix which results singular but at the same time
> positive definite
> >
> > Dear list users,
> > through the "matrixcalc" package I am performing some checks of
> variance
> > matrices (which must be positive definite).
> > In this example, it happens that the matrix A here reported is
> singular but
> > positive definite. Is it possible?
> >
> >               [,1]          [,2]          [,3]          [,4]
> > [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12 [2,] -
> > 1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12 [3,] -
> 8.238960e-13
> > 1.364242e-12  4.809988e+00  7.742369e-01 [4,] -1.240294e-12
> 1.818989e-12
> > 7.742369e-01  1.090411e+00
> >
> > print(is.non.singular.matrix(A, tol = 1e-18)) FALSE
> print(is.positive.definite(A,
> > tol=1e-18)) TRUE
> >
> > Is there something wrong with this matrix?
> > Any comment will be appreciated.
> > Stefano
> >
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> > informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla
> > ricezione. I messaggi di posta elettronica per i client di Regione
> Marche
> > possono contenere informazioni confidenziali e con privilegi legali.
> Se non si ? il
> > destinatario specificato, non leggere, copiare, inoltrare o archiviare
> questo
> > messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al
> mittente
> > ed eliminarlo completamente dal sistema del proprio computer. Ai sensi
> > dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di
> necessit? ed
> > urgenza, la risposta al presente messaggio di posta elettronica pu?
> essere
> > visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only
> by
> > persons entitled to receive the confidential information it may
> contain. E-mail
> > messages to clients of Regione Marche may contain information that is
> > confidential and legally privileged. Please do not read, copy,
> forward, or store
> > this message unless you are an intended recipient of it. If you have
> received
> > this message in error, please forward it to the sender and delete it
> completely
> > from your computer system.
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla ricezione. I messaggi di posta elettronica per i client
> di Regione Marche possono contenere informazioni confidenziali e con
> privilegi legali. Se non si ? il destinatario specificato, non leggere,
> copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto
> questo messaggio per errore, inoltrarlo al mittente ed eliminarlo
> completamente dal sistema del proprio computer. Ai sensi dell'art. 6
> della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza,
> la risposta al presente messaggio di posta elettronica pu? essere
> visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by
> persons entitled to receive the confidential information it may contain.
> E-mail messages to clients of Regione Marche may contain information
> that is confidential and legally privileged. Please do not read, copy,
> forward, or store this message unless you are an intended recipient of
> it. If you have received this message in error, please forward it to the
> sender and delete it completely from your computer system.


From pdalgd at gmail.com  Thu Dec 10 16:53:16 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 10 Dec 2015 16:53:16 +0100
Subject: [R] package MASS - MLE of negative binomial distributions
In-Reply-To: <FCEEA3779CE37C4691254848B5FCEF7B441A98E7@ICTS-S-MBX1.luna.kuleuven.be>
References: <FCEEA3779CE37C4691254848B5FCEF7B441A98E7@ICTS-S-MBX1.luna.kuleuven.be>
Message-ID: <00F73C35-D0F0-498E-B969-14C7FA85049C@gmail.com>

glm.nb fits 6 mean parameters plus 1 theta. 6 x fitdistr fits two parameters each.

-pd

On 10 Dec 2015, at 16:21 , Rik Verdonck <Rik.Verdonck at bio.kuleuven.be> wrote:

> Dear list,
> 
> 
> 
> I have a question about the exact estimate of the maximum likelihood for a negative binomial fit. I'm trying to approach this in two different ways: the first one is a fit using the glm.nb method, and the second one is a fit using the fitdistr function for each condition separately, where I add up all log likelihoods. These two methods do not yield the same values for the log likelihood of the fit. They do yield the same log likelihood if all data are one group (no summation), so I assume I'm doing something wrong when I sum up log likelihoods. Am I not "allowed" to do this?
> 
> 
> Example code:
> library(MASS)
> x<-c(601,619,637,609,594,499,494,507,477,450,400,367,428,359,400,276,260,262,304,342,216,189,152,231,200,104,85,85,85,112)
> groups<-as.factor(c(rep("dist1",5),rep("dist2",5),rep("dist3",5),rep("dist4",5),rep("dist5",5),rep("dist6",5)))
> 
> glm.nb(x~groups)$twologlik
> 
> logliks<-NULL
> for(group in levels(groups))
> {
> 	NBfit<-fitdistr(x[groups==group],"Negative Binomial")
> 	logliks<-c(logliks,NBfit$loglik)
> 	rm(NBfit)
> }
> 
> sum(logliks)*2
> 
> 
> Many thanks!
> Rik
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From rik.verdonck at bio.kuleuven.be  Thu Dec 10 17:04:48 2015
From: rik.verdonck at bio.kuleuven.be (Rik Verdonck)
Date: Thu, 10 Dec 2015 16:04:48 +0000
Subject: [R] package MASS - MLE of negative binomial distributions
In-Reply-To: <00F73C35-D0F0-498E-B969-14C7FA85049C@gmail.com>
References: <FCEEA3779CE37C4691254848B5FCEF7B441A98E7@ICTS-S-MBX1.luna.kuleuven.be>,
	<00F73C35-D0F0-498E-B969-14C7FA85049C@gmail.com>
Message-ID: <FCEEA3779CE37C4691254848B5FCEF7B441A9931@ICTS-S-MBX1.luna.kuleuven.be>

That makes sense indeed, but it also raises further questions to me if you don't mind:
- Is there a good reason why the default assumption would be a constant overdispersion?  Is it because you lose fewer degrees of freedom which would give you more power in comparing models?
- Is there a way to tell glm.nb to fit the thetas separately per group?
- Is there a way to fix theta (i.e. feed it a value) and only make an estimate of the expected value?

Many thanks!
Rik


________________________________________
Van: peter dalgaard [pdalgd at gmail.com]
Verzonden: donderdag 10 december 2015 16:53
Aan: Rik Verdonck
CC: r-help at r-project.org
Onderwerp: Re: [R] package MASS - MLE of negative binomial distributions

glm.nb fits 6 mean parameters plus 1 theta. 6 x fitdistr fits two parameters each.

-pd

On 10 Dec 2015, at 16:21 , Rik Verdonck <Rik.Verdonck at bio.kuleuven.be> wrote:

> Dear list,
>
>
>
> I have a question about the exact estimate of the maximum likelihood for a negative binomial fit. I'm trying to approach this in two different ways: the first one is a fit using the glm.nb method, and the second one is a fit using the fitdistr function for each condition separately, where I add up all log likelihoods. These two methods do not yield the same values for the log likelihood of the fit. They do yield the same log likelihood if all data are one group (no summation), so I assume I'm doing something wrong when I sum up log likelihoods. Am I not "allowed" to do this?
>
>
> Example code:
> library(MASS)
> x<-c(601,619,637,609,594,499,494,507,477,450,400,367,428,359,400,276,260,262,304,342,216,189,152,231,200,104,85,85,85,112)
> groups<-as.factor(c(rep("dist1",5),rep("dist2",5),rep("dist3",5),rep("dist4",5),rep("dist5",5),rep("dist6",5)))
>
> glm.nb(x~groups)$twologlik
>
> logliks<-NULL
> for(group in levels(groups))
> {
>       NBfit<-fitdistr(x[groups==group],"Negative Binomial")
>       logliks<-c(logliks,NBfit$loglik)
>       rm(NBfit)
> }
>
> sum(logliks)*2
>
>
> Many thanks!
> Rik
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com











From pdalgd at gmail.com  Thu Dec 10 17:09:12 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 10 Dec 2015 17:09:12 +0100
Subject: [R] matrix which results singular but at the same time positive
	definite
In-Reply-To: <8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>,
	<ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>
	<8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>
Message-ID: <F86F10C7-D61E-4B72-8A78-A0A4E9082456@gmail.com>

Looks like the ill-conditioning is almost entirely due to scaling, e.g.

> eigen(cov2cor(m))
$values
[1] 1.7234899 1.3380701 0.6619299 0.2765101
...

This is an annoyance in several parts of numerical linear algebra: Routines assume that R^n has all coordinates on a similar scale and therefore think that anything on the order of 1e-7 or so is effectively zero. 

Condition numbers do this too:

> kappa(m)
[1] 1.066582e+13
> kappa(cov2cor(m))
[1] 5.489243


-pd

On 10 Dec 2015, at 16:41 , Stefano Sofia <stefano.sofia at regione.marche.it> wrote:

> Dear John,
> thank you for your considerations.
> This matrix (which is a variance matrix) is part of an algorithm for forward-filtering and backward-sampling of Dynamic Linear Models (West and Harrison, 1997), applied to DLM representation of ARIMA processes (Petris, Petrone, Campagnoli).  It is therefore very difficult to explain why this variance matrix becomes so ill conditioned. This already happens at the first iteration of the algorithm. I will try to work on initial conditions and some fixed parameters.
> 
> Thank you again
> Stefano
> 
> 
> ________________________________________
> Da: Fox, John [jfox at mcmaster.ca]
> Inviato: gioved? 10 dicembre 2015 14.41
> A: Stefano Sofia; r-help at r-project.org
> Oggetto: RE: matrix which results singular but at the same time positive        definite
> 
> Dear Stefano,
> 
> You've already had a couple of informative responses directly addressing your question, but are you aware how ill-conditioned the matrix is (one of the responses alluded to this)?
> 
>> kappa(X, exact=TRUE)
> [1] 7.313338e+12
> 
>> eigen(X)$values
> [1] 4.964711e+00 9.356881e-01 4.863392e-12 6.788344e-13
> 
> Two of the variables have variances around 10^0 and the other two around 10^-12. Of course, you haven't said anything about the context, but does it really make sense to analyze the data on these scales?
> 
> Best,
> John
> 
> -----------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> Web: socserv.mcmaster.ca/jfox
> 
> 
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Stefano Sofia
>> Sent: December 10, 2015 5:08 AM
>> To: r-help at r-project.org
>> Subject: [R] matrix which results singular but at the same time positive definite
>> 
>> Dear list users,
>> through the "matrixcalc" package I am performing some checks of variance
>> matrices (which must be positive definite).
>> In this example, it happens that the matrix A here reported is singular but
>> positive definite. Is it possible?
>> 
>>              [,1]          [,2]          [,3]          [,4]
>> [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12 [2,] -
>> 1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12 [3,] -8.238960e-13
>> 1.364242e-12  4.809988e+00  7.742369e-01 [4,] -1.240294e-12  1.818989e-12
>> 7.742369e-01  1.090411e+00
>> 
>> print(is.non.singular.matrix(A, tol = 1e-18)) FALSE print(is.positive.definite(A,
>> tol=1e-18)) TRUE
>> 
>> Is there something wrong with this matrix?
>> Any comment will be appreciated.
>> Stefano
>> 
>> 
>> ________________________________
>> 
>> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
>> informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla
>> ricezione. I messaggi di posta elettronica per i client di Regione Marche
>> possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il
>> destinatario specificato, non leggere, copiare, inoltrare o archiviare questo
>> messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente
>> ed eliminarlo completamente dal sistema del proprio computer. Ai sensi
>> dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed
>> urgenza, la risposta al presente messaggio di posta elettronica pu? essere
>> visionata da persone estranee al destinatario.
>> IMPORTANT NOTICE: This e-mail message is intended to be received only by
>> persons entitled to receive the confidential information it may contain. E-mail
>> messages to clients of Regione Marche may contain information that is
>> confidential and legally privileged. Please do not read, copy, forward, or store
>> this message unless you are an intended recipient of it. If you have received
>> this message in error, please forward it to the sender and delete it completely
>> from your computer system.
>> 
>>      [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ________________________________
> 
> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
> IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Thu Dec 10 17:19:15 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 10 Dec 2015 17:19:15 +0100
Subject: [R] package MASS - MLE of negative binomial distributions
In-Reply-To: <FCEEA3779CE37C4691254848B5FCEF7B441A9931@ICTS-S-MBX1.luna.kuleuven.be>
References: <FCEEA3779CE37C4691254848B5FCEF7B441A98E7@ICTS-S-MBX1.luna.kuleuven.be>,
	<00F73C35-D0F0-498E-B969-14C7FA85049C@gmail.com>
	<FCEEA3779CE37C4691254848B5FCEF7B441A9931@ICTS-S-MBX1.luna.kuleuven.be>
Message-ID: <371C1D36-C3D4-484A-B44D-FF79D2361CD2@gmail.com>

The author usually points out that this is support software for a book, which you should probably consult.

Generally speaking, theta in glm.nb plays the same role as the variance in least squares, and you can and cannot do essentially the same things with it.

If you want to do more than that, you might want to look at generic MLE routines, like the one in stats4, or Ben Bolker's packages.

-pd

On 10 Dec 2015, at 17:04 , Rik Verdonck <rik.verdonck at bio.kuleuven.be> wrote:

> That makes sense indeed, but it also raises further questions to me if you don't mind:
> - Is there a good reason why the default assumption would be a constant overdispersion?  Is it because you lose fewer degrees of freedom which would give you more power in comparing models?
> - Is there a way to tell glm.nb to fit the thetas separately per group?
> - Is there a way to fix theta (i.e. feed it a value) and only make an estimate of the expected value?
> 
> Many thanks!
> Rik
> 
> 
> ________________________________________
> Van: peter dalgaard [pdalgd at gmail.com]
> Verzonden: donderdag 10 december 2015 16:53
> Aan: Rik Verdonck
> CC: r-help at r-project.org
> Onderwerp: Re: [R] package MASS - MLE of negative binomial distributions
> 
> glm.nb fits 6 mean parameters plus 1 theta. 6 x fitdistr fits two parameters each.
> 
> -pd
> 
> On 10 Dec 2015, at 16:21 , Rik Verdonck <Rik.Verdonck at bio.kuleuven.be> wrote:
> 
>> Dear list,
>> 
>> 
>> 
>> I have a question about the exact estimate of the maximum likelihood for a negative binomial fit. I'm trying to approach this in two different ways: the first one is a fit using the glm.nb method, and the second one is a fit using the fitdistr function for each condition separately, where I add up all log likelihoods. These two methods do not yield the same values for the log likelihood of the fit. They do yield the same log likelihood if all data are one group (no summation), so I assume I'm doing something wrong when I sum up log likelihoods. Am I not "allowed" to do this?
>> 
>> 
>> Example code:
>> library(MASS)
>> x<-c(601,619,637,609,594,499,494,507,477,450,400,367,428,359,400,276,260,262,304,342,216,189,152,231,200,104,85,85,85,112)
>> groups<-as.factor(c(rep("dist1",5),rep("dist2",5),rep("dist3",5),rep("dist4",5),rep("dist5",5),rep("dist6",5)))
>> 
>> glm.nb(x~groups)$twologlik
>> 
>> logliks<-NULL
>> for(group in levels(groups))
>> {
>>      NBfit<-fitdistr(x[groups==group],"Negative Binomial")
>>      logliks<-c(logliks,NBfit$loglik)
>>      rm(NBfit)
>> }
>> 
>> sum(logliks)*2
>> 
>> 
>> Many thanks!
>> Rik
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jfox at mcmaster.ca  Thu Dec 10 17:24:20 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Thu, 10 Dec 2015 16:24:20 +0000
Subject: [R] matrix which results singular but at the same time positive
 definite
In-Reply-To: <F86F10C7-D61E-4B72-8A78-A0A4E9082456@gmail.com>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>,
	<ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>
	<8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>
	<F86F10C7-D61E-4B72-8A78-A0A4E9082456@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F41CD0@FHSDB2D11-2.csu.mcmaster.ca>

Dear Peter,

> -----Original Message-----
> From: peter dalgaard [mailto:pdalgd at gmail.com]
> Sent: Thursday, December 10, 2015 11:09 AM
> To: Stefano Sofia
> Cc: Fox, John; r-help at r-project.org
> Subject: Re: [R] matrix which results singular but at the same time
> positive definite
> 
> Looks like the ill-conditioning is almost entirely due to scaling, e.g.

Yes, that's my point. Sorry I didn't make it clearer.

Best,
 John

> 
> > eigen(cov2cor(m))
> $values
> [1] 1.7234899 1.3380701 0.6619299 0.2765101
> ...
> 
> This is an annoyance in several parts of numerical linear algebra:
> Routines assume that R^n has all coordinates on a similar scale and
> therefore think that anything on the order of 1e-7 or so is effectively
> zero.
> 
> Condition numbers do this too:
> 
> > kappa(m)
> [1] 1.066582e+13
> > kappa(cov2cor(m))
> [1] 5.489243
> 
> 
> -pd
> 
> On 10 Dec 2015, at 16:41 , Stefano Sofia
> <stefano.sofia at regione.marche.it> wrote:
> 
> > Dear John,
> > thank you for your considerations.
> > This matrix (which is a variance matrix) is part of an algorithm for
> forward-filtering and backward-sampling of Dynamic Linear Models (West
> and Harrison, 1997), applied to DLM representation of ARIMA processes
> (Petris, Petrone, Campagnoli).  It is therefore very difficult to
> explain why this variance matrix becomes so ill conditioned. This
> already happens at the first iteration of the algorithm. I will try to
> work on initial conditions and some fixed parameters.
> >
> > Thank you again
> > Stefano
> >
> >
> > ________________________________________
> > Da: Fox, John [jfox at mcmaster.ca]
> > Inviato: gioved? 10 dicembre 2015 14.41
> > A: Stefano Sofia; r-help at r-project.org
> > Oggetto: RE: matrix which results singular but at the same time
> positive        definite
> >
> > Dear Stefano,
> >
> > You've already had a couple of informative responses directly
> addressing your question, but are you aware how ill-conditioned the
> matrix is (one of the responses alluded to this)?
> >
> >> kappa(X, exact=TRUE)
> > [1] 7.313338e+12
> >
> >> eigen(X)$values
> > [1] 4.964711e+00 9.356881e-01 4.863392e-12 6.788344e-13
> >
> > Two of the variables have variances around 10^0 and the other two
> around 10^-12. Of course, you haven't said anything about the context,
> but does it really make sense to analyze the data on these scales?
> >
> > Best,
> > John
> >
> > -----------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > Web: socserv.mcmaster.ca/jfox
> >
> >
> >
> >
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Stefano Sofia
> >> Sent: December 10, 2015 5:08 AM
> >> To: r-help at r-project.org
> >> Subject: [R] matrix which results singular but at the same time
> positive definite
> >>
> >> Dear list users,
> >> through the "matrixcalc" package I am performing some checks of
> variance
> >> matrices (which must be positive definite).
> >> In this example, it happens that the matrix A here reported is
> singular but
> >> positive definite. Is it possible?
> >>
> >>              [,1]          [,2]          [,3]          [,4]
> >> [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12 [2,] -
> >> 1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12 [3,] -
> 8.238960e-13
> >> 1.364242e-12  4.809988e+00  7.742369e-01 [4,] -1.240294e-12
> 1.818989e-12
> >> 7.742369e-01  1.090411e+00
> >>
> >> print(is.non.singular.matrix(A, tol = 1e-18)) FALSE
> print(is.positive.definite(A,
> >> tol=1e-18)) TRUE
> >>
> >> Is there something wrong with this matrix?
> >> Any comment will be appreciated.
> >> Stefano
> >>
> >>
> >> ________________________________
> >>
> >> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu?
> contenere
> >> informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla
> >> ricezione. I messaggi di posta elettronica per i client di Regione
> Marche
> >> possono contenere informazioni confidenziali e con privilegi legali.
> Se non si ? il
> >> destinatario specificato, non leggere, copiare, inoltrare o
> archiviare questo
> >> messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo
> al mittente
> >> ed eliminarlo completamente dal sistema del proprio computer. Ai
> sensi
> >> dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di
> necessit? ed
> >> urgenza, la risposta al presente messaggio di posta elettronica pu?
> essere
> >> visionata da persone estranee al destinatario.
> >> IMPORTANT NOTICE: This e-mail message is intended to be received only
> by
> >> persons entitled to receive the confidential information it may
> contain. E-mail
> >> messages to clients of Regione Marche may contain information that is
> >> confidential and legally privileged. Please do not read, copy,
> forward, or store
> >> this message unless you are an intended recipient of it. If you have
> received
> >> this message in error, please forward it to the sender and delete it
> completely
> >> from your computer system.
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-
> >> guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla ricezione. I messaggi di posta elettronica per i client
> di Regione Marche possono contenere informazioni confidenziali e con
> privilegi legali. Se non si ? il destinatario specificato, non leggere,
> copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto
> questo messaggio per errore, inoltrarlo al mittente ed eliminarlo
> completamente dal sistema del proprio computer. Ai sensi dell'art. 6
> della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza,
> la risposta al presente messaggio di posta elettronica pu? essere
> visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only
> by persons entitled to receive the confidential information it may
> contain. E-mail messages to clients of Regione Marche may contain
> information that is confidential and legally privileged. Please do not
> read, copy, forward, or store this message unless you are an intended
> recipient of it. If you have received this message in error, please
> forward it to the sender and delete it completely from your computer
> system.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 


From markpayneatwork at gmail.com  Thu Dec 10 17:38:59 2015
From: markpayneatwork at gmail.com (Mark R Payne)
Date: Thu, 10 Dec 2015 17:38:59 +0100
Subject: [R] Raster-package - problem with stackApply()
Message-ID: <CAGBzUO_5+gNkS7vZarcC4ezPrFuwVywh9VXsQHxeUcsqXP0PFA@mail.gmail.com>

Hi,

I am trying to use stackApply() to perform averages over subsets of a
brick. However, I am struggling with the indices argument, and how it
should be interpreted. Here is a simple working example illustrating my
problem:

r <- raster()
r[] <- 1

inp <- brick(r,r,r,r,r,r)*(1:6)
res <- stackApply(inp,c(2,2,3,3,1,1),mean)

Now if we look at the values of each object:

> inp
names       : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6
min values  :       1,       2,       3,       4,       5,       6
max values  :       1,       2,       3,       4,       5,       6

> res
names       : layer.1, layer.2, layer.3
min values  :     3.5,     5.5,     1.5
max values  :     3.5,     5.5,     1.5

Now, the problem is that the names and order of the layers in "res" don't
line up with the indices that I provided. You can do the maths in your head
- e.g. the first two layers of "inp" have values of 1 and 2, so their mean
should be 1.5 - however, this is ending up as layer 3 in "res".

So how should the indices argument be interpreted in this context?

Suggestion: A more intuitive format for the "indices" argument in
stackApply might be as a factor - this way the order is implict and
stackApply ends up working similar to split() or tapply()...

Best wishes,

Mark

	[[alternative HTML version deleted]]


From 081024015 at fudan.edu.cn  Thu Dec 10 11:06:52 2015
From: 081024015 at fudan.edu.cn (=?UTF-8?B?5p2O55Cl?=)
Date: Thu, 10 Dec 2015 18:06:52 +0800 (GMT+08:00)
Subject: [R] Errors when compile RInside using intel c++ compiler on windows
Message-ID: <549bdfb4.1a98b.1518b5b9033.Coremail.081024015@fudan.edu.cn>

   I have read the mailing list about compiling R using intel c++ compiler on linux.
http://lists.r-forge.r-project.org/pipermail/rcpp-devel/2013-January/005035.html

    In my environment I am going to compile RInside using intel c++ compiler on windows.
I take the sample0 in RInside example folder for test. It's the "Hello world" example.
After fixing all the compile errors following lingk errors appears:

error LNK2019:unresolved external symblo "public : _thiscall RInside::RInside(int,char.....)"
error LNK2019:unresolved external symblo "public : _thiscall RInside::~RInside(void)"
error LNK2019:unresolved external symblo "public : _thiscall RInside::parseEvalQ(class std::basic_string....)"
.....
    I created RInside.lib from the RInside.dll(comes from the RInside\libs\i386) using pexports and lib command.
After adding the RInside.lib into the project->property->linker->input, the errors still exists.
    Does anyone know how to fix these errors? Or is there anyone who knows how to compile RInside program using intel
c++ compiler on windows?

                                                                                                  Eric


From adrian.waddell at gmail.com  Thu Dec 10 12:09:12 2015
From: adrian.waddell at gmail.com (Adrian Waddell)
Date: Thu, 10 Dec 2015 12:09:12 +0100
Subject: [R] choropleth packages (US)
In-Reply-To: <B4FD6FEF-D170-45B4-B51D-CB8D8070020B@univie.ac.at>
References: <56649494.3050206@gmail.com>
	<B4FD6FEF-D170-45B4-B51D-CB8D8070020B@univie.ac.at>
Message-ID: <CACa4aQ6=SXpsf6QbBQ9pLGd=h_FHiVhGFVQ+_c4Z-p17v4dREQ@mail.gmail.com>

Hi,

You can also use the 'maps' package for the map data and the 'scales'
package for the color mapping.

E.g.

library(maps)
library(scales)

m <- map('state', fill=TRUE, plot=FALSE)

s_data <- tolower(rownames(USArrests))
s_map <- tolower(m$names)

mapping <- lapply(s_data, function(state) {
  which(grepl(state, s_map))
})
## check if the mapping is good!

col_pal <- col_numeric("Greens", domain=NULL, na.color = 'lightyellow')

cols <- rep('lightyellow', length(s_data))

Map(function(indices, col) {
  cols[indices] <<- col
}, mapping, col_pal(USArrests$UrbanPop))

map(m, col=cols, fill=TRUE)


Adrian



On Mon, Dec 7, 2015 at 9:34 AM, Erich Neuwirth
<erich.neuwirth at univie.ac.at> wrote:
> ggplot2 also can do this with
> fortify
> geom_polygon
>
> Von meinem iPad gesendet
>
>> Am 06.12.2015 um 21:03 schrieb Benjamin Tyner <btyner at gmail.com>:
>>
>> Hi
>>
>> I wish to draw a basic choropleth (US, by state) and am wondering if anyone has any recommendations? I've tried the following thus far:
>>
>> 1. choroplethr: this works, but required installation of 30+ dependencies. I would prefer something with fewer dependencies.
>> 2. tmap: this also seems promising, but most of the examples I saw were specific to European maps. Can it be adapted for US?
>> 3. statebins: doesn't draw true choropleths, but I liked that it doesn't have many dependencies.
>>
>> Regards
>> Ben
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From erinm.hodgess at gmail.com  Thu Dec 10 18:10:23 2015
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Thu, 10 Dec 2015 11:10:23 -0600
Subject: [R] Correct notation for functions, packages when using LaTex
Message-ID: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>

Hello everyone!

I am writing up something (quickly) using LaTex and periodically refer to R
functions and packages.

What is the correct way to put those into LaTex, please?  I know that R
itself is {\tt R}, but am not sure about the others.

Thanks for any help,
Sincerely,
Erin
PS  Or should I just be doing this in R Studio, even though there is no
code, please?


-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From kevin.thorpe at utoronto.ca  Thu Dec 10 18:14:39 2015
From: kevin.thorpe at utoronto.ca (Kevin E. Thorpe)
Date: Thu, 10 Dec 2015 12:14:39 -0500
Subject: [R] Correct notation for functions, packages when using LaTex
In-Reply-To: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
References: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
Message-ID: <5669B2FF.8090806@utoronto.ca>

On 12/10/2015 12:10 PM, Erin Hodgess wrote:
> Hello everyone!
>
> I am writing up something (quickly) using LaTex and periodically refer to R
> functions and packages.
>
> What is the correct way to put those into LaTex, please?  I know that R
> itself is {\tt R}, but am not sure about the others.
>
> Thanks for any help,
> Sincerely,
> Erin
> PS  Or should I just be doing this in R Studio, even though there is no
> code, please?
>
>

When I refer to R function in a LaTeX document I tend to use \texttt{} 
(same as your {\tt } construct). Historical convention rendered computer 
code in a monospace font (akin to Courier) so that's what I follow.

-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016


From erinm.hodgess at gmail.com  Thu Dec 10 18:19:52 2015
From: erinm.hodgess at gmail.com (Erin Hodgess)
Date: Thu, 10 Dec 2015 11:19:52 -0600
Subject: [R] Correct notation for functions, packages when using LaTex
In-Reply-To: <5669B2FF.8090806@utoronto.ca>
References: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
	<5669B2FF.8090806@utoronto.ca>
Message-ID: <CACxE24mSYm=DgRg1_rqMmocguuvOfmqJb2DJ1aZfK3-J7GhmRw@mail.gmail.com>

Great,
Thanks!
Erin


On Thu, Dec 10, 2015 at 11:14 AM, Kevin E. Thorpe <kevin.thorpe at utoronto.ca>
wrote:

> On 12/10/2015 12:10 PM, Erin Hodgess wrote:
>
>> Hello everyone!
>>
>> I am writing up something (quickly) using LaTex and periodically refer to
>> R
>> functions and packages.
>>
>> What is the correct way to put those into LaTex, please?  I know that R
>> itself is {\tt R}, but am not sure about the others.
>>
>> Thanks for any help,
>> Sincerely,
>> Erin
>> PS  Or should I just be doing this in R Studio, even though there is no
>> code, please?
>>
>>
>>
> When I refer to R function in a LaTeX document I tend to use \texttt{}
> (same as your {\tt } construct). Historical convention rendered computer
> code in a monospace font (akin to Courier) so that's what I follow.
>
> --
> Kevin E. Thorpe
> Head of Biostatistics,  Applied Health Research Centre (AHRC)
> Li Ka Shing Knowledge Institute of St. Michael's
> Assistant Professor, Dalla Lana School of Public Health
> University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
>



-- 
Erin Hodgess
Associate Professor
Department of Mathematical and Statistics
University of Houston - Downtown
mailto: erinm.hodgess at gmail.com

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 10 18:30:18 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Dec 2015 09:30:18 -0800
Subject: [R] Errors when compile RInside using intel c++ compiler on
	windows
In-Reply-To: <549bdfb4.1a98b.1518b5b9033.Coremail.081024015@fudan.edu.cn>
References: <549bdfb4.1a98b.1518b5b9033.Coremail.081024015@fudan.edu.cn>
Message-ID: <CAGxFJbS-QE=ot8e1sezEh6X-3n3a_-S8Ce0AkLjc=TvmdyCTBw@mail.gmail.com>

Wrong list!. This is about programming **IN** the R language. For your
sort of question, post to the R-devel list.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 10, 2015 at 2:06 AM, ?? <081024015 at fudan.edu.cn> wrote:
>    I have read the mailing list about compiling R using intel c++ compiler on linux.
> http://lists.r-forge.r-project.org/pipermail/rcpp-devel/2013-January/005035.html
>
>     In my environment I am going to compile RInside using intel c++ compiler on windows.
> I take the sample0 in RInside example folder for test. It's the "Hello world" example.
> After fixing all the compile errors following lingk errors appears:
>
> error LNK2019:unresolved external symblo "public : _thiscall RInside::RInside(int,char.....)"
> error LNK2019:unresolved external symblo "public : _thiscall RInside::~RInside(void)"
> error LNK2019:unresolved external symblo "public : _thiscall RInside::parseEvalQ(class std::basic_string....)"
> .....
>     I created RInside.lib from the RInside.dll(comes from the RInside\libs\i386) using pexports and lib command.
> After adding the RInside.lib into the project->property->linker->input, the errors still exists.
>     Does anyone know how to fix these errors? Or is there anyone who knows how to compile RInside program using intel
> c++ compiler on windows?
>
>                                                                                                   Eric
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From JLucke at ria.buffalo.edu  Thu Dec 10 18:32:09 2015
From: JLucke at ria.buffalo.edu (JLucke at ria.buffalo.edu)
Date: Thu, 10 Dec 2015 12:32:09 -0500
Subject: [R] Correct notation for functions, packages when using LaTex
In-Reply-To: <5669B2FF.8090806@utoronto.ca>
References: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
	<5669B2FF.8090806@utoronto.ca>
Message-ID: <OF2AE006E6.82851D6B-ON85257F17.006009CF-85257F17.00605318@ria.buffalo.edu>

Erin
There is a LaTeX package called  listings. It has an R option to 
prettyprint code.   Never used it. 
Joe
 



"Kevin E. Thorpe" <kevin.thorpe at utoronto.ca> 
Sent by: "R-help" <r-help-bounces at r-project.org>
12/10/2015 12:14 PM

To
Erin Hodgess <erinm.hodgess at gmail.com>, 
cc
R help <r-help at stat.math.ethz.ch>
Subject
Re: [R] Correct notation for functions, packages when using LaTex






On 12/10/2015 12:10 PM, Erin Hodgess wrote:
> Hello everyone!
>
> I am writing up something (quickly) using LaTex and periodically refer 
to R
> functions and packages.
>
> What is the correct way to put those into LaTex, please?  I know that R
> itself is {\tt R}, but am not sure about the others.
>
> Thanks for any help,
> Sincerely,
> Erin
> PS  Or should I just be doing this in R Studio, even though there is no
> code, please?
>
>

When I refer to R function in a LaTeX document I tend to use \texttt{} 
(same as your {\tt } construct). Historical convention rendered computer 
code in a monospace font (akin to Courier) so that's what I follow.

-- 
Kevin E. Thorpe
Head of Biostatistics,  Applied Health Research Centre (AHRC)
Li Ka Shing Knowledge Institute of St. Michael's
Assistant Professor, Dalla Lana School of Public Health
University of Toronto
email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide 
http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Thu Dec 10 18:43:44 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 10 Dec 2015 11:43:44 -0600
Subject: [R] Errors when compile RInside using intel c++ compiler
	on	windows
In-Reply-To: <CAGxFJbS-QE=ot8e1sezEh6X-3n3a_-S8Ce0AkLjc=TvmdyCTBw@mail.gmail.com>
References: <549bdfb4.1a98b.1518b5b9033.Coremail.081024015@fudan.edu.cn>
	<CAGxFJbS-QE=ot8e1sezEh6X-3n3a_-S8Ce0AkLjc=TvmdyCTBw@mail.gmail.com>
Message-ID: <C12E3551-6084-4747-9ADA-107AC2FBCD0D@me.com>

Hi,

For clarification, the rcpp support list is the recommended location for RInside support:

  http://lists.r-forge.r-project.org/mailman/listinfo/rcpp-devel

Regards,

Marc


> On Dec 10, 2015, at 11:30 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> 
> Wrong list!. This is about programming **IN** the R language. For your
> sort of question, post to the R-devel list.
> 
> Cheers,
> Bert
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Thu, Dec 10, 2015 at 2:06 AM, ?? <081024015 at fudan.edu.cn> wrote:
>>   I have read the mailing list about compiling R using intel c++ compiler on linux.
>> http://lists.r-forge.r-project.org/pipermail/rcpp-devel/2013-January/005035.html
>> 
>>    In my environment I am going to compile RInside using intel c++ compiler on windows.
>> I take the sample0 in RInside example folder for test. It's the "Hello world" example.
>> After fixing all the compile errors following lingk errors appears:
>> 
>> error LNK2019:unresolved external symblo "public : _thiscall RInside::RInside(int,char.....)"
>> error LNK2019:unresolved external symblo "public : _thiscall RInside::~RInside(void)"
>> error LNK2019:unresolved external symblo "public : _thiscall RInside::parseEvalQ(class std::basic_string....)"
>> .....
>>    I created RInside.lib from the RInside.dll(comes from the RInside\libs\i386) using pexports and lib command.
>> After adding the RInside.lib into the project->property->linker->input, the errors still exists.
>>    Does anyone know how to fix these errors? Or is there anyone who knows how to compile RInside program using intel
>> c++ compiler on windows?
>> 
>>                                                                                                  Eric
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Thu Dec 10 18:47:07 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Dec 2015 09:47:07 -0800
Subject: [R] Errors when compile RInside using intel c++ compiler on
	windows
In-Reply-To: <C12E3551-6084-4747-9ADA-107AC2FBCD0D@me.com>
References: <549bdfb4.1a98b.1518b5b9033.Coremail.081024015@fudan.edu.cn>
	<CAGxFJbS-QE=ot8e1sezEh6X-3n3a_-S8Ce0AkLjc=TvmdyCTBw@mail.gmail.com>
	<C12E3551-6084-4747-9ADA-107AC2FBCD0D@me.com>
Message-ID: <CAGxFJbQaCaniwAJz3pSV6EyV6g6zOVWC1YQRwoKVSCGUUKWYfA@mail.gmail.com>

Thanks, Marc.

-- Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 10, 2015 at 9:43 AM, Marc Schwartz <marc_schwartz at me.com> wrote:
> Hi,
>
> For clarification, the rcpp support list is the recommended location for RInside support:
>
>   http://lists.r-forge.r-project.org/mailman/listinfo/rcpp-devel
>
> Regards,
>
> Marc
>
>
>> On Dec 10, 2015, at 11:30 AM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
>>
>> Wrong list!. This is about programming **IN** the R language. For your
>> sort of question, post to the R-devel list.
>>
>> Cheers,
>> Bert
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, Dec 10, 2015 at 2:06 AM, ?? <081024015 at fudan.edu.cn> wrote:
>>>   I have read the mailing list about compiling R using intel c++ compiler on linux.
>>> http://lists.r-forge.r-project.org/pipermail/rcpp-devel/2013-January/005035.html
>>>
>>>    In my environment I am going to compile RInside using intel c++ compiler on windows.
>>> I take the sample0 in RInside example folder for test. It's the "Hello world" example.
>>> After fixing all the compile errors following lingk errors appears:
>>>
>>> error LNK2019:unresolved external symblo "public : _thiscall RInside::RInside(int,char.....)"
>>> error LNK2019:unresolved external symblo "public : _thiscall RInside::~RInside(void)"
>>> error LNK2019:unresolved external symblo "public : _thiscall RInside::parseEvalQ(class std::basic_string....)"
>>> .....
>>>    I created RInside.lib from the RInside.dll(comes from the RInside\libs\i386) using pexports and lib command.
>>> After adding the RInside.lib into the project->property->linker->input, the errors still exists.
>>>    Does anyone know how to fix these errors? Or is there anyone who knows how to compile RInside program using intel
>>> c++ compiler on windows?
>>>
>>>                                                                                                  Eric
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>


From cute_loomaa at hotmail.com  Thu Dec 10 18:26:16 2015
From: cute_loomaa at hotmail.com (hms Dreams)
Date: Thu, 10 Dec 2015 20:26:16 +0300
Subject: [R] =?windows-1256?q?problem_in_metro=5Fhasting_function=FE?=
Message-ID: <DUB128-W72DED33CC207B2D26ED41C96E90@phx.gbl>

Hello,
I estimated three paramters using non informative prior(all paramters following uniform distribution)
 
the output is:
Error in optim(pars, li_func, control = list(fnscale = -1), hessian = TRUE,  : 
  non-finite finite-difference value [1]
 
How can I solve it using uniform distribution for all paramters??
 
(the same code is working when I use informative prior When I sugessted other distriutions like gamma and exp.) 
 
Thank you
 		 	   		  
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 10 20:23:56 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Dec 2015 11:23:56 -0800
Subject: [R] =?utf-8?q?problem_in_metro=5Fhasting_function=E2=80=8F?=
In-Reply-To: <DUB128-W72DED33CC207B2D26ED41C96E90@phx.gbl>
References: <DUB128-W72DED33CC207B2D26ED41C96E90@phx.gbl>
Message-ID: <CAGxFJbRKY5LpoPCwq+DVa1RQALJAV_n9ea6TMKx7NdohQ_nVcA@mail.gmail.com>

Heh, heh ...

Uniform distributions are not necessarily "non-informative" priors
(itself, a non-definition). See, e.g.
http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf   .
For a basic argument, see:
http://www.amstat.org/publications/jse/v12n2/zhu.pdf

Further discussion is off-topic here (it's a statistical, not an R,
question). I suggest you consult a local statistician for details.

And your question itself is noninformative: how can one tell without
knowing what you are trying to optimize, your data, your starting
values, etc. (unless I have missed something obvious)?

Finally, if this is homework, post elsewhere: this list has a no
homework policy.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 10, 2015 at 9:26 AM, hms Dreams <cute_loomaa at hotmail.com> wrote:
> Hello,
> I estimated three paramters using non informative prior(all paramters following uniform distribution)
>
> the output is:
> Error in optim(pars, li_func, control = list(fnscale = -1), hessian = TRUE,  :
>   non-finite finite-difference value [1]
>
> How can I solve it using uniform distribution for all paramters??
>
> (the same code is working when I use informative prior When I sugessted other distriutions like gamma and exp.)
>
> Thank you
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From istazahn at gmail.com  Thu Dec 10 20:27:03 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 10 Dec 2015 14:27:03 -0500
Subject: [R] Correct notation for functions, packages when using LaTex
In-Reply-To: <OF2AE006E6.82851D6B-ON85257F17.006009CF-85257F17.00605318@ria.buffalo.edu>
References: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
	<5669B2FF.8090806@utoronto.ca>
	<OF2AE006E6.82851D6B-ON85257F17.006009CF-85257F17.00605318@ria.buffalo.edu>
Message-ID: <CA+vqiLHgiMBF3HZzg5fx9+gyUZK9ZVU2A7H_FrbXcWqk6EzzGQ@mail.gmail.com>

While we're at it: There is another LaTeX package called minted. It
has an R option to highlight R code. I've used it and it works well,
though it depends on python and the pygments python package.

--Ista

On Thu, Dec 10, 2015 at 12:32 PM,  <JLucke at ria.buffalo.edu> wrote:
> Erin
> There is a LaTeX package called  listings. It has an R option to
> prettyprint code.   Never used it.
> Joe
>
>
>
>
> "Kevin E. Thorpe" <kevin.thorpe at utoronto.ca>
> Sent by: "R-help" <r-help-bounces at r-project.org>
> 12/10/2015 12:14 PM
>
> To
> Erin Hodgess <erinm.hodgess at gmail.com>,
> cc
> R help <r-help at stat.math.ethz.ch>
> Subject
> Re: [R] Correct notation for functions, packages when using LaTex
>
>
>
>
>
>
> On 12/10/2015 12:10 PM, Erin Hodgess wrote:
>> Hello everyone!
>>
>> I am writing up something (quickly) using LaTex and periodically refer
> to R
>> functions and packages.
>>
>> What is the correct way to put those into LaTex, please?  I know that R
>> itself is {\tt R}, but am not sure about the others.
>>
>> Thanks for any help,
>> Sincerely,
>> Erin
>> PS  Or should I just be doing this in R Studio, even though there is no
>> code, please?
>>
>>
>
> When I refer to R function in a LaTeX document I tend to use \texttt{}
> (same as your {\tt } construct). Historical convention rendered computer
> code in a monospace font (akin to Courier) so that's what I follow.
>
> --
> Kevin E. Thorpe
> Head of Biostatistics,  Applied Health Research Centre (AHRC)
> Li Ka Shing Knowledge Institute of St. Michael's
> Assistant Professor, Dalla Lana School of Public Health
> University of Toronto
> email: kevin.thorpe at utoronto.ca  Tel: 416.864.5776  Fax: 416.864.3016
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From macqueen1 at llnl.gov  Thu Dec 10 20:46:47 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Thu, 10 Dec 2015 19:46:47 +0000
Subject: [R] Raster-package - problem with stackApply()
In-Reply-To: <CAGBzUO_5+gNkS7vZarcC4ezPrFuwVywh9VXsQHxeUcsqXP0PFA@mail.gmail.com>
References: <CAGBzUO_5+gNkS7vZarcC4ezPrFuwVywh9VXsQHxeUcsqXP0PFA@mail.gmail.com>
Message-ID: <D28F1558.14316A%macqueen1@llnl.gov>

Appears to me that results for the third set of indices you supplied (1,1)
ended up in the third layer of the result. Similarly for the other sets of
indices. This makes sense to me.

r-sig-geo might be a better place to ask questions about the raster
package. I've seen other questions about stackApply there.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 12/10/15, 8:38 AM, "R-help on behalf of Mark R Payne"
<r-help-bounces at r-project.org on behalf of markpayneatwork at gmail.com>
wrote:

>Hi,
>
>I am trying to use stackApply() to perform averages over subsets of a
>brick. However, I am struggling with the indices argument, and how it
>should be interpreted. Here is a simple working example illustrating my
>problem:
>
>r <- raster()
>r[] <- 1
>
>inp <- brick(r,r,r,r,r,r)*(1:6)
>res <- stackApply(inp,c(2,2,3,3,1,1),mean)
>
>Now if we look at the values of each object:
>
>> inp
>names       : layer.1, layer.2, layer.3, layer.4, layer.5, layer.6
>min values  :       1,       2,       3,       4,       5,       6
>max values  :       1,       2,       3,       4,       5,       6
>
>> res
>names       : layer.1, layer.2, layer.3
>min values  :     3.5,     5.5,     1.5
>max values  :     3.5,     5.5,     1.5
>
>Now, the problem is that the names and order of the layers in "res" don't
>line up with the indices that I provided. You can do the maths in your
>head
>- e.g. the first two layers of "inp" have values of 1 and 2, so their mean
>should be 1.5 - however, this is ending up as layer 3 in "res".
>
>So how should the indices argument be interpreted in this context?
>
>Suggestion: A more intuitive format for the "indices" argument in
>stackApply might be as a factor - this way the order is implict and
>stackApply ends up working similar to split() or tapply()...
>
>Best wishes,
>
>Mark
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From pai1981 at gmail.com  Thu Dec 10 21:10:24 2015
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Thu, 10 Dec 2015 13:10:24 -0700
Subject: [R] spplot
Message-ID: <CAM9mbiAu+5+4-E6hktAxJe7CDKhrcKUe8-zVCBt6cxDPD-oZQg@mail.gmail.com>

Hi all,

I am using spplot for a spatial map.

spplot(hspdf, "CDP", col = "white", col.regions = blue2red(20), sp.layout =
list(l2), at = seq(1,10,1), colorkey = list(space = "bottom", labels =
list(labels = paste(seq(1,10,1)), cex = 1.5)), sub = list("CDP", cex = 1.5,
font = 2))

I have three questions related to spplot

[1] How to control the limit of xlim and ylim

     I defined the latitude and longitude limit in l2 but it did not work

[2] How to control color scale

      if change the "at = seq(1,10,1)" and "labels = paste(seq(1,10,1))" to
0-10, I only get color 0-5 in plot
     don't know why?

[3] Panel plot

     I would like to panel spplot with hist. How do I do that?

    spplot(hspdf, "CDP", col = "white", col.regions = blue2red(20),
sp.layout = list(l2), at = seq(1,10,1), colorkey = list(space = "bottom",
    labels = list(labels = paste(seq(1,10,1)), cex = 1.5)), sub =
list("CDP", cex = 1.5, font = 2))

       hist(cdp.obsc, col="grey", border="grey", main="CDP", probability=T)

with regards
-Pai

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 10 22:16:42 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Dec 2015 13:16:42 -0800
Subject: [R] =?utf-8?q?problem_in_metro=5Fhasting_function=E2=80=8F?=
In-Reply-To: <DUB128-W5061E1D2A1630927BD344F96E90@phx.gbl>
References: <DUB128-W72DED33CC207B2D26ED41C96E90@phx.gbl>
	<CAGxFJbRKY5LpoPCwq+DVa1RQALJAV_n9ea6TMKx7NdohQ_nVcA@mail.gmail.com>
	<DUB128-W5061E1D2A1630927BD344F96E90@phx.gbl>
Message-ID: <CAGxFJbRW8eg6TmfH0x6ny89SJ14cvC0k-mo37RjA+amJ8ThemQ@mail.gmail.com>

Sara:

Always cc your reply to the list, which I have done here.

No, I cannot help you. Others may be able to now. (They may still need
your data, however).

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 10, 2015 at 12:35 PM, hms Dreams <cute_loomaa at hotmail.com> wrote:
> No, it is not a homework..
>
> The 3 paramters I want to estimate it are : alpha,gam and delta, the range
> of them >0
>
> here my code:
>
>
> library("MHadaptive")
>
>
> baysianlog5=function (param,data)
>
>
>          {  alpha=param[1]
>
>
>           gam=param[2]
>
>
>          delta=param[3]
>
>
>           x=data
>
>            n =length(x)
>
>
>
> logl=n*log(alpha)+n*log(gam)+n*log(1/delta)+(alpha-1)*sum(log(x))-(gam+1)*sum(log(1+(1/delta)*x^alpha))
>
>
>         p=prior5(param)
>
>
>         return(logl+p) #return log(p(x|theta)p(theta))
>
>
> }
>
>
> #non informative prior using uniform
>
>
> prior5=function(param)
>
>
> {   alpha=param[1]
>
>
>           gam=param[2]
>
>
>          delta=param[3]
>
>
>
> prior_alpha=dunif(alpha,0, 1,log=TRUE)
>
>
>  prior_gam=dunif(gam,0,.5,log=TRUE)
>
>
> prior_delta=dunif(delta,0,.8,log=TRUE)
>
>
> return(prior_alpha+ prior_gam +prior_delta)
>
>
> }
>
>
> alphaB5=c();    gamB5=c();deltaB5=c()
>
>
> n=5 ; m=5
>
>
> alpha=2;gam=3;delta=4 #initial values
>
>
> v= runif(n,0,1)
>
>
>   x =delta^(1/alpha)*((1-v)^(-1/gam)-1)^(1/alpha)# quantile
>
>
> mc5 =Metro_Hastings(li_func=baysianlog5,
> pars=c(.8,.2,.2),par_names=c('alpha','gamma','delta'),data=x )
>
>
>
>
>
>
> #the output is
> Error in optim(pars, li_func, control = list(fnscale = -1), hessian = TRUE,
> :
>   non-finite finite-difference value [1]
>
>
> Can you help me
> Sara
>
>> Date: Thu, 10 Dec 2015 11:23:56 -0800
>> Subject: Re: [R] problem in metro_hasting function?
>> From: bgunter.4567 at gmail.com
>> To: cute_loomaa at hotmail.com
>> CC: r-help at r-project.org
>>
>> Heh, heh ...
>>
>> Uniform distributions are not necessarily "non-informative" priors
>> (itself, a non-definition). See, e.g.
>> http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf .
>> For a basic argument, see:
>> http://www.amstat.org/publications/jse/v12n2/zhu.pdf
>>
>> Further discussion is off-topic here (it's a statistical, not an R,
>> question). I suggest you consult a local statistician for details.
>>
>> And your question itself is noninformative: how can one tell without
>> knowing what you are trying to optimize, your data, your starting
>> values, etc. (unless I have missed something obvious)?
>>
>> Finally, if this is homework, post elsewhere: this list has a no
>> homework policy.
>>
>> Cheers,
>> Bert
>>
>>
>> Bert Gunter
>>
>> "The trouble with having an open mind is that people keep coming along
>> and sticking things into it."
>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>
>>
>> On Thu, Dec 10, 2015 at 9:26 AM, hms Dreams <cute_loomaa at hotmail.com>
>> wrote:
>> > Hello,
>> > I estimated three paramters using non informative prior(all paramters
>> > following uniform distribution)
>> >
>> > the output is:
>> > Error in optim(pars, li_func, control = list(fnscale = -1), hessian =
>> > TRUE, :
>> > non-finite finite-difference value [1]
>> >
>> > How can I solve it using uniform distribution for all paramters??
>> >
>> > (the same code is working when I use informative prior When I sugessted
>> > other distriutions like gamma and exp.)
>> >
>> > Thank you
>> >
>> > [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Thu Dec 10 22:26:05 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 10 Dec 2015 13:26:05 -0800
Subject: [R] =?utf-8?q?problem_in_metro=5Fhasting_function=E2=80=8F?=
In-Reply-To: <CAGxFJbRW8eg6TmfH0x6ny89SJ14cvC0k-mo37RjA+amJ8ThemQ@mail.gmail.com>
References: <DUB128-W72DED33CC207B2D26ED41C96E90@phx.gbl>
	<CAGxFJbRKY5LpoPCwq+DVa1RQALJAV_n9ea6TMKx7NdohQ_nVcA@mail.gmail.com>
	<DUB128-W5061E1D2A1630927BD344F96E90@phx.gbl>
	<CAGxFJbRW8eg6TmfH0x6ny89SJ14cvC0k-mo37RjA+amJ8ThemQ@mail.gmail.com>
Message-ID: <CAGxFJbQUsB-HFPmwCgazXwyXjPU9EGKMVQmxh1u5tCgjCW-MUA@mail.gmail.com>

...

Note that if your parameters are uniform on [0, ...] you might be
getting into trouble with (1/param) in your priors and likelihood.

Or you might just be over-parameterized -- your data can't support
your model and you are getting near singularities in your
approximations.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 10, 2015 at 1:16 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> Sara:
>
> Always cc your reply to the list, which I have done here.
>
> No, I cannot help you. Others may be able to now. (They may still need
> your data, however).
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Thu, Dec 10, 2015 at 12:35 PM, hms Dreams <cute_loomaa at hotmail.com> wrote:
>> No, it is not a homework..
>>
>> The 3 paramters I want to estimate it are : alpha,gam and delta, the range
>> of them >0
>>
>> here my code:
>>
>>
>> library("MHadaptive")
>>
>>
>> baysianlog5=function (param,data)
>>
>>
>>          {  alpha=param[1]
>>
>>
>>           gam=param[2]
>>
>>
>>          delta=param[3]
>>
>>
>>           x=data
>>
>>            n =length(x)
>>
>>
>>
>> logl=n*log(alpha)+n*log(gam)+n*log(1/delta)+(alpha-1)*sum(log(x))-(gam+1)*sum(log(1+(1/delta)*x^alpha))
>>
>>
>>         p=prior5(param)
>>
>>
>>         return(logl+p) #return log(p(x|theta)p(theta))
>>
>>
>> }
>>
>>
>> #non informative prior using uniform
>>
>>
>> prior5=function(param)
>>
>>
>> {   alpha=param[1]
>>
>>
>>           gam=param[2]
>>
>>
>>          delta=param[3]
>>
>>
>>
>> prior_alpha=dunif(alpha,0, 1,log=TRUE)
>>
>>
>>  prior_gam=dunif(gam,0,.5,log=TRUE)
>>
>>
>> prior_delta=dunif(delta,0,.8,log=TRUE)
>>
>>
>> return(prior_alpha+ prior_gam +prior_delta)
>>
>>
>> }
>>
>>
>> alphaB5=c();    gamB5=c();deltaB5=c()
>>
>>
>> n=5 ; m=5
>>
>>
>> alpha=2;gam=3;delta=4 #initial values
>>
>>
>> v= runif(n,0,1)
>>
>>
>>   x =delta^(1/alpha)*((1-v)^(-1/gam)-1)^(1/alpha)# quantile
>>
>>
>> mc5 =Metro_Hastings(li_func=baysianlog5,
>> pars=c(.8,.2,.2),par_names=c('alpha','gamma','delta'),data=x )
>>
>>
>>
>>
>>
>>
>> #the output is
>> Error in optim(pars, li_func, control = list(fnscale = -1), hessian = TRUE,
>> :
>>   non-finite finite-difference value [1]
>>
>>
>> Can you help me
>> Sara
>>
>>> Date: Thu, 10 Dec 2015 11:23:56 -0800
>>> Subject: Re: [R] problem in metro_hasting function?
>>> From: bgunter.4567 at gmail.com
>>> To: cute_loomaa at hotmail.com
>>> CC: r-help at r-project.org
>>>
>>> Heh, heh ...
>>>
>>> Uniform distributions are not necessarily "non-informative" priors
>>> (itself, a non-definition). See, e.g.
>>> http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf .
>>> For a basic argument, see:
>>> http://www.amstat.org/publications/jse/v12n2/zhu.pdf
>>>
>>> Further discussion is off-topic here (it's a statistical, not an R,
>>> question). I suggest you consult a local statistician for details.
>>>
>>> And your question itself is noninformative: how can one tell without
>>> knowing what you are trying to optimize, your data, your starting
>>> values, etc. (unless I have missed something obvious)?
>>>
>>> Finally, if this is homework, post elsewhere: this list has a no
>>> homework policy.
>>>
>>> Cheers,
>>> Bert
>>>
>>>
>>> Bert Gunter
>>>
>>> "The trouble with having an open mind is that people keep coming along
>>> and sticking things into it."
>>> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>>>
>>>
>>> On Thu, Dec 10, 2015 at 9:26 AM, hms Dreams <cute_loomaa at hotmail.com>
>>> wrote:
>>> > Hello,
>>> > I estimated three paramters using non informative prior(all paramters
>>> > following uniform distribution)
>>> >
>>> > the output is:
>>> > Error in optim(pars, li_func, control = list(fnscale = -1), hessian =
>>> > TRUE, :
>>> > non-finite finite-difference value [1]
>>> >
>>> > How can I solve it using uniform distribution for all paramters??
>>> >
>>> > (the same code is working when I use informative prior When I sugessted
>>> > other distriutions like gamma and exp.)
>>> >
>>> > Thank you
>>> >
>>> > [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> > https://stat.ethz.ch/mailman/listinfo/r-help
>>> > PLEASE do read the posting guide
>>> > http://www.R-project.org/posting-guide.html
>>> > and provide commented, minimal, self-contained, reproducible code.


From btyner at gmail.com  Fri Dec 11 01:22:13 2015
From: btyner at gmail.com (Benjamin Tyner)
Date: Thu, 10 Dec 2015 19:22:13 -0500
Subject: [R] choropleth packages (US)
In-Reply-To: <CACa4aQ6=SXpsf6QbBQ9pLGd=h_FHiVhGFVQ+_c4Z-p17v4dREQ@mail.gmail.com>
References: <56649494.3050206@gmail.com>
	<B4FD6FEF-D170-45B4-B51D-CB8D8070020B@univie.ac.at>
	<CACa4aQ6=SXpsf6QbBQ9pLGd=h_FHiVhGFVQ+_c4Z-p17v4dREQ@mail.gmail.com>
Message-ID: <566A1735.5080206@gmail.com>

Very nice Adrian. Is there a straightforward way to add Alaska and 
Hawaii at the lower left? (without resorting to choroplethr package)

On 12/10/2015 06:09 AM, Adrian Waddell wrote:
> Hi,
>
> You can also use the 'maps' package for the map data and the 'scales'
> package for the color mapping.
>
> E.g.
>
> library(maps)
> library(scales)
>
> m <- map('state', fill=TRUE, plot=FALSE)
>
> s_data <- tolower(rownames(USArrests))
> s_map <- tolower(m$names)
>
> mapping <- lapply(s_data, function(state) {
>    which(grepl(state, s_map))
> })
> ## check if the mapping is good!
>
> col_pal <- col_numeric("Greens", domain=NULL, na.color = 'lightyellow')
>
> cols <- rep('lightyellow', length(s_data))
>
> Map(function(indices, col) {
>    cols[indices] <<- col
> }, mapping, col_pal(USArrests$UrbanPop))
>
> map(m, col=cols, fill=TRUE)
>
>
> Adrian
>
>
>
> On Mon, Dec 7, 2015 at 9:34 AM, Erich Neuwirth
> <erich.neuwirth at univie.ac.at> wrote:
>> ggplot2 also can do this with
>> fortify
>> geom_polygon
>>
>> Von meinem iPad gesendet
>>
>>> Am 06.12.2015 um 21:03 schrieb Benjamin Tyner <btyner at gmail.com>:
>>>
>>> Hi
>>>
>>> I wish to draw a basic choropleth (US, by state) and am wondering if anyone has any recommendations? I've tried the following thus far:
>>>
>>> 1. choroplethr: this works, but required installation of 30+ dependencies. I would prefer something with fewer dependencies.
>>> 2. tmap: this also seems promising, but most of the examples I saw were specific to European maps. Can it be adapted for US?
>>> 3. statebins: doesn't draw true choropleths, but I liked that it doesn't have many dependencies.
>>>
>>> Regards
>>> Ben
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Fri Dec 11 01:36:42 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 11 Dec 2015 13:36:42 +1300
Subject: [R] [FORGED]  Correct notation for functions,
 packages when using LaTex
In-Reply-To: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
References: <CACxE24nyBmaSqdsLF113Lr0TUWwzx8Z2GFAZZ7LphduqHRtT8w@mail.gmail.com>
Message-ID: <566A1A9A.3080803@auckland.ac.nz>

On 11/12/15 06:10, Erin Hodgess wrote:
> Hello everyone!
>
> I am writing up something (quickly) using LaTex and periodically refer to R
> functions and packages.
>
> What is the correct way to put those into LaTex, please?  I know that R
> itself is {\tt R}, but am not sure about the others.

Actually I much prefer \textsf{R} --- and insist on this in papers 
submitted to ANZJS. The Journal of Statistical Software uses a macro
\proglang which renders R in sans serif font --- so this *must* be the 
correct font.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From cute_loomaa at hotmail.com  Thu Dec 10 22:22:22 2015
From: cute_loomaa at hotmail.com (hms Dreams)
Date: Fri, 11 Dec 2015 00:22:22 +0300
Subject: [R] =?windows-1256?q?problem_in_metro=5Fhasting_function=FE?=
In-Reply-To: <DUB128-W5061E1D2A1630927BD344F96E90@phx.gbl>
References: <DUB128-W72DED33CC207B2D26ED41C96E90@phx.gbl>,
	<CAGxFJbRKY5LpoPCwq+DVa1RQALJAV_n9ea6TMKx7NdohQ_nVcA@mail.gmail.com>,
	<DUB128-W5061E1D2A1630927BD344F96E90@phx.gbl>
Message-ID: <DUB128-W192B44452706FE9B273EEE96E90@phx.gbl>






No, it is not a homework..
 
The 3 paramters I want to estimate it are : alpha,gam and delta, the range of them >0
 
here my code:
 



library("MHadaptive")


baysianlog5=function
(param,data)


         { 
alpha=param[1]


         
gam=param[2]


        
delta=param[3]


         
x=data           n =length(x)


         
logl=n*log(alpha)+n*log(gam)+n*log(1/delta)+(alpha-1)*sum(log(x))-(gam+1)*sum(log(1+(1/delta)*x^alpha))


       
p=prior5(param)


       
return(logl+p) #return log(p(x|theta)p(theta))


}


#non
informative prior using uniform


prior5=function(param)


{   alpha=param[1]


          gam=param[2]


         delta=param[3]




prior_alpha=dunif(alpha,0, 1,log=TRUE)


 prior_gam=dunif(gam,0,.5,log=TRUE)  


prior_delta=dunif(delta,0,.8,log=TRUE)  


return(prior_alpha+
prior_gam +prior_delta) 


}


alphaB5=c();    gamB5=c();deltaB5=c()


n=5 ; m=5


alpha=2;gam=3;delta=4 #initial values 


v= runif(n,0,1)


 
x =delta^(1/alpha)*((1-v)^(-1/gam)-1)^(1/alpha)# quantile


mc5 =Metro_Hastings(li_func=baysianlog5, pars=c(.8,.2,.2),par_names=c('alpha','gamma','delta'),data=x )





  
#the output is
Error in optim(pars, li_func, control = list(fnscale = -1), hessian = TRUE,  : 
  non-finite finite-difference value [1]

 
Can you help me
Sara

> Date: Thu, 10 Dec 2015 11:23:56 -0800
> Subject: Re: [R] problem in metro_hasting function?
> From: bgunter.4567 at gmail.com
> To: cute_loomaa at hotmail.com
> CC: r-help at r-project.org
> 
> Heh, heh ...
> 
> Uniform distributions are not necessarily "non-informative" priors
> (itself, a non-definition). See, e.g.
> http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf   .
> For a basic argument, see:
> http://www.amstat.org/publications/jse/v12n2/zhu.pdf
> 
> Further discussion is off-topic here (it's a statistical, not an R,
> question). I suggest you consult a local statistician for details.
> 
> And your question itself is noninformative: how can one tell without
> knowing what you are trying to optimize, your data, your starting
> values, etc. (unless I have missed something obvious)?
> 
> Finally, if this is homework, post elsewhere: this list has a no
> homework policy.
> 
> Cheers,
> Bert
> 
> 
> Bert Gunter
> 
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
> 
> 
> On Thu, Dec 10, 2015 at 9:26 AM, hms Dreams <cute_loomaa at hotmail.com> wrote:
> > Hello,
> > I estimated three paramters using non informative prior(all paramters following uniform distribution)
> >
> > the output is:
> > Error in optim(pars, li_func, control = list(fnscale = -1), hessian = TRUE,  :
> >   non-finite finite-difference value [1]
> >
> > How can I solve it using uniform distribution for all paramters??
> >
> > (the same code is working when I use informative prior When I sugessted other distriutions like gamma and exp.)
> >
> > Thank you
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
 		 	   		   		 	   		  
	[[alternative HTML version deleted]]


From Erik.Schiele at bnymellon.com  Thu Dec 10 23:30:09 2015
From: Erik.Schiele at bnymellon.com (Schiele, Erik)
Date: Thu, 10 Dec 2015 22:30:09 +0000
Subject: [R] Trading Strategy and Bootstrap
Message-ID: <1464CFE83B5ABB4DB9D6E0A62E9974D4010E5428BA@WTPCPMBMEM18.ams.bnymellon.net>

Hi,

I'm beginning to fool around in R for trading strategy purposes. To keep it simple, I have only played with stock data at this point.

I have created a simple trend following strategy (in blue). Given my statistical background, I am attempting to bootstrap the results and table the same parameters highlighted below with no luck (in green). Any ideas on what I could do differently?

Really Appreciate your help!!! Thanks

library(quantmod)
library(PerformanceAnalytics)

b <- get(getSymbols('SPY'))["2011::"]
s <- get(getSymbols('GLD'))["2011::"]
b$sma1 <- SMA(Cl(s) , 1)
s$sma50 <- SMA(Cl(s) , 50)
s$position <- ifelse(Cl(s) > s$sma50 , 1 , -1)
myReturn <- lag(s$position) * dailyReturn(s)

table.Drawdowns(s$position, top = 5, digits = 1)
table.Stats(s$position, ci = 0.95, digits = 2)
table.SpecificRisk(s$position, b$sma1, Rf = 0, digits = 2)
table.Correlation(s$position, b$sma1)

charts.PerformanceSummary(cbind(dailyReturn(s),myReturn))

N     = 100 # Number of simulations
Loop  = mat.or.vec(N,2,1,1,1)
for (i in 1:N){

  # sample with replacement from return distribution of index
  s.new = (sample(s, length(s), replace = T, prob = NULL))
  # demeaning returns
  s.new = s.new-mean(s)
  # new price series starting at same value as original series
  prices.new = xts(prices[[1]]*exp(cumsum(s.new)))

  # define strategies
  # mean reversion
  s$sma50.new  = SMA(Cl(s.new) , 50)

   # Create buy/sell signals
   # mean reversion
   s$position.new <- ifelse(Cl(s) > s$sma50.new , 1 , -1)

   # replace missing values with zeros
   s$position.new[is.na(s$position.new)]   = 0

   Loop[i,1] = if (mean(s$position.new)  > mean(s$sma50.new)) {1}else{0}
}

#Loop

# plots simulated series
returns.new = cbind(s$sma50.new, cumsum(s$sma50.new))

 chart.CumReturns(returns.new,s$sma50.new,geometric=F)

Erik Schiele
Vice President
Money Markets Trading, Originations and Sales
101 Barclay St, NY NY 10007 3rd Floor
BNY Mellon Capital Markets, LLC
Main Desk 212-815-8222

This is for informational purposes only; from sources the Firm believes reliable; may not be accurate or complete; is subject to change;  is not a recommendation or offer to buy/sell a financial instrument or adopt any investment strategy; is not legal, tax, credit or accounting advice.  Do not use e-mail to submit any instructions - acceptances are at your risk. The Firm or its affiliates lends to, borrows from and provides other products/services to issuers and others, receives compensation therefore, and periodically has a direct or indirect financial interest in the financial instruments/transactions indicated.  Additional risks may exist that are not referenced. Past performance is not indicative of future returns. Other than CDs or CDARS, financial instruments: are not FDIC insured; are not deposits or other obligations of and are not guaranteed by the Firm or any bank or non-bank affiliate; and involve investment risk including possible loss of principal. The Firm is a wholly owned, indirect non-bank subsidiary of The Bank of New York Mellon Corporation, and a member of FINRA and SIPC, and is solely responsible for its obligations and commitments.


The information contained in this e-mail, and any attachment, is confidential and is intended solely for the use of the intended recipient. Access, copying or re-use of the e-mail or any attachment, or any information contained therein, by any other person is not authorized. If you are not the intended recipient please return the e-mail to the sender and delete it from your computer. Although we attempt to sweep e-mail and attachments for viruses, we do not guarantee that either are virus-free and accept no liability for any damage sustained as a result of viruses. 

Please refer to http://disclaimer.bnymellon.com/eu.htm for certain disclosures relating to European legal entities.
	[[alternative HTML version deleted]]


From pmalpartidajr at gmail.com  Thu Dec 10 23:55:17 2015
From: pmalpartidajr at gmail.com (Pedro Malpartida)
Date: Thu, 10 Dec 2015 16:55:17 -0600
Subject: [R] Request Help
Message-ID: <CAGBx1f9gB5Xz3nM0XUjuMudXCQMnAQ0E-s=d0BK0ynYR+z+dLQ@mail.gmail.com>

Hello,

I am currently facing difficulties while doing a couple of financial tasks
in R. I will appreciate if you give a solution code for these issues.

1) If I have to price an european call option non dividend paying stock
being t= 12 months, rf= 3% pa.cc and volatility of 30% per annum.

Plot the gamma of the call option by assuming that

the strike price =100 and it varies from $0 to $200 with a step size of
0.01. Label the x-axis, y-axis as Gamma and the title as Gamma options.


Regards,

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Dec 11 03:20:11 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 10 Dec 2015 18:20:11 -0800
Subject: [R] Request Help
In-Reply-To: <CAGBx1f9gB5Xz3nM0XUjuMudXCQMnAQ0E-s=d0BK0ynYR+z+dLQ@mail.gmail.com>
References: <CAGBx1f9gB5Xz3nM0XUjuMudXCQMnAQ0E-s=d0BK0ynYR+z+dLQ@mail.gmail.com>
Message-ID: <7BD7DF2C-A4E5-4BE6-80F4-399C38AAE4AB@comcast.net>


> On Dec 10, 2015, at 2:55 PM, Pedro Malpartida <pmalpartidajr at gmail.com> wrote:
> 
> Hello,
> 
> I am currently facing difficulties while doing a couple of financial tasks
> in R. I will appreciate if you give a solution code for these issues.
> 
> 1) If I have to price an european call option non dividend paying stock
> being t= 12 months, rf= 3% pa.cc and volatility of 30% per annum.
> 
> Plot the gamma of the call option by assuming that
> 
> the strike price =100 and it varies from $0 to $200 with a step size of
> 0.01. Label the x-axis, y-axis as Gamma and the title as Gamma options.
> 
> 
> Regards,
> 
> 	[[alternative HTML version deleted]]
> 

A) This is a plain text mailing list. 
B) There might be R SIG mailing list where respondents might be willing to construct examples for you to answer what appears to be a business school homework question, but it seems fairly unlikely that the main R mailing list will be a good choice for such a request.

Please _do_ read both the following items (noting that you _were_ already asked to do so):

> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html

And this is definitely expected:

> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From hannah.hlx at gmail.com  Fri Dec 11 04:11:21 2015
From: hannah.hlx at gmail.com (li li)
Date: Thu, 10 Dec 2015 22:11:21 -0500
Subject: [R] Restricted 4-PL curves using drc package
In-Reply-To: <CAGxFJbTSetUGnaLiG18UKU_H3xtk_G1E=mJNzP52uO60sY0fvg@mail.gmail.com>
References: <CAHLnndb92_oqQbcwxkDDTfJ4CXRZhNJMaVUM0CpZtDBY4qHbNw@mail.gmail.com>
	<CAGxFJbTSetUGnaLiG18UKU_H3xtk_G1E=mJNzP52uO60sY0fvg@mail.gmail.com>
Message-ID: <CAHLnndY_UXkek_XV0rF9dzOoTC2wi0FCt-P5BOwU4fvoLD_EyQ@mail.gmail.com>

The option 2 works. Thanks.
  Hanna

2015-12-09 16:24 GMT-05:00 Bert Gunter <bgunter.4567 at gmail.com>:

> 1. Don't know, but, assuming I understand you correctly, probably not.
>
> 2 Note (subject to my understanding again) that you can rephrase your
> query as fitting a 4 parameter logistic to all the data together with
> a grouping factor to indicate the separate curves, allowing two of the
> parameters (e.g. ED50 and slope) to vary by group. You can then use
> standard nonlinear fitting (e.g. via the optimx package) to fit the
> model. Note that you will have to specify 2c +2 starting values for c
> curves. With c "large" (?whatever that means), this may make
> convergence tricky.
>
> If you care to reply, please do so to the list, not to me.
>
> Cheers,
> Bert
>
>
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Wed, Dec 9, 2015 at 12:27 PM, li li <hannah.hlx at gmail.com> wrote:
> > Hi all,
> >   In drc package, is there a function which can be used to fit restricted
> > 4PL curves? For example, we restrict two 4PL curves have the same lower
> and
> > upper asymptotes?
> >   Thanks for the help in advance!
> >     Hanna
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Ramgad82 at gmx.net  Fri Dec 11 12:08:31 2015
From: Ramgad82 at gmx.net (Dagmar)
Date: Fri, 11 Dec 2015 12:08:31 +0100
Subject: [R] expand dataframe but time gap is not the same
In-Reply-To: <CAF8bMcZjxobGuC3_xj_b+5CqTFCGrU_kFwr4CX+byMmcnfTRfA@mail.gmail.com>
References: <56601ECC.9040001@gmx.net>
	<CAF8bMcZjxobGuC3_xj_b+5CqTFCGrU_kFwr4CX+byMmcnfTRfA@mail.gmail.com>
Message-ID: <566AAEAF.4030304@gmx.net>

Hello Bill (and Petr and all),

Thank you very much! That was exactly what I was looking for! I could 
have never accomplished that on my own.

Have a great time,
Tagmarie

Am 09.12.2015 um 18:07 schrieb William Dunlap:
> You can use the approx() function (in that stats package) to put
> 5 equally spaced times between your high and low water times.
> E.g., in the following 'tmp' will be your interpolated times, in seconds
> since 1970, which I clumsily convert to POSIX times (I can never
> remember how to deal with time zones so I make everything use UTC).
>
>> tmp <- with(myframe2, approx(x=seq(1, by=6, length.out=length(Timestamp)), y=as.numeric(Timestamp), xout=seq(1,by=1,length.out=6*(length(Timestamp)-1)+1))$y)
>> as.POSIXct(i, origin="1970-01-01 00:00", tz="UTC")
>   [1] "2013-08-02 22:10:00 UTC" "2013-08-02 23:09:00 UTC"
>   [3] "2013-08-03 00:08:00 UTC" "2013-08-03 01:07:00 UTC"
>   [5] "2013-08-03 02:06:00 UTC" "2013-08-03 03:05:00 UTC"
>   [7] "2013-08-03 04:04:00 UTC" "2013-08-03 05:06:20 UTC"
>   [9] "2013-08-03 06:08:40 UTC" "2013-08-03 07:11:00 UTC"
> [11] "2013-08-03 08:13:20 UTC" "2013-08-03 09:15:40 UTC"
> [13] "2013-08-03 10:18:00 UTC"
>> diff(.Last.value)
> Time differences in mins
>   [1] 59.00000 59.00000 59.00000 59.00000 59.00000 59.00000 62.33333 62.33333
>   [9] 62.33333 62.33333 62.33333 62.33333
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Thu, Dec 3, 2015 at 2:51 AM, Dagmar <Ramgad82 at gmx.net> wrote:
>> Hello,
>> I hope someone can help me with my problem:
>> I have a dataframe like this:
>>
>> datframe <- data.frame(dates=c("02.08.2013","03.08.2013","03.08.2013"),
>> times =c("22:10","4:04", "10:18"), tide =c("NW","HW", "NW"))
>> datframe
>> Timestamp <- paste(datframe$dates, datframe$times)
>> myframe2 <- cbind( Timestamp,datframe)
>> head(myframe2)
>> myframe2$dates <- NULL
>> myframe2$times <- NULL
>> myframe2$Timestamp <- as.POSIXct (strptime(myframe2$Timestamp, "%d.%m.%Y
>> %H:%M"), tz="GMT")
>> head(myframe2)
>> str(myframe2)
>>
>> # In the end I want a frame like this:
>> datframeres <- data.frame(Timestamp=c("2013-08-02 22:10:00", "2013-08-02
>> 23:09:00","2013-08-03 00:08:00","2013-08-03 01:07:00", "2013-08-03
>> 02:06:00", "2013-08-03 03:05:00",
>> "2013-08-03 04:04:00","2013-08-03 05:06:20","2013-08-03 06:08:40",
>> "2013-08-03 07:11:00", "2013-08-03 08:13:20", "2013-08-03 09:15:40",
>> "2013-08-03 10:18:00")
>> , tidalclass =c("NW", "HW-5","HW-4", "HW-3", "HW-2", "HW-1", "HW",
>> "HW+1","HW+2","HW+3", "HW+4","HW+5", "NW"))
>>
>> datframeres
>>
>> # That means: I want to expand to 13 classes instead of the two classes "HW"
>> and "NW": "HW", "HW+1", "HW+2" and so on.
>> # The time gap between HW and NW is not always quite the same (always around
>> 6 hours). So I would divide the time gap by 6 and add this number (0:59 and
>> 1:02:20 respectively) to the timestamp before.
>> # I do not know how to do this. Does anyone know how to do this? Many thanks
>> in advance!
>> Tagmarie
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From aoife.m.doherty at gmail.com  Fri Dec 11 12:32:29 2015
From: aoife.m.doherty at gmail.com (aoife doherty)
Date: Fri, 11 Dec 2015 11:32:29 +0000
Subject: [R] egscore - lambda will not go below 1.1
Message-ID: <CAAsJanbUE_wErDFC4Yhm7sgGGDRxu1sFpu0787+4y1R-_-Kcgw@mail.gmail.com>

Hi all,

I want to associate mortality with ~100K SNPs, in 6,500 samples that are
divided up into 60 breeds.

So it's important to account for population stratification in my analysis.

I'm using egscore (the eigenstrat method) for the association (and I've
tried using the polygen and grammar packages too). The problem is that I
cannot get lambda (the inflation factor) to go below the accepted inflation
threshold of 1.1, it seems to converge around 1.5-1.6 when I include an
increasing number of PC axes into the analysis.

Then, I also tried to use the polygen and grammar packages, but the same
thing happens.

Here is the code that I am using:


library(GenABEL)
#Load the data as a gwaa.class
my.geno.data <- load.gwaa.data(phenofile =
"pheno.dat",genofile="youroutput.dat")

#Calculate the IBS matrix
kin<-ibs(my.geno.data, weight="freq", snpfreq=NULL)
diag(kin) <-hom(my.geno.data)$Var

# Estimation of polygenic model, This estimates the residuals when the
effect of covariates breeds are factored out.
polygen <-polygenic(mortality~breed,kin=kin,my.geno.data)
grammarobject <-qtscore(polygen$pgres,data=my.geno.data,clam=FALSE)

....so then you found that the lambda was still > 1.1

#So then I used egscore on the output from polygen
output <-egscore(polygen$pgresidualY,data=my.geno.data,kin=kin,naxes=X)

....where I iteratively included an increasing number of PC axes (naxes=X),
lambda still > 1.1, and doesn't change if I run egscore on the raw data
instead of the environmental residuals (again iterating through axes), and
it also doesn't change regardless of if I include breed as a co-variate, or
as a stratification variable.

output <-egscore(mortality~breed,data=my.geno.data,kin=kin,naxes=X)

Am I doing something wrong? What else can I try to properly account for
population stratification in an association between trait and SNPs?

Thanks

	[[alternative HTML version deleted]]


From pai1981 at gmail.com  Fri Dec 11 07:19:13 2015
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Thu, 10 Dec 2015 23:19:13 -0700
Subject: [R] problem with spplot
Message-ID: <CAM9mbiDL5G52-WQcXHUjpLpmQQVGKe3t=G9raZ4z_9a=bYh2qg@mail.gmail.com>

Hi all,

I am using spplot for a spatial map.

spplot(hspdf, "CDP", col = "white", col.regions = blue2red(20), sp.layout =
list(l2), at = seq(1,10,1), colorkey = list(space = "bottom", labels =
list(labels = paste(seq(1,10,1)), cex = 1.5)), sub = list("CDP", cex = 1.5,
font = 2))

I have three questions related to spplot

[1] How to control the limit of xlim and ylim

     I defined the latitude and longitude limit in l2 but it did not work

[2] How to control color scale

      if change the "at = seq(1,10,1)" and "labels = paste(seq(1,10,1))" to
0-10, I only get color 0-5 in plot
     don't know why?

[3] Panel plot

     I would like to panel spplot with hist. How do I do that?

    spplot(hspdf, "CDP", col = "white", col.regions = blue2red(20),
sp.layout = list(l2), at = seq(1,10,1), colorkey = list(space = "bottom",
    labels = list(labels = paste(seq(1,10,1)), cex = 1.5)), sub =
list("CDP", cex = 1.5, font = 2))

       hist(cdp.obsc, col="grey", border="grey", main="CDP", probability=T)

with regards

	[[alternative HTML version deleted]]


From mpay at aqua.dtu.dk  Fri Dec 11 09:02:22 2015
From: mpay at aqua.dtu.dk (Mark R. Payne)
Date: Fri, 11 Dec 2015 09:02:22 +0100
Subject: [R] Raster-package - problem with stackApply()
In-Reply-To: <D28F1558.14316A%macqueen1@llnl.gov>
References: <CAGBzUO_5+gNkS7vZarcC4ezPrFuwVywh9VXsQHxeUcsqXP0PFA@mail.gmail.com>
	<D28F1558.14316A%macqueen1@llnl.gov>
Message-ID: <566A830E.8060104@aqua.dtu.dk>

Thanks for the reply Don - that's the problem in a nutshell - I'll 
repost on r-sig-geo....

Mark

On 10/12/15 20:46, MacQueen, Don wrote:
> Appears to me that results for the third set of indices you supplied (1,1)
> ended up in the third layer of the result. Similarly for the other sets of
> indices. This makes sense to me.
>
> r-sig-geo might be a better place to ask questions about the raster
> package. I've seen other questions about stackApply there.
>
> -Don
>

-- 
Mark Payne
Senior Researcher
Centre for Ocean Life
DTU Aqua
---------------------------------------
Technical University of Denmark
National Institute of Aquatic Resources
Charlottenlund Slot,
J?gersborg All? 1
2920 Charlottenlund
Direct +45 35883422
mpay at aqua.dtu.dk
http://www.aqua.dtu.dk/mpay
@MarkPayneAtWork
Skype: MarkPayneAtWork


From diego.pavonjordan at gmail.com  Fri Dec 11 10:25:38 2015
From: diego.pavonjordan at gmail.com (Diego Pavon)
Date: Fri, 11 Dec 2015 11:25:38 +0200
Subject: [R] Error message when running gam (mgcv). object not found
Message-ID: <CAD93_Fr6-2yd0LN1=2O-EGexdCx40iF99YyqiyqbjzaCbw23Yw@mail.gmail.com>

Dear all

I am trying to fit gam to my data but it keeps giving me errors.


It does not find the covariate ArrivalTime even though it is clearly
defined in the data frame. It happens with any covariate that I put in the
s() term... And not only for this data set, but for any dataset that I try
to analyse.

Do you have any idea why I can't run gam? I can run perfectly any other
type of models (GLM, GLMM, ZIP...)..

Any help would be appreciated. Thank you very much.

Best

Diego




-- 
*Diego Pav?n Jord?n*

*Finnish Museum of Natural History*
*PO BOX 17 *

*Helsinki. Finland*



*0445061210https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html
<https://tuhat.halvi.helsinki.fi/portal/en/persons/diego-pavon-jordan%288d5db37c-eddd-4fca-92cd-9c9956a42b4a%29.html>http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile
<http://www.linkedin.com/profile/view?id=170617924&trk=nav_responsive_tab_profile>https://helsinki.academia.edu/DiegoPavon
<https://helsinki.academia.edu/DiegoPavon>*

> T1 <- gam(E1 ~ s(ArrivalTime), data = Owls)Error in as.matrix(x) : object 'ArrivalTime' not found> ls(Owls) [1] "ArrivalTime"        "BroodSize"          "FoodTreatment"      "LogNeg"
 [5] "NegPerChick"        "Nest"               "SexParent"
"SiblingNegotiation"
 [9] "Xcoord"             "Ycoord"

	[[alternative HTML version deleted]]


From dario.beraldi at gmail.com  Fri Dec 11 14:38:47 2015
From: dario.beraldi at gmail.com (Dario Beraldi)
Date: Fri, 11 Dec 2015 13:38:47 +0000
Subject: [R] stopifnot with logical(0)
Message-ID: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>

Hi All,

I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
(never?) throw an exception, at least in these cases:

stopifnot(logical(0) == 1)
stopifnot(logical(0) == TRUE)
stopifnot(logical(0) == FALSE)

My understanding is that logical(0) is an empty set, so I would expect the
above tests to fail.

(I got bitten by this in a piece of code where "x" happened to be
logical(0) and stopifnot didn't catch it)

Thanks!
Dario

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Fri Dec 11 14:46:46 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 11 Dec 2015 13:46:46 +0000
Subject: [R] forest plot metafor
In-Reply-To: <003e01d132b2$424f8450$c6ee8cf0$@it>
References: <003e01d132b2$424f8450$c6ee8cf0$@it>
Message-ID: <566AD3C6.2010803@dewey.myzen.co.uk>

See below

On 09/12/2015 18:48, Mario Petretta wrote:
> Dear all,
>
>
>
> I use metafor package to generate a forest plot showing the weight of each study in the plot.
>
>
>
> I use the code:
>
> library(metafor)
>
> data(dat.bcg)
>
> res <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, measure="RR",
>
>             slab=paste(author, year, sep=", "), method="REML")
>
> forest(res, showweights=TRUE)
>
>
>
> It is possible to order the columns, placing the weights after effect size and CI?
>

There are some fairly extensive examples in the forest.rma 
documentation. You will also need some patience to get everything 
exactly where you want it and looking exactly how you want. Graphics are 
like that.

>
>
> A further query: using escalc (or rma), it is possible to add the weight of each study (other than yi and vi) to the data (in the example: data.bcg)?
>

You can extract the weights from the fitted object using weights.ram.uni 
and put them in the data  frame yourself. I am a bit worried about your 
mention of escalc, you do realise that the weights are a function of the 
fit, not the data?

So weights(res) should be OK here (not tested).

>
>
> Thanks for your attention
>
>
>
> ________________________
>
>
>
> Mario Petretta
>
> Professor of Internal Medicine
>
> Department of Translational Medical Science
>
> Naples University ?Federico II? - Italy
>
>
>
>
>
>    _____
>
>
>   <http://www.avast.com/>
>
> Questa e-mail ? priva di virus e malware perch? ? attiva la protezione avast! Antivirus <http://www.avast.com/>  .
>
>
>
> Nessun virus nel messaggio.
> Controllato da AVG - www.avg.com
> Versione: 2014.0.4259 / Database dei virus: 3705/7093 - Data di rilascio: 14/02/2014
>
>
>
> ---
> Questa e-mail ? stata controllata per individuare virus con Avast antivirus.
> https://www.avast.com/antivirus
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dusa.adrian at unibuc.ro  Fri Dec 11 14:50:28 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Fri, 11 Dec 2015 15:50:28 +0200
Subject: [R] regexp inside and outside brackets
Message-ID: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>

For the regexp aficionados, out there:

I need a regular expression to extract either everything within some
brackets, or everything outside the brackets, in a string.

This would be the test string:
"A1{0}~B0{1} CO{a2}NN{12}"

Everything outside the brackets would be:

"A1 ~B0 CO NN"

and everything inside the brackets would be:

"0 1 a2 12"

I have a working solution involving strsplit(), but I wonder if there is a
more direct way.
Thanks in advance for any hint,
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From phgrosjean at sciviews.org  Fri Dec 11 15:27:40 2015
From: phgrosjean at sciviews.org (phgrosjean at sciviews.org)
Date: Fri, 11 Dec 2015 15:27:40 +0100
Subject: [R] regexp inside and outside brackets
In-Reply-To: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
References: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
Message-ID: <B6D1E299-22DD-47D9-8597-F3ED6ADA33EE@sciviews.org>

It depends the complexity of your expression. If you are sure you don?t have nested brackets, and pairs of brackets always match, this will take everything outside the brackets:

str <- "A1{0}~B0{1} CO{a2}NN{12}?
gsub("\\{[^}]*\\}", " ", str)

Philippe Grosjean


> On 11 Dec 2015, at 14:50, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> 
> For the regexp aficionados, out there:
> 
> I need a regular expression to extract either everything within some
> brackets, or everything outside the brackets, in a string.
> 
> This would be the test string:
> "A1{0}~B0{1} CO{a2}NN{12}"
> 
> Everything outside the brackets would be:
> 
> "A1 ~B0 CO NN"
> 
> and everything inside the brackets would be:
> 
> "0 1 a2 12"
> 
> I have a working solution involving strsplit(), but I wonder if there is a
> more direct way.
> Thanks in advance for any hint,
> Adrian
> 
> -- 
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marc_schwartz at me.com  Fri Dec 11 15:39:09 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 11 Dec 2015 08:39:09 -0600
Subject: [R] regexp inside and outside brackets
In-Reply-To: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
References: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
Message-ID: <65EF0476-2B9B-4510-A230-2134E52C0F80@me.com>


> On Dec 11, 2015, at 7:50 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> 
> For the regexp aficionados, out there:
> 
> I need a regular expression to extract either everything within some
> brackets, or everything outside the brackets, in a string.
> 
> This would be the test string:
> "A1{0}~B0{1} CO{a2}NN{12}"
> 
> Everything outside the brackets would be:
> 
> "A1 ~B0 CO NN"
> 
> and everything inside the brackets would be:
> 
> "0 1 a2 12"
> 
> I have a working solution involving strsplit(), but I wonder if there is a
> more direct way.
> Thanks in advance for any hint,
> Adrian


x <- "A1{0}~B0{1} CO{a2}NN{12}"

The first is a bit easier:

> gsub("\\{[[:alnum:]]*\\}", " ", x)
[1] "A1 ~B0  CO NN "


The second, at least using standard functions, is a bit more cumbersome, given the repeated sequences:

> gsub("\\{|\\}", "", regmatches(x, gregexpr("\\{[[:alnum:]]+\\}", x))[[1]])
[1] "0"  "1"  "a2" "12"

Note that a multi-element vector is returned.

In the above:

> gregexpr("\\{[[:alnum:]]+\\}", x)
[[1]]
[1]  3  9 15 21
attr(,"match.length")
[1] 3 3 4 4
attr(,"useBytes")
[1] TRUE

returns the starting positions of the matches, which are passed to regmatches() to get the actual values:

> regmatches(x, gregexpr("\\{[[:alnum:]]+\\}", x))
[[1]]
[1] "{0}"  "{1}"  "{a2}" "{12}"

The gsub() replaces the returned braces.

You could invert the result of regmatches() to get:

> regmatches(x, gregexpr("\\{[[:alnum:]]+\\}", x), invert = TRUE)[[1]]
[1] "A1"  "~B0" " CO" "NN"  ""   


Of course, this presumes non-nesting of braces, etc.

Regards,

Marc Schwartz


From wdunlap at tibco.com  Fri Dec 11 15:55:52 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 11 Dec 2015 06:55:52 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
Message-ID: <CAF8bMcZc97cn=fgc4sbCszfEmJeDbtauFSoioXma=G6ceBZcTg@mail.gmail.com>

The reason is probably that
   any(logical())
and
   any(!logical())
return FALSE (there are no TRUEs in logical(0)).
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Fri, Dec 11, 2015 at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
> Hi All,
>
> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
> (never?) throw an exception, at least in these cases:
>
> stopifnot(logical(0) == 1)
> stopifnot(logical(0) == TRUE)
> stopifnot(logical(0) == FALSE)
>
> My understanding is that logical(0) is an empty set, so I would expect the
> above tests to fail.
>
> (I got bitten by this in a piece of code where "x" happened to be
> logical(0) and stopifnot didn't catch it)
>
> Thanks!
> Dario
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Fri Dec 11 16:05:11 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 11 Dec 2015 07:05:11 -0800
Subject: [R] regexp inside and outside brackets
In-Reply-To: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
References: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
Message-ID: <C90110C9-47CC-425B-9296-7B04F62C0E07@dcn.davis.ca.us>

The gsub function is your friend. 

s <- "A1{0}~B0{1} CO{a2}NN{12}"
gsub( "([^{}]*)\\{([^{}]*)\\}", "\\1 ", s )
gsub( "([^{}]*)\\{([^{}]*)\\}", "\\2 ", s )

but keep in mind that there are many resources on the Internet for learning about regular expressions... they are hardly R-specific. 

-- 
Sent from my phone. Please excuse my brevity.

On December 11, 2015 5:50:28 AM PST, "Adrian Du?a" <dusa.adrian at unibuc.ro> wrote:
>For the regexp aficionados, out there:
>
>I need a regular expression to extract either everything within some
>brackets, or everything outside the brackets, in a string.
>
>This would be the test string:
>"A1{0}~B0{1} CO{a2}NN{12}"
>
>Everything outside the brackets would be:
>
>"A1 ~B0 CO NN"
>
>and everything inside the brackets would be:
>
>"0 1 a2 12"
>
>I have a working solution involving strsplit(), but I wonder if there
>is a
>more direct way.
>Thanks in advance for any hint,
>Adrian
>
>-- 
>Adrian Dusa
>University of Bucharest
>Romanian Social Data Archive
>Soseaua Panduri nr.90
>050663 Bucharest sector 5
>Romania
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Fri Dec 11 16:12:56 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Fri, 11 Dec 2015 17:12:56 +0200
Subject: [R] regexp inside and outside brackets
In-Reply-To: <C90110C9-47CC-425B-9296-7B04F62C0E07@dcn.davis.ca.us>
References: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
	<C90110C9-47CC-425B-9296-7B04F62C0E07@dcn.davis.ca.us>
Message-ID: <CAJ=0CtB_sLv8gZfKhiRcivJPZ1Wi91V8=Xe8mUkqGa-v9z7ywQ@mail.gmail.com>

Thanks very much, Marc and Jeff.
Jeff's solutions seem to be simple one liners. I really need to learn these
things, too powerful to ignore.

Thank you very much,
Adrian

On Fri, Dec 11, 2015 at 5:05 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> The gsub function is your friend.
>
> s <- "A1{0}~B0{1} CO{a2}NN{12}"
> gsub( "([^{}]*)\\{([^{}]*)\\}", "\\1 ", s )
> gsub( "([^{}]*)\\{([^{}]*)\\}", "\\2 ", s )
>
> but keep in mind that there are many resources on the Internet for
> learning about regular expressions... they are hardly R-specific.
>
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 11, 2015 5:50:28 AM PST, "Adrian Du?a" <dusa.adrian at unibuc.ro>
> wrote:
>>
>> For the regexp aficionados, out there:
>>
>> I need a regular expression to extract either everything within some
>> brackets, or everything outside the brackets, in a string.
>>
>> This would be the test string:
>> "A1{0}~B0{1} CO{a2}NN{12}"
>>
>> Everything outside the brackets would be:
>>
>> "A1 ~B0 CO NN"
>>
>> and everything inside the brackets would be:
>>
>> "0 1 a2 12"
>>
>> I have a working solution involving strsplit(), but I wonder if there is a
>> more direct way.
>> Thanks in advance for any hint,
>> Adrian
>>
>>


-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Dec 11 16:23:09 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 11 Dec 2015 07:23:09 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
Message-ID: <BF1F7038-706D-4537-A627-88CDD9672896@dcn.davis.ca.us>

The goal of the comparison operators is to obtain a logical value.  Why compare logical values... you clearly already have that?

stopifnot( logicalvariable ) and 
stopifnot( !logicalvariable )

are sensible,  but not

stopifnot( logicalvariable == TRUE ) or 
stopifnot( logicalvariable == FALSE )

That said, R lets you construct the department of redundancy department  anyway,  but you have to have a value to compare.  logical(0) is the absence of a logical value, so there is nothing to compare, so the result has to also be the absence of a logical value (logical(0)).

Somewhere, the test that lead to the existence of this empty logical vector had no data. That absence of data is what you need to test for. Or, at the very least, you need to verify that the length of your logical variable is greater than zero before checking its value. 
-- 
Sent from my phone. Please excuse my brevity.

On December 11, 2015 5:38:47 AM PST, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>Hi All,
>
>I'd like to understand the reason why stopifnot(logical(0) == x)
>doesn't
>(never?) throw an exception, at least in these cases:
>
>stopifnot(logical(0) == 1)
>stopifnot(logical(0) == TRUE)
>stopifnot(logical(0) == FALSE)
>
>My understanding is that logical(0) is an empty set, so I would expect
>the
>above tests to fail.
>
>(I got bitten by this in a piece of code where "x" happened to be
>logical(0) and stopifnot didn't catch it)
>
>Thanks!
>Dario
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From marc_schwartz at me.com  Fri Dec 11 16:29:28 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 11 Dec 2015 09:29:28 -0600
Subject: [R] regexp inside and outside brackets
In-Reply-To: <CAJ=0CtB_sLv8gZfKhiRcivJPZ1Wi91V8=Xe8mUkqGa-v9z7ywQ@mail.gmail.com>
References: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
	<C90110C9-47CC-425B-9296-7B04F62C0E07@dcn.davis.ca.us>
	<CAJ=0CtB_sLv8gZfKhiRcivJPZ1Wi91V8=Xe8mUkqGa-v9z7ywQ@mail.gmail.com>
Message-ID: <426344BE-E0A2-4683-BCB1-350964B13758@me.com>

Hi,

Needless to say, Jeff's solution is easier than my second one. I was wrestling in dealing with the greedy nature of regex's and so shifted to thinking about the use of the functions that I proposed in the second scenario.

Also, I was a bit hypo-caffeinated ... ;-)

Regards,

Marc


> On Dec 11, 2015, at 9:12 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> 
> Thanks very much, Marc and Jeff.
> Jeff's solutions seem to be simple one liners. I really need to learn these
> things, too powerful to ignore.
> 
> Thank you very much,
> Adrian
> 
> On Fri, Dec 11, 2015 at 5:05 PM, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
> wrote:
> 
>> The gsub function is your friend.
>> 
>> s <- "A1{0}~B0{1} CO{a2}NN{12}"
>> gsub( "([^{}]*)\\{([^{}]*)\\}", "\\1 ", s )
>> gsub( "([^{}]*)\\{([^{}]*)\\}", "\\2 ", s )
>> 
>> but keep in mind that there are many resources on the Internet for
>> learning about regular expressions... they are hardly R-specific.
>> 
>> --
>> Sent from my phone. Please excuse my brevity.
>> 
>> On December 11, 2015 5:50:28 AM PST, "Adrian Du?a" <dusa.adrian at unibuc.ro>
>> wrote:
>>> 
>>> For the regexp aficionados, out there:
>>> 
>>> I need a regular expression to extract either everything within some
>>> brackets, or everything outside the brackets, in a string.
>>> 
>>> This would be the test string:
>>> "A1{0}~B0{1} CO{a2}NN{12}"
>>> 
>>> Everything outside the brackets would be:
>>> 
>>> "A1 ~B0 CO NN"
>>> 
>>> and everything inside the brackets would be:
>>> 
>>> "0 1 a2 12"
>>> 
>>> I have a working solution involving strsplit(), but I wonder if there is a
>>> more direct way.
>>> Thanks in advance for any hint,
>>> Adrian
>>> 
>>> 
> 
> 
> -- 
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Fri Dec 11 16:38:56 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 11 Dec 2015 15:38:56 +0000
Subject: [R] stopifnot with logical(0)
Message-ID: <248E6FA047A8C746BA491485764190F5371243BB@ESESSMB210.ericsson.se>

I think the inspection of the "stopifnot()" source code may help.

> stopifnot
function (...)
{
    n <- length(ll <- list(...))
    if (n == 0L)
        return(invisible())
    mc <- match.call()
    for (i in 1L:n) if (!(is.logical(r <- ll[[i]]) && !anyNA(r) &&
        all(r))) {
        ch <- deparse(mc[[i + 1]], width.cutoff = 60L)
        if (length(ch) > 1L)
            ch <- paste(ch[1L], "....")
        stop(sprintf(ngettext(length(r), "%s is not TRUE", "%s are not all TRUE"),
            ch), call. = FALSE, domain = NA)
    }
    invisible()
}


The following code may help in understanding.

(arg <- (logical(0) == 1))
(arg <- (logical(0) == TRUE))
(arg <- (logical(0) == FALSE))

n <- length(ll <- list(arg))
n

if (n == 0L)
  return(invisible())

for (i in 1L:n) {
  print(is.logical(r <- ll[[i]]))
  print(!anyNA(r))
  print(all(r))
  if (! (is.logical(r <- ll[[i]]) && !anyNA(r) &&  all(r)) ) {
    print("stop")
  }
}

Executing such code:

> (arg <- (logical(0) == 1))
logical(0)
> (arg <- (logical(0) == TRUE))
logical(0)
> (arg <- (logical(0) == FALSE))
logical(0)
>
> n <- length(ll <- list(arg))
> n
[1] 1
>
> if (n == 0L)
+   return(invisible())
>
> for (i in 1L:n) {
+   print(is.logical(r <- ll[[i]]))
+   print(!anyNA(r))
+   print(all(r))
+   if (! (is.logical(r <- ll[[i]]) && !anyNA(r) &&  all(r)) ) {
+     print("stop")
+   }
+ }
[1] TRUE
[1] TRUE
[1] TRUE
>

See also help(all) and what its "Note" states about all(logical(0))
and consider that:

> is.logical(logical(0))
[1] TRUE


--
GG

	[[alternative HTML version deleted]]


From simon.wood at bath.edu  Fri Dec 11 14:08:26 2015
From: simon.wood at bath.edu (Simon Wood)
Date: Fri, 11 Dec 2015 13:08:26 +0000
Subject: [R] Error message when running gam (mgcv). object not found
In-Reply-To: <CAD93_Fr6-2yd0LN1=2O-EGexdCx40iF99YyqiyqbjzaCbw23Yw@mail.gmail.com>
References: <CAD93_Fr6-2yd0LN1=2O-EGexdCx40iF99YyqiyqbjzaCbw23Yw@mail.gmail.com>
Message-ID: <566ACACA.3030308@bath.edu>

Hi Diego, I suggest you send me an example offline, along with the 
result of  sessionInfo(), best, Simon

On 11/12/15 09:25, Diego Pavon wrote:
> Dear all
>
> I am trying to fit gam to my data but it keeps giving me errors.
>
>
> It does not find the covariate ArrivalTime even though it is clearly
> defined in the data frame. It happens with any covariate that I put in the
> s() term... And not only for this data set, but for any dataset that I try
> to analyse.
>
> Do you have any idea why I can't run gam? I can run perfectly any other
> type of models (GLM, GLMM, ZIP...)..
>
> Any help would be appreciated. Thank you very much.
>
> Best
>
> Diego
>
>
>
>


-- 
Simon Wood, School of Mathematics, University of Bristol BS8 1TW UK
+44 (0)117 33 18273     http://www.maths.bris.ac.uk/~sw15190


From brunu_higa at hotmail.com  Fri Dec 11 16:32:41 2015
From: brunu_higa at hotmail.com (bruno higa)
Date: Fri, 11 Dec 2015 13:32:41 -0200
Subject: [R] Help quadratic plateau
Message-ID: <SNT150-W2674B0774B08C88C036A1AE9EA0@phx.gbl>

i need to do a quadratic plateau. I saw the code and try to put my datas, but i had some errors. 
This is my code , 
for (mun in 1:dm[1]){  # for (mun in 8:dm[1]){    if(nu[mun]>= nmin) {                y1=arquivov[mun,ncv[mun]:27]      vy = as.numeric(y1)        x=seq(1:nu[mun])
      cr=lm(vy~x+I(x^2))      ca_q[mun]=cr$coefficients[[1]]      cl_q[mun]=cr$coefficients[[2]]      cq_q[mun]=cr$coefficients[[3]]            alpha_q=as.numeric(ca_q[mun])      beta_q=as.numeric(cl_q[mun])      gamma_q=as.numeric(cq_q[mun])             fm <- nls(vy ~ Mean(x, alpha, beta, gamma), start=list(alpha=alpha_q, beta=beta_q, gamma=gamma_q),control = list(maxiter = dm[1]))       fm      summary(fm)            plot(vy ~ x)      lines(fitted(fm) ~ x)      with(as.list(coef(fm)), abline(v = -beta/(2 * gamma)))
    }    else{          }}
> vy [1]  77.25989  58.00000  58.00000  58.00000 109.68317  58.00000  80.00000  80.00000  71.33333  69.66667  68.00000  65.33333[13]  58.00000  57.00000  57.00000  57.00000  57.00000  67.00000> alpha_q[1] 67.69373> beta_q[1] 1.321098> gamma_q[1] -0.1129059



Error in nls(vy ~ Mean(x, alpha, beta, gamma), start = list(alpha = alpha_q,  :   singular gradient
 		 	   		  
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Fri Dec 11 17:10:52 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 11 Dec 2015 08:10:52 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
Message-ID: <76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>


> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
> 
> Hi All,
> 
> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
> (never?) throw an exception, at least in these cases:

The usual way to test for a length-0 logical object is to use length():

x <- logical(0)

stopifnot( !length(x) & mode(x)=="logical" )


> 
> stopifnot(logical(0) == 1)
> stopifnot(logical(0) == TRUE)
> stopifnot(logical(0) == FALSE)
> 
> My understanding is that logical(0) is an empty set, so I would expect the
> above tests to fail.
> 
> (I got bitten by this in a piece of code where "x" happened to be
> logical(0) and stopifnot didn't catch it)
> 
> Thanks!
> Dario
-- 

David Winsemius
Alameda, CA, USA


From henrik.bengtsson at gmail.com  Fri Dec 11 17:20:55 2015
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Fri, 11 Dec 2015 08:20:55 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
Message-ID: <CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>

On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>>
>> Hi All,
>>
>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
>> (never?) throw an exception, at least in these cases:
>
> The usual way to test for a length-0 logical object is to use length():
>
> x <- logical(0)
>
> stopifnot( !length(x) & mode(x)=="logical" )

I found

stopifnot(!length(x), mode(x) == "logical")

more helpful when troubleshooting, because it will tell you whether
it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
wrote:

stopifnot(!length(x))
stopifnot(mode(x) == "logical")

/Henrik

>
>
>>
>> stopifnot(logical(0) == 1)
>> stopifnot(logical(0) == TRUE)
>> stopifnot(logical(0) == FALSE)
>>
>> My understanding is that logical(0) is an empty set, so I would expect the
>> above tests to fail.
>>
>> (I got bitten by this in a piece of code where "x" happened to be
>> logical(0) and stopifnot didn't catch it)
>>
>> Thanks!
>> Dario
> --
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From auerbecktj at gmail.com  Fri Dec 11 17:49:34 2015
From: auerbecktj at gmail.com (Tyler Auerbeck)
Date: Fri, 11 Dec 2015 11:49:34 -0500
Subject: [R] Windows R 2.15.1 on Citrix
Message-ID: <CAETY7qNzieqOXAYh-_vYgVKa6MxdDXQebrsWwnpg8+aBUKyb1w@mail.gmail.com>

We're currently having an odd issue on an installation of Windows R 2.15.1
over Citrix. Occasionally we will see the application dissapear. Sometimes
this will happen immediately, after a few minutes, etc. It's never after
the exact same action or same period of time. I've looked at the even logs,
but don't see anything that coordinates with that period of time. Has
anyone run across this issue before? Or can anyone point me to where the R
application may write some of its own logs?

	[[alternative HTML version deleted]]


From mariojmaaz at gmail.com  Fri Dec 11 18:40:51 2015
From: mariojmaaz at gmail.com (=?UTF-8?Q?Mario_Jos=C3=A9_Marques=2DAzevedo?=)
Date: Fri, 11 Dec 2015 15:40:51 -0200
Subject: [R] Weird behaviour function args in ellipses
Message-ID: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>

Dears,

I'm having a weird behaviours when setting arguments in functions.

fn <- function(x, st="mean", b=NULL, col.range="black", ...){
  dots <- list(...)
  cat("col.range =", col.range, "\n")
  cat("dots =\n")
  print(dots)
}

fn(1, b=2,col="red")

# Output
col.range = red
dots =
list()

Why 'col' arguments are not in ellipses, but setting col.range to 'red'?

If I change the position of ellipses

fn2 <- function(x, ..., st="mean", b=NULL, col.range="black"){
  dots <- list(...)
  cat("col.range =", col.range, "\n")
  cat("dots =\n")
  print(dots)
}

fn2(1, b=2, col="red")

# Output
col.range = black
dots =
$col
[1] "red"

It works! I'm using R version 3.2.2.

Best regards!

Mario

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Fri Dec 11 18:49:39 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 11 Dec 2015 12:49:39 -0500
Subject: [R] Windows R 2.15.1 on Citrix
In-Reply-To: <CAETY7qNzieqOXAYh-_vYgVKa6MxdDXQebrsWwnpg8+aBUKyb1w@mail.gmail.com>
References: <CAETY7qNzieqOXAYh-_vYgVKa6MxdDXQebrsWwnpg8+aBUKyb1w@mail.gmail.com>
Message-ID: <566B0CB3.6030908@gmail.com>

On 11/12/2015 11:49 AM, Tyler Auerbeck wrote:
> We're currently having an odd issue on an installation of Windows R 2.15.1
> over Citrix. Occasionally we will see the application dissapear. Sometimes
> this will happen immediately, after a few minutes, etc. It's never after
> the exact same action or same period of time. I've looked at the even logs,
> but don't see anything that coordinates with that period of time. Has
> anyone run across this issue before? Or can anyone point me to where the R
> application may write some of its own logs?

I think you're on your own using such an old version (released June, 
2012).   It's not even the last patch release of the 2.15.x series, 
which was 2.15.3.  And even if you update to the most recent release 
(3.2.3), we probably can't help you with Citrix, though if you identify 
the problem we'll look into it.

Duncan Murdoch


From murdoch.duncan at gmail.com  Fri Dec 11 18:53:45 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 11 Dec 2015 12:53:45 -0500
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
Message-ID: <566B0DA9.509@gmail.com>

On 11/12/2015 12:40 PM, Mario Jos? Marques-Azevedo wrote:
> Dears,
>
> I'm having a weird behaviours when setting arguments in functions.
>
> fn <- function(x, st="mean", b=NULL, col.range="black", ...){
>    dots <- list(...)
>    cat("col.range =", col.range, "\n")
>    cat("dots =\n")
>    print(dots)
> }
>
> fn(1, b=2,col="red")
>
> # Output
> col.range = red
> dots =
> list()
>
> Why 'col' arguments are not in ellipses, but setting col.range to 'red'?

See the R Language Definition manual, section 4.3.2.  Partial matching 
is used on arguments that precede ... in the function definition, exact 
matching on arguments that follow it.

Duncan Murdoch
>
> If I change the position of ellipses
>
> fn2 <- function(x, ..., st="mean", b=NULL, col.range="black"){
>    dots <- list(...)
>    cat("col.range =", col.range, "\n")
>    cat("dots =\n")
>    print(dots)
> }
>
> fn2(1, b=2, col="red")
>
> # Output
> col.range = black
> dots =
> $col
> [1] "red"
>
> It works! I'm using R version 3.2.2.
>
> Best regards!
>
> Mario
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Dec 11 18:55:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 11 Dec 2015 09:55:53 -0800
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
Message-ID: <293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>


> On Dec 11, 2015, at 9:40 AM, Mario Jos? Marques-Azevedo <mariojmaaz at gmail.com> wrote:
> 
> Dears,
> 
> I'm having a weird behaviours when setting arguments in functions.
> 
> fn <- function(x, st="mean", b=NULL, col.range="black", ...){
>  dots <- list(...)
>  cat("col.range =", col.range, "\n")
>  cat("dots =\n")
>  print(dots)
> }
> 
> fn(1, b=2,col="red")
> 
> # Output
> col.range = red

Argument matching is done with `pmatch`. So "col" is a partial match to "col.range". 

See ?match.arg


> dots =
> list()
> 
> Why 'col' arguments are not in ellipses, but setting col.range to 'red'?
> 
> If I change the position of ellipses
> 
> fn2 <- function(x, ..., st="mean", b=NULL, col.range="black"){
>  dots <- list(...)
>  cat("col.range =", col.range, "\n")
>  cat("dots =\n")
>  print(dots)
> }
> 
> fn2(1, b=2, col="red")
> 
> # Output
> col.range = black
> dots =
> $col
> [1] "red"
> 
> It works! I'm using R version 3.2.2.

It's been that way 'forever'.

> 
> Best regards!
> 
> Mario
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From rshepard at appl-ecosys.com  Fri Dec 11 19:44:56 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 11 Dec 2015 10:44:56 -0800 (PST)
Subject: [R] Updating Package Fails: Help on How to Fix Needed
Message-ID: <alpine.LNX.2.11.1512111040130.6535@localhost>

   Trying to update package pbkrtest failed because of a missing object in
another namespace. Not having experienced this issue before now I don't know
what to do to fix the problem. Here's the story:

* installing *source* package ?pbkrtest? ...
** package ?pbkrtest? successfully unpacked and MD5 sums checked
** R
** data
** inst
** preparing package for lazy loading
Warning: replacing previous import by ?stats::sigma? when loading ?pbkrtest?
Error : object ?sigma? is not exported by 'namespace:stats'
ERROR: lazy loading failed for package ?pbkrtest?
* removing ?/usr/lib/R/library/pbkrtest?
* restoring previous ?/usr/lib/R/library/pbkrtest?

The downloaded source packages are in
 	?/tmp/RtmppHtiuJ/downloaded_packages?
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Warning message:
In install.packages(update[instlib == l, "Package"], l, contriburl =
contriburl,  :
   installation of package ?pbkrtest? had non-zero exit status

   Advice needed.

Rich


From mariojmaaz at gmail.com  Fri Dec 11 19:52:10 2015
From: mariojmaaz at gmail.com (=?UTF-8?Q?Mario_Jos=C3=A9_Marques=2DAzevedo?=)
Date: Fri, 11 Dec 2015 16:52:10 -0200
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
	<293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
Message-ID: <CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>

Hi Duncan and David,

Thank you for explanation. I'm really disappointed with this R "resource".
I think that partial match, mainly in function args, must be optional and
not default. We can have many problems and lost hours find errors (it occur
with me). I tried to find a solution to disable partial match, but it seems
that is not possible. Program with hacks for this will be sad.

Best regards!

Mario



On 11 December 2015 at 15:55, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Dec 11, 2015, at 9:40 AM, Mario Jos? Marques-Azevedo <
> mariojmaaz at gmail.com> wrote:
> >
> > Dears,
> >
> > I'm having a weird behaviours when setting arguments in functions.
> >
> > fn <- function(x, st="mean", b=NULL, col.range="black", ...){
> >  dots <- list(...)
> >  cat("col.range =", col.range, "\n")
> >  cat("dots =\n")
> >  print(dots)
> > }
> >
> > fn(1, b=2,col="red")
> >
> > # Output
> > col.range = red
>
> Argument matching is done with `pmatch`. So "col" is a partial match to
> "col.range".
>
> See ?match.arg
>
>
> > dots =
> > list()
> >
> > Why 'col' arguments are not in ellipses, but setting col.range to 'red'?
> >
> > If I change the position of ellipses
> >
> > fn2 <- function(x, ..., st="mean", b=NULL, col.range="black"){
> >  dots <- list(...)
> >  cat("col.range =", col.range, "\n")
> >  cat("dots =\n")
> >  print(dots)
> > }
> >
> > fn2(1, b=2, col="red")
> >
> > # Output
> > col.range = black
> > dots =
> > $col
> > [1] "red"
> >
> > It works! I'm using R version 3.2.2.
>
> It's been that way 'forever'.
>
> >
> > Best regards!
> >
> > Mario
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Fri Dec 11 20:05:28 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 11 Dec 2015 14:05:28 -0500
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <alpine.LNX.2.11.1512111040130.6535@localhost>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
Message-ID: <566B1E78.1060403@gmail.com>

On 11/12/2015 1:44 PM, Rich Shepard wrote:
>     Trying to update package pbkrtest failed because of a missing object in
> another namespace. Not having experienced this issue before now I don't know
> what to do to fix the problem. Here's the story:
>
> * installing *source* package ?pbkrtest? ...
> ** package ?pbkrtest? successfully unpacked and MD5 sums checked
> ** R
> ** data
> ** inst
> ** preparing package for lazy loading
> Warning: replacing previous import by ?stats::sigma? when loading ?pbkrtest?
> Error : object ?sigma? is not exported by 'namespace:stats'
> ERROR: lazy loading failed for package ?pbkrtest?
> * removing ?/usr/lib/R/library/pbkrtest?
> * restoring previous ?/usr/lib/R/library/pbkrtest?
>
> The downloaded source packages are in
>   	?/tmp/RtmppHtiuJ/downloaded_packages?
> Updating HTML index of packages in '.Library'
> Making 'packages.html' ... done
> Warning message:
> In install.packages(update[instlib == l, "Package"], l, contriburl =
> contriburl,  :
>     installation of package ?pbkrtest? had non-zero exit status
>
>     Advice needed.

I don't see that.  You need to give more details:  I'd start with 
sessionInfo(), and the version number of the pbkrtest package that 
you're trying to install.  (If R is downloading it for you, 
available.packages()["pbkrtest",]  will give lots of useful information.)

Duncan Murdoch


From murdoch.duncan at gmail.com  Fri Dec 11 20:13:14 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 11 Dec 2015 14:13:14 -0500
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
	<293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
	<CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
Message-ID: <566B204A.7000400@gmail.com>

On 11/12/2015 1:52 PM, Mario Jos? Marques-Azevedo wrote:
> Hi Duncan and David,
>
> Thank you for explanation. I'm really disappointed with this R "resource".
> I think that partial match, mainly in function args, must be optional and
> not default. We can have many problems and lost hours find errors (it occur
> with me). I tried to find a solution to disable partial match, but it seems
> that is not possible. Program with hacks for this will be sad.

Nowadays with smart editors, I agree that partial matching isn't really 
necessary.  However, R has been around for 20 years, and lots of 
existing code depends on it.   Eventually you'll get to know the quirks 
of the design.

Duncan Murdoch
>
> Best regards!
>
> Mario
>
>
>
> On 11 December 2015 at 15:55, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
> >
> > > On Dec 11, 2015, at 9:40 AM, Mario Jos? Marques-Azevedo <
> > mariojmaaz at gmail.com> wrote:
> > >
> > > Dears,
> > >
> > > I'm having a weird behaviours when setting arguments in functions.
> > >
> > > fn <- function(x, st="mean", b=NULL, col.range="black", ...){
> > >  dots <- list(...)
> > >  cat("col.range =", col.range, "\n")
> > >  cat("dots =\n")
> > >  print(dots)
> > > }
> > >
> > > fn(1, b=2,col="red")
> > >
> > > # Output
> > > col.range = red
> >
> > Argument matching is done with `pmatch`. So "col" is a partial match to
> > "col.range".
> >
> > See ?match.arg
> >
> >
> > > dots =
> > > list()
> > >
> > > Why 'col' arguments are not in ellipses, but setting col.range to 'red'?
> > >
> > > If I change the position of ellipses
> > >
> > > fn2 <- function(x, ..., st="mean", b=NULL, col.range="black"){
> > >  dots <- list(...)
> > >  cat("col.range =", col.range, "\n")
> > >  cat("dots =\n")
> > >  print(dots)
> > > }
> > >
> > > fn2(1, b=2, col="red")
> > >
> > > # Output
> > > col.range = black
> > > dots =
> > > $col
> > > [1] "red"
> > >
> > > It works! I'm using R version 3.2.2.
> >
> > It's been that way 'forever'.
> >
> > >
> > > Best regards!
> > >
> > > Mario
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> >
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Fri Dec 11 20:19:16 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Dec 2015 11:19:16 -0800
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
	<293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
	<CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
Message-ID: <CAGxFJbR0K2pBa0TJ9v0_5K_BasAFtg42q7AnWVvGKo+W7hqA1A@mail.gmail.com>

I normally don't respond to this sort of rant, but I will here. Feel
free to ignore.

My response is: sour grapes! **I'm really disappointed** that you
failed to read (carefully) the documents (R language manual) or
tutorials (numerous and ubiquitous) that clearly explain this. IMO,
you are blaming others for your failings.

If you care to respond, yours will be the last word. I will not
comment further (and probably shouldn't have here).

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Dec 11, 2015 at 10:52 AM, Mario Jos? Marques-Azevedo
<mariojmaaz at gmail.com> wrote:
> Hi Duncan and David,
>
> Thank you for explanation. I'm really disappointed with this R "resource".
> I think that partial match, mainly in function args, must be optional and
> not default. We can have many problems and lost hours find errors (it occur
> with me). I tried to find a solution to disable partial match, but it seems
> that is not possible. Program with hacks for this will be sad.
>
> Best regards!
>
> Mario
>
>
>
> On 11 December 2015 at 15:55, David Winsemius <dwinsemius at comcast.net>
> wrote:
>
>>
>> > On Dec 11, 2015, at 9:40 AM, Mario Jos? Marques-Azevedo <
>> mariojmaaz at gmail.com> wrote:
>> >
>> > Dears,
>> >
>> > I'm having a weird behaviours when setting arguments in functions.
>> >
>> > fn <- function(x, st="mean", b=NULL, col.range="black", ...){
>> >  dots <- list(...)
>> >  cat("col.range =", col.range, "\n")
>> >  cat("dots =\n")
>> >  print(dots)
>> > }
>> >
>> > fn(1, b=2,col="red")
>> >
>> > # Output
>> > col.range = red
>>
>> Argument matching is done with `pmatch`. So "col" is a partial match to
>> "col.range".
>>
>> See ?match.arg
>>
>>
>> > dots =
>> > list()
>> >
>> > Why 'col' arguments are not in ellipses, but setting col.range to 'red'?
>> >
>> > If I change the position of ellipses
>> >
>> > fn2 <- function(x, ..., st="mean", b=NULL, col.range="black"){
>> >  dots <- list(...)
>> >  cat("col.range =", col.range, "\n")
>> >  cat("dots =\n")
>> >  print(dots)
>> > }
>> >
>> > fn2(1, b=2, col="red")
>> >
>> > # Output
>> > col.range = black
>> > dots =
>> > $col
>> > [1] "red"
>> >
>> > It works! I'm using R version 3.2.2.
>>
>> It's been that way 'forever'.
>>
>> >
>> > Best regards!
>> >
>> > Mario
>> >
>> >       [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> David Winsemius
>> Alameda, CA, USA
>>
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Fri Dec 11 20:32:55 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 11 Dec 2015 11:32:55 -0800 (PST)
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <566B1E78.1060403@gmail.com>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
Message-ID: <alpine.LNX.2.11.1512111124050.6535@localhost>

On Fri, 11 Dec 2015, Duncan Murdoch wrote:

> I don't see that.  You need to give more details:  I'd start with
> sessionInfo(), and the version number of the pbkrtest package that you're
> trying to install. (If R is downloading it for you,
> available.packages()["pbkrtest",] will give lots of useful information.)

Duncan,

   Started R and the update.packages() function. Selected the Seattle, WA
one.

pbkrtest :
  Version 0.4-2 installed in /usr/lib/R/library
  Version 0.4-3 available at https://cran.fhcrc.org
Update (y/N/c)?  y
trying URL 'https://cran.fhcrc.org/src/contrib/pbkrtest_0.4-3.tar.gz'
Content type 'application/x-gzip' length 164415 bytes (160 KB)
==================================================
downloaded 160 KB

* installing *source* package ?pbkrtest? ...
** package ?pbkrtest? successfully unpacked and MD5 sums checked
** R
** data
** inst
** preparing package for lazy loading
Warning: replacing previous import by ?stats::sigma? when loading ?pbkrtest?
Error : object ?sigma? is not exported by 'namespace:stats'
ERROR: lazy loading failed for package ?pbkrtest?
* removing ?/usr/lib/R/library/pbkrtest?
* restoring previous ?/usr/lib/R/library/pbkrtest?

The downloaded source packages are in
 	?/tmp/RtmpQGb2HS/downloaded_packages?
Updating HTML index of packages in '.Library'
Making 'packages.html' ... done
Warning message:
In install.packages(update[instlib == l, "Package"], l, contriburl =
contriburl,  :
   installation of package ?pbkrtest? had non-zero exit status

> sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: i486-slackware-linux-gnu (32-bit)
Running under: Slackware 14.1

locale:
  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=C
  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C 
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_3.2.2 tcltk_3.2.2

> available.packages()["pbkrtest",]
                                      Package
                                   "pbkrtest"
                                      Version
                                      "0.4-3"
                                     Priority
                                           NA
                                      Depends
             "R (>= 3.0.2), lme4 (>= 1.1.10)"
                                      Imports 
"Matrix (>= 1.1.1), parallel, MASS, methods"
                                    LinkingTo
                                           NA
                                     Suggests
                                     "gplots"
                                     Enhances
                                           NA
                                      License
                                 "GPL (>= 2)"
                              License_is_FOSS
                                           NA
                        License_restricts_use
                                           NA
                                      OS_type
                                           NA
                                        Archs
                                           NA
                                       MD5sum
                                           NA
                             NeedsCompilation
                                         "no"
                                         File
                                           NA
                                   Repository
          "https://cran.fhcrc.org/src/contrib"

   Installed is lme4 Version: 1.1-10.

Rich


From wdunlap at tibco.com  Fri Dec 11 20:36:11 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 11 Dec 2015 11:36:11 -0800
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <566B1E78.1060403@gmail.com>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
Message-ID: <CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>

stats::sigma was added to R recently.  It is is R-devel now, I don't know
about yesterday's R-3.2.3.
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Fri, Dec 11, 2015 at 11:05 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 11/12/2015 1:44 PM, Rich Shepard wrote:
>>
>>     Trying to update package pbkrtest failed because of a missing object
>> in
>> another namespace. Not having experienced this issue before now I don't
>> know
>> what to do to fix the problem. Here's the story:
>>
>> * installing *source* package ?pbkrtest? ...
>> ** package ?pbkrtest? successfully unpacked and MD5 sums checked
>> ** R
>> ** data
>> ** inst
>> ** preparing package for lazy loading
>> Warning: replacing previous import by ?stats::sigma? when loading
>> ?pbkrtest?
>> Error : object ?sigma? is not exported by 'namespace:stats'
>> ERROR: lazy loading failed for package ?pbkrtest?
>> * removing ?/usr/lib/R/library/pbkrtest?
>> * restoring previous ?/usr/lib/R/library/pbkrtest?
>>
>> The downloaded source packages are in
>>         ?/tmp/RtmppHtiuJ/downloaded_packages?
>> Updating HTML index of packages in '.Library'
>> Making 'packages.html' ... done
>> Warning message:
>> In install.packages(update[instlib == l, "Package"], l, contriburl =
>> contriburl,  :
>>     installation of package ?pbkrtest? had non-zero exit status
>>
>>     Advice needed.
>
>
> I don't see that.  You need to give more details:  I'd start with
> sessionInfo(), and the version number of the pbkrtest package that you're
> trying to install.  (If R is downloading it for you,
> available.packages()["pbkrtest",]  will give lots of useful information.)
>
> Duncan Murdoch
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From rshepard at appl-ecosys.com  Fri Dec 11 21:03:56 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 11 Dec 2015 12:03:56 -0800 (PST)
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
	<CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
Message-ID: <alpine.LNX.2.11.1512111202390.6535@localhost>

On Fri, 11 Dec 2015, William Dunlap wrote:

> stats::sigma was added to R recently.  It is is R-devel now, I don't know
> about yesterday's R-3.2.3.

Bill,

   Okay. I intended to update R after the packages. Will run version upgrade,
then see how the package update goes. Result will be reported here.

Thanks,

Rich


From rshepard at appl-ecosys.com  Fri Dec 11 21:24:53 2015
From: rshepard at appl-ecosys.com (Rich Shepard)
Date: Fri, 11 Dec 2015 12:24:53 -0800 (PST)
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <alpine.LNX.2.11.1512111202390.6535@localhost>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
	<CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
	<alpine.LNX.2.11.1512111202390.6535@localhost>
Message-ID: <alpine.LNX.2.11.1512111221240.6535@localhost>

On Fri, 11 Dec 2015, Rich Shepard wrote:

> On Fri, 11 Dec 2015, William Dunlap wrote:
>
>> stats::sigma was added to R recently.  It is is R-devel now, I don't know
>> about yesterday's R-3.2.3.

>  Okay. I intended to update R after the packages. Will run version upgrade,
> then see how the package update goes. Result will be reported here.

   Well, R-3.2.3 does not include stats::sigma. An upgraded version of stats
is apparently not yet released. I'll wait to upgrade pbkrtes until next
Friday.

Thanks Duncan and Bill,

Rich


From f.harrell at Vanderbilt.Edu  Fri Dec 11 21:26:36 2015
From: f.harrell at Vanderbilt.Edu (Frank Harrell)
Date: Fri, 11 Dec 2015 14:26:36 -0600
Subject: [R] Correct notation for functions, packages when using LaTex
Message-ID: <566B317C.3010406@vanderbilt.edu>

Rolf I believe \textsf is the correct font to use for the symbol R but 
not necessarily for the names of R variables and functions.  I'd like 
more discussion about that.

Frank

-- 
------------------------------------------------------------------------
Frank E Harrell Jr 	Professor and Chairman 	School of Medicine

	Department of *Biostatistics* 	*Vanderbilt University*


	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Fri Dec 11 21:29:52 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Fri, 11 Dec 2015 14:29:52 -0600
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <566B204A.7000400@gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
	<293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
	<CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
	<566B204A.7000400@gmail.com>
Message-ID: <CABdHhvHRE0Osof_=h6UjCE=sBthZ_TRud4x_YRRjHSnEc13AGg@mail.gmail.com>

On Fri, Dec 11, 2015 at 1:13 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 11/12/2015 1:52 PM, Mario Jos? Marques-Azevedo wrote:
>>
>> Hi Duncan and David,
>>
>> Thank you for explanation. I'm really disappointed with this R "resource".
>> I think that partial match, mainly in function args, must be optional and
>> not default. We can have many problems and lost hours find errors (it
>> occur
>> with me). I tried to find a solution to disable partial match, but it
>> seems
>> that is not possible. Program with hacks for this will be sad.
>
>
> Nowadays with smart editors, I agree that partial matching isn't really
> necessary.  However, R has been around for 20 years, and lots of existing
> code depends on it.   Eventually you'll get to know the quirks of the
> design.

And if you really dislike this behavour, you can at least warn on it:

options(
  warnPartialMatchArgs = TRUE,
  warnPartialMatchAttr = TRUE,
  warnPartialMatchDollar = TRUE
)

Hadley

-- 
http://had.co.nz/


From murdoch.duncan at gmail.com  Fri Dec 11 21:40:00 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 11 Dec 2015 15:40:00 -0500
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
	<CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
Message-ID: <566B34A0.4090305@gmail.com>

On 11/12/2015 2:36 PM, William Dunlap wrote:
> stats::sigma was added to R recently.  It is is R-devel now, I don't know
> about yesterday's R-3.2.3.

As Rich saw, it's not.  pbkrtest should have "Depends: R (>= 3.3.0)" 
instead of "Depends: R (>= 3.0.0)" in its DESCRIPTION.
You can see it failed tests on CRAN.

Duncan Murdoch
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Fri, Dec 11, 2015 at 11:05 AM, Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
> > On 11/12/2015 1:44 PM, Rich Shepard wrote:
> >>
> >>     Trying to update package pbkrtest failed because of a missing object
> >> in
> >> another namespace. Not having experienced this issue before now I don't
> >> know
> >> what to do to fix the problem. Here's the story:
> >>
> >> * installing *source* package ?pbkrtest? ...
> >> ** package ?pbkrtest? successfully unpacked and MD5 sums checked
> >> ** R
> >> ** data
> >> ** inst
> >> ** preparing package for lazy loading
> >> Warning: replacing previous import by ?stats::sigma? when loading
> >> ?pbkrtest?
> >> Error : object ?sigma? is not exported by 'namespace:stats'
> >> ERROR: lazy loading failed for package ?pbkrtest?
> >> * removing ?/usr/lib/R/library/pbkrtest?
> >> * restoring previous ?/usr/lib/R/library/pbkrtest?
> >>
> >> The downloaded source packages are in
> >>         ?/tmp/RtmppHtiuJ/downloaded_packages?
> >> Updating HTML index of packages in '.Library'
> >> Making 'packages.html' ... done
> >> Warning message:
> >> In install.packages(update[instlib == l, "Package"], l, contriburl =
> >> contriburl,  :
> >>     installation of package ?pbkrtest? had non-zero exit status
> >>
> >>     Advice needed.
> >
> >
> > I don't see that.  You need to give more details:  I'd start with
> > sessionInfo(), and the version number of the pbkrtest package that you're
> > trying to install.  (If R is downloading it for you,
> > available.packages()["pbkrtest",]  will give lots of useful information.)
> >
> > Duncan Murdoch
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.


From mariojmaaz at gmail.com  Fri Dec 11 21:53:58 2015
From: mariojmaaz at gmail.com (=?UTF-8?Q?Mario_Jos=C3=A9_Marques=2DAzevedo?=)
Date: Fri, 11 Dec 2015 18:53:58 -0200
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <CABdHhvHRE0Osof_=h6UjCE=sBthZ_TRud4x_YRRjHSnEc13AGg@mail.gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
	<293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
	<CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
	<566B204A.7000400@gmail.com>
	<CABdHhvHRE0Osof_=h6UjCE=sBthZ_TRud4x_YRRjHSnEc13AGg@mail.gmail.com>
Message-ID: <CANFObyyK7uXXiMDTzfBgsH+NN+pJqH2=Kkrh5_MaFKFtyFTt7A@mail.gmail.com>

Hi all,

Bert it's all ok! My disappointments are blame of my expectations and not
of R. I started with C and Php and learned to be explicit, 'computer do not
to guess what you want', but R do this 'favour' for us. I love work on R
and for my previous experiences I not expected this behaviour (again my
fault). I read others aspects of manual, but this that I supposed to know
(my fault).
But ok, we learn with our faults. And this is good!
Hadley, thank you for options!
Best regards,
Mario
On 11 Dec 2015 18:30, "Hadley Wickham" <h.wickham at gmail.com> wrote:

> On Fri, Dec 11, 2015 at 1:13 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
> > On 11/12/2015 1:52 PM, Mario Jos? Marques-Azevedo wrote:
> >>
> >> Hi Duncan and David,
> >>
> >> Thank you for explanation. I'm really disappointed with this R
> "resource".
> >> I think that partial match, mainly in function args, must be optional
> and
> >> not default. We can have many problems and lost hours find errors (it
> >> occur
> >> with me). I tried to find a solution to disable partial match, but it
> >> seems
> >> that is not possible. Program with hacks for this will be sad.
> >
> >
> > Nowadays with smart editors, I agree that partial matching isn't really
> > necessary.  However, R has been around for 20 years, and lots of existing
> > code depends on it.   Eventually you'll get to know the quirks of the
> > design.
>
> And if you really dislike this behavour, you can at least warn on it:
>
> options(
>   warnPartialMatchArgs = TRUE,
>   warnPartialMatchAttr = TRUE,
>   warnPartialMatchDollar = TRUE
> )
>
> Hadley
>
> --
> http://had.co.nz/
>

	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Fri Dec 11 21:55:06 2015
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Fri, 11 Dec 2015 22:55:06 +0200
Subject: [R] regexp inside and outside brackets
In-Reply-To: <426344BE-E0A2-4683-BCB1-350964B13758@me.com>
References: <CAJ=0CtCFricSO0Tdrq12YyafgaFO1P15gcdm7508BF0MyOB9tg@mail.gmail.com>
	<C90110C9-47CC-425B-9296-7B04F62C0E07@dcn.davis.ca.us>
	<CAJ=0CtB_sLv8gZfKhiRcivJPZ1Wi91V8=Xe8mUkqGa-v9z7ywQ@mail.gmail.com>
	<426344BE-E0A2-4683-BCB1-350964B13758@me.com>
Message-ID: <CAJ=0CtDiD8SqL3WTUXoQOsG_0k6pm41i0JwMMtroo6s_Et0rAA@mail.gmail.com>

Actually, Marc, I think your solution might be more useful than it first
seemed.
The correct usage of a string would be for someone to provide complete
pairs of outside and inside brackets information, like:
A{1} B{0}

But if a user doesn't provide this "standard" notation, as in:
A{1} B

then your solution helps to trap this error and break the function.
It is of course a different problem than my initial email, but it just
struck me that such an error should ideally be trapped.

Many thanks for the useful insight,
Adrian

On Fri, Dec 11, 2015 at 5:29 PM, Marc Schwartz <marc_schwartz at me.com> wrote:

> Hi,
>
> Needless to say, Jeff's solution is easier than my second one. I was
> wrestling in dealing with the greedy nature of regex's and so shifted to
> thinking about the use of the functions that I proposed in the second
> scenario.
>
> Also, I was a bit hypo-caffeinated ... ;-)
>
> Regards,
>
> Marc
>
>
> > On Dec 11, 2015, at 9:12 AM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:
> >
> > Thanks very much, Marc and Jeff.
> > Jeff's solutions seem to be simple one liners. I really need to learn
> these
> > things, too powerful to ignore.
> >
> > Thank you very much,
> > Adrian
> >
> > On Fri, Dec 11, 2015 at 5:05 PM, Jeff Newmiller <
> jdnewmil at dcn.davis.ca.us>
> > wrote:
> >
> >> The gsub function is your friend.
> >>
> >> s <- "A1{0}~B0{1} CO{a2}NN{12}"
> >> gsub( "([^{}]*)\\{([^{}]*)\\}", "\\1 ", s )
> >> gsub( "([^{}]*)\\{([^{}]*)\\}", "\\2 ", s )
> >>
> >> but keep in mind that there are many resources on the Internet for
> >> learning about regular expressions... they are hardly R-specific.
> >>
> >> --
> >> Sent from my phone. Please excuse my brevity.
> >>
> >> On December 11, 2015 5:50:28 AM PST, "Adrian Du?a" <
> dusa.adrian at unibuc.ro>
> >> wrote:
> >>>
> >>> For the regexp aficionados, out there:
> >>>
> >>> I need a regular expression to extract either everything within some
> >>> brackets, or everything outside the brackets, in a string.
> >>>
> >>> This would be the test string:
> >>> "A1{0}~B0{1} CO{a2}NN{12}"
> >>>
> >>> Everything outside the brackets would be:
> >>>
> >>> "A1 ~B0 CO NN"
> >>>
> >>> and everything inside the brackets would be:
> >>>
> >>> "0 1 a2 12"
> >>>
> >>> I have a working solution involving strsplit(), but I wonder if there
> is a
> >>> more direct way.
> >>> Thanks in advance for any hint,
> >>> Adrian
> >>>
> >>>
> >
> >
> > --
> > Adrian Dusa
> > University of Bucharest
> > Romanian Social Data Archive
> > Soseaua Panduri nr.90
> > 050663 Bucharest sector 5
> > Romania
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>


-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From davidsmi at microsoft.com  Fri Dec 11 22:02:03 2015
From: davidsmi at microsoft.com (David Smith)
Date: Fri, 11 Dec 2015 21:02:03 +0000
Subject: [R] Revolutions blog: November 2015 roundup
Message-ID: <DM2PR0301MB08480D8AE75F433FA34F860BC8EA0@DM2PR0301MB0848.namprd03.prod.outlook.com>

Since 2008, Revolution Analytics (and now Microsoft) staff and guests have written about R every weekday at the
Revolutions blog:
http://blog.revolutionanalytics.com
and every month I post a summary of articles from the previous month of particular interest to readers of r-help.

In case you missed them, here are some articles related to R from the month of November:

You can use emojis as plotting symbols in ggplot2 charts with the emoGG package:
http://blog.revolutionanalytics.com/2015/11/emojis-in-ggplot-graphics.html

A review of local R user group activity in 2015:
http://blog.revolutionanalytics.com/2015/11/r-user-group-activity-2015.html

Giving thanks to the R Core Group: http://blog.revolutionanalytics.com/2015/11/happy-thanksgiving.html

Some best practices for handling secret API keys in R scripts:
http://blog.revolutionanalytics.com/2015/11/how-to-store-and-use-authentication-details-with-r.html

An animated globe showing locations of Marriott and Starwood hotels using the rthreejs package:
http://blog.revolutionanalytics.com/2015/11/marriott.html

PowerBI has added support for R graphics in dashboards:
http://blog.revolutionanalytics.com/2015/11/powerbi-adds-support-for-r.html

A detailed R-based analysis of over a billion taxi rides in New York City:
http://blog.revolutionanalytics.com/2015/11/new-york-taxi-uber.html

Joseph Rickert recommends books for learning the R language and for data analysis in R:
http://blog.revolutionanalytics.com/2015/11/r-recommended-reading.html

The AzureML package has been updated to allow R functions to connect with workspaces, datasets, and experiments in Azure
ML Studio: http://blog.revolutionanalytics.com/2015/11/azureml-update.html

A simulation-based approach to explaining Simpson's Paradox:
http://blog.revolutionanalytics.com/2015/11/fun-with-simpsons-paradox-simulating-confounders.html

Two new surveys show that R continues to be the most popular language for data scientists:
http://blog.revolutionanalytics.com/2015/11/new-surveys-show-continued-popularity-of-r.html

R was featured in many presentations at this year's H2Oworld conference:
http://blog.revolutionanalytics.com/2015/11/h2o-world-2015.html

Some tips on handling packages when working with R projects: http://blog.revolutionanalytics.com/2015/11/r-projects.html

fluent-r, a new R integration library for JVM developers: http://blog.revolutionanalytics.com/2015/11/fluent-r.html

Online investing service Betterment uses R for modeling, analysis and reporting:
http://blog.revolutionanalytics.com/2015/11/betterment-uses-r.html

Applications of R were presented at the EARL conference by Verizon, Pfizer, Wikipedia, and many others:
http://blog.revolutionanalytics.com/2015/11/applications-of-r-at-earl-2015-boston.html

Simulating sample data reproducibly using the wakefield package:
http://blog.revolutionanalytics.com/2015/11/using-the-wakefield-package-to-easily-generate-reproducible-sample-data.html

Using the RJSONIO package to download Bitcoin exchange data:
http://blog.revolutionanalytics.com/2015/11/accessing-bitcoin-data-with-r.html

A series on using differential privacy for machine learning:
http://blog.revolutionanalytics.com/2015/11/differential-privacy-mini-series-from-win-vector.html

The R Consortium has funded its first community project, and is now accepting proposals for future projects:
http://blog.revolutionanalytics.com/2015/11/r-consortium-funds-first-community-project-accepting-proposals-for-more.html

General interest stories (not related to R) in the past month included: ball-moving contraptions in Lego
(http://blog.revolutionanalytics.com/2015/11/gbc.html), why you can't photograph propellers
(http://blog.revolutionanalytics.com/2015/11/because-its-friday-why-you-cant-take-photos-of-propellers.html), fun with
magnets (http://blog.revolutionanalytics.com/2015/11/because-its-friday-magnets.html), and a dangerous playground
(http://blog.revolutionanalytics.com/2015/11/because-its-friday-monash-playground.html).

Meeting times for local R user groups (http://blog.revolutionanalytics.com/local-r-groups.html) can be found on the
updated R Community Calendar at: http://blog.revolutionanalytics.com/calendar.html

If you're looking for more articles about R, you can find summaries from previous months at
http://blog.revolutionanalytics.com/roundups/. You can receive daily blog posts via email using services like
blogtrottr.com, or join the Revolution Analytics mailing list at http://revolutionanalytics.com/newsletter to be alerted
to new articles on a monthly basis.

As always, thanks for the comments and please keep sending suggestions to me at davidsmi at microsoft.com or via Twitter
(I'm @revodavid).

Cheers,
# David

-- 
David M Smith <davidsmi at microsoft.com>
R Community Lead, Microsoft? 
Tel: +1 (312) 9205766 (Chicago IL, USA)
Twitter: @revodavid | Blog: ?http://blog.revolutionanalytics.com


From sdurier.stat at gmail.com  Fri Dec 11 22:09:50 2015
From: sdurier.stat at gmail.com (=?UTF-8?Q?S=c3=a9bastien_Durier?=)
Date: Fri, 11 Dec 2015 22:09:50 +0100
Subject: [R] Question about a passage in R language
Message-ID: <566B3B9E.7060408@gmail.com>

Hello,

In ?Extract, one can read "An empty index selects all values: this is 
most often used to replace all the entries but keep the attributes"
No example is given but if x a vector I interpret this sentence as 
"x[]". And in fact, all attributes seem to be preserved by this indexing.
But in the R language manual 
(https://cran.r-project.org/doc/manuals/r-devel/R-lang.html#Indexing-by-vectors), 
the following passage puzzles me : "Empty. The expression x[] returns x, 
but drops ?irrelevant? attributes from the result." I must misunderstand 
something because it sounds contradictory with the help page.

Thank you

SD


From marc_schwartz at me.com  Fri Dec 11 22:11:21 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Fri, 11 Dec 2015 15:11:21 -0600
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <566B34A0.4090305@gmail.com>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
	<CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
	<566B34A0.4090305@gmail.com>
Message-ID: <B6B50080-A76A-4584-AD0E-783D437CB6F1@me.com>


> On Dec 11, 2015, at 2:40 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 11/12/2015 2:36 PM, William Dunlap wrote:
>> stats::sigma was added to R recently.  It is is R-devel now, I don't know
>> about yesterday's R-3.2.3.
> 
> As Rich saw, it's not.  pbkrtest should have "Depends: R (>= 3.3.0)" instead of "Depends: R (>= 3.0.0)" in its DESCRIPTION.
> You can see it failed tests on CRAN.
> 
> Duncan Murdoch


Just to confirm, having installed 3.2.3 this morning, stats::sigma is not present.

FWIW, it may, in some fashion, be related to the dependency of pbkrtest on lme4.

The ?sigma for lme4 shows the following in Examples:

  methods(sigma)# from R 3.3.0 on, shows methods from pkgs 'stats' *and* 'lme4'
 
So there may be a package namespace/export issue at play here...

Regards,

Marc Schwartz



>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>> 
>> 
>> On Fri, Dec 11, 2015 at 11:05 AM, Duncan Murdoch
>> <murdoch.duncan at gmail.com> wrote:
>> > On 11/12/2015 1:44 PM, Rich Shepard wrote:
>> >>
>> >>     Trying to update package pbkrtest failed because of a missing object
>> >> in
>> >> another namespace. Not having experienced this issue before now I don't
>> >> know
>> >> what to do to fix the problem. Here's the story:
>> >>
>> >> * installing *source* package ?pbkrtest? ...
>> >> ** package ?pbkrtest? successfully unpacked and MD5 sums checked
>> >> ** R
>> >> ** data
>> >> ** inst
>> >> ** preparing package for lazy loading
>> >> Warning: replacing previous import by ?stats::sigma? when loading
>> >> ?pbkrtest?
>> >> Error : object ?sigma? is not exported by 'namespace:stats'
>> >> ERROR: lazy loading failed for package ?pbkrtest?
>> >> * removing ?/usr/lib/R/library/pbkrtest?
>> >> * restoring previous ?/usr/lib/R/library/pbkrtest?
>> >>
>> >> The downloaded source packages are in
>> >>         ?/tmp/RtmppHtiuJ/downloaded_packages?
>> >> Updating HTML index of packages in '.Library'
>> >> Making 'packages.html' ... done
>> >> Warning message:
>> >> In install.packages(update[instlib == l, "Package"], l, contriburl =
>> >> contriburl,  :
>> >>     installation of package ?pbkrtest? had non-zero exit status
>> >>
>> >>     Advice needed.
>> >
>> >
>> > I don't see that.  You need to give more details:  I'd start with
>> > sessionInfo(), and the version number of the pbkrtest package that you're
>> > trying to install.  (If R is downloading it for you,
>> > available.packages()["pbkrtest",]  will give lots of useful information.)
>> >
>> > Duncan Murdoch
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Fri Dec 11 22:27:56 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 11 Dec 2015 13:27:56 -0800
Subject: [R] Question about a passage in R language
In-Reply-To: <566B3B9E.7060408@gmail.com>
References: <566B3B9E.7060408@gmail.com>
Message-ID: <CAGxFJbTAT1PhPJNPh6mh8wCzoELUHLFeknh1F90Sg=fCT4DR1g@mail.gmail.com>

I think we need to consult a lawyer on this one ... :-)

?Extract says that an empty index is "most often used" ... . This is a
vague comment on use, **not** an exact specification of what x[] does.

The R language manual appears to be out of date or wrong: it specifies
that "irrelevant" attributes are anything but names and dimnames in
"Only names and in multi-dimensional arrays dim and dimnames
attributes are retained." As you noted, this is false:

> x <- matrix(1:6,3,2)
> x[]
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6

> attr(x,"fy")<- "funny"
> x
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
attr(,"fy")
[1] "funny"

> x[]
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
attr(,"fy")
[1] "funny" ## this contradicts the Language definition manual passage above


Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Dec 11, 2015 at 1:09 PM, S?bastien Durier
<sdurier.stat at gmail.com> wrote:
> Hello,
>
> In ?Extract, one can read "An empty index selects all values: this is most
> often used to replace all the entries but keep the attributes"
> No example is given but if x a vector I interpret this sentence as "x[]".
> And in fact, all attributes seem to be preserved by this indexing.
> But in the R language manual
> (https://cran.r-project.org/doc/manuals/r-devel/R-lang.html#Indexing-by-vectors),
> the following passage puzzles me : "Empty. The expression x[] returns x, but
> drops ?irrelevant? attributes from the result." I must misunderstand
> something because it sounds contradictory with the help page.
>
> Thank you
>
> SD
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Fri Dec 11 23:42:56 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Sat, 12 Dec 2015 11:42:56 +1300
Subject: [R] Correct notation for functions, packages when using LaTex
In-Reply-To: <566B317C.3010406@vanderbilt.edu>
References: <566B317C.3010406@vanderbilt.edu>
Message-ID: <566B5170.6000301@auckland.ac.nz>

On 12/12/15 09:26, Frank Harrell wrote:
> Rolf I believe \textsf is the correct font to use for the symbol R but
> not necessarily for the names of R variables and functions.  I'd like
> more discussion about that.

I agree; the names of R variables and functions are *code* and should be 
rendered in "typewriter" font using \texttt{}.  (IMHO.)

cheers,

Rolf


-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From ligges at statistik.tu-dortmund.de  Fri Dec 11 23:53:41 2015
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Fri, 11 Dec 2015 23:53:41 +0100
Subject: [R] Updating Package Fails: Help on How to Fix Needed
In-Reply-To: <B6B50080-A76A-4584-AD0E-783D437CB6F1@me.com>
References: <alpine.LNX.2.11.1512111040130.6535@localhost>
	<566B1E78.1060403@gmail.com>
	<CAF8bMcYj3p6-RUv5XY1T1TU1Om=M812nAbYghpAKJoY69Pb4rQ@mail.gmail.com>
	<566B34A0.4090305@gmail.com>
	<B6B50080-A76A-4584-AD0E-783D437CB6F1@me.com>
Message-ID: <566B53F5.4050801@statistik.tu-dortmund.de>

Just FYI: This one slipped through CRAN checks. We are in contact with 
the maintainer.

Best,
Uwe Ligges


On 11.12.2015 22:11, Marc Schwartz wrote:
>
>> On Dec 11, 2015, at 2:40 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> On 11/12/2015 2:36 PM, William Dunlap wrote:
>>> stats::sigma was added to R recently.  It is is R-devel now, I don't know
>>> about yesterday's R-3.2.3.
>>
>> As Rich saw, it's not.  pbkrtest should have "Depends: R (>= 3.3.0)" instead of "Depends: R (>= 3.0.0)" in its DESCRIPTION.
>> You can see it failed tests on CRAN.
>>
>> Duncan Murdoch
>
>
> Just to confirm, having installed 3.2.3 this morning, stats::sigma is not present.
>
> FWIW, it may, in some fashion, be related to the dependency of pbkrtest on lme4.
>
> The ?sigma for lme4 shows the following in Examples:
>
>    methods(sigma)# from R 3.3.0 on, shows methods from pkgs 'stats' *and* 'lme4'
>
> So there may be a package namespace/export issue at play here...
>
> Regards,
>
> Marc Schwartz
>
>
>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>>
>>> On Fri, Dec 11, 2015 at 11:05 AM, Duncan Murdoch
>>> <murdoch.duncan at gmail.com> wrote:
>>>> On 11/12/2015 1:44 PM, Rich Shepard wrote:
>>>>>
>>>>>      Trying to update package pbkrtest failed because of a missing object
>>>>> in
>>>>> another namespace. Not having experienced this issue before now I don't
>>>>> know
>>>>> what to do to fix the problem. Here's the story:
>>>>>
>>>>> * installing *source* package ?pbkrtest? ...
>>>>> ** package ?pbkrtest? successfully unpacked and MD5 sums checked
>>>>> ** R
>>>>> ** data
>>>>> ** inst
>>>>> ** preparing package for lazy loading
>>>>> Warning: replacing previous import by ?stats::sigma? when loading
>>>>> ?pbkrtest?
>>>>> Error : object ?sigma? is not exported by 'namespace:stats'
>>>>> ERROR: lazy loading failed for package ?pbkrtest?
>>>>> * removing ?/usr/lib/R/library/pbkrtest?
>>>>> * restoring previous ?/usr/lib/R/library/pbkrtest?
>>>>>
>>>>> The downloaded source packages are in
>>>>>          ?/tmp/RtmppHtiuJ/downloaded_packages?
>>>>> Updating HTML index of packages in '.Library'
>>>>> Making 'packages.html' ... done
>>>>> Warning message:
>>>>> In install.packages(update[instlib == l, "Package"], l, contriburl =
>>>>> contriburl,  :
>>>>>      installation of package ?pbkrtest? had non-zero exit status
>>>>>
>>>>>      Advice needed.
>>>>
>>>>
>>>> I don't see that.  You need to give more details:  I'd start with
>>>> sessionInfo(), and the version number of the pbkrtest package that you're
>>>> trying to install.  (If R is downloading it for you,
>>>> available.packages()["pbkrtest",]  will give lots of useful information.)
>>>>
>>>> Duncan Murdoch
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From judyoringe at naver.com  Sat Dec 12 04:21:29 2015
From: judyoringe at naver.com (=?UTF-8?B?7J207JWE66aE?=)
Date: Sat, 12 Dec 2015 12:21:29 +0900 (KST)
Subject: [R] =?utf-8?q?Dendrogram_for_k-modes?=
Message-ID: <ca71d9204128faec9060f8e38b262e5@cweb13.nm.nhnsystem.com>

 Hi.
I have two question!
 
1) Is there any way to draw a dendrogram for k-modes?


I used klaR pacakges for kmodes analysis to deal with categorical variables.
 
i heard about the "Clustergram"
Url = {http://www.schonlau.net/clustergram.html}  
 
but i only found the example of Clustergram for K-means..'
 
2)How can I found the result of weighted k-modes? 
? 
?i did the cluster analysis with weighted kmodes(klaR packages)
like..
==================
a&lt;-kmodes(data, 5, TRUE)
==================
 
What i want to see is not only the result of clustering but the result of "weight for each attributes".
I found the code for kmodes(https://github.com/cran/klaR/blob/master/R/kmodes.R#L124)
and below codes seems to save the weights for each attribute..
but i do not know exactly how to extract this result by adding another R command... 



  if(weighted){        ## compute the frequencies of each category for each variable        weights &lt;- vector("list", num_var)        for (i in 1:num_var)             weights[[i]] &lt;- table(data[,i])    } else {        weights &lt;- NULL    }     
 
 
Thanks all!!

	[[alternative HTML version deleted]]


From judyoringe at naver.com  Sat Dec 12 04:29:03 2015
From: judyoringe at naver.com (=?UTF-8?B?7J207JWE66aE?=)
Date: Sat, 12 Dec 2015 12:29:03 +0900 (KST)
Subject: [R] =?utf-8?q?=5BklaR_packages=5D_Weighted_kmodes_=28how_to_deriv?=
 =?utf-8?q?e_the_weights=3F=29?=
In-Reply-To: <ca71d9204128faec9060f8e38b262e5@cweb13.nm.nhnsystem.com>
References: <ca71d9204128faec9060f8e38b262e5@cweb13.nm.nhnsystem.com>
Message-ID: <9795bffc4f598ebd7cb52efa7e183f6@cweb09.nm.nhnsystem.com>

 Hi.
 
How can I found the specific outcome of weighted k-modes? 
? 
?i did the cluster analysis with weighted kmodes(klaR packages)
like..
==================
a&lt;-kmodes(data, 5, TRUE)
==================
 
What i want to see is not only the result of clustering but the result of "weight for each attributes".
I found the code for kmodes(https://github.com/cran/klaR/blob/master/R/kmodes.R#L124)
and below codes seems to save the weights for each attribute..
but i do not know exactly how to extract this result by adding another R command... 
 
  if(weighted){        ## compute the frequencies of each category for each variable        weights &lt;- vector("list", num_var)        for (i in 1:num_var)             weights[[i]] &lt;- table(data[,i])    } else {        weights &lt;- NULL    }     
 
 
Thanks all!!



	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Sat Dec 12 10:54:11 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 12 Dec 2015 10:54:11 +0100
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
Message-ID: <22123.61123.948598.140447@stat.math.ethz.ch>

>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>     on Fri, 11 Dec 2015 08:20:55 -0800 writes:

    > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
    >> 
    >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
    >>> 
    >>> Hi All,
    >>> 
    >>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
    >>> (never?) throw an exception, at least in these cases:
    >> 
    >> The usual way to test for a length-0 logical object is to use length():
    >> 
    >> x <- logical(0)
    >> 
    >> stopifnot( !length(x) & mode(x)=="logical" )

    > I found

    > stopifnot(!length(x), mode(x) == "logical")

    > more helpful when troubleshooting, because it will tell you whether
    > it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
    > wrote:

    > stopifnot(!length(x))
    > stopifnot(mode(x) == "logical")

    > /Henrik

Yes, indeed, thank you Henrik  --- and Jeff Newmiller who's nice
humorous reply added other relevant points.

As author stopifnot(), I do agree with Dario's  "gut feeling"
that stopifnot()  "somehow ought to do the right thing"
in cases such as

   stopifnot(dim(x) == c(3,4))

which is really subtle version of his cases
{But the gut feeling is wrong, as I argue from now on}.

Someone writing the above would want stopifnot() to stop in the
case where x is a simple vector instead of a
matrix/data.frame/... with dimensions c(3,4) ... but it will not
because, as Bill or Jeff explained,  "the empty set is always
true", and so yes indeed,  you have to care about length-0
expressions in stopifnot().

Indeed, in the past, I had thought of  "improving" stopifnot()
by giving a warning or even stop()  for  logical(0)  expressions,
but I quickly dismissed that idea after some experiments.

My conclusion: Breaking such a fundamental lemma of logic as 
  "the empty set is always true"
will lead to all kinds of illogical situations ... so don't do that! 

Martin Maechler,
ETH Zurich


From jrkrideau at inbox.com  Sat Dec 12 13:25:33 2015
From: jrkrideau at inbox.com (John Kane)
Date: Sat, 12 Dec 2015 04:25:33 -0800
Subject: [R] Help quadratic plateau
In-Reply-To: <SNT150-W2674B0774B08C88C036A1AE9EA0@phx.gbl>
Message-ID: <2AC46EBABB8.00000167jrkrideau@inbox.com>

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

It appears that you posted in HTML and the results are very close to unreadable.

Please resend in plain text. 

John Kane
Kingston ON Canada


> -----Original Message-----
> From: brunu_higa at hotmail.com
> Sent: Fri, 11 Dec 2015 13:32:41 -0200
> To: r-help at r-project.org
> Subject: [R] Help quadratic plateau
> 
> i need to do a quadratic plateau. I saw the code and try to put my datas,
> but i had some errors.
> This is my code ,
> for (mun in 1:dm[1]){  # for (mun in 8:dm[1]){    if(nu[mun]>= nmin) {
> y1=arquivov[mun,ncv[mun]:27]      vy = as.numeric(y1)
> x=seq(1:nu[mun])
>       cr=lm(vy~x+I(x^2))      ca_q[mun]=cr$coefficients[[1]]
> cl_q[mun]=cr$coefficients[[2]]      cq_q[mun]=cr$coefficients[[3]]
> alpha_q=as.numeric(ca_q[mun])      beta_q=as.numeric(cl_q[mun])
> gamma_q=as.numeric(cq_q[mun])             fm <- nls(vy ~ Mean(x, alpha,
> beta, gamma), start=list(alpha=alpha_q, beta=beta_q,
> gamma=gamma_q),control = list(maxiter = dm[1]))       fm      summary(fm)
> plot(vy ~ x)      lines(fitted(fm) ~ x)      with(as.list(coef(fm)),
> abline(v = -beta/(2 * gamma)))
>     }    else{          }}
>> vy [1]  77.25989  58.00000  58.00000  58.00000 109.68317  58.00000
>> 80.00000  80.00000  71.33333  69.66667  68.00000  65.33333[13]  58.00000
>> 57.00000  57.00000  57.00000  57.00000  67.00000> alpha_q[1] 67.69373>
>> beta_q[1] 1.321098> gamma_q[1] -0.1129059
> 
> 
> 
> Error in nls(vy ~ Mean(x, alpha, beta, gamma), start = list(alpha =
> alpha_q,  :   singular gradient
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From pdalgd at gmail.com  Sat Dec 12 14:44:45 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 12 Dec 2015 14:44:45 +0100
Subject: [R] stopifnot with logical(0)
In-Reply-To: <22123.61123.948598.140447@stat.math.ethz.ch>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
Message-ID: <7A6AC925-F32B-4674-BDAE-AA3FD112E060@gmail.com>


> On 12 Dec 2015, at 10:54 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> My conclusion: Breaking such a fundamental lemma of logic as 
>  "the empty set is always true"

Umm, that doesn't make sense to me. Surely you mean that "an AND-operation over an empty index set is TRUE"? A similar OR operation is FALSE, i.e. they behave like empty products and sums, respectively.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From murdoch.duncan at gmail.com  Sat Dec 12 15:05:04 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Dec 2015 09:05:04 -0500
Subject: [R] stopifnot with logical(0)
In-Reply-To: <7A6AC925-F32B-4674-BDAE-AA3FD112E060@gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<7A6AC925-F32B-4674-BDAE-AA3FD112E060@gmail.com>
Message-ID: <566C2990.5010703@gmail.com>

On 12/12/2015 8:44 AM, peter dalgaard wrote:
>
>> On 12 Dec 2015, at 10:54 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>
>> My conclusion: Breaking such a fundamental lemma of logic as
>>   "the empty set is always true"
>
> Umm, that doesn't make sense to me. Surely you mean that "an AND-operation over an empty index set is TRUE"? A similar OR operation is FALSE, i.e. they behave like empty products and sums, respectively.
>

How about "the empty set is all true, and all false."


From h.wickham at gmail.com  Sat Dec 12 15:08:54 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Sat, 12 Dec 2015 08:08:54 -0600
Subject: [R] stopifnot with logical(0)
In-Reply-To: <22123.61123.948598.140447@stat.math.ethz.ch>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
Message-ID: <CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>

On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>>     on Fri, 11 Dec 2015 08:20:55 -0800 writes:
>
>     > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>     >>
>     >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>     >>>
>     >>> Hi All,
>     >>>
>     >>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
>     >>> (never?) throw an exception, at least in these cases:
>     >>
>     >> The usual way to test for a length-0 logical object is to use length():
>     >>
>     >> x <- logical(0)
>     >>
>     >> stopifnot( !length(x) & mode(x)=="logical" )
>
>     > I found
>
>     > stopifnot(!length(x), mode(x) == "logical")
>
>     > more helpful when troubleshooting, because it will tell you whether
>     > it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
>     > wrote:
>
>     > stopifnot(!length(x))
>     > stopifnot(mode(x) == "logical")
>
>     > /Henrik
>
> Yes, indeed, thank you Henrik  --- and Jeff Newmiller who's nice
> humorous reply added other relevant points.
>
> As author stopifnot(), I do agree with Dario's  "gut feeling"
> that stopifnot()  "somehow ought to do the right thing"
> in cases such as
>
>    stopifnot(dim(x) == c(3,4))
>
> which is really subtle version of his cases
> {But the gut feeling is wrong, as I argue from now on}.

Personally, I think the problem there is that people forget that == is
vectorised, and for a non-vectorised equality check you really should
use identical:

stopifnot(identical(dim(x), c(3,4)))

Hadley

-- 
http://had.co.nz/


From petretta at unina.it  Sat Dec 12 15:57:52 2015
From: petretta at unina.it (Mario Petretta)
Date: Sat, 12 Dec 2015 15:57:52 +0100
Subject: [R] R:  forest plot metafor
In-Reply-To: <566AD3C6.2010803@dewey.myzen.co.uk>
References: <003e01d132b2$424f8450$c6ee8cf0$@it>
	<566AD3C6.2010803@dewey.myzen.co.uk>
Message-ID: <001501d134ed$7a4dd630$6ee98290$@unina.it>

Many thanks to Professor Michael Dewey for his time.

I apologize for the error about the claim to obtain weights from escalc and I realise that the weights are a function of the fit, not the data.

weights(res) is OK to extract the weights from the fitted object and to put them in the data  frame.

However, I again ask:

Using:
 
 forest(res, showweights=TRUE)

it is possible to directly order the columns, simply placing the weights after effect size and CI?

Sincerely

Mario Petretta




-----Messaggio originale-----
Da: Michael Dewey [mailto:lists at dewey.myzen.co.uk] 
Inviato: venerd? 11 dicembre 2015 14.47
A: Mario Petretta; r-help at r-project.org
Oggetto: Re: [R] forest plot metafor

See below

On 09/12/2015 18:48, Mario Petretta wrote:
> Dear all,
>
>
>
> I use metafor package to generate a forest plot showing the weight of each study in the plot.
>
>
>
> I use the code:
>
> library(metafor)
>
> data(dat.bcg)
>
> res <- rma(ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg, 
> measure="RR",
>
>             slab=paste(author, year, sep=", "), method="REML")
>
> forest(res, showweights=TRUE)
>
>
>
> It is possible to order the columns, placing the weights after effect size and CI?
>

There are some fairly extensive examples in the forest.rma documentation. You will also need some patience to get everything exactly where you want it and looking exactly how you want. Graphics are like that.

>
>
> A further query: using escalc (or rma), it is possible to add the weight of each study (other than yi and vi) to the data (in the example: data.bcg)?
>

You can extract the weights from the fitted object using weights.ram.uni and put them in the data  frame yourself. I am a bit worried about your mention of escalc, you do realise that the weights are a function of the fit, not the data?

So weights(res) should be OK here (not tested).

>
>
> Thanks for your attention
>
>
>
> ________________________
>
>
>
> Mario Petretta
>
> Professor of Internal Medicine
>
> Department of Translational Medical Science
>
> Naples University ?Federico II? - Italy
>
>
>
>
>
>    _____
>
>
>   <http://www.avast.com/>
>
> Questa e-mail ? priva di virus e malware perch? ? attiva la protezione avast! Antivirus <http://www.avast.com/>  .
>
>
>
> Nessun virus nel messaggio.
> Controllato da AVG - www.avg.com
> Versione: 2014.0.4259 / Database dei virus: 3705/7093 - Data di 
> rilascio: 14/02/2014
>
>
>
> ---
> Questa e-mail ? stata controllata per individuare virus con Avast antivirus.
> https://www.avast.com/antivirus
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

--
Michael
http://www.dewey.myzen.co.uk/home.html


From wibbeltjec at hotmail.com  Sat Dec 12 16:35:42 2015
From: wibbeltjec at hotmail.com (Carlijn .)
Date: Sat, 12 Dec 2015 16:35:42 +0100
Subject: [R] Mean effect size in meta-analysis using Metafor
Message-ID: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>



Hi all,



I have a question about doing a meta-analysis, in particular a three-level
meta-analysis using Metafor. 

I have estimated the mean overall effect size of males by using two different
ways: 

1. moderator analysis (male = 0, female = 1) using the whole data set

2. intercept-only model with a subset of the data (only males)



The mean effect size estimated by using the categorical moderator analysis
(1) differs considerably from the overall mean effect size estimated in an
intercept-only model using a subset of the data (2). 



Can someone explain this? Which method gives a better estimation of the
effect?



Thank you in advance!

 		 	   		  
	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sat Dec 12 16:53:23 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 12 Dec 2015 07:53:23 -0800
Subject: [R] Mean effect size in meta-analysis using Metafor
In-Reply-To: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>
References: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>
Message-ID: <CAGxFJbTHOmE8fBWC_HF9_M8DNf7mF82DPydOBYg6hNgACDhaOQ@mail.gmail.com>

**If** I understand correctly, this is because of correlation in
effects due to non-independence between males and females in the data.
As this is primarily a statistical, not an R programming, issue, I
suggest you post on a statistics list like stats.stackexchange.com.
Better yet, talk with a local statistical resource.

If I misunderstand, correction would be appreciated.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 12, 2015 at 7:35 AM, Carlijn . <wibbeltjec at hotmail.com> wrote:
>
>
> Hi all,
>
>
>
> I have a question about doing a meta-analysis, in particular a three-level
> meta-analysis using Metafor.
>
> I have estimated the mean overall effect size of males by using two different
> ways:
>
> 1. moderator analysis (male = 0, female = 1) using the whole data set
>
> 2. intercept-only model with a subset of the data (only males)
>
>
>
> The mean effect size estimated by using the categorical moderator analysis
> (1) differs considerably from the overall mean effect size estimated in an
> intercept-only model using a subset of the data (2).
>
>
>
> Can someone explain this? Which method gives a better estimation of the
> effect?
>
>
>
> Thank you in advance!
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Sat Dec 12 16:57:28 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Dec 2015 10:57:28 -0500
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
Message-ID: <566C43E8.4010004@gmail.com>

On 12/12/2015 9:08 AM, Hadley Wickham wrote:
> On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>>>      on Fri, 11 Dec 2015 08:20:55 -0800 writes:
>>
>>      > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>>      >>
>>      >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>>      >>>
>>      >>> Hi All,
>>      >>>
>>      >>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
>>      >>> (never?) throw an exception, at least in these cases:
>>      >>
>>      >> The usual way to test for a length-0 logical object is to use length():
>>      >>
>>      >> x <- logical(0)
>>      >>
>>      >> stopifnot( !length(x) & mode(x)=="logical" )
>>
>>      > I found
>>
>>      > stopifnot(!length(x), mode(x) == "logical")
>>
>>      > more helpful when troubleshooting, because it will tell you whether
>>      > it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
>>      > wrote:
>>
>>      > stopifnot(!length(x))
>>      > stopifnot(mode(x) == "logical")
>>
>>      > /Henrik
>>
>> Yes, indeed, thank you Henrik  --- and Jeff Newmiller who's nice
>> humorous reply added other relevant points.
>>
>> As author stopifnot(), I do agree with Dario's  "gut feeling"
>> that stopifnot()  "somehow ought to do the right thing"
>> in cases such as
>>
>>     stopifnot(dim(x) == c(3,4))
>>
>> which is really subtle version of his cases
>> {But the gut feeling is wrong, as I argue from now on}.
>
> Personally, I think the problem there is that people forget that == is
> vectorised, and for a non-vectorised equality check you really should
> use identical:
>
> stopifnot(identical(dim(x), c(3,4)))

identical() is a little pickier than people might think:

 > identical(dim(matrix(0, 3,4)), c(3,4))
[1] FALSE
 > identical(dim(matrix(0, 3,4)), c(3L,4L))
[1] TRUE

Duncan Murdoch


From henrik.bengtsson at gmail.com  Sat Dec 12 16:57:38 2015
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sat, 12 Dec 2015 07:57:38 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
Message-ID: <CAFDcVCQtTczozSMXa3_Kw1NyRJki1ePLqtJOXDZo2GdCaaB-Mw@mail.gmail.com>

On Sat, Dec 12, 2015 at 6:08 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>>>     on Fri, 11 Dec 2015 08:20:55 -0800 writes:
>>
>>     > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>>     >>
>>     >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>>     >>>
>>     >>> Hi All,
>>     >>>
>>     >>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
>>     >>> (never?) throw an exception, at least in these cases:
>>     >>
>>     >> The usual way to test for a length-0 logical object is to use length():
>>     >>
>>     >> x <- logical(0)
>>     >>
>>     >> stopifnot( !length(x) & mode(x)=="logical" )
>>
>>     > I found
>>
>>     > stopifnot(!length(x), mode(x) == "logical")
>>
>>     > more helpful when troubleshooting, because it will tell you whether
>>     > it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
>>     > wrote:
>>
>>     > stopifnot(!length(x))
>>     > stopifnot(mode(x) == "logical")
>>
>>     > /Henrik
>>
>> Yes, indeed, thank you Henrik  --- and Jeff Newmiller who's nice
>> humorous reply added other relevant points.
>>
>> As author stopifnot(), I do agree with Dario's  "gut feeling"
>> that stopifnot()  "somehow ought to do the right thing"
>> in cases such as
>>
>>    stopifnot(dim(x) == c(3,4))
>>
>> which is really subtle version of his cases
>> {But the gut feeling is wrong, as I argue from now on}.
>
> Personally, I think the problem there is that people forget that == is
> vectorised, and for a non-vectorised equality check you really should
> use identical:
>
> stopifnot(identical(dim(x), c(3,4)))

Kids, this one of the rare cases where you should not listen to Hadley
;)  Because,

> x <- matrix(1:12, nrow=3, ncol=4)
> dim(x)
[1] 3 4
> identical(dim(x), c(3,4))
[1] FALSE

Why, because:

> storage.mode(dim(x))
[1] "integer"
> storage.mode(c(3,4))
[1] "double"

My rule of thumb is that identical() is awesome, but you really have
to know the inner bits and pieces (*).  When in doubt, use
all.equal(), e.g.

> all.equal(dim(x), c(3,4))
[1] TRUE

Related to Hadley's point, is that using all(x == y) is risky because
R loops of one of the two vectors if one is longer than the other,
e.g.

> all(dim(x) == c(3,4))
[1] TRUE
> all(dim(x) == c(3,4,3,4))
[1] TRUE
> all(dim(x) == c(3,4,3,4,3,4))
[1] TRUE

so one really need to check the lengths as well, e.g.

> all(length(dim(x)) == length(c(3,4)), dim(x) == c(3,4))
[1] TRUE


(*) ADVANCED: I would say its risky to use:

> identical(dim(x), c(3L,4L))
[1] TRUE

because, who knows, in a future version of R we might see
matrices/arrays that support dimensions longer than
.Machine$integer.max which in case dimensions may be stored as
doubles.  This is what we already have for very long vectors today,
cf. help("length").

Henrik

>
> Hadley
>
> --
> http://had.co.nz/


From lists at dewey.myzen.co.uk  Sat Dec 12 17:12:58 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Sat, 12 Dec 2015 16:12:58 +0000
Subject: [R] Mean effect size in meta-analysis using Metafor
In-Reply-To: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>
References: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>
Message-ID: <566C478A.2010608@dewey.myzen.co.uk>

Dear Carlijn

I wonder whether

http://www.metafor-project.org/doku.php/tips:comp_two_independent_estimates

answers your question? If you had given us an example of your fitting 
procedure we might know for sure.


On 12/12/2015 15:35, Carlijn . wrote:
>
>
> Hi all,
>
>
>
> I have a question about doing a meta-analysis, in particular a three-level
> meta-analysis using Metafor.
>
> I have estimated the mean overall effect size of males by using two different
> ways:
>
> 1. moderator analysis (male = 0, female = 1) using the whole data set
>
> 2. intercept-only model with a subset of the data (only males)
>
>
>
> The mean effect size estimated by using the categorical moderator analysis
> (1) differs considerably from the overall mean effect size estimated in an
> intercept-only model using a subset of the data (2).
>
>
>
> Can someone explain this? Which method gives a better estimation of the
> effect?
>
>
>
> Thank you in advance!
>
>   		 	   		
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From wolfgang.viechtbauer at maastrichtuniversity.nl  Sat Dec 12 17:22:20 2015
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Sat, 12 Dec 2015 17:22:20 +0100
Subject: [R] R:  forest plot metafor
In-Reply-To: <001501d134ed$7a4dd630$6ee98290$@unina.it>
References: <003e01d132b2$424f8450$c6ee8cf0$@it>
	<566AD3C6.2010803@dewey.myzen.co.uk>,
	<001501d134ed$7a4dd630$6ee98290$@unina.it>
Message-ID: <077E31A57DA26E46AB0D493C9966AC730F2460CD18@UM-MAIL4112.unimaas.nl>

No, this is not possible. But you can just add the weights yourself. One difficulty here is that you want the weights to the right of the estimates and corresponding CIs and those are always placed at the right of the figure. One possibility would be to make the right margin large and then use mtext() to place the weights in that margin. An example:

library(metafor)
data(dat.bcg)
dat <- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
res <- rma(yi, vi, data=dat, method="FE")
wi <- formatC(weights(res), format="f", digits=1, width=4)
par(mar=c(5,4,4,6))
forest(res, xlim=c(-7,8), digits=c(2,0))
mtext(wi, side=4, at=13:1, line=4, las=2, adj=1)
par(xpd=TRUE)
abline(h=c(0,14))
par(xpd=FALSE)

Adjust as needed for your data.

Best,
Wolfgang
________________________________________
From: R-help [r-help-bounces at r-project.org] On Behalf Of Mario Petretta [petretta at unina.it]
Sent: Saturday, December 12, 2015 3:57 PM
To: r-help at r-project.org
Subject: [R] R:  forest plot metafor

Many thanks to Professor Michael Dewey for his time.

I apologize for the error about the claim to obtain weights from escalc and I realise that the weights are a function of the fit, not the data.

weights(res) is OK to extract the weights from the fitted object and to put them in the data  frame.

However, I again ask:

Using:

 forest(res, showweights=TRUE)

it is possible to directly order the columns, simply placing the weights after effect size and CI?

Sincerely

Mario Petretta

From maitra.mbox.ignored at inbox.com  Sat Dec 12 17:47:54 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 12 Dec 2015 10:47:54 -0600
Subject: [R] plotmath not available on R 3.2.2
Message-ID: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>

Hi,

I recently needed to install a fresh OS (Fedora 23, where R 3.2.2 is the latest), and which meant everything has to be installed from scratch. In so doing, I got the somewhat familiar:

>install.packages('plotmath')
Installing package into ?/usr/lib64/R/library?
(as ?lib? is unspecified)
Warning message:
package ?plotmath? is not available (for R version 3.2.2) 

Is there a way to get around this, other than to download the tarball and install locally?

Best wishes,
Ranjan


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From marc_schwartz at me.com  Sat Dec 12 17:51:16 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Sat, 12 Dec 2015 10:51:16 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
Message-ID: <6A54128C-6A9B-4777-8779-3CB368873812@me.com>


> On Dec 12, 2015, at 10:47 AM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> 
> Hi,
> 
> I recently needed to install a fresh OS (Fedora 23, where R 3.2.2 is the latest), and which meant everything has to be installed from scratch. In so doing, I got the somewhat familiar:
> 
>> install.packages('plotmath')
> Installing package into ?/usr/lib64/R/library?
> (as ?lib? is unspecified)
> Warning message:
> package ?plotmath? is not available (for R version 3.2.2) 
> 
> Is there a way to get around this, other than to download the tarball and install locally?
> 
> Best wishes,
> Ranjan

plotmath is not a package, it is a function:

  ?plotmath

Regards,

Marc Schwartz


From john.peterson.can at gmail.com  Sat Dec 12 15:43:39 2015
From: john.peterson.can at gmail.com (John Peterson)
Date: Sat, 12 Dec 2015 09:43:39 -0500
Subject: [R] metafor package
In-Reply-To: <077E31A57DA26E46AB0D493C9966AC730F24C4677C@UM-MAIL4112.unimaas.nl>
References: <CAMYNiDpKVi=ZSwoWbYTJCqBx1vJVpQ8U8Fbkndr6qiXq6ZAT=w@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C463BF@UM-MAIL4112.unimaas.nl>
	<CAMYNiDo5fyXH+zBMX+Hf+NJgB5QQSQNLtQKhdruwDP8Rg-3BFw@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C4663D@UM-MAIL4112.unimaas.nl>
	<CAMYNiDqL0NeaS2YAfCPvMm5FF0YpDdMECrevncfnLaZrTJQa3w@mail.gmail.com>
	<077E31A57DA26E46AB0D493C9966AC730F24C4677C@UM-MAIL4112.unimaas.nl>
Message-ID: <CAMYNiDrNHcAC=L-oe++-WYXVfWYN_o6oZxMS6Xp65xGAvqFrcw@mail.gmail.com>

Hi Dr. Viechtbauer,

Thank you very much. Putting in sei=se in forest()  fixed the problem.
Thanks for your help.

On 8 December 2015 at 08:37, Viechtbauer Wolfgang (STAT) <
wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:

> The first and second argument of forest() (or more precisely,
> forest.default()) are for the estimates and the corresponding sampling
> variances, respectively. So, if you do forest(rr, se, ...), then the
> function will interpret the standard errors as if they are variances. So,
> you should do forest(rr, sei=se, ...).
>
> And just in case: In all likelihood, those SEs are for the
> *log-transformed* risk ratios, so you should also pass log-transformed risk
> ratios to the function (and then use 'atransf=exp' so results are shown
> with back-transformed x-axis labels and annotations).
>
> Best,
> Wolfgang
>
> --
> Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
>
> > -----Original Message-----
> > From: John Peterson [mailto:john.peterson.can at gmail.com]
> > Sent: Tuesday, December 08, 2015 14:25
> > To: Viechtbauer Wolfgang (STAT)
> > Cc: R-help at r-project.org
> > Subject: Re: [R] metafor package
> >
> > Hi Dr. Viechtbauer,
> >
> > The code provided in the metafor projects website for subgroup includes
> > fitting a random effects model on the entire dataset and fitting a random
> > effects model within subgroups. When I exactly follow this code, my
> > estimates and confidence intervals for estimate within each subgroup
> > matches with what I get in STATA so it seems to be the correct estimate
> > (and CI). However, I don't want to present an overall effect and I want
> > to present only the effect within each subgroup. In my second attempt, I
> > did not run a random effects model within the entire dataset and only ran
> > the models within each subgroup. I generated a variable corresponding to
> > the estimate(risk ratio) and standard error  which I plugged in the
> > forest() function (i.e. forest(rr, se, .....).  When I run this code, the
> > estimate I get for each subgroup is slightly different than the estimate
> > I get for each subgroup in comparison to when I also included the random
> > effects model for the overall effect. (i.e. my first attempt). Is this
> > the right approach? I was not clear when you said passing the estimates
> > and sampling variances to the forest() function. I created a variable
> > corresponding to the estimate and standard error and plugged those in the
> > forest() function. I am not sure if this is the right approach. Thanks,
> >
> > John
> >
> > On 8 December 2015 at 03:47, Viechtbauer Wolfgang (STAT)
> > <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> > Hi John,
> >
> > Please keep r-help copied on the reply.
> >
> > What's the 'previous model'? How do you get estimates within subgroups
> > that 'includes the overall effect'? I really cannot follow you here.
> >
> > Best,
> > Wolfgang
> >
> > --
> > Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and
> > Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD
> > Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com
> >
> > > -----Original Message-----
> > > From: John Peterson [mailto:john.peterson.can at gmail.com]
> > > Sent: Monday, December 07, 2015 22:14
> > > To: Viechtbauer Wolfgang (STAT)
> > > Subject: Re: [R] metafor package
> > >
> > > Hi Dr. Viechtbauer,
> > > Thank you very much for your reply. I tried your advice and was able to
> > > make a forest plot with only the estimates for each subgroup.For the
> > > estiamte for each subgroup, similar to the previous model, I random
> > > effects model within each subgroup. However, I now find the result for
> > > the estiamte within subgroup to be different thant the result for the
> > > previous model. I have tried analyzing this in STATA and I get the same
> > > result as the model which includes the overall effect. Any advice on
> > what
> > > may be wrong here? Thanks greatly,
> > > John
> > >
> > > On 7 December 2015 at 04:02, Viechtbauer Wolfgang (STAT)
> > > <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> > > The code you posted is totally mangled up, but it's just what can be
> > > found here:
> > >
> > > http://www.metafor-
> > project.org/doku.php/plots:forest_plot_with_subgroups
> > >
> > > If you don't want an overall estimate, just pass the estimates and
> > > corresponding sampling variances to the forest() function (and not the
> > > model object). Use the 'rows' argument to specify where the estimates
> > > will be placed and adjust 'ylim' so give you enough space to leave gaps
> > > for headings and the subgroup estimates. Then fit models within the
> > > subgroups (the 'subset' argument is useful here) and use addpoly() to
> > add
> > > the subgroup estimates in the appropriate rows. With text(), you can
> > add
> > > headings as needed.
> > >
> > > If you use weights() on each subgroup model object, you can get the
> > > subgroup weights (that add up to 100% within each subgroup). It's
> > > probably easiest to just add those values with text() in an appropriate
> > > place to the plot.
> > >
> > > Best,
> > > Wolfgang
> > >
> > > --
> > > Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry
> > and
> > > Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200
> > MD
> > > Maastricht, The Netherlands | +31 (43) 388-4170 |
> > http://www.wvbauer.com
> > >
> > > > -----Original Message-----
> > > > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John
> > > > Peterson
> > > > Sent: Monday, December 07, 2015 01:39
> > > > To: r-help at r-project.org
> > > > Subject: [R] metafor package
> > > >
> > > > Hi Everyone,
> > > >
> > > > I am conducting a meta-analysis using the metafor package. I am
> > > > interested
> > > > in obtaining an estimate by subgroup only without showing an overall
> > > > effect. This is directly from the metafor website. How would i modify
> > > > this
> > > > code to only show subgroup effects? Further, I want to show weights
> > by
> > > > subgroup. The option showweights=TRUE does not display weights by
> > > > subgroup
> > > > but by the weight of each study in comparison to all studies (and not
> > > the
> > > > subgroup). You help would be appreciated.
> >
> > [snip garbled code]
>

	[[alternative HTML version deleted]]


From maitra.mbox.ignored at inbox.com  Sat Dec 12 18:32:20 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 12 Dec 2015 11:32:20 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <6A54128C-6A9B-4777-8779-3CB368873812@me.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
Message-ID: <20151212113220.26627747a864cc283a91810e@inbox.com>

On Sat, 12 Dec 2015 10:51:16 -0600 Marc Schwartz <marc_schwartz at me.com> wrote:

> 
> > On Dec 12, 2015, at 10:47 AM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> > 
> > Hi,
> > 
> > I recently needed to install a fresh OS (Fedora 23, where R 3.2.2 is the latest), and which meant everything has to be installed from scratch. In so doing, I got the somewhat familiar:
> > 
> >> install.packages('plotmath')
> > Installing package into ?/usr/lib64/R/library?
> > (as ?lib? is unspecified)
> > Warning message:
> > package ?plotmath? is not available (for R version 3.2.2) 
> > 
> > Is there a way to get around this, other than to download the tarball and install locally?
> > 
> > Best wishes,
> > Ranjan
> 
> plotmath is not a package, it is a function:
> 
>   ?plotmath

Sorry, you are right -- my mistake: the package is grDevices. 

> install.packages('grDevices')
Installing package into ?/usr/lib64/R/library?
(as ?lib? is unspecified)
Warning message:
package ?grDevices? is not available (for R version 3.2.2) 

However, this is no longer an issue for me: I seem to have it installed on this new install (of last week). 

Sorry about the false alarm.

Best wishes,
Ranjan 



> Regards,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
FREE ONLINE PHOTOSHARING - Share your photos online with your friends and family!
Visit http://www.inbox.com/photosharing to find out more!


From murdoch.duncan at gmail.com  Sat Dec 12 18:42:49 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Dec 2015 12:42:49 -0500
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <20151212113220.26627747a864cc283a91810e@inbox.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
Message-ID: <566C5C99.3040605@gmail.com>

On 12/12/2015 12:32 PM, Ranjan Maitra wrote:
> On Sat, 12 Dec 2015 10:51:16 -0600 Marc Schwartz <marc_schwartz at me.com> wrote:
>
>>
>>> On Dec 12, 2015, at 10:47 AM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
>>>
>>> Hi,
>>>
>>> I recently needed to install a fresh OS (Fedora 23, where R 3.2.2 is the latest), and which meant everything has to be installed from scratch. In so doing, I got the somewhat familiar:
>>>
>>>> install.packages('plotmath')
>>> Installing package into ?/usr/lib64/R/library?
>>> (as ?lib? is unspecified)
>>> Warning message:
>>> package ?plotmath? is not available (for R version 3.2.2)
>>>
>>> Is there a way to get around this, other than to download the tarball and install locally?
>>>
>>> Best wishes,
>>> Ranjan
>>
>> plotmath is not a package, it is a function:
>>
>>    ?plotmath
>
> Sorry, you are right -- my mistake: the package is grDevices.
>
>> install.packages('grDevices')
> Installing package into ?/usr/lib64/R/library?
> (as ?lib? is unspecified)
> Warning message:
> package ?grDevices? is not available (for R version 3.2.2)
>
> However, this is no longer an issue for me: I seem to have it installed on this new install (of last week).
>
> Sorry about the false alarm.

grDevices is a "base" package, so it will always be present with a 
proper R installation, and it is only updated when R itself is updated.

Perhaps install.packages() should give a different error message when 
you attempt to install a base package.

Duncan Murdoch


From marc_schwartz at me.com  Sat Dec 12 18:49:58 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Sat, 12 Dec 2015 11:49:58 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <20151212113220.26627747a864cc283a91810e@inbox.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
Message-ID: <425B6EF3-69B8-4D78-A68A-1C1F707C41AE@me.com>


> On Dec 12, 2015, at 11:32 AM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> 
> On Sat, 12 Dec 2015 10:51:16 -0600 Marc Schwartz <marc_schwartz at me.com> wrote:
> 
>> 
>>> On Dec 12, 2015, at 10:47 AM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
>>> 
>>> Hi,
>>> 
>>> I recently needed to install a fresh OS (Fedora 23, where R 3.2.2 is the latest), and which meant everything has to be installed from scratch. In so doing, I got the somewhat familiar:
>>> 
>>>> install.packages('plotmath')
>>> Installing package into ?/usr/lib64/R/library?
>>> (as ?lib? is unspecified)
>>> Warning message:
>>> package ?plotmath? is not available (for R version 3.2.2) 
>>> 
>>> Is there a way to get around this, other than to download the tarball and install locally?
>>> 
>>> Best wishes,
>>> Ranjan
>> 
>> plotmath is not a package, it is a function:
>> 
>>  ?plotmath
> 
> Sorry, you are right -- my mistake: the package is grDevices. 
> 
>> install.packages('grDevices')
> Installing package into ?/usr/lib64/R/library?
> (as ?lib? is unspecified)
> Warning message:
> package ?grDevices? is not available (for R version 3.2.2) 
> 
> However, this is no longer an issue for me: I seem to have it installed on this new install (of last week). 
> 
> Sorry about the false alarm.
> 
> Best wishes,
> Ranjan 


A couple of things:

First, there is a SIG list specifically for R on Fedora and RHEL distributions and their derivatives:

  https://stat.ethz.ch/mailman/listinfo/r-sig-fedora

Second, how did you install R? If you used the precompiled binary RPMS for Fedora that are available by using 'yum':

  sudo yum install R

then the standard R components and their dependencies, that are part of the default distribution, would be installed via the precompiled binary RPMS.

There is no need to install the base or recommended packages separately.

That is the easiest way to install R on Fedora. Yes, it may be a version release or two out of date, especially since 3.2.3 was just released this past week. It will typically take a few days or perhaps a bit longer, before the RPM maintainers release a new binary.

Regards,

Marc Schwartz


From maitra.mbox.ignored at inbox.com  Sat Dec 12 19:33:32 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 12 Dec 2015 12:33:32 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <425B6EF3-69B8-4D78-A68A-1C1F707C41AE@me.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
	<425B6EF3-69B8-4D78-A68A-1C1F707C41AE@me.com>
Message-ID: <20151212123332.6cce9203b04d2644fadbadf5@inbox.com>

> A couple of things:
> 
> First, there is a SIG list specifically for R on Fedora and RHEL distributions and their derivatives:
> 
>   https://stat.ethz.ch/mailman/listinfo/r-sig-fedora
> 
> Second, how did you install R? If you used the precompiled binary RPMS for Fedora that are available by using 'yum':
> 
>   sudo yum install R

Thanks! I install using 

sudo dnf install R-devel libRmath-devel

> then the standard R components and their dependencies, that are part of the default distribution, would be installed via the precompiled binary RPMS.
> 
> There is no need to install the base or recommended packages separately.
> 
> That is the easiest way to install R on Fedora. Yes, it may be a version release or two out of date, especially since 3.2.3 was just released this past week. It will typically take a few days or perhaps a bit longer, before the RPM maintainers release a new binary.

Thanks! I did not believe that this was a Fedora issue, hence I posted here. As it turns out, I have not been posting on this ML for a while, and indeed, I should have actually waited a bit longer...i.e. I was too hasty.

Best wishes,
Ranjan

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From maitra.mbox.ignored at inbox.com  Sat Dec 12 19:35:55 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 12 Dec 2015 12:35:55 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <566C5C99.3040605@gmail.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
	<566C5C99.3040605@gmail.com>
Message-ID: <20151212123555.d54127791491b65d1816cc02@inbox.com>

> >> install.packages('grDevices')
> > Installing package into ?/usr/lib64/R/library?
> > (as ?lib? is unspecified)
> > Warning message:
> > package ?grDevices? is not available (for R version 3.2.2)
> >
> > However, this is no longer an issue for me: I seem to have it installed on this new install (of last week).
> >
> > Sorry about the false alarm.
> 
> grDevices is a "base" package, so it will always be present with a 
> proper R installation, and it is only updated when R itself is updated.

Yes, that is what I thought so too earlier (and indeed is still true, sorry about that). The whole issue started at my end because I misremembered about plotmath which is neither a function nor a package. 
 
> Perhaps install.packages() should give a different error message when 
> you attempt to install a base package.

Yes, that would be more desirable.

Best wishes,
Ranjan

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From marc_schwartz at me.com  Sat Dec 12 20:02:04 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Sat, 12 Dec 2015 13:02:04 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <20151212123332.6cce9203b04d2644fadbadf5@inbox.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
	<425B6EF3-69B8-4D78-A68A-1C1F707C41AE@me.com>
	<20151212123332.6cce9203b04d2644fadbadf5@inbox.com>
Message-ID: <A4EAF1B1-F734-433E-A55C-BBFCC9DB8521@me.com>


> On Dec 12, 2015, at 12:33 PM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> 
>> A couple of things:
>> 
>> First, there is a SIG list specifically for R on Fedora and RHEL distributions and their derivatives:
>> 
>>  https://stat.ethz.ch/mailman/listinfo/r-sig-fedora
>> 
>> Second, how did you install R? If you used the precompiled binary RPMS for Fedora that are available by using 'yum':
>> 
>>  sudo yum install R
> 
> Thanks! I install using 
> 
> sudo dnf install R-devel libRmath-devel


Ok, I have not used Fedora since early 2009, when I moved to Macs, but it looks like 'dnf' is the follow on package manager to 'yum', starting with Fedora 18 in parallel and is now the default, replacing yum since Fedora 22.

In either case, you should be using:

  sudo dnf install R

where R is a meta-package, that will install all of R and its dependencies. You are obviating the value of the package manager by using the incantation that you have above. You don't want to install the individual RPMs via the package manager separately and that is likely what is getting you into trouble here.


> 
>> then the standard R components and their dependencies, that are part of the default distribution, would be installed via the precompiled binary RPMS.
>> 
>> There is no need to install the base or recommended packages separately.
>> 
>> That is the easiest way to install R on Fedora. Yes, it may be a version release or two out of date, especially since 3.2.3 was just released this past week. It will typically take a few days or perhaps a bit longer, before the RPM maintainers release a new binary.
> 
> Thanks! I did not believe that this was a Fedora issue, hence I posted here. As it turns out, I have not been posting on this ML for a while, and indeed, I should have actually waited a bit longer...i.e. I was too hasty.


The Fedora SIG list is for what would typically be OS specific issues, that would not be applicable to other useRs on other Linuxen, Windows and OS X. Installation problems would typically be included in that category.

Regards,

Marc


> 
> Best wishes,
> Ranjan


From maitra.mbox.ignored at inbox.com  Sat Dec 12 20:23:45 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 12 Dec 2015 13:23:45 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <A4EAF1B1-F734-433E-A55C-BBFCC9DB8521@me.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
	<425B6EF3-69B8-4D78-A68A-1C1F707C41AE@me.com>
	<20151212123332.6cce9203b04d2644fadbadf5@inbox.com>
	<A4EAF1B1-F734-433E-A55C-BBFCC9DB8521@me.com>
Message-ID: <20151212132345.62d6654f0a5e462a18f6487f@inbox.com>

OK, I don't know about this thread any more, but installing the developmental packages brings in the core as dependencies. The developmental packages are if one wants to additionally develop (eg build R packages, etc) so this is enough (to also install R).

As an example, if you delete R (or R-core), then R-devel will be erased because the first is a dependency.

HTH, for the matter of a record. 

Thanks again for the responses!

Best wishes,
Ranjan



On Sat, 12 Dec 2015 13:02:04 -0600 Marc Schwartz <marc_schwartz at me.com> wrote:

> 
> > On Dec 12, 2015, at 12:33 PM, Ranjan Maitra <maitra.mbox.ignored at inbox.com> wrote:
> > 
> >> A couple of things:
> >> 
> >> First, there is a SIG list specifically for R on Fedora and RHEL distributions and their derivatives:
> >> 
> >>  https://stat.ethz.ch/mailman/listinfo/r-sig-fedora
> >> 
> >> Second, how did you install R? If you used the precompiled binary RPMS for Fedora that are available by using 'yum':
> >> 
> >>  sudo yum install R
> > 
> > Thanks! I install using 
> > 
> > sudo dnf install R-devel libRmath-devel
> 
> 
> Ok, I have not used Fedora since early 2009, when I moved to Macs, but it looks like 'dnf' is the follow on package manager to 'yum', starting with Fedora 18 in parallel and is now the default, replacing yum since Fedora 22.
> 
> In either case, you should be using:
> 
>   sudo dnf install R
> 
> where R is a meta-package, that will install all of R and its dependencies. You are obviating the value of the package manager by using the incantation that you have above. You don't want to install the individual RPMs via the package manager separately and that is likely what is getting you into trouble here.
> 
> 
> > 
> >> then the standard R components and their dependencies, that are part of the default distribution, would be installed via the precompiled binary RPMS.
> >> 
> >> There is no need to install the base or recommended packages separately.
> >> 
> >> That is the easiest way to install R on Fedora. Yes, it may be a version release or two out of date, especially since 3.2.3 was just released this past week. It will typically take a few days or perhaps a bit longer, before the RPM maintainers release a new binary.
> > 
> > Thanks! I did not believe that this was a Fedora issue, hence I posted here. As it turns out, I have not been posting on this ML for a while, and indeed, I should have actually waited a bit longer...i.e. I was too hasty.
> 
> 
> The Fedora SIG list is for what would typically be OS specific issues, that would not be applicable to other useRs on other Linuxen, Windows and OS X. Installation problems would typically be included in that category.
> 
> Regards,
> 
> Marc
> 
> 
> > 
> > Best wishes,
> > Ranjan
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 


-- 
Important Notice: This mailbox is ignored: e-mails are set to be deleted on receipt. Please respond to the mailing list if appropriate. For those needing to send personal or professional e-mail, please use appropriate addresses.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From maechler at stat.math.ethz.ch  Sat Dec 12 20:51:27 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 12 Dec 2015 20:51:27 +0100
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
Message-ID: <22124.31423.70233.408011@stat.math.ethz.ch>

>>>>> Hadley Wickham <h.wickham at gmail.com>
>>>>>     on Sat, 12 Dec 2015 08:08:54 -0600 writes:

    > On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:
    >>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com> on
    >>>>>>> Fri, 11 Dec 2015 08:20:55 -0800 writes:
    >> 
    >> > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius
    >> <dwinsemius at comcast.net> wrote:
    >> >>
    >> >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi
    >> <dario.beraldi at gmail.com> wrote:
    >> >>>
    >> >>> Hi All,
    >> >>>
    >> >>> I'd like to understand the reason why
    >> stopifnot(logical(0) == x) doesn't >>> (never?) throw an
    >> exception, at least in these cases:
    >> >>
    >> >> The usual way to test for a length-0 logical object is
    >> to use length():
    >> >>
    >> >> x <- logical(0)
    >> >>
    >> >> stopifnot( !length(x) & mode(x)=="logical" )
    >> 
    >> > I found
    >> 
    >> > stopifnot(!length(x), mode(x) == "logical")
    >> 
    >> > more helpful when troubleshooting, because it will tell
    >> you whether > it's !length(x) or mode(x) == "logical"
    >> that is FALSE.  It's as if you > wrote:
    >> 
    >> > stopifnot(!length(x)) > stopifnot(mode(x) == "logical")
    >> 
    >> > /Henrik
    >> 
    >> Yes, indeed, thank you Henrik --- and Jeff Newmiller
    >> who's nice humorous reply added other relevant points.
    >> 
    >> As author stopifnot(), I do agree with Dario's "gut
    >> feeling" that stopifnot() "somehow ought to do the right
    >> thing" in cases such as
    >> 
    >> stopifnot(dim(x) == c(3,4))
    >> 
    >> which is really subtle version of his cases {But the gut
    >> feeling is wrong, as I argue from now on}.

    > Personally, I think the problem there is that people
    > forget that == is vectorised, and for a non-vectorised
    > equality check you really should use identical:

    > stopifnot(identical(dim(x), c(3,4)))

You are right "in theory"  but practice is less easy:
identical() tends to be  too subtle for many users ... even
yourself (;-), not really of course!),  Hadley, in the above case:

Your stopifnot() would *always* stop, i.e., signal an error
because typically all dim() methods return integer, and c(3,4)
is double.
So, if even Hadley gets it wrong so easily, I wonder if its good
to advertize to always use  identical() in such cases.
I indeed would quite often use identical() in such tests, and
you'd too and would quickly find and fix the "trap" of course..
So you are mostly right also in my opinion...

Martin


From murdoch.duncan at gmail.com  Sat Dec 12 21:00:56 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 12 Dec 2015 15:00:56 -0500
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <20151212123555.d54127791491b65d1816cc02@inbox.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
	<566C5C99.3040605@gmail.com>
	<20151212123555.d54127791491b65d1816cc02@inbox.com>
Message-ID: <566C7CF8.1040304@gmail.com>

On 12/12/2015 1:35 PM, Ranjan Maitra wrote:
>>>> install.packages('grDevices')
>>> Installing package into ?/usr/lib64/R/library?
>>> (as ?lib? is unspecified)
>>> Warning message:
>>> package ?grDevices? is not available (for R version 3.2.2)
>>>
>>> However, this is no longer an issue for me: I seem to have it installed on this new install (of last week).
>>>
>>> Sorry about the false alarm.
>>
>> grDevices is a "base" package, so it will always be present with a
>> proper R installation, and it is only updated when R itself is updated.
>
> Yes, that is what I thought so too earlier (and indeed is still true, sorry about that). The whole issue started at my end because I misremembered about plotmath which is neither a function nor a package.
>
>> Perhaps install.packages() should give a different error message when
>> you attempt to install a base package.
>
> Yes, that would be more desirable.

Done now (as a warning, not an error).

Duncan Murdoch


From maitra.mbox.ignored at inbox.com  Sat Dec 12 21:06:35 2015
From: maitra.mbox.ignored at inbox.com (Ranjan Maitra)
Date: Sat, 12 Dec 2015 14:06:35 -0600
Subject: [R] plotmath not available on R 3.2.2
In-Reply-To: <566C7CF8.1040304@gmail.com>
References: <20151212104754.15c82182bf7869444c8c67ce@inbox.com>
	<6A54128C-6A9B-4777-8779-3CB368873812@me.com>
	<20151212113220.26627747a864cc283a91810e@inbox.com>
	<566C5C99.3040605@gmail.com>
	<20151212123555.d54127791491b65d1816cc02@inbox.com>
	<566C7CF8.1040304@gmail.com>
Message-ID: <20151212140635.aea12acd464bac71c9f7fa3a@inbox.com>

> >
> > Yes, that is what I thought so too earlier (and indeed is still true, sorry about that). The whole issue started at my end because I misremembered about plotmath which is neither a function nor a package.
> >
> >> Perhaps install.packages() should give a different error message when
> >> you attempt to install a base package.
> >
> > Yes, that would be more desirable.
> 
> Done now (as a warning, not an error).

Great! Thank you, Duncan!!

Best wishes,
Ranjan

____________________________________________________________
GET FREE 5GB EMAIL - Check out spam free email with many cool features!
Visit http://www.inbox.com/email to find out more!


From matcoelhosilva at yahoo.com.br  Sat Dec 12 19:39:09 2015
From: matcoelhosilva at yahoo.com.br (Mateus Coelho)
Date: Sat, 12 Dec 2015 18:39:09 +0000 (UTC)
Subject: [R] Problem to install rattle Library on RStudio Version 0.99.486
References: <886989383.195234.1449945549237.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <886989383.195234.1449945549237.JavaMail.yahoo@mail.yahoo.com>

Although I already tried to install this package at my RStudio, I can't manage to do it. Would you have any hint to help me solving this problem.
Thanks and greetings?Mateus Coelho Silva
br.linkedin.com/in/mateuscoelhos
(Tel/WApp: +55 31 88535135)
-Graduando em Engenharia de Controle e Automa??o - UFMG
	[[alternative HTML version deleted]]


From marongiu.luigi at gmail.com  Sun Dec 13 17:17:31 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Sun, 13 Dec 2015 16:17:31 +0000
Subject: [R] define number of clusters in kmeans/apcluster analysis
Message-ID: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>

Dear all,
I am trying to do some cluster analysis, both with the base R and the
apcluster. Both methods give 2 clusters, which is what I am looking
for since I am interested in identifying positive and negative
results. However  I could not find a way to fine-tuning the analysis
in order to properly allocate the points; essentially the negative
points should be all those in the lower left portion of the plot (see
example) but some in the top centre are also given to the negative
cluster.
So how can I change the parameters to get better results?
Thank you
L

>>>
x <- c(3.15,    3.07,    2,    3,    2.97,    45,    3.21,    45,
40.55,    2,    22.09,    2.47,    2.97,    2.77,    2.6,    7.35,
4.11,    37.12,    2.73,    36.36,    45,    2.33,    2.49,    45,
2.4,    2.74,    2.64,    45,    2.47,    38.1,    2.47,    37.4,
2.77,    2.37,    45,    2.69,    2.97,    2.7,    2,    2,    2.55,
 11.86,    2.51,    2.68,    2.31,    2.6,    2.45,    2,    2.72,
2.57,    2.09,    3.04,    45,    45,    2.13,    43.82,    2.92,
4.94,    24.82,    2.64,    4.96,    3.65,    2.67,    2.64,    8.04,
  4.56,    44.87,    37.42,    45,    6.2,    2.84,    4.08,    2,
5.03,    2.27,    44.89,    2.41,    2.47,    2.78,    37.47,    45,
 2.76,    45,    2.51,    2.8,    44.8,    6.2,    2.87,    2.23,
18.32,    3.14,    2.1,    2.38,    2.72,    2,    2,    44.41,
3.15,    3.06,    4.8,    2.77,    2.8,    2.71,    44.77,    2.25,
2.69,    28.38,    2,    2.95,    45,    2.79,    2.46,    2.61,
2.78,    2.94,    38.47,    3.29,    2.89,    2.4,    2.23,    2.62,
 4.21,    2.61,    2.81,    2.41,    41.98,    2.39,    36.41,
44.84,    4.73,    2,    2.66,    4.57,    3.01,    42.64,    2.04,
5.49,    15.48,    3.08,    2.7,    2,    2,    2.09,    2,    2.29,
 2.92,    3.39,    3.1,    2,    6.14,    7.03,    4.77,    2.55,
32.36,    20.61,    3.09,    4.46,    44.75,    2,    2.73,    2,
36.05,    3.61,    34.84,    2.69,    5.28,    3.04,    45,    2.47,
 2.58,    2.16,    2.59,    45,    44.08,    2,    37.05,    2.48,
2.46,    38.71,    7.32,    2.95,    2.8,    44.58,    42.24,
36.99,    13.84,    45,    2,    2,    2.38,    45,    45,    43.59,
 2.69,    2.81,    3.05,    2.8,    4.65,    45,    41.46,    2.33,
7.12,    19.18,    4.82,    4.76,    2.51,    3.1,    2.74,    4.99,
 38.06,    2.53,    2.94,    2.93,    6.59,    2.72,    2.94,    2.56,
   2.91,    44.79,    2.98,    42.95,    45,    2.63,    38.44,
2.71,    2,    37.92,    2.69,    2.91,    2.65,    44.48,    6.35,
2.56,    21.94,    3.08,    2.6,    45,    2,    2.62,    2.47,
2.62,    2.73,    2.87,    2.83,    4.56,    44.22,    5.15,    5.13,
  2.76,    7.02,    28.61,    4.87,    5.02,    44.35,    2.26,
2.89,    5.26,    38.01,    44.79,    39.26,    2.91,    4.59,
2.69,    2.61,    34.97,    3,    45,    2.81,    2,    2.65,    2,
37.33,    4.69,    3.26,    38.24,    4.97,    4.62,    2.47,    45,
 4.52,    2.73,    15.66,    6.06,    2.79,    2.87,    45,    45,
45,    4.84,    3.05,    4.89,    4.64,    4.92,    2.74,    7.83,
42.31,    2.88,    6.89,    23.06,    2.94,    4.72,    4.55,    5.52,
   4.48,    4.86,    3.12,    7.68,    43.89,    2.82,    2.64,
3.05,    42.95,    2.33,    3.55,    45,    2.79,    2.47,    45,
2.56,    38.33,    2.73,    2.87,    2.61,    3.01,    2.86,    2.74,
  44.46,    44.54,    2.62,    16.94,    2.53,    2.24,    2.72,    2,
   3.1,    2.88,    7.4,    4.64,    8.25,    3.01,    2.86,    2.46,
  5.67,    44.52,    2.47,    2,    29.01,    2.61,    3.23,    12.3,
  3.9,    2.91,    43.99,    36.99,    43.72,    42.29,    2.63,
3.03,    2.85,    2.58,    2.63,    2.73,    2.57,    2.37,    2.57,
 2.75,    44.14,    39.4,    40.02,    3.08,    45,    4.96,    3,
2.83,    2.74,    2.8,    2.8,    18.88,    4.69,    2.51,    4.32,
2,    2.56,    2.81
)
y <- c(0.014,    0.04,    0.001,    0.023,    0.008,    0,    0.008,
 0.001,    -0.001,    0.002,    0.103,    0,    0.013,    0.005,
0.008,    0.001,    0.011,    0.076,    0.005,    0.045,    -0.001,
0,    0.008,    -0.002,    0.002,    0.016,    0.006,    0.001,
0.002,    0.001,    0.004,    0.086,    0.009,    0.011,    0.002,
0.013,    0.019,    0.007,    0,    0.002,    0.024,    0.119,
0.015,    0.009,    0.013,    0.017,    0.009,    0.009,    0.006,
0.012,    0.002,    0.015,    0,    0.001,    0.002,    0.001,
0.007,    0.004,    0.113,    0.016,    0.013,    0.004,    0.015,
0.005,    0.004,    0.007,    0,    0.081,    0.001,    0.002,
0.014,    0.002,    0,    0.01,    0.003,    0.002,    0.004,
0.004,    0.006,    0.064,    0,    0.014,    0,    0.01,    0.019,
0.002,    0.006,    0.005,    0.003,    0.103,    0.007,    0.008,
0.002,    0.013,    0.007,    0.004,    0.001,    0.04,    0.017,
0.018,    0.002,    0.006,    0.011,    0.003,    0.004,    0.008,
0.115,    0,    0.02,    0,    0.012,    0.009,    0.011,    0.013,
0.004,    0.058,    0.019,    0.006,    0.005,    0.004,    0.012,
0.003,    0.003,    0.004,    0.002,    0.001,    0.002,    0.102,
-0.001,    0.008,    0.002,    0.016,    0.023,    0.014,    0.053,
0.009,    0.001,    0.124,    0.009,    0.008,    0.002,    0.002,
0.013,    0.002,    0.001,    0.042,    0.011,    0.009,    0,
0.004,    0.003,    0.002,    0.005,    0,    0.101,    0.013,
0.009,    0.005,    0.002,    0.007,    0.008,    0.067,    0.002,
0.064,    0.028,    0.007,    0.006,    0,    0.007,    0.006,    0,
 0.001,    0.001,    0.001,    0,    0.088,    0.005,    0.008,
0.098,    0.005,    0.019,    0.007,    0.05,    -0.002,    0.002,
0.129,    0.001,    0.004,    -0.001,    0.002,    -0.001,    0,
0.043,    0.018,    0.019,    0.015,    0.003,    0.006,    0.002,
0.001,    0.002,    0.004,    0.097,    0.025,    0.022,    0.007,
0.011,    0.007,    0.013,    0.061,    0.008,    0.013,    0.028,
0.004,    0.013,    0.005,    0.01,    0.004,    0,    0.006,
-0.001,    0.001,    0.01,    0.061,    0.002,    0.004,    0,
0.011,    0.029,    0.018,    0,    0.003,    0.012,    0.085,
0.015,    0.007,    0.002,    0.003,    0.008,    0.002,    0.007,
0.02,    0.011,    0.02,    0.008,    0.001,    0.003,    0.01,
0.014,    0.001,    0.096,    0.027,    0.024,    0,    0.005,
0.006,    0.024,    0.087,    0.001,    0.083,    0.02,    0.009,
0.009,    0.001,    0,    0.019,    0,    0.003,    -0.001,    0.002,
  0,    0.089,    0.016,    0.01,    0.103,    0.003,    0.01,
0.002,    0.008,    0.005,    0.014,    0.1,    0.007,    0.009,
0.011,    -0.001,    0,    0.002,    0.015,    0.036,    0.018,
0.026,    0.009,    0.008,    0.004,    0.001,    0.014,    0.009,
0.1,    0.026,    0.032,    0.008,    0.011,    0.004,    0.013,
0.019,    0.004,    0.02,    0.015,    0.005,    0.013,    -0.001,
0.013,    0.012,    0,    0.01,    0.002,    0.001,    0.013,
0.066,    0.009,    0.005,    0.002,    0.013,    0.025,    0.006,
0,    0,    0.015,    0.121,    0.006,    0.003,    0.008,    0,
0.012,    0.011,    0.003,    0.022,    0.008,    0.032,    0.007,
0.002,    0.006,    0.007,    0,    0.003,    0.11,    0.01,    0.008,
   0,    0.018,    0.008,    0.001,    0.087,    0,    0.028,
0.011,    0.014,    0.007,    0.001,    0.018,    0.033,    0.021,
0.003,    0.003,    0.007,    -0.001,    0.07,    0.022,    0.009,
0.001,    0.007,    0.031,    0.008,    0.013,    0.01,    0.018,
0.125,    0.01,    0.015,    0.006,    0,    0.015,    0.019
)
z <- cbind(x, y)
k <- kmeans(z, 2)
plot(z, col=k$cluster)

library(apcluster)
m <- apclusterK(negDistMat(r=2), z, K=2, verbose=TRUE)
plot(m, z)


From bgunter.4567 at gmail.com  Sun Dec 13 17:28:25 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sun, 13 Dec 2015 08:28:25 -0800
Subject: [R] define number of clusters in kmeans/apcluster analysis
In-Reply-To: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>
References: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>
Message-ID: <CAGxFJbSz_AO0JzLwuQ9LXYRgv7KMoXQ1HtEXhhfeEV6Wr7mkJg@mail.gmail.com>

It sounds to me like you don't understand cluster analysis. You should
not expect perfect "allocation" of points. I suggest that you consult
references in the man pages of your functions or on the web. You might
also find it useful to post on stats.stackexchange.com or a machine
learning help site, as this is a statistical issue, not an R
programming issue AFAICS (corrections welcome if I'm wrong about
this), and so is somewhat OT here.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sun, Dec 13, 2015 at 8:17 AM, Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
> Dear all,
> I am trying to do some cluster analysis, both with the base R and the
> apcluster. Both methods give 2 clusters, which is what I am looking
> for since I am interested in identifying positive and negative
> results. However  I could not find a way to fine-tuning the analysis
> in order to properly allocate the points; essentially the negative
> points should be all those in the lower left portion of the plot (see
> example) but some in the top centre are also given to the negative
> cluster.
> So how can I change the parameters to get better results?
> Thank you
> L
>
>>>>
> x <- c(3.15,    3.07,    2,    3,    2.97,    45,    3.21,    45,
> 40.55,    2,    22.09,    2.47,    2.97,    2.77,    2.6,    7.35,
> 4.11,    37.12,    2.73,    36.36,    45,    2.33,    2.49,    45,
> 2.4,    2.74,    2.64,    45,    2.47,    38.1,    2.47,    37.4,
> 2.77,    2.37,    45,    2.69,    2.97,    2.7,    2,    2,    2.55,
>  11.86,    2.51,    2.68,    2.31,    2.6,    2.45,    2,    2.72,
> 2.57,    2.09,    3.04,    45,    45,    2.13,    43.82,    2.92,
> 4.94,    24.82,    2.64,    4.96,    3.65,    2.67,    2.64,    8.04,
>   4.56,    44.87,    37.42,    45,    6.2,    2.84,    4.08,    2,
> 5.03,    2.27,    44.89,    2.41,    2.47,    2.78,    37.47,    45,
>  2.76,    45,    2.51,    2.8,    44.8,    6.2,    2.87,    2.23,
> 18.32,    3.14,    2.1,    2.38,    2.72,    2,    2,    44.41,
> 3.15,    3.06,    4.8,    2.77,    2.8,    2.71,    44.77,    2.25,
> 2.69,    28.38,    2,    2.95,    45,    2.79,    2.46,    2.61,
> 2.78,    2.94,    38.47,    3.29,    2.89,    2.4,    2.23,    2.62,
>  4.21,    2.61,    2.81,    2.41,    41.98,    2.39,    36.41,
> 44.84,    4.73,    2,    2.66,    4.57,    3.01,    42.64,    2.04,
> 5.49,    15.48,    3.08,    2.7,    2,    2,    2.09,    2,    2.29,
>  2.92,    3.39,    3.1,    2,    6.14,    7.03,    4.77,    2.55,
> 32.36,    20.61,    3.09,    4.46,    44.75,    2,    2.73,    2,
> 36.05,    3.61,    34.84,    2.69,    5.28,    3.04,    45,    2.47,
>  2.58,    2.16,    2.59,    45,    44.08,    2,    37.05,    2.48,
> 2.46,    38.71,    7.32,    2.95,    2.8,    44.58,    42.24,
> 36.99,    13.84,    45,    2,    2,    2.38,    45,    45,    43.59,
>  2.69,    2.81,    3.05,    2.8,    4.65,    45,    41.46,    2.33,
> 7.12,    19.18,    4.82,    4.76,    2.51,    3.1,    2.74,    4.99,
>  38.06,    2.53,    2.94,    2.93,    6.59,    2.72,    2.94,    2.56,
>    2.91,    44.79,    2.98,    42.95,    45,    2.63,    38.44,
> 2.71,    2,    37.92,    2.69,    2.91,    2.65,    44.48,    6.35,
> 2.56,    21.94,    3.08,    2.6,    45,    2,    2.62,    2.47,
> 2.62,    2.73,    2.87,    2.83,    4.56,    44.22,    5.15,    5.13,
>   2.76,    7.02,    28.61,    4.87,    5.02,    44.35,    2.26,
> 2.89,    5.26,    38.01,    44.79,    39.26,    2.91,    4.59,
> 2.69,    2.61,    34.97,    3,    45,    2.81,    2,    2.65,    2,
> 37.33,    4.69,    3.26,    38.24,    4.97,    4.62,    2.47,    45,
>  4.52,    2.73,    15.66,    6.06,    2.79,    2.87,    45,    45,
> 45,    4.84,    3.05,    4.89,    4.64,    4.92,    2.74,    7.83,
> 42.31,    2.88,    6.89,    23.06,    2.94,    4.72,    4.55,    5.52,
>    4.48,    4.86,    3.12,    7.68,    43.89,    2.82,    2.64,
> 3.05,    42.95,    2.33,    3.55,    45,    2.79,    2.47,    45,
> 2.56,    38.33,    2.73,    2.87,    2.61,    3.01,    2.86,    2.74,
>   44.46,    44.54,    2.62,    16.94,    2.53,    2.24,    2.72,    2,
>    3.1,    2.88,    7.4,    4.64,    8.25,    3.01,    2.86,    2.46,
>   5.67,    44.52,    2.47,    2,    29.01,    2.61,    3.23,    12.3,
>   3.9,    2.91,    43.99,    36.99,    43.72,    42.29,    2.63,
> 3.03,    2.85,    2.58,    2.63,    2.73,    2.57,    2.37,    2.57,
>  2.75,    44.14,    39.4,    40.02,    3.08,    45,    4.96,    3,
> 2.83,    2.74,    2.8,    2.8,    18.88,    4.69,    2.51,    4.32,
> 2,    2.56,    2.81
> )
> y <- c(0.014,    0.04,    0.001,    0.023,    0.008,    0,    0.008,
>  0.001,    -0.001,    0.002,    0.103,    0,    0.013,    0.005,
> 0.008,    0.001,    0.011,    0.076,    0.005,    0.045,    -0.001,
> 0,    0.008,    -0.002,    0.002,    0.016,    0.006,    0.001,
> 0.002,    0.001,    0.004,    0.086,    0.009,    0.011,    0.002,
> 0.013,    0.019,    0.007,    0,    0.002,    0.024,    0.119,
> 0.015,    0.009,    0.013,    0.017,    0.009,    0.009,    0.006,
> 0.012,    0.002,    0.015,    0,    0.001,    0.002,    0.001,
> 0.007,    0.004,    0.113,    0.016,    0.013,    0.004,    0.015,
> 0.005,    0.004,    0.007,    0,    0.081,    0.001,    0.002,
> 0.014,    0.002,    0,    0.01,    0.003,    0.002,    0.004,
> 0.004,    0.006,    0.064,    0,    0.014,    0,    0.01,    0.019,
> 0.002,    0.006,    0.005,    0.003,    0.103,    0.007,    0.008,
> 0.002,    0.013,    0.007,    0.004,    0.001,    0.04,    0.017,
> 0.018,    0.002,    0.006,    0.011,    0.003,    0.004,    0.008,
> 0.115,    0,    0.02,    0,    0.012,    0.009,    0.011,    0.013,
> 0.004,    0.058,    0.019,    0.006,    0.005,    0.004,    0.012,
> 0.003,    0.003,    0.004,    0.002,    0.001,    0.002,    0.102,
> -0.001,    0.008,    0.002,    0.016,    0.023,    0.014,    0.053,
> 0.009,    0.001,    0.124,    0.009,    0.008,    0.002,    0.002,
> 0.013,    0.002,    0.001,    0.042,    0.011,    0.009,    0,
> 0.004,    0.003,    0.002,    0.005,    0,    0.101,    0.013,
> 0.009,    0.005,    0.002,    0.007,    0.008,    0.067,    0.002,
> 0.064,    0.028,    0.007,    0.006,    0,    0.007,    0.006,    0,
>  0.001,    0.001,    0.001,    0,    0.088,    0.005,    0.008,
> 0.098,    0.005,    0.019,    0.007,    0.05,    -0.002,    0.002,
> 0.129,    0.001,    0.004,    -0.001,    0.002,    -0.001,    0,
> 0.043,    0.018,    0.019,    0.015,    0.003,    0.006,    0.002,
> 0.001,    0.002,    0.004,    0.097,    0.025,    0.022,    0.007,
> 0.011,    0.007,    0.013,    0.061,    0.008,    0.013,    0.028,
> 0.004,    0.013,    0.005,    0.01,    0.004,    0,    0.006,
> -0.001,    0.001,    0.01,    0.061,    0.002,    0.004,    0,
> 0.011,    0.029,    0.018,    0,    0.003,    0.012,    0.085,
> 0.015,    0.007,    0.002,    0.003,    0.008,    0.002,    0.007,
> 0.02,    0.011,    0.02,    0.008,    0.001,    0.003,    0.01,
> 0.014,    0.001,    0.096,    0.027,    0.024,    0,    0.005,
> 0.006,    0.024,    0.087,    0.001,    0.083,    0.02,    0.009,
> 0.009,    0.001,    0,    0.019,    0,    0.003,    -0.001,    0.002,
>   0,    0.089,    0.016,    0.01,    0.103,    0.003,    0.01,
> 0.002,    0.008,    0.005,    0.014,    0.1,    0.007,    0.009,
> 0.011,    -0.001,    0,    0.002,    0.015,    0.036,    0.018,
> 0.026,    0.009,    0.008,    0.004,    0.001,    0.014,    0.009,
> 0.1,    0.026,    0.032,    0.008,    0.011,    0.004,    0.013,
> 0.019,    0.004,    0.02,    0.015,    0.005,    0.013,    -0.001,
> 0.013,    0.012,    0,    0.01,    0.002,    0.001,    0.013,
> 0.066,    0.009,    0.005,    0.002,    0.013,    0.025,    0.006,
> 0,    0,    0.015,    0.121,    0.006,    0.003,    0.008,    0,
> 0.012,    0.011,    0.003,    0.022,    0.008,    0.032,    0.007,
> 0.002,    0.006,    0.007,    0,    0.003,    0.11,    0.01,    0.008,
>    0,    0.018,    0.008,    0.001,    0.087,    0,    0.028,
> 0.011,    0.014,    0.007,    0.001,    0.018,    0.033,    0.021,
> 0.003,    0.003,    0.007,    -0.001,    0.07,    0.022,    0.009,
> 0.001,    0.007,    0.031,    0.008,    0.013,    0.01,    0.018,
> 0.125,    0.01,    0.015,    0.006,    0,    0.015,    0.019
> )
> z <- cbind(x, y)
> k <- kmeans(z, 2)
> plot(z, col=k$cluster)
>
> library(apcluster)
> m <- apclusterK(negDistMat(r=2), z, K=2, verbose=TRUE)
> plot(m, z)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From boris.steipe at utoronto.ca  Sun Dec 13 18:04:52 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Sun, 13 Dec 2015 12:04:52 -0500
Subject: [R] define number of clusters in kmeans/apcluster analysis
In-Reply-To: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>
References: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>
Message-ID: <888EE1F6-985E-4D1C-ACD1-2388B7389BF9@utoronto.ca>

You could use the clValid package to run your problem through a variety of different algorithms and evaluate cluster quality, you will learn a lot. https://cran.r-project.org/web/views/Cluster.html gives you many more options. All of the algorithms I am aware of have tunable parameters - but what is the "best" approach really depends on the detailed context of the problem domain. Sometimes dimension reduction with PCA, or even the magical tsne package is useful for preprocessing data. Finally, if you already know how your data set should be partitioned, perhaps you are really looking for a machine learning approach: see here https://cran.r-project.org/web/views/MachineLearning.html


B.



On Dec 13, 2015, at 11:17 AM, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:

> Dear all,
> I am trying to do some cluster analysis, both with the base R and the
> apcluster. Both methods give 2 clusters, which is what I am looking
> for since I am interested in identifying positive and negative
> results. However  I could not find a way to fine-tuning the analysis
> in order to properly allocate the points; essentially the negative
> points should be all those in the lower left portion of the plot (see
> example) but some in the top centre are also given to the negative
> cluster.
> So how can I change the parameters to get better results?
> Thank you
> L
> 
>>>> 
> x <- c(3.15,    3.07,    2,    3,    2.97,    45,    3.21,    45,
> 40.55,    2,    22.09,    2.47,    2.97,    2.77,    2.6,    7.35,
> 4.11,    37.12,    2.73,    36.36,    45,    2.33,    2.49,    45,
> 2.4,    2.74,    2.64,    45,    2.47,    38.1,    2.47,    37.4,
> 2.77,    2.37,    45,    2.69,    2.97,    2.7,    2,    2,    2.55,
> 11.86,    2.51,    2.68,    2.31,    2.6,    2.45,    2,    2.72,
> 2.57,    2.09,    3.04,    45,    45,    2.13,    43.82,    2.92,
> 4.94,    24.82,    2.64,    4.96,    3.65,    2.67,    2.64,    8.04,
>  4.56,    44.87,    37.42,    45,    6.2,    2.84,    4.08,    2,
> 5.03,    2.27,    44.89,    2.41,    2.47,    2.78,    37.47,    45,
> 2.76,    45,    2.51,    2.8,    44.8,    6.2,    2.87,    2.23,
> 18.32,    3.14,    2.1,    2.38,    2.72,    2,    2,    44.41,
> 3.15,    3.06,    4.8,    2.77,    2.8,    2.71,    44.77,    2.25,
> 2.69,    28.38,    2,    2.95,    45,    2.79,    2.46,    2.61,
> 2.78,    2.94,    38.47,    3.29,    2.89,    2.4,    2.23,    2.62,
> 4.21,    2.61,    2.81,    2.41,    41.98,    2.39,    36.41,
> 44.84,    4.73,    2,    2.66,    4.57,    3.01,    42.64,    2.04,
> 5.49,    15.48,    3.08,    2.7,    2,    2,    2.09,    2,    2.29,
> 2.92,    3.39,    3.1,    2,    6.14,    7.03,    4.77,    2.55,
> 32.36,    20.61,    3.09,    4.46,    44.75,    2,    2.73,    2,
> 36.05,    3.61,    34.84,    2.69,    5.28,    3.04,    45,    2.47,
> 2.58,    2.16,    2.59,    45,    44.08,    2,    37.05,    2.48,
> 2.46,    38.71,    7.32,    2.95,    2.8,    44.58,    42.24,
> 36.99,    13.84,    45,    2,    2,    2.38,    45,    45,    43.59,
> 2.69,    2.81,    3.05,    2.8,    4.65,    45,    41.46,    2.33,
> 7.12,    19.18,    4.82,    4.76,    2.51,    3.1,    2.74,    4.99,
> 38.06,    2.53,    2.94,    2.93,    6.59,    2.72,    2.94,    2.56,
>   2.91,    44.79,    2.98,    42.95,    45,    2.63,    38.44,
> 2.71,    2,    37.92,    2.69,    2.91,    2.65,    44.48,    6.35,
> 2.56,    21.94,    3.08,    2.6,    45,    2,    2.62,    2.47,
> 2.62,    2.73,    2.87,    2.83,    4.56,    44.22,    5.15,    5.13,
>  2.76,    7.02,    28.61,    4.87,    5.02,    44.35,    2.26,
> 2.89,    5.26,    38.01,    44.79,    39.26,    2.91,    4.59,
> 2.69,    2.61,    34.97,    3,    45,    2.81,    2,    2.65,    2,
> 37.33,    4.69,    3.26,    38.24,    4.97,    4.62,    2.47,    45,
> 4.52,    2.73,    15.66,    6.06,    2.79,    2.87,    45,    45,
> 45,    4.84,    3.05,    4.89,    4.64,    4.92,    2.74,    7.83,
> 42.31,    2.88,    6.89,    23.06,    2.94,    4.72,    4.55,    5.52,
>   4.48,    4.86,    3.12,    7.68,    43.89,    2.82,    2.64,
> 3.05,    42.95,    2.33,    3.55,    45,    2.79,    2.47,    45,
> 2.56,    38.33,    2.73,    2.87,    2.61,    3.01,    2.86,    2.74,
>  44.46,    44.54,    2.62,    16.94,    2.53,    2.24,    2.72,    2,
>   3.1,    2.88,    7.4,    4.64,    8.25,    3.01,    2.86,    2.46,
>  5.67,    44.52,    2.47,    2,    29.01,    2.61,    3.23,    12.3,
>  3.9,    2.91,    43.99,    36.99,    43.72,    42.29,    2.63,
> 3.03,    2.85,    2.58,    2.63,    2.73,    2.57,    2.37,    2.57,
> 2.75,    44.14,    39.4,    40.02,    3.08,    45,    4.96,    3,
> 2.83,    2.74,    2.8,    2.8,    18.88,    4.69,    2.51,    4.32,
> 2,    2.56,    2.81
> )
> y <- c(0.014,    0.04,    0.001,    0.023,    0.008,    0,    0.008,
> 0.001,    -0.001,    0.002,    0.103,    0,    0.013,    0.005,
> 0.008,    0.001,    0.011,    0.076,    0.005,    0.045,    -0.001,
> 0,    0.008,    -0.002,    0.002,    0.016,    0.006,    0.001,
> 0.002,    0.001,    0.004,    0.086,    0.009,    0.011,    0.002,
> 0.013,    0.019,    0.007,    0,    0.002,    0.024,    0.119,
> 0.015,    0.009,    0.013,    0.017,    0.009,    0.009,    0.006,
> 0.012,    0.002,    0.015,    0,    0.001,    0.002,    0.001,
> 0.007,    0.004,    0.113,    0.016,    0.013,    0.004,    0.015,
> 0.005,    0.004,    0.007,    0,    0.081,    0.001,    0.002,
> 0.014,    0.002,    0,    0.01,    0.003,    0.002,    0.004,
> 0.004,    0.006,    0.064,    0,    0.014,    0,    0.01,    0.019,
> 0.002,    0.006,    0.005,    0.003,    0.103,    0.007,    0.008,
> 0.002,    0.013,    0.007,    0.004,    0.001,    0.04,    0.017,
> 0.018,    0.002,    0.006,    0.011,    0.003,    0.004,    0.008,
> 0.115,    0,    0.02,    0,    0.012,    0.009,    0.011,    0.013,
> 0.004,    0.058,    0.019,    0.006,    0.005,    0.004,    0.012,
> 0.003,    0.003,    0.004,    0.002,    0.001,    0.002,    0.102,
> -0.001,    0.008,    0.002,    0.016,    0.023,    0.014,    0.053,
> 0.009,    0.001,    0.124,    0.009,    0.008,    0.002,    0.002,
> 0.013,    0.002,    0.001,    0.042,    0.011,    0.009,    0,
> 0.004,    0.003,    0.002,    0.005,    0,    0.101,    0.013,
> 0.009,    0.005,    0.002,    0.007,    0.008,    0.067,    0.002,
> 0.064,    0.028,    0.007,    0.006,    0,    0.007,    0.006,    0,
> 0.001,    0.001,    0.001,    0,    0.088,    0.005,    0.008,
> 0.098,    0.005,    0.019,    0.007,    0.05,    -0.002,    0.002,
> 0.129,    0.001,    0.004,    -0.001,    0.002,    -0.001,    0,
> 0.043,    0.018,    0.019,    0.015,    0.003,    0.006,    0.002,
> 0.001,    0.002,    0.004,    0.097,    0.025,    0.022,    0.007,
> 0.011,    0.007,    0.013,    0.061,    0.008,    0.013,    0.028,
> 0.004,    0.013,    0.005,    0.01,    0.004,    0,    0.006,
> -0.001,    0.001,    0.01,    0.061,    0.002,    0.004,    0,
> 0.011,    0.029,    0.018,    0,    0.003,    0.012,    0.085,
> 0.015,    0.007,    0.002,    0.003,    0.008,    0.002,    0.007,
> 0.02,    0.011,    0.02,    0.008,    0.001,    0.003,    0.01,
> 0.014,    0.001,    0.096,    0.027,    0.024,    0,    0.005,
> 0.006,    0.024,    0.087,    0.001,    0.083,    0.02,    0.009,
> 0.009,    0.001,    0,    0.019,    0,    0.003,    -0.001,    0.002,
>  0,    0.089,    0.016,    0.01,    0.103,    0.003,    0.01,
> 0.002,    0.008,    0.005,    0.014,    0.1,    0.007,    0.009,
> 0.011,    -0.001,    0,    0.002,    0.015,    0.036,    0.018,
> 0.026,    0.009,    0.008,    0.004,    0.001,    0.014,    0.009,
> 0.1,    0.026,    0.032,    0.008,    0.011,    0.004,    0.013,
> 0.019,    0.004,    0.02,    0.015,    0.005,    0.013,    -0.001,
> 0.013,    0.012,    0,    0.01,    0.002,    0.001,    0.013,
> 0.066,    0.009,    0.005,    0.002,    0.013,    0.025,    0.006,
> 0,    0,    0.015,    0.121,    0.006,    0.003,    0.008,    0,
> 0.012,    0.011,    0.003,    0.022,    0.008,    0.032,    0.007,
> 0.002,    0.006,    0.007,    0,    0.003,    0.11,    0.01,    0.008,
>   0,    0.018,    0.008,    0.001,    0.087,    0,    0.028,
> 0.011,    0.014,    0.007,    0.001,    0.018,    0.033,    0.021,
> 0.003,    0.003,    0.007,    -0.001,    0.07,    0.022,    0.009,
> 0.001,    0.007,    0.031,    0.008,    0.013,    0.01,    0.018,
> 0.125,    0.01,    0.015,    0.006,    0,    0.015,    0.019
> )
> z <- cbind(x, y)
> k <- kmeans(z, 2)
> plot(z, col=k$cluster)
> 
> library(apcluster)
> m <- apclusterK(negDistMat(r=2), z, K=2, verbose=TRUE)
> plot(m, z)
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Sun Dec 13 19:28:07 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 13 Dec 2015 10:28:07 -0800
Subject: [R] define number of clusters in kmeans/apcluster analysis
In-Reply-To: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>
References: <CAMk+s2TXF_CynbyktJCjFm86ZBFREom_RxJxZbWvnrOCpttDiw@mail.gmail.com>
Message-ID: <CAF8bMcbWrEYUrLGCWa-JzaCQz2GNYvh5hz4bm2_tZ28+UpkvUg@mail.gmail.com>

In addition to the other fine replies, you should also know that
kmeans's results
depend on the relative scales of the data columns (since it is based
on distances
between points).  Your x and y have quite different scales so the distance is
essentially determined only by the differences in the variable with
the larger scale.

E.g., look at the difference in clustering when x is scaled by
different amounts:

par(mfrow=c(2,2), mar=c(3,3,0,0))
for(i in 1:4){k <- kmeans(zs<-cbind(x/(i*5),y/.05), 2); plot(zs,
col=k$cluster, asp=1)}

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sun, Dec 13, 2015 at 8:17 AM, Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
> Dear all,
> I am trying to do some cluster analysis, both with the base R and the
> apcluster. Both methods give 2 clusters, which is what I am looking
> for since I am interested in identifying positive and negative
> results. However  I could not find a way to fine-tuning the analysis
> in order to properly allocate the points; essentially the negative
> points should be all those in the lower left portion of the plot (see
> example) but some in the top centre are also given to the negative
> cluster.
> So how can I change the parameters to get better results?
> Thank you
> L
>
>>>>
> x <- c(3.15,    3.07,    2,    3,    2.97,    45,    3.21,    45,
> 40.55,    2,    22.09,    2.47,    2.97,    2.77,    2.6,    7.35,
> 4.11,    37.12,    2.73,    36.36,    45,    2.33,    2.49,    45,
> 2.4,    2.74,    2.64,    45,    2.47,    38.1,    2.47,    37.4,
> 2.77,    2.37,    45,    2.69,    2.97,    2.7,    2,    2,    2.55,
>  11.86,    2.51,    2.68,    2.31,    2.6,    2.45,    2,    2.72,
> 2.57,    2.09,    3.04,    45,    45,    2.13,    43.82,    2.92,
> 4.94,    24.82,    2.64,    4.96,    3.65,    2.67,    2.64,    8.04,
>   4.56,    44.87,    37.42,    45,    6.2,    2.84,    4.08,    2,
> 5.03,    2.27,    44.89,    2.41,    2.47,    2.78,    37.47,    45,
>  2.76,    45,    2.51,    2.8,    44.8,    6.2,    2.87,    2.23,
> 18.32,    3.14,    2.1,    2.38,    2.72,    2,    2,    44.41,
> 3.15,    3.06,    4.8,    2.77,    2.8,    2.71,    44.77,    2.25,
> 2.69,    28.38,    2,    2.95,    45,    2.79,    2.46,    2.61,
> 2.78,    2.94,    38.47,    3.29,    2.89,    2.4,    2.23,    2.62,
>  4.21,    2.61,    2.81,    2.41,    41.98,    2.39,    36.41,
> 44.84,    4.73,    2,    2.66,    4.57,    3.01,    42.64,    2.04,
> 5.49,    15.48,    3.08,    2.7,    2,    2,    2.09,    2,    2.29,
>  2.92,    3.39,    3.1,    2,    6.14,    7.03,    4.77,    2.55,
> 32.36,    20.61,    3.09,    4.46,    44.75,    2,    2.73,    2,
> 36.05,    3.61,    34.84,    2.69,    5.28,    3.04,    45,    2.47,
>  2.58,    2.16,    2.59,    45,    44.08,    2,    37.05,    2.48,
> 2.46,    38.71,    7.32,    2.95,    2.8,    44.58,    42.24,
> 36.99,    13.84,    45,    2,    2,    2.38,    45,    45,    43.59,
>  2.69,    2.81,    3.05,    2.8,    4.65,    45,    41.46,    2.33,
> 7.12,    19.18,    4.82,    4.76,    2.51,    3.1,    2.74,    4.99,
>  38.06,    2.53,    2.94,    2.93,    6.59,    2.72,    2.94,    2.56,
>    2.91,    44.79,    2.98,    42.95,    45,    2.63,    38.44,
> 2.71,    2,    37.92,    2.69,    2.91,    2.65,    44.48,    6.35,
> 2.56,    21.94,    3.08,    2.6,    45,    2,    2.62,    2.47,
> 2.62,    2.73,    2.87,    2.83,    4.56,    44.22,    5.15,    5.13,
>   2.76,    7.02,    28.61,    4.87,    5.02,    44.35,    2.26,
> 2.89,    5.26,    38.01,    44.79,    39.26,    2.91,    4.59,
> 2.69,    2.61,    34.97,    3,    45,    2.81,    2,    2.65,    2,
> 37.33,    4.69,    3.26,    38.24,    4.97,    4.62,    2.47,    45,
>  4.52,    2.73,    15.66,    6.06,    2.79,    2.87,    45,    45,
> 45,    4.84,    3.05,    4.89,    4.64,    4.92,    2.74,    7.83,
> 42.31,    2.88,    6.89,    23.06,    2.94,    4.72,    4.55,    5.52,
>    4.48,    4.86,    3.12,    7.68,    43.89,    2.82,    2.64,
> 3.05,    42.95,    2.33,    3.55,    45,    2.79,    2.47,    45,
> 2.56,    38.33,    2.73,    2.87,    2.61,    3.01,    2.86,    2.74,
>   44.46,    44.54,    2.62,    16.94,    2.53,    2.24,    2.72,    2,
>    3.1,    2.88,    7.4,    4.64,    8.25,    3.01,    2.86,    2.46,
>   5.67,    44.52,    2.47,    2,    29.01,    2.61,    3.23,    12.3,
>   3.9,    2.91,    43.99,    36.99,    43.72,    42.29,    2.63,
> 3.03,    2.85,    2.58,    2.63,    2.73,    2.57,    2.37,    2.57,
>  2.75,    44.14,    39.4,    40.02,    3.08,    45,    4.96,    3,
> 2.83,    2.74,    2.8,    2.8,    18.88,    4.69,    2.51,    4.32,
> 2,    2.56,    2.81
> )
> y <- c(0.014,    0.04,    0.001,    0.023,    0.008,    0,    0.008,
>  0.001,    -0.001,    0.002,    0.103,    0,    0.013,    0.005,
> 0.008,    0.001,    0.011,    0.076,    0.005,    0.045,    -0.001,
> 0,    0.008,    -0.002,    0.002,    0.016,    0.006,    0.001,
> 0.002,    0.001,    0.004,    0.086,    0.009,    0.011,    0.002,
> 0.013,    0.019,    0.007,    0,    0.002,    0.024,    0.119,
> 0.015,    0.009,    0.013,    0.017,    0.009,    0.009,    0.006,
> 0.012,    0.002,    0.015,    0,    0.001,    0.002,    0.001,
> 0.007,    0.004,    0.113,    0.016,    0.013,    0.004,    0.015,
> 0.005,    0.004,    0.007,    0,    0.081,    0.001,    0.002,
> 0.014,    0.002,    0,    0.01,    0.003,    0.002,    0.004,
> 0.004,    0.006,    0.064,    0,    0.014,    0,    0.01,    0.019,
> 0.002,    0.006,    0.005,    0.003,    0.103,    0.007,    0.008,
> 0.002,    0.013,    0.007,    0.004,    0.001,    0.04,    0.017,
> 0.018,    0.002,    0.006,    0.011,    0.003,    0.004,    0.008,
> 0.115,    0,    0.02,    0,    0.012,    0.009,    0.011,    0.013,
> 0.004,    0.058,    0.019,    0.006,    0.005,    0.004,    0.012,
> 0.003,    0.003,    0.004,    0.002,    0.001,    0.002,    0.102,
> -0.001,    0.008,    0.002,    0.016,    0.023,    0.014,    0.053,
> 0.009,    0.001,    0.124,    0.009,    0.008,    0.002,    0.002,
> 0.013,    0.002,    0.001,    0.042,    0.011,    0.009,    0,
> 0.004,    0.003,    0.002,    0.005,    0,    0.101,    0.013,
> 0.009,    0.005,    0.002,    0.007,    0.008,    0.067,    0.002,
> 0.064,    0.028,    0.007,    0.006,    0,    0.007,    0.006,    0,
>  0.001,    0.001,    0.001,    0,    0.088,    0.005,    0.008,
> 0.098,    0.005,    0.019,    0.007,    0.05,    -0.002,    0.002,
> 0.129,    0.001,    0.004,    -0.001,    0.002,    -0.001,    0,
> 0.043,    0.018,    0.019,    0.015,    0.003,    0.006,    0.002,
> 0.001,    0.002,    0.004,    0.097,    0.025,    0.022,    0.007,
> 0.011,    0.007,    0.013,    0.061,    0.008,    0.013,    0.028,
> 0.004,    0.013,    0.005,    0.01,    0.004,    0,    0.006,
> -0.001,    0.001,    0.01,    0.061,    0.002,    0.004,    0,
> 0.011,    0.029,    0.018,    0,    0.003,    0.012,    0.085,
> 0.015,    0.007,    0.002,    0.003,    0.008,    0.002,    0.007,
> 0.02,    0.011,    0.02,    0.008,    0.001,    0.003,    0.01,
> 0.014,    0.001,    0.096,    0.027,    0.024,    0,    0.005,
> 0.006,    0.024,    0.087,    0.001,    0.083,    0.02,    0.009,
> 0.009,    0.001,    0,    0.019,    0,    0.003,    -0.001,    0.002,
>   0,    0.089,    0.016,    0.01,    0.103,    0.003,    0.01,
> 0.002,    0.008,    0.005,    0.014,    0.1,    0.007,    0.009,
> 0.011,    -0.001,    0,    0.002,    0.015,    0.036,    0.018,
> 0.026,    0.009,    0.008,    0.004,    0.001,    0.014,    0.009,
> 0.1,    0.026,    0.032,    0.008,    0.011,    0.004,    0.013,
> 0.019,    0.004,    0.02,    0.015,    0.005,    0.013,    -0.001,
> 0.013,    0.012,    0,    0.01,    0.002,    0.001,    0.013,
> 0.066,    0.009,    0.005,    0.002,    0.013,    0.025,    0.006,
> 0,    0,    0.015,    0.121,    0.006,    0.003,    0.008,    0,
> 0.012,    0.011,    0.003,    0.022,    0.008,    0.032,    0.007,
> 0.002,    0.006,    0.007,    0,    0.003,    0.11,    0.01,    0.008,
>    0,    0.018,    0.008,    0.001,    0.087,    0,    0.028,
> 0.011,    0.014,    0.007,    0.001,    0.018,    0.033,    0.021,
> 0.003,    0.003,    0.007,    -0.001,    0.07,    0.022,    0.009,
> 0.001,    0.007,    0.031,    0.008,    0.013,    0.01,    0.018,
> 0.125,    0.01,    0.015,    0.006,    0,    0.015,    0.019
> )
> z <- cbind(x, y)
> k <- kmeans(z, 2)
> plot(z, col=k$cluster)
>
> library(apcluster)
> m <- apclusterK(negDistMat(r=2), z, K=2, verbose=TRUE)
> plot(m, z)
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Sun Dec 13 20:31:13 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 13 Dec 2015 11:31:13 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <22123.61123.948598.140447@stat.math.ethz.ch>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
Message-ID: <CAF8bMca0uHf0DgeZZ+_84ON_BKg1oQL-4ArZce-Q3BvO0-trVw@mail.gmail.com>

>  as Bill or Jeff explained,  "the empty set is always true"

My wording was that any(logical(0)) is FALSE because "there are no
TRUEs in logical(0)".
Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Sat, Dec 12, 2015 at 1:54 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>>     on Fri, 11 Dec 2015 08:20:55 -0800 writes:
>
>     > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>     >>
>     >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>     >>>
>     >>> Hi All,
>     >>>
>     >>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
>     >>> (never?) throw an exception, at least in these cases:
>     >>
>     >> The usual way to test for a length-0 logical object is to use length():
>     >>
>     >> x <- logical(0)
>     >>
>     >> stopifnot( !length(x) & mode(x)=="logical" )
>
>     > I found
>
>     > stopifnot(!length(x), mode(x) == "logical")
>
>     > more helpful when troubleshooting, because it will tell you whether
>     > it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
>     > wrote:
>
>     > stopifnot(!length(x))
>     > stopifnot(mode(x) == "logical")
>
>     > /Henrik
>
> Yes, indeed, thank you Henrik  --- and Jeff Newmiller who's nice
> humorous reply added other relevant points.
>
> As author stopifnot(), I do agree with Dario's  "gut feeling"
> that stopifnot()  "somehow ought to do the right thing"
> in cases such as
>
>    stopifnot(dim(x) == c(3,4))
>
> which is really subtle version of his cases
> {But the gut feeling is wrong, as I argue from now on}.
>
> Someone writing the above would want stopifnot() to stop in the
> case where x is a simple vector instead of a
> matrix/data.frame/... with dimensions c(3,4) ... but it will not
> because, as Bill or Jeff explained,  "the empty set is always
> true", and so yes indeed,  you have to care about length-0
> expressions in stopifnot().
>
> Indeed, in the past, I had thought of  "improving" stopifnot()
> by giving a warning or even stop()  for  logical(0)  expressions,
> but I quickly dismissed that idea after some experiments.
>
> My conclusion: Breaking such a fundamental lemma of logic as
>   "the empty set is always true"
> will lead to all kinds of illogical situations ... so don't do that!
>
> Martin Maechler,
> ETH Zurich
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Sun Dec 13 20:40:10 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Sun, 13 Dec 2015 19:40:10 +0000
Subject: [R] define number of clusters in kmeans/apcluster analysis
Message-ID: <248E6FA047A8C746BA491485764190F53714EEF3@ESESSMB210.ericsson.se>

And in case you would like to explore the supervised clustering approach, I may suggest to explore the
use of knn() fed by a training set determined by your cluster assignments expectations.
Some "quick code" to show what I mean.

z <- as.data.frame(cbind(scale(x), scale(y)))
colnames(z) <- c("x", "y")

n <- nrow(z)
train <- seq(1,0.3*n,1)
ztrain <- as.data.frame(z[train,])
cl <- vector(mode="numeric", length=length(train))

for (i in 1:nrow(ztrain)) {
  if (ztrain[i,"y"] > 2 | ztrain[i,"x"] > 0) {
    cl[i] <- 2
  }  else {
    cl[i] <- 1
  }
}
plot(ztrain, col=cl)

library(class)
ztest<- as.data.frame(z[-train,])
knn.model <- knn(ztrain, ztest, cl, k = 3)
plot(ztest, col=knn.model)

ztrain$cl <- cl
ztest$cl <- knn.model

z.res <- rbind(ztrain,ztest)
plot(z.res$x, z.res$y, col=z.res$cl)

--
GG

	[[alternative HTML version deleted]]


From hamednofal85 at gmail.com  Sun Dec 13 11:02:29 2015
From: hamednofal85 at gmail.com (Hamed Nofal)
Date: Sun, 13 Dec 2015 12:02:29 +0200
Subject: [R] how to use confusionMatrix function in solving multi-classes
	problem
Message-ID: <CAP7c-TYpabcdM5dt=iPcPBVyD+OxLZUMxFc-kAg17tgp8Bj=sg@mail.gmail.com>

Dear Colleagues
I need someone to kindly help me solving this problem.
A sample of 89 patients was tested for 4 tumor types (T1, T2, T3, T4).
The results of the operative predicted T stage and those of the pathology
tests are tabulated in the following table:

     .------------.-----------------------------------------------.
     .            .     Operative predicted T stage  .
     .            ------------------------------------------------.
     . Pathology  .    T1   .   T2  .   T3      T4  .
     .------------.----------------------------------------------.
     .    T1      .        1   .      0  .     0  .     0 .
     .------------.---------------------------------------------.
     .    T2      .        0   .     24 .     2  .     0 .
     .------------.--------------------------------------------.
     .    T3      .        0   .     1  .     49       3 .
     .------------.--------------------------------------------.
     .    T4      .        0   .     0  .       0  .    9 .
     .-----------------------------------------------------------

I need to calculate the sensitivity, specificity,accuracy,positive and
negative prediction values, p-value and kappa value of these tests.
I tried to convert the tabulated results into a two-class (0,1) table and
use the function (confusionMatrix) taking the pathology results as the
(goldstandard), but unfortunately,I got unreasonable output. Most
probably,I used the function wrongly.

Thank you and regards

Hamed

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Sun Dec 13 21:16:36 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Sun, 13 Dec 2015 20:16:36 +0000
Subject: [R] how to use confusionMatrix function in solving
 multi-classes problem
Message-ID: <248E6FA047A8C746BA491485764190F53714FF1C@ESESSMB210.ericsson.se>

You may use the "caret" package.

At the following link 2-classes and 3-classes examples:

http://www.inside-r.org/node/86995


--
GG



	[[alternative HTML version deleted]]


From boris.steipe at utoronto.ca  Sun Dec 13 21:18:05 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Sun, 13 Dec 2015 15:18:05 -0500
Subject: [R] how to use confusionMatrix function in solving
	multi-classes problem
In-Reply-To: <CAP7c-TYpabcdM5dt=iPcPBVyD+OxLZUMxFc-kAg17tgp8Bj=sg@mail.gmail.com>
References: <CAP7c-TYpabcdM5dt=iPcPBVyD+OxLZUMxFc-kAg17tgp8Bj=sg@mail.gmail.com>
Message-ID: <216AA9FF-9A85-42E9-8FC6-4D35ACF2DD1D@utoronto.ca>

This looks like homework to me and this list has a No-Homework policy. Now, once you have done your homework (and that includes reading the documentation of the functions you are using), and you are still confused about details, you are welcome to ask again. Please keep the following in mind:

http://adv-r.had.co.nz/Reproducibility.html
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
... and definitely read the posting guide for this list and don't post in HTML.

B.




On Dec 13, 2015, at 5:02 AM, Hamed Nofal <hamednofal85 at gmail.com> wrote:

> Dear Colleagues
> I need someone to kindly help me solving this problem.
> A sample of 89 patients was tested for 4 tumor types (T1, T2, T3, T4).
> The results of the operative predicted T stage and those of the pathology
> tests are tabulated in the following table:
> 
>     .------------.-----------------------------------------------.
>     .            .     Operative predicted T stage  .
>     .            ------------------------------------------------.
>     . Pathology  .    T1   .   T2  .   T3      T4  .
>     .------------.----------------------------------------------.
>     .    T1      .        1   .      0  .     0  .     0 .
>     .------------.---------------------------------------------.
>     .    T2      .        0   .     24 .     2  .     0 .
>     .------------.--------------------------------------------.
>     .    T3      .        0   .     1  .     49       3 .
>     .------------.--------------------------------------------.
>     .    T4      .        0   .     0  .       0  .    9 .
>     .-----------------------------------------------------------
> 
> I need to calculate the sensitivity, specificity,accuracy,positive and
> negative prediction values, p-value and kappa value of these tests.
> I tried to convert the tabulated results into a two-class (0,1) table and
> use the function (confusionMatrix) taking the pathology results as the
> (goldstandard), but unfortunately,I got unreasonable output. Most
> probably,I used the function wrongly.
> 
> Thank you and regards
> 
> Hamed
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Mon Dec 14 00:30:10 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 14 Dec 2015 00:30:10 +0100
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CAF8bMca0uHf0DgeZZ+_84ON_BKg1oQL-4ArZce-Q3BvO0-trVw@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CAF8bMca0uHf0DgeZZ+_84ON_BKg1oQL-4ArZce-Q3BvO0-trVw@mail.gmail.com>
Message-ID: <0628B8F9-BE78-4A5B-A042-DAF1CB3E946F@gmail.com>


> On 13 Dec 2015, at 20:31 , William Dunlap <wdunlap at tibco.com> wrote:
> 
>> as Bill or Jeff explained,  "the empty set is always true"
> 
> My wording was that any(logical(0)) is FALSE because "there are no
> TRUEs in logical(0)".

Yes. My mind still boggles over how the empty set slipped into Boolean algebra.

Another way of flogging the horse is to say that we want[*] 

any(c(x,y)) == any(x) | any(y)

even if x has length 0. In that case c(x,y) == y, so we must have

any(y) == any(logical(0)) | any(y)

which holds for all y if and only if any(logical(0)) is FALSE.

Also, as I tried to say before, any() is to "|" what sum() is to "+" and what all() is to "&" and prod() is to "*". All the operators have an identity element, namely FALSE, 0, TRUE, and 1 respectively, and the generic convention is that for an empty vector, we return the identity element, for the reason given above.  

Peter D.

[*] Assuming logical vectors here. Coercion might well do odd things...


> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> 
> On Sat, Dec 12, 2015 at 1:54 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>>>    on Fri, 11 Dec 2015 08:20:55 -0800 writes:
>> 
>>> On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>> 
>>>>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi <dario.beraldi at gmail.com> wrote:
>>>>> 
>>>>> Hi All,
>>>>> 
>>>>> I'd like to understand the reason why stopifnot(logical(0) == x) doesn't
>>>>> (never?) throw an exception, at least in these cases:
>>>> 
>>>> The usual way to test for a length-0 logical object is to use length():
>>>> 
>>>> x <- logical(0)
>>>> 
>>>> stopifnot( !length(x) & mode(x)=="logical" )
>> 
>>> I found
>> 
>>> stopifnot(!length(x), mode(x) == "logical")
>> 
>>> more helpful when troubleshooting, because it will tell you whether
>>> it's !length(x) or mode(x) == "logical" that is FALSE.  It's as if you
>>> wrote:
>> 
>>> stopifnot(!length(x))
>>> stopifnot(mode(x) == "logical")
>> 
>>> /Henrik
>> 
>> Yes, indeed, thank you Henrik  --- and Jeff Newmiller who's nice
>> humorous reply added other relevant points.
>> 
>> As author stopifnot(), I do agree with Dario's  "gut feeling"
>> that stopifnot()  "somehow ought to do the right thing"
>> in cases such as
>> 
>>   stopifnot(dim(x) == c(3,4))
>> 
>> which is really subtle version of his cases
>> {But the gut feeling is wrong, as I argue from now on}.
>> 
>> Someone writing the above would want stopifnot() to stop in the
>> case where x is a simple vector instead of a
>> matrix/data.frame/... with dimensions c(3,4) ... but it will not
>> because, as Bill or Jeff explained,  "the empty set is always
>> true", and so yes indeed,  you have to care about length-0
>> expressions in stopifnot().
>> 
>> Indeed, in the past, I had thought of  "improving" stopifnot()
>> by giving a warning or even stop()  for  logical(0)  expressions,
>> but I quickly dismissed that idea after some experiments.
>> 
>> My conclusion: Breaking such a fundamental lemma of logic as
>>  "the empty set is always true"
>> will lead to all kinds of illogical situations ... so don't do that!
>> 
>> Martin Maechler,
>> ETH Zurich
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From petretta at unina.it  Mon Dec 14 10:13:17 2015
From: petretta at unina.it (Mario Petretta)
Date: Mon, 14 Dec 2015 10:13:17 +0100
Subject: [R] R:  forest plot metafor
In-Reply-To: <001501d134ed$7a4dd630$6ee98290$@unina.it>
References: <003e01d132b2$424f8450$c6ee8cf0$@it>
	<566AD3C6.2010803@dewey.myzen.co.uk>
	<001501d134ed$7a4dd630$6ee98290$@unina.it>
Message-ID: <004c01d1364f$ae912880$0bb37980$@it>

Many tanks to Wolfgang Viechtbauer for his time and help.

The suggested code works very well

Mario Petretta.

__________________________________________________________________________________
Message: 11
Date: Sat, 12 Dec 2015 17:22:20 +0100
From: "Viechtbauer Wolfgang (STAT)"
	<wolfgang.viechtbauer at maastrichtuniversity.nl>
To: "r-help at r-project.org" <r-help at r-project.org>
Subject: Re: [R] R:  forest plot metafor
Message-ID:
	<077E31A57DA26E46AB0D493C9966AC730F2460CD18 at UM-MAIL4112.unimaas.nl>
Content-Type: text/plain; charset="us-ascii"

No, this is not possible. But you can just add the weights yourself. One difficulty here is that you want the weights to the right of the estimates and corresponding CIs and those are always placed at the right of the figure. One possibility would be to make the right margin large and then use mtext() to place the weights in that margin. An example:

library(metafor)
data(dat.bcg)
dat <- escalc(measure="RR", ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)
res <- rma(yi, vi, data=dat, method="FE")
wi <- formatC(weights(res), format="f", digits=1, width=4)
par(mar=c(5,4,4,6))
forest(res, xlim=c(-7,8), digits=c(2,0))
mtext(wi, side=4, at=13:1, line=4, las=2, adj=1)
par(xpd=TRUE)
abline(h=c(0,14))
par(xpd=FALSE)

Adjust as needed for your data.

Best,
Wolfgang
________________________________________
From: R-help [r-help-bounces at r-project.org] On Behalf Of Mario Petretta [petretta at unina.it]
Sent: Saturday, December 12, 2015 3:57 PM
To: r-help at r-project.org
Subject: [R] R:  forest plot metafor

Many thanks to Professor Michael Dewey for his time.

I apologize for the error about the claim to obtain weights from escalc and I realise that the weights are a function of the fit, not the data.

weights(res) is OK to extract the weights from the fitted object and to put them in the data  frame.

However, I again ask:

Using:

 forest(res, showweights=TRUE)

it is possible to directly order the columns, simply placing the weights after effect size and CI?

Sincerely

Mario Petretta



---
Questa e-mail ? stata controllata per individuare virus con Avast antivirus.
https://www.avast.com/antivirus


From maechler at stat.math.ethz.ch  Mon Dec 14 11:10:41 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Dec 2015 11:10:41 +0100
Subject: [R] stopifnot with logical(0)
In-Reply-To: <566C2990.5010703@gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<7A6AC925-F32B-4674-BDAE-AA3FD112E060@gmail.com>
	<566C2990.5010703@gmail.com>
Message-ID: <22126.38305.37680.980679@stat.math.ethz.ch>

>>>>> "DM" == Duncan Murdoch <murdoch.duncan at gmail.com>
>>>>>     on Sat, 12 Dec 2015 09:05:04 -0500 writes:

    DM> On 12/12/2015 8:44 AM, peter dalgaard wrote:
    >> 
    >>> On 12 Dec 2015, at 10:54 , Martin Maechler
    >>> <maechler at stat.math.ethz.ch> wrote:
    >>> 
    >>> My conclusion: Breaking such a fundamental lemma of
    >>> logic as "the empty set is always true"
    >> 
    >> Umm, that doesn't make sense to me. Surely you mean that
    >> "an AND-operation over an empty index set is TRUE"? A
    >> similar OR operation is FALSE, i.e. they behave like
    >> empty products and sums, respectively.
    >> 

    DM> How about "the empty set is all true, and all false."

or, what the I *meant* with the above:

  "All statements about elements of the empty set are true"

  ((and I still like the short form, even though it is not
    correct strictly logically/mathematically))
    
Of course, Peter is correct,  and that
   any(logical(0))   is  FALSE
is really the only sensical way.


From stefano.sofia at regione.marche.it  Mon Dec 14 12:15:51 2015
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Mon, 14 Dec 2015 11:15:51 +0000
Subject: [R] matrix which results singular but at the same time positive
 definite
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F41CD0@FHSDB2D11-2.csu.mcmaster.ca>
References: <8B435C9568170B469AE31E8891E8CC4F3D4A319E@ESINO.regionemarche.intra>,
	<ACD1644AA6C67E4FBD0C350625508EC810F41B92@FHSDB2D11-2.csu.mcmaster.ca>
	<8B435C9568170B469AE31E8891E8CC4F3D4A3249@ESINO.regionemarche.intra>
	<F86F10C7-D61E-4B72-8A78-A0A4E9082456@gmail.com>,
	<ACD1644AA6C67E4FBD0C350625508EC810F41CD0@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F3D4A344C@ESINO.regionemarche.intra>

Dear John and dear Peter,
I needed time to understand better some practical implications derived from your hints.

Thank you
Stefano

________________________________________
Da: Fox, John [jfox at mcmaster.ca]
Inviato: gioved? 10 dicembre 2015 17.24
A: peter dalgaard; Stefano Sofia
Cc: r-help at r-project.org
Oggetto: RE: [R] matrix which results singular but at the same time positive definite

Dear Peter,

> -----Original Message-----
> From: peter dalgaard [mailto:pdalgd at gmail.com]
> Sent: Thursday, December 10, 2015 11:09 AM
> To: Stefano Sofia
> Cc: Fox, John; r-help at r-project.org
> Subject: Re: [R] matrix which results singular but at the same time
> positive definite
>
> Looks like the ill-conditioning is almost entirely due to scaling, e.g.

Yes, that's my point. Sorry I didn't make it clearer.

Best,
 John

>
> > eigen(cov2cor(m))
> $values
> [1] 1.7234899 1.3380701 0.6619299 0.2765101
> ...
>
> This is an annoyance in several parts of numerical linear algebra:
> Routines assume that R^n has all coordinates on a similar scale and
> therefore think that anything on the order of 1e-7 or so is effectively
> zero.
>
> Condition numbers do this too:
>
> > kappa(m)
> [1] 1.066582e+13
> > kappa(cov2cor(m))
> [1] 5.489243
>
>
> -pd
>
> On 10 Dec 2015, at 16:41 , Stefano Sofia
> <stefano.sofia at regione.marche.it> wrote:
>
> > Dear John,
> > thank you for your considerations.
> > This matrix (which is a variance matrix) is part of an algorithm for
> forward-filtering and backward-sampling of Dynamic Linear Models (West
> and Harrison, 1997), applied to DLM representation of ARIMA processes
> (Petris, Petrone, Campagnoli).  It is therefore very difficult to
> explain why this variance matrix becomes so ill conditioned. This
> already happens at the first iteration of the algorithm. I will try to
> work on initial conditions and some fixed parameters.
> >
> > Thank you again
> > Stefano
> >
> >
> > ________________________________________
> > Da: Fox, John [jfox at mcmaster.ca]
> > Inviato: gioved? 10 dicembre 2015 14.41
> > A: Stefano Sofia; r-help at r-project.org
> > Oggetto: RE: matrix which results singular but at the same time
> positive        definite
> >
> > Dear Stefano,
> >
> > You've already had a couple of informative responses directly
> addressing your question, but are you aware how ill-conditioned the
> matrix is (one of the responses alluded to this)?
> >
> >> kappa(X, exact=TRUE)
> > [1] 7.313338e+12
> >
> >> eigen(X)$values
> > [1] 4.964711e+00 9.356881e-01 4.863392e-12 6.788344e-13
> >
> > Two of the variables have variances around 10^0 and the other two
> around 10^-12. Of course, you haven't said anything about the context,
> but does it really make sense to analyze the data on these scales?
> >
> > Best,
> > John
> >
> > -----------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > Web: socserv.mcmaster.ca/jfox
> >
> >
> >
> >
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Stefano Sofia
> >> Sent: December 10, 2015 5:08 AM
> >> To: r-help at r-project.org
> >> Subject: [R] matrix which results singular but at the same time
> positive definite
> >>
> >> Dear list users,
> >> through the "matrixcalc" package I am performing some checks of
> variance
> >> matrices (which must be positive definite).
> >> In this example, it happens that the matrix A here reported is
> singular but
> >> positive definite. Is it possible?
> >>
> >>              [,1]          [,2]          [,3]          [,4]
> >> [1,]  1.904255e-12 -1.904255e-12 -8.238960e-13 -1.240294e-12 [2,] -
> >> 1.904255e-12  3.637979e-12  1.364242e-12  1.818989e-12 [3,] -
> 8.238960e-13
> >> 1.364242e-12  4.809988e+00  7.742369e-01 [4,] -1.240294e-12
> 1.818989e-12
> >> 7.742369e-01  1.090411e+00
> >>
> >> print(is.non.singular.matrix(A, tol = 1e-18)) FALSE
> print(is.positive.definite(A,
> >> tol=1e-18)) TRUE
> >>
> >> Is there something wrong with this matrix?
> >> Any comment will be appreciated.
> >> Stefano
> >>
> >>
> >> ________________________________
> >>
> >> AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu?
> contenere
> >> informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla
> >> ricezione. I messaggi di posta elettronica per i client di Regione
> Marche
> >> possono contenere informazioni confidenziali e con privilegi legali.
> Se non si ? il
> >> destinatario specificato, non leggere, copiare, inoltrare o
> archiviare questo
> >> messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo
> al mittente
> >> ed eliminarlo completamente dal sistema del proprio computer. Ai
> sensi
> >> dell'art. 6 della DGR n. 1394/2008 si segnala che, in caso di
> necessit? ed
> >> urgenza, la risposta al presente messaggio di posta elettronica pu?
> essere
> >> visionata da persone estranee al destinatario.
> >> IMPORTANT NOTICE: This e-mail message is intended to be received only
> by
> >> persons entitled to receive the confidential information it may
> contain. E-mail
> >> messages to clients of Regione Marche may contain information that is
> >> confidential and legally privileged. Please do not read, copy,
> forward, or store
> >> this message unless you are an intended recipient of it. If you have
> received
> >> this message in error, please forward it to the sender and delete it
> completely
> >> from your computer system.
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-
> >> guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > ________________________________
> >
> > AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere
> informazioni confidenziali, pertanto ? destinato solo a persone
> autorizzate alla ricezione. I messaggi di posta elettronica per i client
> di Regione Marche possono contenere informazioni confidenziali e con
> privilegi legali. Se non si ? il destinatario specificato, non leggere,
> copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto
> questo messaggio per errore, inoltrarlo al mittente ed eliminarlo
> completamente dal sistema del proprio computer. Ai sensi dell'art. 6
> della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza,
> la risposta al presente messaggio di posta elettronica pu? essere
> visionata da persone estranee al destinatario.
> > IMPORTANT NOTICE: This e-mail message is intended to be received only
> by persons entitled to receive the confidential information it may
> contain. E-mail messages to clients of Regione Marche may contain
> information that is confidential and legally privileged. Please do not
> read, copy, forward, or store this message unless you are an intended
> recipient of it. If you have received this message in error, please
> forward it to the sender and delete it completely from your computer
> system.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>


________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

From kogalurshear at gmail.com  Sun Dec 13 17:50:45 2015
From: kogalurshear at gmail.com (Udaya B. Kogalur)
Date: Sun, 13 Dec 2015 11:50:45 -0500
Subject: [R] [R-pkgs] randomForestSRC 2.0.0
Message-ID: <CACsjJ_dnOLdXQP-=10LRJSaDX5SU5gXcP3Ev+256hUGJYcSVMQ@mail.gmail.com>

randomForestSRC 2.0.0 is now available on CRAN.   The package is able
to handle multivariate regression (data sets in which the response is
one or more continuous variables), multivariate classification (data
sets in which the response is one or more factor variables),
multivariate mixed (data sets in which the response is a combination
of continuous and factor variables), survival, competing risk, and
unsupervised forests (data sets in which no response is chosen).

The memory and CPU profiles for the package have improved.  The
package is OpenMP compliant on all popular platforms.  Instructions
for enabling this functionality is available in Rd files, and
pre-compiled OpenMP binaries are also available at:

http://www.ccs.miami.edu/~hishwaran/rfsrc.html.

Linear gains in computation times have been observed on multi-core
desktops and cluster environments using OpenMP.

Finally, the ability to implement user defined split rules is now
available. Instructions and examples for all families are provided in
the package's src directory in splitCustom.c.

Udaya B. Kogalur, Ph.D.
Contract Staff, Dept. of Quantitative Health Sciences, Cleveland
Clinic Foundation

ubkogalur at gmail.com

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From justinenasejje at gmail.com  Mon Dec 14 11:01:33 2015
From: justinenasejje at gmail.com (Justine Nasejje)
Date: Mon, 14 Dec 2015 11:01:33 +0100
Subject: [R] Extract survival probabilities from an Extended Cox model
Message-ID: <CAGjsBx38p_siAkLt1MH2YA+cQase7qxGhYKhB=ii=2avOC_3_w@mail.gmail.com>

Dear All,
           How can I extract survival probabilities from a fitted extended
Cox model in R? Thank you in advance.

	[[alternative HTML version deleted]]


From cg.pettersson at lantmannen.com  Mon Dec 14 11:03:09 2015
From: cg.pettersson at lantmannen.com (CG Pettersson)
Date: Mon, 14 Dec 2015 10:03:09 +0000
Subject: [R] Problems when using estimable() (gmodels) on lme()-objects
Message-ID: <HE1PR02MB0987203EB62DCFD3E1B1BD528CED0@HE1PR02MB0987.eurprd02.prod.outlook.com>

R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree", W7-32

Dear all,
I just upgraded R and downloaded a new version of "gmodels", to use estimable() for extracting fixed effects from lme()-objects.
Now it seems like estimable() have changed its view of dimensions of my lme-objects and contrast matrix dimensions, as I get this error message:

> estimable(y_15,Mat15)
Error in estimable.default(y_15, Mat15) :
 Dimension of structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, : 25x26, not compatible with no of parameters in y_15: 25
Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>

Mat15 is made as a 25x25 contrast matrix, but the updated estimable() sees it as a 25x26.
Why, and what should I do about it?

I do think the change has occurred in estimable() as I tested one year old lme() objects and contrast matrix who worked perfectly then, giving the same error message when run with updated R.
Does estimable() see the row-names as the first column nowdays if I don?t specify anything else?

Best regards
CG Pettersson
Scientific Project Manager, PhD
______________________
Lantm?nnen Corporate R&D
Phone:  +46 10 556 19 85
Mobile: + 46 70 330 66 85
Email: cg.pettersson at lantmannen.com<mailto:cg.pettersson at lantmannen.com>
Visiting Address: S:t G?ransgatan 160 A
Address: Box 30192, SE-104 25 Stockholm
Webb: http://www.lantmannen.com<http://www.lantmannen.com/>
Registered Office: Stockholm
Before printing, think about the environment


	[[alternative HTML version deleted]]


From cg.kavlinge at gmail.com  Mon Dec 14 11:23:07 2015
From: cg.kavlinge at gmail.com (CG Pettersson)
Date: Mon, 14 Dec 2015 11:23:07 +0100
Subject: [R] Problems using estimable() (gmodels) on lme()-objects
Message-ID: <CAEBfj7DNmBHeud7xuhwuFrjPTJ3yZk1dYRBmkFXnwtga9xwT0A@mail.gmail.com>

R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree", W7-32



Dear all,

I just upgraded R and downloaded a new version of ?gmodels?, to use
estimable() for extracting fixed effects from lme()-objects.

Now it seems like estimable() have changed its view of dimensions of my
lme-objects and contrast matrix dimensions, as I get this error message:



> estimable(y_15,Mat15)

Error in estimable.default(y_15, Mat15) :

 Dimension of structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
0, : 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
: 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
: 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
: 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
: 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
: 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
: 25x26, not compatible with no of parameters in y_15: 25

Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

>



Mat15 is made as a 25x25 contrast matrix, but the updated estimable() sees
it as a 25x26.

Why, and what should I do about it?



I do think the change has occurred in estimable() as I tested one year old
lme() objects and contrast matrix who worked perfectly then, giving the
same error message when run with updated R.
Does estimable() see the row-names as the first column nowdays if I don?t
specify anything else?

All the best
/CG Pettersson

	[[alternative HTML version deleted]]


From jnkcarlson at gmail.com  Fri Dec 11 09:44:04 2015
From: jnkcarlson at gmail.com (joel carlson)
Date: Fri, 11 Dec 2015 17:44:04 +0900
Subject: [R] [R-pkgs] New Package "radiomics": First and Second Order
	Matrix	Statistics
Message-ID: <CAG=FcbnD6pqCfZk1U6vvuxnQxUFGzH1K=E5rFAknAYZApE-XPA@mail.gmail.com>

Dear R Users,

I am pleased to inform you that my package, 'radiomics' is now
available on CRAN:
https://cran.r-project.org/web/packages/radiomics/
<http://cran.r-project.org/web/packages/clifro/>

The radiomics package offers 4 classes of texture matrices, and
associated feature sets. These matrices can be used for image
classification, or for extraction of meaningful information from the
input image (as in the case of the research field of radiomics). The
texture matrices are:

 - Gray level co-occurrence matrix (GLCM)

 - Gray level run-length matrix (GLRLM)

 - Gray level size-zone matrix (GLSZM)

 - Multiple gray level size-zone matrix (MGLSZM)

Each of these matrices are calculated from an input grayscale matrix,
and return objects of class 'glcm', 'glrlm', 'glszm' or 'mglszm'.
Features of the matrices can be calculated using the calc_features()
function. calc_features() can also be called on an image matrix, and
will return first-order summary statistics.

The github page can be viewed
here:https://github.com/joelcarlson/radiomics
<https://github.com/ropensci/clifro#enhancing-the-national-climate-database-with-clifro>

Lists of features, and an introduction to the package can be found in
the package vignette.

In-depth explanation of texture analysis matrices can be found here:

http://joelcarlson.me/2015/07/10/radiomics-package/

Any comments, contributions, and feedback are always welcome.

I hope you find this helpful!
Regards
---
Joel Carlson

	[[alternative HTML version deleted]]

_______________________________________________
R-packages mailing list
R-packages at r-project.org
https://stat.ethz.ch/mailman/listinfo/r-packages


From cri.alessandro at gmail.com  Mon Dec 14 14:43:29 2015
From: cri.alessandro at gmail.com (Cristiano Alessandro)
Date: Mon, 14 Dec 2015 14:43:29 +0100
Subject: [R] repeated measure with quantitative independent variable
Message-ID: <566EC781.2040204@gmail.com>

Hi all,

I am new to R, and I am trying to set up a repeated measure analysis 
with a quantitative (as opposed to factorized/categorical) 
within-subjects variable. For a variety of reasons I am not using 
linear-mixed models, rather I am trying to fit a General Linear Model (I 
am aware of assumptions and limitations) to assess whether the value of 
the within-subjects variable affects statistically significantly the 
response variable. I have two questions. To make myself clear I propose 
the following exemplary dataset (where myfactor_nc is the quantitative 
within-subjects variable; i.e. each subject performs the experiment 
three times -- nc_factor=1,2,3 -- and produces the response in variable dv).

dv <- c(1,3,4,2,2,3,2,5,6,3,4,4,3,5,6);
subject <- 
factor(c("s1","s1","s1","s2","s2","s2","s3","s3","s3","s4","s4","s4","s5","s5","s5"));
myfactor_nc <- c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
mydata_nc <- data.frame(dv, subject, myfactor_nc)

*Question 1 (using function aov)*

Easily done...

am1_nc <- aov(dv ~ myfactor_nc + Error(subject/myfactor_nc), data=mydata_nc)
summary(am1_nc)

Unlike the case when myfactor_nc is categorical, this produces three 
error strata: Error: subject, Error: subject:myfactor_nc, Error: Within. 
I cannot understand the meaning of the latter. How is that computed?

*Question 2 (using function lm)*

Now I would like to do the same with the functions lm() and Anova() 
(from the car package). What I have done so far (please correct me if I 
am mistaking) is the following:

# Unstack the dataset
dvm <- with(mydata_nc, cbind(dv[myfactor_nc==1],dv[myfactor_nc==2], 
dv[myfactor_nc==3]))

#Fit the linear model
mlm1 <- lm(dvm ~ 1)

(is that model above correct for my design?)

Now I should use the Anova function, but it seems that it only accepts 
factors, and not quantitative within-subject variable.

Any help is highly appreciated!

Thanks
Cristiano


From arasheedbello at yahoo.com  Mon Dec 14 09:27:20 2015
From: arasheedbello at yahoo.com (Bello A rasheed)
Date: Mon, 14 Dec 2015 08:27:20 +0000 (UTC)
Subject: [R] R-Help
References: <336838222.607982.1450081640228.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <336838222.607982.1450081640228.JavaMail.yahoo@mail.yahoo.com>

Dear R users?

Kindly help me on how run this code because there is error when ever i run it.
It give me an error at?Error: lent >= 1 is not TRUE?
Error: object 'hubPsi' not found
Error in cbind(b = b, A = A, B = B, V = A/B^2, e = B^2/A, gamma. = b/B, ?:?? dims [product 3] do not match the length of object [21]?Thank you in advance********************************************************
R-CODE
Hf0 <- function(x, c=1.35) pmax2(-c,pmin2(x,c))

Hf <- new("functionX",Hf0)

stopifnot(validObject(Hf)) # ok !


?

?
### psiFunc() examples## classical {trivial, notinteresting}:

F1 <- function(x) rep.int(1,length(x))

cPsi <- psiFunc(rho = function(x)x^2 / 2, psi = function(x) x,

??????????????? wgt = F1, Dpsi = F1,

??????????????? Erho = function(x) rep.int(1/2,length(x)),

??????????????? Epsi2 = F1, EDpsi = F1)


?
### MASS? -- ?rlm --- has

##

##-?????psi.huber?? (u, k = 1.345, deriv =0)

##-?????psi.hampel? (u, a = 2, b = 4, c =8, deriv = 0)

##-?????psi.bisquare(u, c = 4.685, deriv = 0)

##??where deriv = 0 :? psi(x)/xi.e.? 'wgt'


?
## MM has more in psi-funs.R (seeabove)

## Reproduce Table 1, p.138 of Hampel,Rousseeuw, Ronchetti, Stahel (1986):
b <- c(seq(0, 3, by = 0.1), 4, 5,Inf)

A <- hubPsi at Epsi2(b)

B <- hubPsi at EDpsi(b)


?
options(width = 100, digits = 7)


?
cbind(b = b, A = A, B = B, V = A/B^2, e= B^2/A,

?????gamma. = b/B, k. = 1 + b^2/A, lambda. = 1/B)


?
(hubPsi2 <- chgDefaults(hubPsi, k =2)) # works too!

?Bello Abdulkadir RasheedPh.D candidate ( mathematics/statistics)
Department of mathematical Science/ faculty of Science
University Technology Malaysia
81310 UTM, Johor Bahru, Johor, Malaysia
IC. NO. 201204M10026
Matric No. PS113124
HP. No.??0060149144837E-mail:?arasheedbello at yahoo.com
supervisor- Assoc. Prof. Robiah Binti Adnan
	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Mon Dec 14 17:10:47 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Mon, 14 Dec 2015 10:10:47 -0600
Subject: [R] stopifnot with logical(0)
In-Reply-To: <22124.31423.70233.408011@stat.math.ethz.ch>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
	<22124.31423.70233.408011@stat.math.ethz.ch>
Message-ID: <CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>

On Sat, Dec 12, 2015 at 1:51 PM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Hadley Wickham <h.wickham at gmail.com>
>>>>>>     on Sat, 12 Dec 2015 08:08:54 -0600 writes:
>
>     > On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
>     > <maechler at stat.math.ethz.ch> wrote:
>     >>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com> on
>     >>>>>>> Fri, 11 Dec 2015 08:20:55 -0800 writes:
>     >>
>     >> > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius
>     >> <dwinsemius at comcast.net> wrote:
>     >> >>
>     >> >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi
>     >> <dario.beraldi at gmail.com> wrote:
>     >> >>>
>     >> >>> Hi All,
>     >> >>>
>     >> >>> I'd like to understand the reason why
>     >> stopifnot(logical(0) == x) doesn't >>> (never?) throw an
>     >> exception, at least in these cases:
>     >> >>
>     >> >> The usual way to test for a length-0 logical object is
>     >> to use length():
>     >> >>
>     >> >> x <- logical(0)
>     >> >>
>     >> >> stopifnot( !length(x) & mode(x)=="logical" )
>     >>
>     >> > I found
>     >>
>     >> > stopifnot(!length(x), mode(x) == "logical")
>     >>
>     >> > more helpful when troubleshooting, because it will tell
>     >> you whether > it's !length(x) or mode(x) == "logical"
>     >> that is FALSE.  It's as if you > wrote:
>     >>
>     >> > stopifnot(!length(x)) > stopifnot(mode(x) == "logical")
>     >>
>     >> > /Henrik
>     >>
>     >> Yes, indeed, thank you Henrik --- and Jeff Newmiller
>     >> who's nice humorous reply added other relevant points.
>     >>
>     >> As author stopifnot(), I do agree with Dario's "gut
>     >> feeling" that stopifnot() "somehow ought to do the right
>     >> thing" in cases such as
>     >>
>     >> stopifnot(dim(x) == c(3,4))
>     >>
>     >> which is really subtle version of his cases {But the gut
>     >> feeling is wrong, as I argue from now on}.
>
>     > Personally, I think the problem there is that people
>     > forget that == is vectorised, and for a non-vectorised
>     > equality check you really should use identical:
>
>     > stopifnot(identical(dim(x), c(3,4)))
>
> You are right "in theory"  but practice is less easy:
> identical() tends to be  too subtle for many users ... even
> yourself (;-), not really of course!),  Hadley, in the above case:
>
> Your stopifnot() would *always* stop, i.e., signal an error
> because typically all dim() methods return integer, and c(3,4)
> is double.
> So, if even Hadley gets it wrong so easily, I wonder if its good
> to advertize to always use  identical() in such cases.
> I indeed would quite often use identical() in such tests, and
> you'd too and would quickly find and fix the "trap" of course..
> So you are mostly right also in my opinion...

Ooops, yes - but you would discover this pretty quickly if you weren't
coding in a email client ;)

I wonder if R is missing an equality operator for this case. Currently:

* == is suboptimal because it's vectorised
* all.equal is suboptimal because it returns TRUE or a text string
* identical is suboptimal because it doesn't do common coercions

Do we need another function (equals()?) that uses the same coercion
rules as == but isn't vectorised? (Like == it would only work with
vectors, so you'd still need identical() for (e.g.) comparing
environments)

Hadley

-- 
http://had.co.nz/


From bgunter.4567 at gmail.com  Mon Dec 14 17:20:12 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 14 Dec 2015 08:20:12 -0800
Subject: [R] Problems using estimable() (gmodels) on lme()-objects
In-Reply-To: <CAEBfj7DNmBHeud7xuhwuFrjPTJ3yZk1dYRBmkFXnwtga9xwT0A@mail.gmail.com>
References: <CAEBfj7DNmBHeud7xuhwuFrjPTJ3yZk1dYRBmkFXnwtga9xwT0A@mail.gmail.com>
Message-ID: <CAGxFJbTMEpSzxHKR24j2ZgoQGqfk9bUuo23VqrE1UEEgg6Wm4A@mail.gmail.com>

If you do not receive a satisfactory reply here, this is the sort of
question that you should email the maintainer for, found by:

> maintainer("gmodels")
[1] "Gregory R. Warnes <greg at warnes.net>"

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Dec 14, 2015 at 2:23 AM, CG Pettersson <cg.kavlinge at gmail.com> wrote:
> R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree", W7-32
>
>
>
> Dear all,
>
> I just upgraded R and downloaded a new version of ?gmodels?, to use
> estimable() for extracting fixed effects from lme()-objects.
>
> Now it seems like estimable() have changed its view of dimensions of my
> lme-objects and contrast matrix dimensions, as I get this error message:
>
>
>
>> estimable(y_15,Mat15)
>
> Error in estimable.default(y_15, Mat15) :
>
>  Dimension of structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
> 0, : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
> : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
> : 25x26, not compatible with no of parameters in y_15: 25
>
> Dimension of 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
>
>>
>
>
>
> Mat15 is made as a 25x25 contrast matrix, but the updated estimable() sees
> it as a 25x26.
>
> Why, and what should I do about it?
>
>
>
> I do think the change has occurred in estimable() as I tested one year old
> lme() objects and contrast matrix who worked perfectly then, giving the
> same error message when run with updated R.
> Does estimable() see the row-names as the first column nowdays if I don?t
> specify anything else?
>
> All the best
> /CG Pettersson
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Mon Dec 14 17:25:42 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Mon, 14 Dec 2015 16:25:42 +0000
Subject: [R] repeated measure with quantitative independent variable
In-Reply-To: <566EC781.2040204@gmail.com>
References: <566EC781.2040204@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F42DA3@FHSDB2D11-2.csu.mcmaster.ca>

Dear Cristiano,

If I understand correctly what you want to do, you should be able to use Anova() in the car package (your second question) by treating your numeric repeated-measures predictor as a factor and defining a single linear contrast for it.

Continuing with your toy example:

> myfactor_nc <- factor(1:3)
> contrasts(myfactor_nc) <- matrix(-1:1, ncol=1)
> idata <- data.frame(myfactor_nc)
> Anova(mlm1, idata=idata, idesign=~myfactor_nc)
Note: model has only an intercept; equivalent type-III tests substituted.

Type III Repeated Measures MANOVA Tests: Pillai test statistic
            Df test stat approx F num Df den Df   Pr(>F)   
(Intercept)  1   0.93790   60.409      1      4 0.001477 **
myfactor_nc  1   0.83478    7.579      2      3 0.067156 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

With just 3 distinct levels, however, you could just make myfactor_nc an ordered factor, not defining the contrasts explicitly, and then you'd get both linear and quadratic contrasts.

I hope this helps,
 John

-----------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.socsci.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> Cristiano Alessandro
> Sent: Monday, December 14, 2015 8:43 AM
> To: r-help at r-project.org
> Subject: [R] repeated measure with quantitative independent variable
> 
> Hi all,
> 
> I am new to R, and I am trying to set up a repeated measure analysis
> with a quantitative (as opposed to factorized/categorical)
> within-subjects variable. For a variety of reasons I am not using
> linear-mixed models, rather I am trying to fit a General Linear Model (I
> am aware of assumptions and limitations) to assess whether the value of
> the within-subjects variable affects statistically significantly the
> response variable. I have two questions. To make myself clear I propose
> the following exemplary dataset (where myfactor_nc is the quantitative
> within-subjects variable; i.e. each subject performs the experiment
> three times -- nc_factor=1,2,3 -- and produces the response in variable
> dv).
> 
> dv <- c(1,3,4,2,2,3,2,5,6,3,4,4,3,5,6);
> subject <-
> factor(c("s1","s1","s1","s2","s2","s2","s3","s3","s3","s4","s4","s4","s5
> ","s5","s5"));
> myfactor_nc <- c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
> mydata_nc <- data.frame(dv, subject, myfactor_nc)
> 
> *Question 1 (using function aov)*
> 
> Easily done...
> 
> am1_nc <- aov(dv ~ myfactor_nc + Error(subject/myfactor_nc),
> data=mydata_nc)
> summary(am1_nc)
> 
> Unlike the case when myfactor_nc is categorical, this produces three
> error strata: Error: subject, Error: subject:myfactor_nc, Error: Within.
> I cannot understand the meaning of the latter. How is that computed?
> 
> *Question 2 (using function lm)*
> 
> Now I would like to do the same with the functions lm() and Anova()
> (from the car package). What I have done so far (please correct me if I
> am mistaking) is the following:
> 
> # Unstack the dataset
> dvm <- with(mydata_nc, cbind(dv[myfactor_nc==1],dv[myfactor_nc==2],
> dv[myfactor_nc==3]))
> 
> #Fit the linear model
> mlm1 <- lm(dvm ~ 1)
> 
> (is that model above correct for my design?)
> 
> Now I should use the Anova function, but it seems that it only accepts
> factors, and not quantitative within-subject variable.
> 
> Any help is highly appreciated!
> 
> Thanks
> Cristiano
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Mon Dec 14 17:30:44 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 14 Dec 2015 11:30:44 -0500
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
	<22124.31423.70233.408011@stat.math.ethz.ch>
	<CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>
Message-ID: <566EEEB4.6050805@gmail.com>

On 14/12/2015 11:10 AM, Hadley Wickham wrote:
> On Sat, Dec 12, 2015 at 1:51 PM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
> >>>>>> Hadley Wickham <h.wickham at gmail.com>
> >>>>>>     on Sat, 12 Dec 2015 08:08:54 -0600 writes:
> >
> >     > On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
> >     > <maechler at stat.math.ethz.ch> wrote:
> >     >>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com> on
> >     >>>>>>> Fri, 11 Dec 2015 08:20:55 -0800 writes:
> >     >>
> >     >> > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius
> >     >> <dwinsemius at comcast.net> wrote:
> >     >> >>
> >     >> >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi
> >     >> <dario.beraldi at gmail.com> wrote:
> >     >> >>>
> >     >> >>> Hi All,
> >     >> >>>
> >     >> >>> I'd like to understand the reason why
> >     >> stopifnot(logical(0) == x) doesn't >>> (never?) throw an
> >     >> exception, at least in these cases:
> >     >> >>
> >     >> >> The usual way to test for a length-0 logical object is
> >     >> to use length():
> >     >> >>
> >     >> >> x <- logical(0)
> >     >> >>
> >     >> >> stopifnot( !length(x) & mode(x)=="logical" )
> >     >>
> >     >> > I found
> >     >>
> >     >> > stopifnot(!length(x), mode(x) == "logical")
> >     >>
> >     >> > more helpful when troubleshooting, because it will tell
> >     >> you whether > it's !length(x) or mode(x) == "logical"
> >     >> that is FALSE.  It's as if you > wrote:
> >     >>
> >     >> > stopifnot(!length(x)) > stopifnot(mode(x) == "logical")
> >     >>
> >     >> > /Henrik
> >     >>
> >     >> Yes, indeed, thank you Henrik --- and Jeff Newmiller
> >     >> who's nice humorous reply added other relevant points.
> >     >>
> >     >> As author stopifnot(), I do agree with Dario's "gut
> >     >> feeling" that stopifnot() "somehow ought to do the right
> >     >> thing" in cases such as
> >     >>
> >     >> stopifnot(dim(x) == c(3,4))
> >     >>
> >     >> which is really subtle version of his cases {But the gut
> >     >> feeling is wrong, as I argue from now on}.
> >
> >     > Personally, I think the problem there is that people
> >     > forget that == is vectorised, and for a non-vectorised
> >     > equality check you really should use identical:
> >
> >     > stopifnot(identical(dim(x), c(3,4)))
> >
> > You are right "in theory"  but practice is less easy:
> > identical() tends to be  too subtle for many users ... even
> > yourself (;-), not really of course!),  Hadley, in the above case:
> >
> > Your stopifnot() would *always* stop, i.e., signal an error
> > because typically all dim() methods return integer, and c(3,4)
> > is double.
> > So, if even Hadley gets it wrong so easily, I wonder if its good
> > to advertize to always use  identical() in such cases.
> > I indeed would quite often use identical() in such tests, and
> > you'd too and would quickly find and fix the "trap" of course..
> > So you are mostly right also in my opinion...
>
> Ooops, yes - but you would discover this pretty quickly if you weren't
> coding in a email client ;)
>
> I wonder if R is missing an equality operator for this case. Currently:
>
> * == is suboptimal because it's vectorised
> * all.equal is suboptimal because it returns TRUE or a text string
> * identical is suboptimal because it doesn't do common coercions
>
> Do we need another function (equals()?) that uses the same coercion
> rules as == but isn't vectorised? (Like == it would only work with
> vectors, so you'd still need identical() for (e.g.) comparing
> environments)
I don't think so.  We already have all(), so all(x == y) would do what 
you want.

Duncan Murdoch


From wdunlap at tibco.com  Mon Dec 14 17:31:29 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 14 Dec 2015 08:31:29 -0800
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
	<22124.31423.70233.408011@stat.math.ethz.ch>
	<CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>
Message-ID: <CAF8bMcaXn+NdnSB+=+Z2o-P+3YDYYsmVpwqMRfgzFBM1UTKgZg@mail.gmail.com>

Hadley wrote
> * all.equal is suboptimal because it returns TRUE or a text string

That feature works ok with stopifnot():
  > stopifnot(all.equal("one", 1))
  Error: all.equal("one", 1) are not all TRUE
and I suppose stopifnot could be enhanced to print the text strings that
all.equal() returns so the user has a better idea of what went wront.

I find that all.equal's comparing of names gets in the way of the intent
of stopifnot
  > stopifnot(all.equal(1:2, c(X=1,Y=2)))
  Error: all.equal(1:2, c(X = 1, Y = 2)) is not TRUE

When we use things like all.equal() or new operators that do
non-recycycling binary operations we may make things simpler
for the programmer, but harder for the user.  I think the user would
rather see a specific message about what is wrong rather than a
note that an obscure function did not return TRUE.
    > f <- function(x) {
         stopifnot(length(x)==1, x>0)
         seq_len(x)
      }
    > f(1:3)
    Error: length(x) == 1 is not TRUE
    > f(-4)
    Error: x > 0 is not TRUE
    > f(4)
    [1] 1 2 3 4

Another step in this direction is to change the call to stop() in stopifnot
from what I assume is
    stop(message, call.=FALSE)
to
    stop(simpleError(message, sys.call(-1)))
so the error message included where error was:
    > f(-1)
    Error in f(-1) : x > 0 is not TRUE
(This is Bug 16188.)

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Mon, Dec 14, 2015 at 8:10 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> On Sat, Dec 12, 2015 at 1:51 PM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>>>>>> Hadley Wickham <h.wickham at gmail.com>
>>>>>>>     on Sat, 12 Dec 2015 08:08:54 -0600 writes:
>>
>>     > On Sat, Dec 12, 2015 at 3:54 AM, Martin Maechler
>>     > <maechler at stat.math.ethz.ch> wrote:
>>     >>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com> on
>>     >>>>>>> Fri, 11 Dec 2015 08:20:55 -0800 writes:
>>     >>
>>     >> > On Fri, Dec 11, 2015 at 8:10 AM, David Winsemius
>>     >> <dwinsemius at comcast.net> wrote:
>>     >> >>
>>     >> >>> On Dec 11, 2015, at 5:38 AM, Dario Beraldi
>>     >> <dario.beraldi at gmail.com> wrote:
>>     >> >>>
>>     >> >>> Hi All,
>>     >> >>>
>>     >> >>> I'd like to understand the reason why
>>     >> stopifnot(logical(0) == x) doesn't >>> (never?) throw an
>>     >> exception, at least in these cases:
>>     >> >>
>>     >> >> The usual way to test for a length-0 logical object is
>>     >> to use length():
>>     >> >>
>>     >> >> x <- logical(0)
>>     >> >>
>>     >> >> stopifnot( !length(x) & mode(x)=="logical" )
>>     >>
>>     >> > I found
>>     >>
>>     >> > stopifnot(!length(x), mode(x) == "logical")
>>     >>
>>     >> > more helpful when troubleshooting, because it will tell
>>     >> you whether > it's !length(x) or mode(x) == "logical"
>>     >> that is FALSE.  It's as if you > wrote:
>>     >>
>>     >> > stopifnot(!length(x)) > stopifnot(mode(x) == "logical")
>>     >>
>>     >> > /Henrik
>>     >>
>>     >> Yes, indeed, thank you Henrik --- and Jeff Newmiller
>>     >> who's nice humorous reply added other relevant points.
>>     >>
>>     >> As author stopifnot(), I do agree with Dario's "gut
>>     >> feeling" that stopifnot() "somehow ought to do the right
>>     >> thing" in cases such as
>>     >>
>>     >> stopifnot(dim(x) == c(3,4))
>>     >>
>>     >> which is really subtle version of his cases {But the gut
>>     >> feeling is wrong, as I argue from now on}.
>>
>>     > Personally, I think the problem there is that people
>>     > forget that == is vectorised, and for a non-vectorised
>>     > equality check you really should use identical:
>>
>>     > stopifnot(identical(dim(x), c(3,4)))
>>
>> You are right "in theory"  but practice is less easy:
>> identical() tends to be  too subtle for many users ... even
>> yourself (;-), not really of course!),  Hadley, in the above case:
>>
>> Your stopifnot() would *always* stop, i.e., signal an error
>> because typically all dim() methods return integer, and c(3,4)
>> is double.
>> So, if even Hadley gets it wrong so easily, I wonder if its good
>> to advertize to always use  identical() in such cases.
>> I indeed would quite often use identical() in such tests, and
>> you'd too and would quickly find and fix the "trap" of course..
>> So you are mostly right also in my opinion...
>
> Ooops, yes - but you would discover this pretty quickly if you weren't
> coding in a email client ;)
>
> I wonder if R is missing an equality operator for this case. Currently:
>
> * == is suboptimal because it's vectorised
> * all.equal is suboptimal because it returns TRUE or a text string
> * identical is suboptimal because it doesn't do common coercions
>
> Do we need another function (equals()?) that uses the same coercion
> rules as == but isn't vectorised? (Like == it would only work with
> vectors, so you'd still need identical() for (e.g.) comparing
> environments)
>
> Hadley
>
> --
> http://had.co.nz/
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From h.wickham at gmail.com  Mon Dec 14 17:45:20 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Mon, 14 Dec 2015 10:45:20 -0600
Subject: [R] stopifnot with logical(0)
In-Reply-To: <566EEEB4.6050805@gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
	<22124.31423.70233.408011@stat.math.ethz.ch>
	<CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>
	<566EEEB4.6050805@gmail.com>
Message-ID: <CABdHhvE2V23HUa9foHiVCX6Z8zvOkm4GywChMYtroMnGLn-2Gg@mail.gmail.com>

>> I wonder if R is missing an equality operator for this case. Currently:
>>
>> * == is suboptimal because it's vectorised
>> * all.equal is suboptimal because it returns TRUE or a text string
>> * identical is suboptimal because it doesn't do common coercions
>>
>> Do we need another function (equals()?) that uses the same coercion
>> rules as == but isn't vectorised? (Like == it would only work with
>> vectors, so you'd still need identical() for (e.g.) comparing
>> environments)
>
> I don't think so.  We already have all(), so all(x == y) would do what you
> want.

But that recycles, which is what we're trying to avoid here.

Hadley

-- 
http://had.co.nz/


From dwinsemius at comcast.net  Mon Dec 14 17:55:39 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 14 Dec 2015 08:55:39 -0800
Subject: [R] Extract survival probabilities from an Extended Cox model
In-Reply-To: <CAGjsBx38p_siAkLt1MH2YA+cQase7qxGhYKhB=ii=2avOC_3_w@mail.gmail.com>
References: <CAGjsBx38p_siAkLt1MH2YA+cQase7qxGhYKhB=ii=2avOC_3_w@mail.gmail.com>
Message-ID: <FBA140A8-0A1F-46A4-9D5D-4D5F388863D2@comcast.net>


> On Dec 14, 2015, at 2:01 AM, Justine Nasejje <justinenasejje at gmail.com> wrote:
> 
> Dear All,
>           How can I extract survival probabilities from a fitted extended
> Cox model in R? Thank you in advance.

Perhaps you should define the term "extended" in the context of Cox models?

Ideally you would offer one of a) reference to a specific help page in a specific package demonstating the constructions of such a model or b) use R code to construct an applicable example. 


> 	[[alternative HTML version deleted]]

And when you do construct that example, do not post in HTML.

-- 

David Winsemius
Alameda, CA, USA


From murdoch.duncan at gmail.com  Mon Dec 14 18:40:58 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 14 Dec 2015 12:40:58 -0500
Subject: [R] stopifnot with logical(0)
In-Reply-To: <CABdHhvE2V23HUa9foHiVCX6Z8zvOkm4GywChMYtroMnGLn-2Gg@mail.gmail.com>
References: <CAEa0nG4FwWOmbTEbPYFsPL5ytZ5j1UxS2hK6QC=N75J-bT1TOw@mail.gmail.com>
	<76A01B1C-187B-4889-8C92-7D4463F80445@comcast.net>
	<CAFDcVCS_QXHefRNGQU+SPm_C_KTq73xX3hGLj6_PrS-7sDa_RQ@mail.gmail.com>
	<22123.61123.948598.140447@stat.math.ethz.ch>
	<CABdHhvER6uXE8d1nyNsw07xGcRGoHQ+9BKvJda0UyZ+yPJuK6Q@mail.gmail.com>
	<22124.31423.70233.408011@stat.math.ethz.ch>
	<CABdHhvFPfSBhYbv84iNsjpLMVOh7vNV1qB6u-rDufL-Tsb-cog@mail.gmail.com>
	<566EEEB4.6050805@gmail.com>
	<CABdHhvE2V23HUa9foHiVCX6Z8zvOkm4GywChMYtroMnGLn-2Gg@mail.gmail.com>
Message-ID: <566EFF2A.7080802@gmail.com>

On 14/12/2015 11:45 AM, Hadley Wickham wrote:
> >> I wonder if R is missing an equality operator for this case. Currently:
> >>
> >> * == is suboptimal because it's vectorised
> >> * all.equal is suboptimal because it returns TRUE or a text string
> >> * identical is suboptimal because it doesn't do common coercions
> >>
> >> Do we need another function (equals()?) that uses the same coercion
> >> rules as == but isn't vectorised? (Like == it would only work with
> >> vectors, so you'd still need identical() for (e.g.) comparing
> >> environments)
> >
> > I don't think so.  We already have all(), so all(x == y) would do what you
> > want.
>
> But that recycles, which is what we're trying to avoid here.

I think this is too special a case to need a function.  Usually all(x == 
y) is the test you want, because so many R functions will recycle.  It's 
not so when x is the dim of an array.  So I could see a weak argument 
for an equalDim() function, but I think it's better just to use

stopifnot(length(dim) == 2, all(dim == c(3,4)))

I know the all() in the second arg isn't needed, but I think it makes 
the intention clearer.  I think there would be less confusion if 
stopifnot() required its args to be single logical values, so I usually 
try to use it that way.

Duncan Murdoch


From soni.archit1989 at gmail.com  Mon Dec 14 19:39:58 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Tue, 15 Dec 2015 00:09:58 +0530
Subject: [R] Print minified XML in tree format
Message-ID: <CAJ7HxBxe9d1uuxpJYi17M7-ib8LVe_xjfp7YcVaS_22b8Mr_QA@mail.gmail.com>

Hi All,

I am new to wonderful world of R, I am stuck at one problem.

I need to print the minified XML string in tree format, i have tried below
code:

ex  <- "<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't
forget me this weekend!</body></note>"> > XML::xmlParse(ex)<?xml
version="1.0"?><note>
  <to>Tove</to>
  <from>Jani</from>
  <heading>Reminder</heading>
  <body>Don't forget me this weekend!</body>
</note>

?and it is working fine for this short XML but it doesnt work for the XML
like:

<?xml version="1.0"?><catalog><book id="bk101"><author>Gambardella,
Matthew</author><title>XML Developer's
Guide</title><genre>Computer</genre><price>44.95</price><publish_date>2000-10-01</publish_date><description>An
in-depth look at creating applications
      with XML.</description></book><book id="bk102"><author>Ralls,
Kim</author><title>Midnight
Rain</title><genre>Fantasy</genre><price>5.95</price><publish_date>2000-12-16</publish_date><description>A
former architect battles corporate zombies,
      an evil sorceress, and her own childhood to become queen
      of the world.</description></book><book
id="bk103"><author>Corets, Eva</author><title>Maeve
Ascendant</title><genre>Fantasy</genre><price>5.95</price><publish_date>2000-11-17</publish_date><description>After
the collapse of a nanotechnology
      society in England, the young survivors lay the
      foundation for a new society.</description></book><book
id="bk104"><author>Corets, Eva</author><title>Oberon's
Legacy</title><genre>Fantasy</genre><price>5.95</price><publish_date>2001-03-10</publish_date><description>In
post-apocalypse England, the mysterious
      agent known only as Oberon helps to create a new life
      for the inhabitants of London. Sequel to Maeve
      Ascendant.</description></book><book id="bk105"><author>Corets,
Eva</author><title>The Sundered
Grail</title><genre>Fantasy</genre><price>5.95</price><publish_date>2001-09-10</publish_date><description>The
two daughters of Maeve, half-sisters,
      battle one another for control of England. Sequel to
      Oberon's Legacy.</description></book><book
id="bk106"><author>Randall, Cynthia</author><title>Lover
Birds</title><genre>Romance</genre><price>4.95</price><publish_date>2000-09-02</publish_date><description>When
Carla meets Paul at an ornithology
      conference, tempers fly as feathers get
ruffled.</description></book><book id="bk107"><author>Thurman,
Paula</author><title>Splish
Splash</title><genre>Romance</genre><price>4.95</price><publish_date>2000-11-02</publish_date><description>A
deep sea diver finds true love twenty
      thousand leagues beneath the sea.</description></book><book
id="bk108"><author>Knorr, Stefan</author><title>Creepy
Crawlies</title><genre>Horror</genre><price>4.95</price><publish_date>2000-12-06</publish_date><description>An
anthology of horror stories about roaches,
      centipedes, scorpions  and other
insects.</description></book><book id="bk109"><author>Kress,
Peter</author><title>Paradox Lost</title><genre>Science
Fiction</genre><price>6.95</price><publish_date>2000-11-02</publish_date><description>After
an inadvertant trip through a Heisenberg
      Uncertainty Device, James Salway discovers the problems
      of being quantum.</description></book><book
id="bk110"><author>O'Brien, Tim</author><title>Microsoft .NET: The
Programming Bible</title><genre>Computer</genre><price>36.95</price><publish_date>2000-12-09</publish_date><description>Microsoft's
.NET initiative is explored in
      detail in this deep programmer's
reference.</description></book><book id="bk111"><author>O'Brien,
Tim</author><title>MSXML3: A Comprehensive
Guide</title><genre>Computer</genre><price>36.95</price><publish_date>2000-12-01</publish_date><description>The
Microsoft MSXML3 parser is covered in
      detail, with attention to XML DOM interfaces, XSLT processing,
      SAX and more.</description></book><book
id="bk112"><author>Galos, Mike</author><title>Visual Studio 7: A
Comprehensive Guide</title><genre>Computer</genre><price>49.95</price><publish_date>2001-04-16</publish_date><description>Microsoft
Visual Studio 7 is explored in depth,
      looking at how Visual Basic, Visual C++, C#, and ASP+ are
      integrated into a comprehensive development
?
environment.</description></book></catalog>??


?Please advise.?


-- 
Regards
Archit

	[[alternative HTML version deleted]]


From jfox at mcmaster.ca  Mon Dec 14 20:37:39 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Mon, 14 Dec 2015 19:37:39 +0000
Subject: [R] repeated measure with quantitative independent variable
In-Reply-To: <566F1434.8000209@gmail.com>
References: <566EC781.2040204@gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F42DA3@FHSDB2D11-2.csu.mcmaster.ca>
	<566F1434.8000209@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F42F36@FHSDB2D11-2.csu.mcmaster.ca>

Dear Cristiano,

> -----Original Message-----
> From: Cristiano Alessandro [mailto:cri.alessandro at gmail.com]
> Sent: Monday, December 14, 2015 2:11 PM
> To: Fox, John
> Cc: r-help at r-project.org
> Subject: Re: [R] repeated measure with quantitative independent variable
> 
> Dear John,
> 
> thanks for your reply! The reason why I did not want to factorize the
> within-subjects variable was to avoid increasing the Df of the model
> from 1 (continuous variable) to k-1 (where k is the number of levels of
> the factors). I am now confused, because you have factorized the
> variable (indeed using "factor"), but the Df of myfactor_nc seems to be
> 1. Could you explain that?

I defined only one (linear) contrast for the factor -- but I made a mistake in how I did it, see below.

> 
> Comparing the results obtained with the two methods I seem to get
> completely different results:
> 
> * aov()*
> 
> dv <- c(1,3,4,2,2,3,2,5,6,3,4,4,3,5,6);
> subject <-
> factor(c("s1","s1","s1","s2","s2","s2","s3","s3","s3","s4","s4","s4","s5
> ","s5","s5"));
> myfactor_nc <- c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
> mydata_nc <- data.frame(dv, subject, myfactor_nc)
> 
> am1_nc <- aov(dv ~ myfactor_nc + Error(subject/myfactor_nc),
> data=mydata_nc)
> summary(am1_nc)
> 
> Error: subject
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  4   12.4     3.1
> 
> Error: subject:myfactor_nc
>              Df Sum Sq Mean Sq F value Pr(>F)
> myfactor_nc  1   14.4    14.4      16 0.0161 *
> Residuals    4    3.6     0.9
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Error: Within
>            Df Sum Sq Mean Sq F value Pr(>F)
> Residuals  5  1.333  0.2667
> 
> 
> *Anova()*
> 
> dvm <- with(mydata_nc, cbind(dv[myfactor_nc==1],dv[myfactor_nc==2],
> dv[myfactor_nc==3]))
> 
> mlm1 <- lm(dvm ~ 1)
> myfactor_nc <- factor(1:3)
> contrasts(myfactor_nc) <- matrix(-1:1, ncol=1)
> idata <- data.frame(myfactor_nc)
> Anova(mlm1, idata=idata, idesign=~myfactor_nc)
> Note: model has only an intercept; equivalent type-III tests
> substituted.
> 
> Type III Repeated Measures MANOVA Tests: Pillai test statistic
>              Df test stat approx F num Df den Df   Pr(>F)
> (Intercept)  1   0.93790   60.409      1      4 0.001477 **
> myfactor_nc  1   0.83478    7.579      2      3 0.067156 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> 
> Why is that?

There are two differences: (1) The repeated-measures analysis reported by default by Anova() is a MANOVA; if you want the univariate analysis comparable to aov(), you have to ask for it. (2) As I mentioned, I made a mistake in defining the contrasts for the factor; what I did implicitly included the quadratic component, which wasn't my intention. 

Here's the correct version, which, as you can see, produces the same result as aov():

> contrasts(myfactor_nc, 1) <- matrix(-1:1, ncol=1)
> contrasts(myfactor_nc)
  [,1]
1   -1
2    0
3    1

> idata <- data.frame(myfactor_nc)

> summary(Anova(mlm1, idata=idata, idesign=~myfactor_nc), multivariate=FALSE)
Note: model has only an intercept; equivalent type-III tests substituted.

Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

                SS num Df Error SS den Df      F   Pr(>F)   
(Intercept) 187.27      1     12.4      4 60.409 0.001477 **
myfactor_nc  14.40      1      3.6      4 16.000 0.016130 * 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

> summary(am1_nc) # your result using aov()

Error: subject
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  4   12.4     3.1               

Error: subject:myfactor_nc
            Df Sum Sq Mean Sq F value Pr(>F)  
myfactor_nc  1   14.4    14.4      16 0.0161 *
Residuals    4    3.6     0.9                 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Error: Within
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  5  1.333  0.2667     

My apologies for the confusion about the contrasts.

John
          
> 
> Thanks a lot
> Cristiano
> 
> 
> On 12/14/2015 05:25 PM, Fox, John wrote:
> > Dear Cristiano,
> >
> > If I understand correctly what you want to do, you should be able to
> use Anova() in the car package (your second question) by treating your
> numeric repeated-measures predictor as a factor and defining a single
> linear contrast for it.
> >
> > Continuing with your toy example:
> >
> >> myfactor_nc <- factor(1:3)
> >> contrasts(myfactor_nc) <- matrix(-1:1, ncol=1)
> >> idata <- data.frame(myfactor_nc)
> >> Anova(mlm1, idata=idata, idesign=~myfactor_nc)
> > Note: model has only an intercept; equivalent type-III tests
> substituted.
> >
> > Type III Repeated Measures MANOVA Tests: Pillai test statistic
> >              Df test stat approx F num Df den Df   Pr(>F)
> > (Intercept)  1   0.93790   60.409      1      4 0.001477 **
> > myfactor_nc  1   0.83478    7.579      2      3 0.067156 .
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> >
> > With just 3 distinct levels, however, you could just make myfactor_nc
> an ordered factor, not defining the contrasts explicitly, and then you'd
> get both linear and quadratic contrasts.
> >
> > I hope this helps,
> >   John
> >
> > -----------------------------------------------
> > John Fox, Professor
> > McMaster University
> > Hamilton, Ontario, Canada
> > http://socserv.socsci.mcmaster.ca/jfox/
> >
> >
> >
> >> -----Original Message-----
> >> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
> >> Cristiano Alessandro
> >> Sent: Monday, December 14, 2015 8:43 AM
> >> To: r-help at r-project.org
> >> Subject: [R] repeated measure with quantitative independent variable
> >>
> >> Hi all,
> >>
> >> I am new to R, and I am trying to set up a repeated measure analysis
> >> with a quantitative (as opposed to factorized/categorical)
> >> within-subjects variable. For a variety of reasons I am not using
> >> linear-mixed models, rather I am trying to fit a General Linear Model
> (I
> >> am aware of assumptions and limitations) to assess whether the value
> of
> >> the within-subjects variable affects statistically significantly the
> >> response variable. I have two questions. To make myself clear I
> propose
> >> the following exemplary dataset (where myfactor_nc is the
> quantitative
> >> within-subjects variable; i.e. each subject performs the experiment
> >> three times -- nc_factor=1,2,3 -- and produces the response in
> variable
> >> dv).
> >>
> >> dv <- c(1,3,4,2,2,3,2,5,6,3,4,4,3,5,6);
> >> subject <-
> >>
> factor(c("s1","s1","s1","s2","s2","s2","s3","s3","s3","s4","s4","s4","s5
> >> ","s5","s5"));
> >> myfactor_nc <- c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
> >> mydata_nc <- data.frame(dv, subject, myfactor_nc)
> >>
> >> *Question 1 (using function aov)*
> >>
> >> Easily done...
> >>
> >> am1_nc <- aov(dv ~ myfactor_nc + Error(subject/myfactor_nc),
> >> data=mydata_nc)
> >> summary(am1_nc)
> >>
> >> Unlike the case when myfactor_nc is categorical, this produces three
> >> error strata: Error: subject, Error: subject:myfactor_nc, Error:
> Within.
> >> I cannot understand the meaning of the latter. How is that computed?
> >>
> >> *Question 2 (using function lm)*
> >>
> >> Now I would like to do the same with the functions lm() and Anova()
> >> (from the car package). What I have done so far (please correct me if
> I
> >> am mistaking) is the following:
> >>
> >> # Unstack the dataset
> >> dvm <- with(mydata_nc, cbind(dv[myfactor_nc==1],dv[myfactor_nc==2],
> >> dv[myfactor_nc==3]))
> >>
> >> #Fit the linear model
> >> mlm1 <- lm(dvm ~ 1)
> >>
> >> (is that model above correct for my design?)
> >>
> >> Now I should use the Anova function, but it seems that it only
> accepts
> >> factors, and not quantitative within-subject variable.
> >>
> >> Any help is highly appreciated!
> >>
> >> Thanks
> >> Cristiano
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/posting-
> >> guide.html
> >> and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Mon Dec 14 20:54:53 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Mon, 14 Dec 2015 19:54:53 +0000
Subject: [R] Print minified XML in tree format
Message-ID: <248E6FA047A8C746BA491485764190F537156709@ESESSMB207.ericsson.se>

I may suggest this quick guide:

http://gastonsanchez.com/work/webdata/getting_web_data_r4_parsing_xml_html.pdf

and the following link:

http://www.r-datacollection.com/


I apologize for not being more specific.


--
GG



	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Mon Dec 14 21:01:45 2015
From: f_j_rod at hotmail.com (Frank S.)
Date: Mon, 14 Dec 2015 21:01:45 +0100
Subject: [R] Random selection of a fixed number of values by interval
Message-ID: <BAY168-W11585F0C67A674E725435BAED0@phx.gbl>

Dear R users,
 
I'm writing to this list because I must get a random sample (without replacement) from a given vector, but the clue is that I need to extract a fixed number of values by each prespecified 1-unit interval. As an example I try to say, I have a data frame that looks like this (my real dataframe is bigger):
 
data <- data.frame(id = 1:70, value=  c(0.68, 2.96, 1.93, 5.63, 3.08, 3.10, 2.99, 1.79, 2.96, 0.85, 11.79, 0.06, 4.31, 0.64, 1.43, 0.88, 2.79, 4.67,
      1.23, 1.43, 3.05, 2.44, 2.55, 3.82, 3.55, 1.56, 7.25, 2.75, 9.64, 5.14, 3.54, 3.12, 0.17, 1.07, 4.08, 4.47, 5.58, 7.41, 0.85, 4.30, 7.58,
      0.58, 1.40, 4.74, 5.04, 0.14, 1.14, 3.28, 7.84, 0.07, 3.97, 1.02, 3.47, 0.66, 2.38, 0.06, 0.67, 0.48, 4.48, 0.12, 3.82, 2.27, 0.93, 0.30, 
      0.73, 0.33, 2.91, 0.81, 0.18, 0.42))
 
And I would like to select, in a random manner:
 
10 id's whose value belongs to [0,1) interval
7 id's whose value belongs to [1,2)
5 id's whose value belongs to [2,3)
5 id's whose value belongs to [3,4)
3 id's whose value belongs to [4,5)
 
# I have the following values by each 1-unit interval:
table(cut(data$value, include.lowest = T, right = FALSE, breaks = 0:ceiling(max(data$value))))
 
and the size vector:
size <- c(10, 7, 5, 5, 3) 
 
But I'm not able to get it by using sample function. Does anyone have some idea?
 
Thank you very much for any suggestions!!
 
Frank S.

 
 
 
 		 	   		  
	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Mon Dec 14 21:46:49 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Mon, 14 Dec 2015 20:46:49 +0000
Subject: [R] Random selection of a fixed number of values by interval
In-Reply-To: <BAY168-W11585F0C67A674E725435BAED0@phx.gbl>
References: <BAY168-W11585F0C67A674E725435BAED0@phx.gbl>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E8E5B@mb02.ads.tamu.edu>

There are lots of ways to do this. For example,

> groups <- cut(data$value, include.lowest = T, right = FALSE,
+      breaks = 0:ceiling(max(data$value)))
> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
> size <- c(10, 7, 5, 5, 3)
> set.seed(42)
> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
+      size[x]))
> names(samples) <- grp
> samples
$`[0,1)`
 [1] 69 68 33 63 56 46 65 12 50 58

$`[1,2)`
[1] 20 34 43  8 15 52 19

$`[2,3)`
[1]  7 22 62 28  2

$`[3,4)`
[1] 61 53  5 25 21

$`[4,5)`
[1] 59 35 40

> 
> groups <- cut(data$value, include.lowest = T, right = FALSE,
+      breaks = 0:ceiling(max(data$value)))
> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
> size <- c(10, 7, 5, 5, 3)
> set.seed(42)
> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
+      size[x]))
> names(samples) <- grp
> samples
$`[0,1)`
 [1] 69 68 33 63 56 46 65 12 50 58

$`[1,2)`
[1] 20 34 43  8 15 52 19

$`[2,3)`
[1]  7 22 62 28  2

$`[3,4)`
[1] 61 53  5 25 21

$`[4,5)`
[1] 59 35 40


-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank S.
Sent: Monday, December 14, 2015 2:02 PM
To: r-help at r-project.org
Subject: [R] Random selection of a fixed number of values by interval

Dear R users,

I'm writing to this list because I must get a random sample (without replacement) from a given vector, but the clue is that I need to extract a fixed number of values by each prespecified 1-unit interval. As an example I try to say, I have a data frame that looks like this (my real dataframe is bigger):

data <- data.frame(id = 1:70, value=  c(0.68, 2.96, 1.93, 5.63, 3.08, 3.10, 2.99, 1.79, 2.96, 0.85, 11.79, 0.06, 4.31, 0.64, 1.43, 0.88, 2.79, 4.67,
      1.23, 1.43, 3.05, 2.44, 2.55, 3.82, 3.55, 1.56, 7.25, 2.75, 9.64, 5.14, 3.54, 3.12, 0.17, 1.07, 4.08, 4.47, 5.58, 7.41, 0.85, 4.30, 7.58,
      0.58, 1.40, 4.74, 5.04, 0.14, 1.14, 3.28, 7.84, 0.07, 3.97, 1.02, 3.47, 0.66, 2.38, 0.06, 0.67, 0.48, 4.48, 0.12, 3.82, 2.27, 0.93, 0.30, 
      0.73, 0.33, 2.91, 0.81, 0.18, 0.42))

And I would like to select, in a random manner:

10 id's whose value belongs to [0,1) interval
7 id's whose value belongs to [1,2)
5 id's whose value belongs to [2,3)
5 id's whose value belongs to [3,4)
3 id's whose value belongs to [4,5)

# I have the following values by each 1-unit interval:
table(cut(data$value, include.lowest = T, right = FALSE, breaks = 0:ceiling(max(data$value))))

and the size vector:
size <- c(10, 7, 5, 5, 3) 

But I'm not able to get it by using sample function. Does anyone have some idea?

Thank you very much for any suggestions!!

Frank S.




 		 	   		  
	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cri.alessandro at gmail.com  Mon Dec 14 20:10:44 2015
From: cri.alessandro at gmail.com (Cristiano Alessandro)
Date: Mon, 14 Dec 2015 20:10:44 +0100
Subject: [R] repeated measure with quantitative independent variable
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F42DA3@FHSDB2D11-2.csu.mcmaster.ca>
References: <566EC781.2040204@gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F42DA3@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <566F1434.8000209@gmail.com>

Dear John,

thanks for your reply! The reason why I did not want to factorize the 
within-subjects variable was to avoid increasing the Df of the model 
from 1 (continuous variable) to k-1 (where k is the number of levels of 
the factors). I am now confused, because you have factorized the 
variable (indeed using "factor"), but the Df of myfactor_nc seems to be 
1. Could you explain that?

Comparing the results obtained with the two methods I seem to get 
completely different results:

* aov()*

dv <- c(1,3,4,2,2,3,2,5,6,3,4,4,3,5,6);
subject <- 
factor(c("s1","s1","s1","s2","s2","s2","s3","s3","s3","s4","s4","s4","s5","s5","s5"));
myfactor_nc <- c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
mydata_nc <- data.frame(dv, subject, myfactor_nc)

am1_nc <- aov(dv ~ myfactor_nc + Error(subject/myfactor_nc), data=mydata_nc)
summary(am1_nc)

Error: subject
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals  4   12.4     3.1

Error: subject:myfactor_nc
             Df Sum Sq Mean Sq F value Pr(>F)
myfactor_nc  1   14.4    14.4      16 0.0161 *
Residuals    4    3.6     0.9
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Error: Within
           Df Sum Sq Mean Sq F value Pr(>F)
Residuals  5  1.333  0.2667


*Anova()*

dvm <- with(mydata_nc, cbind(dv[myfactor_nc==1],dv[myfactor_nc==2], 
dv[myfactor_nc==3]))

mlm1 <- lm(dvm ~ 1)
myfactor_nc <- factor(1:3)
contrasts(myfactor_nc) <- matrix(-1:1, ncol=1)
idata <- data.frame(myfactor_nc)
Anova(mlm1, idata=idata, idesign=~myfactor_nc)
Note: model has only an intercept; equivalent type-III tests substituted.

Type III Repeated Measures MANOVA Tests: Pillai test statistic
             Df test stat approx F num Df den Df   Pr(>F)
(Intercept)  1   0.93790   60.409      1      4 0.001477 **
myfactor_nc  1   0.83478    7.579      2      3 0.067156 .
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Why is that?

Thanks a lot
Cristiano


On 12/14/2015 05:25 PM, Fox, John wrote:
> Dear Cristiano,
>
> If I understand correctly what you want to do, you should be able to use Anova() in the car package (your second question) by treating your numeric repeated-measures predictor as a factor and defining a single linear contrast for it.
>
> Continuing with your toy example:
>
>> myfactor_nc <- factor(1:3)
>> contrasts(myfactor_nc) <- matrix(-1:1, ncol=1)
>> idata <- data.frame(myfactor_nc)
>> Anova(mlm1, idata=idata, idesign=~myfactor_nc)
> Note: model has only an intercept; equivalent type-III tests substituted.
>
> Type III Repeated Measures MANOVA Tests: Pillai test statistic
>              Df test stat approx F num Df den Df   Pr(>F)
> (Intercept)  1   0.93790   60.409      1      4 0.001477 **
> myfactor_nc  1   0.83478    7.579      2      3 0.067156 .
> ---
> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
>
> With just 3 distinct levels, however, you could just make myfactor_nc an ordered factor, not defining the contrasts explicitly, and then you'd get both linear and quadratic contrasts.
>
> I hope this helps,
>   John
>
> -----------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.socsci.mcmaster.ca/jfox/
>
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of
>> Cristiano Alessandro
>> Sent: Monday, December 14, 2015 8:43 AM
>> To: r-help at r-project.org
>> Subject: [R] repeated measure with quantitative independent variable
>>
>> Hi all,
>>
>> I am new to R, and I am trying to set up a repeated measure analysis
>> with a quantitative (as opposed to factorized/categorical)
>> within-subjects variable. For a variety of reasons I am not using
>> linear-mixed models, rather I am trying to fit a General Linear Model (I
>> am aware of assumptions and limitations) to assess whether the value of
>> the within-subjects variable affects statistically significantly the
>> response variable. I have two questions. To make myself clear I propose
>> the following exemplary dataset (where myfactor_nc is the quantitative
>> within-subjects variable; i.e. each subject performs the experiment
>> three times -- nc_factor=1,2,3 -- and produces the response in variable
>> dv).
>>
>> dv <- c(1,3,4,2,2,3,2,5,6,3,4,4,3,5,6);
>> subject <-
>> factor(c("s1","s1","s1","s2","s2","s2","s3","s3","s3","s4","s4","s4","s5
>> ","s5","s5"));
>> myfactor_nc <- c(1,2,3,1,2,3,1,2,3,1,2,3,1,2,3)
>> mydata_nc <- data.frame(dv, subject, myfactor_nc)
>>
>> *Question 1 (using function aov)*
>>
>> Easily done...
>>
>> am1_nc <- aov(dv ~ myfactor_nc + Error(subject/myfactor_nc),
>> data=mydata_nc)
>> summary(am1_nc)
>>
>> Unlike the case when myfactor_nc is categorical, this produces three
>> error strata: Error: subject, Error: subject:myfactor_nc, Error: Within.
>> I cannot understand the meaning of the latter. How is that computed?
>>
>> *Question 2 (using function lm)*
>>
>> Now I would like to do the same with the functions lm() and Anova()
>> (from the car package). What I have done so far (please correct me if I
>> am mistaking) is the following:
>>
>> # Unstack the dataset
>> dvm <- with(mydata_nc, cbind(dv[myfactor_nc==1],dv[myfactor_nc==2],
>> dv[myfactor_nc==3]))
>>
>> #Fit the linear model
>> mlm1 <- lm(dvm ~ 1)
>>
>> (is that model above correct for my design?)
>>
>> Now I should use the Anova function, but it seems that it only accepts
>> factors, and not quantitative within-subject variable.
>>
>> Any help is highly appreciated!
>>
>> Thanks
>> Cristiano
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From pai1981 at gmail.com  Mon Dec 14 20:20:31 2015
From: pai1981 at gmail.com (Debasish Pai Mazumder)
Date: Mon, 14 Dec 2015 12:20:31 -0700
Subject: [R] paneling spplot and hist
Message-ID: <CAM9mbiAXQJCswqyK6VjpDfrcY6VpMcEp6Cd1D1ciLJoW-V+7FQ@mail.gmail.com>

Hi all,

I am new in R. I am trying to panel spplot and hist.

spplot(hspdf, "CDP", col = "white", col.regions = blue2red(20), sp.layout =
list(l2), at = seq(1,10,1), colorkey = list(space = "bottom", labels =
list(labels = paste(seq(1,10,1)), cex = 1.5)), sub = list("CDP", cex = 1.5,
font = 2))

hist(cdp.obsc, col="grey", border="grey", main="CDP", probability=T)
lines(c.breaks, obs.cdp.d, col="blue")
lines(c.breaks, obs.cdp.e, col="red")


I have tried with par(mfrow=c(2,1))and layout(matrix...., both don't work.

Any advice?

with regards
-Pai

	[[alternative HTML version deleted]]


From adrian.waddell at gmail.com  Mon Dec 14 21:05:18 2015
From: adrian.waddell at gmail.com (Adrian Waddell)
Date: Mon, 14 Dec 2015 21:05:18 +0100
Subject: [R] choropleth packages (US)
In-Reply-To: <566A1735.5080206@gmail.com>
References: <56649494.3050206@gmail.com>
	<B4FD6FEF-D170-45B4-B51D-CB8D8070020B@univie.ac.at>
	<CACa4aQ6=SXpsf6QbBQ9pLGd=h_FHiVhGFVQ+_c4Z-p17v4dREQ@mail.gmail.com>
	<566A1735.5080206@gmail.com>
Message-ID: <CACa4aQ5QJWZZRmH-JT4OibB5=azRg=S-gMc0n_h3HDd-0hYNHA@mail.gmail.com>

Alaska and Hawaii can be found in the 'world' or 'world2' databases of
the 'maps' package. The following is a bit a hack but it works

----

library(maps)
library(scales)


mergeMaps <- function(...) {
  maps <- list(...)
  if (length(maps) < 2)
    stop("need at least two maps")
  map <- maps[[1]]
  for (i in 2:length(maps)) {
    map$x <- c(map$x, NA, maps[[i]]$x)
    map$y <- c(map$y, NA, maps[[i]]$y)
    map$names <- c(map$names, maps[[i]]$names)
  }
  map$range <- c(range(map$x, na.rm = TRUE), range(map$y, na.rm = TRUE))
  map
}

shiftMap <- function(map, xmin=-180) {
  sel <- !is.na(map$x)
  map$x <- (map$x - xmin) %% 360 + xmin
  map$range <- c(range(map$x, na.rm = TRUE), range(map$y, na.rm = TRUE))
  map
}

m <- shiftMap(mergeMaps(map('state', fill=TRUE, plot=FALSE),
                        map('world', 'USA:Alaska', fill=TRUE, plot=FALSE),
                        map('world', 'Hawaii', fill=TRUE, plot=FALSE)),
              xmin=0)

s_data <- tolower(rownames(USArrests))
s_map <- tolower(m$names)

mapping <- lapply(s_data, function(state) {
  which(grepl(state, s_map))
})
## check if the mapping is good!

col_pal <- col_numeric("Greens", domain=NULL, na.color = 'lightyellow')

cols <- rep('lightyellow', length(s_data))

Map(function(indices, col) {
  cols[indices] <<- col
}, mapping, col_pal(USArrests$UrbanPop))

map(m, col=cols, fill=TRUE)
# map.axexs()

## or with no borders
map(m, col=cols, fill=TRUE, border=NA)

----

Greetings,

Adrian



On Fri, Dec 11, 2015 at 1:22 AM, Benjamin Tyner <btyner at gmail.com> wrote:
> Very nice Adrian. Is there a straightforward way to add Alaska and Hawaii at
> the lower left? (without resorting to choroplethr package)
>
>
> On 12/10/2015 06:09 AM, Adrian Waddell wrote:
>>
>> Hi,
>>
>> You can also use the 'maps' package for the map data and the 'scales'
>> package for the color mapping.
>>
>> E.g.
>>
>> library(maps)
>> library(scales)
>>
>> m <- map('state', fill=TRUE, plot=FALSE)
>>
>> s_data <- tolower(rownames(USArrests))
>> s_map <- tolower(m$names)
>>
>> mapping <- lapply(s_data, function(state) {
>>    which(grepl(state, s_map))
>> })
>> ## check if the mapping is good!
>>
>> col_pal <- col_numeric("Greens", domain=NULL, na.color = 'lightyellow')
>>
>> cols <- rep('lightyellow', length(s_data))
>>
>> Map(function(indices, col) {
>>    cols[indices] <<- col
>> }, mapping, col_pal(USArrests$UrbanPop))
>>
>> map(m, col=cols, fill=TRUE)
>>
>>
>> Adrian
>>
>>
>>
>> On Mon, Dec 7, 2015 at 9:34 AM, Erich Neuwirth
>> <erich.neuwirth at univie.ac.at> wrote:
>>>
>>> ggplot2 also can do this with
>>> fortify
>>> geom_polygon
>>>
>>> Von meinem iPad gesendet
>>>
>>>> Am 06.12.2015 um 21:03 schrieb Benjamin Tyner <btyner at gmail.com>:
>>>>
>>>> Hi
>>>>
>>>> I wish to draw a basic choropleth (US, by state) and am wondering if
>>>> anyone has any recommendations? I've tried the following thus far:
>>>>
>>>> 1. choroplethr: this works, but required installation of 30+
>>>> dependencies. I would prefer something with fewer dependencies.
>>>> 2. tmap: this also seems promising, but most of the examples I saw were
>>>> specific to European maps. Can it be adapted for US?
>>>> 3. statebins: doesn't draw true choropleths, but I liked that it doesn't
>>>> have many dependencies.
>>>>
>>>> Regards
>>>> Ben
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>
>


From bgunter.4567 at gmail.com  Mon Dec 14 22:22:29 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 14 Dec 2015 13:22:29 -0800
Subject: [R] Random selection of a fixed number of values by interval
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E8E5B@mb02.ads.tamu.edu>
References: <BAY168-W11585F0C67A674E725435BAED0@phx.gbl>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E8E5B@mb02.ads.tamu.edu>
Message-ID: <CAGxFJbQ1RTYntUzw7U5ne+JkvZnVvJSz4QA-kE_vGodUV5Z8WQ@mail.gmail.com>

Yes.

May I suggest:

grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")

can be obtained more simply as
grp <- levels(groups)[1:5]

 and one slight aesthetic change in the indexing:

from:
samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]], size[x]))

to:
samples <- lapply(1:5, function(x) sample(data[groups==grp[x],"id"],  size[x]))

(rows and columns in a data frame can be simultaneously indexed)

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Dec 14, 2015 at 12:46 PM, David L Carlson <dcarlson at tamu.edu> wrote:
> There are lots of ways to do this. For example,
>
>> groups <- cut(data$value, include.lowest = T, right = FALSE,
> +      breaks = 0:ceiling(max(data$value)))
>> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
>> size <- c(10, 7, 5, 5, 3)
>> set.seed(42)
>> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
> +      size[x]))
>> names(samples) <- grp
>> samples
> $`[0,1)`
>  [1] 69 68 33 63 56 46 65 12 50 58
>
> $`[1,2)`
> [1] 20 34 43  8 15 52 19
>
> $`[2,3)`
> [1]  7 22 62 28  2
>
> $`[3,4)`
> [1] 61 53  5 25 21
>
> $`[4,5)`
> [1] 59 35 40
>
>>
>> groups <- cut(data$value, include.lowest = T, right = FALSE,
> +      breaks = 0:ceiling(max(data$value)))
>> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
>> size <- c(10, 7, 5, 5, 3)
>> set.seed(42)
>> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
> +      size[x]))
>> names(samples) <- grp
>> samples
> $`[0,1)`
>  [1] 69 68 33 63 56 46 65 12 50 58
>
> $`[1,2)`
> [1] 20 34 43  8 15 52 19
>
> $`[2,3)`
> [1]  7 22 62 28  2
>
> $`[3,4)`
> [1] 61 53  5 25 21
>
> $`[4,5)`
> [1] 59 35 40
>
>
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
>
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank S.
> Sent: Monday, December 14, 2015 2:02 PM
> To: r-help at r-project.org
> Subject: [R] Random selection of a fixed number of values by interval
>
> Dear R users,
>
> I'm writing to this list because I must get a random sample (without replacement) from a given vector, but the clue is that I need to extract a fixed number of values by each prespecified 1-unit interval. As an example I try to say, I have a data frame that looks like this (my real dataframe is bigger):
>
> data <- data.frame(id = 1:70, value=  c(0.68, 2.96, 1.93, 5.63, 3.08, 3.10, 2.99, 1.79, 2.96, 0.85, 11.79, 0.06, 4.31, 0.64, 1.43, 0.88, 2.79, 4.67,
>       1.23, 1.43, 3.05, 2.44, 2.55, 3.82, 3.55, 1.56, 7.25, 2.75, 9.64, 5.14, 3.54, 3.12, 0.17, 1.07, 4.08, 4.47, 5.58, 7.41, 0.85, 4.30, 7.58,
>       0.58, 1.40, 4.74, 5.04, 0.14, 1.14, 3.28, 7.84, 0.07, 3.97, 1.02, 3.47, 0.66, 2.38, 0.06, 0.67, 0.48, 4.48, 0.12, 3.82, 2.27, 0.93, 0.30,
>       0.73, 0.33, 2.91, 0.81, 0.18, 0.42))
>
> And I would like to select, in a random manner:
>
> 10 id's whose value belongs to [0,1) interval
> 7 id's whose value belongs to [1,2)
> 5 id's whose value belongs to [2,3)
> 5 id's whose value belongs to [3,4)
> 3 id's whose value belongs to [4,5)
>
> # I have the following values by each 1-unit interval:
> table(cut(data$value, include.lowest = T, right = FALSE, breaks = 0:ceiling(max(data$value))))
>
> and the size vector:
> size <- c(10, 7, 5, 5, 3)
>
> But I'm not able to get it by using sample function. Does anyone have some idea?
>
> Thank you very much for any suggestions!!
>
> Frank S.
>
>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Mon Dec 14 22:31:45 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 14 Dec 2015 13:31:45 -0800
Subject: [R] Random selection of a fixed number of values by interval
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E8E5B@mb02.ads.tamu.edu>
References: <BAY168-W11585F0C67A674E725435BAED0@phx.gbl>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E8E5B@mb02.ads.tamu.edu>
Message-ID: <82BD8F87-8347-4256-8868-A2D70C50FA3A@comcast.net>


> On Dec 14, 2015, at 12:46 PM, David L Carlson <dcarlson at tamu.edu> wrote:
> 
> There are lots of ways to do this. For example,

Another method with mapply:

 mapply(function( n, vals) {sample(vals$id, n)} ,   # no replacement is the default for sample
            vals= split(data, findInterval(data$value, 0:5) )[1:5] , # drops the values at 5 or above
            n=c(10,7,5,5,3)   )

$`1`
 [1] 12 64 10 60 70 58 33 50 57 68

$`2`
[1] 43  8 15 26 19  3 20

$`3`
[1] 55  9 62 17 67

$`4`
[1] 61 21 31 24 48

$`5`
[1] 44 13 36

> 
>> groups <- cut(data$value, include.lowest = T, right = FALSE,
> +      breaks = 0:ceiling(max(data$value)))
>> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
>> size <- c(10, 7, 5, 5, 3)
>> set.seed(42)
>> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
> +      size[x]))
>> names(samples) <- grp
>> samples
> $`[0,1)`
> [1] 69 68 33 63 56 46 65 12 50 58
> 
> $`[1,2)`
> [1] 20 34 43  8 15 52 19
> 
> $`[2,3)`
> [1]  7 22 62 28  2
> 
> $`[3,4)`
> [1] 61 53  5 25 21
> 
> $`[4,5)`
> [1] 59 35 40
> 
>> 
>> groups <- cut(data$value, include.lowest = T, right = FALSE,
> +      breaks = 0:ceiling(max(data$value)))
>> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
>> size <- c(10, 7, 5, 5, 3)
>> set.seed(42)
>> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
> +      size[x]))
>> names(samples) <- grp
>> samples
> $`[0,1)`
> [1] 69 68 33 63 56 46 65 12 50 58
> 
> $`[1,2)`
> [1] 20 34 43  8 15 52 19
> 
> $`[2,3)`
> [1]  7 22 62 28  2
> 
> $`[3,4)`
> [1] 61 53  5 25 21
> 
> $`[4,5)`
> [1] 59 35 40
> 
> 
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
> 
> 
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank S.
> Sent: Monday, December 14, 2015 2:02 PM
> To: r-help at r-project.org
> Subject: [R] Random selection of a fixed number of values by interval
> 
> Dear R users,
> 
> I'm writing to this list because I must get a random sample (without replacement) from a given vector, but the clue is that I need to extract a fixed number of values by each prespecified 1-unit interval. As an example I try to say, I have a data frame that looks like this (my real dataframe is bigger):
> 
> data <- data.frame(id = 1:70, value=  c(0.68, 2.96, 1.93, 5.63, 3.08, 3.10, 2.99, 1.79, 2.96, 0.85, 11.79, 0.06, 4.31, 0.64, 1.43, 0.88, 2.79, 4.67,
>      1.23, 1.43, 3.05, 2.44, 2.55, 3.82, 3.55, 1.56, 7.25, 2.75, 9.64, 5.14, 3.54, 3.12, 0.17, 1.07, 4.08, 4.47, 5.58, 7.41, 0.85, 4.30, 7.58,
>      0.58, 1.40, 4.74, 5.04, 0.14, 1.14, 3.28, 7.84, 0.07, 3.97, 1.02, 3.47, 0.66, 2.38, 0.06, 0.67, 0.48, 4.48, 0.12, 3.82, 2.27, 0.93, 0.30, 
>      0.73, 0.33, 2.91, 0.81, 0.18, 0.42))
> 
> And I would like to select, in a random manner:
> 
> 10 id's whose value belongs to [0,1) interval
> 7 id's whose value belongs to [1,2)
> 5 id's whose value belongs to [2,3)
> 5 id's whose value belongs to [3,4)
> 3 id's whose value belongs to [4,5)
> 
> # I have the following values by each 1-unit interval:
> table(cut(data$value, include.lowest = T, right = FALSE, breaks = 0:ceiling(max(data$value))))
> 
> and the size vector:
> size <- c(10, 7, 5, 5, 3) 
> 
> But I'm not able to get it by using sample function. Does anyone have some idea?
> 
> Thank you very much for any suggestions!!
> 
> Frank S.
> 
> 
> 
> 
> 		 	   		  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dleybman at gmail.com  Mon Dec 14 22:34:48 2015
From: dleybman at gmail.com (Dmitri Leybman)
Date: Mon, 14 Dec 2015 16:34:48 -0500
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
Message-ID: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>

I have a spreadsheet with five different columns standing for five
different variables:

Variable 1 Variable 2 Variable 3 Variable 4 Variable 5 0 0 7 1 0 0 0 7 0 0 1
1 8 2 0 5 5 8 0 0 1 4 8 1 0 4 5 8 0 0 0 1 7 2 1
I am trying to create five box and whiskers plots on a single graph with a
five x-label ticks named for each one of
the variables along with color coding. The names for the x-label would be
"Meeting"[ pertains to Variable1] "Meeting2"[pertains to Variable 2]
Meeting3[pertains to Variable 3], Meeting4[pertains to Variable4],
Meeting5[pertains to Variable5].

I have tried:

boxplot(data, las = 2, col =
c("red", "blue", "black", "aquamarine1", "darkorange3")
, at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),
names = c("Meeting 1", "'Meeting2", "Meeting3", "Meeting4","Meeting5")


Error: unexpected string constant in:
"boxplot(data, las=2, col= c('red', 'blue', 'red', 'red', red')
boxplot(data, las=2, col= c('"

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Mon Dec 14 22:54:35 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 14 Dec 2015 13:54:35 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
Message-ID: <4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>


> On Dec 14, 2015, at 1:34 PM, Dmitri Leybman <dleybman at gmail.com> wrote:
> 
> I have a spreadsheet with five different columns standing for five
> different variables:
> 
> Variable 1 Variable 2 Variable 3 Variable 4 Variable 5 0 0 7 1 0 0 0 7 0 0 1
> 1 8 2 0 5 5 8 0 0 1 4 8 1 0 4 5 8 0 0 0 1 7 2 1
> I am trying to create five box and whiskers plots on a single graph with a
> five x-label ticks named for each one of
> the variables along with color coding. The names for the x-label would be
> "Meeting"[ pertains to Variable1] "Meeting2"[pertains to Variable 2]
> Meeting3[pertains to Variable 3], Meeting4[pertains to Variable4],
> Meeting5[pertains to Variable5].
> 
> I have tried:
> 
> boxplot(data, las = 2, col =
> c("red", "blue", "black", "aquamarine1", "darkorange3")
> , at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),
> names = c("Meeting 1", "'Meeting2",
                         ^
Extra single quote about here


> "Meeting3", "Meeting4","Meeting5")
> 
> 
> Error: unexpected string constant in:
> "boxplot(data, las=2, col= c('red', 'blue', 'red', 'red', red')
> boxplot(data, las=2, col= c('"
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From pdalgd at gmail.com  Mon Dec 14 23:42:03 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 14 Dec 2015 23:42:03 +0100
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
Message-ID: <6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>


> On 14 Dec 2015, at 22:54 , David Winsemius <dwinsemius at comcast.net> wrote:
> 
>> 
>> On Dec 14, 2015, at 1:34 PM, Dmitri Leybman <dleybman at gmail.com> wrote:
>> 
>> I have a spreadsheet with five different columns standing for five
>> different variables:
>> 
>> Variable 1 Variable 2 Variable 3 Variable 4 Variable 5 0 0 7 1 0 0 0 7 0 0 1
>> 1 8 2 0 5 5 8 0 0 1 4 8 1 0 4 5 8 0 0 0 1 7 2 1
>> I am trying to create five box and whiskers plots on a single graph with a
>> five x-label ticks named for each one of
>> the variables along with color coding. The names for the x-label would be
>> "Meeting"[ pertains to Variable1] "Meeting2"[pertains to Variable 2]
>> Meeting3[pertains to Variable 3], Meeting4[pertains to Variable4],
>> Meeting5[pertains to Variable5].
>> 
>> I have tried:
>> 
>> boxplot(data, las = 2, col =
>> c("red", "blue", "black", "aquamarine1", "darkorange3")
>> , at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),
>> names = c("Meeting 1", "'Meeting2",
>                         ^
> Extra single quote about here
> 

Hmm, at face value, that should just become part of the label...

> 
>> "Meeting3", "Meeting4","Meeting5")
>> 
>> 
>> Error: unexpected string constant in:
>> "boxplot(data, las=2, col= c('red', 'blue', 'red', 'red', red')
>> boxplot(data, las=2, col= c('"


...but the error message doesn't match the input, which is lacking a right parenthesis. So is there a previous incomplete command maybe, or are we just being shown random snippets of things that didn't work??

I think we need to see a full transcript


>> 
>> 	[[alternative HTML version deleted]]
>> 


...and PLEASE in plain text, not HTML.

>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sidoti.23 at buckeyemail.osu.edu  Tue Dec 15 01:48:34 2015
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Tue, 15 Dec 2015 00:48:34 +0000
Subject: [R] Hexbin: Counting Bins That Meet Certain Criteria
Message-ID: <CY1PR0101MB10048C9DFF962DAF602BFF63ABEE0@CY1PR0101MB1004.prod.exchangelabs.com>

Greetings!

Is there a way to count the bins in a hexbin plot that meet certain criteria? For instance, what if I wanted to count the bins (hexes) that have a datapoint density of some number x?

Thank you!


From judyoringe at naver.com  Tue Dec 15 03:06:26 2015
From: judyoringe at naver.com (Ahreum Lee)
Date: Tue, 15 Dec 2015 11:06:26 +0900 (KST)
Subject: [R] =?utf-8?q?Draw_a_dendrogram_with_ROCK_clustering_=28cba_packa?=
 =?utf-8?b?Z2Up?=
Message-ID: <bd39ed46145de8c6e6bcdbef84afe9e@cweb08.nm.nhnsystem.com>

 Dear all. 
 
Hi, 
Currently, i do clustering analysis with several algorithms in R.  
one of them is ROCK clustering.  
 
Thanks to "cba" package in R, i could easily analyze with ROCK.
(https://cran.r-project.org/web/packages/cba/cba.pdf)
 
but what I want to see is a dendrogram which could be intuitive to define the number of clusters. 
 
I tried to find possible functions which could draw a dendrogram within cba packages.
but i couldn't...
 
Is there any way to draw a dendrogram with ROCK clustering outcome ?  
 
Any comments would be helpful for me to keep on this research.


Thanks !




All the best, 


Ahreum Lee. 

	[[alternative HTML version deleted]]


From f_j_rod at hotmail.com  Tue Dec 15 10:27:39 2015
From: f_j_rod at hotmail.com (Frank S.)
Date: Tue, 15 Dec 2015 10:27:39 +0100
Subject: [R] Random selection of a fixed number of values by interval
In-Reply-To: <82BD8F87-8347-4256-8868-A2D70C50FA3A@comcast.net>
References: <BAY168-W11585F0C67A674E725435BAED0@phx.gbl>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E8E5B@mb02.ads.tamu.edu>,
	<82BD8F87-8347-4256-8868-A2D70C50FA3A@comcast.net>
Message-ID: <BAY168-W104ED84A8EFF5D8831D81F2BAEE0@phx.gbl>

Many thanks to David L Carlson, Ben Gunter and David Winsemius for your quick and very elegant solutions!!
With your  list answers I am learning sa lot of things that will help me in the future to program.
 
Best,
 
Frank S.
 
> Subject: Re: [R] Random selection of a fixed number of values by interval
> From: dwinsemius at comcast.net
> Date: Mon, 14 Dec 2015 13:31:45 -0800
> CC: f_j_rod at hotmail.com; r-help at r-project.org
> To: dcarlson at tamu.edu
> 
> 
> > On Dec 14, 2015, at 12:46 PM, David L Carlson <dcarlson at tamu.edu> wrote:
> > 
> > There are lots of ways to do this. For example,
> 
> Another method with mapply:
> 
>  mapply(function( n, vals) {sample(vals$id, n)} ,   # no replacement is the default for sample
>             vals= split(data, findInterval(data$value, 0:5) )[1:5] , # drops the values at 5 or above
>             n=c(10,7,5,5,3)   )
> 
> $`1`
>  [1] 12 64 10 60 70 58 33 50 57 68
> 
> $`2`
> [1] 43  8 15 26 19  3 20
> 
> $`3`
> [1] 55  9 62 17 67
> 
> $`4`
> [1] 61 21 31 24 48
> 
> $`5`
> [1] 44 13 36
> 
> > 
> >> groups <- cut(data$value, include.lowest = T, right = FALSE,
> > +      breaks = 0:ceiling(max(data$value)))
> >> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
> >> size <- c(10, 7, 5, 5, 3)
> >> set.seed(42)
> >> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
> > +      size[x]))
> >> names(samples) <- grp
> >> samples
> > $`[0,1)`
> > [1] 69 68 33 63 56 46 65 12 50 58
> > 
> > $`[1,2)`
> > [1] 20 34 43  8 15 52 19
> > 
> > $`[2,3)`
> > [1]  7 22 62 28  2
> > 
> > $`[3,4)`
> > [1] 61 53  5 25 21
> > 
> > $`[4,5)`
> > [1] 59 35 40
> > 
> >> 
> >> groups <- cut(data$value, include.lowest = T, right = FALSE,
> > +      breaks = 0:ceiling(max(data$value)))
> >> grp <- c("[0,1)", "[1,2)", "[2,3)", "[3,4)", "[4,5)")
> >> size <- c(10, 7, 5, 5, 3)
> >> set.seed(42)
> >> samples <- lapply(1:5, function(x) sample(data$id[groups==grp[x]],
> > +      size[x]))
> >> names(samples) <- grp
> >> samples
> > $`[0,1)`
> > [1] 69 68 33 63 56 46 65 12 50 58
> > 
> > $`[1,2)`
> > [1] 20 34 43  8 15 52 19
> > 
> > $`[2,3)`
> > [1]  7 22 62 28  2
> > 
> > $`[3,4)`
> > [1] 61 53  5 25 21
> > 
> > $`[4,5)`
> > [1] 59 35 40
> > 
> > 
> > -------------------------------------
> > David L Carlson
> > Department of Anthropology
> > Texas A&M University
> > College Station, TX 77840-4352
> > 
> > 
> > 
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Frank S.
> > Sent: Monday, December 14, 2015 2:02 PM
> > To: r-help at r-project.org
> > Subject: [R] Random selection of a fixed number of values by interval
> > 
> > Dear R users,
> > 
> > I'm writing to this list because I must get a random sample (without replacement) from a given vector, but the clue is that I need to extract a fixed number of values by each prespecified 1-unit interval. As an example I try to say, I have a data frame that looks like this (my real dataframe is bigger):
> > 
> > data <- data.frame(id = 1:70, value=  c(0.68, 2.96, 1.93, 5.63, 3.08, 3.10, 2.99, 1.79, 2.96, 0.85, 11.79, 0.06, 4.31, 0.64, 1.43, 0.88, 2.79, 4.67,
> >      1.23, 1.43, 3.05, 2.44, 2.55, 3.82, 3.55, 1.56, 7.25, 2.75, 9.64, 5.14, 3.54, 3.12, 0.17, 1.07, 4.08, 4.47, 5.58, 7.41, 0.85, 4.30, 7.58,
> >      0.58, 1.40, 4.74, 5.04, 0.14, 1.14, 3.28, 7.84, 0.07, 3.97, 1.02, 3.47, 0.66, 2.38, 0.06, 0.67, 0.48, 4.48, 0.12, 3.82, 2.27, 0.93, 0.30, 
> >      0.73, 0.33, 2.91, 0.81, 0.18, 0.42))
> > 
> > And I would like to select, in a random manner:
> > 
> > 10 id's whose value belongs to [0,1) interval
> > 7 id's whose value belongs to [1,2)
> > 5 id's whose value belongs to [2,3)
> > 5 id's whose value belongs to [3,4)
> > 3 id's whose value belongs to [4,5)
> > 
> > # I have the following values by each 1-unit interval:
> > table(cut(data$value, include.lowest = T, right = FALSE, breaks = 0:ceiling(max(data$value))))
> > 
> > and the size vector:
> > size <- c(10, 7, 5, 5, 3) 
> > 
> > But I'm not able to get it by using sample function. Does anyone have some idea?
> > 
> > Thank you very much for any suggestions!!
> > 
> > Frank S.
> > 
> > 
> > 
> > 
> > 		 	   		  
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
 		 	   		  
	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Tue Dec 15 10:53:41 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 15 Dec 2015 09:53:41 +0000
Subject: [R] plot several lines programmaticaly
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004A6D@SRVEXCHMBX.precheza.cz>

Dear all

I am stuck with output of some result. Sorry for not providing working example but res is quite a big list and I believe you are able to understand my problem.

I have npks variable, which can be anything from 1 to let say 5.

I get result called here res as a nested list with several components and I want to plot these components based on npks value. I got this far

par(mfrow=c(npks,1)) # slit the device
for (i in 1:npks) {
plot(res[[i]]$x, res[[i]]$y, pch=20)

if (i ==1) { #lines in case npks==1
lines(res[[i]]$x, res[[i]]$fitpk[i,], col=i, lwd=3)}

if(i==2) { # lines in case npks ==2
lines(res[[i]]$x, res[[i]]$fitpk[1,], col=1, lwd=3)
lines(res[[i]]$x, res[[i]]$fitpk[i,], col=i, lwd=3)
}

I can follow this further and expand lines plotting with ifs but it seems to me that there must be better solution I overlooked.

Here is a structure of res

> str(res)
List of 2
 $ :List of 15
  ..$ intnr  : int 1
  ..$ x      : Named num [1:153] 34.8 34.8 34.9 34.9 34.9 ...
  .. ..- attr(*, "names")= chr [1:153] "2484" "2485" "2486" "2487" ...
  ..$ y      : num [1:153] 3.08 -2.91 5.1 12.1 42.11 ...
  ..$ fit    : num [1:153] 15.2 15.3 15.4 15.5 15.7 ...
  ..$ fitpk  : num [1, 1:153] 2.35 2.49 2.65 2.81 2.99 ...
  ..$ basl   : num [1:153] 258 258 258 258 258 ...
  ..$ baslchg: num [1:153] 12.8 12.8 12.7 12.7 12.7 ...
  ..$ rss    : num 1354
  ..$ num.ker: int 1
  ..$ par    : num [1:6] 12.8649 -0.0409 4549.1417 1.9927 66.939 ...
  ..$ parbl  : Named num [1:2] 12.86 -4.09
  .. ..- attr(*, "names")= chr [1:2] "" "2485"
  ..$ parpks : num [1, 1:5] 35.494 4549.142 715.324 0.129 1.993
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:5] "loc" "height" "intens" "FWHM" ...
  ..$ accept : logi FALSE
  ..$ alpha  : num 0.1
  ..$ thresh : Named num 3.81
  .. ..- attr(*, "names")= chr "90%"
 $ :List of 15
  ..$ intnr  : int 1
  ..$ x      : Named num [1:153] 34.8 34.8 34.9 34.9 34.9 ...
  .. ..- attr(*, "names")= chr [1:153] "2484" "2485" "2486" "2487" ...
  ..$ y      : num [1:153] 3.08 -2.91 5.1 12.1 42.11 ...
  ..$ fit    : num [1:153] 15 15 15.1 15.1 15.2 ...
  ..$ fitpk  : num [1:2, 1:153] 2.1329 0.0859 2.2666 0.0876 2.4109 ...
  ..$ basl   : num [1:153] 258 258 258 258 258 ...
  ..$ baslchg: num [1:153] 12.8 12.7 12.6 12.5 12.4 ...
  ..$ rss    : num 1325
  ..$ num.ker: num 2
  ..$ par    : num [1:10] 12.8649 -0.0942 4540.4742 2.0373 66.9394 ...
  ..$ parbl  : Named num [1:2] 12.86 -9.42
  .. ..- attr(*, "names")= chr [1:2] "" "2485"
  ..$ parpks : num [1:2, 1:5] 35.5 35.9 4540.5 39.4 715.3 ...
  .. ..- attr(*, "dimnames")=List of 2
  .. .. ..$ : NULL
  .. .. ..$ : chr [1:5] "loc" "height" "intens" "FWHM" ...
  ..$ accept : logi FALSE
  ..$ alpha  : num 0.1
  ..$ thresh : Named num 3.81
  .. ..- attr(*, "names")= chr "90%"
>

Best Regards
Petr


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From Gerrit.Eichner at math.uni-giessen.de  Tue Dec 15 12:30:41 2015
From: Gerrit.Eichner at math.uni-giessen.de (Gerrit Eichner)
Date: Tue, 15 Dec 2015 12:30:41 +0100 (MET)
Subject: [R] plot several lines programmaticaly
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004A6D@SRVEXCHMBX.precheza.cz>
References: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004A6D@SRVEXCHMBX.precheza.cz>
Message-ID: <Pine.SOC.4.64.1512151223360.11200@solcom.hrz.uni-giessen.de>

Hi, Petr,

from your example it's not doubtlessly clear to me sure how the sequence 
of ifs should really be continued, but couldn't a nested loop help? 
Something like:

for (i in 1:npks) {
  plot(res[[i]]$x, res[[i]]$y, pch=20)

  for (j in 1:i) {
   lines(res[[i]]$x, res[[i]]$fitpk[j,], col=j, lwd=3)}
   }
  }

(However, this seems too simple ...)

  Hth  --  Gerrit


On Tue, 15 Dec 2015, PIKAL Petr wrote:

> Dear all
>
> I am stuck with output of some result. Sorry for not providing working 
> example but res is quite a big list and I believe you are able to 
> understand my problem.
>
> I have npks variable, which can be anything from 1 to let say 5.
>
> I get result called here res as a nested list with several components 
> and I want to plot these components based on npks value. I got this far
>
> par(mfrow=c(npks,1)) # slit the device
> for (i in 1:npks) {
> plot(res[[i]]$x, res[[i]]$y, pch=20)
>
> if (i ==1) { #lines in case npks==1
> lines(res[[i]]$x, res[[i]]$fitpk[i,], col=i, lwd=3)}
>
> if(i==2) { # lines in case npks ==2
> lines(res[[i]]$x, res[[i]]$fitpk[1,], col=1, lwd=3)
> lines(res[[i]]$x, res[[i]]$fitpk[i,], col=i, lwd=3)
> }
>
> I can follow this further and expand lines plotting with ifs but it 
> seems to me that there must be better solution I overlooked.
>
> Here is a structure of res
>
>> str(res)
> List of 2
> $ :List of 15
>  ..$ intnr  : int 1
>  ..$ x      : Named num [1:153] 34.8 34.8 34.9 34.9 34.9 ...
>  .. ..- attr(*, "names")= chr [1:153] "2484" "2485" "2486" "2487" ...
>  ..$ y      : num [1:153] 3.08 -2.91 5.1 12.1 42.11 ...
>  ..$ fit    : num [1:153] 15.2 15.3 15.4 15.5 15.7 ...
>  ..$ fitpk  : num [1, 1:153] 2.35 2.49 2.65 2.81 2.99 ...
>  ..$ basl   : num [1:153] 258 258 258 258 258 ...
>  ..$ baslchg: num [1:153] 12.8 12.8 12.7 12.7 12.7 ...
>  ..$ rss    : num 1354
>  ..$ num.ker: int 1
>  ..$ par    : num [1:6] 12.8649 -0.0409 4549.1417 1.9927 66.939 ...
>  ..$ parbl  : Named num [1:2] 12.86 -4.09
>  .. ..- attr(*, "names")= chr [1:2] "" "2485"
>  ..$ parpks : num [1, 1:5] 35.494 4549.142 715.324 0.129 1.993
>  .. ..- attr(*, "dimnames")=List of 2
>  .. .. ..$ : NULL
>  .. .. ..$ : chr [1:5] "loc" "height" "intens" "FWHM" ...
>  ..$ accept : logi FALSE
>  ..$ alpha  : num 0.1
>  ..$ thresh : Named num 3.81
>  .. ..- attr(*, "names")= chr "90%"
> $ :List of 15
>  ..$ intnr  : int 1
>  ..$ x      : Named num [1:153] 34.8 34.8 34.9 34.9 34.9 ...
>  .. ..- attr(*, "names")= chr [1:153] "2484" "2485" "2486" "2487" ...
>  ..$ y      : num [1:153] 3.08 -2.91 5.1 12.1 42.11 ...
>  ..$ fit    : num [1:153] 15 15 15.1 15.1 15.2 ...
>  ..$ fitpk  : num [1:2, 1:153] 2.1329 0.0859 2.2666 0.0876 2.4109 ...
>  ..$ basl   : num [1:153] 258 258 258 258 258 ...
>  ..$ baslchg: num [1:153] 12.8 12.7 12.6 12.5 12.4 ...
>  ..$ rss    : num 1325
>  ..$ num.ker: num 2
>  ..$ par    : num [1:10] 12.8649 -0.0942 4540.4742 2.0373 66.9394 ...
>  ..$ parbl  : Named num [1:2] 12.86 -9.42
>  .. ..- attr(*, "names")= chr [1:2] "" "2485"
>  ..$ parpks : num [1:2, 1:5] 35.5 35.9 4540.5 39.4 715.3 ...
>  .. ..- attr(*, "dimnames")=List of 2
>  .. .. ..$ : NULL
>  .. .. ..$ : chr [1:5] "loc" "height" "intens" "FWHM" ...
>  ..$ accept : logi FALSE
>  ..$ alpha  : num 0.1
>  ..$ thresh : Named num 3.81
>  .. ..- attr(*, "names")= chr "90%"
>>
>
> Best Regards
> Petr


From bodenhofer at bioinf.jku.at  Tue Dec 15 12:57:23 2015
From: bodenhofer at bioinf.jku.at (Ulrich Bodenhofer)
Date: Tue, 15 Dec 2015 12:57:23 +0100
Subject: [R] define number of clusters in kmeans/apcluster analysis
Message-ID: <56700023.2090007@bioinf.jku.at>

Dear Luigi,

As the others have replied already, you cannot expect a clustering 
algorithm to produce exactly the result that you expect intuitively. The 
results of clustering algorithms depend largely on the parameters and, 
even more importantly, on the distance/similarity measure that is used. 
k-means, for instance, uses the Euclidean distance. As a result, it 
works nicely for spherical clusters that have approximately the same 
radius. APCluster, unless you don't choose a different similarity, uses 
negative squared distances which leads to very similar properties. Your 
data set consists of two clusters, one of which is much more spread out. 
That some parts of the larger cluster are being assigned to the other 
cluster looks weird, but it is perfectly explained by the properties of 
the algorithms. There is a lot of literature about the properties of 
clustering algorithms around. That's my 2 cents about this. In your 
case, however, as already pointed out in Bill Dunlap's reply, the 
scaling is the more important issue. k-means and apcluster do not 
perform any scaling of the data. Your two axes differ strongly in terms 
of scaling. Enter the following to see how the two clustering algorithms 
"see" your data (i.e. with two equally scaled axes):

     plot(z, xlim=c(0, 50), ylim=c(0, 50))

Given this, it is no longer surprising that both algorithms split the 
data in the way they do.

Actually, if you re-scale the data, apcluster produces the result you 
expect:

    z2 <- scale(z)
    m <- apclusterK(negDistMat(r=2), z2, K=2, verbose=TRUE)
    plot(m, z2)
    plot(m, z) ## it even works to superimpose the clustering result on
    the original data

I hope that helps.

Best regards,
Ulrich


From petr.pikal at precheza.cz  Tue Dec 15 13:23:06 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 15 Dec 2015 12:23:06 +0000
Subject: [R] plot several lines programmaticaly
In-Reply-To: <Pine.SOC.4.64.1512151223360.11200@solcom.hrz.uni-giessen.de>
References: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004A6D@SRVEXCHMBX.precheza.cz>
	<Pine.SOC.4.64.1512151223360.11200@solcom.hrz.uni-giessen.de>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004B1A@SRVEXCHMBX.precheza.cz>

Hi Gerrit,

Thanks, I tried it and it seems to be viable option. I knew I overlooked something pretty simple.

Cheers
Petr


> -----Original Message-----
> From: Gerrit Eichner [mailto:Gerrit.Eichner at math.uni-giessen.de]
> Sent: Tuesday, December 15, 2015 12:31 PM
> To: PIKAL Petr
> Cc: r-help at r-project.org
> Subject: Re: [R] plot several lines programmaticaly
>
> Hi, Petr,
>
> from your example it's not doubtlessly clear to me sure how the
> sequence of ifs should really be continued, but couldn't a nested loop
> help?
> Something like:
>
> for (i in 1:npks) {
>   plot(res[[i]]$x, res[[i]]$y, pch=20)
>
>   for (j in 1:i) {
>    lines(res[[i]]$x, res[[i]]$fitpk[j,], col=j, lwd=3)}
>    }
>   }
>
> (However, this seems too simple ...)
>
>   Hth  --  Gerrit
>
>
> On Tue, 15 Dec 2015, PIKAL Petr wrote:
>
> > Dear all
> >
> > I am stuck with output of some result. Sorry for not providing
> working
> > example but res is quite a big list and I believe you are able to
> > understand my problem.
> >
> > I have npks variable, which can be anything from 1 to let say 5.
> >
> > I get result called here res as a nested list with several components
> > and I want to plot these components based on npks value. I got this
> > far
> >
> > par(mfrow=c(npks,1)) # slit the device for (i in 1:npks) {
> > plot(res[[i]]$x, res[[i]]$y, pch=20)
> >
> > if (i ==1) { #lines in case npks==1
> > lines(res[[i]]$x, res[[i]]$fitpk[i,], col=i, lwd=3)}
> >
> > if(i==2) { # lines in case npks ==2
> > lines(res[[i]]$x, res[[i]]$fitpk[1,], col=1, lwd=3) lines(res[[i]]$x,
> > res[[i]]$fitpk[i,], col=i, lwd=3) }
> >
> > I can follow this further and expand lines plotting with ifs but it
> > seems to me that there must be better solution I overlooked.
> >
> > Here is a structure of res
> >
> >> str(res)
> > List of 2
> > $ :List of 15
> >  ..$ intnr  : int 1
> >  ..$ x      : Named num [1:153] 34.8 34.8 34.9 34.9 34.9 ...
> >  .. ..- attr(*, "names")= chr [1:153] "2484" "2485" "2486" "2487" ...
> >  ..$ y      : num [1:153] 3.08 -2.91 5.1 12.1 42.11 ...
> >  ..$ fit    : num [1:153] 15.2 15.3 15.4 15.5 15.7 ...
> >  ..$ fitpk  : num [1, 1:153] 2.35 2.49 2.65 2.81 2.99 ...
> >  ..$ basl   : num [1:153] 258 258 258 258 258 ...
> >  ..$ baslchg: num [1:153] 12.8 12.8 12.7 12.7 12.7 ...
> >  ..$ rss    : num 1354
> >  ..$ num.ker: int 1
> >  ..$ par    : num [1:6] 12.8649 -0.0409 4549.1417 1.9927 66.939 ...
> >  ..$ parbl  : Named num [1:2] 12.86 -4.09  .. ..- attr(*, "names")=
> > chr [1:2] "" "2485"
> >  ..$ parpks : num [1, 1:5] 35.494 4549.142 715.324 0.129 1.993  ..
> ..-
> > attr(*, "dimnames")=List of 2  .. .. ..$ : NULL  .. .. ..$ : chr
> [1:5]
> > "loc" "height" "intens" "FWHM" ...
> >  ..$ accept : logi FALSE
> >  ..$ alpha  : num 0.1
> >  ..$ thresh : Named num 3.81
> >  .. ..- attr(*, "names")= chr "90%"
> > $ :List of 15
> >  ..$ intnr  : int 1
> >  ..$ x      : Named num [1:153] 34.8 34.8 34.9 34.9 34.9 ...
> >  .. ..- attr(*, "names")= chr [1:153] "2484" "2485" "2486" "2487" ...
> >  ..$ y      : num [1:153] 3.08 -2.91 5.1 12.1 42.11 ...
> >  ..$ fit    : num [1:153] 15 15 15.1 15.1 15.2 ...
> >  ..$ fitpk  : num [1:2, 1:153] 2.1329 0.0859 2.2666 0.0876 2.4109 ...
> >  ..$ basl   : num [1:153] 258 258 258 258 258 ...
> >  ..$ baslchg: num [1:153] 12.8 12.7 12.6 12.5 12.4 ...
> >  ..$ rss    : num 1325
> >  ..$ num.ker: num 2
> >  ..$ par    : num [1:10] 12.8649 -0.0942 4540.4742 2.0373 66.9394 ...
> >  ..$ parbl  : Named num [1:2] 12.86 -9.42  .. ..- attr(*, "names")=
> > chr [1:2] "" "2485"
> >  ..$ parpks : num [1:2, 1:5] 35.5 35.9 4540.5 39.4 715.3 ...
> >  .. ..- attr(*, "dimnames")=List of 2
> >  .. .. ..$ : NULL
> >  .. .. ..$ : chr [1:5] "loc" "height" "intens" "FWHM" ...
> >  ..$ accept : logi FALSE
> >  ..$ alpha  : num 0.1
> >  ..$ thresh : Named num 3.81
> >  .. ..- attr(*, "names")= chr "90%"
> >>
> >
> > Best Regards
> > Petr

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From pgilbert902 at gmail.com  Tue Dec 15 15:28:57 2015
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Tue, 15 Dec 2015 09:28:57 -0500
Subject: [R] matrix which results singular but at the same time positive
 definite
In-Reply-To: <mailman.5.1449831602.13581.r-help@r-project.org>
References: <mailman.5.1449831602.13581.r-help@r-project.org>
Message-ID: <567023A9.7040306@gmail.com>

Stefano

I think in other response to in this thread you got the answer to the 
question you asked, but you may be asking the wrong question. I'm not 
familiar with the specific papers you mention and you have not provided 
enough detail about what you are doing, so I am guessing a bit. The term 
"dynamic linear model" can refer to both linear ARMA/ARIMA models and to 
linear state-space models, however some authors use it to refer 
exclusively to state-space models and from your phrasing I am guessing 
you are doing that. There would be many state-space models equivalent to 
a given ARMA/ARIMA model, but without specifying structural aspects of 
the system you will likely be using one of the innovations form 
state-space models that are equivalent. In an innovations form 
state-space model the state is defined as an expectation. From a 
practical point of view, this is one of the most important differences 
between an innovation form and a non-innovations form state-space model. 
Since the expectation is known exactly, the state-tracking error is 
zero. That means the covariance matrix from the filter or smoother 
should be a zero matrix, which you should not be trying to invert. In a 
non-innovations form the state has a physical interpretation rather than 
being an expectation, so the state tracking error should not be 
degenerate in that case.

I mention all this because your covariance matrix looks very close to zero.

Paul Gilbert

On 12/11/2015 06:00 AM, r-help-request at r-project.org wrote:
> Dear John, thank you for your considerations. This matrix (which is a
> variance matrix) is part of an algorithm for forward-filtering and
> backward-sampling of Dynamic Linear Models (West and Harrison, 1997),
> applied to DLM representation of ARIMA processes (Petris, Petrone,
> Campagnoli).  It is therefore very difficult to explain why this
> variance matrix becomes so ill conditioned. This already happens at
> the first iteration of the algorithm. I will try to work on initial
> conditions and some fixed parameters.
>
> Thank you again Stefano
>


From dleybman at gmail.com  Tue Dec 15 16:06:28 2015
From: dleybman at gmail.com (Dmitri Leybman)
Date: Tue, 15 Dec 2015 10:06:28 -0500
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
Message-ID: <CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>

Apologies for the HTML.

This is the initial snippet of the values:

Var1 Var2 Var3 Var4 Var5
0 0 7 1 0
0 0 7 0 0
1 1 8 2 0
5 5 8 0 0
1 4 8 1 0
4 5 8 0 0
0 1 7 2 1
5 1 7 0 0
2 4 9 0 1
1 2 9 2 NA
1 5 7 1 0
4 1 8 0 0
2 7 7 1 0
7 7 6 2 NA
5 2 7 0 0
0 1 7 0 4
1 3 8 1 0
1 5 7 2 0
7 2 8 0 0
7 0 8 2 0
7 5 8 2 0
2 0 9 1 0
1 6 8 1 0
3 4 7 0 2



I have tried:


   boxplot(data, las = 2, col =

   c("red", "blue", "black", "aquamarine1", "darkorange3"),

   at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),

   names = c("Meeting1", "Meeting2", "Meeting3", "Meeting4","Meeting5")


and have gotten a '+' at the end meaning I am missing something.


I used a modify version of the code I found on R-Help for Box-Whiskers plot
formation <http://www.r-bloggers.com/box-plot-with-r-tutorial/>


The code this blogger used was

       boxplot(data, ylab =?Oxigen (%)?, xlab =?Time?, las = 2,

       col =
 c(?red?,?sienna?,?palevioletred1?,?royalblue2?,?red?,?sienna?,?palevioletred1?,

       ?royalblue2?,?red?,?sienna?,?palevioletred1?,?royalblue2?),

        at = c(1,2,3,4,  6,7,8,9, 11,12,13,14), par(mar = c(12, 5, 4, 2) +
0.1),

        names = c(?Station 1?,?Station 2?,?Station 3?,?Station 4?,?Station


        1?,?Station 2?,?Station 3?,?Station 4?,?Station 1?,?Station
2?,?Station

         3?,?Station 4?))


I am trying to use the code above to create a five different
box-and-whiskers plots color-coded. I don't need the "Station 1" or
"Station 2" labels but I used this code as a starting point for the code I
wanted write.  .


I wanted the y-axis to read "Number of People Attended" instead of "Oxigen
(%)"


and the x-axis to refer to the particular meeting in question:


   Meeting1, Meeting2, Meeting3, Meeting4, Meeting5


which corresponds to variables var1, var2, var3, var4, var5.


Each of the box and whiskers would show the median number, along with
interquartile values, of each of these Meetings. The color coding is not
necessarily but I think would make it a bit easier to read for observers.

I apologize again for the convoluted reply and the HTML error.


On Mon, Dec 14, 2015 at 5:42 PM, peter dalgaard <pdalgd at gmail.com> wrote:

>
> > On 14 Dec 2015, at 22:54 , David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >>
> >> On Dec 14, 2015, at 1:34 PM, Dmitri Leybman <dleybman at gmail.com> wrote:
> >>
> >> I have a spreadsheet with five different columns standing for five
> >> different variables:
> >>
> >> Variable 1 Variable 2 Variable 3 Variable 4 Variable 5 0 0 7 1 0 0 0 7
> 0 0 1
> >> 1 8 2 0 5 5 8 0 0 1 4 8 1 0 4 5 8 0 0 0 1 7 2 1
> >> I am trying to create five box and whiskers plots on a single graph
> with a
> >> five x-label ticks named for each one of
> >> the variables along with color coding. The names for the x-label would
> be
> >> "Meeting"[ pertains to Variable1] "Meeting2"[pertains to Variable 2]
> >> Meeting3[pertains to Variable 3], Meeting4[pertains to Variable4],
> >> Meeting5[pertains to Variable5].
> >>
> >> I have tried:
> >>
> >> boxplot(data, las = 2, col =
> >> c("red", "blue", "black", "aquamarine1", "darkorange3")
> >> , at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),
> >> names = c("Meeting 1", "'Meeting2",
> >                         ^
> > Extra single quote about here
> >
>
> Hmm, at face value, that should just become part of the label...
>
> >
> >> "Meeting3", "Meeting4","Meeting5")
> >>
> >>
> >> Error: unexpected string constant in:
> >> "boxplot(data, las=2, col= c('red', 'blue', 'red', 'red', red')
> >> boxplot(data, las=2, col= c('"
>
>
> ...but the error message doesn't match the input, which is lacking a right
> parenthesis. So is there a previous incomplete command maybe, or are we
> just being shown random snippets of things that didn't work??
>
> I think we need to see a full transcript
>
>
> >>
> >>      [[alternative HTML version deleted]]
> >>
>
>
> ...and PLEASE in plain text, not HTML.
>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From S.Ellison at LGCGroup.com  Tue Dec 15 16:33:05 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 15 Dec 2015 15:33:05 +0000
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>



 
> I have tried:
> 
>    boxplot(data, las = 2, col =
>    c("red", "blue", "black", "aquamarine1", "darkorange3"),
>    at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),
>    names = c("Meeting1", "Meeting2", "Meeting3", "Meeting4","Meeting5")
> 
> and have gotten a '+' at the end meaning I am missing something.

You are missing the closing bracket on the boxplot() command.
Just finish with a ')'

S Ellison


*******************************************************************
This email and any attachments are confidential. Any use, copying or
disclosure other than by the intended recipient is unauthorised. If 
you have received this message in error, please notify the sender 
immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com 
and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From maechler at stat.math.ethz.ch  Tue Dec 15 16:55:31 2015
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Dec 2015 16:55:31 +0100
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <22128.14323.686365.992339@stat.math.ethz.ch>



   [............]	  

    > You are missing the closing bracket on the boxplot()
    > command.  Just finish with a ')'

Hmm... I once learned

 '()' =: parenthesis/es
 '[]' =: bracket(s)
 '{}' =: brace(s)

Of course, I'm not a native English speaker, and my teacher(s) /
teaching material may have been biased ... but, as all three
symbol pairs play an important role in R, I think it would be
really really helpful,  if we could agree on using the same
precise English here.

I'm happy to re-learn, but I'd really like to end up with three
different simple English words, if possible.
(Yes, I know and have seen/heard "curly braces", "round
 parentheses", ... but I'd hope we can do without the extra adjective.)

Thank you, well versed English (or "American") learned readers
of R-help, for wise guidance on this ...

Martin


From ragland.debra at yahoo.com  Tue Dec 15 16:56:02 2015
From: ragland.debra at yahoo.com (debra ragland)
Date: Tue, 15 Dec 2015 15:56:02 +0000 (UTC)
Subject: [R] adding vector values to corresponding components of a list
References: <1128108365.1583148.1450194962325.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1128108365.1583148.1450194962325.JavaMail.yahoo@mail.yahoo.com>

Hello,

I have tried this 2 ways and I keep coming to a dead end as I am not very proficient in R.

I have a logical matrix, where I would like to generate every row-wise pair of logical values for further testing. That is row1, row2; row 1, row3 etc. Ideally, I would like to assign a pre-generated vector of values to each pair such that each combination of rows may be split into 2 groups with each variable appearing only once. 

This is similar to the WRONG way I've been trying to do this;

vector = runif(4, 1.0, 2.0)
logmat<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)
logmat1 = logmat[1:2,]
dbv=rep(vector, 2)
split(dbv,logmat1)


As you can see there's no proper distinction between what should be absolutely assigned to "TRUE" and what should be assigned to "FALSE". --Note my actual data is named.

To try and correct this mistake on a subset of my data I've been trying to use what I'll call "method 1" where

 a = c(TRUE, TRUE, TRUE, FALSE, FALSE, TRUE)
 names(a)=c(a,b,c,d,e,f)

 b = c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE)
 names(b)=c("a","b","c","d","e","f")

Using this function, I am able to separate which variables are exactly "TRUE" or "FALSE

logical_groups = function (a, b) {
r = list()
s = a + b
r$'TRUE' = names(subset(s, s == 2))
r$'FALSE' = names(subset(s, s == 0))
r$'OR' = names(subset(s, s==1))
return(r)
}

> logical_groups(a,b)
$`TRUE`
[1] "a" "b"

$`FALSE`
[1] "d"

$OR
[1] "c" "e" "f"


The return, as you can see is a list. 
I have a vector of values that match the names a-f in a and b above, i.e.
vector=runif(6, 1.0, 2.0)
names(vector) = c("a","b","c","d","e","f")

and I would like to add these values to the corresponding (names in the) list so that I may run the wilcox.test correctly using the 'TRUE' and 'FALSE elements. But I am not sure how to achieve this.

I have also tried "method 2"-- starting with;
t=rbind(a, b, vector) to give;
>t
a        b        c       d        e        f
a      1.000000 1.000000 1.000000 0.00000 0.000000 1.000000
b      1.000000 1.000000 0.000000 0.00000 1.000000 0.000000
vector 1.012097 1.431088 1.832276 1.12801 1.780018 1.682804

where I then try 
t2 =t[1,]+t[2,]
t3=rbind(t2, t[3,]) to give;
> t3
a        b        c       d        e        f
t2 2.000000 2.000000 1.000000 0.00000 1.000000 1.000000
   1.012097 1.431088 1.832276 1.12801 1.780018 1.682804

But again, I am not sure how to split the t3 matrix where the first row == 2 or == 0.

I have been searching but I think I'm just confusing myself. I am very late in my pregnancy and my brain just won't function fully. 

I hope this makes sense. Any help is appreciated.


From marc_schwartz at me.com  Tue Dec 15 17:05:41 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 15 Dec 2015 10:05:41 -0600
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <22128.14323.686365.992339@stat.math.ethz.ch>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
Message-ID: <24F25AC7-AE79-437B-A4BC-717875C391EE@me.com>


> On Dec 15, 2015, at 9:55 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> 
> 
>   [............]	  
> 
>> You are missing the closing bracket on the boxplot()
>> command.  Just finish with a ')'
> 
> Hmm... I once learned
> 
> '()' =: parenthesis/es
> '[]' =: bracket(s)
> '{}' =: brace(s)


Martin,

The above is how I would refer to each.

Regards,

Marc


> 
> Of course, I'm not a native English speaker, and my teacher(s) /
> teaching material may have been biased ... but, as all three
> symbol pairs play an important role in R, I think it would be
> really really helpful,  if we could agree on using the same
> precise English here.
> 
> I'm happy to re-learn, but I'd really like to end up with three
> different simple English words, if possible.
> (Yes, I know and have seen/heard "curly braces", "round
> parentheses", ... but I'd hope we can do without the extra adjective.)
> 
> Thank you, well versed English (or "American") learned readers
> of R-help, for wise guidance on this ...
> 
> Martin
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Dec 15 17:10:59 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 15 Dec 2015 16:10:59 +0000
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004BC1@SRVEXCHMBX.precheza.cz>

Hi

I do not see any problem. Due to HTML posting we are not able to decipher what you are missing. Probably right parentheses.

I would start from beginning by

boxplot(data)

and gradually add required colour and other items to your boxplot.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dmitri
> Leybman
> Sent: Tuesday, December 15, 2015 4:06 PM
> To: peter dalgaard; r-help at r-project.org; dwinsemius at comcast.net
> Subject: Re: [R] Make a box-whiskers plot in R with 5 variables, color
> coded.
>
> Apologies for the HTML.
>
> This is the initial snippet of the values:
>
> Var1 Var2 Var3 Var4 Var5
> 0 0 7 1 0
> 0 0 7 0 0
> 1 1 8 2 0
> 5 5 8 0 0
> 1 4 8 1 0
> 4 5 8 0 0
> 0 1 7 2 1
> 5 1 7 0 0
> 2 4 9 0 1
> 1 2 9 2 NA
> 1 5 7 1 0
> 4 1 8 0 0
> 2 7 7 1 0
> 7 7 6 2 NA
> 5 2 7 0 0
> 0 1 7 0 4
> 1 3 8 1 0
> 1 5 7 2 0
> 7 2 8 0 0
> 7 0 8 2 0
> 7 5 8 2 0
> 2 0 9 1 0
> 1 6 8 1 0
> 3 4 7 0 2
>
>
>
> I have tried:
>
>
>    boxplot(data, las = 2, col =
>
>    c("red", "blue", "black", "aquamarine1", "darkorange3"),
>
>    at = c(1, 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1),
>
>    names = c("Meeting1", "Meeting2", "Meeting3", "Meeting4","Meeting5")
>
>
> and have gotten a '+' at the end meaning I am missing something.
>
>
> I used a modify version of the code I found on R-Help for Box-Whiskers
> plot formation <http://www.r-bloggers.com/box-plot-with-r-tutorial/>
>
>
> The code this blogger used was
>
>        boxplot(data, ylab =?Oxigen (%)?, xlab =?Time?, las = 2,
>
>        col =
>
> c(?red?,?sienna?,?palevioletred1?,?royalblue2?,?red?,?sienna?,?paleviol
> etred1?,
>
>        ?royalblue2?,?red?,?sienna?,?palevioletred1?,?royalblue2?),
>
>         at = c(1,2,3,4,  6,7,8,9, 11,12,13,14), par(mar = c(12, 5, 4,
> 2) + 0.1),
>
>         names = c(?Station 1?,?Station 2?,?Station 3?,?Station
> 4?,?Station
>
>
>         1?,?Station 2?,?Station 3?,?Station 4?,?Station 1?,?Station
> 2?,?Station
>
>          3?,?Station 4?))
>
>
> I am trying to use the code above to create a five different box-and-
> whiskers plots color-coded. I don't need the "Station 1" or "Station 2"
> labels but I used this code as a starting point for the code I wanted
> write.  .
>
>
> I wanted the y-axis to read "Number of People Attended" instead of
> "Oxigen (%)"
>
>
> and the x-axis to refer to the particular meeting in question:
>
>
>    Meeting1, Meeting2, Meeting3, Meeting4, Meeting5
>
>
> which corresponds to variables var1, var2, var3, var4, var5.
>
>
> Each of the box and whiskers would show the median number, along with
> interquartile values, of each of these Meetings. The color coding is
> not necessarily but I think would make it a bit easier to read for
> observers.
>
> I apologize again for the convoluted reply and the HTML error.
>
>
> On Mon, Dec 14, 2015 at 5:42 PM, peter dalgaard <pdalgd at gmail.com>
> wrote:
>
> >
> > > On 14 Dec 2015, at 22:54 , David Winsemius <dwinsemius at comcast.net>
> > wrote:
> > >
> > >>
> > >> On Dec 14, 2015, at 1:34 PM, Dmitri Leybman <dleybman at gmail.com>
> wrote:
> > >>
> > >> I have a spreadsheet with five different columns standing for five
> > >> different variables:
> > >>
> > >> Variable 1 Variable 2 Variable 3 Variable 4 Variable 5 0 0 7 1 0 0
> > >> 0 7
> > 0 0 1
> > >> 1 8 2 0 5 5 8 0 0 1 4 8 1 0 4 5 8 0 0 0 1 7 2 1 I am trying to
> > >> create five box and whiskers plots on a single graph
> > with a
> > >> five x-label ticks named for each one of the variables along with
> > >> color coding. The names for the x-label would
> > be
> > >> "Meeting"[ pertains to Variable1] "Meeting2"[pertains to Variable
> > >> 2] Meeting3[pertains to Variable 3], Meeting4[pertains to
> > >> Variable4], Meeting5[pertains to Variable5].
> > >>
> > >> I have tried:
> > >>
> > >> boxplot(data, las = 2, col =
> > >> c("red", "blue", "black", "aquamarine1", "darkorange3") , at =
> c(1,
> > >> 2, 3, 4, 5), par(mar = c(12, 5, 4, 2) + 0.1), names = c("Meeting
> > >> 1", "'Meeting2",
> > >                         ^
> > > Extra single quote about here
> > >
> >
> > Hmm, at face value, that should just become part of the label...
> >
> > >
> > >> "Meeting3", "Meeting4","Meeting5")
> > >>
> > >>
> > >> Error: unexpected string constant in:
> > >> "boxplot(data, las=2, col= c('red', 'blue', 'red', 'red', red')
> > >> boxplot(data, las=2, col= c('"
> >
> >
> > ...but the error message doesn't match the input, which is lacking a
> > right parenthesis. So is there a previous incomplete command maybe,
> or
> > are we just being shown random snippets of things that didn't work??
> >
> > I think we need to see a full transcript
> >
> >
> > >>
> > >>      [[alternative HTML version deleted]]
> > >>
> >
> >
> > ...and PLEASE in plain text, not HTML.
> >
> > >> ______________________________________________
> > >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > >> and provide commented, minimal, self-contained, reproducible code.
> > >
> > > David Winsemius
> > > Alameda, CA, USA
> > >
> > > ______________________________________________
> > > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> >
> > --
> > Peter Dalgaard, Professor,
> > Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
> > 2000 Frederiksberg, Denmark
> > Phone: (+45)38153501
> > Office: A 4.23
> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From clint at ecy.wa.gov  Tue Dec 15 17:54:26 2015
From: clint at ecy.wa.gov (Clint Bowman)
Date: Tue, 15 Dec 2015 08:54:26 -0800 (PST)
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <22128.14323.686365.992339@stat.math.ethz.ch>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
Message-ID: <alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>

Martin,

I grew up in the Midwest of the United States--about as native English 
speaker as you could find.  I was taught exactly the same as you have 
learned.

Clint

Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600

         USPS:           PO Box 47600, Olympia, WA 98504-7600
         Parcels:        300 Desmond Drive, Lacey, WA 98503-1274

On Tue, 15 Dec 2015, Martin Maechler wrote:

>
>
>   [............]
>
>    > You are missing the closing bracket on the boxplot()
>    > command.  Just finish with a ')'
>
> Hmm... I once learned
>
> '()' =: parenthesis/es
> '[]' =: bracket(s)
> '{}' =: brace(s)
>
> Of course, I'm not a native English speaker, and my teacher(s) /
> teaching material may have been biased ... but, as all three
> symbol pairs play an important role in R, I think it would be
> really really helpful,  if we could agree on using the same
> precise English here.
>
> I'm happy to re-learn, but I'd really like to end up with three
> different simple English words, if possible.
> (Yes, I know and have seen/heard "curly braces", "round
> parentheses", ... but I'd hope we can do without the extra adjective.)
>
> Thank you, well versed English (or "American") learned readers
> of R-help, for wise guidance on this ...
>
> Martin
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From bgunter.4567 at gmail.com  Tue Dec 15 17:58:19 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 15 Dec 2015 08:58:19 -0800
Subject: [R] adding vector values to corresponding components of a list
In-Reply-To: <1128108365.1583148.1450194962325.JavaMail.yahoo@mail.yahoo.com>
References: <1128108365.1583148.1450194962325.JavaMail.yahoo.ref@mail.yahoo.com>
	<1128108365.1583148.1450194962325.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbQhLmb6VbovE86wbdnCMCNnuO-Dp3=yD421BGwYhvPwCg@mail.gmail.com>

I as not able to make sense of your post. Maybe others can. However,
you have posted here several times, now. You should therefore know
that providing a reproducible example (providing data by e.g. dput() )
**and showing exactly what output you want** would improve your chance
of a helpful response.

'Nuff said.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 15, 2015 at 7:56 AM, debra ragland via R-help
<r-help at r-project.org> wrote:
> Hello,
>
> I have tried this 2 ways and I keep coming to a dead end as I am not very proficient in R.
>
> I have a logical matrix, where I would like to generate every row-wise pair of logical values for further testing. That is row1, row2; row 1, row3 etc. Ideally, I would like to assign a pre-generated vector of values to each pair such that each combination of rows may be split into 2 groups with each variable appearing only once.
>
> This is similar to the WRONG way I've been trying to do this;
>
> vector = runif(4, 1.0, 2.0)
> logmat<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)
> logmat1 = logmat[1:2,]
> dbv=rep(vector, 2)
> split(dbv,logmat1)
>
>
> As you can see there's no proper distinction between what should be absolutely assigned to "TRUE" and what should be assigned to "FALSE". --Note my actual data is named.
>
> To try and correct this mistake on a subset of my data I've been trying to use what I'll call "method 1" where
>
>  a = c(TRUE, TRUE, TRUE, FALSE, FALSE, TRUE)
>  names(a)=c(a,b,c,d,e,f)
>
>  b = c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE)
>  names(b)=c("a","b","c","d","e","f")
>
> Using this function, I am able to separate which variables are exactly "TRUE" or "FALSE
>
> logical_groups = function (a, b) {
> r = list()
> s = a + b
> r$'TRUE' = names(subset(s, s == 2))
> r$'FALSE' = names(subset(s, s == 0))
> r$'OR' = names(subset(s, s==1))
> return(r)
> }
>
>> logical_groups(a,b)
> $`TRUE`
> [1] "a" "b"
>
> $`FALSE`
> [1] "d"
>
> $OR
> [1] "c" "e" "f"
>
>
> The return, as you can see is a list.
> I have a vector of values that match the names a-f in a and b above, i.e.
> vector=runif(6, 1.0, 2.0)
> names(vector) = c("a","b","c","d","e","f")
>
> and I would like to add these values to the corresponding (names in the) list so that I may run the wilcox.test correctly using the 'TRUE' and 'FALSE elements. But I am not sure how to achieve this.
>
> I have also tried "method 2"-- starting with;
> t=rbind(a, b, vector) to give;
>>t
> a        b        c       d        e        f
> a      1.000000 1.000000 1.000000 0.00000 0.000000 1.000000
> b      1.000000 1.000000 0.000000 0.00000 1.000000 0.000000
> vector 1.012097 1.431088 1.832276 1.12801 1.780018 1.682804
>
> where I then try
> t2 =t[1,]+t[2,]
> t3=rbind(t2, t[3,]) to give;
>> t3
> a        b        c       d        e        f
> t2 2.000000 2.000000 1.000000 0.00000 1.000000 1.000000
>    1.012097 1.431088 1.832276 1.12801 1.780018 1.682804
>
> But again, I am not sure how to split the t3 matrix where the first row == 2 or == 0.
>
> I have been searching but I think I'm just confusing myself. I am very late in my pregnancy and my brain just won't function fully.
>
> I hope this makes sense. Any help is appreciated.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jake.w.russ at gmail.com  Tue Dec 15 15:24:24 2015
From: jake.w.russ at gmail.com (Jake Russ)
Date: Tue, 15 Dec 2015 09:24:24 -0500
Subject: [R] Fixed effects regression with state-specific trends
Message-ID: <CANbWT+EL=bNJ+1h5Ek+aW=Y8faWx+Th2w238ZFPqCq-+W2174Q@mail.gmail.com>

Dear All,

I am trying to implement a regression with state-specific trends in R. I
can implement this in Stata with ease, but I am hoping to preserve my R
workflow. I suspect there is an "R formula trick" that I'm just not
understanding and I would be grateful to anyone who could help me
understand "the R way."

I posted a similar question to Stackoverflow, but the solution from there
is not playing nice with plm() and I am hoping for a better answer,
http://stackoverflow.com/questions/34232834/fixed-effects-regression-with-state-specific-trends

My panel data looks like the following,

df <- data.frame(state = rep(c("a", "b", "c"), each = 10), time = seq(1,
10, 1), stringsAsFactors = FALSE)

I would like to create state-specific trends that look like these three new
columns

df2 <- cbind(df, model.matrix( ~ state - 1, data = df) * df$time)

To achieve this in Stata, I would simply write

xi: i.state i.state*time
reg y x _I*

The problem I have with the SO model.matrix solution is that I have greater
than 200 "states." Currently, I am generating more than 200 trend columns
with model.matrix and then I am building a very long formula string with
each column by name, I then turn that string into a formula with
as.formula(). Passing the resulting formula to lm() works but it is
cumbersome, and it is also choking the fixed effects version of plm().

Is there an R formula trick to this that I am missing?

Thank you,

Jake

	[[alternative HTML version deleted]]


From jimmygao0204 at 163.com  Tue Dec 15 14:41:25 2015
From: jimmygao0204 at 163.com (JimmyGao)
Date: Tue, 15 Dec 2015 21:41:25 +0800 (CST)
Subject: [R] how to plot two variables in a figure using ggplot2
Message-ID: <12230c91.1267b.151a5dfc8e4.Coremail.jimmygao0204@163.com>

Hi everyone,

Now,I want to use the following data to produce a figure.But I don't know how to do it.Is anyone have some experiences?

X axis using variable XX and Y axis using variable OA and KA

XX OA KA

1  1243 0.8157 0.7790
2  2486 0.8190 0.7829
3  3729 0.8278 0.7934
4  4972 0.8354 0.8026
5  6215 0.8475 0.8140
6  7458 0.8530 0.8224
7  8701 0.8668 0.8301
8  9944 0.8790 0.8540
9 11187 0.8990 0.8790


Thank you very much.
Jimmy









 





 
	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Dec 15 18:18:27 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 15 Dec 2015 17:18:27 +0000
Subject: [R] Hexbin: Counting Bins That Meet Certain Criteria
In-Reply-To: <CY1PR0101MB10048C9DFF962DAF602BFF63ABEE0@CY1PR0101MB1004.prod.exchangelabs.com>
References: <CY1PR0101MB10048C9DFF962DAF602BFF63ABEE0@CY1PR0101MB1004.prod.exchangelabs.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E91CA@mb02.ads.tamu.edu>

Something like

> library(hexbin)
> set.seed(42)
> xy <- matrix(rnorm(1000), 500)
> xy.hex <- hexbin(xy)
> table(xy.hex at count)

  1   2   3   4   5   6   7   8 
159  60  33  16   6   1   2   1 
> sum(xy.hex at count >= 3)
[1] 59

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sidoti, Salvatore A.
Sent: Monday, December 14, 2015 6:49 PM
To: r-help at r-project.org
Subject: [R] Hexbin: Counting Bins That Meet Certain Criteria

Greetings!

Is there a way to count the bins in a hexbin plot that meet certain criteria? For instance, what if I wanted to count the bins (hexes) that have a datapoint density of some number x?

Thank you!

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From soni.archit1989 at gmail.com  Tue Dec 15 14:09:15 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Tue, 15 Dec 2015 07:09:15 -0600
Subject: [R] TIBCO Enterprise Runtime for R
Message-ID: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>

Hi All,

I have the code to print XML tree that is working successfully in R Studio
but is failing when i try to work it out with TERR:

x<- XML::xmlParse(y)

y is input (Coming from a row only once)
x is output

The above code is working in R studio but doesnt work in TERR, please
suggest.

TIBCO Enterprise Runtime for R returned an error: 'Error in
as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a data.frame'.
at
Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
funcClient) at
Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
at
Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
at
Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()

-- 
Regards
Archit

	[[alternative HTML version deleted]]


From soni.archit1989 at gmail.com  Tue Dec 15 14:12:57 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Tue, 15 Dec 2015 07:12:57 -0600
Subject: [R] Print minified XML in tree format
In-Reply-To: <248E6FA047A8C746BA491485764190F537156709@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F537156709@ESESSMB207.ericsson.se>
Message-ID: <CAJ7HxBwWbOYUAbLPT-L7f+d7EBzHSRBn_8weeqp14GG1b=QY2A@mail.gmail.com>

Thanks Giorgio, for the docs! :)

However my main aim was to  make it run in TERR(TIBCO Enterprise Runtime
Engine for R), but it is throwing some different error but was working fine
in R Studio.

Error:    Could not execute function call.
TIBCO Enterprise Runtime for R returned an error: 'Error in
as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a data.frame'.
   at
Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
funcClient)
   at
Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.<RunFunction>d__0.MoveNext()
   at
Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.<ExecuteFunction>d__0.MoveNext()
   at
Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.<ExecuteFunction>d__6.MoveNext()

Much thanks for the docs

On Mon, Dec 14, 2015 at 1:54 PM, Giorgio Garziano <
giorgio.garziano at ericsson.com> wrote:

> I may suggest this quick guide:
>
>
> http://gastonsanchez.com/work/webdata/getting_web_data_r4_parsing_xml_html.pdf
>
> and the following link:
>
> http://www.r-datacollection.com/
>
>
> I apologize for not being more specific.
>
>
> --
> GG
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards
Archit

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Tue Dec 15 18:19:33 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 15 Dec 2015 17:19:33 +0000
Subject: [R] adding vector values to corresponding components of a list
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E91E1@mb02.ads.tamu.edu>

With each post you seem to be changing what you are trying to accomplish. Your WRONG way seems pretty close if I understand you at all. Just tag the names to the numeric vector instead of the logical vector, eg:

> names(dbv) <- letters[1:8]
> split(dbv,logmat1)
$`FALSE`
       a        d        g        h 
1.540183 1.172661 1.038135 1.172661 

$`TRUE`
       b        c        e        f 
1.295202 1.038135 1.540183 1.295202

But the other problem is that you may not realize what is happening. Using logmat1 (a matrix) as a vector in split means that it is converted by columns:

> logmat1
      [,1]  [,2] [,3]  [,4]
[1,] FALSE  TRUE TRUE FALSE
[2,]  TRUE FALSE TRUE FALSE

> as.vector(logmat1)
[1] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE

Earlier it seemed you wanted to combine the by rows:

c(logmat1[1,], logmat[2,])
[1] FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE

As you can see, it makes a difference.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of debra ragland via R-help
Sent: Tuesday, December 15, 2015 9:56 AM
To: R-help
Subject: [R] adding vector values to corresponding components of a list

Hello,

I have tried this 2 ways and I keep coming to a dead end as I am not very proficient in R.

I have a logical matrix, where I would like to generate every row-wise pair of logical values for further testing. That is row1, row2; row 1, row3 etc. Ideally, I would like to assign a pre-generated vector of values to each pair such that each combination of rows may be split into 2 groups with each variable appearing only once. 

This is similar to the WRONG way I've been trying to do this;

vector = runif(4, 1.0, 2.0)
logmat<-matrix(c(rep(c(F,T,F),3), rep(c(T,F,T),3), rep(c(T,T,F),3), rep(c(F,F,T),3)), ncol=4)
logmat1 = logmat[1:2,]
dbv=rep(vector, 2)
split(dbv,logmat1)


As you can see there's no proper distinction between what should be absolutely assigned to "TRUE" and what should be assigned to "FALSE". --Note my actual data is named.

To try and correct this mistake on a subset of my data I've been trying to use what I'll call "method 1" where

 a = c(TRUE, TRUE, TRUE, FALSE, FALSE, TRUE)
 names(a)=c(a,b,c,d,e,f)

 b = c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE)
 names(b)=c("a","b","c","d","e","f")

Using this function, I am able to separate which variables are exactly "TRUE" or "FALSE

logical_groups = function (a, b) {
r = list()
s = a + b
r$'TRUE' = names(subset(s, s == 2))
r$'FALSE' = names(subset(s, s == 0))
r$'OR' = names(subset(s, s==1))
return(r)
}

> logical_groups(a,b)
$`TRUE`
[1] "a" "b"

$`FALSE`
[1] "d"

$OR
[1] "c" "e" "f"


The return, as you can see is a list. 
I have a vector of values that match the names a-f in a and b above, i.e.
vector=runif(6, 1.0, 2.0)
names(vector) = c("a","b","c","d","e","f")

and I would like to add these values to the corresponding (names in the) list so that I may run the wilcox.test correctly using the 'TRUE' and 'FALSE elements. But I am not sure how to achieve this.

I have also tried "method 2"-- starting with;
t=rbind(a, b, vector) to give;
>t
a        b        c       d        e        f
a      1.000000 1.000000 1.000000 0.00000 0.000000 1.000000
b      1.000000 1.000000 0.000000 0.00000 1.000000 0.000000
vector 1.012097 1.431088 1.832276 1.12801 1.780018 1.682804

where I then try 
t2 =t[1,]+t[2,]
t3=rbind(t2, t[3,]) to give;
> t3
a        b        c       d        e        f
t2 2.000000 2.000000 1.000000 0.00000 1.000000 1.000000
   1.012097 1.431088 1.832276 1.12801 1.780018 1.682804

But again, I am not sure how to split the t3 matrix where the first row == 2 or == 0.

I have been searching but I think I'm just confusing myself. I am very late in my pregnancy and my brain just won't function fully. 

I hope this makes sense. Any help is appreciated.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From twoo872 at gmail.com  Tue Dec 15 13:12:16 2015
From: twoo872 at gmail.com (One Two)
Date: Tue, 15 Dec 2015 13:12:16 +0100
Subject: [R] error on multiple "==" in bquote()
Message-ID: <CADB71-EtBnk5HpZvCZeOQLmSZ1WnT_ADpA0qtt_UhjvTv2742w@mail.gmail.com>

Hello,

I noticed that the function bquote throws an error if there are
multiple "==" in the first argument. E.g.
> txt.main <- bquote(1 == 2 == 3)
Error: unexpected '==' in "txt.main <- bquote(1 == 2 =="

Is this behaviour intended or is it a bug? Because I don't see a
reason for this behaviour and didn't find anything in the
documentation either.

I know that one can use grouping as a workaround like this:
> txt.main <- bquote({1 == 2} == 3)
I am rather interested in whether I should file a bug about this
behaviour or not.

I'm using RStudio in this environment:
> sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: i386-w64-mingw32/i386 (32-bit)
Running under: Windows 7 (build 7601) Service Pack 1

locale:
[1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252
LC_MONETARY=German_Germany.1252
[4] LC_NUMERIC=C                    LC_TIME=German_Germany.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_3.2.3

Best Regards,
Martin


From dwinsemius at comcast.net  Tue Dec 15 18:21:52 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 15 Dec 2015 09:21:52 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
Message-ID: <8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>


> On Dec 15, 2015, at 8:54 AM, Clint Bowman <clint at ecy.wa.gov> wrote:
> 
> Martin,
> 
> I grew up in the Midwest of the United States--about as native English speaker as you could find.  I was taught exactly the same as you have learned.

As with your experience, Clint and Martin, but my online experience is that those speaking "English English" often refer to "(" as "brackets". As a result I generally now call them square-brackets to avoid ambiguity.

-- 
David.

> 
> Clint
> 
> Clint Bowman			INTERNET:	clint at ecy.wa.gov
> Air Quality Modeler		INTERNET:	clint at math.utah.edu
> Department of Ecology		VOICE:		(360) 407-6815
> PO Box 47600			FAX:		(360) 407-7534
> Olympia, WA 98504-7600
> 
>        USPS:           PO Box 47600, Olympia, WA 98504-7600
>        Parcels:        300 Desmond Drive, Lacey, WA 98503-1274
> 
> On Tue, 15 Dec 2015, Martin Maechler wrote:
> 
>> 
>> 
>>  [............]
>> 
>>   > You are missing the closing bracket on the boxplot()
>>   > command.  Just finish with a ')'
>> 
>> Hmm... I once learned
>> 
>> '()' =: parenthesis/es
>> '[]' =: bracket(s)
>> '{}' =: brace(s)
>> 
>> Of course, I'm not a native English speaker, and my teacher(s) /
>> teaching material may have been biased ... but, as all three
>> symbol pairs play an important role in R, I think it would be
>> really really helpful,  if we could agree on using the same
>> precise English here.
>> 
>> I'm happy to re-learn, but I'd really like to end up with three
>> different simple English words, if possible.
>> (Yes, I know and have seen/heard "curly braces", "round
>> parentheses", ... but I'd hope we can do without the extra adjective.)
>> 
>> Thank you, well versed English (or "American") learned readers
>> of R-help, for wise guidance on this ...
>> 
>> Martin
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jsorkin at grecc.umaryland.edu  Tue Dec 15 18:49:58 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 15 Dec 2015 12:49:58 -0500
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
Message-ID: <56700C88020000CB00143662@smtp.medicine.umaryland.edu>

On this side of the Atlantic, the symbols ( or ) are properly called parenthesis not brackets. Consider the expression parenthetical expression, which means something enclosed in parentheses.
John


> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)


> On Dec 15, 2015, at 12:23 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 15, 2015, at 8:54 AM, Clint Bowman <clint at ecy.wa.gov> wrote:
>> 
>> Martin,
>> 
>> I grew up in the Midwest of the United States--about as native English speaker as you could find.  I was taught exactly the same as you have learned.
> 
> As with your experience, Clint and Martin, but my online experience is that those speaking "English English" often refer to "(" as "brackets". As a result I generally now call them square-brackets to avoid ambiguity.
> 
> -- 
> David.
> 
>> 
>> Clint
>> 
>> Clint Bowman            INTERNET:    clint at ecy.wa.gov
>> Air Quality Modeler        INTERNET:    clint at math.utah.edu
>> Department of Ecology        VOICE:        (360) 407-6815
>> PO Box 47600            FAX:        (360) 407-7534
>> Olympia, WA 98504-7600
>> 
>>       USPS:           PO Box 47600, Olympia, WA 98504-7600
>>       Parcels:        300 Desmond Drive, Lacey, WA 98503-1274
>> 
>>> On Tue, 15 Dec 2015, Martin Maechler wrote:
>>> 
>>> 
>>> 
>>> [............]
>>> 
>>>> You are missing the closing bracket on the boxplot()
>>>> command.  Just finish with a ')'
>>> 
>>> Hmm... I once learned
>>> 
>>> '()' =: parenthesis/es
>>> '[]' =: bracket(s)
>>> '{}' =: brace(s)
>>> 
>>> Of course, I'm not a native English speaker, and my teacher(s) /
>>> teaching material may have been biased ... but, as all three
>>> symbol pairs play an important role in R, I think it would be
>>> really really helpful,  if we could agree on using the same
>>> precise English here.
>>> 
>>> I'm happy to re-learn, but I'd really like to end up with three
>>> different simple English words, if possible.
>>> (Yes, I know and have seen/heard "curly braces", "round
>>> parentheses", ... but I'd hope we can do without the extra adjective.)
>>> 
>>> Thank you, well versed English (or "American") learned readers
>>> of R-help, for wise guidance on this ...
>>> 
>>> Martin
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From dwinsemius at comcast.net  Tue Dec 15 18:51:21 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 15 Dec 2015 09:51:21 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
Message-ID: <A80DF627-3759-422F-8FF1-95CCBA4DA63D@comcast.net>


> On Dec 15, 2015, at 9:21 AM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 15, 2015, at 8:54 AM, Clint Bowman <clint at ecy.wa.gov> wrote:
>> 
>> Martin,
>> 
>> I grew up in the Midwest of the United States--about as native English speaker as you could find.  I was taught exactly the same as you have learned.
> 
> As with your experience, Clint and Martin, but my online experience is that those speaking "English English" often refer to "(" as "brackets". As a result I generally now call them square-brackets to avoid ambiguity.

I intended to say that I call "[" and "]" square-brackets.

-- 
> David.
> 
>> 
>> Clint
>> 
>> Clint Bowman			INTERNET:	clint at ecy.wa.gov
>> Air Quality Modeler		INTERNET:	clint at math.utah.edu
>> Department of Ecology		VOICE:		(360) 407-6815
>> PO Box 47600			FAX:		(360) 407-7534
>> Olympia, WA 98504-7600
>> 
>>       USPS:           PO Box 47600, Olympia, WA 98504-7600
>>       Parcels:        300 Desmond Drive, Lacey, WA 98503-1274
>> 
>> On Tue, 15 Dec 2015, Martin Maechler wrote:
>> 
>>> 
>>> 
>>> [............]
>>> 
>>>> You are missing the closing bracket on the boxplot()
>>>> command.  Just finish with a ')'
>>> 
>>> Hmm... I once learned
>>> 
>>> '()' =: parenthesis/es
>>> '[]' =: bracket(s)
>>> '{}' =: brace(s)
>>> 
>>> Of course, I'm not a native English speaker, and my teacher(s) /
>>> teaching material may have been biased ... but, as all three
>>> symbol pairs play an important role in R, I think it would be
>>> really really helpful,  if we could agree on using the same
>>> precise English here.
>>> 
>>> I'm happy to re-learn, but I'd really like to end up with three
>>> different simple English words, if possible.
>>> (Yes, I know and have seen/heard "curly braces", "round
>>> parentheses", ... but I'd hope we can do without the extra adjective.)
>>> 
>>> Thank you, well versed English (or "American") learned readers
>>> of R-help, for wise guidance on this ...
>>> 
>>> Martin
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bhh at xs4all.nl  Tue Dec 15 18:58:59 2015
From: bhh at xs4all.nl (Berend Hasselman)
Date: Tue, 15 Dec 2015 18:58:59 +0100
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <56700C88020000CB00143662@smtp.medicine.umaryland.edu>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
	<56700C88020000CB00143662@smtp.medicine.umaryland.edu>
Message-ID: <1ACB40FC-6765-452A-803D-1072B997F370@xs4all.nl>


> On 15 Dec 2015, at 18:49, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> 
> On this side of the Atlantic, the symbols ( or ) are properly called parenthesis not brackets. Consider the expression parenthetical expression, which means something enclosed in parentheses.
> John
> 


According to http://www.oxforddictionaries.com/words/brackets
I gather that to avoid ambiguity and/or confusion one could/would use round brackets for () (parentheses).


Berend


From pdalgd at gmail.com  Tue Dec 15 19:10:37 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 15 Dec 2015 19:10:37 +0100
Subject: [R] error on multiple "==" in bquote()
In-Reply-To: <CADB71-EtBnk5HpZvCZeOQLmSZ1WnT_ADpA0qtt_UhjvTv2742w@mail.gmail.com>
References: <CADB71-EtBnk5HpZvCZeOQLmSZ1WnT_ADpA0qtt_UhjvTv2742w@mail.gmail.com>
Message-ID: <9B4D2379-76B2-4CE3-9A75-D7AD62CD5505@gmail.com>


> On 15 Dec 2015, at 13:12 , One Two <twoo872 at gmail.com> wrote:
> 
> Hello,
> 
> I noticed that the function bquote throws an error if there are
> multiple "==" in the first argument. E.g.
>> txt.main <- bquote(1 == 2 == 3)
> Error: unexpected '==' in "txt.main <- bquote(1 == 2 =="
> 
> Is this behaviour intended or is it a bug? Because I don't see a
> reason for this behaviour and didn't find anything in the
> documentation either.

It's a consequence of the grammar of R. At some point, it was decided that a == b == c would have undesirable semantics[*], so == (and the other relational operators) became non-associative. It is a basic design feature of plotmath that it uses unevaluated R expressions, which have to be syntactically valid.

-pd

[*] As in

0 == 1 == 2
0 == (1 == 2)
0 == FALSE
0 == 0
TRUE

(which works from the 2nd line onwards, but then you can be assumed that you know what you are doing...)
 
> 
> I know that one can use grouping as a workaround like this:
>> txt.main <- bquote({1 == 2} == 3)
> I am rather interested in whether I should file a bug about this
> behaviour or not.
> 
> I'm using RStudio in this environment:
>> sessionInfo()
> R version 3.2.3 (2015-12-10)
> Platform: i386-w64-mingw32/i386 (32-bit)
> Running under: Windows 7 (build 7601) Service Pack 1
> 
> locale:
> [1] LC_COLLATE=German_Germany.1252  LC_CTYPE=German_Germany.1252
> LC_MONETARY=German_Germany.1252
> [4] LC_NUMERIC=C                    LC_TIME=German_Germany.1252
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] tools_3.2.3
> 
> Best Regards,
> Martin
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jsorkin at grecc.umaryland.edu  Tue Dec 15 19:12:42 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Tue, 15 Dec 2015 13:12:42 -0500
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <1ACB40FC-6765-452A-803D-1072B997F370@xs4all.nl>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
	<56700C88020000CB00143662@smtp.medicine.umaryland.edu>
	<1ACB40FC-6765-452A-803D-1072B997F370@xs4all.nl>
Message-ID: <567011CA020000CB0014368B@smtp.medicine.umaryland.edu>

Just as there are several types of punctuation marks, 
, ; : .
also called comma, semi-colon, colon, period (or full stop on the east side of the Atlantic),
so to are there two types of brackets
[   )  
also called square brackets, parenthesis.
 
It is clear that a ) although is a type of bracket it is called a parenthesis, just as , is called a comma, which is a type of punctuation mark.
John
 
 
 



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 
>>> Berend Hasselman <bhh at xs4all.nl> 12/15/15 12:59 PM >>>

> On 15 Dec 2015, at 18:49, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:
> 
> On this side of the Atlantic, the symbols ( or ) are properly called parenthesis not brackets. Consider the expression parenthetical expression, which means something enclosed in parentheses.
> John
> 


According to http://www.oxforddictionaries.com/words/brackets
I gather that to avoid ambiguity and/or confusion one could/would use round brackets for () (parentheses).


Berend
Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From dwinsemius at comcast.net  Tue Dec 15 20:05:25 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 15 Dec 2015 11:05:25 -0800
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
Message-ID: <6AA4D22A-5B66-41FA-8656-4492BF84D038@comcast.net>


> On Dec 15, 2015, at 5:09 AM, Archit Soni <soni.archit1989 at gmail.com> wrote:
> 
> Hi All,
> 
> I have the code to print XML tree that is working successfully in R Studio
> but is failing when i try to work it out with TERR:
> 
> x<- XML::xmlParse(y)
> 
> y is input (Coming from a row only once)
> x is output
> 
> The above code is working in R studio but doesnt work in TERR, please
> suggest.
> 
> TIBCO Enterprise Runtime for R returned an error: 'Error in
> as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
> coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a data.frame'.
> at
> Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
> funcClient) at
> Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
> at
> Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
> at
> Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
> 

I think you should be contacting TIBCO.

> -- 
> Regards
> Archit
> 
> 	[[alternative HTML version deleted]]

And do read the Posting Guide.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From S.Ellison at LGCGroup.com  Tue Dec 15 20:17:59 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 15 Dec 2015 19:17:59 +0000
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <567011CA020000CB0014368B@smtp.medicine.umaryland.edu>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
	<56700C88020000CB00143662@smtp.medicine.umaryland.edu>
	<1ACB40FC-6765-452A-803D-1072B997F370@xs4all.nl>
	<567011CA020000CB0014368B@smtp.medicine.umaryland.edu>
Message-ID: <1A8C1289955EF649A09086A153E2672403C97B15D5@GBTEDVPEXCMB04.corp.lgc-group.com>

> It is clear that a ) although is a type of bracket it is called a parenthesis, just as ,
> is called a comma, which is a type of punctuation mark.

These things are called parentheses because of what they do, not what they are. 
A parenthesis is any word or phrase inserted as an explanation or afterthought into text that would be is grammatically complete without it, usually bounded by punctuation. The bounding punctuation marks are then called parentheses, and can be round, square, or curly brackets, dashes, or just commas.

So (),  [], {}, - ... -  and , ... , are _all_ pairs of parentheses in grammatical usage. 

Three of them are also kinds of bracket, but not the only kinds. 

There's a disturbingly extensive article on it at 
https://en.wikipedia.org/wiki/Bracket#Names_for_various_bracket_symbols

which suggests to me that the terms are unlikely to be standardised quickly.

Steve E


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From ddalthorp at usgs.gov  Tue Dec 15 21:51:39 2015
From: ddalthorp at usgs.gov (Dalthorp, Daniel)
Date: Tue, 15 Dec 2015 12:51:39 -0800
Subject: [R] close specific graphics device
Message-ID: <CAJeYpE_gyGjcZCaifpaq2ix5gpsWCJJsp4mM77LqK+HH-aNtuw@mail.gmail.com>

dev.off(which) can be used to close a specific graphics device where
"which" is the index of the device, but is there a way to assign a custom
number (or name) to a windows device so that specific window can be later
closed via dev.off (or some other method) if it is open?

The following does NOT work. The target device is not open when its dev.off
is called, and another window that later got assigned the original index
associated with the target device is closed instead.

plot(0,0,type='n') # target window to close
text(0,0,"close me")
targetindex<-dev.cur()

# unbeknownst to the programmer, user closes device by clicking the red "X"
or...
dev.off()

# user draws a new graph that he wants to keep open
plot(1,1,type='n')
text(1,1,"do not close me")

# now it's time for the program to close the original graphics device (if
it still happens to be open)
dev.off(targetindex)

# the wrong device has been closed because the original window had closed
and the index associated with original graph is now associated with
something else

----

I'm looking for something like:

dev.off(which = "original figure") or dev.off(which = n), where n is a
custom index (like 10000) that will not be later assigned to a different
device [unless explicitly assigned that index].

Any help would be greatly appreciated.

Thanks!



-- 
Dan Dalthorp, PhD
USGS Forest and Rangeland Ecosystem Science Center
Forest Sciences Lab, Rm 189
3200 SW Jefferson Way
Corvallis, OR 97331
ph: 541-750-0953
ddalthorp at usgs.gov

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Tue Dec 15 22:08:01 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Tue, 15 Dec 2015 16:08:01 -0500
Subject: [R] close specific graphics device
In-Reply-To: <CAJeYpE_gyGjcZCaifpaq2ix5gpsWCJJsp4mM77LqK+HH-aNtuw@mail.gmail.com>
References: <CAJeYpE_gyGjcZCaifpaq2ix5gpsWCJJsp4mM77LqK+HH-aNtuw@mail.gmail.com>
Message-ID: <CAM_vju=N_n1tLhXs9MuudODxSv5qbY6mH=K7Vgno7OGmqCwKKQ@mail.gmail.com>

You could keep track of the device number using dev.cur() when you
create the plot, and some combination of
dev.set()
dev.list()
dev.close(which=mydev)

to manage them.

Sarah

On Tue, Dec 15, 2015 at 3:51 PM, Dalthorp, Daniel <ddalthorp at usgs.gov> wrote:
> dev.off(which) can be used to close a specific graphics device where
> "which" is the index of the device, but is there a way to assign a custom
> number (or name) to a windows device so that specific window can be later
> closed via dev.off (or some other method) if it is open?
>
> The following does NOT work. The target device is not open when its dev.off
> is called, and another window that later got assigned the original index
> associated with the target device is closed instead.
>
> plot(0,0,type='n') # target window to close
> text(0,0,"close me")
> targetindex<-dev.cur()
>
> # unbeknownst to the programmer, user closes device by clicking the red "X"
> or...
> dev.off()
>
> # user draws a new graph that he wants to keep open
> plot(1,1,type='n')
> text(1,1,"do not close me")
>
> # now it's time for the program to close the original graphics device (if
> it still happens to be open)
> dev.off(targetindex)
>
> # the wrong device has been closed because the original window had closed
> and the index associated with original graph is now associated with
> something else
>
> ----
>
> I'm looking for something like:
>
> dev.off(which = "original figure") or dev.off(which = n), where n is a
> custom index (like 10000) that will not be later assigned to a different
> device [unless explicitly assigned that index].
>
> Any help would be greatly appreciated.
>
> Thanks!
>
>

-- 
Sarah Goslee
http://www.functionaldiversity.org


From drjimlemon at gmail.com  Tue Dec 15 22:19:34 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 16 Dec 2015 08:19:34 +1100
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <1A8C1289955EF649A09086A153E2672403C97B15D5@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.LRH.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<8323B1F1-AF60-470C-9C06-BA77412BDE0B@comcast.net>
	<56700C88020000CB00143662@smtp.medicine.umaryland.edu>
	<1ACB40FC-6765-452A-803D-1072B997F370@xs4all.nl>
	<567011CA020000CB0014368B@smtp.medicine.umaryland.edu>
	<1A8C1289955EF649A09086A153E2672403C97B15D5@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <CA+8X3fXXYf7x4ie8rmLgbi89t-LiLLD1WNaVrWbS+GiqSsusJw@mail.gmail.com>

Hi,
My understanding is:

() - parentheses
{} - braces
[] - square brackets
<> - angle brackets

Jim


On Wed, Dec 16, 2015 at 6:17 AM, S Ellison <S.Ellison at lgcgroup.com> wrote:

> > It is clear that a ) although is a type of bracket it is called a
> parenthesis, just as ,
> > is called a comma, which is a type of punctuation mark.
>
> These things are called parentheses because of what they do, not what they
> are.
> A parenthesis is any word or phrase inserted as an explanation or
> afterthought into text that would be is grammatically complete without it,
> usually bounded by punctuation. The bounding punctuation marks are then
> called parentheses, and can be round, square, or curly brackets, dashes, or
> just commas.
>
> So (),  [], {}, - ... -  and , ... , are _all_ pairs of parentheses in
> grammatical usage.
>
> Three of them are also kinds of bracket, but not the only kinds.
>
> There's a disturbingly extensive article on it at
> https://en.wikipedia.org/wiki/Bracket#Names_for_various_bracket_symbols
>
> which suggests to me that the terms are unlikely to be standardised
> quickly.
>
> Steve E
>
>
> *******************************************************************
> This email and any attachments are confidential. Any u...{{dropped:13}}


From benraffin at gmail.com  Tue Dec 15 21:54:15 2015
From: benraffin at gmail.com (Benjamin)
Date: Tue, 15 Dec 2015 21:54:15 +0100
Subject: [R] party package: is cforest really bagging?
Message-ID: <CAM9oB=zA27F_CWtjiZSiVH_REwCpcF4494v_PiYE+r=PkgwHMA@mail.gmail.com>

Hi all,

I'm using the "party" package to create random forest of regression trees. I've
created a ForestControl class in order to limit my number of trees (ntree),
of nodes (maxdepth) and of variables I use to fit a tree (mtry). One thing
I'm not sure of is if the cforest algo is using subsets of my training set
for each tree it generates or not.

I've seen in the documentation that it is bagging so I assume it should.
But I'm not sure to understand well what the "subset" input is in that
function.

I'm also puzzled by the results I get using ctree: when plotting the tree,
I see that all my variables of my training set are classified in the
different terminal tree nodes while I would have exepected that it only
uses a subset here too.

So my question is, is cforest doing the same thing as ctree or is it really
bagging my training set?

Thanks in advance for you help!

Ben

	[[alternative HTML version deleted]]


From ken_gosier at yahoo.com  Tue Dec 15 20:44:15 2015
From: ken_gosier at yahoo.com (Ken Gosier)
Date: Tue, 15 Dec 2015 19:44:15 +0000 (UTC)
Subject: [R] problem with Rcpp and boost threadpool
References: <1561196273.1345541.1450208655454.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1561196273.1345541.1450208655454.JavaMail.yahoo@mail.yahoo.com>

I'm having a problem calling a local library through Rcpp with R Studio Server. It's a bit perplexing, since I have no issues when I call it from R at the command line. 

I've written an analytics library which uses boost's threadpool functionality for running multiple threads. I've stripped everything down to the bare essentials, the minimum code which will cause the problem -- this code just starts the threads in the threadpool, then exits them: 


#include <Rcpp.h> 

#include <boost/asio.hpp> 
#include <boost/bind.hpp> 
#include <boost/shared_ptr.hpp> 
#include <boost/thread.hpp> 
#include <boost/thread/thread.hpp> 

RcppExport SEXP test_thread() 
{ 
BEGIN_RCPP 
    double retdbl = 10.4; 

    boost::shared_ptr<boost::asio::io_service::work> threadpool_work; 
    boost::asio::io_service threadpool_io_service; 
    boost::thread_group threadpool_threads; 

    threadpool_work.reset(  new 
    boost::asio::io_service::work(threadpool_io_service)  ); 
    for (int i = 0; i < 6; ++i) { 
        threadpool_threads.create_thread( 
            boost::bind(&boost::asio::io_service::run, &threadpool_io_service)); 
    } 

    threadpool_io_service.stop(); 
    threadpool_work.reset(); 
    threadpool_threads.join_all(); 

    return(  Rcpp::wrap(retdbl)  ); 
END_RCPP 
} 


When I run from command-line R, there's no problem, I get the double returned. However, when I run through R Studio Server, it either hangs endlessly, or it crashes when it gets to the create_thread statement. 

My version info is: 

R: R version 3.1.1 (2014-07-10) -- "Sock it to Me" 
R Studio: 0.99.489 
Linux: Linux Debian-Jessie 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1+deb8u6 (2015-11-09) x86_64 GNU/Linux 
boost: 1.55

Many thanks for any help!


From drjimlemon at gmail.com  Tue Dec 15 23:38:58 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 16 Dec 2015 09:38:58 +1100
Subject: [R] close specific graphics device
In-Reply-To: <CAJeYpE_gyGjcZCaifpaq2ix5gpsWCJJsp4mM77LqK+HH-aNtuw@mail.gmail.com>
References: <CAJeYpE_gyGjcZCaifpaq2ix5gpsWCJJsp4mM77LqK+HH-aNtuw@mail.gmail.com>
Message-ID: <CA+8X3fWBZzW1aJWqGs_L2PkgehxemvF3UPC9Uqvaz0Cn9PuX4Q@mail.gmail.com>

Hi Dan,
The range of device numbers seems to be 1-63. There doesn't appear to be a
means of explicitly setting the device number when calling dev.new, and
devices are numbered sequentially when they are opened. This means that
even if you did know that the device number was, say, 4 it would be
possible to close that device and open another device with the number 4.

I suppose it would be possible to write wrapper functions for this, but I
have to leave at the moment, so perhaps tomorrow.

Jim

On Wed, Dec 16, 2015 at 7:51 AM, Dalthorp, Daniel <ddalthorp at usgs.gov>
wrote:

> dev.off(which) can be used to close a specific graphics device where
> "which" is the index of the device, but is there a way to assign a custom
> number (or name) to a windows device so that specific window can be later
> closed via dev.off (or some other method) if it is open?
>
> The following does NOT work. The target device is not open when its dev.off
> is called, and another window that later got assigned the original index
> associated with the target device is closed instead.
>
> plot(0,0,type='n') # target window to close
> text(0,0,"close me")
> targetindex<-dev.cur()
>
> # unbeknownst to the programmer, user closes device by clicking the red "X"
> or...
> dev.off()
>
> # user draws a new graph that he wants to keep open
> plot(1,1,type='n')
> text(1,1,"do not close me")
>
> # now it's time for the program to close the original graphics device (if
> it still happens to be open)
> dev.off(targetindex)
>
> # the wrong device has been closed because the original window had closed
> and the index associated with original graph is now associated with
> something else
>
> ----
>
> I'm looking for something like:
>
> dev.off(which = "original figure") or dev.off(which = n), where n is a
> custom index (like 10000) that will not be later assigned to a different
> device [unless explicitly assigned that index].
>
> Any help would be greatly appreciated.
>
> Thanks!
>
>
>
> --
> Dan Dalthorp, PhD
> USGS Forest and Rangeland Ecosystem Science Center
> Forest Sciences Lab, Rm 189
> 3200 SW Jefferson Way
> Corvallis, OR 97331
> ph: 541-750-0953
> ddalthorp at usgs.gov
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Tue Dec 15 23:41:24 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 15 Dec 2015 14:41:24 -0800
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
Message-ID: <CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>

It looks like you are calling TERR from Spotfire.  The Spotfire/TERR interface
can only pass TERR data.frames (eq. to Spotfire tables) back to Spotfire and
XMLInternalDocuments cannot be columns of data.frames (in neither TERR nor
R).

You should contact TIBCO support and/or participate in the forums at
community.tibco.com to see how to solve your problem.


Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Tue, Dec 15, 2015 at 5:09 AM, Archit Soni <soni.archit1989 at gmail.com> wrote:
> Hi All,
>
> I have the code to print XML tree that is working successfully in R Studio
> but is failing when i try to work it out with TERR:
>
> x<- XML::xmlParse(y)
>
> y is input (Coming from a row only once)
> x is output
>
> The above code is working in R studio but doesnt work in TERR, please
> suggest.
>
> TIBCO Enterprise Runtime for R returned an error: 'Error in
> as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
> coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a data.frame'.
> at
> Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
> funcClient) at
> Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
> at
> Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
> at
> Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
>
> --
> Regards
> Archit
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Tue Dec 15 23:55:42 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 15 Dec 2015 14:55:42 -0800
Subject: [R] problem with Rcpp and boost threadpool
In-Reply-To: <1561196273.1345541.1450208655454.JavaMail.yahoo@mail.yahoo.com>
References: <1561196273.1345541.1450208655454.JavaMail.yahoo.ref@mail.yahoo.com>
	<1561196273.1345541.1450208655454.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CAGxFJbRBN4Goba1jeya0Fh4ctSq6rM_LUynOp_62WXxU6ojGSA@mail.gmail.com>

There is a Rcpp-devel mailing list that should be more suitable for this post.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 15, 2015 at 11:44 AM, Ken Gosier via R-help
<r-help at r-project.org> wrote:
> I'm having a problem calling a local library through Rcpp with R Studio Server. It's a bit perplexing, since I have no issues when I call it from R at the command line.
>
> I've written an analytics library which uses boost's threadpool functionality for running multiple threads. I've stripped everything down to the bare essentials, the minimum code which will cause the problem -- this code just starts the threads in the threadpool, then exits them:
>
>
> #include <Rcpp.h>
>
> #include <boost/asio.hpp>
> #include <boost/bind.hpp>
> #include <boost/shared_ptr.hpp>
> #include <boost/thread.hpp>
> #include <boost/thread/thread.hpp>
>
> RcppExport SEXP test_thread()
> {
> BEGIN_RCPP
>     double retdbl = 10.4;
>
>     boost::shared_ptr<boost::asio::io_service::work> threadpool_work;
>     boost::asio::io_service threadpool_io_service;
>     boost::thread_group threadpool_threads;
>
>     threadpool_work.reset(  new
>     boost::asio::io_service::work(threadpool_io_service)  );
>     for (int i = 0; i < 6; ++i) {
>         threadpool_threads.create_thread(
>             boost::bind(&boost::asio::io_service::run, &threadpool_io_service));
>     }
>
>     threadpool_io_service.stop();
>     threadpool_work.reset();
>     threadpool_threads.join_all();
>
>     return(  Rcpp::wrap(retdbl)  );
> END_RCPP
> }
>
>
> When I run from command-line R, there's no problem, I get the double returned. However, when I run through R Studio Server, it either hangs endlessly, or it crashes when it gets to the create_thread statement.
>
> My version info is:
>
> R: R version 3.1.1 (2014-07-10) -- "Sock it to Me"
> R Studio: 0.99.489
> Linux: Linux Debian-Jessie 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1+deb8u6 (2015-11-09) x86_64 GNU/Linux
> boost: 1.55
>
> Many thanks for any help!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Dec 16 01:07:43 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 16 Dec 2015 01:07:43 +0100
Subject: [R] Weird behaviour function args in ellipses
In-Reply-To: <566B204A.7000400@gmail.com>
References: <CANFObyx_62eHF35fV7xPXNRdh4FK1XhXyp98aPDJ=83vmPC=gg@mail.gmail.com>
	<293BAC20-248C-4EFB-8B0A-3AA046987F37@comcast.net>
	<CANFObyzcq9tnceKzPkidTguCgt_vvN9=sZPYYtrmdBENrNB-fg@mail.gmail.com>
	<566B204A.7000400@gmail.com>
Message-ID: <C2273186-263F-452C-B161-77DC7318D0F0@gmail.com>


> On 11 Dec 2015, at 20:13 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 11/12/2015 1:52 PM, Mario Jos? Marques-Azevedo wrote:
>> Hi Duncan and David,
>> 
>> Thank you for explanation. I'm really disappointed with this R "resource".
>> I think that partial match, mainly in function args, must be optional and
>> not default. We can have many problems and lost hours find errors (it occur
>> with me). I tried to find a solution to disable partial match, but it seems
>> that is not possible. Program with hacks for this will be sad.
> 
> Nowadays with smart editors, I agree that partial matching isn't really necessary.  However, R has been around for 20 years, and lots of existing code depends on it.   Eventually you'll get to know the quirks of the design.

Yes, partial matching is largely regretted by its inventors, but it is hard to get rid of due to various arcane bits of history. For instance, have you noticed that seq() doesn't have a "length" argument? I.e.

> seq(2, length=3)
[1] 2 3 4

actually only works due to partial matching -- the argument is really "length.out". Now, looking that the code for seq.default offers a hint at the: It uses things like length(by) internally. In ancient times (in S) that would give you a type mismatch, but one of the things that R changed was to have function lookup disregard non-function objects.

In other places, some argument names have a period added for similar reasons.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From josh.m.ulrich at gmail.com  Wed Dec 16 02:27:27 2015
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 15 Dec 2015 19:27:27 -0600
Subject: [R] Trading Strategy and Bootstrap
In-Reply-To: <1464CFE83B5ABB4DB9D6E0A62E9974D4010E5428BA@WTPCPMBMEM18.ams.bnymellon.net>
References: <1464CFE83B5ABB4DB9D6E0A62E9974D4010E5428BA@WTPCPMBMEM18.ams.bnymellon.net>
Message-ID: <CAPPM_gRLj3amk4kpBfRyTnob5GUasEvvLJ8JBwKeEPvcdjp2mg@mail.gmail.com>

On Thu, Dec 10, 2015 at 4:30 PM, Schiele, Erik
<Erik.Schiele at bnymellon.com> wrote:
> Hi,
>
> I'm beginning to fool around in R for trading strategy purposes. To keep it simple, I have only played with stock data at this point.
>
> I have created a simple trend following strategy (in blue). Given my statistical background, I am attempting to bootstrap the results and table the same parameters highlighted below with no luck (in green). Any ideas on what I could do differently?
>
People will be more likely to respond and help if you provide a more
specific problem statement than, "with no luck".  What's wrong with
your current solution?  What other things have you tried and why
weren't they sufficient?

Also, please do not cross-post.  It's inconsiderate to those who don't
follow both lists: they won't know whether someone has provided a
satisfactory answer on the list/forum they don't follow.

> Really Appreciate your help!!! Thanks
>
> library(quantmod)
> library(PerformanceAnalytics)
>
> b <- get(getSymbols('SPY'))["2011::"]
> s <- get(getSymbols('GLD'))["2011::"]
> b$sma1 <- SMA(Cl(s) , 1)
> s$sma50 <- SMA(Cl(s) , 50)
> s$position <- ifelse(Cl(s) > s$sma50 , 1 , -1)
> myReturn <- lag(s$position) * dailyReturn(s)
>
> table.Drawdowns(s$position, top = 5, digits = 1)
> table.Stats(s$position, ci = 0.95, digits = 2)
> table.SpecificRisk(s$position, b$sma1, Rf = 0, digits = 2)
> table.Correlation(s$position, b$sma1)
>
> charts.PerformanceSummary(cbind(dailyReturn(s),myReturn))
>
> N     = 100 # Number of simulations
> Loop  = mat.or.vec(N,2,1,1,1)
> for (i in 1:N){
>
>   # sample with replacement from return distribution of index
>   s.new = (sample(s, length(s), replace = T, prob = NULL))
>   # demeaning returns
>   s.new = s.new-mean(s)
>   # new price series starting at same value as original series
>   prices.new = xts(prices[[1]]*exp(cumsum(s.new)))
>
>   # define strategies
>   # mean reversion
>   s$sma50.new  = SMA(Cl(s.new) , 50)
>
>    # Create buy/sell signals
>    # mean reversion
>    s$position.new <- ifelse(Cl(s) > s$sma50.new , 1 , -1)
>
>    # replace missing values with zeros
>    s$position.new[is.na(s$position.new)]   = 0
>
>    Loop[i,1] = if (mean(s$position.new)  > mean(s$sma50.new)) {1}else{0}
> }
>
> #Loop
>
> # plots simulated series
> returns.new = cbind(s$sma50.new, cumsum(s$sma50.new))
>
>  chart.CumReturns(returns.new,s$sma50.new,geometric=F)
>
> Erik Schiele
> Vice President
> Money Markets Trading, Originations and Sales
> 101 Barclay St, NY NY 10007 3rd Floor
> BNY Mellon Capital Markets, LLC
> Main Desk 212-815-8222
>
> This is for informational purposes only; from sources the Firm believes reliable; may not be accurate or complete; is subject to change;  is not a recommendation or offer to buy/sell a financial instrument or adopt any investment strategy; is not legal, tax, credit or accounting advice.  Do not use e-mail to submit any instructions - acceptances are at your risk. The Firm or its affiliates lends to, borrows from and provides other products/services to issuers and others, receives compensation therefore, and periodically has a direct or indirect financial interest in the financial instruments/transactions indicated.  Additional risks may exist that are not referenced. Past performance is not indicative of future returns. Other than CDs or CDARS, financial instruments: are not FDIC insured; are not deposits or other obligations of and are not guaranteed by the Firm or any bank or non-bank affiliate; and involve investment risk including possible loss of principal. The Firm is !
>  a wholly owned, indirect non-bank subsidiary of The Bank of New York Mellon Corporation, and a member of FINRA and SIPC, and is solely responsible for its obligations and commitments.
>
>
> The information contained in this e-mail, and any attachment, is confidential and is intended solely for the use of the intended recipient. Access, copying or re-use of the e-mail or any attachment, or any information contained therein, by any other person is not authorized. If you are not the intended recipient please return the e-mail to the sender and delete it from your computer. Although we attempt to sweep e-mail and attachments for viruses, we do not guarantee that either are virus-free and accept no liability for any damage sustained as a result of viruses.
>
> Please refer to http://disclaimer.bnymellon.com/eu.htm for certain disclosures relating to European legal entities.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From soni.archit1989 at gmail.com  Wed Dec 16 09:35:59 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Wed, 16 Dec 2015 02:35:59 -0600
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
	<CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>
Message-ID: <CAJ7HxBwLhz_kTU356hAE4cyJ0H=+FGU-jHVYxAw7Ag+pyQHPRQ@mail.gmail.com>

Yes William i'll see if i can get any help from TIBCommunity, but my code
worked in RStudio.

On Tue, Dec 15, 2015 at 4:41 PM, William Dunlap <wdunlap at tibco.com> wrote:

> It looks like you are calling TERR from Spotfire.  The Spotfire/TERR
> interface
> can only pass TERR data.frames (eq. to Spotfire tables) back to Spotfire
> and
> XMLInternalDocuments cannot be columns of data.frames (in neither TERR nor
> R).
>
> You should contact TIBCO support and/or participate in the forums at
> community.tibco.com to see how to solve your problem.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Tue, Dec 15, 2015 at 5:09 AM, Archit Soni <soni.archit1989 at gmail.com>
> wrote:
> > Hi All,
> >
> > I have the code to print XML tree that is working successfully in R
> Studio
> > but is failing when i try to work it out with TERR:
> >
> > x<- XML::xmlParse(y)
> >
> > y is input (Coming from a row only once)
> > x is output
> >
> > The above code is working in R studio but doesnt work in TERR, please
> > suggest.
> >
> > TIBCO Enterprise Runtime for R returned an error: 'Error in
> > as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
> > coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a
> data.frame'.
> > at
> >
> Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
> > funcClient) at
> >
> Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
> > at
> >
> Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
> > at
> >
> Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
> >
> > --
> > Regards
> > Archit
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
Regards
Archit

	[[alternative HTML version deleted]]


From matteo.richiardi at gmail.com  Wed Dec 16 12:53:19 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Wed, 16 Dec 2015 11:53:19 +0000
Subject: [R] Create a data.table by looping over variable names
Message-ID: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>

I apologise for this very basic question, but I found no answers on the web.
I want to create a data.table with n columns, named i1 ... i'n', with only
one row with value = 100 for every variable.

I can do it "by hand":

M.dt <- data.table(i1=100,i2=100,i3=100)

but I would like to make it in a loop, as 'n' is large in my case.


Thanks.

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Dec 16 13:55:24 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 16 Dec 2015 12:55:24 +0000
Subject: [R] Create a data.table by looping over variable names
In-Reply-To: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
References: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5004DEB@SRVEXCHMBX.precheza.cz>

Hi

Not sure about data table but

dat<-rep(100, 100)
dim(dat) <- c(1,100)
dat <- data.frame(dat)
names(dat) <-paste("i", 1:100, sep="")

gives you data.frame. I presume you can change it to data.table.

I cannot resist to comment that maybe list is better option. But as I do not know what you want to do with your data.table I may be completely out.

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matteo
> Richiardi
> Sent: Wednesday, December 16, 2015 12:53 PM
> To: r-help at r-project.org
> Subject: [R] Create a data.table by looping over variable names
>
> I apologise for this very basic question, but I found no answers on the
> web.
> I want to create a data.table with n columns, named i1 ... i'n', with
> only one row with value = 100 for every variable.
>
> I can do it "by hand":
>
> M.dt <- data.table(i1=100,i2=100,i3=100)
>
> but I would like to make it in a loop, as 'n' is large in my case.
>
>
> Thanks.
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From giorgio.garziano at ericsson.com  Wed Dec 16 14:11:47 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 16 Dec 2015 13:11:47 +0000
Subject: [R] Create a data.table by looping over variable names
Message-ID: <248E6FA047A8C746BA491485764190F53715C78A@ESESSMB207.ericsson.se>

library(data.table)
dat <- as.data.table(matrix(100, nrow=1, ncol=100))
colnames(dat) <- gsub("V", "i", colnames(dat))

--
GG




	[[alternative HTML version deleted]]


From matteo.richiardi at gmail.com  Wed Dec 16 15:03:38 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Wed, 16 Dec 2015 15:03:38 +0100
Subject: [R] Create a data.table by looping over variable names
In-Reply-To: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
References: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
Message-ID: <CABSrU1J2w+26cXrio+_Ex5WkK82CX_KjZPt280mr_=-OXAE2qg@mail.gmail.com>

Hi,
thanks a lot to everybody for your help. Very nice suggestions!
Matteo

On 16 December 2015 at 12:53, Matteo Richiardi <matteo.richiardi at gmail.com>
wrote:

> I apologise for this very basic question, but I found no answers on the
> web.
> I want to create a data.table with n columns, named i1 ... i'n', with only
> one row with value = 100 for every variable.
>
> I can do it "by hand":
>
> M.dt <- data.table(i1=100,i2=100,i3=100)
>
> but I would like to make it in a loop, as 'n' is large in my case.
>
>
> Thanks.
>

	[[alternative HTML version deleted]]


From pmhm at leeds.ac.uk  Wed Dec 16 11:10:33 2015
From: pmhm at leeds.ac.uk (=?iso-8859-1?Q?Hana_Mandov=E1?=)
Date: Wed, 16 Dec 2015 10:10:33 +0000
Subject: [R] NIPALS & categorical variables
Message-ID: <AMSPR03MB16150A3006E45892C40724585EF0@AMSPR03MB161.eurprd03.prod.outlook.com>

Hello,

Big help is needed. Can you use nipals for a dataset that contains both categorical as well as numerical variables?
I used nipals instead of pca as some of my numerical variables contain missing values, but my categorical variables are complete.

Many many thanks for your help, I've been stuck on this for ages.

Hana

Hana Mandov?
PhD Researcher
EPSRC Centre for Doctoral Training in Bioenergy
Faculty of Engineering
University of Leeds
Leeds LS2 9JT
United Kingdom
E: pmhm at leeds.ac.uk<mailto:pmhm at leeds.ac.uk>
W: www.engineering.leeds.ac.uk/bioenergy/<http://www.engineering.leeds.ac.uk/bioenergy/>
P Help save paper - every piece counts! Do you really need to print this email?


	[[alternative HTML version deleted]]


From atajti at gmail.com  Wed Dec 16 13:12:14 2015
From: atajti at gmail.com (=?UTF-8?Q?Andr=C3=A1s_Tajti?=)
Date: Wed, 16 Dec 2015 13:12:14 +0100
Subject: [R] Create a data.table by looping over variable names
In-Reply-To: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
References: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
Message-ID: <CALCQKuVz-K=UNysrrcu5z3GzU_Rf+cgwFRqJDrjuLUBT7tehig@mail.gmail.com>

Dear Matteo,
I found a one-liner solution to this problem:
n <- 100
data.table(i0 = 100)[, (paste0("i", 1:n)) := 100]

the data.table(i0=100) creates a one column-one row data.table,

Between the [] brakets i create a column name vector with paste, and
surround it with () to make it recognised as names (for details, look at
the Dateails of ?`:=`), and it will append columns to the i0 data.table.

This cannot be done with a 0-column data.table.

Good luck:
Andr?s Tajti




On 16 December 2015 at 12:53, Matteo Richiardi <matteo.richiardi at gmail.com>
wrote:

> I apologise for this very basic question, but I found no answers on the
> web.
> I want to create a data.table with n columns, named i1 ... i'n', with only
> one row with value = 100 for every variable.
>
> I can do it "by hand":
>
> M.dt <- data.table(i1=100,i2=100,i3=100)
>
> but I would like to make it in a loop, as 'n' is large in my case.
>
>
> Thanks.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From c06n.rm at gmail.com  Wed Dec 16 13:36:40 2015
From: c06n.rm at gmail.com (=?utf-8?Q?David_K=C3=A4thner?=)
Date: Wed, 16 Dec 2015 13:36:40 +0100
Subject: [R] Create a data.table by looping over variable names
In-Reply-To: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
References: <CABSrU1K+GJW_Af0ouiYFPxFWHDhbVtsUrQ3C-aWUFz2kr3Ni5Q@mail.gmail.com>
Message-ID: <284929B6-3647-41B8-BB57-5DD173185339@gmail.com>

Maybe there?s a cleverer way, but that?s what I can think off the top of my head:

n <- 100
M.dt <- data.table(matrix(ncol = 100))
M.dt[] <- 100
names(M.dt) <- sapply(1:100, function(x) paste0("i", x))

The sapply is your loop.

> Am 16.12.2015 um 12:53 schrieb Matteo Richiardi <matteo.richiardi at gmail.com>:
> 
> I apologise for this very basic question, but I found no answers on the web.
> I want to create a data.table with n columns, named i1 ... i'n', with only
> one row with value = 100 for every variable.
> 
> I can do it "by hand":
> 
> M.dt <- data.table(i1=100,i2=100,i3=100)
> 
> but I would like to make it in a loop, as 'n' is large in my case.
> 
> 
> Thanks.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From GullenJ at michigan.gov  Wed Dec 16 14:37:29 2015
From: GullenJ at michigan.gov (Gullen, James (MDE))
Date: Wed, 16 Dec 2015 13:37:29 +0000
Subject: [R]  how to plot two variables in a figure using ggplot2
Message-ID: <CY1PR09MB0442C942EB7DB509F9C06C13D7EF0@CY1PR09MB0442.namprd09.prod.outlook.com>

Greetings!

You don't specify what type of "figure" you're looking for...here are two possibilities to get you started:

As a note, it would have made things slightly easier if you had used dput() to provide the data to us. It took a little massaging in notepad before getting it into R.

#Read data from clipboard after removing blank line and column numbers in notepad...
mydata <- read.table(file="clipboard", sep=" ", header =TRUE)
str(mydata)

#Restructure the data to long for ggplot2...there are other ways to do this, also.
mydata2 <- data.frame(group=rep(c("OA","KA"),  each=9), xvar=rep(mydata$XX, times=2), yvar=c(mydata$OA, mydata$KA))

 #Look at format of the data
mydata2

#Now to the plotting...
require(ggplot2)

#You might want a barplot...
plot1 <- ggplot(data=mydata2, aes(x=xvar, y=yvar,group=group, fill=group)) +
  geom_bar(stat="identity", position="dodge") + ggtitle("Barchart of OA and KA")

plot1

#..or you might want a lineplot...
plot2 <-ggplot(data=mydata2, aes(x=xvar, y=yvar, group=group, colour=group)) +
  geom_line() + ggtitle("Linechart of OA and KA")

plot2

The line chart is very easy to do in ggplot2 even if you don't restructure the data to long format, if that is what you were looking for.

Hope this helps and best regards!

Jim Gullen, Ph.D
Higher Education Data Reporting Consultant
Office of Professional Preparation Services
Michigan Department of Education

Customer Service is a priority at the Michigan Department of Education - helping Michigan schools, families, and communities improve the achievement and well-being of ALL our children.


From jrkrideau at inbox.com  Wed Dec 16 16:16:21 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 16 Dec 2015 07:16:21 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables,
 color  coded.
In-Reply-To: <1A8C1289955EF649A09086A153E2672403C97B15D5@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <8323b1f1-af60-470c-9c06-ba77412bde0b@comcast.net>
	<1a8c1289955ef649a09086a153e2672403c97b1571@gbtedvpexcmb04.corp.lgc-group.com>
	<ca+djrjnwjrwj_wbvbg_man_-mbmsgd-cnywjrvsnao=2=s2hsg@mail.gmail.com>
	<6d2781a7-b78a-4dd2-9de8-8b9e2610941a@gmail.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.lrh.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<567011ca020000cb0014368b@smtp.medicine.umaryland.edu>
	<4e1f492d-8c4c-4512-805e-a4f25ed4030c@comcast.net>
	<56700c88020000cb00143662@smtp.medicine.umaryland.edu>
	<ca+djrjk=ewrvoibb6pmnjerb9mnp5_dmshsvxz7tppspqlg2va@mail.gmail.com>
	<1acb40fc-6765-452a-803d-1072b997f370@xs4all.nl>
Message-ID: <5E8CCF30F16.00000F47jrkrideau@inbox.com>



John Kane
Kingston ON Canada


> -----Original Message-----
> From: s.ellison at lgcgroup.com
> Sent: Tue, 15 Dec 2015 19:17:59 +0000
> To: r-help at r-project.org
> Subject: Re: [R] Make a box-whiskers plot in R with 5 variables, color
> coded.
> 
>> It is clear that a ) although is a type of bracket it is called a
>> parenthesis, just as ,
>> is called a comma, which is a type of punctuation mark.
> 
> These things are called parentheses because of what they do, not what
> they are.
> A parenthesis is any word or phrase inserted as an explanation or
> afterthought into text that would be is grammatically complete without
> it, usually bounded by punctuation. The bounding punctuation marks are
> then called parentheses, and can be round, square, or curly brackets,
> dashes, or just commas.
> 
> So (),  [], {}, - ... -  and , ... , are _all_ pairs of parentheses in
> grammatical usage.
> 
> Three of them are also kinds of bracket, but not the only kinds.
> 
> There's a disturbingly extensive article on it at
> https://en.wikipedia.org/wiki/Bracket#Names_for_various_bracket_symbols

I have lived next door to the USA for most of my life and never realized that American usage is 'brackets' for  [ ] .  I would use the term brackets in normal use for ( ) and "square brackets for [ ]. 

> 
> which suggests to me that the terms are unlikely to be standardised
> quickly.
> 
> Steve E

I am not expecting standardization any time this year.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jrkrideau at inbox.com  Wed Dec 16 16:32:21 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 16 Dec 2015 07:32:21 -0800
Subject: [R] how to plot two variables in a figure using ggplot2
In-Reply-To: <12230c91.1267b.151a5dfc8e4.Coremail.jimmygao0204@163.com>
Message-ID: <5EB08E82D6D.00000F82jrkrideau@inbox.com>

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html for some general suggestions on posting here. In particular have a look at dput() or do a ?dput in R for the best way to supply sample data.

To do the plot you want you will  need to reshape your data from ?wide? format to ?long? and then plot it with ggplot2

See ?melt in the reshape2 package for how to reshape the data.
Normally you would use the ?colour? option to separately plot the two lines so something like should do it.

ggplot(mydata, aes( XX, value, colour = variable)) + some.plot.option.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: jimmygao0204 at 163.com
> Sent: Tue, 15 Dec 2015 21:41:25 +0800 (CST)
> To: r-help at r-project.org
> Subject: [R] how to plot two variables in a figure using ggplot2
> 
> Hi everyone,
> 
> Now,I want to use the following data to produce a figure.But I don't know
> how to do it.Is anyone have some experiences?
> 
> X axis using variable XX and Y axis using variable OA and KA
> 
> XX OA KA
> 
> 1  1243 0.8157 0.7790
> 2  2486 0.8190 0.7829
> 3  3729 0.8278 0.7934
> 4  4972 0.8354 0.8026
> 5  6215 0.8475 0.8140
> 6  7458 0.8530 0.8224
> 7  8701 0.8668 0.8301
> 8  9944 0.8790 0.8540
> 9 11187 0.8990 0.8790
> 
> 
> Thank you very much.
> Jimmy
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
TRY FREE IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if5
Capture screenshots, upload images, edit and send them to your friends
through IMs, post on Twitter?, Facebook?, MySpace?, LinkedIn? ? FAST!


From h.wickham at gmail.com  Wed Dec 16 16:34:20 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 16 Dec 2015 09:34:20 -0600
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <22128.14323.686365.992339@stat.math.ethz.ch>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
Message-ID: <CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>

On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
>
>    [............]
>
>     > You are missing the closing bracket on the boxplot()
>     > command.  Just finish with a ')'
>
> Hmm... I once learned
>
>  '()' =: parenthesis/es
>  '[]' =: bracket(s)
>  '{}' =: brace(s)
>
> Of course, I'm not a native English speaker, and my teacher(s) /
> teaching material may have been biased ... but, as all three
> symbol pairs play an important role in R, I think it would be
> really really helpful,  if we could agree on using the same
> precise English here.
>
> I'm happy to re-learn, but I'd really like to end up with three
> different simple English words, if possible.
> (Yes, I know and have seen/heard "curly braces", "round
>  parentheses", ... but I'd hope we can do without the extra adjective.)

I think this is what Americans are taught, but I can never remember
which is which. I use round brackets, square brackets, and squiggly
brackets, which are memorable, and even if you're not familiar with
the terms you can easily understand what I mean.

Hadley

-- 
http://had.co.nz/


From wdunlap at tibco.com  Wed Dec 16 16:56:27 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 16 Dec 2015 07:56:27 -0800
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAJ7HxBwLhz_kTU356hAE4cyJ0H=+FGU-jHVYxAw7Ag+pyQHPRQ@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
	<CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>
	<CAJ7HxBwLhz_kTU356hAE4cyJ0H=+FGU-jHVYxAw7Ag+pyQHPRQ@mail.gmail.com>
Message-ID: <CAF8bMcaZnhGaaEPzsUJPJo7sBOdNh77aAL_6bCQEW=MuMmxRCw@mail.gmail.com>

Your code works in TERR under Spotfire, but since Spotfire deals with
rectangular data sets the glue code between them puts your objects in
a data.frame, which is not legal.

What do you hope to do with the XMLInteralDocument object in Spotfire?
 Such objects depend on R internal pointers and don't even survive a
save/load cycle in R.  You probably want to a column of data, numeric
or character, from it and send that back to Spotfire.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Dec 16, 2015 at 12:35 AM, Archit Soni <soni.archit1989 at gmail.com> wrote:
> Yes William i'll see if i can get any help from TIBCommunity, but my code
> worked in RStudio.
>
> On Tue, Dec 15, 2015 at 4:41 PM, William Dunlap <wdunlap at tibco.com> wrote:
>>
>> It looks like you are calling TERR from Spotfire.  The Spotfire/TERR
>> interface
>> can only pass TERR data.frames (eq. to Spotfire tables) back to Spotfire
>> and
>> XMLInternalDocuments cannot be columns of data.frames (in neither TERR nor
>> R).
>>
>> You should contact TIBCO support and/or participate in the forums at
>> community.tibco.com to see how to solve your problem.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>>
>> On Tue, Dec 15, 2015 at 5:09 AM, Archit Soni <soni.archit1989 at gmail.com>
>> wrote:
>> > Hi All,
>> >
>> > I have the code to print XML tree that is working successfully in R
>> > Studio
>> > but is failing when i try to work it out with TERR:
>> >
>> > x<- XML::xmlParse(y)
>> >
>> > y is input (Coming from a row only once)
>> > x is output
>> >
>> > The above code is working in R studio but doesnt work in TERR, please
>> > suggest.
>> >
>> > TIBCO Enterprise Runtime for R returned an error: 'Error in
>> > as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
>> > coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a
>> > data.frame'.
>> > at
>> >
>> > Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
>> > funcClient) at
>> >
>> > Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
>> > at
>> >
>> > Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
>> > at
>> >
>> > Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
>> >
>> > --
>> > Regards
>> > Archit
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Regards
> Archit


From lists at dewey.myzen.co.uk  Wed Dec 16 17:03:13 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Wed, 16 Dec 2015 16:03:13 +0000
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
Message-ID: <56718B41.8040907@dewey.myzen.co.uk>

As a speaker of the dialect of British English current in southern 
England I think:

1 - the generic term for all three is brackets. As a child I was taught 
the precedence rules for arithmetic operators by the mnemonic BODMAS 
(the O stands for 'of')
2 - careful speakers of the dialect who know all three use exactly the 
terms used in the canton of Zuerich by Martin as he uses them
3 - I would use square brackets and curly brackets to a non-technical 
audience but I could not bring myself to say round brackets


On 16/12/2015 15:34, Hadley Wickham wrote:
> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>
>>
>>     [............]
>>
>>      > You are missing the closing bracket on the boxplot()
>>      > command.  Just finish with a ')'
>>
>> Hmm... I once learned
>>
>>   '()' =: parenthesis/es
>>   '[]' =: bracket(s)
>>   '{}' =: brace(s)
>>
>> Of course, I'm not a native English speaker, and my teacher(s) /
>> teaching material may have been biased ... but, as all three
>> symbol pairs play an important role in R, I think it would be
>> really really helpful,  if we could agree on using the same
>> precise English here.
>>
>> I'm happy to re-learn, but I'd really like to end up with three
>> different simple English words, if possible.
>> (Yes, I know and have seen/heard "curly braces", "round
>>   parentheses", ... but I'd hope we can do without the extra adjective.)
>
> I think this is what Americans are taught, but I can never remember
> which is which. I use round brackets, square brackets, and squiggly
> brackets, which are memorable, and even if you're not familiar with
> the terms you can easily understand what I mean.
>
> Hadley
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From dcarlson at tamu.edu  Wed Dec 16 17:28:17 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 16 Dec 2015 16:28:17 +0000
Subject: [R] how to plot two variables in a figure using ggplot2
In-Reply-To: <5EB08E82D6D.00000F82jrkrideau@inbox.com>
References: <12230c91.1267b.151a5dfc8e4.Coremail.jimmygao0204@163.com>
	<5EB08E82D6D.00000F82jrkrideau@inbox.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E9DFE@mb02.ads.tamu.edu>

You can also do this with matplot() which does not require reshaping the data.

Assuming your data is a data frame called dat:

> dput(dat)
structure(list(XX = c(1243L, 2486L, 3729L, 4972L, 6215L, 7458L, 
8701L, 9944L, 11187L), OA = c(0.8157, 0.819, 0.8278, 0.8354, 
0.8475, 0.853, 0.8668, 0.879, 0.899), KA = c(0.779, 0.7829, 0.7934, 
0.8026, 0.814, 0.8224, 0.8301, 0.854, 0.879)), .Names = c("XX", 
"OA", "KA"), class = "data.frame", row.names = c("1", "2", "3", 
"4", "5", "6", "7", "8", "9"))

> matplot(dat$XX, dat[, 2:3], type="l", xlab="XX", ylab="OA & KA")
> legend("topleft", c("OA", "KA"), col=1:2, lty=1:2)

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352



-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of John Kane
Sent: Wednesday, December 16, 2015 9:32 AM
To: JimmyGao; r-help mailing list
Subject: Re: [R] how to plot two variables in a figure using ggplot2

Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html for some general suggestions on posting here. In particular have a look at dput() or do a ?dput in R for the best way to supply sample data.

To do the plot you want you will  need to reshape your data from ?wide? format to ?long? and then plot it with ggplot2

See ?melt in the reshape2 package for how to reshape the data.
Normally you would use the ?colour? option to separately plot the two lines so something like should do it.

ggplot(mydata, aes( XX, value, colour = variable)) + some.plot.option.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: jimmygao0204 at 163.com
> Sent: Tue, 15 Dec 2015 21:41:25 +0800 (CST)
> To: r-help at r-project.org
> Subject: [R] how to plot two variables in a figure using ggplot2
> 
> Hi everyone,
> 
> Now,I want to use the following data to produce a figure.But I don't know
> how to do it.Is anyone have some experiences?
> 
> X axis using variable XX and Y axis using variable OA and KA
> 
> XX OA KA
> 
> 1  1243 0.8157 0.7790
> 2  2486 0.8190 0.7829
> 3  3729 0.8278 0.7934
> 4  4972 0.8354 0.8026
> 5  6215 0.8475 0.8140
> 6  7458 0.8530 0.8224
> 7  8701 0.8668 0.8301
> 8  9944 0.8790 0.8540
> 9 11187 0.8990 0.8790
> 
> 
> Thank you very much.
> Jimmy
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
TRY FREE IM TOOLPACK at http://www.imtoolpack.com/default.aspx?rc=if5
Capture screenshots, upload images, edit and send them to your friends
through IMs, post on Twitter?, Facebook?, MySpace?, LinkedIn? ? FAST!

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From h.wickham at gmail.com  Wed Dec 16 17:42:01 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 16 Dec 2015 10:42:01 -0600
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
Message-ID: <CABdHhvGnPeikypPzEd5MtLeG3ZE=k5bCzV47QncaN5+8yWoy2Q@mail.gmail.com>

On Wed, Dec 16, 2015 at 9:34 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>
>>
>>    [............]
>>
>>     > You are missing the closing bracket on the boxplot()
>>     > command.  Just finish with a ')'
>>
>> Hmm... I once learned
>>
>>  '()' =: parenthesis/es
>>  '[]' =: bracket(s)
>>  '{}' =: brace(s)
>>
>> Of course, I'm not a native English speaker, and my teacher(s) /
>> teaching material may have been biased ... but, as all three
>> symbol pairs play an important role in R, I think it would be
>> really really helpful,  if we could agree on using the same
>> precise English here.
>>
>> I'm happy to re-learn, but I'd really like to end up with three
>> different simple English words, if possible.
>> (Yes, I know and have seen/heard "curly braces", "round
>>  parentheses", ... but I'd hope we can do without the extra adjective.)
>
> I think this is what Americans are taught, but I can never remember
> which is which. I use round brackets, square brackets, and squiggly
> brackets, which are memorable, and even if you're not familiar with
> the terms you can easily understand what I mean.

I should mention that all three terms have accompanying arm motions ;)

Hadley

-- 
http://had.co.nz/


From soni.archit1989 at gmail.com  Wed Dec 16 18:12:02 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Wed, 16 Dec 2015 22:42:02 +0530
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAF8bMcaZnhGaaEPzsUJPJo7sBOdNh77aAL_6bCQEW=MuMmxRCw@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
	<CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>
	<CAJ7HxBwLhz_kTU356hAE4cyJ0H=+FGU-jHVYxAw7Ag+pyQHPRQ@mail.gmail.com>
	<CAF8bMcaZnhGaaEPzsUJPJo7sBOdNh77aAL_6bCQEW=MuMmxRCw@mail.gmail.com>
Message-ID: <CAJ7HxBy_nBiKaByKsB0rbKHUwNHOo6BLiKH3wiXqw3tQYbW65g@mail.gmail.com>

Thanks for the reply William, I wish to print the XML tree format in a text
area.
Can we do that by some way ? or is there a way to achieve that in
IronPython, i am open for both options :)

Thanks again for reply.

On Wed, Dec 16, 2015 at 9:26 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Your code works in TERR under Spotfire, but since Spotfire deals with
> rectangular data sets the glue code between them puts your objects in
> a data.frame, which is not legal.
>
> What do you hope to do with the XMLInteralDocument object in Spotfire?
>  Such objects depend on R internal pointers and don't even survive a
> save/load cycle in R.  You probably want to a column of data, numeric
> or character, from it and send that back to Spotfire.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, Dec 16, 2015 at 12:35 AM, Archit Soni <soni.archit1989 at gmail.com>
> wrote:
> > Yes William i'll see if i can get any help from TIBCommunity, but my code
> > worked in RStudio.
> >
> > On Tue, Dec 15, 2015 at 4:41 PM, William Dunlap <wdunlap at tibco.com>
> wrote:
> >>
> >> It looks like you are calling TERR from Spotfire.  The Spotfire/TERR
> >> interface
> >> can only pass TERR data.frames (eq. to Spotfire tables) back to Spotfire
> >> and
> >> XMLInternalDocuments cannot be columns of data.frames (in neither TERR
> nor
> >> R).
> >>
> >> You should contact TIBCO support and/or participate in the forums at
> >> community.tibco.com to see how to solve your problem.
> >>
> >>
> >> Bill Dunlap
> >> TIBCO Software
> >> wdunlap tibco.com
> >>
> >>
> >> On Tue, Dec 15, 2015 at 5:09 AM, Archit Soni <soni.archit1989 at gmail.com
> >
> >> wrote:
> >> > Hi All,
> >> >
> >> > I have the code to print XML tree that is working successfully in R
> >> > Studio
> >> > but is failing when i try to work it out with TERR:
> >> >
> >> > x<- XML::xmlParse(y)
> >> >
> >> > y is input (Coming from a row only once)
> >> > x is output
> >> >
> >> > The above code is working in R studio but doesnt work in TERR, please
> >> > suggest.
> >> >
> >> > TIBCO Enterprise Runtime for R returned an error: 'Error in
> >> > as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
> >> > coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a
> >> > data.frame'.
> >> > at
> >> >
> >> >
> Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
> >> > funcClient) at
> >> >
> >> >
> Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
> >> > at
> >> >
> >> >
> Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
> >> > at
> >> >
> >> >
> Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
> >> >
> >> > --
> >> > Regards
> >> > Archit
> >> >
> >> >         [[alternative HTML version deleted]]
> >> >
> >> > ______________________________________________
> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> > PLEASE do read the posting guide
> >> > http://www.R-project.org/posting-guide.html
> >> > and provide commented, minimal, self-contained, reproducible code.
> >
> >
> >
> >
> > --
> > Regards
> > Archit
>



-- 
Regards
Archit

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Dec 16 18:25:45 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 16 Dec 2015 09:25:45 -0800
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
	<CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
Message-ID: <CAF8bMcZMi8FGNcLx5twirahdqbxhzFEcqQ=4STaG8kdnWinbMQ@mail.gmail.com>

showIntegral <- function (f, xmin, xmax, n = 16, fractionFromLeft = 0.5)
{
    stopifnot(fractionFromLeft >= 0, fractionFromLeft <= 1, n >=
        1)
    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
    abline(h = 0)
    dx <- (xmax - xmin)/n
    right <- xmin + (1:n) * dx
    left <- right - dx
    mid <- right - dx * (1 - fractionFromLeft)
    fm <- f(mid)
    rect(left, 0, right, fm, density = 20, border = "red")
    points(mid, fm, col = "red", cex = 1.25, pch = 19)
    sum(fm * dx)
}
> showIntegral(function(x)1/x, 1, 4, n=3) - sum(1/(0.5 + (1:3)))
[1] 0
> showIntegral(function(x)1/x, 1, 4, n=3, fractionFromLeft=0) - sum(1/(0 + (1:3)))
[1] 0
> showIntegral(function(x)1/x, 1, 4, n=3, fractionFromLeft=1) - sum(1/(1 + (1:3)))
[1] 0

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Dec 16, 2015 at 9:00 AM, Steven Stoline <sstoline at gmail.com> wrote:
> Dear William: Left and Right Riemann Sums
>
>
> Is there is a way to modify your function to compute Left Riemann Sum and
> Right Riemann Sum. I tried to modify yours, but i was not be able to make it
> work correctly.
>
> This is your function used to compute the Middle Riemann Sum.
>
>
>
> showIntegral.med <- function (f, xmin, xmax, n = 16)
> {
>     curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>     abline(h = 0)
>     dx <- (xmax - xmin)/n
>     right <- xmin + (1:n) * dx
>     left <- right - dx
>     mid <- right - dx/2
>     fm <- f(mid)
>     rect(left, 0, right, fm, density = 20, border = "red")
>     points(mid, fm, col = "red", cex = 1.25, pch = 19)
>     sum(fm * dx)
> }
>
>
>
> ### Example 1: f(x) = x^2  , xmin=-4, xmax=4
> ### ===============================
>
>
>
> showIntegral.med(f=function(x)x^2, xmin=-4, xmax=4, n=16)
>
>
>
> with many thanks
> steve
>
> On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com> wrote:
>>
>> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
>> is at 0, but you want it to be at -4.  You should turn this into a
>> function
>> so you don't have to remember how the variables in your code depend
>> on one another.   E.g.,
>>
>> showIntegral <- function (f, xmin, xmax, n = 16)
>> {
>>     curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>>     abline(h = 0)
>>     dx <- (xmax - xmin)/n
>>     right <- xmin + (1:n) * dx
>>     left <- right - dx
>>     mid <- right - dx/2
>>     fm <- f(mid)
>>     rect(left, 0, right, fm, density = 20, border = "red")
>>     points(mid, fm, col = "red", cex = 1.25, pch = 19)
>>     sum(fm * dx)
>> }
>> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
>> [1] 42.5
>> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
>> [1] 42.66602
>> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
>> [1] 42.66663
>>
>> > 2*4^3/3
>> [1] 42.66667
>> > showIntegral
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>>
>> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
>> wrote:
>> > Dear Peter: in my previous email I forgot to reply to the list too
>> >
>> > I used your code for more than one examples, and it works nicely. But
>> > when
>> > I tried to use for the the function: f(x) = x^2, it looks like I am
>> > missing
>> > something, but I could not figured it out.
>> >
>> > This what I used:
>> >
>> >
>> >
>> > f <- function(x) x^2
>> >
>> > curve(f(x), from=-4, to=4, lwd=2, col="blue")
>> > abline(h=0)
>> > n <- 16
>> > dx <- 8/n
>> > right <- (1:n)*dx
>> > left <- right - dx
>> > mid <- right - dx/2
>> > fm <- f(mid)
>> > rect(left,0,right,fm, density = 20, border = "red")
>> > points(mid, fm, col = "red", cex = 1.25, pch=19)
>> > sum(fm*dx)
>> >
>> >
>> >
>> > 1/3 * (64+64)
>> >
>> >
>> >
>> > with many thanks
>> > steve
>> >
>> > On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
>> > wrote:
>> >
>> >> many thanks
>> >>
>> >> steve
>> >>
>> >> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
>> >> wrote:
>> >>
>> >>> Something like this?
>> >>>
>> >>> f <- function(x) x^3-2*x
>> >>> curve(f(x), from=0, to=4)
>> >>> abline(h=0)
>> >>> n <- 16
>> >>> dx <- 4/n
>> >>> right <- (1:n)*dx
>> >>> left <- right - dx
>> >>> mid <- right - dx/2
>> >>> fm <- f(mid)
>> >>> points(mid, fm)
>> >>> rect(left,0,right,fm)
>> >>>
>> >>> sum(fm*dx)
>> >>>
>> >>> 1/4 * 4^4 - 4^2
>> >>>
>> >>>
>> >>> -pd
>> >>>
>> >>>
>> >>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com> wrote:
>> >>>
>> >>> > Dear All:
>> >>> >
>> >>> > I am trying to explain to my students how to calculate the definite
>> >>> > integral using the Riemann sum. Can someone help me to graph the
>> >>> > area
>> >>> under
>> >>> > the curve of the function, showing the curve as well as the
>> >>> > rectangles
>> >>> > between 0 and 4..
>> >>> >
>> >>> > *f(x) = x^3 - 2*x *
>> >>> >
>> >>> > over the interval [0 , 4]
>> >>> >
>> >>> >
>> >>> >
>> >>> > with many thanks
>> >>> > steve
>> >>> >
>> >>> > --
>> >>> > Steven M. Stoline
>> >>> > 1123 Forest Avenue
>> >>> > Portland, ME 04112
>> >>> > sstoline at gmail.com
>> >>> >
>> >>> >       [[alternative HTML version deleted]]
>> >>> >
>> >>> > ______________________________________________
>> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >>> > PLEASE do read the posting guide
>> >>> http://www.R-project.org/posting-guide.html
>> >>> > and provide commented, minimal, self-contained, reproducible code.
>> >>>
>> >>> --
>> >>> Peter Dalgaard, Professor,
>> >>> Center for Statistics, Copenhagen Business School
>> >>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> >>> Phone: (+45)38153501
>> >>> Office: A 4.23
>> >>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>>
>> >>
>> >>
>> >> --
>> >> Steven M. Stoline
>> >> 1123 Forest Avenue
>> >> Portland, ME 04112
>> >> sstoline at gmail.com
>> >>
>> >
>> >
>> >
>> > --
>> > Steven M. Stoline
>> > 1123 Forest Avenue
>> > Portland, ME 04112
>> > sstoline at gmail.com
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> > http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>
>
>
>
> --
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com


From wdunlap at tibco.com  Wed Dec 16 18:33:51 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 16 Dec 2015 09:33:51 -0800
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAJ7HxBy_nBiKaByKsB0rbKHUwNHOo6BLiKH3wiXqw3tQYbW65g@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
	<CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>
	<CAJ7HxBwLhz_kTU356hAE4cyJ0H=+FGU-jHVYxAw7Ag+pyQHPRQ@mail.gmail.com>
	<CAF8bMcaZnhGaaEPzsUJPJo7sBOdNh77aAL_6bCQEW=MuMmxRCw@mail.gmail.com>
	<CAJ7HxBy_nBiKaByKsB0rbKHUwNHOo6BLiKH3wiXqw3tQYbW65g@mail.gmail.com>
Message-ID: <CAF8bMcYwQXthO_FcLWHBJ0t_88y4du+nEoayRbJihN=OSKGciQ@mail.gmail.com>

You can use capture.output(xmlParseOutput) to make a character vector with
one string per line of printed R output and send that back to Spotfire
for display.

You will need to consult with Spotfire experts (at TIBCO support or
community.tibco.com) to figure out the best way to display this in
Spotfire.  You may have to embed it
in html to get the formatting right.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


On Wed, Dec 16, 2015 at 9:12 AM, Archit Soni <soni.archit1989 at gmail.com> wrote:
> Thanks for the reply William, I wish to print the XML tree format in a text
> area.
> Can we do that by some way ? or is there a way to achieve that in
> IronPython, i am open for both options :)
>
> Thanks again for reply.
>
> On Wed, Dec 16, 2015 at 9:26 PM, William Dunlap <wdunlap at tibco.com> wrote:
>>
>> Your code works in TERR under Spotfire, but since Spotfire deals with
>> rectangular data sets the glue code between them puts your objects in
>> a data.frame, which is not legal.
>>
>> What do you hope to do with the XMLInteralDocument object in Spotfire?
>>  Such objects depend on R internal pointers and don't even survive a
>> save/load cycle in R.  You probably want to a column of data, numeric
>> or character, from it and send that back to Spotfire.
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>>
>> On Wed, Dec 16, 2015 at 12:35 AM, Archit Soni <soni.archit1989 at gmail.com>
>> wrote:
>> > Yes William i'll see if i can get any help from TIBCommunity, but my
>> > code
>> > worked in RStudio.
>> >
>> > On Tue, Dec 15, 2015 at 4:41 PM, William Dunlap <wdunlap at tibco.com>
>> > wrote:
>> >>
>> >> It looks like you are calling TERR from Spotfire.  The Spotfire/TERR
>> >> interface
>> >> can only pass TERR data.frames (eq. to Spotfire tables) back to
>> >> Spotfire
>> >> and
>> >> XMLInternalDocuments cannot be columns of data.frames (in neither TERR
>> >> nor
>> >> R).
>> >>
>> >> You should contact TIBCO support and/or participate in the forums at
>> >> community.tibco.com to see how to solve your problem.
>> >>
>> >>
>> >> Bill Dunlap
>> >> TIBCO Software
>> >> wdunlap tibco.com
>> >>
>> >>
>> >> On Tue, Dec 15, 2015 at 5:09 AM, Archit Soni
>> >> <soni.archit1989 at gmail.com>
>> >> wrote:
>> >> > Hi All,
>> >> >
>> >> > I have the code to print XML tree that is working successfully in R
>> >> > Studio
>> >> > but is failing when i try to work it out with TERR:
>> >> >
>> >> > x<- XML::xmlParse(y)
>> >> >
>> >> > y is input (Coming from a row only once)
>> >> > x is output
>> >> >
>> >> > The above code is working in R studio but doesnt work in TERR, please
>> >> > suggest.
>> >> >
>> >> > TIBCO Enterprise Runtime for R returned an error: 'Error in
>> >> > as.data.frame.default(passed.args[[i]], stringsAsFactors = s : cannot
>> >> > coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a
>> >> > data.frame'.
>> >> > at
>> >> >
>> >> >
>> >> > Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
>> >> > funcClient) at
>> >> >
>> >> >
>> >> > Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
>> >> > at
>> >> >
>> >> >
>> >> > Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
>> >> > at
>> >> >
>> >> >
>> >> > Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
>> >> >
>> >> > --
>> >> > Regards
>> >> > Archit
>> >> >
>> >> >         [[alternative HTML version deleted]]
>> >> >
>> >> > ______________________________________________
>> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> >> > https://stat.ethz.ch/mailman/listinfo/r-help
>> >> > PLEASE do read the posting guide
>> >> > http://www.R-project.org/posting-guide.html
>> >> > and provide commented, minimal, self-contained, reproducible code.
>> >
>> >
>> >
>> >
>> > --
>> > Regards
>> > Archit
>
>
>
>
> --
> Regards
> Archit


From sstoline at gmail.com  Wed Dec 16 18:00:26 2015
From: sstoline at gmail.com (Steven Stoline)
Date: Wed, 16 Dec 2015 12:00:26 -0500
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
Message-ID: <CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>

Dear William: *Left and Right Riemann Sums*


Is there is a way to modify your function to compute Left Riemann Sum and
Right Riemann Sum. I tried to modify yours, but i was not be able to make
it work correctly.

This is your function used to compute the Middle Riemann Sum.



showIntegral.med <- function (f, xmin, xmax, n = 16)
{
    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
    abline(h = 0)
    dx <- (xmax - xmin)/n
    right <- xmin + (1:n) * dx
    left <- right - dx
    mid <- right - dx/2
    fm <- f(mid)
    rect(left, 0, right, fm, density = 20, border = "red")
    points(mid, fm, col = "red", cex = 1.25, pch = 19)
    sum(fm * dx)
}



### Example 1: f(x) = x^2  , xmin=-4, xmax=4
### ===============================



showIntegral.med(f=function(x)x^2, xmin=-4, xmax=4, n=16)



with many thanks
steve

On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com> wrote:

> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
> is at 0, but you want it to be at -4.  You should turn this into a function
> so you don't have to remember how the variables in your code depend
> on one another.   E.g.,
>
> showIntegral <- function (f, xmin, xmax, n = 16)
> {
>     curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>     abline(h = 0)
>     dx <- (xmax - xmin)/n
>     right <- xmin + (1:n) * dx
>     left <- right - dx
>     mid <- right - dx/2
>     fm <- f(mid)
>     rect(left, 0, right, fm, density = 20, border = "red")
>     points(mid, fm, col = "red", cex = 1.25, pch = 19)
>     sum(fm * dx)
> }
> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
> [1] 42.5
> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
> [1] 42.66602
> > showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
> [1] 42.66663
>
> > 2*4^3/3
> [1] 42.66667
> > showIntegral
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
> wrote:
> > Dear Peter: in my previous email I forgot to reply to the list too
> >
> > I used your code for more than one examples, and it works nicely. But
> when
> > I tried to use for the the function: f(x) = x^2, it looks like I am
> missing
> > something, but I could not figured it out.
> >
> > This what I used:
> >
> >
> >
> > f <- function(x) x^2
> >
> > curve(f(x), from=-4, to=4, lwd=2, col="blue")
> > abline(h=0)
> > n <- 16
> > dx <- 8/n
> > right <- (1:n)*dx
> > left <- right - dx
> > mid <- right - dx/2
> > fm <- f(mid)
> > rect(left,0,right,fm, density = 20, border = "red")
> > points(mid, fm, col = "red", cex = 1.25, pch=19)
> > sum(fm*dx)
> >
> >
> >
> > 1/3 * (64+64)
> >
> >
> >
> > with many thanks
> > steve
> >
> > On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
> wrote:
> >
> >> many thanks
> >>
> >> steve
> >>
> >> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
> wrote:
> >>
> >>> Something like this?
> >>>
> >>> f <- function(x) x^3-2*x
> >>> curve(f(x), from=0, to=4)
> >>> abline(h=0)
> >>> n <- 16
> >>> dx <- 4/n
> >>> right <- (1:n)*dx
> >>> left <- right - dx
> >>> mid <- right - dx/2
> >>> fm <- f(mid)
> >>> points(mid, fm)
> >>> rect(left,0,right,fm)
> >>>
> >>> sum(fm*dx)
> >>>
> >>> 1/4 * 4^4 - 4^2
> >>>
> >>>
> >>> -pd
> >>>
> >>>
> >>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com> wrote:
> >>>
> >>> > Dear All:
> >>> >
> >>> > I am trying to explain to my students how to calculate the definite
> >>> > integral using the Riemann sum. Can someone help me to graph the area
> >>> under
> >>> > the curve of the function, showing the curve as well as the
> rectangles
> >>> > between 0 and 4..
> >>> >
> >>> > *f(x) = x^3 - 2*x *
> >>> >
> >>> > over the interval [0 , 4]
> >>> >
> >>> >
> >>> >
> >>> > with many thanks
> >>> > steve
> >>> >
> >>> > --
> >>> > Steven M. Stoline
> >>> > 1123 Forest Avenue
> >>> > Portland, ME 04112
> >>> > sstoline at gmail.com
> >>> >
> >>> >       [[alternative HTML version deleted]]
> >>> >
> >>> > ______________________________________________
> >>> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> > https://stat.ethz.ch/mailman/listinfo/r-help
> >>> > PLEASE do read the posting guide
> >>> http://www.R-project.org/posting-guide.html
> >>> > and provide commented, minimal, self-contained, reproducible code.
> >>>
> >>> --
> >>> Peter Dalgaard, Professor,
> >>> Center for Statistics, Copenhagen Business School
> >>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >>> Phone: (+45)38153501
> >>> Office: A 4.23
> >>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>
> >>
> >> --
> >> Steven M. Stoline
> >> 1123 Forest Avenue
> >> Portland, ME 04112
> >> sstoline at gmail.com
> >>
> >
> >
> >
> > --
> > Steven M. Stoline
> > 1123 Forest Avenue
> > Portland, ME 04112
> > sstoline at gmail.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>



-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From glennmschultz at me.com  Wed Dec 16 16:45:42 2015
From: glennmschultz at me.com (Glenn Schultz)
Date: Wed, 16 Dec 2015 15:45:42 +0000 (GMT)
Subject: [R] S4 Class to Array
Message-ID: <308922ed-b418-4460-9d98-8e460d51b7e8@me.com>

Hello All,

I would like to be able to convert and S4 class to an array but I seem to be stuck. ?Reading the documentation I think I need to use setAs() and then as()?Below?is a minimal example but I cannot get it to work. ?So I am doing something wrong but I don't know what it is.

Any insights/help are appreciated,
- Glenn

setClass("AnObject", 
representation(
dataone = "numeric",
datatwo = "numeric"))

FillObject <- function(){
one <- rep(10, 20)
two <- rep(10,20)

new("AnObject",
dataone = one,
datatwo = two)
}

MyObject <- FillObject()

setAs(from = "AnObject" , to = "array", def = function(from) from(AnObject))

NewObject <- as(MyObject, "array")

From dwinsemius at comcast.net  Wed Dec 16 19:43:15 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 10:43:15 -0800
Subject: [R] S4 Class to Array
In-Reply-To: <308922ed-b418-4460-9d98-8e460d51b7e8@me.com>
References: <308922ed-b418-4460-9d98-8e460d51b7e8@me.com>
Message-ID: <C9273423-D53C-4DB9-A002-AAC40549D070@comcast.net>


> On Dec 16, 2015, at 7:45 AM, Glenn Schultz <glennmschultz at me.com> wrote:
> 
> Hello All,
> 
> I would like to be able to convert and S4 class to an array but I seem to be stuck.  Reading the documentation I think I need to use setAs() and then as() Below is a minimal example but I cannot get it to work.  So I am doing something wrong but I don't know what it is.
> 
> Any insights/help are appreciated,
> - Glenn
> 
> setClass("AnObject", representation(
> dataone = "numeric",
> datatwo = "numeric"))
> 
> FillObject <- function(){
> one <- rep(10, 20)
> two <- rep(10,20)
> 
> new("AnObject",
> dataone = one,
> datatwo = two)
> }
> 
> MyObject <- FillObject()
> 
> setAs(from = "AnObject" , to = "array", def = function(from) from(AnObject))

Just imitating the example on the ?setAs page I get no error with:

setAs("AnObject" ,"array", def = function(from,to) 
                                        cbind(from at dataone,from at datatwo) )

NewObject <- as(MyObject, "array")


-- 

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Wed Dec 16 20:09:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 11:09:53 -0800
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
	<CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
Message-ID: <E87D8C82-672F-4AAE-BCB8-37887D36DC8A@comcast.net>


> On Dec 16, 2015, at 9:00 AM, Steven Stoline <sstoline at gmail.com> wrote:
> 
> Dear William: *Left and Right Riemann Sums*
> 
> 
> Is there is a way to modify your function to compute Left Riemann Sum and
> Right Riemann Sum. I tried to modify yours, but i was not be able to make
> it work correctly.
> 
> This is your function used to compute the Middle Riemann Sum.

I think it's actually Dalgaard's method.
> 
> showIntegral.med <- function (f, xmin, xmax, n = 16)
> {
>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>    abline(h = 0)
>    dx <- (xmax - xmin)/n
>    right <- xmin + (1:n) * dx
>    left <- right - dx
>    mid <- right - dx/2
>    fm <- f(mid)
>    rect(left, 0, right, fm, density = 20, border = "red")
>    points(mid, fm, col = "red", cex = 1.25, pch = 19)
>    sum(fm * dx)
> }
> 
> 
> 
> ### Example 1: f(x) = x^2  , xmin=-4, xmax=4
> ### ===============================
> 
> 
> 
> showIntegral.med(f=function(x)x^2, xmin=-4, xmax=4, n=16)

Wouldn't it just involve skipping the 'mid' calculations and using either the right or left values? Illustration for right:

showIntegral.rt <- function (f, xmin, xmax, n = 16)
{
   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
   abline(h = 0)
   dx <- (xmax - xmin)/n
   right <- xmin + (1:n) * dx
   left <- right - dx
   fr <- f(right)
   rect(left, 0, right, fr, density = 20, border = "red")
   points(right, fr, col = "red", cex = 1.25, pch = 19)
   sum(fr * dx)
}

You can make it prettier with plotmath:

showIntegral.rt <- function (f, xmin, xmax, n = 16)
{
   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
   abline(h = 0)
   dx <- (xmax - xmin)/n
   right <- xmin + (1:n) * dx
   left <- right - dx
   fr <- f(right)
   rect(left, 0, right, fr, density = 20, border = "red")
   points(right, fr, col = "red", cex = 1.25, pch = 19)
   sum(fr * dx)
 text(0,10,   # might want to do some adaptive positioning instead
   bquote( integral( .(body(f) )*dx, a, b) == .( sum(fr * dx )) ) )
}

-- 
David.

> 
> 
> 
> with many thanks
> steve
> 
> On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
>> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
>> is at 0, but you want it to be at -4.  You should turn this into a function
>> so you don't have to remember how the variables in your code depend
>> on one another.   E.g.,
>> 
>> showIntegral <- function (f, xmin, xmax, n = 16)
>> {
>>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>>    abline(h = 0)
>>    dx <- (xmax - xmin)/n
>>    right <- xmin + (1:n) * dx
>>    left <- right - dx
>>    mid <- right - dx/2
>>    fm <- f(mid)
>>    rect(left, 0, right, fm, density = 20, border = "red")
>>    points(mid, fm, col = "red", cex = 1.25, pch = 19)
>>    sum(fm * dx)
>> }
>>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
>> [1] 42.5
>>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
>> [1] 42.66602
>>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
>> [1] 42.66663
>> 
>>> 2*4^3/3
>> [1] 42.66667
>>> showIntegral
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>> 
>> 
>> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
>> wrote:
>>> Dear Peter: in my previous email I forgot to reply to the list too
>>> 
>>> I used your code for more than one examples, and it works nicely. But
>> when
>>> I tried to use for the the function: f(x) = x^2, it looks like I am
>> missing
>>> something, but I could not figured it out.
>>> 
>>> This what I used:
>>> 
>>> 
>>> 
>>> f <- function(x) x^2
>>> 
>>> curve(f(x), from=-4, to=4, lwd=2, col="blue")
>>> abline(h=0)
>>> n <- 16
>>> dx <- 8/n
>>> right <- (1:n)*dx
>>> left <- right - dx
>>> mid <- right - dx/2
>>> fm <- f(mid)
>>> rect(left,0,right,fm, density = 20, border = "red")
>>> points(mid, fm, col = "red", cex = 1.25, pch=19)
>>> sum(fm*dx)
>>> 
>>> 
>>> 
>>> 1/3 * (64+64)
>>> 
>>> 
>>> 
>>> with many thanks
>>> steve
>>> 
>>> On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
>> wrote:
>>> 
>>>> many thanks
>>>> 
>>>> steve
>>>> 
>>>> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
>> wrote:
>>>> 
>>>>> Something like this?
>>>>> 
>>>>> f <- function(x) x^3-2*x
>>>>> curve(f(x), from=0, to=4)
>>>>> abline(h=0)
>>>>> n <- 16
>>>>> dx <- 4/n
>>>>> right <- (1:n)*dx
>>>>> left <- right - dx
>>>>> mid <- right - dx/2
>>>>> fm <- f(mid)
>>>>> points(mid, fm)
>>>>> rect(left,0,right,fm)
>>>>> 
>>>>> sum(fm*dx)
>>>>> 
>>>>> 1/4 * 4^4 - 4^2
>>>>> 
>>>>> 
>>>>> -pd
>>>>> 
>>>>> 
>>>>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com> wrote:
>>>>> 
>>>>>> Dear All:
>>>>>> 
>>>>>> I am trying to explain to my students how to calculate the definite
>>>>>> integral using the Riemann sum. Can someone help me to graph the area
>>>>> under
>>>>>> the curve of the function, showing the curve as well as the
>> rectangles
>>>>>> between 0 and 4..
>>>>>> 
>>>>>> *f(x) = x^3 - 2*x *
>>>>>> 
>>>>>> over the interval [0 , 4]
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> with many thanks
>>>>>> steve
>>>>>> 
>>>>>> --
>>>>>> Steven M. Stoline
>>>>>> 1123 Forest Avenue
>>>>>> Portland, ME 04112
>>>>>> sstoline at gmail.com
>>>>>> 
>>>>>>      [[alternative HTML version deleted]]
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>>> --
>>>>> Peter Dalgaard, Professor,
>>>>> Center for Statistics, Copenhagen Business School
>>>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>>>> Phone: (+45)38153501
>>>>> Office: A 4.23
>>>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>> 
>>>> 
>>>> --
>>>> Steven M. Stoline
>>>> 1123 Forest Avenue
>>>> Portland, ME 04112
>>>> sstoline at gmail.com
>>>> 
>>> 
>>> 
>>> 
>>> --
>>> Steven M. Stoline
>>> 1123 Forest Avenue
>>> Portland, ME 04112
>>> sstoline at gmail.com
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> 
> 
> -- 
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From pdalgd at gmail.com  Wed Dec 16 21:24:53 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 16 Dec 2015 21:24:53 +0100
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <CAHDp66DQcXqij0JuC5XBaJSeFZ9SqaoQABPk=XD6cPp=Ts-CCw@mail.gmail.com>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
	<CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
	<E87D8C82-672F-4AAE-BCB8-37887D36DC8A@comcast.net>
	<CAHDp66DQcXqij0JuC5XBaJSeFZ9SqaoQABPk=XD6cPp=Ts-CCw@mail.gmail.com>
Message-ID: <A569DF75-9F33-41E6-99F1-6C1A98547720@gmail.com>


> On 16 Dec 2015, at 20:58 , Steven Stoline <sstoline at gmail.com> wrote:
> 
> Dear David:
> 
> 
> could you please try it for the functions f(x)=x^2 from -4 to 4 and the function f(x) = sqrt(x) from 0 to 4, and look watch the graphs.

Looks fine to me, at least the non-plotmath version?

-pd

> 
> 
> Thank you very much for your helps.
> 
> 
> steve
> 
> 
> On Wed, Dec 16, 2015 at 2:09 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > On Dec 16, 2015, at 9:00 AM, Steven Stoline <sstoline at gmail.com> wrote:
> >
> > Dear William: *Left and Right Riemann Sums*
> >
> >
> > Is there is a way to modify your function to compute Left Riemann Sum and
> > Right Riemann Sum. I tried to modify yours, but i was not be able to make
> > it work correctly.
> >
> > This is your function used to compute the Middle Riemann Sum.
> 
> I think it's actually Dalgaard's method.
> >
> > showIntegral.med <- function (f, xmin, xmax, n = 16)
> > {
> >    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
> >    abline(h = 0)
> >    dx <- (xmax - xmin)/n
> >    right <- xmin + (1:n) * dx
> >    left <- right - dx
> >    mid <- right - dx/2
> >    fm <- f(mid)
> >    rect(left, 0, right, fm, density = 20, border = "red")
> >    points(mid, fm, col = "red", cex = 1.25, pch = 19)
> >    sum(fm * dx)
> > }
> >
> >
> >
> > ### Example 1: f(x) = x^2  , xmin=-4, xmax=4
> > ### ===============================
> >
> >
> >
> > showIntegral.med(f=function(x)x^2, xmin=-4, xmax=4, n=16)
> 
> Wouldn't it just involve skipping the 'mid' calculations and using either the right or left values? Illustration for right:
> 
> showIntegral.rt <- function (f, xmin, xmax, n = 16)
> {
>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>    abline(h = 0)
>    dx <- (xmax - xmin)/n
>    right <- xmin + (1:n) * dx
>    left <- right - dx
>    fr <- f(right)
>    rect(left, 0, right, fr, density = 20, border = "red")
>    points(right, fr, col = "red", cex = 1.25, pch = 19)
>    sum(fr * dx)
> }
> 
> You can make it prettier with plotmath:
> 
> showIntegral.rt <- function (f, xmin, xmax, n = 16)
> {
>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>    abline(h = 0)
>    dx <- (xmax - xmin)/n
>    right <- xmin + (1:n) * dx
>    left <- right - dx
>    fr <- f(right)
>    rect(left, 0, right, fr, density = 20, border = "red")
>    points(right, fr, col = "red", cex = 1.25, pch = 19)
>    sum(fr * dx)
>  text(0,10,   # might want to do some adaptive positioning instead
>    bquote( integral( .(body(f) )*dx, a, b) == .( sum(fr * dx )) ) )
> }
> 
> --
> David.
> 
> >
> >
> >
> > with many thanks
> > steve
> >
> > On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com> wrote:
> >
> >> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
> >> is at 0, but you want it to be at -4.  You should turn this into a function
> >> so you don't have to remember how the variables in your code depend
> >> on one another.   E.g.,
> >>
> >> showIntegral <- function (f, xmin, xmax, n = 16)
> >> {
> >>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
> >>    abline(h = 0)
> >>    dx <- (xmax - xmin)/n
> >>    right <- xmin + (1:n) * dx
> >>    left <- right - dx
> >>    mid <- right - dx/2
> >>    fm <- f(mid)
> >>    rect(left, 0, right, fm, density = 20, border = "red")
> >>    points(mid, fm, col = "red", cex = 1.25, pch = 19)
> >>    sum(fm * dx)
> >> }
> >>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
> >> [1] 42.5
> >>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
> >> [1] 42.66602
> >>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
> >> [1] 42.66663
> >>
> >>> 2*4^3/3
> >> [1] 42.66667
> >>> showIntegral
> >> Bill Dunlap
> >> TIBCO Software
> >> wdunlap tibco.com
> >>
> >>
> >> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
> >> wrote:
> >>> Dear Peter: in my previous email I forgot to reply to the list too
> >>>
> >>> I used your code for more than one examples, and it works nicely. But
> >> when
> >>> I tried to use for the the function: f(x) = x^2, it looks like I am
> >> missing
> >>> something, but I could not figured it out.
> >>>
> >>> This what I used:
> >>>
> >>>
> >>>
> >>> f <- function(x) x^2
> >>>
> >>> curve(f(x), from=-4, to=4, lwd=2, col="blue")
> >>> abline(h=0)
> >>> n <- 16
> >>> dx <- 8/n
> >>> right <- (1:n)*dx
> >>> left <- right - dx
> >>> mid <- right - dx/2
> >>> fm <- f(mid)
> >>> rect(left,0,right,fm, density = 20, border = "red")
> >>> points(mid, fm, col = "red", cex = 1.25, pch=19)
> >>> sum(fm*dx)
> >>>
> >>>
> >>>
> >>> 1/3 * (64+64)
> >>>
> >>>
> >>>
> >>> with many thanks
> >>> steve
> >>>
> >>> On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
> >> wrote:
> >>>
> >>>> many thanks
> >>>>
> >>>> steve
> >>>>
> >>>> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
> >> wrote:
> >>>>
> >>>>> Something like this?
> >>>>>
> >>>>> f <- function(x) x^3-2*x
> >>>>> curve(f(x), from=0, to=4)
> >>>>> abline(h=0)
> >>>>> n <- 16
> >>>>> dx <- 4/n
> >>>>> right <- (1:n)*dx
> >>>>> left <- right - dx
> >>>>> mid <- right - dx/2
> >>>>> fm <- f(mid)
> >>>>> points(mid, fm)
> >>>>> rect(left,0,right,fm)
> >>>>>
> >>>>> sum(fm*dx)
> >>>>>
> >>>>> 1/4 * 4^4 - 4^2
> >>>>>
> >>>>>
> >>>>> -pd
> >>>>>
> >>>>>
> >>>>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com> wrote:
> >>>>>
> >>>>>> Dear All:
> >>>>>>
> >>>>>> I am trying to explain to my students how to calculate the definite
> >>>>>> integral using the Riemann sum. Can someone help me to graph the area
> >>>>> under
> >>>>>> the curve of the function, showing the curve as well as the
> >> rectangles
> >>>>>> between 0 and 4..
> >>>>>>
> >>>>>> *f(x) = x^3 - 2*x *
> >>>>>>
> >>>>>> over the interval [0 , 4]
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> with many thanks
> >>>>>> steve
> >>>>>>
> >>>>>> --
> >>>>>> Steven M. Stoline
> >>>>>> 1123 Forest Avenue
> >>>>>> Portland, ME 04112
> >>>>>> sstoline at gmail.com
> >>>>>>
> >>>>>>      [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> --
> >>>>> Peter Dalgaard, Professor,
> >>>>> Center for Statistics, Copenhagen Business School
> >>>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >>>>> Phone: (+45)38153501
> >>>>> Office: A 4.23
> >>>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>
> >>>>
> >>>> --
> >>>> Steven M. Stoline
> >>>> 1123 Forest Avenue
> >>>> Portland, ME 04112
> >>>> sstoline at gmail.com
> >>>>
> >>>
> >>>
> >>>
> >>> --
> >>> Steven M. Stoline
> >>> 1123 Forest Avenue
> >>> Portland, ME 04112
> >>> sstoline at gmail.com
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
> >
> > --
> > Steven M. Stoline
> > 1123 Forest Avenue
> > Portland, ME 04112
> > sstoline at gmail.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> 
> 
> 
> -- 
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sstoline at gmail.com  Wed Dec 16 20:58:04 2015
From: sstoline at gmail.com (Steven Stoline)
Date: Wed, 16 Dec 2015 14:58:04 -0500
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <E87D8C82-672F-4AAE-BCB8-37887D36DC8A@comcast.net>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
	<CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
	<E87D8C82-672F-4AAE-BCB8-37887D36DC8A@comcast.net>
Message-ID: <CAHDp66DQcXqij0JuC5XBaJSeFZ9SqaoQABPk=XD6cPp=Ts-CCw@mail.gmail.com>

Dear David:


could you please try it for the functions *f(x)=x^2* from *-4* to *4* and
the function *f(x) = sqrt(x)* from *0* to *4*, and look watch the graphs.


Thank you very much for your helps.


steve


On Wed, Dec 16, 2015 at 2:09 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Dec 16, 2015, at 9:00 AM, Steven Stoline <sstoline at gmail.com> wrote:
> >
> > Dear William: *Left and Right Riemann Sums*
> >
> >
> > Is there is a way to modify your function to compute Left Riemann Sum and
> > Right Riemann Sum. I tried to modify yours, but i was not be able to make
> > it work correctly.
> >
> > This is your function used to compute the Middle Riemann Sum.
>
> I think it's actually Dalgaard's method.
> >
> > showIntegral.med <- function (f, xmin, xmax, n = 16)
> > {
> >    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
> >    abline(h = 0)
> >    dx <- (xmax - xmin)/n
> >    right <- xmin + (1:n) * dx
> >    left <- right - dx
> >    mid <- right - dx/2
> >    fm <- f(mid)
> >    rect(left, 0, right, fm, density = 20, border = "red")
> >    points(mid, fm, col = "red", cex = 1.25, pch = 19)
> >    sum(fm * dx)
> > }
> >
> >
> >
> > ### Example 1: f(x) = x^2  , xmin=-4, xmax=4
> > ### ===============================
> >
> >
> >
> > showIntegral.med(f=function(x)x^2, xmin=-4, xmax=4, n=16)
>
> Wouldn't it just involve skipping the 'mid' calculations and using either
> the right or left values? Illustration for right:
>
> showIntegral.rt <- function (f, xmin, xmax, n = 16)
> {
>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>    abline(h = 0)
>    dx <- (xmax - xmin)/n
>    right <- xmin + (1:n) * dx
>    left <- right - dx
>    fr <- f(right)
>    rect(left, 0, right, fr, density = 20, border = "red")
>    points(right, fr, col = "red", cex = 1.25, pch = 19)
>    sum(fr * dx)
> }
>
> You can make it prettier with plotmath:
>
> showIntegral.rt <- function (f, xmin, xmax, n = 16)
> {
>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>    abline(h = 0)
>    dx <- (xmax - xmin)/n
>    right <- xmin + (1:n) * dx
>    left <- right - dx
>    fr <- f(right)
>    rect(left, 0, right, fr, density = 20, border = "red")
>    points(right, fr, col = "red", cex = 1.25, pch = 19)
>    sum(fr * dx)
>  text(0,10,   # might want to do some adaptive positioning instead
>    bquote( integral( .(body(f) )*dx, a, b) == .( sum(fr * dx )) ) )
> }
>
> --
> David.
>
> >
> >
> >
> > with many thanks
> > steve
> >
> > On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com>
> wrote:
> >
> >> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
> >> is at 0, but you want it to be at -4.  You should turn this into a
> function
> >> so you don't have to remember how the variables in your code depend
> >> on one another.   E.g.,
> >>
> >> showIntegral <- function (f, xmin, xmax, n = 16)
> >> {
> >>    curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
> >>    abline(h = 0)
> >>    dx <- (xmax - xmin)/n
> >>    right <- xmin + (1:n) * dx
> >>    left <- right - dx
> >>    mid <- right - dx/2
> >>    fm <- f(mid)
> >>    rect(left, 0, right, fm, density = 20, border = "red")
> >>    points(mid, fm, col = "red", cex = 1.25, pch = 19)
> >>    sum(fm * dx)
> >> }
> >>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
> >> [1] 42.5
> >>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
> >> [1] 42.66602
> >>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
> >> [1] 42.66663
> >>
> >>> 2*4^3/3
> >> [1] 42.66667
> >>> showIntegral
> >> Bill Dunlap
> >> TIBCO Software
> >> wdunlap tibco.com
> >>
> >>
> >> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
> >> wrote:
> >>> Dear Peter: in my previous email I forgot to reply to the list too
> >>>
> >>> I used your code for more than one examples, and it works nicely. But
> >> when
> >>> I tried to use for the the function: f(x) = x^2, it looks like I am
> >> missing
> >>> something, but I could not figured it out.
> >>>
> >>> This what I used:
> >>>
> >>>
> >>>
> >>> f <- function(x) x^2
> >>>
> >>> curve(f(x), from=-4, to=4, lwd=2, col="blue")
> >>> abline(h=0)
> >>> n <- 16
> >>> dx <- 8/n
> >>> right <- (1:n)*dx
> >>> left <- right - dx
> >>> mid <- right - dx/2
> >>> fm <- f(mid)
> >>> rect(left,0,right,fm, density = 20, border = "red")
> >>> points(mid, fm, col = "red", cex = 1.25, pch=19)
> >>> sum(fm*dx)
> >>>
> >>>
> >>>
> >>> 1/3 * (64+64)
> >>>
> >>>
> >>>
> >>> with many thanks
> >>> steve
> >>>
> >>> On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
> >> wrote:
> >>>
> >>>> many thanks
> >>>>
> >>>> steve
> >>>>
> >>>> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
> >> wrote:
> >>>>
> >>>>> Something like this?
> >>>>>
> >>>>> f <- function(x) x^3-2*x
> >>>>> curve(f(x), from=0, to=4)
> >>>>> abline(h=0)
> >>>>> n <- 16
> >>>>> dx <- 4/n
> >>>>> right <- (1:n)*dx
> >>>>> left <- right - dx
> >>>>> mid <- right - dx/2
> >>>>> fm <- f(mid)
> >>>>> points(mid, fm)
> >>>>> rect(left,0,right,fm)
> >>>>>
> >>>>> sum(fm*dx)
> >>>>>
> >>>>> 1/4 * 4^4 - 4^2
> >>>>>
> >>>>>
> >>>>> -pd
> >>>>>
> >>>>>
> >>>>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com>
> wrote:
> >>>>>
> >>>>>> Dear All:
> >>>>>>
> >>>>>> I am trying to explain to my students how to calculate the definite
> >>>>>> integral using the Riemann sum. Can someone help me to graph the
> area
> >>>>> under
> >>>>>> the curve of the function, showing the curve as well as the
> >> rectangles
> >>>>>> between 0 and 4..
> >>>>>>
> >>>>>> *f(x) = x^3 - 2*x *
> >>>>>>
> >>>>>> over the interval [0 , 4]
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>> with many thanks
> >>>>>> steve
> >>>>>>
> >>>>>> --
> >>>>>> Steven M. Stoline
> >>>>>> 1123 Forest Avenue
> >>>>>> Portland, ME 04112
> >>>>>> sstoline at gmail.com
> >>>>>>
> >>>>>>      [[alternative HTML version deleted]]
> >>>>>>
> >>>>>> ______________________________________________
> >>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>>> PLEASE do read the posting guide
> >>>>> http://www.R-project.org/posting-guide.html
> >>>>>> and provide commented, minimal, self-contained, reproducible code.
> >>>>>
> >>>>> --
> >>>>> Peter Dalgaard, Professor,
> >>>>> Center for Statistics, Copenhagen Business School
> >>>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >>>>> Phone: (+45)38153501
> >>>>> Office: A 4.23
> >>>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>
> >>>>
> >>>> --
> >>>> Steven M. Stoline
> >>>> 1123 Forest Avenue
> >>>> Portland, ME 04112
> >>>> sstoline at gmail.com
> >>>>
> >>>
> >>>
> >>>
> >>> --
> >>> Steven M. Stoline
> >>> 1123 Forest Avenue
> >>> Portland, ME 04112
> >>> sstoline at gmail.com
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide
> >> http://www.R-project.org/posting-guide.html
> >>> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> >
> >
> > --
> > Steven M. Stoline
> > 1123 Forest Avenue
> > Portland, ME 04112
> > sstoline at gmail.com
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>


-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From sierrastew at mindspring.com  Wed Dec 16 19:29:59 2015
From: sierrastew at mindspring.com (John Karon)
Date: Wed, 16 Dec 2015 11:29:59 -0700
Subject: [R] unable to download library
Message-ID: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>

I am trying to download the ?Formula? library using install.packages(?Formula?); I have had no problem downloading other libraries recently.  I receive the error message

Warning: unable to access index for repository https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2
Warning message:
package ?Formula? is not available (for R version 3.2.2) 

I receive the same message adding lib=<path> to the install.packages call.  An internet search suggests using download.file(), but that requires knowing the url of the file in the repository.
Can someone tell me the url (just add Formula to the url above?) or suggest another solution?  Thanks.
John Karon
	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec 16 22:09:30 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 13:09:30 -0800
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <A569DF75-9F33-41E6-99F1-6C1A98547720@gmail.com>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
	<CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
	<E87D8C82-672F-4AAE-BCB8-37887D36DC8A@comcast.net>
	<CAHDp66DQcXqij0JuC5XBaJSeFZ9SqaoQABPk=XD6cPp=Ts-CCw@mail.gmail.com>
	<A569DF75-9F33-41E6-99F1-6C1A98547720@gmail.com>
Message-ID: <1C687152-21A2-455E-AFAF-3998769E2E63@comcast.net>


> On Dec 16, 2015, at 12:24 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
>> On 16 Dec 2015, at 20:58 , Steven Stoline <sstoline at gmail.com> wrote:
>> 
>> Dear David:
>> 
>> 
>> could you please try it for the functions f(x)=x^2 from -4 to 4 and the function f(x) = sqrt(x) from 0 to 4, and look watch the graphs.
> 
> Looks fine to me, at least the non-plotmath version?

Here's a plotmath version that positions the expression in the center of the plot region.

showIntegral.rt <- function (f, xmin, xmax, n = 16)
{
   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
   abline(h = 0);
   dx <- (xmax - xmin)/n
   right <- xmin + (1:n) * dx
   left <- right - dx
   fr <- f(right)
   rect(left, 0, right, fr, density = 5, border = "red")
   points(right, fr, col = "red", cex = 1.25, pch = 19)
  
 text(x=(xmax-xmin)/2,
      y=(f(xmax)-f(xmin))/2, 
      bquote( integral(.(body(f))*dx, a, b) == .(sum(fr * dx)) ),
      col="orange" )
}

Works as desired with sqrt(x)

showIntegral.rt(f=function(x) sqrt(x), xmin=0, xmax=4, n=5000)

With higher n, one gets an animation effect in the painting of the interactive window.

-- 
David.
> 
> -pd
> 
>> 
>> 
>> Thank you very much for your helps.
>> 
>> 
>> steve
>> 
>> 
>> On Wed, Dec 16, 2015 at 2:09 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> On Dec 16, 2015, at 9:00 AM, Steven Stoline <sstoline at gmail.com> wrote:
>>> 
>>> Dear William: *Left and Right Riemann Sums*
>>> 
>>> 
>>> Is there is a way to modify your function to compute Left Riemann Sum and
>>> Right Riemann Sum. I tried to modify yours, but i was not be able to make
>>> it work correctly.
>>> 
>>> This is your function used to compute the Middle Riemann Sum.
>> 
>> I think it's actually Dalgaard's method.
>>> 
>>> showIntegral.med <- function (f, xmin, xmax, n = 16)
>>> {
>>>   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>>>   abline(h = 0)
>>>   dx <- (xmax - xmin)/n
>>>   right <- xmin + (1:n) * dx
>>>   left <- right - dx
>>>   mid <- right - dx/2
>>>   fm <- f(mid)
>>>   rect(left, 0, right, fm, density = 20, border = "red")
>>>   points(mid, fm, col = "red", cex = 1.25, pch = 19)
>>>   sum(fm * dx)
>>> }
>>> 
>>> 
>>> 
>>> ### Example 1: f(x) = x^2  , xmin=-4, xmax=4
>>> ### ===============================
>>> 
>>> 
>>> 
>>> showIntegral.med(f=function(x)x^2, xmin=-4, xmax=4, n=16)
>> 
>> Wouldn't it just involve skipping the 'mid' calculations and using either the right or left values? Illustration for right:
>> 
>> showIntegral.rt <- function (f, xmin, xmax, n = 16)
>> {
>>   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>>   abline(h = 0)
>>   dx <- (xmax - xmin)/n
>>   right <- xmin + (1:n) * dx
>>   left <- right - dx
>>   fr <- f(right)
>>   rect(left, 0, right, fr, density = 20, border = "red")
>>   points(right, fr, col = "red", cex = 1.25, pch = 19)
>>   sum(fr * dx)
>> }
>> 
>> You can make it prettier with plotmath:
>> 
>> showIntegral.rt <- function (f, xmin, xmax, n = 16)
>> {
>>   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>>   abline(h = 0)
>>   dx <- (xmax - xmin)/n
>>   right <- xmin + (1:n) * dx
>>   left <- right - dx
>>   fr <- f(right)
>>   rect(left, 0, right, fr, density = 20, border = "red")
>>   points(right, fr, col = "red", cex = 1.25, pch = 19)
>>   sum(fr * dx)
>> text(0,10,   # might want to do some adaptive positioning instead
>>   bquote( integral( .(body(f) )*dx, a, b) == .( sum(fr * dx )) ) )
>> }
>> 
>> --
>> David.
>> 
>>> 
>>> 
>>> 
>>> with many thanks
>>> steve
>>> 
>>> On Sat, Nov 28, 2015 at 1:11 PM, William Dunlap <wdunlap at tibco.com> wrote:
>>> 
>>>> Your right <- (1:n)*dx mean that your leftmost rectangle's left edge
>>>> is at 0, but you want it to be at -4.  You should turn this into a function
>>>> so you don't have to remember how the variables in your code depend
>>>> on one another.   E.g.,
>>>> 
>>>> showIntegral <- function (f, xmin, xmax, n = 16)
>>>> {
>>>>   curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
>>>>   abline(h = 0)
>>>>   dx <- (xmax - xmin)/n
>>>>   right <- xmin + (1:n) * dx
>>>>   left <- right - dx
>>>>   mid <- right - dx/2
>>>>   fm <- f(mid)
>>>>   rect(left, 0, right, fm, density = 20, border = "red")
>>>>   points(mid, fm, col = "red", cex = 1.25, pch = 19)
>>>>   sum(fm * dx)
>>>> }
>>>>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=16)
>>>> [1] 42.5
>>>>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=256)
>>>> [1] 42.66602
>>>>> showIntegral(f=function(x)x^2, xmin=-4, xmax=4, n=1024)
>>>> [1] 42.66663
>>>> 
>>>>> 2*4^3/3
>>>> [1] 42.66667
>>>>> showIntegral
>>>> Bill Dunlap
>>>> TIBCO Software
>>>> wdunlap tibco.com
>>>> 
>>>> 
>>>> On Fri, Nov 27, 2015 at 9:50 PM, Steven Stoline <sstoline at gmail.com>
>>>> wrote:
>>>>> Dear Peter: in my previous email I forgot to reply to the list too
>>>>> 
>>>>> I used your code for more than one examples, and it works nicely. But
>>>> when
>>>>> I tried to use for the the function: f(x) = x^2, it looks like I am
>>>> missing
>>>>> something, but I could not figured it out.
>>>>> 
>>>>> This what I used:
>>>>> 
>>>>> 
>>>>> 
>>>>> f <- function(x) x^2
>>>>> 
>>>>> curve(f(x), from=-4, to=4, lwd=2, col="blue")
>>>>> abline(h=0)
>>>>> n <- 16
>>>>> dx <- 8/n
>>>>> right <- (1:n)*dx
>>>>> left <- right - dx
>>>>> mid <- right - dx/2
>>>>> fm <- f(mid)
>>>>> rect(left,0,right,fm, density = 20, border = "red")
>>>>> points(mid, fm, col = "red", cex = 1.25, pch=19)
>>>>> sum(fm*dx)
>>>>> 
>>>>> 
>>>>> 
>>>>> 1/3 * (64+64)
>>>>> 
>>>>> 
>>>>> 
>>>>> with many thanks
>>>>> steve
>>>>> 
>>>>> On Fri, Nov 27, 2015 at 3:36 PM, Steven Stoline <sstoline at gmail.com>
>>>> wrote:
>>>>> 
>>>>>> many thanks
>>>>>> 
>>>>>> steve
>>>>>> 
>>>>>> On Fri, Nov 27, 2015 at 9:20 AM, peter dalgaard <pdalgd at gmail.com>
>>>> wrote:
>>>>>> 
>>>>>>> Something like this?
>>>>>>> 
>>>>>>> f <- function(x) x^3-2*x
>>>>>>> curve(f(x), from=0, to=4)
>>>>>>> abline(h=0)
>>>>>>> n <- 16
>>>>>>> dx <- 4/n
>>>>>>> right <- (1:n)*dx
>>>>>>> left <- right - dx
>>>>>>> mid <- right - dx/2
>>>>>>> fm <- f(mid)
>>>>>>> points(mid, fm)
>>>>>>> rect(left,0,right,fm)
>>>>>>> 
>>>>>>> sum(fm*dx)
>>>>>>> 
>>>>>>> 1/4 * 4^4 - 4^2
>>>>>>> 
>>>>>>> 
>>>>>>> -pd
>>>>>>> 
>>>>>>> 
>>>>>>> On 27 Nov 2015, at 13:52 , Steven Stoline <sstoline at gmail.com> wrote:
>>>>>>> 
>>>>>>>> Dear All:
>>>>>>>> 
>>>>>>>> I am trying to explain to my students how to calculate the definite
>>>>>>>> integral using the Riemann sum. Can someone help me to graph the area
>>>>>>> under
>>>>>>>> the curve of the function, showing the curve as well as the
>>>> rectangles
>>>>>>>> between 0 and 4..
>>>>>>>> 
>>>>>>>> *f(x) = x^3 - 2*x *
>>>>>>>> 
>>>>>>>> over the interval [0 , 4]
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> with many thanks
>>>>>>>> steve
>>>>>>>> 
>>>>>>>> --
> 

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Wed Dec 16 22:53:58 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Dec 2015 13:53:58 -0800
Subject: [R] unable to download library
In-Reply-To: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
Message-ID: <CAGxFJbRYqT4XeOGVxSKGeLpg4d-FKR8xydPhQcPmBRiUmyMoqg@mail.gmail.com>

Try another repository.

I also recently had problems with downloading from Berkeley using
secure downloads (https: ) . Switching repositories fixed it for me.

Disclaimer: I know next to nothing about internet protocols; so if my
suggestion doesn't help, don't ask me why not. I won't have a clue.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 16, 2015 at 10:29 AM, John Karon <sierrastew at mindspring.com> wrote:
> I am trying to download the ?Formula? library using install.packages(?Formula?); I have had no problem downloading other libraries recently.  I receive the error message
>
> Warning: unable to access index for repository https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2
> Warning message:
> package ?Formula? is not available (for R version 3.2.2)
>
> I receive the same message adding lib=<path> to the install.packages call.  An internet search suggests using download.file(), but that requires knowing the url of the file in the repository.
> Can someone tell me the url (just add Formula to the url above?) or suggest another solution?  Thanks.
> John Karon
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From r.turner at auckland.ac.nz  Wed Dec 16 22:58:27 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Thu, 17 Dec 2015 10:58:27 +1300
Subject: [R] unable to download library
In-Reply-To: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
Message-ID: <5671DE83.4070401@auckland.ac.nz>

On 17/12/15 07:29, John Karon wrote:
> I am trying to download the ?Formula? library using
> install.packages(?Formula?); I have had no problem downloading other
> libraries recently.  I receive the error message
>
> Warning: unable to access index for repository
> https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2 Warning
> message: package ?Formula? is not available (for R version 3.2.2)
>
> I receive the same message adding lib=<path> to the install.packages
> call.  An internet search suggests using download.file(), but that
> requires knowing the url of the file in the repository. Can someone
> tell me the url (just add Formula to the url above?) or suggest
> another solution?  Thanks.

I installed "Formula" just now, under R 3.2.2, using install.packages(), 
with no problem. Did you have a *look* at the URL

     https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2

???

Going there I see that the file Formula_1.2-1.zip is present and I was 
able to download it by clicking on it.

So if you insist on using download.file() you could just tack 
"/Formula_1.2-1.zip" onto the end of your URL.  But there is really no 
reason to do that; install.packages() works.

It is a mystery what your problem could be,

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From pdalgd at gmail.com  Wed Dec 16 23:28:44 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 16 Dec 2015 23:28:44 +0100
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CABdHhvGnPeikypPzEd5MtLeG3ZE=k5bCzV47QncaN5+8yWoy2Q@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
	<CABdHhvGnPeikypPzEd5MtLeG3ZE=k5bCzV47QncaN5+8yWoy2Q@mail.gmail.com>
Message-ID: <59757167-E9F5-477C-84DF-7D6D290489D8@gmail.com>


> On 16 Dec 2015, at 17:42 , Hadley Wickham <h.wickham at gmail.com> wrote:
> 
> On Wed, Dec 16, 2015 at 9:34 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
>> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
>> <maechler at stat.math.ethz.ch> wrote:
>>> 
>>> 
>>>   [............]
>>> 
>>>> You are missing the closing bracket on the boxplot()
>>>> command.  Just finish with a ')'
>>> 
>>> Hmm... I once learned
>>> 
>>> '()' =: parenthesis/es
>>> '[]' =: bracket(s)
>>> '{}' =: brace(s)
>>> 
>>> Of course, I'm not a native English speaker, and my teacher(s) /
>>> teaching material may have been biased ... but, as all three
>>> symbol pairs play an important role in R, I think it would be
>>> really really helpful,  if we could agree on using the same
>>> precise English here.
>>> 
>>> I'm happy to re-learn, but I'd really like to end up with three
>>> different simple English words, if possible.
>>> (Yes, I know and have seen/heard "curly braces", "round
>>> parentheses", ... but I'd hope we can do without the extra adjective.)
>> 
>> I think this is what Americans are taught, but I can never remember
>> which is which. I use round brackets, square brackets, and squiggly
>> brackets, which are memorable, and even if you're not familiar with
>> the terms you can easily understand what I mean.
> 
> I should mention that all three terms have accompanying arm motions ;)
> 

I just wonder whether the original poster managed to brace himself for the oncoming avalanche....

-pd 

> Hadley
> 
> -- 
> http://had.co.nz/
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Wed Dec 16 23:40:53 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 16 Dec 2015 23:40:53 +0100
Subject: [R] Graphing the Area of Definite Integral
In-Reply-To: <1C687152-21A2-455E-AFAF-3998769E2E63@comcast.net>
References: <CAHDp66CP-bv+ojUD_39V7ZdduYNJYCRMDbJg-zSr_coRx4BipA@mail.gmail.com>
	<2D194A57-7B09-4F3C-95BD-5C7866D65C35@gmail.com>
	<CAHDp66D3y+uDSv1sNhQH4JGPippqx4tPHT3H8W2nN2z5-o0o5w@mail.gmail.com>
	<CAHDp66AM5vLRJzvfXEDfkz+M-zPuNoxZNhPm11Bq666B6YXafw@mail.gmail.com>
	<CAF8bMcY1SH9pufuP7ujymNLrR18BF_xvdAuRTKDN1syM0ZedfQ@mail.gmail.com>
	<CAHDp66CPxMb2zTDhdTE0-i_qTf-BYvTf75xLscjD4dLOx8MmZw@mail.gmail.com>
	<E87D8C82-672F-4AAE-BCB8-37887D36DC8A@comcast.net>
	<CAHDp66DQcXqij0JuC5XBaJSeFZ9SqaoQABPk=XD6cPp=Ts-CCw@mail.gmail.com>
	<A569DF75-9F33-41E6-99F1-6C1A98547720@gmail.com>
	<1C687152-21A2-455E-AFAF-3998769E2E63@comcast.net>
Message-ID: <BE69156E-5F39-49FC-9A3D-6CF9D0AB021F@gmail.com>


> On 16 Dec 2015, at 22:09 , David Winsemius <dwinsemius at comcast.net> wrote:
> 
>>> and look watch the graphs.
>> 
>> Looks fine to me, at least the non-plotmath version?
> 
> Here's a plotmath version that positions the expression in the center of the plot region.

Just in case: I wasn't implying an issue with the plot math version, I just never tried it.

(And by the way, the story was that I sketched a solution and Bill wrapped it as a function).

For completeness, here's a version of the trapezoidal rule (the only slightly tricky bit is that polygon() does not vectorize like rect() does):

showIntegral.tr <- function (f, xmin, xmax, n = 16)
{
  curve(f(x), from = xmin, to = xmax, lwd = 2, col = "blue")
  abline(h = 0)
  dx <- (xmax - xmin)/n
  right <- xmin + (1:n) * dx
  left <- right - dx
  fl <- f(left)
  fr <- f(right)
  PP <- Vectorize(function(l,r,fl,fr)
     polygon(c(l, r, r, l), c(0, 0, fr, fl) , density=20, border = "red"))
  PP(left,right, fl, fr)
  sum((fr+fl)/2 * dx)
}

showIntegral.tr(sqrt, xmin=0, xmax=4)


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dwinsemius at comcast.net  Thu Dec 17 00:00:22 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 15:00:22 -0800
Subject: [R] unable to download library
In-Reply-To: <5671DE83.4070401@auckland.ac.nz>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
	<5671DE83.4070401@auckland.ac.nz>
Message-ID: <5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>


> On Dec 16, 2015, at 1:58 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> On 17/12/15 07:29, John Karon wrote:
>> I am trying to download the ?Formula? library using
>> install.packages(?Formula?); I have had no problem downloading other
>> libraries recently.  I receive the error message
>> 
>> Warning: unable to access index for repository
>> https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2 Warning
>> message: package ?Formula? is not available (for R version 3.2.2)
>> 
>> I receive the same message adding lib=<path> to the install.packages
>> call.  An internet search suggests using download.file(), but that
>> requires knowing the url of the file in the repository. Can someone
>> tell me the url (just add Formula to the url above?) or suggest
>> another solution?  Thanks.
> 
> I installed "Formula" just now, under R 3.2.2, using install.packages(), with no problem. Did you have a *look* at the URL
> 
>    https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2
> 
> ???

When I attempt to open that with Chrome I get an error:

This webpage is not available

DNS_PROBE_FINISHED_NXDOMAIN

Like Bert, I also get an error report when I attempt to us the Berkeley repo, but I do not get an error when I use the CRAN repo in Switzerland.


> 
> Going there I see that the file Formula_1.2-1.zip is present and I was able to download it by clicking on it.

Well, those of us using Macs would not choose the zip file.

> 
> So if you insist on using download.file() you could just tack "/Formula_1.2-1.zip" onto the end of your URL.  But there is really no reason to do that; install.packages() works.

Well it works for some people with some repos but not for all people with all repos. I have had problems since upgrading to R 3.2.3.


sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.11.2 (El Capitan)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] grDevices utils     datasets  stats     graphics  grid     
[7] methods   base     

other attached packages:
 [1] maps_3.0.1      rms_4.4-0       SparseM_1.7     Hmisc_3.17-0   
 [5] ggplot2_1.0.1   Formula_1.2-1   survival_2.38-3 sos_1.3-8      
 [9] brew_1.0-6      lattice_0.20-33

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.2         RColorBrewer_1.1-2  plyr_1.8.3         
 [4] tools_3.2.3         rpart_4.1-10        digest_0.6.8       
 [7] polspline_1.1.12    gtable_0.1.2        nlme_3.1-122       
[10] Matrix_1.2-3        mvtnorm_1.0-3       proto_0.3-10       
[13] gridExtra_2.0.0     stringr_1.0.0       cluster_2.0.3      
[16] MatrixModels_0.4-1  nnet_7.3-11         foreign_0.8-66     
[19] multcomp_1.4-1      latticeExtra_0.6-26 TH.data_1.0-6      
[22] reshape2_1.4.1      magrittr_1.5        codetools_0.2-14   
[25] scales_0.3.0.9000   MASS_7.3-45         splines_3.2.3      
[28] colorspace_1.2-6    quantreg_5.19       sandwich_2.3-4     
[31] stringi_1.0-1       acepack_1.3-3.3     munsell_0.4.2      
[34] zoo_1.7-12         


> 
> It is a mystery what your problem could be,
> 
> cheers,
> 
> Rolf Turner
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Thu Dec 17 00:09:04 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Dec 2015 15:09:04 -0800
Subject: [R] unable to download library
In-Reply-To: <5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
	<5671DE83.4070401@auckland.ac.nz>
	<5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>
Message-ID: <CAGxFJbSd3vSr7=ZPjhmDB5kKFAm-BfOAkcAmZPqTBqAqp9ruow@mail.gmail.com>

I would just like to confirm that, like David W. , I am on a Mac.

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 16, 2015 at 3:00 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 16, 2015, at 1:58 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>>
>> On 17/12/15 07:29, John Karon wrote:
>>> I am trying to download the ?Formula? library using
>>> install.packages(?Formula?); I have had no problem downloading other
>>> libraries recently.  I receive the error message
>>>
>>> Warning: unable to access index for repository
>>> https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2 Warning
>>> message: package ?Formula? is not available (for R version 3.2.2)
>>>
>>> I receive the same message adding lib=<path> to the install.packages
>>> call.  An internet search suggests using download.file(), but that
>>> requires knowing the url of the file in the repository. Can someone
>>> tell me the url (just add Formula to the url above?) or suggest
>>> another solution?  Thanks.
>>
>> I installed "Formula" just now, under R 3.2.2, using install.packages(), with no problem. Did you have a *look* at the URL
>>
>>    https://cran.cnr.Berkeley.edu/bin/windows/contrib/3.2
>>
>> ???
>
> When I attempt to open that with Chrome I get an error:
>
> This webpage is not available
>
> DNS_PROBE_FINISHED_NXDOMAIN
>
> Like Bert, I also get an error report when I attempt to us the Berkeley repo, but I do not get an error when I use the CRAN repo in Switzerland.
>
>
>>
>> Going there I see that the file Formula_1.2-1.zip is present and I was able to download it by clicking on it.
>
> Well, those of us using Macs would not choose the zip file.
>
>>
>> So if you insist on using download.file() you could just tack "/Formula_1.2-1.zip" onto the end of your URL.  But there is really no reason to do that; install.packages() works.
>
> Well it works for some people with some repos but not for all people with all repos. I have had problems since upgrading to R 3.2.3.
>
>
> sessionInfo()
> R version 3.2.3 (2015-12-10)
> Platform: x86_64-apple-darwin13.4.0 (64-bit)
> Running under: OS X 10.11.2 (El Capitan)
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] grDevices utils     datasets  stats     graphics  grid
> [7] methods   base
>
> other attached packages:
>  [1] maps_3.0.1      rms_4.4-0       SparseM_1.7     Hmisc_3.17-0
>  [5] ggplot2_1.0.1   Formula_1.2-1   survival_2.38-3 sos_1.3-8
>  [9] brew_1.0-6      lattice_0.20-33
>
> loaded via a namespace (and not attached):
>  [1] Rcpp_0.12.2         RColorBrewer_1.1-2  plyr_1.8.3
>  [4] tools_3.2.3         rpart_4.1-10        digest_0.6.8
>  [7] polspline_1.1.12    gtable_0.1.2        nlme_3.1-122
> [10] Matrix_1.2-3        mvtnorm_1.0-3       proto_0.3-10
> [13] gridExtra_2.0.0     stringr_1.0.0       cluster_2.0.3
> [16] MatrixModels_0.4-1  nnet_7.3-11         foreign_0.8-66
> [19] multcomp_1.4-1      latticeExtra_0.6-26 TH.data_1.0-6
> [22] reshape2_1.4.1      magrittr_1.5        codetools_0.2-14
> [25] scales_0.3.0.9000   MASS_7.3-45         splines_3.2.3
> [28] colorspace_1.2-6    quantreg_5.19       sandwich_2.3-4
> [31] stringi_1.0-1       acepack_1.3-3.3     munsell_0.4.2
> [34] zoo_1.7-12
>
>
>>
>> It is a mystery what your problem could be,
>>
>> cheers,
>>
>> Rolf Turner
>>
>> --
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dleybman at gmail.com  Thu Dec 17 00:27:44 2015
From: dleybman at gmail.com (Dmitri Leybman)
Date: Wed, 16 Dec 2015 18:27:44 -0500
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <59757167-E9F5-477C-84DF-7D6D290489D8@gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
	<CABdHhvGnPeikypPzEd5MtLeG3ZE=k5bCzV47QncaN5+8yWoy2Q@mail.gmail.com>
	<59757167-E9F5-477C-84DF-7D6D290489D8@gmail.com>
Message-ID: <CA+djRJmEh54EtCUdhSjLzEV4N3sDP2hZ78bpfrqRukF3K8tFUQ@mail.gmail.com>

Not a problem at all. I figured the motley crue here couldn't be bracketed
into narrow categories.

An apparent thesis demonstrated here would be that we all speak a slightly
different form of English.

:)

On Wednesday, December 16, 2015, peter dalgaard <pdalgd at gmail.com> wrote:

>
> > On 16 Dec 2015, at 17:42 , Hadley Wickham <h.wickham at gmail.com
> <javascript:;>> wrote:
> >
> > On Wed, Dec 16, 2015 at 9:34 AM, Hadley Wickham <h.wickham at gmail.com
> <javascript:;>> wrote:
> >> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
> >> <maechler at stat.math.ethz.ch <javascript:;>> wrote:
> >>>
> >>>
> >>>   [............]
> >>>
> >>>> You are missing the closing bracket on the boxplot()
> >>>> command.  Just finish with a ')'
> >>>
> >>> Hmm... I once learned
> >>>
> >>> '()' =: parenthesis/es
> >>> '[]' =: bracket(s)
> >>> '{}' =: brace(s)
> >>>
> >>> Of course, I'm not a native English speaker, and my teacher(s) /
> >>> teaching material may have been biased ... but, as all three
> >>> symbol pairs play an important role in R, I think it would be
> >>> really really helpful,  if we could agree on using the same
> >>> precise English here.
> >>>
> >>> I'm happy to re-learn, but I'd really like to end up with three
> >>> different simple English words, if possible.
> >>> (Yes, I know and have seen/heard "curly braces", "round
> >>> parentheses", ... but I'd hope we can do without the extra adjective.)
> >>
> >> I think this is what Americans are taught, but I can never remember
> >> which is which. I use round brackets, square brackets, and squiggly
> >> brackets, which are memorable, and even if you're not familiar with
> >> the terms you can easily understand what I mean.
> >
> > I should mention that all three terms have accompanying arm motions ;)
> >
>
> I just wonder whether the original poster managed to brace himself for the
> oncoming avalanche....
>
> -pd
>
> > Hadley
> >
> > --
> > http://had.co.nz/
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk <javascript:;>  Priv: PDalgd at gmail.com <javascript:;>
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Thu Dec 17 00:44:39 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 17 Dec 2015 00:44:39 +0100
Subject: [R] unable to download library
In-Reply-To: <5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
	<5671DE83.4070401@auckland.ac.nz>
	<5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>
Message-ID: <F3224F05-1AB6-437F-BBE6-F16434E9F845@gmail.com>


> On 17 Dec 2015, at 00:00 , David Winsemius <dwinsemius at comcast.net> wrote:
> 
> When I attempt to open that with Chrome I get an error:
> 
> This webpage is not available
> 
> DNS_PROBE_FINISHED_NXDOMAIN
> 
> Like Bert, I also get an error report when I attempt to us the Berkeley repo, but I do not get an error when I use the CRAN repo in Switzerland.

That'll be a name server issue. Nothing to do with R per se. If it worked for some and not for others for a while, it is most likely due to cached data.

pd$ host cran.cnr.Berkeley.edu
Host cran.cnr.Berkeley.edu not found: 3(NXDOMAIN)

pd$ host cnr.Berkeley.edu
cnr.Berkeley.edu has address 169.229.201.201


-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From NordlDJ at dshs.wa.gov  Thu Dec 17 00:51:52 2015
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Wed, 16 Dec 2015 23:51:52 +0000
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CA+djRJmEh54EtCUdhSjLzEV4N3sDP2hZ78bpfrqRukF3K8tFUQ@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
	<CABdHhvGnPeikypPzEd5MtLeG3ZE=k5bCzV47QncaN5+8yWoy2Q@mail.gmail.com>
	<59757167-E9F5-477C-84DF-7D6D290489D8@gmail.com>
	<CA+djRJmEh54EtCUdhSjLzEV4N3sDP2hZ78bpfrqRukF3K8tFUQ@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662EDCBF0C@WAXMXOLYMB025.WAX.wa.lcl>

Which is why England and the United States have been described as two countries divided  by a common language.  (Could probably throw Scotland and Australia, and  others, into the mix as well ... notice the parethenses, or nice round brackets, or  ?  :-}  )

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dmitri Leybman
Sent: Wednesday, December 16, 2015 3:28 PM
To: peter dalgaard
Cc: r-help at r-project.org; Martin Maechler
Subject: Re: [R] Make a box-whiskers plot in R with 5 variables, color coded.

Not a problem at all. I figured the motley crue here couldn't be bracketed into narrow categories.

An apparent thesis demonstrated here would be that we all speak a slightly different form of English.

:)

On Wednesday, December 16, 2015, peter dalgaard <pdalgd at gmail.com> wrote:

>
> > On 16 Dec 2015, at 17:42 , Hadley Wickham <h.wickham at gmail.com
> <javascript:;>> wrote:
> >
> > On Wed, Dec 16, 2015 at 9:34 AM, Hadley Wickham <h.wickham at gmail.com
> <javascript:;>> wrote:
> >> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler 
> >> <maechler at stat.math.ethz.ch <javascript:;>> wrote:
> >>>
> >>>
> >>>   [............]
> >>>
> >>>> You are missing the closing bracket on the boxplot() command.  
> >>>> Just finish with a ')'
> >>>
> >>> Hmm... I once learned
> >>>
> >>> '()' =: parenthesis/es
> >>> '[]' =: bracket(s)
> >>> '{}' =: brace(s)
> >>>
> >>> Of course, I'm not a native English speaker, and my teacher(s) / 
> >>> teaching material may have been biased ... but, as all three 
> >>> symbol pairs play an important role in R, I think it would be 
> >>> really really helpful,  if we could agree on using the same 
> >>> precise English here.
> >>>
> >>> I'm happy to re-learn, but I'd really like to end up with three 
> >>> different simple English words, if possible.
> >>> (Yes, I know and have seen/heard "curly braces", "round 
> >>> parentheses", ... but I'd hope we can do without the extra 
> >>> adjective.)
> >>
> >> I think this is what Americans are taught, but I can never remember 
> >> which is which. I use round brackets, square brackets, and squiggly 
> >> brackets, which are memorable, and even if you're not familiar with 
> >> the terms you can easily understand what I mean.
> >
> > I should mention that all three terms have accompanying arm motions 
> > ;)
> >
>
> I just wonder whether the original poster managed to brace himself for 
> the oncoming avalanche....
>
> -pd
>
> > Hadley
> >
> > --
> > http://had.co.nz/
> >
> > ______________________________________________
> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE 
> > and
> more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School Solbjerg Plads 3, 
> 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk <javascript:;>  Priv: PDalgd at gmail.com 
> <javascript:;>
>
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and 
> more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec 17 01:05:04 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 16:05:04 -0800
Subject: [R] unable to download library
In-Reply-To: <F3224F05-1AB6-437F-BBE6-F16434E9F845@gmail.com>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
	<5671DE83.4070401@auckland.ac.nz>
	<5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>
	<F3224F05-1AB6-437F-BBE6-F16434E9F845@gmail.com>
Message-ID: <AB8D6DD9-65C0-476E-B038-96B32C95272F@comcast.net>


> On Dec 16, 2015, at 3:44 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
>> On 17 Dec 2015, at 00:00 , David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>> When I attempt to open that with Chrome I get an error:
>> 
>> This webpage is not available
>> 
>> DNS_PROBE_FINISHED_NXDOMAIN
>> 
>> Like Bert, I also get an error report when I attempt to us the Berkeley repo, but I do not get an error when I use the CRAN repo in Switzerland.
> 
> That'll be a name server issue. Nothing to do with R per se. If it worked for some and not for others for a while, it is most likely due to cached data.
> 
> pd$ host cran.cnr.Berkeley.edu
> Host cran.cnr.Berkeley.edu not found: 3(NXDOMAIN)
> 
> pd$ host cnr.Berkeley.edu
> cnr.Berkeley.edu has address 169.229.201.201

I get 

davidwinsemius$ host cran.cnr.Berkeley.edu
cran.cnr.Berkeley.edu is an alias for nature.Berkeley.edu.
nature.Berkeley.edu has address 169.229.201.201
nature.Berkeley.edu has IPv6 address 2607:f140:0:8000::201
nature.Berkeley.edu mail is handled by 5 mx1.ist.Berkeley.edu.
nature.Berkeley.edu mail is handled by 5 mx0.ist.Berkeley.edu.

davidwinsemius$ host cnr.Berkeley.edu
cnr.Berkeley.edu has address 169.229.201.201


When I attempt to open cnr.Berkeley.edu in a browser I get the welcome page from Berkeley's College of Natural Resources ... after redirection to https://nature.berkeley.edu/

When I try trimming the 'cran' from the original link:

https://cran.cnr.berkeley.edu/bin/windows/contrib/3.2

I get page not found from https://nature.berkeley.edu/bin/windows/contrib/3.2

So, how do we determine whether Bert's and my DNS servers have changed more recently (since we are both much closer to the Berkeley campus and perhaps Rolf just has an old ip address), or that Rolf has connection to a more uptodate DNS server? As with Bert my network-fu is inadequate to resolve this

-- 
David
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Thu Dec 17 01:09:33 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 16 Dec 2015 16:09:33 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA27662EDCBF0C@WAXMXOLYMB025.WAX.wa.lcl>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
	<CABdHhvGnPeikypPzEd5MtLeG3ZE=k5bCzV47QncaN5+8yWoy2Q@mail.gmail.com>
	<59757167-E9F5-477C-84DF-7D6D290489D8@gmail.com>
	<CA+djRJmEh54EtCUdhSjLzEV4N3sDP2hZ78bpfrqRukF3K8tFUQ@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA27662EDCBF0C@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <CAGxFJbQ0+wpk1_doOXo5T7fr5U2CHW=GPMdgDo451ZsM7MhaJg@mail.gmail.com>

...
which reminds me of Prof. Henry Higgins's comment in My Fair Lady in
the song "Why can't the English" :

There even are places where English completely disappears.
Why, in America, they haven't used it for years!

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 16, 2015 at 3:51 PM, Nordlund, Dan (DSHS/RDA)
<NordlDJ at dshs.wa.gov> wrote:
> Which is why England and the United States have been described as two countries divided  by a common language.  (Could probably throw Scotland and Australia, and  others, into the mix as well ... notice the parethenses, or nice round brackets, or  ?  :-}  )
>
> Dan
>
> Daniel Nordlund, PhD
> Research and Data Analysis Division
> Services & Enterprise Support Administration
> Washington State Department of Social and Health Services
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Dmitri Leybman
> Sent: Wednesday, December 16, 2015 3:28 PM
> To: peter dalgaard
> Cc: r-help at r-project.org; Martin Maechler
> Subject: Re: [R] Make a box-whiskers plot in R with 5 variables, color coded.
>
> Not a problem at all. I figured the motley crue here couldn't be bracketed into narrow categories.
>
> An apparent thesis demonstrated here would be that we all speak a slightly different form of English.
>
> :)
>
> On Wednesday, December 16, 2015, peter dalgaard <pdalgd at gmail.com> wrote:
>
>>
>> > On 16 Dec 2015, at 17:42 , Hadley Wickham <h.wickham at gmail.com
>> <javascript:;>> wrote:
>> >
>> > On Wed, Dec 16, 2015 at 9:34 AM, Hadley Wickham <h.wickham at gmail.com
>> <javascript:;>> wrote:
>> >> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
>> >> <maechler at stat.math.ethz.ch <javascript:;>> wrote:
>> >>>
>> >>>
>> >>>   [............]
>> >>>
>> >>>> You are missing the closing bracket on the boxplot() command.
>> >>>> Just finish with a ')'
>> >>>
>> >>> Hmm... I once learned
>> >>>
>> >>> '()' =: parenthesis/es
>> >>> '[]' =: bracket(s)
>> >>> '{}' =: brace(s)
>> >>>
>> >>> Of course, I'm not a native English speaker, and my teacher(s) /
>> >>> teaching material may have been biased ... but, as all three
>> >>> symbol pairs play an important role in R, I think it would be
>> >>> really really helpful,  if we could agree on using the same
>> >>> precise English here.
>> >>>
>> >>> I'm happy to re-learn, but I'd really like to end up with three
>> >>> different simple English words, if possible.
>> >>> (Yes, I know and have seen/heard "curly braces", "round
>> >>> parentheses", ... but I'd hope we can do without the extra
>> >>> adjective.)
>> >>
>> >> I think this is what Americans are taught, but I can never remember
>> >> which is which. I use round brackets, square brackets, and squiggly
>> >> brackets, which are memorable, and even if you're not familiar with
>> >> the terms you can easily understand what I mean.
>> >
>> > I should mention that all three terms have accompanying arm motions
>> > ;)
>> >
>>
>> I just wonder whether the original poster managed to brace himself for
>> the oncoming avalanche....
>>
>> -pd
>>
>> > Hadley
>> >
>> > --
>> > http://had.co.nz/
>> >
>> > ______________________________________________
>> > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
>> > and
>> more, see
>> > https://stat.ethz.ch/mailman/listinfo/r-help
>> > PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> > and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School Solbjerg Plads 3,
>> 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk <javascript:;>  Priv: PDalgd at gmail.com
>> <javascript:;>
>>
>> ______________________________________________
>> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
>> more, see https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>
> --
> Sent from Gmail Mobile
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From matteo.richiardi at gmail.com  Thu Dec 17 01:18:56 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Thu, 17 Dec 2015 00:18:56 +0000
Subject: [R] Applying a function to a matrix using indexes as arguments
Message-ID: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>

I have to evolve each element of a matrix W

W <- matrix(0,2,3)

according to some function which uses the indices of the matrix [i,j] as
arguments:
w.fun = function(i,j) {
  return A[i]*B[j]/(C[i,j])
}

where
A<-c(100,100)
B<-c(200,200,200)
C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)

How can I do it, without recurring to a loop? Also, in my application I
need to pass the function another argument.

Thanks a lot for your suggestions.
Matteo

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Dec 17 01:36:16 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 16 Dec 2015 16:36:16 -0800
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>
References: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>
Message-ID: <732466D9-4052-4FC1-ADBD-C0BF1B369E42@dcn.davis.ca.us>

This calculation divides by values centered around zero.  The only context that I can think of that would require such silliness is a homework problem, and this list has a no-homework policy. If not, then mentioning the theory you are applying might help someone point you at an existing function that achieves your goals while avoiding divide-by-zero errors. 

Since you also posted in HTML I gather that you have not read the Posting Guide mentioned below.  Avoiding HTML on this list is to your benefit,  since using it inevitably leads to others seeing a garbled version of what you sent. Please read the PG for more important guidance. 
-- 
Sent from my phone. Please excuse my brevity.

On December 16, 2015 4:18:56 PM PST, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
>I have to evolve each element of a matrix W
>
>W <- matrix(0,2,3)
>
>according to some function which uses the indices of the matrix [i,j]
>as
>arguments:
>w.fun = function(i,j) {
>  return A[i]*B[j]/(C[i,j])
>}
>
>where
>A<-c(100,100)
>B<-c(200,200,200)
>C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
>
>How can I do it, without recurring to a loop? Also, in my application I
>need to pass the function another argument.
>
>Thanks a lot for your suggestions.
>Matteo
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From matteo.richiardi at gmail.com  Thu Dec 17 01:41:13 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Thu, 17 Dec 2015 00:41:13 +0000
Subject: [R] Applying a function to a matrix using indexes as arguments
Message-ID: <CABSrU1Ji2GU+ntsuP8Y3R6Xo1=N3k-D7fFKm0sAFA5hci-ELbw@mail.gmail.com>

My problem is of course more complicated, and is obviously not a homework.
I just wanted to provide a minimal working example. You can replace the
matrix C with a matrix containing any number, for what matters. Btw,
because numbers are extracted from a Gaussian distribution, the likelihood
that you draw a 0 is actually zero.

Apart from this, apologies for having posted an html version.

On 17 December 2015 at 00:36, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
wrote:

> This calculation divides by values centered around zero. The only context
> that I can think of that would require such silliness is a homework
> problem, and this list has a no-homework policy. If not, then mentioning
> the theory you are applying might help someone point you at an existing
> function that achieves your goals while avoiding divide-by-zero errors.
>
> Since you also posted in HTML I gather that you have not read the Posting
> Guide mentioned below. Avoiding HTML on this list is to your benefit, since
> using it inevitably leads to others seeing a garbled version of what you
> sent. Please read the PG for more important guidance.
> --
> Sent from my phone. Please excuse my brevity.
>
> On December 16, 2015 4:18:56 PM PST, Matteo Richiardi <
> matteo.richiardi at gmail.com> wrote:
>
>> I have to evolve each element of a matrix W
>>
>> W <- matrix(0,2,3)
>>
>> according to some function which uses the indices of the matrix [i,j] as
>> arguments:
>> w.fun = function(i,j) {
>>   return A[i]*B[j]/(C[i,j])
>> }
>>
>> where
>> A<-c(100,100)
>> B<-c(200,200,200)
>> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
>>
>> How can I do it, without recurring to a loop? Also, in my application I
>> need to pass the function another argument.
>>
>> Thanks a lot for your suggestions.
>> Matteo
>>
>>  [[alternative HTML version deleted]]
>>
>> ------------------------------
>>
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal,
>> self-contained, reproducible code.
>>
>>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Thu Dec 17 02:00:52 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 16 Dec 2015 17:00:52 -0800
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <CABSrU1Ji2GU+ntsuP8Y3R6Xo1=N3k-D7fFKm0sAFA5hci-ELbw@mail.gmail.com>
References: <CABSrU1Ji2GU+ntsuP8Y3R6Xo1=N3k-D7fFKm0sAFA5hci-ELbw@mail.gmail.com>
Message-ID: <89A272FB-54A9-4A28-B008-0F2C33F908CF@dcn.davis.ca.us>

Would

outer( A, B, `*` ) / C

do the trick for you? 
-- 
Sent from my phone. Please excuse my brevity.

On December 16, 2015 4:41:13 PM PST, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
>My problem is of course more complicated, and is obviously not a
>homework.
>I just wanted to provide a minimal working example. You can replace the
>matrix C with a matrix containing any number, for what matters. Btw,
>because numbers are extracted from a Gaussian distribution, the
>likelihood
>that you draw a 0 is actually zero.
>
>Apart from this, apologies for having posted an html version.
>
>On 17 December 2015 at 00:36, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> This calculation divides by values centered around zero. The only
>context
>> that I can think of that would require such silliness is a homework
>> problem, and this list has a no-homework policy. If not, then
>mentioning
>> the theory you are applying might help someone point you at an
>existing
>> function that achieves your goals while avoiding divide-by-zero
>errors.
>>
>> Since you also posted in HTML I gather that you have not read the
>Posting
>> Guide mentioned below. Avoiding HTML on this list is to your benefit,
>since
>> using it inevitably leads to others seeing a garbled version of what
>you
>> sent. Please read the PG for more important guidance.
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On December 16, 2015 4:18:56 PM PST, Matteo Richiardi <
>> matteo.richiardi at gmail.com> wrote:
>>
>>> I have to evolve each element of a matrix W
>>>
>>> W <- matrix(0,2,3)
>>>
>>> according to some function which uses the indices of the matrix
>[i,j] as
>>> arguments:
>>> w.fun = function(i,j) {
>>>   return A[i]*B[j]/(C[i,j])
>>> }
>>>
>>> where
>>> A<-c(100,100)
>>> B<-c(200,200,200)
>>> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
>>>
>>> How can I do it, without recurring to a loop? Also, in my
>application I
>>> need to pass the function another argument.
>>>
>>> Thanks a lot for your suggestions.
>>> Matteo
>>>
>>>  [[alternative HTML version deleted]]
>>>
>>> ------------------------------
>>>
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal,
>>> self-contained, reproducible code.
>>>
>>>

	[[alternative HTML version deleted]]


From zhiqiu.hu at gmail.com  Thu Dec 17 02:20:44 2015
From: zhiqiu.hu at gmail.com (Zhiqiu Hu)
Date: Wed, 16 Dec 2015 18:20:44 -0700
Subject: [R] How to bypass the pause in a program invoked using the "system"
	command
Message-ID: <CAJQNJzdzCZVDWm3LCjPxy+9GkipOg++OoQFYbwrPKnGf6XT8fg@mail.gmail.com>

Dear Friends,

I want to run a standalone console program (emBayesB) from R script on
windows console. However, because the program asks user to press the enter
key at each time after the analysis, the R code always stuck after invoking
the emBayesB program with the "system()" function.

For reproduce the issue,  please save the following content as
a "c:\RScript.R" file
##### start ########
AppPath="c:\\bin\\emBayesB_gs.exe"
system(AppPath, wait = TRUE)
##### end ##########

And copy the "emBayesB_gs.exe" file into "c:\\bin". The emBayesB program is
freely accessible at (additional file 3)  the follow website
http://www.biomedcentral.com/1471-2105/11/529/additional

Then, if you try to run the following command on windows console, you will
see the issue.
#####  start of console command  ##########
c:\\R\\bin\\Rscript.exe c:\\RScript.R
#####  end of console command  ##########

FYI, if you submit the "c:\\R\\bin\\Rscript.exe c:\\RScript.R" use the
"system()" function in R (tested with R-studio), you will not required to
press the enter key to finish the run.

Although, the emBayesB program was used in my case, but I suppose it is a
general issue as you call any standalone program that require users
to press key to exit, by run R scripts on windows console .

Any suggestion is appreciated!

Please let me know if you require more information about the issue.

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Dec 17 02:34:01 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 17:34:01 -0800
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>
References: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>
Message-ID: <A40D56E6-C9BF-45A8-B9A6-B418D4B22076@comcast.net>


> On Dec 16, 2015, at 4:18 PM, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
> 
> I have to evolve each element of a matrix W
> 
> W <- matrix(0,2,3)
> 
> according to some function which uses the indices of the matrix [i,j] as
> arguments:
> w.fun = function(i,j) {
>  return A[i]*B[j]/(C[i,j])
> }
> 
> where
> A<-c(100,100)
> B<-c(200,200,200)
> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
> 
> How can I do it, without recurring to a loop? Also, in my application I
> need to pass the function another argument.

mapply( function( i,j,fac) {fac*A[i]*B[j]/C[i,j]},
         i=row(W), 
         j=col(W),
         MoreArgs=list(fac=10) )
[1]  -86207.97  325768.16 -135764.41 -913036.95 -142509.39
[6]  243715.67


N.B. all of the *apply functions are really loops.

-- 
David.
> 
> Thanks a lot for your suggestions.
> Matteo
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Thu Dec 17 02:37:10 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 16 Dec 2015 17:37:10 -0800
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <A40D56E6-C9BF-45A8-B9A6-B418D4B22076@comcast.net>
References: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>
	<A40D56E6-C9BF-45A8-B9A6-B418D4B22076@comcast.net>
Message-ID: <F80E0549-A0E1-4A48-81F9-CF15C9E72296@comcast.net>


> On Dec 16, 2015, at 5:34 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
>> On Dec 16, 2015, at 4:18 PM, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
>> 
>> I have to evolve each element of a matrix W
>> 
>> W <- matrix(0,2,3)
>> 
>> according to some function which uses the indices of the matrix [i,j] as
>> arguments:
>> w.fun = function(i,j) {
>> return A[i]*B[j]/(C[i,j])
>> }
>> 
>> where
>> A<-c(100,100)
>> B<-c(200,200,200)
>> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
>> 
>> How can I do it, without recurring to a loop? Also, in my application I
>> need to pass the function another argument.
> 
> mapply( function( i,j,fac) {fac*A[i]*B[j]/C[i,j]},
>         i=row(W), 
>         j=col(W),
>         MoreArgs=list(fac=10) )
> [1]  -86207.97  325768.16 -135764.41 -913036.95 -142509.39
> [6]  243715.67
> 
> 
> N.B. all of the *apply functions are really loops.

Furthermore, you can neatly populate the W matrix with the `[<-` function:

 W[] <- mapply( function( i,j,fac) {fac*A[i]*B[j]/C[i,j]}, 
                i=row(W), j=col(W),MoreArgs=list(fac=10) )

> W
          [,1]      [,2]      [,3]
[1,] -86207.97 -135764.4 -142509.4
[2,] 325768.16 -913037.0  243715.7

> 


> -- 
> David.
>> 
>> Thanks a lot for your suggestions.
>> Matteo
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Thu Dec 17 03:03:20 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 16 Dec 2015 18:03:20 -0800
Subject: [R] How to bypass the pause in a program invoked using the
	"system"	command
In-Reply-To: <CAJQNJzdzCZVDWm3LCjPxy+9GkipOg++OoQFYbwrPKnGf6XT8fg@mail.gmail.com>
References: <CAJQNJzdzCZVDWm3LCjPxy+9GkipOg++OoQFYbwrPKnGf6XT8fg@mail.gmail.com>
Message-ID: <73ADB90A-F431-486A-BD33-CD6DA7C9261B@dcn.davis.ca.us>

You cannot, in general.  If the program is written to process the stdin input you might be able to feed it input that it expects, but many DOS/Windows console programs use BIOS to directly access the keyboard. 
-- 
Sent from my phone. Please excuse my brevity.

On December 16, 2015 5:20:44 PM PST, Zhiqiu Hu <zhiqiu.hu at gmail.com> wrote:
>Dear Friends,
>
>I want to run a standalone console program (emBayesB) from R script on
>windows console. However, because the program asks user to press the
>enter
>key at each time after the analysis, the R code always stuck after
>invoking
>the emBayesB program with the "system()" function.
>
>For reproduce the issue,  please save the following content as
>a "c:\RScript.R" file
>##### start ########
>AppPath="c:\\bin\\emBayesB_gs.exe"
>system(AppPath, wait = TRUE)
>##### end ##########
>
>And copy the "emBayesB_gs.exe" file into "c:\\bin". The emBayesB
>program is
>freely accessible at (additional file 3)  the follow website
>http://www.biomedcentral.com/1471-2105/11/529/additional
>
>Then, if you try to run the following command on windows console, you
>will
>see the issue.
>#####  start of console command  ##########
>c:\\R\\bin\\Rscript.exe c:\\RScript.R
>#####  end of console command  ##########
>
>FYI, if you submit the "c:\\R\\bin\\Rscript.exe c:\\RScript.R" use the
>"system()" function in R (tested with R-studio), you will not required
>to
>press the enter key to finish the run.
>
>Although, the emBayesB program was used in my case, but I suppose it is
>a
>general issue as you call any standalone program that require users
>to press key to exit, by run R scripts on windows console .
>
>Any suggestion is appreciated!
>
>Please let me know if you require more information about the issue.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jwd at surewest.net  Thu Dec 17 05:52:26 2015
From: jwd at surewest.net (jwd)
Date: Wed, 16 Dec 2015 20:52:26 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
References: <CA+djRJk=ewRVOibB6PMNjerB9Mnp5_dmshSVXZ7TPpspqLg2VA@mail.gmail.com>
	<4E1F492D-8C4C-4512-805E-A4F25ED4030C@comcast.net>
	<6D2781A7-B78A-4DD2-9DE8-8B9E2610941A@gmail.com>
	<CA+djRJnWjrwJ_wbvBG_man_-mbmSGD-CNYwJRvsnAO=2=S2hSg@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403C97B1571@GBTEDVPEXCMB04.corp.lgc-group.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<CABdHhvEib2tJL4R_D-UER7zCAuPSiF=Mjo3F3JNAMxdZxfqMnQ@mail.gmail.com>
Message-ID: <20151216205226.6b5e6ff9@Draco.site>

On Wed, 16 Dec 2015 09:34:20 -0600
Hadley Wickham <h.wickham at gmail.com> wrote:

> On Tue, Dec 15, 2015 at 9:55 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
> >
> >
> >    [............]
> >
> >     > You are missing the closing bracket on the boxplot()
> >     > command.  Just finish with a ')'
> >
> > Hmm... I once learned
> >
> >  '()' =: parenthesis/es
> >  '[]' =: bracket(s)
> >  '{}' =: brace(s)
> >
That is correct, though I've generally heard braces heard called
"curly braces."

JWDougherty


From jwd at surewest.net  Thu Dec 17 06:01:08 2015
From: jwd at surewest.net (jwd)
Date: Wed, 16 Dec 2015 21:01:08 -0800
Subject: [R] Make a box-whiskers plot in R with 5 variables, color coded.
In-Reply-To: <5E8CCF30F16.00000F47jrkrideau@inbox.com>
References: <8323b1f1-af60-470c-9c06-ba77412bde0b@comcast.net>
	<1a8c1289955ef649a09086a153e2672403c97b1571@gbtedvpexcmb04.corp.lgc-group.com>
	<ca+djrjnwjrwj_wbvbg_man_-mbmsgd-cnywjrvsnao=2=s2hsg@mail.gmail.com>
	<6d2781a7-b78a-4dd2-9de8-8b9e2610941a@gmail.com>
	<22128.14323.686365.992339@stat.math.ethz.ch>
	<alpine.lrh.2.20.1512150852440.7316@aeolus.ecy.wa.gov>
	<567011ca020000cb0014368b@smtp.medicine.umaryland.edu>
	<4e1f492d-8c4c-4512-805e-a4f25ed4030c@comcast.net>
	<56700c88020000cb00143662@smtp.medicine.umaryland.edu>
	<ca+djrjk=ewrvoibb6pmnjerb9mnp5_dmshsvxz7tppspqlg2va@mail.gmail.com>
	<1acb40fc-6765-452a-803d-1072b997f370@xs4all.nl>
	<5E8CCF30F16.00000F47jrkrideau@inbox.com>
Message-ID: <20151216210108.67341f65@Draco.site>

On Wed, 16 Dec 2015 07:16:21 -0800
John Kane <jrkrideau at inbox.com> wrote:

...
> 
> I have lived next door to the USA for most of my life and never
> realized that American usage is 'brackets' for  [ ] .  I would use
> the term brackets in normal use for ( ) and "square brackets for [ ]. 
> ...

There's a lot of fog in the air.  Properly, the use in the USA is as
described by Martin.  But ever since the late '60s there has been a
de-emphasis of linguistic precision and a tendency to "good enough."
Sad.

JWDougherty


From henrik.bengtsson at gmail.com  Thu Dec 17 06:58:31 2015
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Wed, 16 Dec 2015 21:58:31 -0800
Subject: [R] close specific graphics device
In-Reply-To: <CA+8X3fWBZzW1aJWqGs_L2PkgehxemvF3UPC9Uqvaz0Cn9PuX4Q@mail.gmail.com>
References: <CAJeYpE_gyGjcZCaifpaq2ix5gpsWCJJsp4mM77LqK+HH-aNtuw@mail.gmail.com>
	<CA+8X3fWBZzW1aJWqGs_L2PkgehxemvF3UPC9Uqvaz0Cn9PuX4Q@mail.gmail.com>
Message-ID: <CAFDcVCS2Mzt62vk1+qHbR1DbhXGvaX64Or9TfhnQTPbFfUK03Q@mail.gmail.com>

The R.devices package provides functions for this.  For instance, you
can open several devices with different labels and the close then in
whatever order you'd like:

> library("R.devices")
> devSet("foo")
> plot(1:10)
> devSet("bar")
> plot(10:1)
> devSet("foo")
> points(10:1)
> devSet("bar")
> devOff("foo")
> devOff("bar")

Alternatively, you can specify the 'label' argument when you use devNew(), e.g.

> devNew("x11", label="foo")
> plot(1:10)
> devNew("png", filename="myplot.png", label="bar")
> plot(10:1)
> devOff("foo")
> devOff("bar")

The R.devices package also allows you to open a device with any index
number in [2,63], e.g.

> devSet(43)

regardless of whether devices 2 to 42 already exists or not.

Also, if you don't already know, R.devices provides devEval(), e.g.

devEval("png", name="myplot", {
  plot(10:1)
})

which guarantees that the device is closed afterward, i.e. no more
forgetting to use dev.off().  Also, filename extensions etc are
automatically taken care of.  You can plot to multiple image types at
the same time, e.g.

devEval(c("png", "pdf", "eps"), name="myplot", aspectRatio=2/3, {
  plot(10:1)
})

There are also "quick" functions such as:

toPNG("myplot", aspectRatio=2/3, {
  plot(10:1)
})

Hope this helps

Henrik

PS. R.devices 2.13.2 is rolling out on CRAN right now - make sure to
use that version if your doing _unbalanced_ opening/closing with
labels as in my first example.

On Tue, Dec 15, 2015 at 2:38 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
> Hi Dan,
> The range of device numbers seems to be 1-63. There doesn't appear to be a
> means of explicitly setting the device number when calling dev.new, and
> devices are numbered sequentially when they are opened. This means that
> even if you did know that the device number was, say, 4 it would be
> possible to close that device and open another device with the number 4.
>
> I suppose it would be possible to write wrapper functions for this, but I
> have to leave at the moment, so perhaps tomorrow.
>
> Jim
>
> On Wed, Dec 16, 2015 at 7:51 AM, Dalthorp, Daniel <ddalthorp at usgs.gov>
> wrote:
>
>> dev.off(which) can be used to close a specific graphics device where
>> "which" is the index of the device, but is there a way to assign a custom
>> number (or name) to a windows device so that specific window can be later
>> closed via dev.off (or some other method) if it is open?
>>
>> The following does NOT work. The target device is not open when its dev.off
>> is called, and another window that later got assigned the original index
>> associated with the target device is closed instead.
>>
>> plot(0,0,type='n') # target window to close
>> text(0,0,"close me")
>> targetindex<-dev.cur()
>>
>> # unbeknownst to the programmer, user closes device by clicking the red "X"
>> or...
>> dev.off()
>>
>> # user draws a new graph that he wants to keep open
>> plot(1,1,type='n')
>> text(1,1,"do not close me")
>>
>> # now it's time for the program to close the original graphics device (if
>> it still happens to be open)
>> dev.off(targetindex)
>>
>> # the wrong device has been closed because the original window had closed
>> and the index associated with original graph is now associated with
>> something else
>>
>> ----
>>
>> I'm looking for something like:
>>
>> dev.off(which = "original figure") or dev.off(which = n), where n is a
>> custom index (like 10000) that will not be later assigned to a different
>> device [unless explicitly assigned that index].
>>
>> Any help would be greatly appreciated.
>>
>> Thanks!
>>
>>
>>
>> --
>> Dan Dalthorp, PhD
>> USGS Forest and Rangeland Ecosystem Science Center
>> Forest Sciences Lab, Rm 189
>> 3200 SW Jefferson Way
>> Corvallis, OR 97331
>> ph: 541-750-0953
>> ddalthorp at usgs.gov
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Thu Dec 17 09:39:29 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 17 Dec 2015 09:39:29 +0100
Subject: [R] unable to download library
In-Reply-To: <AB8D6DD9-65C0-476E-B038-96B32C95272F@comcast.net>
References: <39592E5347604E8C8E3AE30159A1CE8B@JohnKaronHP>
	<5671DE83.4070401@auckland.ac.nz>
	<5F1733E3-53BF-467A-A640-0123D0275C7F@comcast.net>
	<F3224F05-1AB6-437F-BBE6-F16434E9F845@gmail.com>
	<AB8D6DD9-65C0-476E-B038-96B32C95272F@comcast.net>
Message-ID: <93EC462A-DC01-41BD-8DF3-7C51981B5D7B@gmail.com>


> On 17 Dec 2015, at 01:05 , David Winsemius <dwinsemius at comcast.net> wrote:
> 
>> 
>> On Dec 16, 2015, at 3:44 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>> 
>> 
>>> On 17 Dec 2015, at 00:00 , David Winsemius <dwinsemius at comcast.net> wrote:
>>> 
>>> When I attempt to open that with Chrome I get an error:
>>> 
>>> This webpage is not available
>>> 
>>> DNS_PROBE_FINISHED_NXDOMAIN
>>> 
>>> Like Bert, I also get an error report when I attempt to us the Berkeley repo, but I do not get an error when I use the CRAN repo in Switzerland.
>> 
>> That'll be a name server issue. Nothing to do with R per se. If it worked for some and not for others for a while, it is most likely due to cached data.
>> 
>> pd$ host cran.cnr.Berkeley.edu
>> Host cran.cnr.Berkeley.edu not found: 3(NXDOMAIN)
>> 
>> pd$ host cnr.Berkeley.edu
>> cnr.Berkeley.edu has address 169.229.201.201
> 
> I get 
> 
> davidwinsemius$ host cran.cnr.Berkeley.edu
> cran.cnr.Berkeley.edu is an alias for nature.Berkeley.edu.
> nature.Berkeley.edu has address 169.229.201.201
> nature.Berkeley.edu has IPv6 address 2607:f140:0:8000::201
> nature.Berkeley.edu mail is handled by 5 mx1.ist.Berkeley.edu.
> nature.Berkeley.edu mail is handled by 5 mx0.ist.Berkeley.edu.
> 
> davidwinsemius$ host cnr.Berkeley.edu
> cnr.Berkeley.edu has address 169.229.201.201
> 
> 
> When I attempt to open cnr.Berkeley.edu in a browser I get the welcome page from Berkeley's College of Natural Resources ... after redirection to https://nature.berkeley.edu/
> 
> When I try trimming the 'cran' from the original link:
> 
> https://cran.cnr.berkeley.edu/bin/windows/contrib/3.2
> 
> I get page not found from https://nature.berkeley.edu/bin/windows/contrib/3.2
> 
> So, how do we determine whether Bert's and my DNS servers have changed more recently (since we are both much closer to the Berkeley campus and perhaps Rolf just has an old ip address), or that Rolf has connection to a more uptodate DNS server? As with Bert my network-fu is inadequate to resolve this
> 

Dunno, but mine has changed overnight:

$ host cran.cnr.Berkeley.edu
cran.cnr.Berkeley.edu is an alias for nature.Berkeley.edu.
nature.Berkeley.edu has address 169.229.201.201
nature.Berkeley.edu has IPv6 address 2607:f140:0:8000::201
nature.Berkeley.edu mail is handled by 5 mx0.ist.Berkeley.edu.
nature.Berkeley.edu mail is handled by 5 mx1.ist.Berkeley.edu.

and the web page at

https://cran.cnr.berkeley.edu/bin/windows/contrib/3.2

works now. So just apply the most basic network-fu: Sit back and let the admins sort it out, then wait for cache updates...

-pd

> -- 
> David
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
> 
> David Winsemius
> Alameda, CA, USA

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From matteo.richiardi at gmail.com  Thu Dec 17 12:00:09 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Thu, 17 Dec 2015 11:00:09 +0000
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <F80E0549-A0E1-4A48-81F9-CF15C9E72296@comcast.net>
References: <CABSrU1LbgrY6HJp0KJ0irHVPMQHLJdD-o=dayuwDMVpiWLERtw@mail.gmail.com>
	<A40D56E6-C9BF-45A8-B9A6-B418D4B22076@comcast.net>
	<F80E0549-A0E1-4A48-81F9-CF15C9E72296@comcast.net>
Message-ID: <CABSrU1+WovoaePe7sHC6iBy0_SdFrA7-ovjtUPFn8s-Ewn3xcQ@mail.gmail.com>

Hi David,
that's a great answer, thanks so much. I imagined apply() was involved in
the solution, but I was unable to find how myself. Thanks again.
Matteo

On 17 December 2015 at 01:37, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Dec 16, 2015, at 5:34 PM, David Winsemius <dwinsemius at comcast.net>
> wrote:
> >
> >
> >> On Dec 16, 2015, at 4:18 PM, Matteo Richiardi <
> matteo.richiardi at gmail.com> wrote:
> >>
> >> I have to evolve each element of a matrix W
> >>
> >> W <- matrix(0,2,3)
> >>
> >> according to some function which uses the indices of the matrix [i,j] as
> >> arguments:
> >> w.fun = function(i,j) {
> >> return A[i]*B[j]/(C[i,j])
> >> }
> >>
> >> where
> >> A<-c(100,100)
> >> B<-c(200,200,200)
> >> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
> >>
> >> How can I do it, without recurring to a loop? Also, in my application I
> >> need to pass the function another argument.
> >
> > mapply( function( i,j,fac) {fac*A[i]*B[j]/C[i,j]},
> >         i=row(W),
> >         j=col(W),
> >         MoreArgs=list(fac=10) )
> > [1]  -86207.97  325768.16 -135764.41 -913036.95 -142509.39
> > [6]  243715.67
> >
> >
> > N.B. all of the *apply functions are really loops.
>
> Furthermore, you can neatly populate the W matrix with the `[<-` function:
>
>  W[] <- mapply( function( i,j,fac) {fac*A[i]*B[j]/C[i,j]},
>                 i=row(W), j=col(W),MoreArgs=list(fac=10) )
>
> > W
>           [,1]      [,2]      [,3]
> [1,] -86207.97 -135764.4 -142509.4
> [2,] 325768.16 -913037.0  243715.7
>
> >
>
>
> > --
> > David.
> >>
> >> Thanks a lot for your suggestions.
> >> Matteo
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >
> > David Winsemius
> > Alameda, CA, USA
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Thu Dec 17 12:23:44 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Thu, 17 Dec 2015 22:23:44 +1100
Subject: [R] NIPALS & categorical variables
In-Reply-To: <AMSPR03MB16150A3006E45892C40724585EF0@AMSPR03MB161.eurprd03.prod.outlook.com>
References: <AMSPR03MB16150A3006E45892C40724585EF0@AMSPR03MB161.eurprd03.prod.outlook.com>
Message-ID: <CA+8X3fXqp9GvMWeDjVyiAR7VHdpNS8pRywB5Jy-ReLjZKpkgzw@mail.gmail.com>

Hi Hana,
If you are using nipals from the plsdepot package, the answer seems to be
maybe. If your categorical variables are at least ordinal level, and you
can arrange the values so that the factor numbering reflects this, it may
be possible to use the numeric values.

"...the variables are assumed to be quantitative, ideally measured on a
more or less continuous scale."

(from http://gastonsanchez.com/plsdepot_nipals.pdf). Obviously you would be
seriously stretching the assumptions of the method, but you will have to
decide whether that is okay.

Jim


On Wed, Dec 16, 2015 at 9:10 PM, Hana Mandov? <pmhm at leeds.ac.uk> wrote:

> Hello,
>
> Big help is needed. Can you use nipals for a dataset that contains both
> categorical as well as numerical variables?
> I used nipals instead of pca as some of my numerical variables contain
> missing values, but my categorical variables are complete.
>
> Many many thanks for your help, I've been stuck on this for ages.
>
> Hana
>
> Hana Mandov?
> PhD Researcher
> EPSRC Centre for Doctoral Training in Bioenergy
> Faculty of Engineering
> University of Leeds
> Leeds LS2 9JT
> United Kingdom
> E: pmhm at leeds.ac.uk<mailto:pmhm at leeds.ac.uk>
> W: www.engineering.leeds.ac.uk/bioenergy/<
> http://www.engineering.leeds.ac.uk/bioenergy/>
> P Help save paper - every piece counts! Do you really need to print this
> email?
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From highstat at highstat.com  Thu Dec 17 13:24:56 2015
From: highstat at highstat.com (Highland Statistics Ltd)
Date: Thu, 17 Dec 2015 08:24:56 -0400
Subject: [R] Course: Introduction to GAM and GAMM
Message-ID: <5672A998.5050505@highstat.com>

We would like to announce the following statistics course:

Course: Introduction to GAM and GAMM
Where:  University of Konstanz, Konstanz, Germany
When:   7-11 March 2016

Course website: http://www.highstat.com/statscourse.htm
Course flyer: http://highstat.com/Courses/Flyer2016_03Konstanz.pdf



Kind regards,

Alain Zuur

-- 
Dr. Alain F. Zuur

First author of:
1. Beginner's Guide to GAMM with R (2014).
2. Beginner's Guide to GLM and GLMM with R (2013).
3. Beginner's Guide to GAM with R (2012).
4. Zero Inflated Models and GLMM with R (2012).
5. A Beginner's Guide to R (2009).
6. Mixed effects models and extensions in ecology with R (2009).
7. Analysing Ecological Data (2007).

Highland Statistics Ltd.
9 St Clair Wynd
UK - AB41 6DZ Newburgh
Tel:   0044 1358 788177
Email: highstat at highstat.com
URL:   www.highstat.com


From manishm at dbs.com  Thu Dec 17 10:36:47 2015
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Thu, 17 Dec 2015 09:36:47 +0000
Subject: [R] Variable Selection for Logistic Regression
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08D89256@W01GMAILDAGA52.reg1.1bank.dbs.com>

Hi,

I have a dataset with approx 400K Rows and 900 columns with a single dependent variable of 0/1 flag. The independent variables are both categorical and numerical. I have looked as SO/Cross Validated Posts but couldn't get an answer for this.

Since I cannot try all possible combinations of variables or even attempt single model with all 900 columns, I am planning to create independent models of each variable using something like below -

out = NULL
xnames = colnames(train)[!colnames(train) %in% ignoredcols]
for (f in xnames) {
    glmm = glm(train$conversion_flag ~ train[,f] - 1 , family = binomial)
    out = rbind.fill(out,as.data.frame(cbind(f,fmsb::NagelkerkeR2(glmm)[2]$R2)))
    out = rbind.fill(out,as.data.frame(cbind(f,'AIC',summary(glmm)$aic)))
}

This will give me the individual AIC and pseudo R2 for each of the variables. Post that I plan to select the variables with the best scores for both AIC and pseudoR2. Does this approach make sense?

I obviously will use a nfold cross validation in the final model to ensure accuracy and avoid over fitting. However before I reach that I plan to use the above to select which variables to use.

Thanks,
Manish
CONFIDENTIAL NOTE:
The information contained in this email is intended only...{{dropped:11}}


From brunu_higa at hotmail.com  Thu Dec 17 12:30:10 2015
From: brunu_higa at hotmail.com (bruno higa)
Date: Thu, 17 Dec 2015 09:30:10 -0200
Subject: [R] Stop the notices about the r-forum
Message-ID: <SNT150-W23033554636C54713F92C9E9E00@phx.gbl>

Hey, i cant stop the emails about the forum, i delete my account and i want it. Can you help me ? Att.  		 	   		  
	[[alternative HTML version deleted]]


From G.Maubach at weinwolf.de  Thu Dec 17 13:50:29 2015
From: G.Maubach at weinwolf.de (G.Maubach at weinwolf.de)
Date: Thu, 17 Dec 2015 13:50:29 +0100
Subject: [R] Filtering Cases with != NA
Message-ID: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>

Dear All,

I am new to "R" and search for a solution to exclude cases if a certain 
variable contains NA for a case.

Example

No Name Turnover
1 Smith 1500
2 Mayor 200
3 Miller 
4 Batic 750

I would like to create a subset excluding case 3 Miller NA.

I tried to following:

new_dataset <- subset(dataset, subset = Turnover != NA)

This does not work. The new_dataset contains all variables but not cases 
are left. R responds "Variables with all observations missing".

How could I do it right?

Kind regards

Georg


	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Thu Dec 17 16:31:14 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 17 Dec 2015 10:31:14 -0500
Subject: [R] Stop the notices about the r-forum
In-Reply-To: <SNT150-W23033554636C54713F92C9E9E00@phx.gbl>
References: <SNT150-W23033554636C54713F92C9E9E00@phx.gbl>
Message-ID: <CAM_vjukwO=u6sLT2qQ7vTBibOopSYo6nW=QUR_M=+qY4m6Vm2g@mail.gmail.com>

On Thu, Dec 17, 2015 at 6:30 AM, bruno higa <brunu_higa at hotmail.com> wrote:
> Hey, i cant stop the emails about the forum, i delete my account and i want it. Can you help me ? Att.
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Yep, right there. On all those emails you've been getting.


From r at catwhisker.org  Thu Dec 17 16:33:30 2015
From: r at catwhisker.org (David Wolfskill)
Date: Thu, 17 Dec 2015 07:33:30 -0800
Subject: [R] Filtering Cases with != NA
In-Reply-To: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>
References: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>
Message-ID: <20151217153330.GU1173@albert.catwhisker.org>

On Thu, Dec 17, 2015 at 01:50:29PM +0100, G.Maubach at weinwolf.de wrote:
> Dear All,
> 
> I am new to "R" and search for a solution to exclude cases if a certain 
> variable contains NA for a case.
> ... 
> I would like to create a subset excluding case 3 Miller NA.
> 
> I tried to following:
> 
> new_dataset <- subset(dataset, subset = Turnover != NA)
> 
> This does not work. The new_dataset contains all variables but not cases 
> are left. R responds "Variables with all observations missing".
> 
> How could I do it right?
> ....

Please review the documentation for na.omit() (and related functions.

Peace,
david
-- 
David H. Wolfskill				r at catwhisker.org
Those who would murder in the name of God or prophet are blasphemous cowards.

See http://www.catwhisker.org/~david/publickey.gpg for my public key.
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 603 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151217/14e53d97/attachment.bin>

From sarah.goslee at gmail.com  Thu Dec 17 16:34:16 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Thu, 17 Dec 2015 10:34:16 -0500
Subject: [R] Filtering Cases with != NA
In-Reply-To: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>
References: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>
Message-ID: <CAM_vju=OyBzmDiapYnN6ohkFW8QsCVEFYJrEAPp0YiCqYLuZaA@mail.gmail.com>

You need the is.na() function:

> dataset <- data.frame(No = 1:4, Name = c("Smith", "Mayor", "Miller", "Baltic"), Turnover = c(1500, 200, NA, 750))
> dataset
  No   Name Turnover
1  1  Smith     1500
2  2  Mayor      200
3  3 Miller       NA
4  4 Baltic      750

[1]  TRUE  TRUE FALSE  TRUE
> dataset[complete.cases(dataset), ]
  No   Name Turnover
1  1  Smith     1500
2  2  Mayor      200
4  4 Baltic      750

> dataset[!is.na(dataset$Turnover), ]
  No   Name Turnover
1  1  Smith     1500
2  2  Mayor      200
4  4 Baltic      750

> subset(dataset, !is.na(Turnover))
  No   Name Turnover
1  1  Smith     1500
2  2  Mayor      200
4  4 Baltic      750



On Thu, Dec 17, 2015 at 7:50 AM,  <G.Maubach at weinwolf.de> wrote:
> Dear All,
>
> I am new to "R" and search for a solution to exclude cases if a certain
> variable contains NA for a case.
>
> Example
>
> No Name Turnover
> 1 Smith 1500
> 2 Mayor 200
> 3 Miller
> 4 Batic 750
>
> I would like to create a subset excluding case 3 Miller NA.
>
> I tried to following:
>
> new_dataset <- subset(dataset, subset = Turnover != NA)
>
> This does not work. The new_dataset contains all variables but not cases
> are left. R responds "Variables with all observations missing".
>
> How could I do it right?
>
> Kind regards
>
> Georg
>


From jholtman at gmail.com  Thu Dec 17 16:34:57 2015
From: jholtman at gmail.com (jim holtman)
Date: Thu, 17 Dec 2015 10:34:57 -0500
Subject: [R] Filtering Cases with != NA
In-Reply-To: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>
References: <OF48A166A3.43418EA9-ONC1257F1E.0045FD44-C1257F1E.00468E19@lotus.hawesko.de>
Message-ID: <CAAxdm-5Jinx5ZFfeniCX6ZkK5vU4HNss9Tb2rZoQkexsPq--0g@mail.gmail.com>

use the 'is.na' function:

new_dataset <- subset(dataset, subset = !is.na(Turnover))


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Thu, Dec 17, 2015 at 7:50 AM, <G.Maubach at weinwolf.de> wrote:

> Dear All,
>
> I am new to "R" and search for a solution to exclude cases if a certain
> variable contains NA for a case.
>
> Example
>
> No Name Turnover
> 1 Smith 1500
> 2 Mayor 200
> 3 Miller
> 4 Batic 750
>
> I would like to create a subset excluding case 3 Miller NA.
>
> I tried to following:
>
> new_dataset <- subset(dataset, subset = Turnover != NA)
>
> This does not work. The new_dataset contains all variables but not cases
> are left. R responds "Variables with all observations missing".
>
> How could I do it right?
>
> Kind regards
>
> Georg
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From yixuan.qiu at cos.name  Thu Dec 17 16:42:50 2015
From: yixuan.qiu at cos.name (Yixuan Qiu)
Date: Thu, 17 Dec 2015 10:42:50 -0500
Subject: [R] Variable Selection for Logistic Regression
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08D89256@W01GMAILDAGA52.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08D89256@W01GMAILDAGA52.reg1.1bank.dbs.com>
Message-ID: <CAFr_7yFgo0F6x_tdk-faJ0Yx+CJPmtLS=iHXZf4VaaTuFE0SaQ@mail.gmail.com>

Hello Manish,

In my point of view, the "marginal" selection of variables can be
misleading since it ignores the correlation between independent
variables. I would suggest using some modern variable selection
techniques such as Lasso-typed methods.

One method that directly fits your need is the L1-regularized logistic
regression implemented by the glmnet package. Here is a vignette that
should be helpful for you:
http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#log


Best,
Yixuan


2015-12-17 4:36 GMT-05:00 Manish MAHESHWARI <manishm at dbs.com>:
> Hi,
>
> I have a dataset with approx 400K Rows and 900 columns with a single dependent variable of 0/1 flag. The independent variables are both categorical and numerical. I have looked as SO/Cross Validated Posts but couldn't get an answer for this.
>
> Since I cannot try all possible combinations of variables or even attempt single model with all 900 columns, I am planning to create independent models of each variable using something like below -
>
> out = NULL
> xnames = colnames(train)[!colnames(train) %in% ignoredcols]
> for (f in xnames) {
>     glmm = glm(train$conversion_flag ~ train[,f] - 1 , family = binomial)
>     out = rbind.fill(out,as.data.frame(cbind(f,fmsb::NagelkerkeR2(glmm)[2]$R2)))
>     out = rbind.fill(out,as.data.frame(cbind(f,'AIC',summary(glmm)$aic)))
> }
>
> This will give me the individual AIC and pseudo R2 for each of the variables. Post that I plan to select the variables with the best scores for both AIC and pseudoR2. Does this approach make sense?
>
> I obviously will use a nfold cross validation in the final model to ensure accuracy and avoid over fitting. However before I reach that I plan to use the above to select which variables to use.
>
> Thanks,
> Manish
> CONFIDENTIAL NOTE:
> The information contained in this email is intended on...{{dropped:15}}


From dcarlson at tamu.edu  Thu Dec 17 17:38:17 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Thu, 17 Dec 2015 16:38:17 +0000
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <89A272FB-54A9-4A28-B008-0F2C33F908CF@dcn.davis.ca.us>
References: <CABSrU1Ji2GU+ntsuP8Y3R6Xo1=N3k-D7fFKm0sAFA5hci-ELbw@mail.gmail.com>
	<89A272FB-54A9-4A28-B008-0F2C33F908CF@dcn.davis.ca.us>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6EA46C@mb02.ads.tamu.edu>

Also

A %*% t(B) / C


Which works because when a vector is converted to a matrix, it becomes a 1-column matrix. The documentation for t() points this out but there is a typo:

"When x is a vector, it is treated as a column, i.e., the result is a 1-row matrix."

Should be a "1-column matrix"

> as.matrix(A)
     [,1]
[1,]  100
[2,]  200

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jeff Newmiller
Sent: Wednesday, December 16, 2015 7:01 PM
To: Matteo Richiardi; r-help at r-project.org
Subject: Re: [R] Applying a function to a matrix using indexes as arguments

Would

outer( A, B, `*` ) / C

do the trick for you? 
-- 
Sent from my phone. Please excuse my brevity.

On December 16, 2015 4:41:13 PM PST, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
>My problem is of course more complicated, and is obviously not a
>homework.
>I just wanted to provide a minimal working example. You can replace the
>matrix C with a matrix containing any number, for what matters. Btw,
>because numbers are extracted from a Gaussian distribution, the
>likelihood
>that you draw a 0 is actually zero.
>
>Apart from this, apologies for having posted an html version.
>
>On 17 December 2015 at 00:36, Jeff Newmiller <jdnewmil at dcn.davis.ca.us>
>wrote:
>
>> This calculation divides by values centered around zero. The only
>context
>> that I can think of that would require such silliness is a homework
>> problem, and this list has a no-homework policy. If not, then
>mentioning
>> the theory you are applying might help someone point you at an
>existing
>> function that achieves your goals while avoiding divide-by-zero
>errors.
>>
>> Since you also posted in HTML I gather that you have not read the
>Posting
>> Guide mentioned below. Avoiding HTML on this list is to your benefit,
>since
>> using it inevitably leads to others seeing a garbled version of what
>you
>> sent. Please read the PG for more important guidance.
>> --
>> Sent from my phone. Please excuse my brevity.
>>
>> On December 16, 2015 4:18:56 PM PST, Matteo Richiardi <
>> matteo.richiardi at gmail.com> wrote:
>>
>>> I have to evolve each element of a matrix W
>>>
>>> W <- matrix(0,2,3)
>>>
>>> according to some function which uses the indices of the matrix
>[i,j] as
>>> arguments:
>>> w.fun = function(i,j) {
>>>   return A[i]*B[j]/(C[i,j])
>>> }
>>>
>>> where
>>> A<-c(100,100)
>>> B<-c(200,200,200)
>>> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
>>>
>>> How can I do it, without recurring to a loop? Also, in my
>application I
>>> need to pass the function another argument.
>>>
>>> Thanks a lot for your suggestions.
>>> Matteo
>>>
>>>  [[alternative HTML version deleted]]
>>>
>>> ------------------------------
>>>
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal,
>>> self-contained, reproducible code.
>>>
>>>

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec 17 18:54:22 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 17 Dec 2015 09:54:22 -0800
Subject: [R] Applying a function to a matrix using indexes as arguments
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6EA46C@mb02.ads.tamu.edu>
References: <CABSrU1Ji2GU+ntsuP8Y3R6Xo1=N3k-D7fFKm0sAFA5hci-ELbw@mail.gmail.com>
	<89A272FB-54A9-4A28-B008-0F2C33F908CF@dcn.davis.ca.us>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6EA46C@mb02.ads.tamu.edu>
Message-ID: <24C130FA-B9A7-49AD-B295-A5470E124858@comcast.net>


> On Dec 17, 2015, at 8:38 AM, David L Carlson <dcarlson at tamu.edu> wrote:
> 
> Also
> 
> A %*% t(B) / C
> 

Which would be equivalent to:

> outer(A, B)/C
           [,1]      [,2]       [,3]
[1,]   10863.03 -14390.53 -287691.41
[2,] -127580.06 -13576.77   83597.56

And perhaps even more efficiently: tcrossprod(A,B)/C

The OP will need to determine whether the "other argument" he mentioned can be applied to this result in a similarly "vectorized" manner. The reason that I chose  `mapply` was that its `MoreArgs`-parameter provided an explicit mechanism for passing a further argument list to the function inside the implicit loop.

-- 
David.
> 
> Which works because when a vector is converted to a matrix, it becomes a 1-column matrix. The documentation for t() points this out but there is a typo:
> 
> "When x is a vector, it is treated as a column, i.e., the result is a 1-row matrix."
> 
> Should be a "1-column matrix"
> 
>> as.matrix(A)
>     [,1]
> [1,]  100
> [2,]  200
> 
> -------------------------------------
> David L Carlson
> Department of Anthropology
> Texas A&M University
> College Station, TX 77840-4352
> 
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Jeff Newmiller
> Sent: Wednesday, December 16, 2015 7:01 PM
> To: Matteo Richiardi; r-help at r-project.org
> Subject: Re: [R] Applying a function to a matrix using indexes as arguments
> 
> Would
> 
> outer( A, B, `*` ) / C
> 
> do the trick for you? 
> -- 
> Sent from my phone. Please excuse my brevity.
> 
> On December 16, 2015 4:41:13 PM PST, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
>> My problem is of course more complicated, and is obviously not a
>> homework.
>> I just wanted to provide a minimal working example. You can replace the
>> matrix C with a matrix containing any number, for what matters. Btw,
>> because numbers are extracted from a Gaussian distribution, the
>> likelihood
>> that you draw a 0 is actually zero.
>> 
>> Apart from this, apologies for having posted an html version.
>> 
> snipped 
>>> 
>>> On December 16, 2015 4:18:56 PM PST, Matteo Richiardi <
>>> matteo.richiardi at gmail.com> wrote:
>>> 
>>>> I have to evolve each element of a matrix W
>>>> 
>>>> W <- matrix(0,2,3)
>>>> 
>>>> according to some function which uses the indices of the matrix
>> [i,j] as
>>>> arguments:
>>>> w.fun = function(i,j) {
>>>>  return A[i]*B[j]/(C[i,j])
>>>> }
>>>> 
>>>> where
>>>> A<-c(100,100)
>>>> B<-c(200,200,200)
>>>> C <- matrix( rnorm(6,mean=0,sd=1), 2, 3)
>>>> 
>>>> How can I do it, without recurring to a loop? Also, in my
>> application I
>>>> need to pass the function another argument.
>>>> 
>>>> Thanks a lot for your suggestions.
>>>> Matteo
>>>> 


David Winsemius
Alameda, CA, USA


From fahad.usman at openreach.co.uk  Thu Dec 17 16:24:20 2015
From: fahad.usman at openreach.co.uk (fahad.usman at openreach.co.uk)
Date: Thu, 17 Dec 2015 15:24:20 +0000
Subject: [R] Gather columns based on multiple columns using tidyr
Message-ID: <9237949d508e4c8583ab4e17f7f63535@tpw09926dag11f.domain1.systemhost.net>

Hi,

Sorry for this direct approach but I am stuck with a stupid data that I would like to reformat.

The datafile is location at: fileURL <- http://www.metoffice.gov.uk/pub/data/weather/uk/climate/datasets/Rainfall/ranked/Scotland.txt

You can read the data by:

if(!file.exists("scotland_rainfall.txt")){
        #this will download the file in the current working directory
        download.file(fileURL,destfile = "scotland_rainfall.txt")
        dateDownload <- Sys.Date() #15-12-2015

}

> head(scotland_weather)
    Jan Year.1   Feb Year.2   Mar Year.3   Apr Year.4   May Year.5   Jun Year.6   Jul Year.7   Aug Year.8   Sep Year.9   Oct Year.10   Nov Year.11   Dec Year.12
1 293.8   1993 278.1   1990 238.5   1994 191.1   1947 191.4   2011 155.0   1938 185.6   1940 216.5   1985 267.6   1950 258.1    1935 262.0    2009 300.7    2013
2 292.2   1928 258.8   1997 233.4   1990 149.0   1910 168.7   1986 137.9   2002 181.4   1988 211.9   1992 221.2   1981 254.0    1954 245.3    2015 268.5    1986
3 275.6   2008 244.7   2002 201.3   1992 146.8   1934 155.9   1925 137.8   1948 170.1   1939 202.3   2009 193.9   1982 248.8    2014 244.8    1938 267.2    1929
4 252.3   2015 227.9   1989 200.2   1967 142.1   1949 149.5   2015 137.7   1931 165.8   2010 191.4   1962 189.7   2011 247.7    1938 242.2    2006 265.4    2011
5 246.2   1974 224.9   2014 180.2   1979 133.5   1950 137.4   2003 135.0   1966 162.9   1956 190.3   2014 189.7   1927 242.3    1983 231.3    1917 264.0    2006
6 245.0   1975 195.6   1995 180.0   1989 132.9   1932 129.7   2007 131.7   2004 159.9   1985 189.1   2004 189.6   1985 240.9    2001 229.9    1981 261.0    1912


> tail(scotland_weather)

     Jan Year.1  Feb Year.2  Mar Year.3  Apr Year.4  May Year.5  Jun Year.6  Jul Year.7  Aug Year.8  Sep Year.9  Oct Year.10  Nov Year.11  Dec Year.12

101 71.2   1987 34.4   1947 50.9   1918 44.6   1982 34.1   1978 38.8   1940 49.2   2005 46.2   2003 50.7   2015 76.5    1973 57.1    1942 62.7    2010

102 57.9   1997 33.7   1917 44.4   1953 38.5   1918 32.1   1919 36.9   1932 47.8   1989 46.1   1983 49.6   1959 74.6    1922 54.9    1958 59.9    1963

103 57.9   1941 31.8   1963 39.7   1924 31.7   1981 28.8   1994 33.2   1921 45.8   1983 37.6   1955 48.5   1910 69.9    1972 53.9    1925 55.0    1995

104 57.6   1940 24.2   1930 38.8   1969 29.0   1938 26.1   2008 32.8   1925 39.7   1919 33.0   1995 40.0   1933 62.9    1914 53.6    1983 43.4    1927

105 51.9   1929 20.0   1986 37.4   1931 19.8   1980 24.0   1980 30.9   1941 33.7   1955 21.9   1976 39.2   2014 60.7    1951 42.3    1937 40.2    1933

106 38.6   1963 10.3   1932 28.7   1929 14.0   1974 22.5   1984 30.1   1988 32.7   1913  5.1   1947 31.7   1972 19.4    1946 28.8    1945   NA      NA

How can I format this into:

Year   month     rainfall_mm
1993    Jan            293.8
1990   Feb           278.1
....

> dput(head(scotland_weather,6))
structure(list(Jan = c(293.8, 292.2, 275.6, 252.3, 246.2, 245
), Year.1 = c(1993L, 1928L, 2008L, 2015L, 1974L, 1975L), Feb = c(278.1,
258.8, 244.7, 227.9, 224.9, 195.6), Year.2 = c(1990L, 1997L,
2002L, 1989L, 2014L, 1995L), Mar = c(238.5, 233.4, 201.3, 200.2,
180.2, 180), Year.3 = c(1994L, 1990L, 1992L, 1967L, 1979L, 1989L
), Apr = c(191.1, 149, 146.8, 142.1, 133.5, 132.9), Year.4 = c(1947L,
1910L, 1934L, 1949L, 1950L, 1932L), May = c(191.4, 168.7, 155.9,
149.5, 137.4, 129.7), Year.5 = c(2011L, 1986L, 1925L, 2015L,
2003L, 2007L), Jun = c(155, 137.9, 137.8, 137.7, 135, 131.7),
    Year.6 = c(1938L, 2002L, 1948L, 1931L, 1966L, 2004L), Jul = c(185.6,
    181.4, 170.1, 165.8, 162.9, 159.9), Year.7 = c(1940L, 1988L,
    1939L, 2010L, 1956L, 1985L), Aug = c(216.5, 211.9, 202.3,
    191.4, 190.3, 189.1), Year.8 = c(1985L, 1992L, 2009L, 1962L,
    2014L, 2004L), Sep = c(267.6, 221.2, 193.9, 189.7, 189.7,
    189.6), Year.9 = c(1950L, 1981L, 1982L, 2011L, 1927L, 1985L
    ), Oct = c(258.1, 254, 248.8, 247.7, 242.3, 240.9), Year.10 = c(1935L,
    1954L, 2014L, 1938L, 1983L, 2001L), Nov = c(262, 245.3, 244.8,
    242.2, 231.3, 229.9), Year.11 = c(2009L, 2015L, 1938L, 2006L,
    1917L, 1981L), Dec = c(300.7, 268.5, 267.2, 265.4, 264, 261
    ), Year.12 = c(2013L, 1986L, 1929L, 2011L, 2006L, 1912L),
    X1.12 = c(743.6, 649.5, 645.4, 638.3, 608.9, 592.8), Year.13 = c(2014L,
    1995L, 2000L, 2007L, 1990L, 2015L), X1.13 = c(409.5, 401.3,
    393.7, 393.2, 391.7, 389.1), Year.14 = c(1986L, 2015L, 1994L,
    1967L, 1992L, 1913L), X1.14 = c(455.6, 435.6, 427.8, 422.6,
    397, 390.1), Year.15 = c(1985L, 1948L, 2009L, 1956L, 2004L,
    1938L), X1.15 = c(661.2, 633.8, 615.8, 594.5, 590.6, 589.2
    ), Year.16 = c(1981L, 1954L, 1938L, 1935L, 1982L, 2006L),
    X1.16 = structure(c(105L, 104L, 103L, 102L, 101L, 100L), .Label = c("  1091.2",
    "  1138.2", "  1158.2", "  1166.0", "  1168.8", "  1174.1",
    "  1189.4", "  1214.2", "  1219.3", "  1220.0", "  1222.0",
    "  1231.5", "  1239.5", "  1250.0", "  1255.4", "  1266.1",
    "  1269.7", "  1274.2", "  1276.0", "  1281.1", "  1283.5",
    "  1301.7", "  1305.4", "  1306.4", "  1311.0", "  1311.1",
    "  1314.3", "  1315.8", "  1324.6", "  1325.3", "  1337.6",
    "  1348.6", "  1351.5", "  1355.6", "  1356.1", "  1356.7",
    "  1357.8", "  1366.9", "  1374.7", "  1376.5", "  1377.9",
    "  1378.5", "  1390.2", "  1397.6", "  1406.7", "  1406.9",
    "  1407.5", "  1407.9", "  1414.0", "  1425.3", "  1426.5",
    "  1429.6", "  1430.8", "  1431.6", "  1436.4", "  1438.0",
    "  1438.8", "  1445.9", "  1446.6", "  1448.6", "  1455.0",
    "  1458.6", "  1459.0", "  1460.9", "  1461.3", "  1464.4",
    "  1465.7", "  1466.4", "  1467.3", "  1473.9", "  1478.4",
    "  1478.6", "  1491.3", "  1493.2", "  1503.9", "  1520.3",
    "  1530.4", "  1532.5", "  1536.3", "  1558.0", "  1561.4",
    "  1566.8", "  1579.2", "  1582.3", "  1585.0", "  1585.5",
    "  1592.6", "  1607.8", "  1623.8", "  1627.8", "  1631.0",
    "  1657.1", "  1670.7", "  1672.8", "  1683.6", "  1686.1",
    "  1690.4", "  1692.9", "  1696.7", "  1716.5", "  1720.0",
    "  1735.8", "  1756.8", "  1828.1", "  1886.4", "NA"), class = "factor"),
    Year.17 = structure(c(102L, 81L, 105L, 29L, 99L, 45L), .Label = c("  1910",
    "  1911", "  1912", "  1913", "  1914", "  1915", "  1916",
    "  1917", "  1918", "  1919", "  1920", "  1921", "  1922",
    "  1923", "  1924", "  1925", "  1926", "  1927", "  1928",
    "  1929", "  1930", "  1931", "  1932", "  1933", "  1934",
    "  1935", "  1936", "  1937", "  1938", "  1939", "  1940",
    "  1941", "  1942", "  1943", "  1944", "  1945", "  1946",
    "  1947", "  1948", "  1949", "  1950", "  1951", "  1952",
    "  1953", "  1954", "  1955", "  1956", "  1957", "  1958",
    "  1959", "  1960", "  1961", "  1962", "  1963", "  1964",
    "  1965", "  1966", "  1967", "  1968", "  1969", "  1970",
    "  1971", "  1972", "  1973", "  1974", "  1975", "  1976",
    "  1977", "  1978", "  1979", "  1980", "  1981", "  1982",
    "  1983", "  1984", "  1985", "  1986", "  1987", "  1988",
    "  1989", "  1990", "  1991", "  1992", "  1993", "  1994",
    "  1995", "  1996", "  1997", "  1998", "  1999", "  2000",
    "  2001", "  2002", "  2003", "  2004", "  2005", "  2006",
    "  2007", "  2008", "  2009", "  2010", "  2011", "  2012",
    "  2013", "  2014", "NA"), class = "factor")), .Names = c("Jan",
"Year.1", "Feb", "Year.2", "Mar", "Year.3", "Apr", "Year.4",
"May", "Year.5", "Jun", "Year.6", "Jul", "Year.7", "Aug", "Year.8",
"Sep", "Year.9", "Oct", "Year.10", "Nov", "Year.11", "Dec", "Year.12",
"X1.12", "Year.13", "X1.13", "Year.14", "X1.14", "Year.15", "X1.15",
"Year.16", "X1.16", "Year.17"), row.names = c(NA, 6L), class = "data.frame")

Is there an easier way to do it with tidyr?

Regards,

Fahad Usman
Network Engineering | CIO | Openreach
Web: www.openreach.co.uk<http://www.openreach.co.uk/>
Openreach is delivering fibre broadband services to communities across the UK as well as installing and maintaining the communications infrastructure that links homes, businesses, public and voluntary sector organisations to their Communications Providers' networks.
Think before you print! Consider the environment before printing this e-mail.

This email contains BT information, which may be privileged or confidential.
It's meant only for the individual(s) or entity named above. If you're not the intended
recipient, note that disclosing, copying, distributing or using this information
is prohibited. If you've received this email in error, please let me know immediately
on the email address above. Thank you.
We monitor our email system, and may record your emails.
British Telecommunications plc
Registered office: 81 Newgate Street London EC1A 7AJ
Registered in England no: 1800000


	[[alternative HTML version deleted]]


From istazahn at gmail.com  Thu Dec 17 20:48:59 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Thu, 17 Dec 2015 14:48:59 -0500
Subject: [R] Gather columns based on multiple columns using tidyr
In-Reply-To: <9237949d508e4c8583ab4e17f7f63535@tpw09926dag11f.domain1.systemhost.net>
References: <9237949d508e4c8583ab4e17f7f63535@tpw09926dag11f.domain1.systemhost.net>
Message-ID: <CA+vqiLH_uJB44aBY4to3PkgmwUiQ=mCGjGcvD_YhVCV+tf3FHg@mail.gmail.com>

Hi Fahad,

Easier than what? You didn't tell us what you tried, nor why you were
unhappy with it. I'm only passingly familiar with tidyr, but I came up
with


library(tidyr)
library(dplyr)

read.table("scotland_rainfall.txt", skip = 7, header=TRUE, fill = TRUE) %>%
  select(-WIN, -SPR, -SUM, -AUT, -ANN) %>%
  gather(junk1, year, starts_with("Year")) %>%
  gather(month, rainfall_mm, one_of(toupper(month.abb))) %>%
  arrange(year, month) %>%
  select(-junk1) -> scotland_weather

Best,
Ista

On Thu, Dec 17, 2015 at 10:24 AM,  <fahad.usman at openreach.co.uk> wrote:
> Hi,
>
> Sorry for this direct approach but I am stuck with a stupid data that I would like to reformat.
>
> The datafile is location at: fileURL <- http://www.metoffice.gov.uk/pub/data/weather/uk/climate/datasets/Rainfall/ranked/Scotland.txt
>
> You can read the data by:
>
> if(!file.exists("scotland_rainfall.txt")){
>         #this will download the file in the current working directory
>         download.file(fileURL,destfile = "scotland_rainfall.txt")
>         dateDownload <- Sys.Date() #15-12-2015
>
> }
>
>> head(scotland_weather)
>     Jan Year.1   Feb Year.2   Mar Year.3   Apr Year.4   May Year.5   Jun Year.6   Jul Year.7   Aug Year.8   Sep Year.9   Oct Year.10   Nov Year.11   Dec Year.12
> 1 293.8   1993 278.1   1990 238.5   1994 191.1   1947 191.4   2011 155.0   1938 185.6   1940 216.5   1985 267.6   1950 258.1    1935 262.0    2009 300.7    2013
> 2 292.2   1928 258.8   1997 233.4   1990 149.0   1910 168.7   1986 137.9   2002 181.4   1988 211.9   1992 221.2   1981 254.0    1954 245.3    2015 268.5    1986
> 3 275.6   2008 244.7   2002 201.3   1992 146.8   1934 155.9   1925 137.8   1948 170.1   1939 202.3   2009 193.9   1982 248.8    2014 244.8    1938 267.2    1929
> 4 252.3   2015 227.9   1989 200.2   1967 142.1   1949 149.5   2015 137.7   1931 165.8   2010 191.4   1962 189.7   2011 247.7    1938 242.2    2006 265.4    2011
> 5 246.2   1974 224.9   2014 180.2   1979 133.5   1950 137.4   2003 135.0   1966 162.9   1956 190.3   2014 189.7   1927 242.3    1983 231.3    1917 264.0    2006
> 6 245.0   1975 195.6   1995 180.0   1989 132.9   1932 129.7   2007 131.7   2004 159.9   1985 189.1   2004 189.6   1985 240.9    2001 229.9    1981 261.0    1912
>
>
>> tail(scotland_weather)
>
>      Jan Year.1  Feb Year.2  Mar Year.3  Apr Year.4  May Year.5  Jun Year.6  Jul Year.7  Aug Year.8  Sep Year.9  Oct Year.10  Nov Year.11  Dec Year.12
>
> 101 71.2   1987 34.4   1947 50.9   1918 44.6   1982 34.1   1978 38.8   1940 49.2   2005 46.2   2003 50.7   2015 76.5    1973 57.1    1942 62.7    2010
>
> 102 57.9   1997 33.7   1917 44.4   1953 38.5   1918 32.1   1919 36.9   1932 47.8   1989 46.1   1983 49.6   1959 74.6    1922 54.9    1958 59.9    1963
>
> 103 57.9   1941 31.8   1963 39.7   1924 31.7   1981 28.8   1994 33.2   1921 45.8   1983 37.6   1955 48.5   1910 69.9    1972 53.9    1925 55.0    1995
>
> 104 57.6   1940 24.2   1930 38.8   1969 29.0   1938 26.1   2008 32.8   1925 39.7   1919 33.0   1995 40.0   1933 62.9    1914 53.6    1983 43.4    1927
>
> 105 51.9   1929 20.0   1986 37.4   1931 19.8   1980 24.0   1980 30.9   1941 33.7   1955 21.9   1976 39.2   2014 60.7    1951 42.3    1937 40.2    1933
>
> 106 38.6   1963 10.3   1932 28.7   1929 14.0   1974 22.5   1984 30.1   1988 32.7   1913  5.1   1947 31.7   1972 19.4    1946 28.8    1945   NA      NA
>
> How can I format this into:
>
> Year   month     rainfall_mm
> 1993    Jan            293.8
> 1990   Feb           278.1
> ....
>
>> dput(head(scotland_weather,6))
> structure(list(Jan = c(293.8, 292.2, 275.6, 252.3, 246.2, 245
> ), Year.1 = c(1993L, 1928L, 2008L, 2015L, 1974L, 1975L), Feb = c(278.1,
> 258.8, 244.7, 227.9, 224.9, 195.6), Year.2 = c(1990L, 1997L,
> 2002L, 1989L, 2014L, 1995L), Mar = c(238.5, 233.4, 201.3, 200.2,
> 180.2, 180), Year.3 = c(1994L, 1990L, 1992L, 1967L, 1979L, 1989L
> ), Apr = c(191.1, 149, 146.8, 142.1, 133.5, 132.9), Year.4 = c(1947L,
> 1910L, 1934L, 1949L, 1950L, 1932L), May = c(191.4, 168.7, 155.9,
> 149.5, 137.4, 129.7), Year.5 = c(2011L, 1986L, 1925L, 2015L,
> 2003L, 2007L), Jun = c(155, 137.9, 137.8, 137.7, 135, 131.7),
>     Year.6 = c(1938L, 2002L, 1948L, 1931L, 1966L, 2004L), Jul = c(185.6,
>     181.4, 170.1, 165.8, 162.9, 159.9), Year.7 = c(1940L, 1988L,
>     1939L, 2010L, 1956L, 1985L), Aug = c(216.5, 211.9, 202.3,
>     191.4, 190.3, 189.1), Year.8 = c(1985L, 1992L, 2009L, 1962L,
>     2014L, 2004L), Sep = c(267.6, 221.2, 193.9, 189.7, 189.7,
>     189.6), Year.9 = c(1950L, 1981L, 1982L, 2011L, 1927L, 1985L
>     ), Oct = c(258.1, 254, 248.8, 247.7, 242.3, 240.9), Year.10 = c(1935L,
>     1954L, 2014L, 1938L, 1983L, 2001L), Nov = c(262, 245.3, 244.8,
>     242.2, 231.3, 229.9), Year.11 = c(2009L, 2015L, 1938L, 2006L,
>     1917L, 1981L), Dec = c(300.7, 268.5, 267.2, 265.4, 264, 261
>     ), Year.12 = c(2013L, 1986L, 1929L, 2011L, 2006L, 1912L),
>     X1.12 = c(743.6, 649.5, 645.4, 638.3, 608.9, 592.8), Year.13 = c(2014L,
>     1995L, 2000L, 2007L, 1990L, 2015L), X1.13 = c(409.5, 401.3,
>     393.7, 393.2, 391.7, 389.1), Year.14 = c(1986L, 2015L, 1994L,
>     1967L, 1992L, 1913L), X1.14 = c(455.6, 435.6, 427.8, 422.6,
>     397, 390.1), Year.15 = c(1985L, 1948L, 2009L, 1956L, 2004L,
>     1938L), X1.15 = c(661.2, 633.8, 615.8, 594.5, 590.6, 589.2
>     ), Year.16 = c(1981L, 1954L, 1938L, 1935L, 1982L, 2006L),
>     X1.16 = structure(c(105L, 104L, 103L, 102L, 101L, 100L), .Label = c("  1091.2",
>     "  1138.2", "  1158.2", "  1166.0", "  1168.8", "  1174.1",
>     "  1189.4", "  1214.2", "  1219.3", "  1220.0", "  1222.0",
>     "  1231.5", "  1239.5", "  1250.0", "  1255.4", "  1266.1",
>     "  1269.7", "  1274.2", "  1276.0", "  1281.1", "  1283.5",
>     "  1301.7", "  1305.4", "  1306.4", "  1311.0", "  1311.1",
>     "  1314.3", "  1315.8", "  1324.6", "  1325.3", "  1337.6",
>     "  1348.6", "  1351.5", "  1355.6", "  1356.1", "  1356.7",
>     "  1357.8", "  1366.9", "  1374.7", "  1376.5", "  1377.9",
>     "  1378.5", "  1390.2", "  1397.6", "  1406.7", "  1406.9",
>     "  1407.5", "  1407.9", "  1414.0", "  1425.3", "  1426.5",
>     "  1429.6", "  1430.8", "  1431.6", "  1436.4", "  1438.0",
>     "  1438.8", "  1445.9", "  1446.6", "  1448.6", "  1455.0",
>     "  1458.6", "  1459.0", "  1460.9", "  1461.3", "  1464.4",
>     "  1465.7", "  1466.4", "  1467.3", "  1473.9", "  1478.4",
>     "  1478.6", "  1491.3", "  1493.2", "  1503.9", "  1520.3",
>     "  1530.4", "  1532.5", "  1536.3", "  1558.0", "  1561.4",
>     "  1566.8", "  1579.2", "  1582.3", "  1585.0", "  1585.5",
>     "  1592.6", "  1607.8", "  1623.8", "  1627.8", "  1631.0",
>     "  1657.1", "  1670.7", "  1672.8", "  1683.6", "  1686.1",
>     "  1690.4", "  1692.9", "  1696.7", "  1716.5", "  1720.0",
>     "  1735.8", "  1756.8", "  1828.1", "  1886.4", "NA"), class = "factor"),
>     Year.17 = structure(c(102L, 81L, 105L, 29L, 99L, 45L), .Label = c("  1910",
>     "  1911", "  1912", "  1913", "  1914", "  1915", "  1916",
>     "  1917", "  1918", "  1919", "  1920", "  1921", "  1922",
>     "  1923", "  1924", "  1925", "  1926", "  1927", "  1928",
>     "  1929", "  1930", "  1931", "  1932", "  1933", "  1934",
>     "  1935", "  1936", "  1937", "  1938", "  1939", "  1940",
>     "  1941", "  1942", "  1943", "  1944", "  1945", "  1946",
>     "  1947", "  1948", "  1949", "  1950", "  1951", "  1952",
>     "  1953", "  1954", "  1955", "  1956", "  1957", "  1958",
>     "  1959", "  1960", "  1961", "  1962", "  1963", "  1964",
>     "  1965", "  1966", "  1967", "  1968", "  1969", "  1970",
>     "  1971", "  1972", "  1973", "  1974", "  1975", "  1976",
>     "  1977", "  1978", "  1979", "  1980", "  1981", "  1982",
>     "  1983", "  1984", "  1985", "  1986", "  1987", "  1988",
>     "  1989", "  1990", "  1991", "  1992", "  1993", "  1994",
>     "  1995", "  1996", "  1997", "  1998", "  1999", "  2000",
>     "  2001", "  2002", "  2003", "  2004", "  2005", "  2006",
>     "  2007", "  2008", "  2009", "  2010", "  2011", "  2012",
>     "  2013", "  2014", "NA"), class = "factor")), .Names = c("Jan",
> "Year.1", "Feb", "Year.2", "Mar", "Year.3", "Apr", "Year.4",
> "May", "Year.5", "Jun", "Year.6", "Jul", "Year.7", "Aug", "Year.8",
> "Sep", "Year.9", "Oct", "Year.10", "Nov", "Year.11", "Dec", "Year.12",
> "X1.12", "Year.13", "X1.13", "Year.14", "X1.14", "Year.15", "X1.15",
> "Year.16", "X1.16", "Year.17"), row.names = c(NA, 6L), class = "data.frame")
>
> Is there an easier way to do it with tidyr?
>
> Regards,
>
> Fahad Usman
> Network Engineering | CIO | Openreach
> Web: www.openreach.co.uk<http://www.openreach.co.uk/>
> Openreach is delivering fibre broadband services to communities across the UK as well as installing and maintaining the communications infrastructure that links homes, businesses, public and voluntary sector organisations to their Communications Providers' networks.
> Think before you print! Consider the environment before printing this e-mail.
>
> This email contains BT information, which may be privileged or confidential.
> It's meant only for the individual(s) or entity named above. If you're not the intended
> recipient, note that disclosing, copying, distributing or using this information
> is prohibited. If you've received this email in error, please let me know immediately
> on the email address above. Thank you.
> We monitor our email system, and may record your emails.
> British Telecommunications plc
> Registered office: 81 Newgate Street London EC1A 7AJ
> Registered in England no: 1800000
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dcarlson at tamu.edu  Thu Dec 17 21:01:57 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Thu, 17 Dec 2015 20:01:57 +0000
Subject: [R] Gather columns based on multiple columns using tidyr
In-Reply-To: <CA+vqiLH_uJB44aBY4to3PkgmwUiQ=mCGjGcvD_YhVCV+tf3FHg@mail.gmail.com>
References: <9237949d508e4c8583ab4e17f7f63535@tpw09926dag11f.domain1.systemhost.net>
	<CA+vqiLH_uJB44aBY4to3PkgmwUiQ=mCGjGcvD_YhVCV+tf3FHg@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6EA851@mb02.ads.tamu.edu>

Here's another approach, but it doesn't use tidy or dplyr:

> Rain.col <- seq(1, 24, by=2)
> Year.col <- seq(2, 24, by =2)
> Scotland.lst <- lapply(1:12, function(x) data.frame(Year = 
+      scotland_weather[ , Year.col[x]], month = month.abb[x],
+       rainfall_mm = scotland_weather[ , Rain.col[x]]))
> Scotland.df <- do.call(rbind, Scotland.lst)
> head(Scotland.df)
  Year month rainfall_mm
1 1993   Jan       293.8
2 1928   Jan       292.2
3 2008   Jan       275.6
4 2015   Jan       252.3
5 1974   Jan       246.2
6 1975   Jan       245.0
> tail(Scotland.df)
   Year month rainfall_mm
67 2013   Dec       300.7
68 1986   Dec       268.5
69 1929   Dec       267.2
70 2011   Dec       265.4
71 2006   Dec       264.0
72 1912   Dec       261.0

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ista Zahn
Sent: Thursday, December 17, 2015 1:49 PM
To: fahad.usman at openreach.co.uk
Cc: r-help at r-project.org
Subject: Re: [R] Gather columns based on multiple columns using tidyr

Hi Fahad,

Easier than what? You didn't tell us what you tried, nor why you were
unhappy with it. I'm only passingly familiar with tidyr, but I came up
with


library(tidyr)
library(dplyr)

read.table("scotland_rainfall.txt", skip = 7, header=TRUE, fill = TRUE) %>%
  select(-WIN, -SPR, -SUM, -AUT, -ANN) %>%
  gather(junk1, year, starts_with("Year")) %>%
  gather(month, rainfall_mm, one_of(toupper(month.abb))) %>%
  arrange(year, month) %>%
  select(-junk1) -> scotland_weather

Best,
Ista

On Thu, Dec 17, 2015 at 10:24 AM,  <fahad.usman at openreach.co.uk> wrote:
> Hi,
>
> Sorry for this direct approach but I am stuck with a stupid data that I would like to reformat.
>
> The datafile is location at: fileURL <- http://www.metoffice.gov.uk/pub/data/weather/uk/climate/datasets/Rainfall/ranked/Scotland.txt
>
> You can read the data by:
>
> if(!file.exists("scotland_rainfall.txt")){
>         #this will download the file in the current working directory
>         download.file(fileURL,destfile = "scotland_rainfall.txt")
>         dateDownload <- Sys.Date() #15-12-2015
>
> }
>
>> head(scotland_weather)
>     Jan Year.1   Feb Year.2   Mar Year.3   Apr Year.4   May Year.5   Jun Year.6   Jul Year.7   Aug Year.8   Sep Year.9   Oct Year.10   Nov Year.11   Dec Year.12
> 1 293.8   1993 278.1   1990 238.5   1994 191.1   1947 191.4   2011 155.0   1938 185.6   1940 216.5   1985 267.6   1950 258.1    1935 262.0    2009 300.7    2013
> 2 292.2   1928 258.8   1997 233.4   1990 149.0   1910 168.7   1986 137.9   2002 181.4   1988 211.9   1992 221.2   1981 254.0    1954 245.3    2015 268.5    1986
> 3 275.6   2008 244.7   2002 201.3   1992 146.8   1934 155.9   1925 137.8   1948 170.1   1939 202.3   2009 193.9   1982 248.8    2014 244.8    1938 267.2    1929
> 4 252.3   2015 227.9   1989 200.2   1967 142.1   1949 149.5   2015 137.7   1931 165.8   2010 191.4   1962 189.7   2011 247.7    1938 242.2    2006 265.4    2011
> 5 246.2   1974 224.9   2014 180.2   1979 133.5   1950 137.4   2003 135.0   1966 162.9   1956 190.3   2014 189.7   1927 242.3    1983 231.3    1917 264.0    2006
> 6 245.0   1975 195.6   1995 180.0   1989 132.9   1932 129.7   2007 131.7   2004 159.9   1985 189.1   2004 189.6   1985 240.9    2001 229.9    1981 261.0    1912
>
>
>> tail(scotland_weather)
>
>      Jan Year.1  Feb Year.2  Mar Year.3  Apr Year.4  May Year.5  Jun Year.6  Jul Year.7  Aug Year.8  Sep Year.9  Oct Year.10  Nov Year.11  Dec Year.12
>
> 101 71.2   1987 34.4   1947 50.9   1918 44.6   1982 34.1   1978 38.8   1940 49.2   2005 46.2   2003 50.7   2015 76.5    1973 57.1    1942 62.7    2010
>
> 102 57.9   1997 33.7   1917 44.4   1953 38.5   1918 32.1   1919 36.9   1932 47.8   1989 46.1   1983 49.6   1959 74.6    1922 54.9    1958 59.9    1963
>
> 103 57.9   1941 31.8   1963 39.7   1924 31.7   1981 28.8   1994 33.2   1921 45.8   1983 37.6   1955 48.5   1910 69.9    1972 53.9    1925 55.0    1995
>
> 104 57.6   1940 24.2   1930 38.8   1969 29.0   1938 26.1   2008 32.8   1925 39.7   1919 33.0   1995 40.0   1933 62.9    1914 53.6    1983 43.4    1927
>
> 105 51.9   1929 20.0   1986 37.4   1931 19.8   1980 24.0   1980 30.9   1941 33.7   1955 21.9   1976 39.2   2014 60.7    1951 42.3    1937 40.2    1933
>
> 106 38.6   1963 10.3   1932 28.7   1929 14.0   1974 22.5   1984 30.1   1988 32.7   1913  5.1   1947 31.7   1972 19.4    1946 28.8    1945   NA      NA
>
> How can I format this into:
>
> Year   month     rainfall_mm
> 1993    Jan            293.8
> 1990   Feb           278.1
> ....
>
>> dput(head(scotland_weather,6))
> structure(list(Jan = c(293.8, 292.2, 275.6, 252.3, 246.2, 245
> ), Year.1 = c(1993L, 1928L, 2008L, 2015L, 1974L, 1975L), Feb = c(278.1,
> 258.8, 244.7, 227.9, 224.9, 195.6), Year.2 = c(1990L, 1997L,
> 2002L, 1989L, 2014L, 1995L), Mar = c(238.5, 233.4, 201.3, 200.2,
> 180.2, 180), Year.3 = c(1994L, 1990L, 1992L, 1967L, 1979L, 1989L
> ), Apr = c(191.1, 149, 146.8, 142.1, 133.5, 132.9), Year.4 = c(1947L,
> 1910L, 1934L, 1949L, 1950L, 1932L), May = c(191.4, 168.7, 155.9,
> 149.5, 137.4, 129.7), Year.5 = c(2011L, 1986L, 1925L, 2015L,
> 2003L, 2007L), Jun = c(155, 137.9, 137.8, 137.7, 135, 131.7),
>     Year.6 = c(1938L, 2002L, 1948L, 1931L, 1966L, 2004L), Jul = c(185.6,
>     181.4, 170.1, 165.8, 162.9, 159.9), Year.7 = c(1940L, 1988L,
>     1939L, 2010L, 1956L, 1985L), Aug = c(216.5, 211.9, 202.3,
>     191.4, 190.3, 189.1), Year.8 = c(1985L, 1992L, 2009L, 1962L,
>     2014L, 2004L), Sep = c(267.6, 221.2, 193.9, 189.7, 189.7,
>     189.6), Year.9 = c(1950L, 1981L, 1982L, 2011L, 1927L, 1985L
>     ), Oct = c(258.1, 254, 248.8, 247.7, 242.3, 240.9), Year.10 = c(1935L,
>     1954L, 2014L, 1938L, 1983L, 2001L), Nov = c(262, 245.3, 244.8,
>     242.2, 231.3, 229.9), Year.11 = c(2009L, 2015L, 1938L, 2006L,
>     1917L, 1981L), Dec = c(300.7, 268.5, 267.2, 265.4, 264, 261
>     ), Year.12 = c(2013L, 1986L, 1929L, 2011L, 2006L, 1912L),
>     X1.12 = c(743.6, 649.5, 645.4, 638.3, 608.9, 592.8), Year.13 = c(2014L,
>     1995L, 2000L, 2007L, 1990L, 2015L), X1.13 = c(409.5, 401.3,
>     393.7, 393.2, 391.7, 389.1), Year.14 = c(1986L, 2015L, 1994L,
>     1967L, 1992L, 1913L), X1.14 = c(455.6, 435.6, 427.8, 422.6,
>     397, 390.1), Year.15 = c(1985L, 1948L, 2009L, 1956L, 2004L,
>     1938L), X1.15 = c(661.2, 633.8, 615.8, 594.5, 590.6, 589.2
>     ), Year.16 = c(1981L, 1954L, 1938L, 1935L, 1982L, 2006L),
>     X1.16 = structure(c(105L, 104L, 103L, 102L, 101L, 100L), .Label = c("  1091.2",
>     "  1138.2", "  1158.2", "  1166.0", "  1168.8", "  1174.1",
>     "  1189.4", "  1214.2", "  1219.3", "  1220.0", "  1222.0",
>     "  1231.5", "  1239.5", "  1250.0", "  1255.4", "  1266.1",
>     "  1269.7", "  1274.2", "  1276.0", "  1281.1", "  1283.5",
>     "  1301.7", "  1305.4", "  1306.4", "  1311.0", "  1311.1",
>     "  1314.3", "  1315.8", "  1324.6", "  1325.3", "  1337.6",
>     "  1348.6", "  1351.5", "  1355.6", "  1356.1", "  1356.7",
>     "  1357.8", "  1366.9", "  1374.7", "  1376.5", "  1377.9",
>     "  1378.5", "  1390.2", "  1397.6", "  1406.7", "  1406.9",
>     "  1407.5", "  1407.9", "  1414.0", "  1425.3", "  1426.5",
>     "  1429.6", "  1430.8", "  1431.6", "  1436.4", "  1438.0",
>     "  1438.8", "  1445.9", "  1446.6", "  1448.6", "  1455.0",
>     "  1458.6", "  1459.0", "  1460.9", "  1461.3", "  1464.4",
>     "  1465.7", "  1466.4", "  1467.3", "  1473.9", "  1478.4",
>     "  1478.6", "  1491.3", "  1493.2", "  1503.9", "  1520.3",
>     "  1530.4", "  1532.5", "  1536.3", "  1558.0", "  1561.4",
>     "  1566.8", "  1579.2", "  1582.3", "  1585.0", "  1585.5",
>     "  1592.6", "  1607.8", "  1623.8", "  1627.8", "  1631.0",
>     "  1657.1", "  1670.7", "  1672.8", "  1683.6", "  1686.1",
>     "  1690.4", "  1692.9", "  1696.7", "  1716.5", "  1720.0",
>     "  1735.8", "  1756.8", "  1828.1", "  1886.4", "NA"), class = "factor"),
>     Year.17 = structure(c(102L, 81L, 105L, 29L, 99L, 45L), .Label = c("  1910",
>     "  1911", "  1912", "  1913", "  1914", "  1915", "  1916",
>     "  1917", "  1918", "  1919", "  1920", "  1921", "  1922",
>     "  1923", "  1924", "  1925", "  1926", "  1927", "  1928",
>     "  1929", "  1930", "  1931", "  1932", "  1933", "  1934",
>     "  1935", "  1936", "  1937", "  1938", "  1939", "  1940",
>     "  1941", "  1942", "  1943", "  1944", "  1945", "  1946",
>     "  1947", "  1948", "  1949", "  1950", "  1951", "  1952",
>     "  1953", "  1954", "  1955", "  1956", "  1957", "  1958",
>     "  1959", "  1960", "  1961", "  1962", "  1963", "  1964",
>     "  1965", "  1966", "  1967", "  1968", "  1969", "  1970",
>     "  1971", "  1972", "  1973", "  1974", "  1975", "  1976",
>     "  1977", "  1978", "  1979", "  1980", "  1981", "  1982",
>     "  1983", "  1984", "  1985", "  1986", "  1987", "  1988",
>     "  1989", "  1990", "  1991", "  1992", "  1993", "  1994",
>     "  1995", "  1996", "  1997", "  1998", "  1999", "  2000",
>     "  2001", "  2002", "  2003", "  2004", "  2005", "  2006",
>     "  2007", "  2008", "  2009", "  2010", "  2011", "  2012",
>     "  2013", "  2014", "NA"), class = "factor")), .Names = c("Jan",
> "Year.1", "Feb", "Year.2", "Mar", "Year.3", "Apr", "Year.4",
> "May", "Year.5", "Jun", "Year.6", "Jul", "Year.7", "Aug", "Year.8",
> "Sep", "Year.9", "Oct", "Year.10", "Nov", "Year.11", "Dec", "Year.12",
> "X1.12", "Year.13", "X1.13", "Year.14", "X1.14", "Year.15", "X1.15",
> "Year.16", "X1.16", "Year.17"), row.names = c(NA, 6L), class = "data.frame")
>
> Is there an easier way to do it with tidyr?
>
> Regards,
>
> Fahad Usman
> Network Engineering | CIO | Openreach
> Web: www.openreach.co.uk<http://www.openreach.co.uk/>
> Openreach is delivering fibre broadband services to communities across the UK as well as installing and maintaining the communications infrastructure that links homes, businesses, public and voluntary sector organisations to their Communications Providers' networks.
> Think before you print! Consider the environment before printing this e-mail.
>
> This email contains BT information, which may be privileged or confidential.
> It's meant only for the individual(s) or entity named above. If you're not the intended
> recipient, note that disclosing, copying, distributing or using this information
> is prohibited. If you've received this email in error, please let me know immediately
> on the email address above. Thank you.
> We monitor our email system, and may record your emails.
> British Telecommunications plc
> Registered office: 81 Newgate Street London EC1A 7AJ
> Registered in England no: 1800000
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From cjpauw at gmail.com  Thu Dec 17 21:27:19 2015
From: cjpauw at gmail.com (Christiaan Pauw)
Date: Thu, 17 Dec 2015 22:27:19 +0200
Subject: [R] Variable Selection for Logistic Regression
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08D89256@W01GMAILDAGA52.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08D89256@W01GMAILDAGA52.reg1.1bank.dbs.com>
Message-ID: <CAJESSmYRxUPh5bVXKZwXNj1cWTEd7B_dv2Jw0jKax2uRNpUyPA@mail.gmail.com>

Lasso is an obvious choice by it may also be interesting to look at the
variable importance from a random forest model
On 17 Dec 2015 17:28, "Manish MAHESHWARI" <manishm at dbs.com> wrote:

> Hi,
>
> I have a dataset with approx 400K Rows and 900 columns with a single
> dependent variable of 0/1 flag. The independent variables are both
> categorical and numerical. I have looked as SO/Cross Validated Posts but
> couldn't get an answer for this.
>
> Since I cannot try all possible combinations of variables or even attempt
> single model with all 900 columns, I am planning to create independent
> models of each variable using something like below -
>
> out = NULL
> xnames = colnames(train)[!colnames(train) %in% ignoredcols]
> for (f in xnames) {
>     glmm = glm(train$conversion_flag ~ train[,f] - 1 , family = binomial)
>     out =
> rbind.fill(out,as.data.frame(cbind(f,fmsb::NagelkerkeR2(glmm)[2]$R2)))
>     out = rbind.fill(out,as.data.frame(cbind(f,'AIC',summary(glmm)$aic)))
> }
>
> This will give me the individual AIC and pseudo R2 for each of the
> variables. Post that I plan to select the variables with the best scores
> for both AIC and pseudoR2. Does this approach make sense?
>
> I obviously will use a nfold cross validation in the final model to ensure
> accuracy and avoid over fitting. However before I reach that I plan to use
> the above to select which variables to use.
>
> Thanks,
> Manish
> CONFIDENTIAL NOTE:
> The information contained in this email is intended on...{{dropped:13}}


From klebyn at yahoo.com.br  Fri Dec 18 01:07:34 2015
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Thu, 17 Dec 2015 22:07:34 -0200
Subject: [R] How to use the options "usecommand" and "command" of tktable?
Message-ID: <56734E46.6050807@yahoo.com.br>

How to capture the output from the "command" option of tktable and how 
to send input to it?

As far as I understand, the most appropriate way to use the tktable is 
through the flag usecommand and command.

It fires the string information and cell coordinates and wait for a new 
string to set a new value.

It's I understood by example in pure TCL given in reference [1].

Is there how to translate this code to R?

I believe that the signs of tktable not reach the level of R and will 
need to be accessed by the lower level commands. But how?

Thank you in advance for dedicated attention.

cleber

###
[1] - http://dbaspot.com/object/showthread.php?t=785221&pagenumber=

### my R code
library( tcltk )
tclRequire( "Tktable" )
tclServiceMode( FALSE )
top <- tktoplevel()

TableCommand <- function( x=NULL ) {
     cat( class( x ), '\n' )
     cat( x , '\n' )
}

tab <- tkwidget( top, 'table', rows=2, cols=2, usecommand=TRUE, 
command=TableCommand )
tcl('pack', tab, expand=TRUE, fill='both' )
tclServiceMode( TRUE )

### TCL from [1]
package require Tktable

namespace eval ::test {}

proc ::test::build_table {} {

catch {destroy .test}
variable top [toplevel .test]

variable myarray
array unset myarray

variable table [table $top.table \
-rows 5 \
-height 5 \
-cols 2 \
-command "[namespace current]::TableCommand %S %r %c"]

pack $table -expand true -fill both
$table tag config complete -background blue
}

proc ::test::TableCommand {S r c} {

variable table
variable myarray

if {$S != ""} {
# I only want integers in my cells!
puts "\"$S\" into $r,$c ?"

if {[string is integer -strict $S]} {
$table tag cell complete "$r,$c"
set myarray($r,$c) $S
} else {
$table tag cell !complete "$r,$c"
array unset myarray $r,$c
}
}
if {[info exists myarray($r,$c)]} {
return $myarray($r,$c)
}
return "$r,$c"
}

::test::build_table




---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From sidoti.23 at buckeyemail.osu.edu  Thu Dec 17 22:25:20 2015
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Thu, 17 Dec 2015 21:25:20 +0000
Subject: [R] Converting from Continuous 2D Points to Continuous 2D Vectors
In-Reply-To: <mailman.0.1450030036.31803.r-help@r-project.org>
References: <mailman.0.1450030036.31803.r-help@r-project.org>
Message-ID: <CY1PR0101MB1004E533C2B3AA381E59BE3DABE00@CY1PR0101MB1004.prod.exchangelabs.com>

Greetings!

I have a fairly large dataframe (df) with pathing information in the form of continuous x,y coordinates:

df$x
df$y

With these data, I would like to:
1. Calculate a set of continuous vectors
2. Determine the angle between each of these vectors (in degrees)
3. Count the number of angles in the dataframe that meet a certain threshold (i.e. <90?)

Here's what I've come up with so far:

### Function that calculates the angle between two vectors in 2D space:

angle <- function(x,y){ # x and y are vectors
  dot.prod <- x%*%y 
  norm.x <- norm(x,type="2")
  norm.y <- norm(y,type="2")
  theta <- acos(dot.prod / (norm.x * norm.y))
  (180*as.numeric(theta))/pi # returns the angle in degrees
}

### Test the function:
x <- as.matrix(c(2,1))
y <- as.matrix(c(1,2))
angle(t(x),y)
[1] 36.8699

Thank you!

From sidoti.23 at buckeyemail.osu.edu  Fri Dec 18 02:06:28 2015
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Fri, 18 Dec 2015 01:06:28 +0000
Subject: [R] Hexbin: Counting Bins That Meet Certain Criteria
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6E91CA@mb02.ads.tamu.edu>
References: <CY1PR0101MB10048C9DFF962DAF602BFF63ABEE0@CY1PR0101MB1004.prod.exchangelabs.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6E91CA@mb02.ads.tamu.edu>
Message-ID: <CY1PR0101MB100483CE053D0FCD4015FC68ABE10@CY1PR0101MB1004.prod.exchangelabs.com>

Thank you, David! I adapted this code and it works very nicely with my data. 

Just to give you a bit of background, I am a behavioral ecologist. I am currently studying the general search patterns of wolf spiders and I have a lot of tracking data to process. I am not a coder, although I am slowing becoming one out of necessity!

I created a new post today, so everyone should be seeing it shortly...

Salvatore A. Sidoti
PhD Student
Graduate Teaching Assistant

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: Tuesday, December 15, 2015 12:18 PM
To: Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu>; r-help at r-project.org
Subject: RE: Hexbin: Counting Bins That Meet Certain Criteria

Something like

> library(hexbin)
> set.seed(42)
> xy <- matrix(rnorm(1000), 500)
> xy.hex <- hexbin(xy)
> table(xy.hex at count)

  1   2   3   4   5   6   7   8 
159  60  33  16   6   1   2   1 
> sum(xy.hex at count >= 3)
[1] 59

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sidoti, Salvatore A.
Sent: Monday, December 14, 2015 6:49 PM
To: r-help at r-project.org
Subject: [R] Hexbin: Counting Bins That Meet Certain Criteria

Greetings!

Is there a way to count the bins in a hexbin plot that meet certain criteria? For instance, what if I wanted to count the bins (hexes) that have a datapoint density of some number x?

Thank you!

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From sabasehrish at yahoo.com  Thu Dec 17 22:13:23 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Thu, 17 Dec 2015 21:13:23 +0000 (UTC)
Subject: [R] Error in linear regression
References: <797805015.300917.1450386804117.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <797805015.300917.1450386804117.JavaMail.yahoo@mail.yahoo.com>

Hi?I am trying to apply linear regression on the attached data of two variables (DODGX, TRMCX) in R by taking into account time lag=5 for both of them. Each time I run this command, it gives me following error:
Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :?? NA/NaN/Inf in 'y'In addition: Warning message:In model.response(mf, "numeric") : NAs introduced by coercion
Following is the programming I am using:

data<-read.csv(file="---",header=T)A<-as.matrix(data$DODGX)B<-as.matrix(data$TRMCX)
nrow<-nrow(A)A1<-matrix(NA,nrow,1)A2<-matrix(NA,nrow,1)A3<-matrix(NA,nrow,1)A4<-matrix(NA,nrow,1)A5<-matrix(NA,nrow,1)A1[2:nrow,1]<-A[1:(nrow-1),1]A2[3:nrow,1]<-A[1:(nrow-2),1]A3[4:nrow,1]<-A[1:(nrow-3),1]A4[5:nrow,1]<-A[1:(nrow-4),1]A5[6:nrow,1]<-A[1:(nrow-5),1]nrow<-nrow(B)B1<-matrix(NA,nrow,1)B2<-matrix(NA,nrow,1)B3<-matrix(NA,nrow,1)B4<-matrix(NA,nrow,1)B5<-matrix(NA,nrow,1)B1[2:nrow,1]<-B[1:(nrow-1),1]B2[3:nrow,1]<-B[1:(nrow-2),1]B3[4:nrow,1]<-B[1:(nrow-3),1]B4[5:nrow,1]<-B[1:(nrow-4),1]B5[6:nrow,1]<-B[1:(nrow-5),1]
reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)


Kindly guide me in this regard.
Thanks.
Saba

From dwinsemius at comcast.net  Fri Dec 18 03:11:39 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 17 Dec 2015 18:11:39 -0800
Subject: [R] Error in linear regression
In-Reply-To: <797805015.300917.1450386804117.JavaMail.yahoo@mail.yahoo.com>
References: <797805015.300917.1450386804117.JavaMail.yahoo.ref@mail.yahoo.com>
	<797805015.300917.1450386804117.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <11CCC265-93D4-4FBA-8BED-AFCFF51B2CC4@comcast.net>


> On Dec 17, 2015, at 1:13 PM, Saba Sehrish via R-help <r-help at r-project.org> wrote:
> 
> Hi I am trying to apply linear regression on the attached data

The is no attached data; please read the posting guide. Do not post with .csv or .doc files. You can have commas as separators but an attachment must have a .txt extension.

> of two variables (DODGX, TRMCX) in R by taking into account time lag=5 for both of them. Each time I run this command, it gives me following error:
> Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :   NA/NaN/Inf in 'y'In addition: Warning message:In model.response(mf, "numeric") : NAs introduced by coercion
> Following is the programming I am using:
> 
> data<-read.csv(file="---",header=T)A<-as.matrix(data$DODGX)B<-as.matrix(data$TRMCX)
> nrow<-nrow(A)A1<-matrix(NA,nrow,1)A2<-matrix(NA,nrow,1)A3<-matrix(NA,nrow,1)A4<-matrix(NA,nrow,1)A5<-matrix(NA,nrow,1)A1[2:nrow,1]<-A[1:(nrow-1),1]A2[3:nrow,1]<-A[1:(nrow-2),1]A3[4:nrow,1]<-A[1:(nrow-3),1]A4[5:nrow,1]<-A[1:(nrow-4),1]A5[6:nrow,1]<-A[1:(nrow-5),1]nrow<-nrow(B)B1<-matrix(NA,nrow,1)B2<-matrix(NA,nrow,1)B3<-matrix(NA,nrow,1)B4<-matrix(NA,nrow,1)B5<-matrix(NA,nrow,1)B1[2:nrow,1]<-B[1:(nrow-1),1]B2[3:nrow,1]<-B[1:(nrow-2),1]B3[4:nrow,1]<-B[1:(nrow-3),1]B4[5:nrow,1]<-B[1:(nrow-4),1]B5[6:nrow,1]<-B[1:(nrow-5),1]
> reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)

I do not see the usual html delted message but nonetheless your code has arrived without any linebreaks. Linebreaks are syntactically necessary. So pleas learn to post with plain text in a format that does mangle the ability of humans to read this code.


-- 
David Winsemius
Alameda, CA, USA


From boris.steipe at utoronto.ca  Fri Dec 18 03:59:57 2015
From: boris.steipe at utoronto.ca (Boris Steipe)
Date: Thu, 17 Dec 2015 21:59:57 -0500
Subject: [R] Converting from Continuous 2D Points to Continuous 2D
	Vectors
In-Reply-To: <CY1PR0101MB1004E533C2B3AA381E59BE3DABE00@CY1PR0101MB1004.prod.exchangelabs.com>
References: <mailman.0.1450030036.31803.r-help@r-project.org>
	<CY1PR0101MB1004E533C2B3AA381E59BE3DABE00@CY1PR0101MB1004.prod.exchangelabs.com>
Message-ID: <7BAE980E-A495-4661-8FFC-386391D3F5EE@utoronto.ca>

Ok. And what is the problem now?

B.



On Dec 17, 2015, at 4:25 PM, Sidoti, Salvatore A. <sidoti.23 at buckeyemail.osu.edu> wrote:

> Greetings!
> 
> I have a fairly large dataframe (df) with pathing information in the form of continuous x,y coordinates:
> 
> df$x
> df$y
> 
> With these data, I would like to:
> 1. Calculate a set of continuous vectors
> 2. Determine the angle between each of these vectors (in degrees)
> 3. Count the number of angles in the dataframe that meet a certain threshold (i.e. <90?)
> 
> Here's what I've come up with so far:
> 
> ### Function that calculates the angle between two vectors in 2D space:
> 
> angle <- function(x,y){ # x and y are vectors
>  dot.prod <- x%*%y 
>  norm.x <- norm(x,type="2")
>  norm.y <- norm(y,type="2")
>  theta <- acos(dot.prod / (norm.x * norm.y))
>  (180*as.numeric(theta))/pi # returns the angle in degrees
> }
> 
> ### Test the function:
> x <- as.matrix(c(2,1))
> y <- as.matrix(c(1,2))
> angle(t(x),y)
> [1] 36.8699
> 
> Thank you!
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From sabasehrish at yahoo.com  Fri Dec 18 04:58:39 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Fri, 18 Dec 2015 03:58:39 +0000 (UTC)
Subject: [R] Error in linear regression
In-Reply-To: <11CCC265-93D4-4FBA-8BED-AFCFF51B2CC4@comcast.net>
References: <11CCC265-93D4-4FBA-8BED-AFCFF51B2CC4@comcast.net>
Message-ID: <1262917431.427810.1450411119306.JavaMail.yahoo@mail.yahoo.com>

Hi
Please find the attachment with (.txt) extension and I hope the command is visible now.
library(lmtest)data<-read.csv(file="---",header=T,sep=",")A<-as.matrix(data$DODGX)B<-as.matrix(data$TRMCX)
nrow<-nrow(A)A1<-matrix(NA,nrow,1)A2<-matrix(NA,nrow,1)
A3<-matrix(NA,nrow,1)
A4<-matrix(NA,nrow,1)
A5<-matrix(NA,nrow,1)
A1[2:nrow,1]<-A[1:(nrow-1),1]A2[3:nrow,1]<-A[1:(nrow-2),1]
A3[4:nrow,1]<-A[1:(nrow-3),1]
A4[5:nrow,1]<-A[1:(nrow-4),1]
A5[6:nrow,1]<-A[1:(nrow-5),1]
nrow<-nrow(B)B1<-matrix(NA,nrow,1)B2<-matrix(NA,nrow,1)
B3<-matrix(NA,nrow,1)
B4<-matrix(NA,nrow,1)
B5<-matrix(NA,nrow,1)
B1[2:nrow,1]<-B[1:(nrow-1),1]B2[3:nrow,1]<-B[1:(nrow-2),1]
B3[4:nrow,1]<-B[1:(nrow-3),1]
B4[5:nrow,1]<-B[1:(nrow-4),1]
B5[6:nrow,1]<-B[1:(nrow-5),1]
reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)


Following error is occurring:
Error in lm.fit(x,y,offset = offset, singular.ok = singular.ok, ...) :?NA/NaN/lnf in 'y'In addition: Warning message:In model.response(mf,"numeric") : NAs introduced by coercion
RegardsSaba

 

    On Friday, 18 December 2015, 15:11, David Winsemius <dwinsemius at comcast.net> wrote:
 

 
> On Dec 17, 2015, at 1:13 PM, Saba Sehrish via R-help <r-help at r-project.org> wrote:
> 
> Hi I am trying to apply linear regression on the attached data

The is no attached data; please read the posting guide. Do not post with .csv or .doc files. You can have commas as separators but an attachment must have a .txt extension.

> of two variables (DODGX, TRMCX) in R by taking into account time lag=5 for both of them. Each time I run this command, it gives me following error:
> Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :? NA/NaN/Inf in 'y'In addition: Warning message:In model.response(mf, "numeric") : NAs introduced by coercion
> Following is the programming I am using:
> 
> data<-read.csv(file="---",header=T)A<-as.matrix(data$DODGX)B<-as.matrix(data$TRMCX)
> nrow<-nrow(A)A1<-matrix(NA,nrow,1)A2<-matrix(NA,nrow,1)A3<-matrix(NA,nrow,1)A4<-matrix(NA,nrow,1)A5<-matrix(NA,nrow,1)A1[2:nrow,1]<-A[1:(nrow-1),1]A2[3:nrow,1]<-A[1:(nrow-2),1]A3[4:nrow,1]<-A[1:(nrow-3),1]A4[5:nrow,1]<-A[1:(nrow-4),1]A5[6:nrow,1]<-A[1:(nrow-5),1]nrow<-nrow(B)B1<-matrix(NA,nrow,1)B2<-matrix(NA,nrow,1)B3<-matrix(NA,nrow,1)B4<-matrix(NA,nrow,1)B5<-matrix(NA,nrow,1)B1[2:nrow,1]<-B[1:(nrow-1),1]B2[3:nrow,1]<-B[1:(nrow-2),1]B3[4:nrow,1]<-B[1:(nrow-3),1]B4[5:nrow,1]<-B[1:(nrow-4),1]B5[6:nrow,1]<-B[1:(nrow-5),1]
> reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)

I do not see the usual html delted message but nonetheless your code has arrived without any linebreaks. Linebreaks are syntactically necessary. So pleas learn to post with plain text in a format that does mangle the ability of humans to read this code.


-- 
David Winsemius
Alameda, CA, USA


  
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Monthly data.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151218/3be240bc/attachment.txt>

From sabasehrish at yahoo.com  Fri Dec 18 05:12:23 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Fri, 18 Dec 2015 04:12:23 +0000 (UTC)
Subject: [R] Error-linear regression
References: <885979758.415377.1450411943266.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <885979758.415377.1450411943266.JavaMail.yahoo@mail.yahoo.com>

Hi?
I am trying to apply linear regression on the attached data of two variables (DODGX, TRMCX) in R by taking time lag=5 for both of them. Each time I run this command, it gives me following error:

Error in lm.fit(x,y,offset = offset, singular.ok = singular.ok, ...) :?
NA/NaN/lnf in 'y'?
In addition:?Warning message:?
In model.response(mf,"numeric") : NAs introduced by coercion

Following is the command:

library(lmtest)data<-read.csv(file="---",header=T,sep=",")
A<-as.matrix(data$DODGX)?
B<-as.matrix(data$TRMCX)?

nrow<-nrow(A)?
A1<-matrix(NA,nrow,1)?
A2<-matrix(NA,nrow,1)

A3<-matrix(NA,nrow,1)

A4<-matrix(NA,nrow,1)

A5<-matrix(NA,nrow,1)

A1[2:nrow,1]<-A[1:(nrow-1),1]?
A2[3:nrow,1]<-A[1:(nrow-2),1]

A3[4:nrow,1]<-A[1:(nrow-3),1]

A4[5:nrow,1]<-A[1:(nrow-4),1]

A5[6:nrow,1]<-A[1:(nrow-5),1]

nrow<-nrow(B)?
B1<-matrix(NA,nrow,1)?
B2<-matrix(NA,nrow,1)

B3<-matrix(NA,nrow,1)

B4<-matrix(NA,nrow,1)

B5<-matrix(NA,nrow,1)

B1[2:nrow,1]<-B[1:(nrow-1),1]?
B2[3:nrow,1]<-B[1:(nrow-2),1]

B3[4:nrow,1]<-B[1:(nrow-3),1]?
B4[5:nrow,1]<-B[1:(nrow-4),1]
B5[6:nrow,1]<-B[1:(nrow-5),1]?

reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)?
reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)



Regards
Saba
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: Monthly data.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151218/27cef314/attachment.txt>

From dwinsemius at comcast.net  Fri Dec 18 07:28:28 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 17 Dec 2015 22:28:28 -0800
Subject: [R] Error in linear regression
In-Reply-To: <1262917431.427810.1450411119306.JavaMail.yahoo@mail.yahoo.com>
References: <11CCC265-93D4-4FBA-8BED-AFCFF51B2CC4@comcast.net>
	<1262917431.427810.1450411119306.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <770B5A19-CB9F-4157-8123-9CD4F63CE33A@comcast.net>


> On Dec 17, 2015, at 7:58 PM, Saba Sehrish <sabasehrish at yahoo.com> wrote:
> 
> Hi
> 
> Please find the attachment with (.txt) extension and I hope the command is visible now.
> 
> library(lmtest)

Why is this library loaded? I see no calls to functions from that package.

> data<-read.csv(file="---",header=T,sep=",")

The first two lines of that file ARE:

DODGX,TRMCX
"739,171,876.13","-30,023,111.44"

So R will not unpack the character values that appear to be inside the comma separated values. The first value will be "739,171,876.13" and the second one will be "-30,023,111.44"

I'm guessing this is not what you expected. Learn to examine your data before diving into the manipulation and analysis of it.


> A<-as.matrix(data$DODGX)
> B<-as.matrix(data$TRMCX)

Shouldn't the building of a matrix have dimensions?

> 
> nrow<-nrow(A)
> A1<-matrix(NA,nrow,1)
> A2<-matrix(NA,nrow,1)
> A3<-matrix(NA,nrow,1)
> A4<-matrix(NA,nrow,1)
> A5<-matrix(NA,nrow,1)

You should describe in English what you are trying to accomplish.

-- 
David

> A1[2:nrow,1]<-A[1:(nrow-1),1]
> A2[3:nrow,1]<-A[1:(nrow-2),1]
> A3[4:nrow,1]<-A[1:(nrow-3),1]
> A4[5:nrow,1]<-A[1:(nrow-4),1]
> A5[6:nrow,1]<-A[1:(nrow-5),1]
> nrow<-nrow(B)
> B1<-matrix(NA,nrow,1)
> B2<-matrix(NA,nrow,1)
> B3<-matrix(NA,nrow,1)
> B4<-matrix(NA,nrow,1)
> B5<-matrix(NA,nrow,1)
> B1[2:nrow,1]<-B[1:(nrow-1),1]
> B2[3:nrow,1]<-B[1:(nrow-2),1]
> B3[4:nrow,1]<-B[1:(nrow-3),1]
> B4[5:nrow,1]<-B[1:(nrow-4),1]
> B5[6:nrow,1]<-B[1:(nrow-5),1]
> 
> reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)
> reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)
> 
> 
> 
> Following error is occurring:
> 
> Error in lm.fit(x,y,offset = offset, singular.ok = singular.ok, ...) :
>  NA/NaN/lnf in 'y'
> In addition: Warning message:
> In model.response(mf,"numeric") : NAs introduced by coercion
> 
> Regards
> Saba
> 
> 
> 
> 
> On Friday, 18 December 2015, 15:11, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> 
> 
> > On Dec 17, 2015, at 1:13 PM, Saba Sehrish via R-help <r-help at r-project.org> wrote:
> > 
> > Hi I am trying to apply linear regression on the attached data
> 
> The is no attached data; please read the posting guide. Do not post with .csv or .doc files. You can have commas as separators but an attachment must have a .txt extension.
> 
> 
> > of two variables (DODGX, TRMCX) in R by taking into account time lag=5 for both of them. Each time I run this command, it gives me following error:
> > Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :  NA/NaN/Inf in 'y'In addition: Warning message:In model.response(mf, "numeric") : NAs introduced by coercion
> > Following is the programming I am using:
> > 
> > data<-read.csv(file="---",header=T)A<-as.matrix(data$DODGX)B<-as.matrix(data$TRMCX)
> > nrow<-nrow(A)A1<-matrix(NA,nrow,1)A2<-matrix(NA,nrow,1)A3<-matrix(NA,nrow,1)A4<-matrix(NA,nrow,1)A5<-matrix(NA,nrow,1)A1[2:nrow,1]<-A[1:(nrow-1),1]A2[3:nrow,1]<-A[1:(nrow-2),1]A3[4:nrow,1]<-A[1:(nrow-3),1]A4[5:nrow,1]<-A[1:(nrow-4),1]A5[6:nrow,1]<-A[1:(nrow-5),1]nrow<-nrow(B)B1<-matrix(NA,nrow,1)B2<-matrix(NA,nrow,1)B3<-matrix(NA,nrow,1)B4<-matrix(NA,nrow,1)B5<-matrix(NA,nrow,1)B1[2:nrow,1]<-B[1:(nrow-1),1]B2[3:nrow,1]<-B[1:(nrow-2),1]B3[4:nrow,1]<-B[1:(nrow-3),1]B4[5:nrow,1]<-B[1:(nrow-4),1]B5[6:nrow,1]<-B[1:(nrow-5),1]
> > reg1<-lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)reg2<-lm(B~B1+B2+B3+B4+B5+A1+A2+A3+A4+A5)
> 
> 
> I do not see the usual html delted message but nonetheless your code has arrived without any linebreaks. Linebreaks are syntactically necessary. So pleas learn to post with plain text in a format that does mangle the ability of humans to read this code.
> 
> 
> -- 
> David Winsemius
> Alameda, CA, USA
> 
> 
> 
> <Monthly data.txt>

David Winsemius
Alameda, CA, USA


From marna.wagley at gmail.com  Fri Dec 18 04:33:02 2015
From: marna.wagley at gmail.com (Marna Wagley)
Date: Thu, 17 Dec 2015 19:33:02 -0800
Subject: [R] Would you please help me to create a table in R?
Message-ID: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>

Hi R users,
I am struggling to create a table in R. I did in Excel but I have a lots of
data and thought it might be easy in  R  but  I am new in R. How to get
"output table" for this example data?

This is an example.

#####
raw.data<-structure(list(Time1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L), .Label = "SiteA", class = "factor"), Time2 =
structure(c(1L,
4L, 2L, 1L, 2L, 1L, 2L, 1L, 3L, 3L, 3L), .Label = c("0", "SiteA",
"SiteB", "SiteC"), class = "factor"), Time3 = structure(c(2L,
2L, 2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 3L), .Label = c("0", "SiteA",
"SiteB"), class = "factor")), .Names = c("Time1", "Time2", "Time3"
), class = "data.frame", row.names = c(NA, -11L))

raw.data

#####
table.format<-structure(list(Time1 = structure(c(NA, NA, NA, NA, NA, 1L,
NA,
NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = "Released A", class =
"factor"),
    Time2 = structure(c(NA, NA, 2L, NA, NA, NA, 3L, NA, NA, NA,
    4L, NA, NA, NA, 1L, NA), .Label = c("Dead", "Re-captured.at.A",
    "Re-captured.at.B", "Re-captured.at.C"), class = "factor"),
    Time3 = structure(c(2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L,
    4L, 1L, 2L, 3L, 4L, 1L), .Label = c("Dead", "Re-captured.at.A",
    "Re-captured.at.B", "Re-captured.at.C"), class = "factor")), .Names =
c("Time1",
"Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
))

table.format

###
output<-structure(list(Time1 = c(NA, NA, NA, NA, NA, 11L, NA, NA, NA,
NA, NA, NA, NA, NA, NA, NA), Time2 = c(NA, NA, 3L, NA, NA, NA,
3L, NA, NA, NA, 1L, NA, NA, NA, 4L, NA), Time3 = c(2L, NA, NA,
1L, NA, 2L, NA, 1L, 1L, NA, NA, NA, 1L, 1L, NA, 2L)), .Names = c("Time1",
"Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
))

output

I want to get the table like "output". Any possibility to get it in R?

I will really appreciate for your help. I am struggling to create  this
type of table.


Sincerely,
Marna

	[[alternative HTML version deleted]]


From saikatduttachowdhury at gmail.com  Fri Dec 18 04:52:41 2015
From: saikatduttachowdhury at gmail.com (Saikat Dutta Chowdhury)
Date: Fri, 18 Dec 2015 09:22:41 +0530
Subject: [R] Help needed in plotting 2d histogram
Message-ID: <CAHqQAzU06ty3GRWwQ5UY2SYAg_cg=Tt0iMhHTeDPFNwTMqeKkA@mail.gmail.com>

Hello.
I have more than one phi-psi time series data files each having 50000 time
points.
I am trying to create density plots (2d histograms) for each and then I
would like to compare them. But to be able to compare one plot with the
other I need to have similar colour densities (i.e colour gradient (which
depends on counts) will be same for each). But I am unable to control the
colour gradient and thus unable to compare the plots.

I was using ggplot2 library. After getting the phi-psi data into R I was
doing the following steps:


*df <- data.frame(phi[,2],psi[,2])*

*p <- ggplot(df,aes(phi[,2],psi[,2]),ymin=c(-180,180),xmin=c(-180,180))*
*p + stat_bin2d(bins=100)*

after executing these steps I am getting the density plot with the colour
gradient legend at the left side with a certain range (say 0-200). I want
to control this colour gradient parameter so that I can fix this range to
0-600 for all the plots and thus be able to compare them.

Any help would be appreciated.

With regards,
-- 
Saikat Dutta Chowdhury
Research Fellow
University of Calcutta
India
Mobile: 8017650842

	[[alternative HTML version deleted]]


From chanmo5752 at gmail.com  Fri Dec 18 05:06:19 2015
From: chanmo5752 at gmail.com (Bradley Wolf)
Date: Thu, 17 Dec 2015 22:06:19 -0600
Subject: [R] Cannot subset a specific mean from this function
Message-ID: <CAAT8Cgc1N1yt0RVh4RiLbG-DPXnkT74169C7NnXPHBma9+wL4g@mail.gmail.com>

HI,
  I am very new to R (and programming in general) and am taking a class in
it now. I am getting creamed. I want the last part of the function
statement below to give me the mean of the data set of nitrates. When I do
it I get an error statement stating:

Error in pollutantmean$nitrate :
  object of type 'closure' is not subsettable

I am bringing in 332 records. I want the mean of nitrates for 52 of those
records. Any suggestions would be helpful.

pollutantmean <- function(specdata, nitrate,id = 1:332){
  files<-list.files(specdata, full.names = TRUE)
  dat<-data.frame()
  for (i in 280:332)
  dat <- rbind(dat, read.csv(files_full[i]))
}
str(pollutantmean)
mean(pollutantmean$nitrate, na.rm = TRUE)

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Dec 18 08:08:09 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 18 Dec 2015 18:08:09 +1100
Subject: [R] Help needed in plotting 2d histogram
In-Reply-To: <CAHqQAzU06ty3GRWwQ5UY2SYAg_cg=Tt0iMhHTeDPFNwTMqeKkA@mail.gmail.com>
References: <CAHqQAzU06ty3GRWwQ5UY2SYAg_cg=Tt0iMhHTeDPFNwTMqeKkA@mail.gmail.com>
Message-ID: <CA+8X3fWm=djDw1gMOrd0w43gdegVUCaYcR71haAS-K4q99N6NQ@mail.gmail.com>

Hi Saikat,
I don't know whether this can be done with ggplot, but if you don't get
another answer, have a look at the second example for the "barp" function
in the plotrix package. This shows how to get a specified range on the
color legend using the "xrange" argument.

Jim


On Fri, Dec 18, 2015 at 2:52 PM, Saikat Dutta Chowdhury <
saikatduttachowdhury at gmail.com> wrote:

> Hello.
> I have more than one phi-psi time series data files each having 50000 time
> points.
> I am trying to create density plots (2d histograms) for each and then I
> would like to compare them. But to be able to compare one plot with the
> other I need to have similar colour densities (i.e colour gradient (which
> depends on counts) will be same for each). But I am unable to control the
> colour gradient and thus unable to compare the plots.
>
> I was using ggplot2 library. After getting the phi-psi data into R I was
> doing the following steps:
>
>
> *df <- data.frame(phi[,2],psi[,2])*
>
> *p <- ggplot(df,aes(phi[,2],psi[,2]),ymin=c(-180,180),xmin=c(-180,180))*
> *p + stat_bin2d(bins=100)*
>
> after executing these steps I am getting the density plot with the colour
> gradient legend at the left side with a certain range (say 0-200). I want
> to control this colour gradient parameter so that I can fix this range to
> 0-600 for all the plots and thus be able to compare them.
>
> Any help would be appreciated.
>
> With regards,
> --
> Saikat Dutta Chowdhury
> Research Fellow
> University of Calcutta
> India
> Mobile: 8017650842
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Dec 18 08:38:26 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 18 Dec 2015 18:38:26 +1100
Subject: [R] Would you please help me to create a table in R?
In-Reply-To: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>
References: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>
Message-ID: <CA+8X3fVVskOftnHtYOsqmvnu1HNBWP6e3rYufrCkufgnGXbdrw@mail.gmail.com>

Hi Marna,
A bit hard to understand. If raw.data is a record of 11 individuals
released at site A at Time 1 and recaptured at either A or B or neither at
Time2 or Time3, it doesn't seem to bear any consistent relationship to the
numeric coding in table.format or the output at the bottom. Could you
explain what the correspondence between the tables is?

Jim


On Fri, Dec 18, 2015 at 2:33 PM, Marna Wagley <marna.wagley at gmail.com>
wrote:

> Hi R users,
> I am struggling to create a table in R. I did in Excel but I have a lots of
> data and thought it might be easy in  R  but  I am new in R. How to get
> "output table" for this example data?
>
> This is an example.
>
> #####
> raw.data<-structure(list(Time1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L), .Label = "SiteA", class = "factor"), Time2 =
> structure(c(1L,
> 4L, 2L, 1L, 2L, 1L, 2L, 1L, 3L, 3L, 3L), .Label = c("0", "SiteA",
> "SiteB", "SiteC"), class = "factor"), Time3 = structure(c(2L,
> 2L, 2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 3L), .Label = c("0", "SiteA",
> "SiteB"), class = "factor")), .Names = c("Time1", "Time2", "Time3"
> ), class = "data.frame", row.names = c(NA, -11L))
>
> raw.data
>
> #####
> table.format<-structure(list(Time1 = structure(c(NA, NA, NA, NA, NA, 1L,
> NA,
> NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = "Released A", class =
> "factor"),
>     Time2 = structure(c(NA, NA, 2L, NA, NA, NA, 3L, NA, NA, NA,
>     4L, NA, NA, NA, 1L, NA), .Label = c("Dead", "Re-captured.at.A",
>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor"),
>     Time3 = structure(c(2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L,
>     4L, 1L, 2L, 3L, 4L, 1L), .Label = c("Dead", "Re-captured.at.A",
>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor")), .Names =
> c("Time1",
> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
> ))
>
> table.format
>
> ###
> output<-structure(list(Time1 = c(NA, NA, NA, NA, NA, 11L, NA, NA, NA,
> NA, NA, NA, NA, NA, NA, NA), Time2 = c(NA, NA, 3L, NA, NA, NA,
> 3L, NA, NA, NA, 1L, NA, NA, NA, 4L, NA), Time3 = c(2L, NA, NA,
> 1L, NA, 2L, NA, 1L, 1L, NA, NA, NA, 1L, 1L, NA, 2L)), .Names = c("Time1",
> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
> ))
>
> output
>
> I want to get the table like "output". Any possibility to get it in R?
>
> I will really appreciate for your help. I am struggling to create  this
> type of table.
>
>
> Sincerely,
> Marna
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Fri Dec 18 08:54:49 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 18 Dec 2015 18:54:49 +1100
Subject: [R] Cannot subset a specific mean from this function
In-Reply-To: <CAAT8Cgc1N1yt0RVh4RiLbG-DPXnkT74169C7NnXPHBma9+wL4g@mail.gmail.com>
References: <CAAT8Cgc1N1yt0RVh4RiLbG-DPXnkT74169C7NnXPHBma9+wL4g@mail.gmail.com>
Message-ID: <CA+8X3fVF43=3J_JAozVFmp_cCbPw_0_wwjGF4CAb7eYHxJmRPQ@mail.gmail.com>

Hi Bradley,
I think I can see one or two reasons why you are getting creamed.

First, pollutantmean is a function, and there is no method for taking the
mean of a function.

Second, nitrate is unlikely to be an extractable element of pollutantmean
given the above code (that's the error).

I suspect that you have been asked to read the last 53 files of a set of
332.
When reading each one, extract the values of the element "nitrate" from
each dataframe that is created.
concatenate these "nitrate" values as you go, resulting in a vector of
nitrate concentration values
take the mean of that vector, probably removing any NA values
then return that mean value from the function.

While I'm not going to do the job for you, your assigned the names of the
files to the variable "files" and then tried to read from a variable named
"files_full". That won't work.

You then tried to merge the dataframes by adding each successive one to the
previous ones.

This will work, but you haven't returned the result from the function, or
assigned it to another variable.

You then ask for the structure of "pollutantmean" which is a function, not
a data frame

If you stick with the data frame accumulation, I suggest putting a:

return(dat)

as the last line of the function, and assigning it to something:

pollutant.df<-pollutantmean(...)

Then look at the structure of pollutant.df and if you are lucky, there will
be a "nitrate" element.

Jim




On Fri, Dec 18, 2015 at 3:06 PM, Bradley Wolf <chanmo5752 at gmail.com> wrote:

> HI,
>   I am very new to R (and programming in general) and am taking a class in
> it now. I am getting creamed. I want the last part of the function
> statement below to give me the mean of the data set of nitrates. When I do
> it I get an error statement stating:
>
> Error in pollutantmean$nitrate :
>   object of type 'closure' is not subsettable
>
> I am bringing in 332 records. I want the mean of nitrates for 52 of those
> records. Any suggestions would be helpful.
>
> pollutantmean <- function(specdata, nitrate,id = 1:332){
>   files<-list.files(specdata, full.names = TRUE)
>   dat<-data.frame()
>   for (i in 280:332)
>   dat <- rbind(dat, read.csv(files_full[i]))
> }
> str(pollutantmean)
> mean(pollutantmean$nitrate, na.rm = TRUE)
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From Marios.BARLAS at cea.fr  Fri Dec 18 10:09:06 2015
From: Marios.BARLAS at cea.fr (BARLAS Marios 247554)
Date: Fri, 18 Dec 2015 09:09:06 +0000
Subject: [R] Numerical differentiation of a vector of values
Message-ID: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>

Hi everyone,

I have sets of experimental values representing I-V curves. I want to get the derivative of such numerical data but in r there doesn' seem to be a premade function. I can write something of my own, but I was wondering if there is a standard package out there?

Thanks in advance!


From mohsenhs82 at yahoo.com  Fri Dec 18 10:57:38 2015
From: mohsenhs82 at yahoo.com (mohsen hs)
Date: Fri, 18 Dec 2015 09:57:38 +0000 (UTC)
Subject: [R] qqPlot vs qqcomp
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>

Hello all
I am using the following two commands and get two completely different qq plots while meanlog and sdlog?are almost the same. Any help is highly appreciated.?dev.new() ; qqPlot(serving, dist = "lnorm", estimate.params = TRUE, add.line = TRUE)

fitln <- fitdist(serving, "lnorm")
dev.new();qqcomp(fitln)
Many thanksMohsen
	[[alternative HTML version deleted]]


From klebyn at yahoo.com.br  Fri Dec 18 11:31:13 2015
From: klebyn at yahoo.com.br (Cleber Borges)
Date: Fri, 18 Dec 2015 08:31:13 -0200
Subject: [R] How to use the options "usecommand" and "command" of
	tktable?
In-Reply-To: <56734E46.6050807@yahoo.com.br>
References: <56734E46.6050807@yahoo.com.br>
Message-ID: <5673E071.6000404@yahoo.com.br>

# only a update the R code (with strange behavior)
library( tcltk ); tclRequire( "Tktable" )
top <- tktoplevel()
tcl('variable', 'myarray')
tcl('array', 'unset', 'myarray')
x <- 0
tabCmd <- function() {
     x <<- x + 1
     return( as.tclObj( paste(x) ) )
}
tab <- tkwidget( top, 'table', rows=2, cols=2, command=tabCmd )
tcl('pack', tab, expand=TRUE, fill='both' )
tcl( tab, 'index', 'topleft' )
tcl( tab, 'index', 'bottomright' )
tcl( tab, 'get', '0,0' )
tcl( tab, 'get', '0,1' )
tcl( tab, 'get', '1,0' )
tcl( tab, 'get', '1,1' )

x
Em 17/12/2015 22:07, Cleber N.Borges escreveu:
> How to capture the output from the "command" option of tktable and how 
> to send input to it?
>
> As far as I understand, the most appropriate way to use the tktable is 
> through the flag usecommand and command.
>
> It fires the string information and cell coordinates and wait for a 
> new string to set a new value.
>
> It's I understood by example in pure TCL given in reference [1].
>
> Is there how to translate this code to R?
>
> I believe that the signs of tktable not reach the level of R and will 
> need to be accessed by the lower level commands. But how?
>
> Thank you in advance for dedicated attention.
>
> cleber
>
> ###
> [1] - http://dbaspot.com/object/showthread.php?t=785221&pagenumber=
>
> ### my R code
> library( tcltk )
> tclRequire( "Tktable" )
> tclServiceMode( FALSE )
> top <- tktoplevel()
>
> TableCommand <- function( x=NULL ) {
>     cat( class( x ), '\n' )
>     cat( x , '\n' )
> }
>
> tab <- tkwidget( top, 'table', rows=2, cols=2, usecommand=TRUE, 
> command=TableCommand )
> tcl('pack', tab, expand=TRUE, fill='both' )
> tclServiceMode( TRUE )
>
> ### TCL from [1]
> package require Tktable
>
> namespace eval ::test {}
>
> proc ::test::build_table {} {
>
> catch {destroy .test}
> variable top [toplevel .test]
>
> variable myarray
> array unset myarray
>
> variable table [table $top.table \
> -rows 5 \
> -height 5 \
> -cols 2 \
> -command "[namespace current]::TableCommand %S %r %c"]
>
> pack $table -expand true -fill both
> $table tag config complete -background blue
> }
>
> proc ::test::TableCommand {S r c} {
>
> variable table
> variable myarray
>
> if {$S != ""} {
> # I only want integers in my cells!
> puts "\"$S\" into $r,$c ?"
>
> if {[string is integer -strict $S]} {
> $table tag cell complete "$r,$c"
> set myarray($r,$c) $S
> } else {
> $table tag cell !complete "$r,$c"
> array unset myarray $r,$c
> }
> }
> if {[info exists myarray($r,$c)]} {
> return $myarray($r,$c)
> }
> return "$r,$c"
> }
>
> ::test::build_table
>
>
>
>
> ---
> Este email foi escaneado pelo Avast antiv?rus.
> https://www.avast.com/antivirus
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


---
Este email foi escaneado pelo Avast antiv?rus.
https://www.avast.com/antivirus


From amelia_marsh08 at yahoo.com  Fri Dec 18 12:20:47 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Fri, 18 Dec 2015 11:20:47 +0000 (UTC)
Subject: [R] Changing X axis values
References: <1195407133.715890.1450437647879.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1195407133.715890.1450437647879.JavaMail.yahoo@mail.yahoo.com>

Dear Forum,

Assuming I need to plot a graph. In the code I have defined X axis range as 



xlim=c(0,18000)


In the plot, the values visible w.r.t X axis are 0, 5000, 10000, 18000.

To improve the graph clarity, is there any way I can show the values of X axis as 0, 1000, 2000, 3000, 4000, 5000 .......18000 i.e. the values increase by 1000 instead of 5000.

Regards

Amelia


From drjimlemon at gmail.com  Fri Dec 18 12:30:12 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 18 Dec 2015 22:30:12 +1100
Subject: [R] Changing X axis values
In-Reply-To: <1195407133.715890.1450437647879.JavaMail.yahoo@mail.yahoo.com>
References: <1195407133.715890.1450437647879.JavaMail.yahoo.ref@mail.yahoo.com>
	<1195407133.715890.1450437647879.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <CA+8X3fWo+iC5prUbzM1GL0+tuEAzPnPrvDe8v47wLssYbfidYA@mail.gmail.com>

Hi Amelia,
The usual way is:

plot(...,xaxt="n")
axis(1,at=seq(0,18000,by=1000)

However, you will get overlapping labels unless you use a small font or a
large graphics device. You may want to look at the staxlab function in the
plotrix package.

Jim


On Fri, Dec 18, 2015 at 10:20 PM, Amelia Marsh via R-help <
r-help at r-project.org> wrote:

> Dear Forum,
>
> Assuming I need to plot a graph. In the code I have defined X axis range as
>
>
>
> xlim=c(0,18000)
>
>
> In the plot, the values visible w.r.t X axis are 0, 5000, 10000, 18000.
>
> To improve the graph clarity, is there any way I can show the values of X
> axis as 0, 1000, 2000, 3000, 4000, 5000 .......18000 i.e. the values
> increase by 1000 instead of 5000.
>
> Regards
>
> Amelia
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Fri Dec 18 12:52:14 2015
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Fri, 18 Dec 2015 11:52:14 +0000
Subject: [R] Changing X axis values
In-Reply-To: <CA+8X3fWo+iC5prUbzM1GL0+tuEAzPnPrvDe8v47wLssYbfidYA@mail.gmail.com>
References: <1195407133.715890.1450437647879.JavaMail.yahoo.ref@mail.yahoo.com>
	<1195407133.715890.1450437647879.JavaMail.yahoo@mail.yahoo.com>
	<CA+8X3fWo+iC5prUbzM1GL0+tuEAzPnPrvDe8v47wLssYbfidYA@mail.gmail.com>
Message-ID: <5673F36E.4090603@dewey.myzen.co.uk>

Dear Amelia


As well as Jim's excellent advice you may want to look at the las 
parameter which enables you to alter the orientation. Of course you then 
have to lie on your side to read them or turn your monitor through pi/2

On 18/12/2015 11:30, Jim Lemon wrote:
> Hi Amelia,
> The usual way is:
>
> plot(...,xaxt="n")
> axis(1,at=seq(0,18000,by=1000)
>
> However, you will get overlapping labels unless you use a small font or a
> large graphics device. You may want to look at the staxlab function in the
> plotrix package.
>
> Jim
>
>
> On Fri, Dec 18, 2015 at 10:20 PM, Amelia Marsh via R-help <
> r-help at r-project.org> wrote:
>
>> Dear Forum,
>>
>> Assuming I need to plot a graph. In the code I have defined X axis range as
>>
>>
>>
>> xlim=c(0,18000)
>>
>>
>> In the plot, the values visible w.r.t X axis are 0, 5000, 10000, 18000.
>>
>> To improve the graph clarity, is there any way I can show the values of X
>> axis as 0, 1000, 2000, 3000, 4000, 5000 .......18000 i.e. the values
>> increase by 1000 instead of 5000.
>>
>> Regards
>>
>> Amelia
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From petr.pikal at precheza.cz  Fri Dec 18 14:01:05 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Fri, 18 Dec 2015 13:01:05 +0000
Subject: [R] Numerical differentiation of a vector of values
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50053B8@SRVEXCHMBX.precheza.cz>

Hi

I may be completely wrong, but did you consider

?diff

Cheers
Petr

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of BARLAS
> Marios 247554
> Sent: Friday, December 18, 2015 10:09 AM
> To: r-help at r-project.org
> Subject: [R] Numerical differentiation of a vector of values
>
> Hi everyone,
>
> I have sets of experimental values representing I-V curves. I want to
> get the derivative of such numerical data but in r there doesn' seem to
> be a premade function. I can write something of my own, but I was
> wondering if there is a standard package out there?
>
> Thanks in advance!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From malnamalja at gmx.de  Fri Dec 18 14:42:38 2015
From: malnamalja at gmx.de (=?UTF-8?Q?Jannes_M=c3=bcnchow?=)
Date: Fri, 18 Dec 2015 14:42:38 +0100
Subject: [R] paneling spplot and hist
Message-ID: <56740D4E.80805@gmx.de>

Hi Pai,

there are at least two solutions two your problem.

The first solution makes use of R's base graphics instead of using the 
lattice-based spplot function:

library("sp")
library("RColorBrewers")
library("classInt")


# example more or less copied from the Applied Spatial Data Analysis 
with R book

# load the meuse dataset
data(meuse)
# use the xy-columns to transform the data.frame into a
# SpatialPointsDataFrame (SPDF)
coordinates(meuse) <-~ x + y
# specify color palette we are going to use
pal <- RColorBrewer::brewer.pal(5, name = "Blues")
# find fisher class intervals using the fisher-algorithm
q5 <- classIntervals(meuse$zinc, n = 5, style = "fisher")
# find the colors for each point
q5_col <- findColours(q5, pal)

# set up a plot window with one row and two columns (2 panels)
par(mfrow = c(1, 2))
# plot the SPDF into the first panel
plot(meuse, pch = 16, col = q5_col)
# add a legend
legend("topleft", fill = attr(q5Colours, "palette"),
        legend = names(attr(q5Colours, "table")), bty = "n", cex = 0.5)
# plot a histogram into the second panel
hist(meuse at data$copper, main = "Histogram", xlab = "zinc")

The second solution uses the packages lattice, grid and sp.

# using sp, lattice and grid
library("grid")
library("lattice")
grid.newpage()
# set up a plot window with one row and two columns (2 panels)
pushViewport(viewport(layout = grid.layout(1, 2)))
vp1 <- viewport(x = 0, y = 0,
                 height = 1, width = 0.5,
                 just = c("left", "bottom"),
                 name = "left",
                 layout = grid.layout(nrow = 1, ncol = 2))
pushViewport(vp1)
# plot the SPDF into the first panel
p <- spplot(meuse, "zinc")
print(p, newpage = FALSE)
upViewport(1)
vp2 <- viewport(x = 1, y = 0,
                 height = 1, width = 0.5,
                 just = c("right", "bottom"),
                 name = "right")
pushViewport(vp2)
# plot a lattice histogram into the second panel
print(histogram(meuse at data$copper, main = ""), newpage = FALSE)
upViewport(1)


For more information on the grid package and how to use it with spplot, 
you might want to read the package documentation and this blog 
http://teachpress.environmentalinformatics-marburg.de/2013/06/plotting-multiple-plots-on-one-page-with-the-grid-package-3/

Hope this helps,

Jannes


Dec 14, 2015; 8:20pm Debasish Pai Mazumder 
<http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=user_nodes&user=381655> 
Debasish Pai Mazumder 
<http://r.789695.n4.nabble.com/template/NamlServlet.jtp?macro=user_nodes&user=381655> 
paneling spplot and hist

Hi all,

I am new in R. I am trying to panel spplot and hist.

spplot(hspdf, "CDP", col = "white", col.regions = blue2red(20), sp.layout =
list(l2), at = seq(1,10,1), colorkey = list(space = "bottom", labels =
list(labels = paste(seq(1,10,1)), cex = 1.5)), sub = list("CDP", cex = 1.5,
font = 2))

hist(cdp.obsc, col="grey", border="grey", main="CDP", probability=T)
lines(c.breaks, obs.cdp.d, col="blue")
lines(c.breaks, obs.cdp.e, col="red")


I have tried with par(mfrow=c(2,1))and layout(matrix...., both don't work.

Any advice?

with regards
-Pai

         [[alternative HTML version deleted]]

______________________________________________
[hidden email] 
<http://r.789695.n4.nabble.com/user/SendEmail.jtp?type=node&node=4715547&i=0> 
mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From amelia_marsh08 at yahoo.com  Fri Dec 18 13:09:43 2015
From: amelia_marsh08 at yahoo.com (Amelia Marsh)
Date: Fri, 18 Dec 2015 12:09:43 +0000 (UTC)
Subject: [R] Changing X axis values
In-Reply-To: <CA+8X3fWo+iC5prUbzM1GL0+tuEAzPnPrvDe8v47wLssYbfidYA@mail.gmail.com>
References: <CA+8X3fWo+iC5prUbzM1GL0+tuEAzPnPrvDe8v47wLssYbfidYA@mail.gmail.com>
Message-ID: <645977162.736409.1450440583853.JavaMail.yahoo@mail.yahoo.com>

Dear Sir
Thanks a lot for your great help. I had tried the argument by = 1000, but wasn't aware of "seq". Thanks again.
With regards
Amelia 

    On Friday, 18 December 2015 5:00 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
 

 Hi Amelia,The usual way is:
plot(...,xaxt="n")axis(1,at=seq(0,18000,by=1000)
However, you will get overlapping labels unless you use a small font or a large graphics device. You may want to look at the staxlab function in the plotrix package.
Jim

On Fri, Dec 18, 2015 at 10:20 PM, Amelia Marsh via R-help <r-help at r-project.org> wrote:

Dear Forum,

Assuming I need to plot a graph. In the code I have defined X axis range as



xlim=c(0,18000)


In the plot, the values visible w.r.t X axis are 0, 5000, 10000, 18000.

To improve the graph clarity, is there any way I can show the values of X axis as 0, 1000, 2000, 3000, 4000, 5000 .......18000 i.e. the values increase by 1000 instead of 5000.

Regards

Amelia

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.




   
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Dec 18 14:57:47 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 18 Dec 2015 05:57:47 -0800
Subject: [R] Numerical differentiation of a vector of values
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>
Message-ID: <EC5A1FD2-3B83-43E9-80D6-35FCD73B6C3E@dcn.davis.ca.us>

Did you really fail to type "R numeric derivative" into Google, or did you do it and then not notice the built in numericDeriv function or the contributed numderiv package?

-- 
Sent from my phone. Please excuse my brevity.

On December 18, 2015 1:09:06 AM PST, BARLAS Marios 247554 <Marios.BARLAS at cea.fr> wrote:
>Hi everyone,
>
>I have sets of experimental values representing I-V curves. I want to
>get the derivative of such numerical data but in r there doesn' seem to
>be a premade function. I can write something of my own, but I was
>wondering if there is a standard package out there?
>
>Thanks in advance!
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Dec 18 16:52:24 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 18 Dec 2015 07:52:24 -0800
Subject: [R] Numerical differentiation of a vector of values
In-Reply-To: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>
References: <01BFC0B2B4ABFC4CB432008F852D76616CC81C@EXDAG0-A1.intra.cea.fr>
Message-ID: <CAF8bMcZxKZhH7u5BKEpexcP=kSmQY85MZXEM6Z_K=oYrYStbrw@mail.gmail.com>

You can fit a spline to the points and evaluate the derivative
of the fitted spline where you want.  The built-in splines package
has functions for this, as do other packages on CRAN.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Dec 18, 2015 at 1:09 AM, BARLAS Marios 247554 <Marios.BARLAS at cea.fr>
wrote:

> Hi everyone,
>
> I have sets of experimental values representing I-V curves. I want to get
> the derivative of such numerical data but in r there doesn' seem to be a
> premade function. I can write something of my own, but I was wondering if
> there is a standard package out there?
>
> Thanks in advance!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From fahad.usman at openreach.co.uk  Fri Dec 18 15:49:38 2015
From: fahad.usman at openreach.co.uk (fahad.usman at openreach.co.uk)
Date: Fri, 18 Dec 2015 14:49:38 +0000
Subject: [R] Gather columns based on multiple columns using tidyr
In-Reply-To: <CA+vqiLH_uJB44aBY4to3PkgmwUiQ=mCGjGcvD_YhVCV+tf3FHg@mail.gmail.com>
References: <9237949d508e4c8583ab4e17f7f63535@tpw09926dag11f.domain1.systemhost.net>
	<CA+vqiLH_uJB44aBY4to3PkgmwUiQ=mCGjGcvD_YhVCV+tf3FHg@mail.gmail.com>
Message-ID: <0e36de679b134ef4a5e467720651971b@tpw09926dag11f.domain1.systemhost.net>

Dear Ista,

Many thanks for your reply. 

Your tidyr solution didn?t work:
================================================================
> dput(head(scotland_weather,6))
structure(list(Jan = c(293.8, 292.2, 275.6, 252.3, 246.2, 245
), Year.1 = c(1993L, 1928L, 2008L, 2015L, 1974L, 1975L), Feb = c(278.1, 
258.8, 244.7, 227.9, 224.9, 195.6), Year.2 = c(1990L, 1997L, 
2002L, 1989L, 2014L, 1995L), Mar = c(238.5, 233.4, 201.3, 200.2, 
180.2, 180), Year.3 = c(1994L, 1990L, 1992L, 1967L, 1979L, 1989L
), Apr = c(191.1, 149, 146.8, 142.1, 133.5, 132.9), Year.4 = c(1947L, 
1910L, 1934L, 1949L, 1950L, 1932L), May = c(191.4, 168.7, 155.9, 
149.5, 137.4, 129.7), Year.5 = c(2011L, 1986L, 1925L, 2015L, 
2003L, 2007L), Jun = c(155, 137.9, 137.8, 137.7, 135, 131.7), 
    Year.6 = c(1938L, 2002L, 1948L, 1931L, 1966L, 2004L), Jul = c(185.6, 
    181.4, 170.1, 165.8, 162.9, 159.9), Year.7 = c(1940L, 1988L, 
    1939L, 2010L, 1956L, 1985L), Aug = c(216.5, 211.9, 202.3, 
    191.4, 190.3, 189.1), Year.8 = c(1985L, 1992L, 2009L, 1962L, 
    2014L, 2004L), Sep = c(267.6, 221.2, 193.9, 189.7, 189.7, 
    189.6), Year.9 = c(1950L, 1981L, 1982L, 2011L, 1927L, 1985L
    ), Oct = c(258.1, 254, 248.8, 247.7, 242.3, 240.9), Year.10 = c(1935L, 
    1954L, 2014L, 1938L, 1983L, 2001L), Nov = c(262, 245.3, 244.8, 
    242.2, 231.3, 229.9), Year.11 = c(2009L, 2015L, 1938L, 2006L, 
    1917L, 1981L), Dec = c(300.7, 268.5, 267.2, 265.4, 264, 261
    ), Year.12 = c(2013L, 1986L, 1929L, 2011L, 2006L, 1912L)), .Names = c("Jan", 
"Year.1", "Feb", "Year.2", "Mar", "Year.3", "Apr", "Year.4", 
"May", "Year.5", "Jun", "Year.6", "Jul", "Year.7", "Aug", "Year.8", 
"Sep", "Year.9", "Oct", "Year.10", "Nov", "Year.11", "Dec", "Year.12"
), row.names = c(NA, 6L), class = "data.frame")
================================================================
> scotland_weather%>%
+ gather(junk1, year, starts_with("Year")) %>%
+ gather(month, rainfall_mm, one_of(toupper(month.abb)))%>%
+ arrange(year, month) %>%
+ select(-junk1)
Error: cannot arrange column of class 'function'
In addition: Warning message:
attributes are not identical across measure variables; they will be dropped
==================================================================

I tried:

data.frame(month= names(scotland_weather)[!grepl('Year', names(scotland_weather))], 
                           year=c(t(scotland_weather[c(FALSE,TRUE)])), 
                           rainfall_mm= c(t(scotland_weather[c(TRUE,FALSE)])))

Which works fine...but I am looking for a tidyr solution.

Regards,


Fahad Usman
Network Engineering | CIO | Openreach?
Web: www.openreach.co.uk

-----Original Message-----
From: Ista Zahn [mailto:istazahn at gmail.com] 
Sent: 17 December 2015 19:49
To: Usman,F,Fahad,BVG1C3 R
Cc: r-help at r-project.org
Subject: Re: [R] Gather columns based on multiple columns using tidyr

Hi Fahad,

Easier than what? You didn't tell us what you tried, nor why you were unhappy with it. I'm only passingly familiar with tidyr, but I came up with


library(tidyr)
library(dplyr)

read.table("scotland_rainfall.txt", skip = 7, header=TRUE, fill = TRUE) %>%
  select(-WIN, -SPR, -SUM, -AUT, -ANN) %>%
  gather(junk1, year, starts_with("Year")) %>%
  gather(month, rainfall_mm, one_of(toupper(month.abb))) %>%
  arrange(year, month) %>%
  select(-junk1) -> scotland_weather

Best,
Ista

On Thu, Dec 17, 2015 at 10:24 AM,  <fahad.usman at openreach.co.uk> wrote:
> Hi,
>
> Sorry for this direct approach but I am stuck with a stupid data that I would like to reformat.
>
> The datafile is location at: fileURL <- 
> http://www.metoffice.gov.uk/pub/data/weather/uk/climate/datasets/Rainf
> all/ranked/Scotland.txt
>
> You can read the data by:
>
> if(!file.exists("scotland_rainfall.txt")){
>         #this will download the file in the current working directory
>         download.file(fileURL,destfile = "scotland_rainfall.txt")
>         dateDownload <- Sys.Date() #15-12-2015
>
> }
>
>> head(scotland_weather)
>     Jan Year.1   Feb Year.2   Mar Year.3   Apr Year.4   May Year.5   Jun Year.6   Jul Year.7   Aug Year.8   Sep Year.9   Oct Year.10   Nov Year.11   Dec Year.12
> 1 293.8   1993 278.1   1990 238.5   1994 191.1   1947 191.4   2011 155.0   1938 185.6   1940 216.5   1985 267.6   1950 258.1    1935 262.0    2009 300.7    2013
> 2 292.2   1928 258.8   1997 233.4   1990 149.0   1910 168.7   1986 137.9   2002 181.4   1988 211.9   1992 221.2   1981 254.0    1954 245.3    2015 268.5    1986
> 3 275.6   2008 244.7   2002 201.3   1992 146.8   1934 155.9   1925 137.8   1948 170.1   1939 202.3   2009 193.9   1982 248.8    2014 244.8    1938 267.2    1929
> 4 252.3   2015 227.9   1989 200.2   1967 142.1   1949 149.5   2015 137.7   1931 165.8   2010 191.4   1962 189.7   2011 247.7    1938 242.2    2006 265.4    2011
> 5 246.2   1974 224.9   2014 180.2   1979 133.5   1950 137.4   2003 135.0   1966 162.9   1956 190.3   2014 189.7   1927 242.3    1983 231.3    1917 264.0    2006
> 6 245.0   1975 195.6   1995 180.0   1989 132.9   1932 129.7   2007 131.7   2004 159.9   1985 189.1   2004 189.6   1985 240.9    2001 229.9    1981 261.0    1912
>
>
>> tail(scotland_weather)
>
>      Jan Year.1  Feb Year.2  Mar Year.3  Apr Year.4  May Year.5  Jun 
> Year.6  Jul Year.7  Aug Year.8  Sep Year.9  Oct Year.10  Nov Year.11  
> Dec Year.12
>
> 101 71.2   1987 34.4   1947 50.9   1918 44.6   1982 34.1   1978 38.8   1940 49.2   2005 46.2   2003 50.7   2015 76.5    1973 57.1    1942 62.7    2010
>
> 102 57.9   1997 33.7   1917 44.4   1953 38.5   1918 32.1   1919 36.9   1932 47.8   1989 46.1   1983 49.6   1959 74.6    1922 54.9    1958 59.9    1963
>
> 103 57.9   1941 31.8   1963 39.7   1924 31.7   1981 28.8   1994 33.2   1921 45.8   1983 37.6   1955 48.5   1910 69.9    1972 53.9    1925 55.0    1995
>
> 104 57.6   1940 24.2   1930 38.8   1969 29.0   1938 26.1   2008 32.8   1925 39.7   1919 33.0   1995 40.0   1933 62.9    1914 53.6    1983 43.4    1927
>
> 105 51.9   1929 20.0   1986 37.4   1931 19.8   1980 24.0   1980 30.9   1941 33.7   1955 21.9   1976 39.2   2014 60.7    1951 42.3    1937 40.2    1933
>
> 106 38.6   1963 10.3   1932 28.7   1929 14.0   1974 22.5   1984 30.1   1988 32.7   1913  5.1   1947 31.7   1972 19.4    1946 28.8    1945   NA      NA
>
> How can I format this into:
>
> Year   month     rainfall_mm
> 1993    Jan            293.8
> 1990   Feb           278.1
> ....
>
>> dput(head(scotland_weather,6))
> structure(list(Jan = c(293.8, 292.2, 275.6, 252.3, 246.2, 245 ), 
> Year.1 = c(1993L, 1928L, 2008L, 2015L, 1974L, 1975L), Feb = c(278.1, 
> 258.8, 244.7, 227.9, 224.9, 195.6), Year.2 = c(1990L, 1997L, 2002L, 
> 1989L, 2014L, 1995L), Mar = c(238.5, 233.4, 201.3, 200.2, 180.2, 180), 
> Year.3 = c(1994L, 1990L, 1992L, 1967L, 1979L, 1989L ), Apr = c(191.1, 
> 149, 146.8, 142.1, 133.5, 132.9), Year.4 = c(1947L, 1910L, 1934L, 
> 1949L, 1950L, 1932L), May = c(191.4, 168.7, 155.9, 149.5, 137.4, 
> 129.7), Year.5 = c(2011L, 1986L, 1925L, 2015L, 2003L, 2007L), Jun = 
> c(155, 137.9, 137.8, 137.7, 135, 131.7),
>     Year.6 = c(1938L, 2002L, 1948L, 1931L, 1966L, 2004L), Jul = c(185.6,
>     181.4, 170.1, 165.8, 162.9, 159.9), Year.7 = c(1940L, 1988L,
>     1939L, 2010L, 1956L, 1985L), Aug = c(216.5, 211.9, 202.3,
>     191.4, 190.3, 189.1), Year.8 = c(1985L, 1992L, 2009L, 1962L,
>     2014L, 2004L), Sep = c(267.6, 221.2, 193.9, 189.7, 189.7,
>     189.6), Year.9 = c(1950L, 1981L, 1982L, 2011L, 1927L, 1985L
>     ), Oct = c(258.1, 254, 248.8, 247.7, 242.3, 240.9), Year.10 = c(1935L,
>     1954L, 2014L, 1938L, 1983L, 2001L), Nov = c(262, 245.3, 244.8,
>     242.2, 231.3, 229.9), Year.11 = c(2009L, 2015L, 1938L, 2006L,
>     1917L, 1981L), Dec = c(300.7, 268.5, 267.2, 265.4, 264, 261
>     ), Year.12 = c(2013L, 1986L, 1929L, 2011L, 2006L, 1912L),
>     X1.12 = c(743.6, 649.5, 645.4, 638.3, 608.9, 592.8), Year.13 = c(2014L,
>     1995L, 2000L, 2007L, 1990L, 2015L), X1.13 = c(409.5, 401.3,
>     393.7, 393.2, 391.7, 389.1), Year.14 = c(1986L, 2015L, 1994L,
>     1967L, 1992L, 1913L), X1.14 = c(455.6, 435.6, 427.8, 422.6,
>     397, 390.1), Year.15 = c(1985L, 1948L, 2009L, 1956L, 2004L,
>     1938L), X1.15 = c(661.2, 633.8, 615.8, 594.5, 590.6, 589.2
>     ), Year.16 = c(1981L, 1954L, 1938L, 1935L, 1982L, 2006L),
>     X1.16 = structure(c(105L, 104L, 103L, 102L, 101L, 100L), .Label = c("  1091.2",
>     "  1138.2", "  1158.2", "  1166.0", "  1168.8", "  1174.1",
>     "  1189.4", "  1214.2", "  1219.3", "  1220.0", "  1222.0",
>     "  1231.5", "  1239.5", "  1250.0", "  1255.4", "  1266.1",
>     "  1269.7", "  1274.2", "  1276.0", "  1281.1", "  1283.5",
>     "  1301.7", "  1305.4", "  1306.4", "  1311.0", "  1311.1",
>     "  1314.3", "  1315.8", "  1324.6", "  1325.3", "  1337.6",
>     "  1348.6", "  1351.5", "  1355.6", "  1356.1", "  1356.7",
>     "  1357.8", "  1366.9", "  1374.7", "  1376.5", "  1377.9",
>     "  1378.5", "  1390.2", "  1397.6", "  1406.7", "  1406.9",
>     "  1407.5", "  1407.9", "  1414.0", "  1425.3", "  1426.5",
>     "  1429.6", "  1430.8", "  1431.6", "  1436.4", "  1438.0",
>     "  1438.8", "  1445.9", "  1446.6", "  1448.6", "  1455.0",
>     "  1458.6", "  1459.0", "  1460.9", "  1461.3", "  1464.4",
>     "  1465.7", "  1466.4", "  1467.3", "  1473.9", "  1478.4",
>     "  1478.6", "  1491.3", "  1493.2", "  1503.9", "  1520.3",
>     "  1530.4", "  1532.5", "  1536.3", "  1558.0", "  1561.4",
>     "  1566.8", "  1579.2", "  1582.3", "  1585.0", "  1585.5",
>     "  1592.6", "  1607.8", "  1623.8", "  1627.8", "  1631.0",
>     "  1657.1", "  1670.7", "  1672.8", "  1683.6", "  1686.1",
>     "  1690.4", "  1692.9", "  1696.7", "  1716.5", "  1720.0",
>     "  1735.8", "  1756.8", "  1828.1", "  1886.4", "NA"), class = "factor"),
>     Year.17 = structure(c(102L, 81L, 105L, 29L, 99L, 45L), .Label = c("  1910",
>     "  1911", "  1912", "  1913", "  1914", "  1915", "  1916",
>     "  1917", "  1918", "  1919", "  1920", "  1921", "  1922",
>     "  1923", "  1924", "  1925", "  1926", "  1927", "  1928",
>     "  1929", "  1930", "  1931", "  1932", "  1933", "  1934",
>     "  1935", "  1936", "  1937", "  1938", "  1939", "  1940",
>     "  1941", "  1942", "  1943", "  1944", "  1945", "  1946",
>     "  1947", "  1948", "  1949", "  1950", "  1951", "  1952",
>     "  1953", "  1954", "  1955", "  1956", "  1957", "  1958",
>     "  1959", "  1960", "  1961", "  1962", "  1963", "  1964",
>     "  1965", "  1966", "  1967", "  1968", "  1969", "  1970",
>     "  1971", "  1972", "  1973", "  1974", "  1975", "  1976",
>     "  1977", "  1978", "  1979", "  1980", "  1981", "  1982",
>     "  1983", "  1984", "  1985", "  1986", "  1987", "  1988",
>     "  1989", "  1990", "  1991", "  1992", "  1993", "  1994",
>     "  1995", "  1996", "  1997", "  1998", "  1999", "  2000",
>     "  2001", "  2002", "  2003", "  2004", "  2005", "  2006",
>     "  2007", "  2008", "  2009", "  2010", "  2011", "  2012",
>     "  2013", "  2014", "NA"), class = "factor")), .Names = c("Jan", 
> "Year.1", "Feb", "Year.2", "Mar", "Year.3", "Apr", "Year.4", "May", 
> "Year.5", "Jun", "Year.6", "Jul", "Year.7", "Aug", "Year.8", "Sep", 
> "Year.9", "Oct", "Year.10", "Nov", "Year.11", "Dec", "Year.12", 
> "X1.12", "Year.13", "X1.13", "Year.14", "X1.14", "Year.15", "X1.15", 
> "Year.16", "X1.16", "Year.17"), row.names = c(NA, 6L), class = 
> "data.frame")
>
> Is there an easier way to do it with tidyr?
>
> Regards,
>
> Fahad Usman
> Network Engineering | CIO | Openreach
> Web: www.openreach.co.uk<http://www.openreach.co.uk/>
> Openreach is delivering fibre broadband services to communities across the UK as well as installing and maintaining the communications infrastructure that links homes, businesses, public and voluntary sector organisations to their Communications Providers' networks.
> Think before you print! Consider the environment before printing this e-mail.
>
> This email contains BT information, which may be privileged or confidential.
> It's meant only for the individual(s) or entity named above. If you're 
> not the intended recipient, note that disclosing, copying, 
> distributing or using this information is prohibited. If you've 
> received this email in error, please let me know immediately on the email address above. Thank you.
> We monitor our email system, and may record your emails.
> British Telecommunications plc
> Registered office: 81 Newgate Street London EC1A 7AJ Registered in 
> England no: 1800000
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

From chanmo5752 at gmail.com  Fri Dec 18 16:23:51 2015
From: chanmo5752 at gmail.com (Bradley Wolf)
Date: Fri, 18 Dec 2015 09:23:51 -0600
Subject: [R] Cannot subset a specific mean from this function
In-Reply-To: <CA+8X3fVF43=3J_JAozVFmp_cCbPw_0_wwjGF4CAb7eYHxJmRPQ@mail.gmail.com>
References: <CAAT8Cgc1N1yt0RVh4RiLbG-DPXnkT74169C7NnXPHBma9+wL4g@mail.gmail.com>
	<CA+8X3fVF43=3J_JAozVFmp_cCbPw_0_wwjGF4CAb7eYHxJmRPQ@mail.gmail.com>
Message-ID: <CAAT8CgdtvNmcA=1YKHCwSDf4En=9AH6zOx58+3QKt3PpK0Qv_g@mail.gmail.com>

Jim,
  Thank you for your response.  I am not asking you for you to do my work.
I am really new to programming (in any context) and really haven't gotten
the grasp of the logic yet. I have used R before but in the context of R
Commander and am trying this so I can be a more robust user.

That being said, I wanted the last 53 file names read. The irony of my
problem is, I can do the assignment if it was to get the mean of all the
files (I am  reading 332 csv's).

Thanks,

Brad

	[[alternative HTML version deleted]]


From francesca.mancini at ubs.com  Fri Dec 18 16:42:38 2015
From: francesca.mancini at ubs.com (francesca.mancini at ubs.com)
Date: Fri, 18 Dec 2015 15:42:38 +0000
Subject: [R] multidimensional splines
Message-ID: <4B15C58DA3FDD24F94C394C6631AA73EAD0B@NZURC601PN3.UBSPROD.MSAD.UBS.NET>

Dear all,

I am looking for a package which performs splining in more than one dimension with polynomials of order>1, exactly as the package "splines" does in 1D.
For the moment I was only able to find the package "polspline", whose function "polymars" performs just piecewise linear splines, so not exactly what I'm looking for.
Do you know of anything else?

Thanks a lot!
-------------- next part --------------
Check out our new brand campaign: www.ubs.com/together

Visit our website at http://www.ubs.com

This message contains confidential information and is in...{{dropped:21}}

From wdunlap at tibco.com  Fri Dec 18 18:20:31 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 18 Dec 2015 09:20:31 -0800
Subject: [R] multidimensional splines
In-Reply-To: <4B15C58DA3FDD24F94C394C6631AA73EAD0B@NZURC601PN3.UBSPROD.MSAD.UBS.NET>
References: <4B15C58DA3FDD24F94C394C6631AA73EAD0B@NZURC601PN3.UBSPROD.MSAD.UBS.NET>
Message-ID: <CAF8bMcYxStojb8FYmr6faucyaE2Pn=_mcQosuSRUMW6sPm+ehA@mail.gmail.com>

Have you considered thin plate splines, which are a natural
extension of one-dimensional splines to higher dimensional
spaces, but are not piecewise polynomials?  The Tps function
in the fields package will fit a thin plate spline to irregularly
spaced data.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Dec 18, 2015 at 7:42 AM, <francesca.mancini at ubs.com> wrote:

> Dear all,
>
> I am looking for a package which performs splining in more than one
> dimension with polynomials of order>1, exactly as the package "splines"
> does in 1D.
> For the moment I was only able to find the package "polspline", whose
> function "polymars" performs just piecewise linear splines, so not exactly
> what I'm looking for.
> Do you know of anything else?
>
> Thanks a lot!
>
> Check out our new brand campaign: www.ubs.com/together
>
> Visit our website at http://www.ubs.com
>
> This message contains confidential information and is ...{{dropped:13}}


From dcarlson at tamu.edu  Fri Dec 18 18:23:27 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Fri, 18 Dec 2015 17:23:27 +0000
Subject: [R] Converting from Continuous 2D Points to Continuous 2D
	Vectors
In-Reply-To: <CY1PR0101MB1004E533C2B3AA381E59BE3DABE00@CY1PR0101MB1004.prod.exchangelabs.com>
References: <mailman.0.1450030036.31803.r-help@r-project.org>
	<CY1PR0101MB1004E533C2B3AA381E59BE3DABE00@CY1PR0101MB1004.prod.exchangelabs.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6EAEAD@mb02.ads.tamu.edu>

Look at the CRAN Task View: "Handling and Analyzing Spatio-Temporal Data," particularly the section on "Moving objects, trajectories." The tools you need are probably already available.

https://cran.r-project.org/web/views/SpatioTemporal.html

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Sidoti, Salvatore A.
Sent: Thursday, December 17, 2015 3:25 PM
To: r-help at r-project.org
Subject: [R] Converting from Continuous 2D Points to Continuous 2D Vectors

Greetings!

I have a fairly large dataframe (df) with pathing information in the form of continuous x,y coordinates:

df$x
df$y

With these data, I would like to:
1. Calculate a set of continuous vectors
2. Determine the angle between each of these vectors (in degrees)
3. Count the number of angles in the dataframe that meet a certain threshold (i.e. <90?)

Here's what I've come up with so far:

### Function that calculates the angle between two vectors in 2D space:

angle <- function(x,y){ # x and y are vectors
  dot.prod <- x%*%y 
  norm.x <- norm(x,type="2")
  norm.y <- norm(y,type="2")
  theta <- acos(dot.prod / (norm.x * norm.y))
  (180*as.numeric(theta))/pi # returns the angle in degrees
}

### Test the function:
x <- as.matrix(c(2,1))
y <- as.matrix(c(1,2))
angle(t(x),y)
[1] 36.8699

Thank you!
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marna.wagley at gmail.com  Fri Dec 18 18:57:19 2015
From: marna.wagley at gmail.com (Marna Wagley)
Date: Fri, 18 Dec 2015 09:57:19 -0800
Subject: [R] Would you please help me to create a table in R?
In-Reply-To: <CA+8X3fVVskOftnHtYOsqmvnu1HNBWP6e3rYufrCkufgnGXbdrw@mail.gmail.com>
References: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>
	<CA+8X3fVVskOftnHtYOsqmvnu1HNBWP6e3rYufrCkufgnGXbdrw@mail.gmail.com>
Message-ID: <CAMwU6B0W53A0AY_TNuNi-2JuoYex843S-MnHgJfNc3nGbJqvVA@mail.gmail.com>

Dear Jim,
I am sorry for not explaining the question in a clear way. I am trying to
explain it, let's see how much clear I can make it.

For the given example, (raw.data). Let's say, the 11 birds were marked and
released at site A in a landscape (a combination of sites siteA, siteB,
site C) in 2010.

In 2011, we revisited at the sites (siteA, SiteB, SiteC), among the 11
birds, we recaptured 3 individuals at Site A, 3 at Site B, 1 at site C but
we could not see 4 individuals at any sites.

Again in 2012, we revisited at the sites (SiteA, SiteB, SiteC), we again
recaptured 2 individuals which was recaptured at Site A in 2011 but 1
individual could not seen at any sites (in another words: in 2011 at the
site A  we had recaptured 3 individuals, among them 2 were again seen at
site A, but we could not see one individual in any sites).

Similarly,we had recaptured 3 individuals in 2011 at the site B, but in
2012, among three individuals, one was again recaptured in Site B but 2
individuals could not be seen at any sites.

Likewise, for SiteC in 2012, we had recaptured 1 at that site (siteC), but
in next year (2012), we could not see at any of the sites.

Again, we had not seen 4 individuals in 2011, but later in 2012, among
these 4 individuals, we recaptured one individual for each site  B and C,
and 2 individuals could not be seen.

I have given the example dataset. If it is really confusing and takes your
lots of time, please forget it, I will try to do in Excel manually,but it
is taking so much time and easily making errors. Your help will be very
useful.

Sincerely,


raw.data<-structure(list(Time1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
1L, 1L, 1L, 1L), .Label = "SiteA", class = "factor"), Time2 =
structure(c(1L, 4L, 2L, 1L, 2L, 1L, 2L, 1L, 3L, 3L, 3L), .Label = c("0",
"SiteA",
"SiteB", "SiteC"), class = "factor"), Time3 = structure(c(2L,
2L, 2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 3L), .Label = c("0", "SiteA",
"SiteB"), class = "factor")), .Names = c("Time1", "Time2", "Time3"
), class = "data.frame", row.names = c(NA, -11L))

raw.data

#####
table.format<-structure(list(Time1 = structure(c(NA, NA, NA, NA, NA, 1L,
NA,NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = "Released A", class =
"factor"),Time2 = structure(c(NA, NA, 2L, NA, NA, NA, 3L, NA, NA, NA,
    4L, NA, NA, NA, 1L, NA), .Label = c("NotSeen", "Re-captured.at.A",
    "Re-captured.at.B", "Re-captured.at.C"), class = "factor"),
    Time3 = structure(c(2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L,
    4L, 1L, 2L, 3L, 4L, 1L), .Label = c("NotSeen", "Re-captured.at.A",
    "Re-captured.at.B", "Re-captured.at.C"), class = "factor")), .Names =
c("Time1","Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
))

table.format

###
output<-structure(list(Time1 = c(NA, NA, NA, NA, NA, 11L, NA, NA, NA,
NA, NA, NA, NA, NA, NA, NA), Time2 = c(NA, NA, 3L, NA, NA, NA,
3L, NA, NA, NA, 1L, NA, NA, NA, 4L, NA), Time3 = c(2L, NA, NA,
1L, NA, 2L, NA, 1L, 1L, NA, NA, NA, 1L, 1L, NA, 2L)), .Names = c("Time1",
"Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
))

output

##################################################
####################################################


On Thu, Dec 17, 2015 at 11:38 PM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi Marna,
> A bit hard to understand. If raw.data is a record of 11 individuals
> released at site A at Time 1 and recaptured at either A or B or neither at
> Time2 or Time3, it doesn't seem to bear any consistent relationship to the
> numeric coding in table.format or the output at the bottom. Could you
> explain what the correspondence between the tables is?
>
> Jim
>
>
> On Fri, Dec 18, 2015 at 2:33 PM, Marna Wagley <marna.wagley at gmail.com>
> wrote:
>
>> Hi R users,
>> I am struggling to create a table in R. I did in Excel but I have a lots
>> of
>> data and thought it might be easy in  R  but  I am new in R. How to get
>> "output table" for this example data?
>>
>> This is an example.
>>
>> #####
>> raw.data<-structure(list(Time1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
>> 1L, 1L, 1L, 1L), .Label = "SiteA", class = "factor"), Time2 =
>> structure(c(1L,
>> 4L, 2L, 1L, 2L, 1L, 2L, 1L, 3L, 3L, 3L), .Label = c("0", "SiteA",
>> "SiteB", "SiteC"), class = "factor"), Time3 = structure(c(2L,
>> 2L, 2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 3L), .Label = c("0", "SiteA",
>> "SiteB"), class = "factor")), .Names = c("Time1", "Time2", "Time3"
>> ), class = "data.frame", row.names = c(NA, -11L))
>>
>> raw.data
>>
>> #####
>> table.format<-structure(list(Time1 = structure(c(NA, NA, NA, NA, NA, 1L,
>> NA,
>> NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = "Released A", class =
>> "factor"),
>>     Time2 = structure(c(NA, NA, 2L, NA, NA, NA, 3L, NA, NA, NA,
>>     4L, NA, NA, NA, 1L, NA), .Label = c("Dead", "Re-captured.at.A",
>>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor"),
>>     Time3 = structure(c(2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L,
>>     4L, 1L, 2L, 3L, 4L, 1L), .Label = c("Dead", "Re-captured.at.A",
>>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor")), .Names =
>> c("Time1",
>> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
>> ))
>>
>> table.format
>>
>> ###
>> output<-structure(list(Time1 = c(NA, NA, NA, NA, NA, 11L, NA, NA, NA,
>> NA, NA, NA, NA, NA, NA, NA), Time2 = c(NA, NA, 3L, NA, NA, NA,
>> 3L, NA, NA, NA, 1L, NA, NA, NA, 4L, NA), Time3 = c(2L, NA, NA,
>> 1L, NA, 2L, NA, 1L, 1L, NA, NA, NA, 1L, 1L, NA, 2L)), .Names = c("Time1",
>> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
>> ))
>>
>> output
>>
>> I want to get the table like "output". Any possibility to get it in R?
>>
>> I will really appreciate for your help. I am struggling to create  this
>> type of table.
>>
>>
>> Sincerely,
>> Marna
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From gliddeca at science.oregonstate.edu  Fri Dec 18 18:42:16 2015
From: gliddeca at science.oregonstate.edu (Caroline Glidden)
Date: Fri, 18 Dec 2015 09:42:16 -0800
Subject: [R] Missing data in RMark
Message-ID: <D98009DC-3F5A-4730-AFE2-F409573AAF18@science.oregonstate.edu>

I am currently trying to run a Known Fates Model in RMark with individual time varying covariates. However, for animals that died early in the study or were not captured at one capture period I, of course, do not have data for all of their time points. I thought that NAs would not matter when the LD capture history was 00, and therefore, having no data during the time points the animal was unable to be sampled would not be a big deal. However, RMark cannot fit my model due to the NAs. I was wondering how I should code the individual time varying covariates for time points at which the animal has no data due to it not being captured or due to it dying?
	[[alternative HTML version deleted]]


From fahad.usman at openreach.co.uk  Fri Dec 18 18:55:44 2015
From: fahad.usman at openreach.co.uk (fahad.usman at openreach.co.uk)
Date: Fri, 18 Dec 2015 17:55:44 +0000
Subject: [R] Gather columns based on multiple columns using tidyr
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6EA851@mb02.ads.tamu.edu>
References: <9237949d508e4c8583ab4e17f7f63535@tpw09926dag11f.domain1.systemhost.net>
	<CA+vqiLH_uJB44aBY4to3PkgmwUiQ=mCGjGcvD_YhVCV+tf3FHg@mail.gmail.com>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6EA851@mb02.ads.tamu.edu>
Message-ID: <7a15798062d94df78c740af205ca90ff@tpw09926dag11f.domain1.systemhost.net>

Hi David,

I did something similar:

years <- scotland_weather%>%
                        select(starts_with("Year"))%>%
                        gather(year.col,year)%>%
                        select(-year.col)

months <- scotland_weather[seq(1, 24, 2)]%>%
          gather(month,rainfall_mm)


scotland_weather <- cbind(years,months)

However, would have liked more sophisticated solution using tidyr

Regards,

Fahad Usman
Network Engineering | CIO | Openreach?
Web: www.openreach.co.uk

-----Original Message-----
From: David L Carlson [mailto:dcarlson at tamu.edu] 
Sent: 17 December 2015 20:02
To: Ista Zahn; Usman,F,Fahad,BVG1C3 R
Cc: r-help at r-project.org
Subject: RE: [R] Gather columns based on multiple columns using tidyr

Here's another approach, but it doesn't use tidy or dplyr:

> Rain.col <- seq(1, 24, by=2)
> Year.col <- seq(2, 24, by =2)
> Scotland.lst <- lapply(1:12, function(x) data.frame(Year =
+      scotland_weather[ , Year.col[x]], month = month.abb[x],
+       rainfall_mm = scotland_weather[ , Rain.col[x]]))
> Scotland.df <- do.call(rbind, Scotland.lst)
> head(Scotland.df)
  Year month rainfall_mm
1 1993   Jan       293.8
2 1928   Jan       292.2
3 2008   Jan       275.6
4 2015   Jan       252.3
5 1974   Jan       246.2
6 1975   Jan       245.0
> tail(Scotland.df)
   Year month rainfall_mm
67 2013   Dec       300.7
68 1986   Dec       268.5
69 1929   Dec       267.2
70 2011   Dec       265.4
71 2006   Dec       264.0
72 1912   Dec       261.0

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ista Zahn
Sent: Thursday, December 17, 2015 1:49 PM
To: fahad.usman at openreach.co.uk
Cc: r-help at r-project.org
Subject: Re: [R] Gather columns based on multiple columns using tidyr

Hi Fahad,

Easier than what? You didn't tell us what you tried, nor why you were unhappy with it. I'm only passingly familiar with tidyr, but I came up with


library(tidyr)
library(dplyr)

read.table("scotland_rainfall.txt", skip = 7, header=TRUE, fill = TRUE) %>%
  select(-WIN, -SPR, -SUM, -AUT, -ANN) %>%
  gather(junk1, year, starts_with("Year")) %>%
  gather(month, rainfall_mm, one_of(toupper(month.abb))) %>%
  arrange(year, month) %>%
  select(-junk1) -> scotland_weather

Best,
Ista

On Thu, Dec 17, 2015 at 10:24 AM,  <fahad.usman at openreach.co.uk> wrote:
> Hi,
>
> Sorry for this direct approach but I am stuck with a stupid data that I would like to reformat.
>
> The datafile is location at: fileURL <- 
> http://www.metoffice.gov.uk/pub/data/weather/uk/climate/datasets/Rainf
> all/ranked/Scotland.txt
>
> You can read the data by:
>
> if(!file.exists("scotland_rainfall.txt")){
>         #this will download the file in the current working directory
>         download.file(fileURL,destfile = "scotland_rainfall.txt")
>         dateDownload <- Sys.Date() #15-12-2015
>
> }
>
>> head(scotland_weather)
>     Jan Year.1   Feb Year.2   Mar Year.3   Apr Year.4   May Year.5   Jun Year.6   Jul Year.7   Aug Year.8   Sep Year.9   Oct Year.10   Nov Year.11   Dec Year.12
> 1 293.8   1993 278.1   1990 238.5   1994 191.1   1947 191.4   2011 155.0   1938 185.6   1940 216.5   1985 267.6   1950 258.1    1935 262.0    2009 300.7    2013
> 2 292.2   1928 258.8   1997 233.4   1990 149.0   1910 168.7   1986 137.9   2002 181.4   1988 211.9   1992 221.2   1981 254.0    1954 245.3    2015 268.5    1986
> 3 275.6   2008 244.7   2002 201.3   1992 146.8   1934 155.9   1925 137.8   1948 170.1   1939 202.3   2009 193.9   1982 248.8    2014 244.8    1938 267.2    1929
> 4 252.3   2015 227.9   1989 200.2   1967 142.1   1949 149.5   2015 137.7   1931 165.8   2010 191.4   1962 189.7   2011 247.7    1938 242.2    2006 265.4    2011
> 5 246.2   1974 224.9   2014 180.2   1979 133.5   1950 137.4   2003 135.0   1966 162.9   1956 190.3   2014 189.7   1927 242.3    1983 231.3    1917 264.0    2006
> 6 245.0   1975 195.6   1995 180.0   1989 132.9   1932 129.7   2007 131.7   2004 159.9   1985 189.1   2004 189.6   1985 240.9    2001 229.9    1981 261.0    1912
>
>
>> tail(scotland_weather)
>
>      Jan Year.1  Feb Year.2  Mar Year.3  Apr Year.4  May Year.5  Jun 
> Year.6  Jul Year.7  Aug Year.8  Sep Year.9  Oct Year.10  Nov Year.11  
> Dec Year.12
>
> 101 71.2   1987 34.4   1947 50.9   1918 44.6   1982 34.1   1978 38.8   1940 49.2   2005 46.2   2003 50.7   2015 76.5    1973 57.1    1942 62.7    2010
>
> 102 57.9   1997 33.7   1917 44.4   1953 38.5   1918 32.1   1919 36.9   1932 47.8   1989 46.1   1983 49.6   1959 74.6    1922 54.9    1958 59.9    1963
>
> 103 57.9   1941 31.8   1963 39.7   1924 31.7   1981 28.8   1994 33.2   1921 45.8   1983 37.6   1955 48.5   1910 69.9    1972 53.9    1925 55.0    1995
>
> 104 57.6   1940 24.2   1930 38.8   1969 29.0   1938 26.1   2008 32.8   1925 39.7   1919 33.0   1995 40.0   1933 62.9    1914 53.6    1983 43.4    1927
>
> 105 51.9   1929 20.0   1986 37.4   1931 19.8   1980 24.0   1980 30.9   1941 33.7   1955 21.9   1976 39.2   2014 60.7    1951 42.3    1937 40.2    1933
>
> 106 38.6   1963 10.3   1932 28.7   1929 14.0   1974 22.5   1984 30.1   1988 32.7   1913  5.1   1947 31.7   1972 19.4    1946 28.8    1945   NA      NA
>
> How can I format this into:
>
> Year   month     rainfall_mm
> 1993    Jan            293.8
> 1990   Feb           278.1
> ....
>
>> dput(head(scotland_weather,6))
> structure(list(Jan = c(293.8, 292.2, 275.6, 252.3, 246.2, 245 ), 
> Year.1 = c(1993L, 1928L, 2008L, 2015L, 1974L, 1975L), Feb = c(278.1,
> 258.8, 244.7, 227.9, 224.9, 195.6), Year.2 = c(1990L, 1997L, 2002L, 
> 1989L, 2014L, 1995L), Mar = c(238.5, 233.4, 201.3, 200.2, 180.2, 180),
> Year.3 = c(1994L, 1990L, 1992L, 1967L, 1979L, 1989L ), Apr = c(191.1,
> 149, 146.8, 142.1, 133.5, 132.9), Year.4 = c(1947L, 1910L, 1934L, 
> 1949L, 1950L, 1932L), May = c(191.4, 168.7, 155.9, 149.5, 137.4, 
> 129.7), Year.5 = c(2011L, 1986L, 1925L, 2015L, 2003L, 2007L), Jun = 
> c(155, 137.9, 137.8, 137.7, 135, 131.7),
>     Year.6 = c(1938L, 2002L, 1948L, 1931L, 1966L, 2004L), Jul = c(185.6,
>     181.4, 170.1, 165.8, 162.9, 159.9), Year.7 = c(1940L, 1988L,
>     1939L, 2010L, 1956L, 1985L), Aug = c(216.5, 211.9, 202.3,
>     191.4, 190.3, 189.1), Year.8 = c(1985L, 1992L, 2009L, 1962L,
>     2014L, 2004L), Sep = c(267.6, 221.2, 193.9, 189.7, 189.7,
>     189.6), Year.9 = c(1950L, 1981L, 1982L, 2011L, 1927L, 1985L
>     ), Oct = c(258.1, 254, 248.8, 247.7, 242.3, 240.9), Year.10 = c(1935L,
>     1954L, 2014L, 1938L, 1983L, 2001L), Nov = c(262, 245.3, 244.8,
>     242.2, 231.3, 229.9), Year.11 = c(2009L, 2015L, 1938L, 2006L,
>     1917L, 1981L), Dec = c(300.7, 268.5, 267.2, 265.4, 264, 261
>     ), Year.12 = c(2013L, 1986L, 1929L, 2011L, 2006L, 1912L),
>     X1.12 = c(743.6, 649.5, 645.4, 638.3, 608.9, 592.8), Year.13 = c(2014L,
>     1995L, 2000L, 2007L, 1990L, 2015L), X1.13 = c(409.5, 401.3,
>     393.7, 393.2, 391.7, 389.1), Year.14 = c(1986L, 2015L, 1994L,
>     1967L, 1992L, 1913L), X1.14 = c(455.6, 435.6, 427.8, 422.6,
>     397, 390.1), Year.15 = c(1985L, 1948L, 2009L, 1956L, 2004L,
>     1938L), X1.15 = c(661.2, 633.8, 615.8, 594.5, 590.6, 589.2
>     ), Year.16 = c(1981L, 1954L, 1938L, 1935L, 1982L, 2006L),
>     X1.16 = structure(c(105L, 104L, 103L, 102L, 101L, 100L), .Label = c("  1091.2",
>     "  1138.2", "  1158.2", "  1166.0", "  1168.8", "  1174.1",
>     "  1189.4", "  1214.2", "  1219.3", "  1220.0", "  1222.0",
>     "  1231.5", "  1239.5", "  1250.0", "  1255.4", "  1266.1",
>     "  1269.7", "  1274.2", "  1276.0", "  1281.1", "  1283.5",
>     "  1301.7", "  1305.4", "  1306.4", "  1311.0", "  1311.1",
>     "  1314.3", "  1315.8", "  1324.6", "  1325.3", "  1337.6",
>     "  1348.6", "  1351.5", "  1355.6", "  1356.1", "  1356.7",
>     "  1357.8", "  1366.9", "  1374.7", "  1376.5", "  1377.9",
>     "  1378.5", "  1390.2", "  1397.6", "  1406.7", "  1406.9",
>     "  1407.5", "  1407.9", "  1414.0", "  1425.3", "  1426.5",
>     "  1429.6", "  1430.8", "  1431.6", "  1436.4", "  1438.0",
>     "  1438.8", "  1445.9", "  1446.6", "  1448.6", "  1455.0",
>     "  1458.6", "  1459.0", "  1460.9", "  1461.3", "  1464.4",
>     "  1465.7", "  1466.4", "  1467.3", "  1473.9", "  1478.4",
>     "  1478.6", "  1491.3", "  1493.2", "  1503.9", "  1520.3",
>     "  1530.4", "  1532.5", "  1536.3", "  1558.0", "  1561.4",
>     "  1566.8", "  1579.2", "  1582.3", "  1585.0", "  1585.5",
>     "  1592.6", "  1607.8", "  1623.8", "  1627.8", "  1631.0",
>     "  1657.1", "  1670.7", "  1672.8", "  1683.6", "  1686.1",
>     "  1690.4", "  1692.9", "  1696.7", "  1716.5", "  1720.0",
>     "  1735.8", "  1756.8", "  1828.1", "  1886.4", "NA"), class = "factor"),
>     Year.17 = structure(c(102L, 81L, 105L, 29L, 99L, 45L), .Label = c("  1910",
>     "  1911", "  1912", "  1913", "  1914", "  1915", "  1916",
>     "  1917", "  1918", "  1919", "  1920", "  1921", "  1922",
>     "  1923", "  1924", "  1925", "  1926", "  1927", "  1928",
>     "  1929", "  1930", "  1931", "  1932", "  1933", "  1934",
>     "  1935", "  1936", "  1937", "  1938", "  1939", "  1940",
>     "  1941", "  1942", "  1943", "  1944", "  1945", "  1946",
>     "  1947", "  1948", "  1949", "  1950", "  1951", "  1952",
>     "  1953", "  1954", "  1955", "  1956", "  1957", "  1958",
>     "  1959", "  1960", "  1961", "  1962", "  1963", "  1964",
>     "  1965", "  1966", "  1967", "  1968", "  1969", "  1970",
>     "  1971", "  1972", "  1973", "  1974", "  1975", "  1976",
>     "  1977", "  1978", "  1979", "  1980", "  1981", "  1982",
>     "  1983", "  1984", "  1985", "  1986", "  1987", "  1988",
>     "  1989", "  1990", "  1991", "  1992", "  1993", "  1994",
>     "  1995", "  1996", "  1997", "  1998", "  1999", "  2000",
>     "  2001", "  2002", "  2003", "  2004", "  2005", "  2006",
>     "  2007", "  2008", "  2009", "  2010", "  2011", "  2012",
>     "  2013", "  2014", "NA"), class = "factor")), .Names = c("Jan",
> "Year.1", "Feb", "Year.2", "Mar", "Year.3", "Apr", "Year.4", "May", 
> "Year.5", "Jun", "Year.6", "Jul", "Year.7", "Aug", "Year.8", "Sep", 
> "Year.9", "Oct", "Year.10", "Nov", "Year.11", "Dec", "Year.12", 
> "X1.12", "Year.13", "X1.13", "Year.14", "X1.14", "Year.15", "X1.15", 
> "Year.16", "X1.16", "Year.17"), row.names = c(NA, 6L), class = 
> "data.frame")
>
> Is there an easier way to do it with tidyr?
>
> Regards,
>
> Fahad Usman
> Network Engineering | CIO | Openreach
> Web: www.openreach.co.uk<http://www.openreach.co.uk/>
> Openreach is delivering fibre broadband services to communities across the UK as well as installing and maintaining the communications infrastructure that links homes, businesses, public and voluntary sector organisations to their Communications Providers' networks.
> Think before you print! Consider the environment before printing this e-mail.
>
> This email contains BT information, which may be privileged or confidential.
> It's meant only for the individual(s) or entity named above. If you're 
> not the intended recipient, note that disclosing, copying, 
> distributing or using this information is prohibited. If you've 
> received this email in error, please let me know immediately on the email address above. Thank you.
> We monitor our email system, and may record your emails.
> British Telecommunications plc
> Registered office: 81 Newgate Street London EC1A 7AJ Registered in 
> England no: 1800000
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Dec 18 19:52:48 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Dec 2015 10:52:48 -0800
Subject: [R] multidimensional splines
In-Reply-To: <4B15C58DA3FDD24F94C394C6631AA73EAD0B@NZURC601PN3.UBSPROD.MSAD.UBS.NET>
References: <4B15C58DA3FDD24F94C394C6631AA73EAD0B@NZURC601PN3.UBSPROD.MSAD.UBS.NET>
Message-ID: <31340DA0-7BBA-4B67-A33A-EE736FCF6156@comcast.net>


> On Dec 18, 2015, at 7:42 AM, <francesca.mancini at ubs.com> <francesca.mancini at ubs.com> wrote:
> 
> Dear all,
> 
> I am looking for a package which performs splining in more than one dimension with polynomials of order>1, exactly as the package "splines" does in 1D.
> For the moment I was only able to find the package "polspline", whose function "polymars" performs just piecewise linear splines, so not exactly what I'm looking for.
> Do you know of anything else?

I have pleasing experience with the `rcs` function from package 'rms'. One can build such multidimensional splines with the formula interface using the "*" crossing-operator. It is limited to only piecewise cubic splines. You can specify the knots or let them be optimized by the regression function. It's effective use does require that you learn the requirements of the surrounding helper functions and data description methods supplied by the rms/Hmisc packages.

 
-- 
David

> 

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Fri Dec 18 20:21:05 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 18 Dec 2015 11:21:05 -0800
Subject: [R] multidimensional splines
In-Reply-To: <31340DA0-7BBA-4B67-A33A-EE736FCF6156@comcast.net>
References: <4B15C58DA3FDD24F94C394C6631AA73EAD0B@NZURC601PN3.UBSPROD.MSAD.UBS.NET>
	<31340DA0-7BBA-4B67-A33A-EE736FCF6156@comcast.net>
Message-ID: <CAGxFJbS0aGv_axjGzRej1Yi07e-uj6mTG_TQGCftgdjHBN4TSw@mail.gmail.com>

1. I do not think what the OP requested exists, at least as I
understand her  -- how would one define the "surfaces" where the
piecewise multidimensional polynomials are "joined"? (only 1-d space
is ordered)

2. But, as she has already been told, there are various ways of doing
multivariate smoothing splines. The mars() function in packages mda
and earth is another. Apparently the one in earth adds "extra
features" to the one in mda.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Dec 18, 2015 at 10:52 AM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
>> On Dec 18, 2015, at 7:42 AM, <francesca.mancini at ubs.com> <francesca.mancini at ubs.com> wrote:
>>
>> Dear all,
>>
>> I am looking for a package which performs splining in more than one dimension with polynomials of order>1, exactly as the package "splines" does in 1D.
>> For the moment I was only able to find the package "polspline", whose function "polymars" performs just piecewise linear splines, so not exactly what I'm looking for.
>> Do you know of anything else?
>
> I have pleasing experience with the `rcs` function from package 'rms'. One can build such multidimensional splines with the formula interface using the "*" crossing-operator. It is limited to only piecewise cubic splines. You can specify the knots or let them be optimized by the regression function. It's effective use does require that you learn the requirements of the surrounding helper functions and data description methods supplied by the rms/Hmisc packages.
>
>
> --
> David
>
>>
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From marongiu.luigi at gmail.com  Fri Dec 18 21:22:16 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Fri, 18 Dec 2015 20:22:16 +0000
Subject: [R] lattice strip.custom in plot for multiple groups
Message-ID: <CAMk+s2TRKN_1aSgdi6AhnA1Cbu=w=Oy5HO18zgLTj7=NMe60-Q@mail.gmail.com>

Dear all,
I am plotting the count of some data subdivided in different groups.
the variables i am using are:
A = tests performed
B = positive/negative results (but in this case the results are all
positive, so all 1)
C = multiple (1) or single (0) test applied
D = count of instances
E = cases (1) or controls (0)

the plot looks the way I need and it is created with lattice's
barchart. However the strip should indicate "cases"/"controls" and
instead only shows "E"/"E" so I think I messed somewhere.
Would you know what did I missed?
Thank you
L

The example:
>>>
A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
'b',    'c',    'd',    'b',    'c',    'd')
B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
  1,    1,    1)
C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
  1,    1,    1)
D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
 1,    5,    0,    0)
E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
  1,    1,    1)
DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
library(lattice)
barchart(
    A ~ D|E,
    DF,
    groups = C,
    stack = TRUE,
    main = "Comparison of test results",
    xlab = "Count",
    col = c("yellow", "orange"),
    par.settings = list(
        strip.background = list(col="light grey"),
        superpose.polygon=list(col= c("yellow", "orange"))
    ),
    scales = list(
        alternating = FALSE
    ),
    key = list(
        space="top",
        columns=2,
        text=list(c("Single infections", "Multiple infections"), col="black"),
        rectangles=list(col=c("yellow", "orange"))
    ),
    strip = strip.custom(factor.levels = c("Cases","Controls"),
                         par.strip.text = list(cex = 1)
    )
)


From marongiu.luigi at gmail.com  Fri Dec 18 21:32:02 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Fri, 18 Dec 2015 20:32:02 +0000
Subject: [R] introduce axis break lattice plot multipanel
Message-ID: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>

Dear all,
I am plotting some data using lattice's barchart. One of the counts I
am plotting has a very large value with respect to the other variables
and I would like to introduce a break in the axis to compensate for
this 'anomaly' and give more breath to the other bars. In this example
the high count is for the variable 'b' and I would like to introduce a
break of 50-100 in the x axis.
I have seen from the internet that the common approach is to stack
together two figures but in my case I am using a multipanel plot and I
think this way won't work for my plot.
Is there a simple way to introduce a break in the axis of a multipanel
lattice plot?
Thank you
L

the example:
>>>
A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
'b',    'c',    'd',    'b',    'c',    'd')
B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
  1,    1,    1)
C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
  1,    1,    1)
D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
 1,    5,    0,    0)
E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
  1,    1,    1)
DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
library(lattice)
barchart(
    A ~ D|E,
    DF,
    groups = C,
    stack = TRUE,
    main = "Comparison of test results",
    xlab = "Count",
    col = c("yellow", "orange"),
    par.settings = list(
        strip.background = list(col="light grey"),
        superpose.polygon=list(col= c("yellow", "orange"))
    ),
    scales = list(
        alternating = FALSE
    ),
    key = list(
        space="top",
        columns=2,
        text=list(c("Single infections", "Multiple infections"), col="black"),
        rectangles=list(col=c("yellow", "orange"))
    ),
)


From dwinsemius at comcast.net  Fri Dec 18 22:18:19 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Dec 2015 13:18:19 -0800
Subject: [R] lattice strip.custom in plot for multiple groups
In-Reply-To: <CAMk+s2TRKN_1aSgdi6AhnA1Cbu=w=Oy5HO18zgLTj7=NMe60-Q@mail.gmail.com>
References: <CAMk+s2TRKN_1aSgdi6AhnA1Cbu=w=Oy5HO18zgLTj7=NMe60-Q@mail.gmail.com>
Message-ID: <DA3027F9-DDB9-48C3-9522-707FE3E4AA5D@comcast.net>


> On Dec 18, 2015, at 12:22 PM, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> 
> Dear all,
> I am plotting the count of some data subdivided in different groups.
> the variables i am using are:
> A = tests performed
> B = positive/negative results (but in this case the results are all
> positive, so all 1)
> C = multiple (1) or single (0) test applied
> D = count of instances
> E = cases (1) or controls (0)
> 
> the plot looks the way I need and it is created with lattice's
> barchart. However the strip should indicate "cases"/"controls" and
> instead only shows "E"/"E" so I think I messed somewhere.
> Would you know what did I missed?
> Thank you
> L
> 
> The example:
>>>> 
> A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
> 'b',    'c',    'd',    'b',    'c',    'd')
> B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>  1,    1,    1)
> C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
>  1,    1,    1)
> D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
> 1,    5,    0,    0)
> E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
>  1,    1,    1)
> DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
> library(lattice)
> barchart(
>    A ~ D|E,
>    DF,
>    groups = C,
>    stack = TRUE,
>    main = "Comparison of test results",
>    xlab = "Count",
>    col = c("yellow", "orange"),
>    par.settings = list(
>        strip.background = list(col="light grey"),
>        superpose.polygon=list(col= c("yellow", "orange"))
>    ),
>    scales = list(
>        alternating = FALSE
>    ),
>    key = list(
>        space="top",
>        columns=2,
>        text=list(c("Single infections", "Multiple infections"), col="black"),
>        rectangles=list(col=c("yellow", "orange"))
>    ),
>    strip = strip.custom(factor.levels = c("Cases","Controls"),

Also need to set the logical flag strip.levels to TRUE.

strip = strip.custom(factor.levels = c("Cases","Controls"), 
                      strip.levels=TRUE,
                        par.strip.text = list(cex = 1)

If you do not want the variable name "E" displayed then set the correct flag to FALSE. See: 

?strip.custom

-- 
David


>                         par.strip.text = list(cex = 1)
>    )
> )
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From bgunter.4567 at gmail.com  Fri Dec 18 22:20:32 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Fri, 18 Dec 2015 13:20:32 -0800
Subject: [R] introduce axis break lattice plot multipanel
In-Reply-To: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>
References: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>
Message-ID: <CAGxFJbTVJTBiRjxS1fUuM19fMhMrXQDh6-ybrz-3aGzDVPpHuQ@mail.gmail.com>

Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Fri, Dec 18, 2015 at 12:32 PM, Luigi Marongiu
<marongiu.luigi at gmail.com> wrote:
> Dear all,
> I am plotting some data using lattice's barchart. One of the counts I
> am plotting has a very large value with respect to the other variables
> and I would like to introduce a break in the axis to compensate for
> this 'anomaly' and give more breath to the other bars. In this example
> the high count is for the variable 'b' and I would like to introduce a
> break of 50-100 in the x axis.
> I have seen from the internet that the common approach is to stack
> together two figures but in my case I am using a multipanel plot and I
> think this way won't work for my plot.
> Is there a simple way to introduce a break in the axis of a multipanel
> lattice plot?

No. (Thank goodness -- it defeats the purpose of the display).
-- Bert

> Thank you
> L
>
> the example:
>>>>
> A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
> 'b',    'c',    'd',    'b',    'c',    'd')
> B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>   1,    1,    1)
> C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
>   1,    1,    1)
> D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
>  1,    5,    0,    0)
> E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
>   1,    1,    1)
> DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
> library(lattice)
> barchart(
>     A ~ D|E,
>     DF,
>     groups = C,
>     stack = TRUE,
>     main = "Comparison of test results",
>     xlab = "Count",
>     col = c("yellow", "orange"),
>     par.settings = list(
>         strip.background = list(col="light grey"),
>         superpose.polygon=list(col= c("yellow", "orange"))
>     ),
>     scales = list(
>         alternating = FALSE
>     ),
>     key = list(
>         space="top",
>         columns=2,
>         text=list(c("Single infections", "Multiple infections"), col="black"),
>         rectangles=list(col=c("yellow", "orange"))
>     ),
> )
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Fri Dec 18 22:24:10 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Dec 2015 13:24:10 -0800
Subject: [R] introduce axis break lattice plot multipanel
In-Reply-To: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>
References: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>
Message-ID: <9266E242-33B8-4B1C-BC8E-0FE9E120D7FF@comcast.net>


> On Dec 18, 2015, at 12:32 PM, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
> 
> Dear all,
> I am plotting some data using lattice's barchart. One of the counts I
> am plotting has a very large value with respect to the other variables
> and I would like to introduce a break in the axis to compensate for
> this 'anomaly' and give more breath to the other bars. In this example
> the high count is for the variable 'b' and I would like to introduce a
> break of 50-100 in the x axis.
> I have seen from the internet that the common approach is to stack
> together two figures but in my case I am using a multipanel plot and I
> think this way won't work for my plot.
> Is there a simple way to introduce a break in the axis of a multipanel
> lattice plot?

If there is, then it's not easy to find. I tried with `sos::findFn("axis break lattice")`.

There are base graphic broken axes offered in package:plotrix. If you are sticking with lattice, then I would suggest a logged y-axis. Or use an annotating up-arrow with a label where the ylim was restricted.

-- 
David.
> Thank you
> L
> 
> the example:
>>>> 
> A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
> 'b',    'c',    'd',    'b',    'c',    'd')
> B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
>  1,    1,    1)
> C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
>  1,    1,    1)
> D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
> 1,    5,    0,    0)
> E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
>  1,    1,    1)
> DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
> library(lattice)
> barchart(
>    A ~ D|E,
>    DF,
>    groups = C,
>    stack = TRUE,
>    main = "Comparison of test results",
>    xlab = "Count",
>    col = c("yellow", "orange"),
>    par.settings = list(
>        strip.background = list(col="light grey"),
>        superpose.polygon=list(col= c("yellow", "orange"))
>    ),
>    scales = list(
>        alternating = FALSE
>    ),
>    key = list(
>        space="top",
>        columns=2,
>        text=list(c("Single infections", "Multiple infections"), col="black"),
>        rectangles=list(col=c("yellow", "orange"))
>    ),
> )
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Fri Dec 18 22:28:44 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Fri, 18 Dec 2015 13:28:44 -0800
Subject: [R] Cannot subset a specific mean from this function
In-Reply-To: <CAAT8CgdtvNmcA=1YKHCwSDf4En=9AH6zOx58+3QKt3PpK0Qv_g@mail.gmail.com>
References: <CAAT8Cgc1N1yt0RVh4RiLbG-DPXnkT74169C7NnXPHBma9+wL4g@mail.gmail.com>
	<CA+8X3fVF43=3J_JAozVFmp_cCbPw_0_wwjGF4CAb7eYHxJmRPQ@mail.gmail.com>
	<CAAT8CgdtvNmcA=1YKHCwSDf4En=9AH6zOx58+3QKt3PpK0Qv_g@mail.gmail.com>
Message-ID: <6A7905F0-AB4C-4EE6-90D8-0B2993F84A9B@comcast.net>


> On Dec 18, 2015, at 7:23 AM, Bradley Wolf <chanmo5752 at gmail.com> wrote:
> 
> Jim,
>  Thank you for your response.  I am not asking you for you to do my work.
> I am really new to programming (in any context) and really haven't gotten
> the grasp of the logic yet. I have used R before but in the context of R
> Commander and am trying this so I can be a more robust user.

Your question which you have not copied as requested by the posting guide is clearly part of a homework assignment for one of the online programming classes. I believe you were asked in the course introduction to submit questions to a web interface. Rhelp has a no homework policy.

-- 
David.

> 
> That being said, I wanted the last 53 file names read. The irony of my
> problem is, I can do the assignment if it was to get the mean of all the
> files (I am  reading 332 csv's).
> 
> Thanks,
> 
> Brad
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From S.Ellison at LGCGroup.com  Fri Dec 18 22:30:38 2015
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Fri, 18 Dec 2015 21:30:38 +0000
Subject: [R] Would you please help me to create a table in R?
In-Reply-To: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>
References: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403C945DB6B@GBTEDVPEXCMB04.corp.lgc-group.com>

> I want to get the table like "output". Any possibility to get it in R?

What do the rows represent in 'output'? Places? Times? Individuals?
What do the numbers in the table relate to? Individual bird identifier? Number of birds?



*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From drjimlemon at gmail.com  Fri Dec 18 23:58:51 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Sat, 19 Dec 2015 09:58:51 +1100
Subject: [R] Would you please help me to create a table in R?
In-Reply-To: <CAMwU6B0W53A0AY_TNuNi-2JuoYex843S-MnHgJfNc3nGbJqvVA@mail.gmail.com>
References: <CAMwU6B2L1RJ8xy+cZ-_pwFyoT=oj_YRC2DXXOkJzEsdNSDpVoQ@mail.gmail.com>
	<CA+8X3fVVskOftnHtYOsqmvnu1HNBWP6e3rYufrCkufgnGXbdrw@mail.gmail.com>
	<CAMwU6B0W53A0AY_TNuNi-2JuoYex843S-MnHgJfNc3nGbJqvVA@mail.gmail.com>
Message-ID: <CA+8X3fV6xaCBvTsu99nnvO3PFU9tEnGVX=zmeHNnVwhyXZNdug@mail.gmail.com>

Hi Marna,
Okay. I think I have a better idea now. I still don't quite get what
"output" represents, but I think the "table" you want is something like
what is produced by this code:

birdlevels<-c("0","SiteA","SiteB","SiteC")
raw.data<-data.frame(T2010=factor(rep("SiteA",11),levels=birdlevels),
 T2011=factor(c("0","SiteC","SiteA","0","SiteA","0","SiteA","0",
 "SiteB","SiteB","SiteB"),levels=birdlevels),
 T2012=factor(c("SiteA","SiteA","SiteA","0","SiteA","0","0",
 "SiteB","SiteB","0","SiteB"),levels=birdlevels))
birdcol<-c("gray","red","green","blue")
library(plotrix)
sizetree(raw.data,col=list(birdcol,birdcol,birdcol),
 main="Counts at release/capture sites")

and attached below. That is, I think what you want is a display of the
spatial transitions of the birds over time. If I'm correct, then maybe a
table like this will help:

                2010        2011         2012
Site A           11             3              4
Site B             0             3              3
Site C             0             1              0
Unknown         0             4              4

Jim


On Sat, Dec 19, 2015 at 4:57 AM, Marna Wagley <marna.wagley at gmail.com>
wrote:

> Dear Jim,
> I am sorry for not explaining the question in a clear way. I am trying to
> explain it, let's see how much clear I can make it.
>
> For the given example, (raw.data). Let's say, the 11 birds were marked and
> released at site A in a landscape (a combination of sites siteA, siteB,
> site C) in 2010.
>
> In 2011, we revisited at the sites (siteA, SiteB, SiteC), among the 11
> birds, we recaptured 3 individuals at Site A, 3 at Site B, 1 at site C but
> we could not see 4 individuals at any sites.
>
> Again in 2012, we revisited at the sites (SiteA, SiteB, SiteC), we again
> recaptured 2 individuals which was recaptured at Site A in 2011 but 1
> individual could not seen at any sites (in another words: in 2011 at the
> site A  we had recaptured 3 individuals, among them 2 were again seen at
> site A, but we could not see one individual in any sites).
>
> Similarly,we had recaptured 3 individuals in 2011 at the site B, but in
> 2012, among three individuals, one was again recaptured in Site B but 2
> individuals could not be seen at any sites.
>
> Likewise, for SiteC in 2012, we had recaptured 1 at that site (siteC), but
> in next year (2012), we could not see at any of the sites.
>
> Again, we had not seen 4 individuals in 2011, but later in 2012, among
> these 4 individuals, we recaptured one individual for each site  B and C,
> and 2 individuals could not be seen.
>
> I have given the example dataset. If it is really confusing and takes your
> lots of time, please forget it, I will try to do in Excel manually,but it
> is taking so much time and easily making errors. Your help will be very
> useful.
>
> Sincerely,
>
>
> raw.data<-structure(list(Time1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L), .Label = "SiteA", class = "factor"), Time2 =
> structure(c(1L, 4L, 2L, 1L, 2L, 1L, 2L, 1L, 3L, 3L, 3L), .Label = c("0",
> "SiteA",
> "SiteB", "SiteC"), class = "factor"), Time3 = structure(c(2L,
> 2L, 2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 3L), .Label = c("0", "SiteA",
> "SiteB"), class = "factor")), .Names = c("Time1", "Time2", "Time3"
> ), class = "data.frame", row.names = c(NA, -11L))
>
> raw.data
>
> #####
> table.format<-structure(list(Time1 = structure(c(NA, NA, NA, NA, NA, 1L,
> NA,NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = "Released A", class =
> "factor"),Time2 = structure(c(NA, NA, 2L, NA, NA, NA, 3L, NA, NA, NA,
>     4L, NA, NA, NA, 1L, NA), .Label = c("NotSeen", "Re-captured.at.A",
>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor"),
>     Time3 = structure(c(2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L,
>     4L, 1L, 2L, 3L, 4L, 1L), .Label = c("NotSeen", "Re-captured.at.A",
>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor")), .Names =
> c("Time1","Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
> ))
>
> table.format
>
> ###
> output<-structure(list(Time1 = c(NA, NA, NA, NA, NA, 11L, NA, NA, NA,
> NA, NA, NA, NA, NA, NA, NA), Time2 = c(NA, NA, 3L, NA, NA, NA,
> 3L, NA, NA, NA, 1L, NA, NA, NA, 4L, NA), Time3 = c(2L, NA, NA,
> 1L, NA, 2L, NA, 1L, 1L, NA, NA, NA, 1L, 1L, NA, 2L)), .Names = c("Time1",
> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
> ))
>
> output
>
> ##################################################
> ####################################################
>
>
> On Thu, Dec 17, 2015 at 11:38 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
>
>> Hi Marna,
>> A bit hard to understand. If raw.data is a record of 11 individuals
>> released at site A at Time 1 and recaptured at either A or B or neither at
>> Time2 or Time3, it doesn't seem to bear any consistent relationship to the
>> numeric coding in table.format or the output at the bottom. Could you
>> explain what the correspondence between the tables is?
>>
>> Jim
>>
>>
>> On Fri, Dec 18, 2015 at 2:33 PM, Marna Wagley <marna.wagley at gmail.com>
>> wrote:
>>
>>> Hi R users,
>>> I am struggling to create a table in R. I did in Excel but I have a lots
>>> of
>>> data and thought it might be easy in  R  but  I am new in R. How to get
>>> "output table" for this example data?
>>>
>>> This is an example.
>>>
>>> #####
>>> raw.data<-structure(list(Time1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L,
>>> 1L, 1L, 1L, 1L), .Label = "SiteA", class = "factor"), Time2 =
>>> structure(c(1L,
>>> 4L, 2L, 1L, 2L, 1L, 2L, 1L, 3L, 3L, 3L), .Label = c("0", "SiteA",
>>> "SiteB", "SiteC"), class = "factor"), Time3 = structure(c(2L,
>>> 2L, 2L, 1L, 2L, 1L, 1L, 3L, 3L, 1L, 3L), .Label = c("0", "SiteA",
>>> "SiteB"), class = "factor")), .Names = c("Time1", "Time2", "Time3"
>>> ), class = "data.frame", row.names = c(NA, -11L))
>>>
>>> raw.data
>>>
>>> #####
>>> table.format<-structure(list(Time1 = structure(c(NA, NA, NA, NA, NA, 1L,
>>> NA,
>>> NA, NA, NA, NA, NA, NA, NA, NA, NA), .Label = "Released A", class =
>>> "factor"),
>>>     Time2 = structure(c(NA, NA, 2L, NA, NA, NA, 3L, NA, NA, NA,
>>>     4L, NA, NA, NA, 1L, NA), .Label = c("Dead", "Re-captured.at.A",
>>>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor"),
>>>     Time3 = structure(c(2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L,
>>>     4L, 1L, 2L, 3L, 4L, 1L), .Label = c("Dead", "Re-captured.at.A",
>>>     "Re-captured.at.B", "Re-captured.at.C"), class = "factor")), .Names =
>>> c("Time1",
>>> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
>>> ))
>>>
>>> table.format
>>>
>>> ###
>>> output<-structure(list(Time1 = c(NA, NA, NA, NA, NA, 11L, NA, NA, NA,
>>> NA, NA, NA, NA, NA, NA, NA), Time2 = c(NA, NA, 3L, NA, NA, NA,
>>> 3L, NA, NA, NA, 1L, NA, NA, NA, 4L, NA), Time3 = c(2L, NA, NA,
>>> 1L, NA, 2L, NA, 1L, 1L, NA, NA, NA, 1L, 1L, NA, 2L)), .Names = c("Time1",
>>> "Time2", "Time3"), class = "data.frame", row.names = c(NA, -16L
>>> ))
>>>
>>> output
>>>
>>> I want to get the table like "output". Any possibility to get it in R?
>>>
>>> I will really appreciate for your help. I am struggling to create  this
>>> type of table.
>>>
>>>
>>> Sincerely,
>>> Marna
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>
>>
>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: birdsites.pdf
Type: application/pdf
Size: 4709 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151219/9e10ec9f/attachment.pdf>

From sabasehrish at yahoo.com  Sat Dec 19 02:31:09 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Sat, 19 Dec 2015 01:31:09 +0000 (UTC)
Subject: [R] error in vcovNW
References: <1126748302.763705.1450488669663.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1126748302.763705.1450488669663.JavaMail.yahoo@mail.yahoo.com>

Hi?I am using NeweyWest standard errors to correct lm( ) output. For example:
lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)
vcovNW<-NeweyWest(lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5))

I am using package(sandwich) for NeweyWest. Now when I run this command, it gives following error:
Error in solve.default(diag(ncol(umat)) - apply(var.fit$ar, 2:3, sum)) :system is computationally singular: reciprocal condition number = 7.49468e-18

Attached herewith is data for A&B, A1,A2,A3,A4,A5,B1,B2,B3,B4,B5 are simply lag variables. Can you help me removing this error please?
Saba
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: data.txt
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151219/d41cb052/attachment.txt>

From Achim.Zeileis at uibk.ac.at  Sat Dec 19 03:06:44 2015
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Sat, 19 Dec 2015 03:06:44 +0100 (CET)
Subject: [R] error in vcovNW
In-Reply-To: <1126748302.763705.1450488669663.JavaMail.yahoo@mail.yahoo.com>
References: <1126748302.763705.1450488669663.JavaMail.yahoo.ref@mail.yahoo.com>
	<1126748302.763705.1450488669663.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <alpine.DEB.2.11.1512190302120.2546@paninaro.uibk.ac.at>

On Sat, 19 Dec 2015, Saba Sehrish via R-help wrote:

> Hi?I am using NeweyWest standard errors to correct lm( ) output. For example:
> lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)
> vcovNW<-NeweyWest(lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5))
>
> I am using package(sandwich) for NeweyWest. Now when I run this command, it gives following error:
> Error in solve.default(diag(ncol(umat)) - apply(var.fit$ar, 2:3, sum)) :system is computationally singular: reciprocal condition number = 7.49468e-18
>
> Attached herewith is data for A&B, A1,A2,A3,A4,A5,B1,B2,B3,B4,B5 are 
> simply lag variables. Can you help me removing this error please?

Without trying to replicate the error, there are at least two issues:

(1) You should scale your data to use more reasonable orders of magnitude, 
e.g., in millions. This will help avoiding numerical problems.

(2) More importantly, you should not employ HAC/Newey-West standard errors 
in autoregressive models. If you use an autoregressive specification, you 
should capture all relevant autocorrelations - and then no HAC estimator 
is necessary. Alternatively, one may treat autocorrelation as a nuisance 
parameter and not model it - but instead capture it in HAC standard 
errors. Naturally, the former strategy will typically perform better if 
the autocorrelations are more substantial.

> Saba

From sabasehrish at yahoo.com  Sat Dec 19 03:53:57 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Sat, 19 Dec 2015 02:53:57 +0000 (UTC)
Subject: [R] error in vcovNW
In-Reply-To: <alpine.DEB.2.11.1512190302120.2546@paninaro.uibk.ac.at>
References: <alpine.DEB.2.11.1512190302120.2546@paninaro.uibk.ac.at>
Message-ID: <1615240575.763613.1450493637990.JavaMail.yahoo@mail.yahoo.com>

Thank you. The issue is resolved by scaling the data in millions.
Saba 

    On Saturday, 19 December 2015, 15:06, Achim Zeileis <Achim.Zeileis at uibk.ac.at> wrote:
 

 On Sat, 19 Dec 2015, Saba Sehrish via R-help wrote:

> Hi?I am using NeweyWest standard errors to correct lm( ) output. For example:
> lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)
> vcovNW<-NeweyWest(lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5))
>
> I am using package(sandwich) for NeweyWest. Now when I run this command, it gives following error:
> Error in solve.default(diag(ncol(umat)) - apply(var.fit$ar, 2:3, sum)) :system is computationally singular: reciprocal condition number = 7.49468e-18
>
> Attached herewith is data for A&B, A1,A2,A3,A4,A5,B1,B2,B3,B4,B5 are 
> simply lag variables. Can you help me removing this error please?

Without trying to replicate the error, there are at least two issues:

(1) You should scale your data to use more reasonable orders of magnitude, 
e.g., in millions. This will help avoiding numerical problems.

(2) More importantly, you should not employ HAC/Newey-West standard errors 
in autoregressive models. If you use an autoregressive specification, you 
should capture all relevant autocorrelations - and then no HAC estimator 
is necessary. Alternatively, one may treat autocorrelation as a nuisance 
parameter and not model it - but instead capture it in HAC standard 
errors. Naturally, the former strategy will typically perform better if 
the autocorrelations are more substantial.

> Saba

  
	[[alternative HTML version deleted]]


From joeceradini at gmail.com  Fri Dec 18 19:50:56 2015
From: joeceradini at gmail.com (Joe Ceradini)
Date: Fri, 18 Dec 2015 11:50:56 -0700
Subject: [R] Missing data in RMark
In-Reply-To: <D98009DC-3F5A-4730-AFE2-F409573AAF18@science.oregonstate.edu>
References: <D98009DC-3F5A-4730-AFE2-F409573AAF18@science.oregonstate.edu>
Message-ID: <CAKq2vL5mfXv5cTY+oJ2UszsFGg62TCWqMxSvr6=fi2-GBa=WVQ@mail.gmail.com>

Caroline - the phidot forum is an *excellent* spot to post this question.
There is an entire RMark subforum.
www.phidot.org/forum/index.php
Even just searching this forum will probably give you some answers.

Also, this book has a whole section on individual covariates and approaches
for dealing with missing values (it is an amazing MARK resource in general):
www.phidot.org/software/mark/docs/book/

The problem you are having is a fundamental problem with individual
covariates that are unknown for individuals that were not captured.

Joe

On Fri, Dec 18, 2015 at 10:42 AM, Caroline Glidden <
gliddeca at science.oregonstate.edu> wrote:

> I am currently trying to run a Known Fates Model in RMark with individual
> time varying covariates. However, for animals that died early in the study
> or were not captured at one capture period I, of course, do not have data
> for all of their time points. I thought that NAs would not matter when the
> LD capture history was 00, and therefore, having no data during the time
> points the animal was unable to be sampled would not be a big deal.
> However, RMark cannot fit my model due to the NAs. I was wondering how I
> should code the individual time varying covariates for time points at which
> the animal has no data due to it not being captured or due to it dying?
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>



-- 
Cooperative Fish and Wildlife Research Unit
Zoology and Physiology Dept.
University of Wyoming
JoeCeradini at gmail.com / 914.707.8506
wyocoopunit.org

	[[alternative HTML version deleted]]


From webweb8537 at gmail.com  Fri Dec 18 20:14:46 2015
From: webweb8537 at gmail.com (Web Web)
Date: Fri, 18 Dec 2015 21:14:46 +0200
Subject: [R] Help needed in data cleaning
Message-ID: <CAMQbFmMJm7NA05++rHhhFYBTMQ72whEzEk+pZVApY0F_tvwO5A@mail.gmail.com>

Hello,
           I need some help in data cleaning using R. my CSV file looks as
follows.

"id","gender","age","category1","category2","category3","category4","category5","category6","category7","category8","category9","category10"1,"Male",22,"movies","music","travel","cloths","grocery",,,,,2,"Male",28,"travel","books","movies",,,,,,,3,"Female",27,"rent","fuel","grocery","cloths",,,,,,4,"Female",22,"rent","grocery","travel","movies","cloths",,,,,5,"Female",22,"rent","online-shopping","utiliy",,,,,,,

I need to reformat as follows.

id gender age category            rank1 Male    22  movies
  11 Male    22  music                21 Male    22  travel
   31 Male    22  cloths               41 Male    22  grocery
    51 Male    22  books                NA1 Male    22  rent
      NA1 Male    22  fuel                 NA1 Male    22  utility
         NA1 Male    22  online-shopping      NA
...................................5 Female    22  movies
NA5 Female    22  music              NA5 Female    22  travel
   NA5 Female    22  cloths             NA5 Female    22  grocery
      NA5 Female    22  books              NA5 Female    22  rent
         15 Female    22  fuel               NA5 Female    22  utility
           NA5 Female    22  online-shopping    2

So far My efforts are as follows.

mini <- read.csv("~/MS/coding/mini.csv", header=FALSE)
mini_clean <- mini[-1,]
df_mini <- melt(df_clean, id.vars=c("V1","V2","V3"))
sqldf('select * from df_mini order by  "V1"')

Now I want to know what is the best way to fill all missing categories for
all users.

Thanks
Nash

	[[alternative HTML version deleted]]


From chanmo5752 at gmail.com  Fri Dec 18 23:26:12 2015
From: chanmo5752 at gmail.com (Bradley Wolf)
Date: Fri, 18 Dec 2015 16:26:12 -0600
Subject: [R] Cannot subset a specific mean from this function
In-Reply-To: <6A7905F0-AB4C-4EE6-90D8-0B2993F84A9B@comcast.net>
References: <CAAT8Cgc1N1yt0RVh4RiLbG-DPXnkT74169C7NnXPHBma9+wL4g@mail.gmail.com>
	<CA+8X3fVF43=3J_JAozVFmp_cCbPw_0_wwjGF4CAb7eYHxJmRPQ@mail.gmail.com>
	<CAAT8CgdtvNmcA=1YKHCwSDf4En=9AH6zOx58+3QKt3PpK0Qv_g@mail.gmail.com>
	<6A7905F0-AB4C-4EE6-90D8-0B2993F84A9B@comcast.net>
Message-ID: <CAAT8CgeG=GFEvbS_mO=vzBU1Ah4WN9kkZunActorDqj2qZX4dw@mail.gmail.com>

David,
  I withdrew from the class because I couldn't understand it. Apologies. I
shall ask no more questions.

Brad

On Fri, Dec 18, 2015 at 3:28 PM, David Winsemius <dwinsemius at comcast.net>
wrote:

>
> > On Dec 18, 2015, at 7:23 AM, Bradley Wolf <chanmo5752 at gmail.com> wrote:
> >
> > Jim,
> >  Thank you for your response.  I am not asking you for you to do my work.
> > I am really new to programming (in any context) and really haven't gotten
> > the grasp of the logic yet. I have used R before but in the context of R
> > Commander and am trying this so I can be a more robust user.
>
> Your question which you have not copied as requested by the posting guide
> is clearly part of a homework assignment for one of the online programming
> classes. I believe you were asked in the course introduction to submit
> questions to a web interface. Rhelp has a no homework policy.
>
> --
> David.
>
> >
> > That being said, I wanted the last 53 file names read. The irony of my
> > problem is, I can do the assignment if it was to get the mean of all the
> > files (I am  reading 332 csv's).
> >
> > Thanks,
> >
> > Brad
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
>

	[[alternative HTML version deleted]]


From Achim.Zeileis at uibk.ac.at  Sat Dec 19 11:26:25 2015
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Sat, 19 Dec 2015 11:26:25 +0100 (CET)
Subject: [R] error in vcovNW
In-Reply-To: <1615240575.763613.1450493637990.JavaMail.yahoo@mail.yahoo.com>
References: <alpine.DEB.2.11.1512190302120.2546@paninaro.uibk.ac.at>
	<1615240575.763613.1450493637990.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <alpine.DEB.2.11.1512191125390.4794@paninaro.uibk.ac.at>

On Sat, 19 Dec 2015, Saba Sehrish wrote:

> Thank you. The issue is resolved by scaling the data in millions.

That solves the numerical problem but the second issue (inappropriateness 
of the Newey-West estimator for an autoregressive model) persists.

> Saba
> 
> 
> On Saturday, 19 December 2015, 15:06, Achim Zeileis
> <Achim.Zeileis at uibk.ac.at> wrote:
> 
> 
> On Sat, 19 Dec 2015, Saba Sehrish via R-help wrote:
> 
> > Hi?I am using NeweyWest standard errors to correct lm( ) output. For
> example:
> > lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)
> > vcovNW<-NeweyWest(lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5))
> >
> > I am using package(sandwich) for NeweyWest. Now when I run this command,
> it gives following error:
> > Error in solve.default(diag(ncol(umat)) - apply(var.fit$ar, 2:3, sum))
> :system is computationally singular: reciprocal condition number =
> 7.49468e-18
> >
> > Attached herewith is data for A&B, A1,A2,A3,A4,A5,B1,B2,B3,B4,B5 are
> > simply lag variables. Can you help me removing this error please?
> 
> Without trying to replicate the error, there are at least two issues:
> 
> (1) You should scale your data to use more reasonable orders of magnitude,
> e.g., in millions. This will help avoiding numerical problems.
> 
> (2) More importantly, you should not employ HAC/Newey-West standard errors
> in autoregressive models. If you use an autoregressive specification, you
> should capture all relevant autocorrelations - and then no HAC estimator
> is necessary. Alternatively, one may treat autocorrelation as a nuisance
> parameter and not model it - but instead capture it in HAC standard
> errors. Naturally, the former strategy will typically perform better if
> the autocorrelations are more substantial.
> 
> > Saba
> 
> 
> 
>

From sabasehrish at yahoo.com  Sat Dec 19 13:25:32 2015
From: sabasehrish at yahoo.com (Saba Sehrish)
Date: Sat, 19 Dec 2015 12:25:32 +0000 (UTC)
Subject: [R] error in vcovNW
In-Reply-To: <alpine.DEB.2.11.1512191125390.4794@paninaro.uibk.ac.at>
References: <alpine.DEB.2.11.1512191125390.4794@paninaro.uibk.ac.at>
Message-ID: <107425639.844939.1450527932618.JavaMail.yahoo@mail.yahoo.com>

Hi
Thanks for the reminder.
Actually I want to analyse whether present value of variable A is Granger caused by lag values of B and test linear hypothesis "B1,B2,B3,B4,B5=0".
Therefore, to get robust standard error NeweyWest estimates are applied.
Saba 

    On Saturday, 19 December 2015, 23:26, Achim Zeileis <Achim.Zeileis at uibk.ac.at> wrote:
 

 On Sat, 19 Dec 2015, Saba Sehrish wrote:

> Thank you. The issue is resolved by scaling the data in millions.

That solves the numerical problem but the second issue (inappropriateness 
of the Newey-West estimator for an autoregressive model) persists.

> Saba
> 
> 
> On Saturday, 19 December 2015, 15:06, Achim Zeileis
> <Achim.Zeileis at uibk.ac.at> wrote:
> 
> 
> On Sat, 19 Dec 2015, Saba Sehrish via R-help wrote:
> 
> > Hi?I am using NeweyWest standard errors to correct lm( ) output. For
> example:
> > lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5)
> > vcovNW<-NeweyWest(lm(A~A1+A2+A3+A4+A5+B1+B2+B3+B4+B5))
> >
> > I am using package(sandwich) for NeweyWest. Now when I run this command,
> it gives following error:
> > Error in solve.default(diag(ncol(umat)) - apply(var.fit$ar, 2:3, sum))
> :system is computationally singular: reciprocal condition number =
> 7.49468e-18
> >
> > Attached herewith is data for A&B, A1,A2,A3,A4,A5,B1,B2,B3,B4,B5 are
> > simply lag variables. Can you help me removing this error please?
> 
> Without trying to replicate the error, there are at least two issues:
> 
> (1) You should scale your data to use more reasonable orders of magnitude,
> e.g., in millions. This will help avoiding numerical problems.
> 
> (2) More importantly, you should not employ HAC/Newey-West standard errors
> in autoregressive models. If you use an autoregressive specification, you
> should capture all relevant autocorrelations - and then no HAC estimator
> is necessary. Alternatively, one may treat autocorrelation as a nuisance
> parameter and not model it - but instead capture it in HAC standard
> errors. Naturally, the former strategy will typically perform better if
> the autocorrelations are more substantial.
> 
> > Saba
> 
> 
> 
>

  
	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Sat Dec 19 14:42:21 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Sat, 19 Dec 2015 23:42:21 +1000
Subject: [R] introduce axis break lattice plot multipanel
In-Reply-To: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>
References: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>
Message-ID: <001201d13a63$16895f90$439c1eb0$@bigpond.com>

Hi Luigi

I suppose a cheats way out would be to put

    scales = list(alternating = FALSE,
                  x = list(relation = "free") ),

Regards

Duncan


Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
Marongiu
Sent: Saturday, 19 December 2015 06:32
To: r-help
Subject: [R] introduce axis break lattice plot multipanel

Dear all,
I am plotting some data using lattice's barchart. One of the counts I
am plotting has a very large value with respect to the other variables
and I would like to introduce a break in the axis to compensate for
this 'anomaly' and give more breath to the other bars. In this example
the high count is for the variable 'b' and I would like to introduce a
break of 50-100 in the x axis.
I have seen from the internet that the common approach is to stack
together two figures but in my case I am using a multipanel plot and I
think this way won't work for my plot.
Is there a simple way to introduce a break in the axis of a multipanel
lattice plot?
Thank you
L

the example:
>>>
A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
'b',    'c',    'd',    'b',    'c',    'd')
B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
  1,    1,    1)
C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
  1,    1,    1)
D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
 1,    5,    0,    0)
E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
  1,    1,    1)
DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
library(lattice)
barchart(
    A ~ D|E,
    DF,
    groups = C,
    stack = TRUE,
    main = "Comparison of test results",
    xlab = "Count",
    col = c("yellow", "orange"),
    par.settings = list(
        strip.background = list(col="light grey"),
        superpose.polygon=list(col= c("yellow", "orange"))
    ),
    scales = list(
        alternating = FALSE
    ),
    key = list(
        space="top",
        columns=2,
        text=list(c("Single infections", "Multiple infections"),
col="black"),
        rectangles=list(col=c("yellow", "orange"))
    ),
)

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From abhinabaroy09 at gmail.com  Sat Dec 19 20:26:03 2015
From: abhinabaroy09 at gmail.com (Abhinaba Roy)
Date: Sun, 20 Dec 2015 00:56:03 +0530
Subject: [R] Changing column names by unique factor levels
Message-ID: <CANtKHPV52AOXRNhFp35ZM-f7VeRa_fOMZUW37Y9tN5_H=24rbw@mail.gmail.com>

Hi R helpers,

I am stuck with a very strange problem.

My input data is

structure(list(Date = structure(c(1447007400, 1447007400, 1447093800,
1447093800), tzone = "", class = c("POSIXct", "POSIXt")), Prod = c("Colg",
"P&G", "Colg", "P&G"), Amt = c(57, 11, 62, 77), Amt1 = c(86,
72, 20, 23), Amt2 = c(42, 55, 30, 13), Amt3 = c(75, 12, 13, 23
)), .Names = c("Date", "Prod", "Amt", "Amt1", "Amt2", "Amt3"), row.names =
c(NA,
-4L), class = "data.frame")

And, my output data should look like

structure(list(Date = structure(c(1447007400, 1447093800), tzone = "",
class = c("POSIXct",
"POSIXt")), Colg_Amt = c(55, 88), Colg_Amt1 = c(34, 47), Colg_Amt2 = c(63,
36), Colg_Amt3 = c(98, 14), P.G_Amt = c(13, 89), P.G_Amt1 = c(35,
44), P.G_Amt2 = c(90, 64), P.G_Amt3 = c(22, 12)), .Names = c("Date",
"Colg_Amt", "Colg_Amt1", "Colg_Amt2", "Colg_Amt3", "P.G_Amt",
"P.G_Amt1", "P.G_Amt2", "P.G_Amt3"), row.names = c(NA, -2L), class =
"data.frame")

The problem to be solved is that I would like the final data to have the
column names prefixed by the Prod values. I think it can be done using
dcast, but not sure how to use it efficiently.

How can I do this in R?

Thanks,
Abhinaba

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Sun Dec 20 00:33:53 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Sat, 19 Dec 2015 15:33:53 -0800
Subject: [R] Changing column names by unique factor levels
In-Reply-To: <CANtKHPV52AOXRNhFp35ZM-f7VeRa_fOMZUW37Y9tN5_H=24rbw@mail.gmail.com>
References: <CANtKHPV52AOXRNhFp35ZM-f7VeRa_fOMZUW37Y9tN5_H=24rbw@mail.gmail.com>
Message-ID: <CAGxFJbQ5R46D4J6xz=krUFOkb=7bM1gv_Tv1ghHFEkNkjRCBJw@mail.gmail.com>

1. Why do you think this is a "very strange" problem?

2. Your minimal example is helpful, but confusing: the entries in the
2nd table appear to have nothing to do with the first. I'll assume
that it's just the *form* that you want to show. Also, your
specification may be incomplete: Do the rows continue to alternate in
this regular way? Are there just a few unique products or many?
Assuming regularity and just a few unique products, then simple
indexing is all that's necessary.

Let your original frame be name df. Then

n<- seq_len(nrow(df))
new.df <-cbind(df[n%%2 ==1,-2],df[n%%2==0,-(1:2)])
names(new.df)[-1]<-
as.vector(t(outer(unique(df$Prod),names(df)[-(1:2)],paste,sep="_")))


produces a new.df with the entries from df and the form of your second
data frame. Is that what you want??
If so, great! If not, someone else will have to help, as I don't care
to waste time chasing poorly phrased questions.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Sat, Dec 19, 2015 at 11:26 AM, Abhinaba Roy <abhinabaroy09 at gmail.com> wrote:
> Hi R helpers,
>
> I am stuck with a very strange problem.
>
> My input data is
>
> structure(list(Date = structure(c(1447007400, 1447007400, 1447093800,
> 1447093800), tzone = "", class = c("POSIXct", "POSIXt")), Prod = c("Colg",
> "P&G", "Colg", "P&G"), Amt = c(57, 11, 62, 77), Amt1 = c(86,
> 72, 20, 23), Amt2 = c(42, 55, 30, 13), Amt3 = c(75, 12, 13, 23
> )), .Names = c("Date", "Prod", "Amt", "Amt1", "Amt2", "Amt3"), row.names =
> c(NA,
> -4L), class = "data.frame")
>
> And, my output data should look like
>
> structure(list(Date = structure(c(1447007400, 1447093800), tzone = "",
> class = c("POSIXct",
> "POSIXt")), Colg_Amt = c(55, 88), Colg_Amt1 = c(34, 47), Colg_Amt2 = c(63,
> 36), Colg_Amt3 = c(98, 14), P.G_Amt = c(13, 89), P.G_Amt1 = c(35,
> 44), P.G_Amt2 = c(90, 64), P.G_Amt3 = c(22, 12)), .Names = c("Date",
> "Colg_Amt", "Colg_Amt1", "Colg_Amt2", "Colg_Amt3", "P.G_Amt",
> "P.G_Amt1", "P.G_Amt2", "P.G_Amt3"), row.names = c(NA, -2L), class =
> "data.frame")
>
> The problem to be solved is that I would like the final data to have the
> column names prefixed by the Prod values. I think it can be done using
> dcast, but not sure how to use it efficiently.
>
> How can I do this in R?
>
> Thanks,
> Abhinaba
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From vindoggy at hotmail.com  Sun Dec 20 05:39:21 2015
From: vindoggy at hotmail.com (Vinny Mo)
Date: Sun, 20 Dec 2015 04:39:21 +0000
Subject: [R] Use of "file.choose()" or "change working directory" tab
 causing stall on Mac
Message-ID: <BN3PR15MB0804E322FCC4F4391A963258CCE30@BN3PR15MB0804.namprd15.prod.outlook.com>

Hello,


I used to use the "file.choose()" command quite a lot, as well as the "change working directory" drop down tab as part of my workflow with R, but for over 1 year both of these actions have caused the spinning-wheel to crash R (just R, not any other program).


The issue seems to happen when the GUI pops up, and happens about 80% of the time I use either of these actions. This issue has remained constant across different computers (though all macs), different R builds, and different Mac OS's. I had asked about this issue before, and had hoped that this bug might be fixed at some point, but it has persisted.


I know I can work around this issue programmatically by typing these commands manually, but both of these features represent a nice function that R has that I'd like to continue to use as was intended. Does anyone have any idea how I might be able to get this functionality back, or if the R Gods have any thoughts about addressing this issue?

	[[alternative HTML version deleted]]


From marongiu.luigi at gmail.com  Sun Dec 20 14:02:21 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Sun, 20 Dec 2015 13:02:21 +0000
Subject: [R] neuralnet to discriminate a given outcome by giving cutoff
	outputs
Message-ID: <CAMk+s2Qy14AzNR_thriiheTz2u2=Nk1Za08++qCH-454=f8apw@mail.gmail.com>

Dear all,
I am trying to use neural networks to discriminate positive and
negative outcomes from a test; this outcome is given in the z variable
of the example I am providing. Each outcome is associated with a pair
of variables x and y and I was planning to identify the cut-offs that
could separate the positive from the negative results by splitting the
x/y space into 4 quadrants, the positive results lying in the 2nd
quadrant (the top right). In the example, the two cut-offs for x and y
are indicated with green lines (the values are absolutely subjective),
positive results are in red.
I was thinking of using the neural networks (implemented with the
neuralnet package) to obtain the x and y cut-offs but the returned
weight are off-scale. I tried to increment the number of hidden
neurons, to use more replicates and a maximum number of iteractions
but the results are never optimal.
Since I am completely new in neural networks usage, could you tell me
if the implementation is fine, where could I improve the model and
whether the neural network is the right approach for the problem I
need to solve?
Thank you
L

>>>
x<-c(31.1,    32.07,    33.01,    36.05,    17.88,    26.91,    27.35,
   38.44,    35.92,    38.08,    26.14,    40.35,    36.12,    21.51,
  22.75,    37.2,    16.94,    25.61,    25.83,    34.73,    36.93,
36.02,    37.43,    36.09,    34.74,    35.85,    16.35,    35.25,
26.98,    4.74,    2.89,    40.39,    42.18,    37.76,    2.73,
2.76,    2.64,    41.94,    45,    3.78,    2.86,    45,    2.77,
4.61,    2.52,    42.31,    2.92,    2.94,    3.01,    2.61,    45,
45,    44.54,    44.37,    36.24,    2.11,    2,    3.97,    44.54)
y<-c(0.08,    0.054,    0.082,    0.095,    0.135,    0.129,    0.099,
   0.072,    0.091,    0.093,    0.104,    0.077,    0.107,    0.108,
  0.107,    0.088,    0.121,    0.111,    0.069,    0.106,    0.041,
 0.091,    0.054,    0.083,    0.089,    0.086,    0.073,    0.085,
0.091,    0.008,    0.018,    0,    0,    0.001,    0.002,    0.011,
 0.001,    0.002,    -0.001,    0.005,    0.011,    0.002,    0.008,
 0.006,    0.006,    0.001,    0.006,    0.003,    0.015,    0.003,
0.002,    0.001,    0,    0.001,    0.002,    0.001,    -0.001,
0.002,    -0.001)
z<-c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
 1,    1,    1,    1,    1,    1,    0,    0,    0,    0,    0,    0,
  0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
0)
m<-data.frame(x,y,z)
pos <- subset(m, z == 1)
neg <- subset(m, z == 0)
# show points
X <- max(m$x)
Y <- max(m$y)
plot(pos$x, pos$y, pch=1, col="red",
     xlim=c(1,X), ylim=c(0,Y),
     xlab="x", ylab="y", main ="Training dataset",
     sub="Red = positive outcome; Black = negative outcome")
points(neg$x, neg$y, pch=1, col="black")
abline(v=6, lty=2, col="green")
abline(h=0.03, lty=2, col="green")
library(neuralnet)
tnm <- neuralnet(z ~ x + y,
                 data=m,
                 hidden=1,
                 algorithm = "rprop+",
                 err.fct = "ce",
                 act.fct = "logistic",
                 linear.output = FALSE
)
tnm$result.matrix
co.x <- tnm$result.matrix[5]
co.y <- tnm$result.matrix[6]
abline(v=co.x, lty=2, col="blue")
abline(h=co.y, lty=2, col="blue")


From rbaer at atsu.edu  Sun Dec 20 16:34:36 2015
From: rbaer at atsu.edu (Robert Baer)
Date: Sun, 20 Dec 2015 09:34:36 -0600
Subject: [R] Use of "file.choose()" or "change working directory" tab
 causing stall on Mac
In-Reply-To: <BN3PR15MB0804E322FCC4F4391A963258CCE30@BN3PR15MB0804.namprd15.prod.outlook.com>
References: <BN3PR15MB0804E322FCC4F4391A963258CCE30@BN3PR15MB0804.namprd15.prod.outlook.com>
Message-ID: <5676CA8C.9010600@atsu.edu>



On 12/19/2015 10:39 PM, Vinny Mo wrote:
> Hello,
>
>
> I used to use the "file.choose()" command quite a lot, as well as the "change working directory" drop down tab as part of my workflow with R, but for over 1 year both of these actions have caused the spinning-wheel to crash R (just R, not any other program).
>
>
> The issue seems to happen when the GUI pops up, and happens about 80% of the time I use either of these actions. This issue has remained constant across different computers (though all macs), different R builds, and different Mac OS's. I had asked about this issue before, and had hoped that this bug might be fixed at some point, but it has persisted.
The posting guide asks for a reproducible example.  This is problematic 
if you only have only an 80% failure rate. Nevertheless, do you have a 
verbatim example that has failed at least once?  If a particular 
formulation fails one time for you, does it always fail or can a certain 
syntax work 1 in 5 times?

If this is a mac-only problem you might get more help on that mailing list:
*https://stat.ethz.ch/mailman/listinfo/r-sig-mac***

You should install the most recent version of R and reproduce the 
problem there.  When posting again, it would be helpful to supply the 
results of
R.Version()   for your setup.

>
> I know I can work around this issue programmatically by typing these commands manually, but both of these features represent a nice function that R has that I'd like to continue to use as was intended. Does anyone have any idea how I might be able to get this functionality back, or if the R Gods have any thoughts about addressing this issue?
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Sun Dec 20 20:14:34 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Sun, 20 Dec 2015 19:14:34 +0000
Subject: [R] neuralnet to discriminate a given outcome by giving cutoff
 outputs
Message-ID: <248E6FA047A8C746BA491485764190F53715E539@ESESSMB207.ericsson.se>

I would tackle the problem in the following way:

lm.model <- lm(z~ x + y, data=m)
summary(lm.model)



Call:

lm(formula = z ~ x + y, data = m)



Residuals:

        Min          1Q      Median          3Q         Max

-0.34476713 -0.09571506 -0.01786731  0.05225554  0.51693389



Coefficients:

                Estimate   Std. Error  t value               Pr(>|t|)

(Intercept) -0.071612058  0.041196651 -1.73830              0.0876543 .

x            0.003952998  0.001336833  2.95699              0.0045417 **

y            9.968145059  0.461213516 21.61286 < 0.000000000000000222 ***

---

Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1



Residual standard error: 0.157775 on 56 degrees of freedom

Multiple R-squared:  0.905464, Adjusted R-squared:  0.9020877

F-statistic: 268.1834 on 2 and 56 DF,  p-value: < 0.00000000000000022204


coef.model <- coef(lm.model)

z.hat <- coef.model[1]+coef.model[2]*x+coef.model[3]*y

z.hat.discrete <- rep(1, length(z.hat))
z.hat.discrete[z.hat <0.4] <- 0


> all.equal(z, z.hat.discrete)

[1] TRUE


I apologise for using such naif approach in place of neural net.

--
GG

http://around-r.blogspot.it


	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Sun Dec 20 22:19:45 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Sun, 20 Dec 2015 21:19:45 +0000
Subject: [R] neuralnet to discriminate a given outcome by giving cutoff
 outputs
Message-ID: <248E6FA047A8C746BA491485764190F53715E592@ESESSMB207.ericsson.se>

About my previous answer, I should have taken advantage of glm() in place of lm(), as the response is binomial.

--
GG


	[[alternative HTML version deleted]]


From dulcalma at bigpond.com  Mon Dec 21 04:28:35 2015
From: dulcalma at bigpond.com (Duncan Mackay)
Date: Mon, 21 Dec 2015 13:28:35 +1000
Subject: [R] introduce axis break lattice plot multipanel
In-Reply-To: <CAMk+s2STtxAz-v3S7Ls00wXgvOrC+2u-m8z8yDUEcMBHU4fEKA@mail.gmail.com>
References: <CAMk+s2RiB92pzffe5yaZiP0R++Fbso==TqO23zf+XW+KLH7Hnw@mail.gmail.com>	<001201d13a63$16895f90$439c1eb0$@bigpond.com>
	<CAMk+s2STtxAz-v3S7Ls00wXgvOrC+2u-m8z8yDUEcMBHU4fEKA@mail.gmail.com>
Message-ID: <000301d13b9f$ae0902c0$0a1b0840$@bigpond.com>

Hi Luigi

 

If you went down the relation = ?free? road you could change the panel widths to reflect the scales 

eg if lhs was 1:200 and rhs was 1:100 then the lhs panel would be twice as wide as the rhs.

 

I have some code reasonably handy I could send it to you if needed.

 

Regards

 

Duncan

 

From: Luigi Marongiu [mailto:marongiu.luigi at gmail.com] 
Sent: Sunday, 20 December 2015 20:35
To: Duncan Mackay
Subject: RE: [R] introduce axis break lattice plot multipanel

 

thank you duncan,
but in that case the scale of the two panel would be different. i think ill keep it as it comes.
best wishes of a merry xmas
luigi

On 19 Dec 2015 13:42, "Duncan Mackay" <dulcalma at bigpond.com> wrote:

Hi Luigi

I suppose a cheats way out would be to put

    scales = list(alternating = FALSE,
                  x = list(relation = "free") ),

Regards

Duncan


Duncan Mackay
Department of Agronomy and Soil Science
University of New England
Armidale NSW 2351
Email: home: mackay at northnet.com.au

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Luigi
Marongiu
Sent: Saturday, 19 December 2015 06:32
To: r-help
Subject: [R] introduce axis break lattice plot multipanel

Dear all,
I am plotting some data using lattice's barchart. One of the counts I
am plotting has a very large value with respect to the other variables
and I would like to introduce a break in the axis to compensate for
this 'anomaly' and give more breath to the other bars. In this example
the high count is for the variable 'b' and I would like to introduce a
break of 50-100 in the x axis.
I have seen from the internet that the common approach is to stack
together two figures but in my case I am using a multipanel plot and I
think this way won't work for my plot.
Is there a simple way to introduce a break in the axis of a multipanel
lattice plot?
Thank you
L

the example:
>>>
A <- c('a',    'b',    'c',    'd',    'a',    'b',    'c',    'd',
'b',    'c',    'd',    'b',    'c',    'd')
B <- c(1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
  1,    1,    1)
C <- c(0,    0,    0,    0,    1,    1,    1,    1,    0,    0,    0,
  1,    1,    1)
D <- c(4,    120,    7,    23,    4,    24,    3,    12,    7,    1,
 1,    5,    0,    0)
E <- c(0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,
  1,    1,    1)
DF <- data.frame(A, B, C, D, E, stringsAsFactors = FALSE)
library(lattice)
barchart(
    A ~ D|E,
    DF,
    groups = C,
    stack = TRUE,
    main = "Comparison of test results",
    xlab = "Count",
    col = c("yellow", "orange"),
    par.settings = list(
        strip.background = list(col="light grey"),
        superpose.polygon=list(col= c("yellow", "orange"))
    ),
    scales = list(
        alternating = FALSE
    ),
    key = list(
        space="top",
        columns=2,
        text=list(c("Single infections", "Multiple infections"),
col="black"),
        rectangles=list(col=c("yellow", "orange"))
    ),
)

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Mon Dec 21 08:09:26 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 21 Dec 2015 07:09:26 +0000
Subject: [R] Help needed in data cleaning
In-Reply-To: <CAMQbFmMJm7NA05++rHhhFYBTMQ72whEzEk+pZVApY0F_tvwO5A@mail.gmail.com>
References: <CAMQbFmMJm7NA05++rHhhFYBTMQ72whEzEk+pZVApY0F_tvwO5A@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007663@SRVEXCHMBX.precheza.cz>

Hi

Your question posted in HTML is somewhat scrambled. Based on what I understand from it, maybe you shall inspect ?merge.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Web Web
> Sent: Friday, December 18, 2015 8:15 PM
> To: r-help at r-project.org
> Subject: [R] Help needed in data cleaning
>
> Hello,
>            I need some help in data cleaning using R. my CSV file looks
> as
> follows.
>
> "id","gender","age","category1","category2","category3","category4","ca
> tegory5","category6","category7","category8","category9","category10"1,
> "Male",22,"movies","music","travel","cloths","grocery",,,,,2,"Male",28,
> "travel","books","movies",,,,,,,3,"Female",27,"rent","fuel","grocery","
> cloths",,,,,,4,"Female",22,"rent","grocery","travel","movies","cloths",
> ,,,,5,"Female",22,"rent","online-shopping","utiliy",,,,,,,
>
> I need to reformat as follows.
>
> id gender age category            rank1 Male    22  movies
>   11 Male    22  music                21 Male    22  travel
>    31 Male    22  cloths               41 Male    22  grocery
>     51 Male    22  books                NA1 Male    22  rent
>       NA1 Male    22  fuel                 NA1 Male    22  utility
>          NA1 Male    22  online-shopping      NA
> ...................................5 Female    22  movies
> NA5 Female    22  music              NA5 Female    22  travel
>    NA5 Female    22  cloths             NA5 Female    22  grocery
>       NA5 Female    22  books              NA5 Female    22  rent
>          15 Female    22  fuel               NA5 Female    22  utility
>            NA5 Female    22  online-shopping    2
>
> So far My efforts are as follows.
>
> mini <- read.csv("~/MS/coding/mini.csv", header=FALSE)
> mini_clean <- mini[-1,]
> df_mini <- melt(df_clean, id.vars=c("V1","V2","V3"))
> sqldf('select * from df_mini order by  "V1"')
>
> Now I want to know what is the best way to fill all missing categories
> for
> all users.
>
> Thanks
> Nash
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From drjimlemon at gmail.com  Mon Dec 21 11:47:25 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 21 Dec 2015 21:47:25 +1100
Subject: [R] Help needed in data cleaning
In-Reply-To: <CAMQbFmMJm7NA05++rHhhFYBTMQ72whEzEk+pZVApY0F_tvwO5A@mail.gmail.com>
References: <CAMQbFmMJm7NA05++rHhhFYBTMQ72whEzEk+pZVApY0F_tvwO5A@mail.gmail.com>
Message-ID: <CA+8X3fVp=xQJV8-4rSfK36+Yn37UXdzqYitG5_jVffaWJikBJw@mail.gmail.com>

Hi Nash,
If I understand your question correctly, you want the "mice" package.
Hopefully you have more data than your example.

Jim


On Sat, Dec 19, 2015 at 6:14 AM, Web Web <webweb8537 at gmail.com> wrote:

> Hello,
>            I need some help in data cleaning using R. my CSV file looks as
> follows.
>
>
> "id","gender","age","category1","category2","category3","category4","category5","category6","category7","category8","category9","category10"1,"Male",22,"movies","music","travel","cloths","grocery",,,,,2,"Male",28,"travel","books","movies",,,,,,,3,"Female",27,"rent","fuel","grocery","cloths",,,,,,4,"Female",22,"rent","grocery","travel","movies","cloths",,,,,5,"Female",22,"rent","online-shopping","utiliy",,,,,,,
>
> I need to reformat as follows.
>
> id gender age category            rank1 Male    22  movies
>   11 Male    22  music                21 Male    22  travel
>    31 Male    22  cloths               41 Male    22  grocery
>     51 Male    22  books                NA1 Male    22  rent
>       NA1 Male    22  fuel                 NA1 Male    22  utility
>          NA1 Male    22  online-shopping      NA
> ...................................5 Female    22  movies
> NA5 Female    22  music              NA5 Female    22  travel
>    NA5 Female    22  cloths             NA5 Female    22  grocery
>       NA5 Female    22  books              NA5 Female    22  rent
>          15 Female    22  fuel               NA5 Female    22  utility
>            NA5 Female    22  online-shopping    2
>
> So far My efforts are as follows.
>
> mini <- read.csv("~/MS/coding/mini.csv", header=FALSE)
> mini_clean <- mini[-1,]
> df_mini <- melt(df_clean, id.vars=c("V1","V2","V3"))
> sqldf('select * from df_mini order by  "V1"')
>
> Now I want to know what is the best way to fill all missing categories for
> all users.
>
> Thanks
> Nash
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wibbeltjec at hotmail.com  Mon Dec 21 14:34:50 2015
From: wibbeltjec at hotmail.com (Carlijn .)
Date: Mon, 21 Dec 2015 14:34:50 +0100
Subject: [R] Mean effect size in meta-analysis using Metafor
In-Reply-To: <566C478A.2010608@dewey.myzen.co.uk>
References: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>,
	<566C478A.2010608@dewey.myzen.co.uk>
Message-ID: <DUB122-W3088A43A1EE63E17C14D21C0E40@phx.gbl>

Dear Michael,

Thanks for your reaction. The estimates in the example in the link are not exactly identical, but almost. However, in my case, there is a substantial difference between the estimates of the overall effect. 

For example: I want to estimate the effect size of different mental health disorders. I start with estimating the effect size of substance use disorder (SUD), using a three-level random effects model. 

In the first approach I have fitted an intercept only model with a subset of the data including only data on SUD. The overall effect size of SUD is d = 0.185 (SE = 0.085), p = .033. 

Using the second approach, I have included a categorical moderator 'disorder' (SUD, DBD, ADHD). The reference group is SUD and I have added the predictors DBD (i.e., DBD is coded with '1', and SUD and ADHD with '0') and ADHD (i.e., ADHD is coded with '1' and SUD and DBD with '0'). The mean effect size of SUD (intercept) is d = 0.300 (SE = 0.104), p = .005.

To conclude, there is a substantial difference between the estimated effect sizes (d = 0.185 versus d = 0.300). 
 

Approach 1:
> summary(sud, digits=3)
Multivariate Meta-Analysis Model (k = 49; method: REML)
  logLik  Deviance       AIC       BIC      AICc  
 -11.808    23.616    29.616    35.230    30.161  

Variance Components: 
                estim   sqrt  nlvls  fixed  factor
sigma^2.1  0.042  0.206     49     no       y
sigma^2.2  0.050  0.223     13     no      ID

Test for Heterogeneity: 
Q(df = 48) = 409.874, p-val < .001

Model Results:

estimate    se         tval       pval       ci.lb      ci.ub          
   0.185      0.085    2.189    0.033    0.015    0.356        * 

Approach 2:
> summary(external, digits=3)
Multivariate Meta-Analysis Model (k = 123; method: REML)
  logLik  Deviance       AIC       BIC      AICc  
 -58.470   116.940   126.940   140.878   127.467  

Variance Components: 
                  estim   sqrt  nlvls  fixed  factor
sigma^2.1  0.065  0.256    123     no     y
sigma^2.2  0.113  0.336     17     no      ID

Test for Residual Heterogeneity: 
QE(df = 120) = 846.602, p-val < .001

Test of Moderators (coefficient(s) 2,3): 
QM(df = 2) = 0.956, p-val = 0.387

Model Results:

                  estimate     se       tval       pval      ci.lb  ci.ub    
intrcpt      0.300       0.104     2.874    0.005    0.093  0.506  **
DBD          0.114       0.084     1.354     0.178   -0.053  0.281    
ADHD        0.087       0.104     0.836    0.405   -0.119  0.293    


> Subject: Re: [R] Mean effect size in meta-analysis using Metafor
> To: wibbeltjec at hotmail.com; r-help at r-project.org
> From: lists at dewey.myzen.co.uk
> Date: Sat, 12 Dec 2015 16:12:58 +0000
> 
> Dear Carlijn
> 
> I wonder whether
> 
> http://www.metafor-project.org/doku.php/tips:comp_two_independent_estimates
> 
> answers your question? If you had given us an example of your fitting 
> procedure we might know for sure.
> 
> 
> On 12/12/2015 15:35, Carlijn . wrote:
> >
> >
> > Hi all,
> >
> >
> >
> > I have a question about doing a meta-analysis, in particular a three-level
> > meta-analysis using Metafor.
> >
> > I have estimated the mean overall effect size of males by using two different
> > ways:
> >
> > 1. moderator analysis (male = 0, female = 1) using the whole data set
> >
> > 2. intercept-only model with a subset of the data (only males)
> >
> >
> >
> > The mean effect size estimated by using the categorical moderator analysis
> > (1) differs considerably from the overall mean effect size estimated in an
> > intercept-only model using a subset of the data (2).
> >
> >
> >
> > Can someone explain this? Which method gives a better estimation of the
> > effect?
> >
> >
> >
> > Thank you in advance!
> >
> >   		 	   		
> > 	[[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> 
> -- 
> Michael
> http://www.dewey.myzen.co.uk/home.html
 		 	   		  
	[[alternative HTML version deleted]]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Mon Dec 21 14:58:07 2015
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (STAT))
Date: Mon, 21 Dec 2015 13:58:07 +0000
Subject: [R] Mean effect size in meta-analysis using Metafor
In-Reply-To: <DUB122-W3088A43A1EE63E17C14D21C0E40@phx.gbl>
References: <DUB122-W2AA838060A59F802223E5C0EB0@phx.gbl>,
	<566C478A.2010608@dewey.myzen.co.uk>
	<DUB122-W3088A43A1EE63E17C14D21C0E40@phx.gbl>
Message-ID: <6f033982cccb469ba4014b513e2ddb0b@UM-MAIL3216.unimaas.nl>

Try this:

When fitting the model based on the subset data, fix the variance components to the values from the model with the categorical moderator. So, in your rma.mv() call, use:

sigma2=c(0.065, 0.113)

(ideally, use values that are less rounded). The overall effect size estimate from this model should then be identical to the intercept from the model with the categorical moderator, indicating that the discrepancy is exactly due to the fact described under the link provided by Michael.

If not, there is something else going on.

Best,
Wolfgang

-- 
Wolfgang Viechtbauer, Ph.D., Statistician | Department of Psychiatry and    
Neuropsychology | Maastricht University | P.O. Box 616 (VIJV1) | 6200 MD    
Maastricht, The Netherlands | +31 (43) 388-4170 | http://www.wvbauer.com    

> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Carlijn .
> Sent: Monday, December 21, 2015 14:35
> To: Michael Dewey; r-help at r-project.org
> Subject: Re: [R] Mean effect size in meta-analysis using Metafor
> 
> Dear Michael,
> 
> Thanks for your reaction. The estimates in the example in the link are
> not exactly identical, but almost. However, in my case, there is a
> substantial difference between the estimates of the overall effect.
> 
> For example: I want to estimate the effect size of different mental
> health disorders. I start with estimating the effect size of substance
> use disorder (SUD), using a three-level random effects model.
> 
> In the first approach I have fitted an intercept only model with a subset
> of the data including only data on SUD. The overall effect size of SUD is
> d = 0.185 (SE = 0.085), p = .033.
> 
> Using the second approach, I have included a categorical moderator
> 'disorder' (SUD, DBD, ADHD). The reference group is SUD and I have added
> the predictors DBD (i.e., DBD is coded with '1', and SUD and ADHD with
> '0') and ADHD (i.e., ADHD is coded with '1' and SUD and DBD with '0').
> The mean effect size of SUD (intercept) is d = 0.300 (SE = 0.104), p =
> .005.
> 
> To conclude, there is a substantial difference between the estimated
> effect sizes (d = 0.185 versus d = 0.300).
> 
> Approach 1:
> > summary(sud, digits=3)
> Multivariate Meta-Analysis Model (k = 49; method: REML)
>   logLik  Deviance       AIC       BIC      AICc
>  -11.808    23.616    29.616    35.230    30.161
> 
> Variance Components:
>            estim   sqrt  nlvls  fixed  factor
> sigma^2.1  0.042  0.206     49     no       y
> sigma^2.2  0.050  0.223     13     no      ID
> 
> Test for Heterogeneity:
> Q(df = 48) = 409.874, p-val < .001
> 
> Model Results:
> 
> estimate       se     tval     pval    ci.lb    ci.ub
>    0.185    0.085    2.189    0.033    0.015    0.356        *
> 
> Approach 2:
> > summary(external, digits=3)
> Multivariate Meta-Analysis Model (k = 123; method: REML)
>   logLik  Deviance       AIC       BIC      AICc
>  -58.470   116.940   126.940   140.878   127.467
> 
> Variance Components:
>            estim   sqrt  nlvls  fixed  factor
> sigma^2.1  0.065  0.256    123     no       y
> sigma^2.2  0.113  0.336     17     no      ID
> 
> Test for Residual Heterogeneity:
> QE(df = 120) = 846.602, p-val < .001
> 
> Test of Moderators (coefficient(s) 2,3):
> QM(df = 2) = 0.956, p-val = 0.387
> 
> Model Results:
> 
>           estimate          se      tval     pval    ci.lb  ci.ub
> intrcpt      0.300       0.104     2.874    0.005    0.093  0.506  **
> DBD          0.114       0.084     1.354    0.178   -0.053  0.281
> ADHD         0.087       0.104     0.836    0.405   -0.119  0.293
> 
> > Subject: Re: [R] Mean effect size in meta-analysis using Metafor
> > To: wibbeltjec at hotmail.com; r-help at r-project.org
> > From: lists at dewey.myzen.co.uk
> > Date: Sat, 12 Dec 2015 16:12:58 +0000
> >
> > Dear Carlijn
> >
> > I wonder whether
> >
> > http://www.metafor-
> project.org/doku.php/tips:comp_two_independent_estimates
> >
> > answers your question? If you had given us an example of your fitting
> > procedure we might know for sure.
> >
> > On 12/12/2015 15:35, Carlijn . wrote:
> > >
> > >
> > > Hi all,
> > >
> > > I have a question about doing a meta-analysis, in particular a three-
> level
> > > meta-analysis using Metafor.
> > >
> > > I have estimated the mean overall effect size of males by using two
> different
> > > ways:
> > >
> > > 1. moderator analysis (male = 0, female = 1) using the whole data set
> > >
> > > 2. intercept-only model with a subset of the data (only males)
> > >
> > > The mean effect size estimated by using the categorical moderator
> analysis
> > > (1) differs considerably from the overall mean effect size estimated
> in an
> > > intercept-only model using a subset of the data (2).
> > >
> > > Can someone explain this? Which method gives a better estimation of
> the
> > > effect?
> > >
> > > Thank you in advance!


From helmut.schuetz at bebac.at  Mon Dec 21 16:25:01 2015
From: helmut.schuetz at bebac.at (=?UTF-8?Q?Helmut_Sch=c3=bctz?=)
Date: Mon, 21 Dec 2015 16:25:01 +0100
Subject: [R] persp3D: Adjusting labels other than centered?
Message-ID: <567819CD.7080900@bebac.at>

Dear all,

for a publication I have to change cex.axis and cex.lab. I'm already 
using Paul Murrell's workaround to increase the distance of the axes' 
titles by adding "\n". However, I found no method to either increase the 
distance of the labels or set their justification (by adj or hadj). They 
are centered (adj 0.5), which may lead to overlapping. Example:

x <- seq(-10, 10, length = 100)
y <- x
f <- function(x,y) { r <- sqrt(x^2+y^2); 0.1*sin(r)/r }
z <- outer(x, y, f)
library(plot3D)
persp3D(x = x, y = y, z = z, ticktype = "detailed",
         colkey = FALSE, cex.lab = 2, cex.axis = 1.5,
         xlab = "\nmy x", ylab = "\nmy y", zlab = "\n\nmy z")

?persp mentions hadj only for the colkey-argument
?perspbox tells me that other arguments can be passed to persp
There I find that additional parameters might be set in par. However, 
adj seems to be valid only for text, mtext and title (not for labels in 
axis).

Now I'm stuck. Is there a method to(1) pass an argument to adjust text 
and (2) if yes, can it be set independently for the three axes?
In my example centered for x and y is fine, but for z I would liketo 
have the labels right-justified.
I was thinking about writing the labels by text3D (and format them by 
sprintf), but then I'm facing another collision. In order to prevent the 
default-labels to be displayed I have to switch to ticktype="simple". 
Then I (1) loose the ticks and (2) get the arrows which I don't want.

THX in advance,
Helmut

Environment: x86_64-w64-mingw32/x64 (64-bit), R 3.2.2, plot3D 1.0-2

-- 
Ing. Helmut Sch?tz
BEBAC - Consultancy Services for
Bioequivalence and Bioavailability Studies
Neubaugasse 36/11
1070 Vienna, Austria
VAT No  ATU61115625
DUNS    300370568
tel     +43 1 2311746
mobile  +43 699 10792458
e-mail  helmut.schuetz at bebac.at
web     http://bebac.at/
contact http://bebac.at/Contact.htm
forum   http://forum.bebac.at/

This e-mail is confidential and may also be legally privileged. If you
are not the intended recipient please reply to sender, do not disclose
its contents to any person and delete the e-mail. Any unauthorized
review, use, disclosure, copying or distribution is strictly prohibited.


	[[alternative HTML version deleted]]


From lorenzo.isella at gmail.com  Mon Dec 21 16:59:47 2015
From: lorenzo.isella at gmail.com (Lorenzo Isella)
Date: Mon, 21 Dec 2015 16:59:47 +0100
Subject: [R] Checkpoints in Caret
Message-ID: <20151221155947.GC3777@localhost.localdomain>

Dear All,
I am training a model using caret.
Everything is fine, but the training will take several days.
I wonder if there is the possibility of creating some checkpoints in
caret so that the training of a complex model can be split up into
several jobs to be executed in different days.
I googled a bit, but I did not find any answer to my question.
Any hint is appreciated.
Cheers

Lorenzo


From rsherry8 at comcast.net  Mon Dec 21 20:51:09 2015
From: rsherry8 at comcast.net (Robert Sherry)
Date: Mon, 21 Dec 2015 14:51:09 -0500
Subject: [R] F Distribution
Message-ID: <5678582D.90704@comcast.net>


When I use a table, from a Schaum book, I see that for the 95 
percentile, with v_1 = 1 and v_2 = 1 the value is 161. In the modern era,
looking values up in a table is less than ideal. Therefore, I would 
expect R to have a function to do this and based upon my
reading of the documentation, I would expect the following call to get 
the value I expect:
      pf( .95,1, 1)
However, it produces
     0.4918373
Therefore, I conclude that I am using the wrong function. What function 
should I use?

Thanks
Bob


From fneiman at monticello.org  Mon Dec 21 20:55:58 2015
From: fneiman at monticello.org (Fraser D. Neiman)
Date: Mon, 21 Dec 2015 19:55:58 +0000
Subject: [R] F Distribution
In-Reply-To: <5678582D.90704@comcast.net>
References: <5678582D.90704@comcast.net>
Message-ID: <2176AD174D58CB4ABBDA99F3458C201720BA6100@GRANGER.monticello.org>

Dear Bob,

You want...

> qf( .95,1, 1)
[1] 161.4476

Best, Fraser

-----Original Message-----
From: Robert Sherry [mailto:rsherry8 at comcast.net] 
Sent: Monday, December 21, 2015 2:51 PM
To: R Project Help
Subject: [R] F Distribution


When I use a table, from a Schaum book, I see that for the 95 percentile, with v_1 = 1 and v_2 = 1 the value is 161. In the modern era, looking values up in a table is less than ideal. Therefore, I would expect R to have a function to do this and based upon my reading of the documentation, I would expect the following call to get the value I expect:
      pf( .95,1, 1)
However, it produces
     0.4918373
Therefore, I conclude that I am using the wrong function. What function should I use?

Thanks
Bob

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From peter.langfelder at gmail.com  Mon Dec 21 20:57:42 2015
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Mon, 21 Dec 2015 11:57:42 -0800
Subject: [R] F Distribution
In-Reply-To: <5678582D.90704@comcast.net>
References: <5678582D.90704@comcast.net>
Message-ID: <CA+hbrhX7faQm1GNnCXe0D4Ar6ibRV+F+SKePHyNwgkAQbG--Rw@mail.gmail.com>

You want to use qf which gives you the value at a given percentile. pf
gives you the p-value for a given value of F (inverse)

> qf(0.95, 1, 1)
[1] 161.4476

> pf(161.4476, 1, 1)
[1] 0.95



Peter

On Mon, Dec 21, 2015 at 11:51 AM, Robert Sherry <rsherry8 at comcast.net> wrote:
>
> When I use a table, from a Schaum book, I see that for the 95 percentile,
> with v_1 = 1 and v_2 = 1 the value is 161. In the modern era,
> looking values up in a table is less than ideal. Therefore, I would expect R
> to have a function to do this and based upon my
> reading of the documentation, I would expect the following call to get the
> value I expect:
>      pf( .95,1, 1)
> However, it produces
>     0.4918373
> Therefore, I conclude that I am using the wrong function. What function
> should I use?
>
> Thanks
> Bob
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Mon Dec 21 21:02:24 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 21 Dec 2015 21:02:24 +0100
Subject: [R] F Distribution
In-Reply-To: <5678582D.90704@comcast.net>
References: <5678582D.90704@comcast.net>
Message-ID: <03CF4F9E-E91E-4E26-A461-B9E428AF1586@gmail.com>

qf() (as you might have figured from the help page for pf()....)

-pd 

> On 21 Dec 2015, at 20:51 , Robert Sherry <rsherry8 at comcast.net> wrote:
> 
> 
> When I use a table, from a Schaum book, I see that for the 95 percentile, with v_1 = 1 and v_2 = 1 the value is 161. In the modern era,
> looking values up in a table is less than ideal. Therefore, I would expect R to have a function to do this and based upon my
> reading of the documentation, I would expect the following call to get the value I expect:
>     pf( .95,1, 1)
> However, it produces
>    0.4918373
> Therefore, I conclude that I am using the wrong function. What function should I use?
> 
> Thanks
> Bob
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From r.turner at auckland.ac.nz  Tue Dec 22 02:22:21 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 22 Dec 2015 14:22:21 +1300
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <5678A5CD.3000302@auckland.ac.nz>


On 18/12/15 22:57, mohsen hs via R-help wrote:

> Hello all I am using the following two commands and get two
> completely different qq plots while meanlog and sdlog are almost the
> same.

What are "meanlog" and "sdlog"?  My telepathic capacity is severely 
limited, and R's mind_read() function is still under development.

Any help is highly appreciated. dev.new() ; qqPlot(serving,
> dist = "lnorm", estimate.params = TRUE, add.line = TRUE)
> fitln <- fitdist(serving, "lnorm") dev.new();qqcomp(fitln)

There is no "qqPlot()" function in the automatically loaded packages in 
R.  There is a qqplot() function but it appears to have a different 
argument list.

So where (what package) does qqPlot() come from?

And while you're at it, how about a *reproducible* example?

Please refrain from posting in HTML.

When you are seeking help, have the decency to express yourself in a 
manner such that providing help is possible.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From topjetboy at yahoo.com  Mon Dec 21 22:52:01 2015
From: topjetboy at yahoo.com (topjetboy)
Date: Mon, 21 Dec 2015 21:52:01 +0000 (UTC)
Subject: [R] Error running predict
References: <722624350.1564960.1450734721045.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <722624350.1564960.1450734721045.JavaMail.yahoo@mail.yahoo.com>

using R 3.2.3 on windows.
> library(caret); library(kernlab);data(spam)
> intrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)> training <- spam[intrain,]
> testing <- spam[-intrain]> set.seed(32343)> modelFit <- train(type ~.,data=training, method="glm")
> modelFit> modelFit$finalModel
Degrees of Freedom: 3450 Total (i.e. Null); ?3393 ResidualNull Deviance: ? ? ?4628?Residual Deviance: 1355 ? ? ? ? AIC: 1471
next line gives an error:> predictions <- predict(modelFit, newdata=testing)
Error in eval(expr, envir, enclos) : object 'make' not found
What is 'make'? Is it looking for the gnu-C compiler?

	[[alternative HTML version deleted]]


From sidoti.23 at buckeyemail.osu.edu  Tue Dec 22 00:10:01 2015
From: sidoti.23 at buckeyemail.osu.edu (Sidoti, Salvatore A.)
Date: Mon, 21 Dec 2015 23:10:01 +0000
Subject: [R]  Nested Avova: Unequal # of Observations
Message-ID: <CY1PR0101MB100472F615DB7FCEBB6B2522ABE40@CY1PR0101MB1004.prod.exchangelabs.com>

I have two experimental groups (treatment & control) with 6 sets of observations nested within each group. The number of observations in each set is not equal.


How do I set up a such an ANOVA in R?


Thank You!


Salvatore Sidoti

PhD Student

Graduate Teaching Assistant

	[[alternative HTML version deleted]]


From nouri4 at yahoo.com  Tue Dec 22 05:24:51 2015
From: nouri4 at yahoo.com (knouri)
Date: Tue, 22 Dec 2015 04:24:51 +0000 (UTC)
Subject: [R] clogit & weights
References: <1240931293.2124154.1450758291399.JavaMail.yahoo.ref@mail.yahoo.com>
Message-ID: <1240931293.2124154.1450758291399.JavaMail.yahoo@mail.yahoo.com>

Merry Christmas everyone:
I have the following data(mydat) and would like to fit a conditional logistic regression model considering "weights".
id? case?exposure?? weights
1?????1?????????1????????? 2
1?????0?????????0????????? 2
2?????1?????????1????????? 2 
2?????0?????????0????????? 2 
3?????1?????????1????????? 1 
3?????0?????????0????????? 1 
4?????1?????????0???????? ?2 
4?????0?????????1????????? 2 ?The R function"clogit" is for such purposes but it ignores weights.?I tried function"mclogit" instead which seems that it considers the weights option:##############################################################options(scipen=999)library(mclogit)# create the above data frameid????????? = c(1,1,2,2,3,3,4,4)case????? =?c(1,0,1,0,1,0,1,0)exposure = c(1,0,1,0,1,0,0,1)weights? = c(2,2,2,2,1,1,2,2)(mydata??= data.frame(id,case,exposure,weights)) fit??????= mclogit(cbind(case,id) ~ exposure,weights=weights, data=mydata)summary(fit)######################################################################
The answer,however,?doesn't seem to be?correct. Could anyone?pleaseprovides me with some solution to this??Thanks in advance,Keramat Nourijelyani,PhD??

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Tue Dec 22 06:46:31 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Mon, 21 Dec 2015 21:46:31 -0800
Subject: [R] Nested Avova: Unequal # of Observations
In-Reply-To: <CY1PR0101MB100472F615DB7FCEBB6B2522ABE40@CY1PR0101MB1004.prod.exchangelabs.com>
References: <CY1PR0101MB100472F615DB7FCEBB6B2522ABE40@CY1PR0101MB1004.prod.exchangelabs.com>
Message-ID: <CAGxFJbSwgFWv-3M44pi7JZJ19tp+SrggFRRr6x3Svn7eU75gZw@mail.gmail.com>

Looks like homework. If so, you should know that there is a no
homework policy on this list.

If not, it looks like you should do some on your own by
?anova
?t.test

And perhaps some of:

(a) Consulting a statistics text;
(b) Going through an R tutorial (there are many good ones on the Web);
(c) Showing us what you have done and what errors have occurred. You
should not expect us to do your work for you, at least IMHO (others
may differ).

Cheers,
Bert
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Mon, Dec 21, 2015 at 3:10 PM, Sidoti, Salvatore A.
<sidoti.23 at buckeyemail.osu.edu> wrote:
> I have two experimental groups (treatment & control) with 6 sets of observations nested within each group. The number of observations in each set is not equal.
>
>
> How do I set up a such an ANOVA in R?
>
>
> Thank You!
>
>
> Salvatore Sidoti
>
> PhD Student
>
> Graduate Teaching Assistant
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From taze4 at web.de  Tue Dec 22 08:57:38 2015
From: taze4 at web.de (taze4 at web.de)
Date: Tue, 22 Dec 2015 08:57:38 +0100
Subject: [R] geomorph: adding confidence ellipses
Message-ID: <trinity-7bccada3-396b-45d3-a90d-2f89e8eaaacd-1450771058921@3capp-webde-bs07>

Hello everyone,
I?m currently working with package geomorph in R to generate a PCA plot. Now I want to add confidence ellipses to my plot. I couldn?t find any command integrated in package geomorph for doing this.
So I tried package vegan for adding confidence ellipses but the command does not work with geomorph as my data is a 3D-array (for the PCA) and vegan needs a 2D-array for creating confidence ellipses. Changing the data from 3D to 2D doesn?t work either as in this case I can?t plot the PCA at all.

Does anyone know a command in geomorph for confidence ellipses or a package compatible with geomorph for adding confidence ellipses?

Technical details: I am currently working with RStudio Version 0.99.484

My code so far:
library(rgl)
library(ape)
library(geomorph)

morpho<-readland.tps(file,specID="ID") #adding my data#
classifiers<-read.table(file,header=T) #adding the classifiers#
attach(classifiers)

morpho.gpa<-gpagen(morpho, pointscale=1,ShowPlot=T) #doing the Procrustes Fit#
plotTangentSpace(morpho,axis1=2,axis2=3,label=T,group=classifiers$Stretch) #plotting the PCA with Stretch as group, there are 4 different levels in group#

Now I want to add confidence ellipses. As I know no command in geomorph, I used vegan:
library(lattice)
library(permute)
library(vegan)
ordiellipse(morpho,classifiers$Stretch,kind="sd",conf=0.95)

This gives me the following error message:
Error in ordiellipse(morpho, classifiers$Stretch, kind = "sd", conf = 0.95) : 
  ordiellipse needs two dimensions

My data for the PCA is a 3D-array:
dim(morpho) #asking if data is 3D or 2D, as my command returns 3 values, the data  is in the form of a 3D-array#
[1] 16  2 88

Changing my data into a 2D-array does not solve the problem, because then I am unable to plot the PCA at all:
morpho2D<-two.d.array(morpho.gpa$coords)
plotTangentSpace(morpho2D,axis1=2,axis2=3,label=T,group=classifiers$Stretch)
Error in plotTangentSpace(morpho2D, axis1 = 2, axis2 = 3, label = T, group = classifiers$Stretch) : 
  Data matrix not a 3D array (see 'arrayspecs').

I am grateful for any help on this topic!
Kind regards,
Tatze


This is what my data looks like:
morpho[,,1] #I have 16 landmarks per fish, in total 87 fish; each of the 16 landmarks is given as coordinates#
          [,1]     [,2]
 [1,] 3.632444 4.795980
 [2,] 4.767132 4.726264
 [3,] 5.428232 4.735880
 [4,] 8.721712 3.935348
 [5,] 9.810724 3.899288
 [6,] 9.844380 2.704500
 [7,] 8.361112 2.716520
 [8,] 5.750368 2.646804
 [9,] 3.240592 2.488140
[10,] 3.440124 3.074716
[11,] 3.663696 4.344028
[12,] 2.995384 4.358452
[13,] 1.382300 4.211808
[14,] 0.613020 3.673312
[15,] 1.447208 3.137220
[16,] 2.055420 2.853548

head(classifiers)
  ID   Nr TotalLength Weight Sex Stretch riverbank  centroid
1  0 L010        11.5   15.0   1     Kel     Kel_l 12.180512
2  1 L094         8.0    7.4   1     Reg     Reg_r  8.459462
3  2 L033         9.6   12.6   1    Vils    Vils_r 10.262179


From giorgio.garziano at ericsson.com  Tue Dec 22 11:57:14 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 22 Dec 2015 10:57:14 +0000
Subject: [R] Error running predict
Message-ID: <248E6FA047A8C746BA491485764190F53715EA4C@ESESSMB207.ericsson.se>

You forgot to put the comma after "-intrain" in the following assignment:

testing <- spam[-intrain, ]

"make" is one of the data columns of spam dataset.

> colnames(spam)
[1] "make"


--
GG


	[[alternative HTML version deleted]]


From soni.archit1989 at gmail.com  Tue Dec 22 13:34:46 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Tue, 22 Dec 2015 06:34:46 -0600
Subject: [R] Trycatch in R
Message-ID: <CAJ7HxBzE7uf4_iwb6Ky-FBV8pi=ktJqjKitNG9fFD3bwKwN9aQ@mail.gmail.com>

Hi All,

Please help me in the below code:

tryCatch({
 x <- capture.output(XML::xmlParse(y))
}, warning = function(w) {
  x <- capture.output(w)
}, error = function(e) {
  x <-capture.output(e)
}, finally = {

})

I am using Tibco Enterprise Runtime for R to get value of the parsed XML in
vector format, and spotfire is displaying each line as a row in new table,
but can you please suggest how to do error handling in this case when no
row is selected and we run this script, it is throwing error for length of
argument is 0, i need to display an error message as first row of the table.

Please suggest.

-- 
Regards
Archit

	[[alternative HTML version deleted]]


From viveksutra at gmail.com  Tue Dec 22 13:41:01 2015
From: viveksutra at gmail.com (Vivek Sutradhara)
Date: Tue, 22 Dec 2015 13:41:01 +0100
Subject: [R] problem with the use of parallel foreach
Message-ID: <CAHLp6SAhhVqpVUf27pVUQhHT1-7bkpSwZ6hhS5nEEgwb=tSxUg@mail.gmail.com>

Hi,
I am having a problem with the use of the foreach package. It is strange
that my code works when i use the %do% function but not with %dopar%.

Let me explain. I am new to parallel and foreach packages. I have data in
the form of very large files, and they are in the form of data tables. I
have saved them as rds files, for taking advantage of the compression
capability.

I will try to make reproducible example as follows :

setwd('C:/Rtrials/parallelTrials')
library(parallel)
#no_ofCores<-detectCores()
library(doSNOW)
cl <- makeCluster(2, type="SOCK")
registerDoSNOW(cl)

library(data.table)
# example data
mt<-data.table(mtcars)
#split mtcars into 4 data.tables and save into 4 rds files (to mimic my
file structure)
nlow<-1
for (i in 1:4) {
  filei<-paste0('mt',i,'.rds')
  nhigh<-i*8
  mti<-mt[nlow:nhigh]
  saveRDS(mti,file=filei)
  nlow<-i*8+1
}

# read the files in parallel and aggregate
mt6<-foreach(j=1:4,.combine='rbind') %dopar% {           # works with %do%
  filej<-paste0('mt',j,'.rds')
  mtj<-readRDS(filej)
  mtj[cyl == 6]
}
stopCluster(cl)

I get the following error message when using %dopar% :

Error in { : task 1 failed - "object 'cyl' not found"

When I change the %dopar% command to %do%, I do not get an error
message. What is the problem in the use of %dopar%?

I would appreciate help in troubleshooting.

Instead of the foreach loop, I tried the same with a for loop. After
saving the aggregated result, I had to delete the table from the
currently read file, do garbage collection and then read in a new
file. Something like the following :

dtAll<-mt[0]
for (j in 1:25) {
  filetxt<-paste0('mt',j,'.rds')
  dtj<-readRDS(filetext)
  dtAll<-rbind[list(dtAll,dtj)]
  rm(dtj);gc()
}

How is garbage collection handled in parallel computing? With the .combine
= 'rbind' option, this may not be necessary. Could somebody comment on
this? Would it be better to use the 'rbindlist' option instead of 'rbind'?

First, I would like to know what my problem with %dopar% is.

Thanks for any help that I can get.


Vivek

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Tue Dec 22 13:33:27 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 22 Dec 2015 12:33:27 +0000
Subject: [R] Checkpoints in Caret
Message-ID: <248E6FA047A8C746BA491485764190F53715EAD0@ESESSMB207.ericsson.se>

I found some information about parallel processing in R that might be of your interest:

http://topepo.github.io/caret/parallel.html

http://www.vikparuchuri.com/blog/parallel-r-model-prediction-building/

https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf

http://michaeljkoontz.weebly.com/uploads/1/9/9/4/19940979/parallel.pdf


Also consider:

http://www.teraproc.com/front-page-posts/r-on-demand/


Hope it helps.

--

GG



	[[alternative HTML version deleted]]


From therneau at mayo.edu  Tue Dec 22 14:21:26 2015
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 22 Dec 2015 07:21:26 -0600
Subject: [R] clogit and weights
In-Reply-To: <mailman.5.1450782003.9009.r-help@r-project.org>
References: <mailman.5.1450782003.9009.r-help@r-project.org>
Message-ID: <c10f8b$23rdfv@ironport10.mayo.edu>

How should the weights be treated?  If they are multiple observation weights (a weight of 
"3" is shorthand for 3 subjects) that leads to a different likelihood than sampling 
weights ("3" means to give this one subject more influence).  The clogit command can't 
read your mind and so has chosen not to make a guess.

Also, please do not post in html.  As you see below it leads to a mangled message.

Terry Therneau


On 12/22/2015 05:00 AM, r-help-request at r-project.org wrote:
> Merry Christmas everyone:
> I have the following data(mydat) and would like to fit a conditional logistic regression model considering "weights".
> id? case?exposure?? weights
> 1?????1?????????1????????? 2
> 1?????0?????????0????????? 2
> 2?????1?????????1????????? 2
> 2?????0?????????0????????? 2
> 3?????1?????????1????????? 1
> 3?????0?????????0????????? 1
> 4?????1?????????0???????? ?2
> 4?????0?????????1????????? 2 ?The R function"clogit" is for such purposes but it ignores weights.?I tried function"mclogit" instead which seems that it considers the weights option:##############################################################options(scipen=999)library(mclogit)# create the above data frameid????????? = c(1,1,2,2,3,3,4,4)case????? =?c(1,0,1,0,1,0,1,0)exposure = c(1,0,1,0,1,0,0,1)weights? = c(2,2,2,2,1,1,2,2)(mydata??= data.frame(id,case,exposure,weights)) fit??????= mclogit(cbind(case,id) ~ exposure,weights=weights, data=mydata)summary(fit)######################################################################
> The answer,however,?doesn't seem to be?correct. Could anyone?pleaseprovides me with some solution to this??Thanks in advance,Keramat Nourijelyani,PhD??
>
> 	[[alternative HTML version deleted]]
>


From giorgio.garziano at ericsson.com  Tue Dec 22 14:28:38 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 22 Dec 2015 13:28:38 +0000
Subject: [R] Checkpoints in Caret
Message-ID: <248E6FA047A8C746BA491485764190F53715EB2E@ESESSMB207.ericsson.se>

Further, trying to be more specific about checkpoint with R, I may suggest the following readings.



Look for "checkpoint models" in:



                http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/8/docs-website/h2o-docs/booklets/DeepLearning_Vignette.pdf



Look for "checkpointing" in:



https://github.com/h2oai/h2o-training-book/blob/master/hands-on_training/deep_learning.md





However, that is not exactly what you asked for.


--
GG



	[[alternative HTML version deleted]]


From aoife.m.doherty at gmail.com  Tue Dec 22 14:29:34 2015
From: aoife.m.doherty at gmail.com (aoife doherty)
Date: Tue, 22 Dec 2015 13:29:34 +0000
Subject: [R] Fwd:
In-Reply-To: <CAAsJanY6i6+vYf2_99RviC9azCNrB+u8=PypmnsC8ZcaBEQGmw@mail.gmail.com>
References: <CAAsJanY6i6+vYf2_99RviC9azCNrB+u8=PypmnsC8ZcaBEQGmw@mail.gmail.com>
Message-ID: <CAAsJanZf+RwpK57evx7OZwx781coyNxaeyXOs4zNMju_zgHiyQ@mail.gmail.com>

Dear all,




I am hoping to use the mt.maxT package, so I'm trying to understand how the
maxT algorithm works in the multtest package.

I have an example of data:

SNP1 p value= 0.02
SNP1 permutation p values = 0.03, 0.03, 0.03, 0.04, 0.04.

SNP2 p value =0.03
SNP2 permutation p values = 0.4,0.5,0.5,0.6,0.7

SNP3 p value = 0.3
SNP3 p value permutation p values = 0.8,0.9,0.9,0.9,0.9

I was told that for this data set, the corrected P value for SNP1 is 1/6,
for SNP2 is 5/12 and SNP3 is 1. I am so confused as to how these numbers
were reached.

I've tried to come up with an explanation in my head, I would appreciate if
someone could tell me where I'm going wrong;

   1.

   First, you get the smallest permuted P Value across all of the SNPs for
   each permutation. In this case, it's: Perm1 smallest P val= 0.03 Perm2 =
   0.03 Perm3 = 0.03 Perm4 = 0.04 Perm5 = 0.04
   2.

   Then I was told that the corrected p value for SNP1 is 1/6. To get the
   6, is this the 5 permuted p values + my p value? So it's 1/6 chance of
   seeing a p value as low as mine in the original p value + permuted p value
   set.
   3.

   For SNP2 with an uncorrected p value of 0.03, I am totally confused as
   to how the answer is 5/12. I know ties count as 0.5. So in my smallest
   permuted data set: 0.03, 0.03, 0.03, 0.04, 0.04: if you add them up
   (allowing 0.03 and 0.04 to be worth 0.5 each since they are both ties), the
   sum of the 5 permuted values is 2.5. So then the chance of seeing 0.03,
   using the logic from step 1, is that it's 2.5/5, or 3.5/6 if you add in the
   uncorrected p value? Which isn't 5/12, but if you don't add in the original
   p value 0.03, the answer will be 2.5/6, which is 5/12....but I don't
   understand why not to add in the original p value in this step, when I did
   it in step 1.

I'm obviously not understanding something, if someone could really simply
explain the calculation process/algorithm for maxT corrected p values for
this example I would appreciate it.

Thanks

	[[alternative HTML version deleted]]


From trichter at uni-bremen.de  Tue Dec 22 16:13:07 2015
From: trichter at uni-bremen.de (Tim Richter-Heitmann)
Date: Tue, 22 Dec 2015 16:13:07 +0100
Subject: [R] Regular R freezes
Message-ID: <56796883.1050709@uni-bremen.de>

Dear List,

some days ago my R started to regularily freezes, especially during 
these operations:

mso(,permutations=999), package vegan
anova.cca(), package vegan
forward.sel(), package packfor

I am running R on a 64-bit Windows 7 installation, 4 cores, 8 GB RAM.
I just today updated R to 3.2.3, all packages, and R-Studio to their 
most recent versions.
Still the error persists.
Two things are strange:

All these functions (and more) have been working without any problem for 
2 years.
The freezes occur at different points in the algorithms. For 
forward.sel(), a stepwise variable selection process
in species distribution modelling, the system fails at different steps 
(say after variable 8 or 18), but inevitably fails.
R-Studio is still responsive meanwhile, but the "Stop"-Buttom does not 
work, and the programm needs to be restarted.

Is there any way of troubleshooting or finding out who the culprit is?

Thanks.

-- 
Tim Richter-Heitmann (M.Sc.)
PhD Candidate



International Max-Planck Research School for Marine Microbiology
University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069


From dcarlson at tamu.edu  Tue Dec 22 16:35:28 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Tue, 22 Dec 2015 15:35:28 +0000
Subject: [R] geomorph: adding confidence ellipses
In-Reply-To: <trinity-7bccada3-396b-45d3-a90d-2f89e8eaaacd-1450771058921@3capp-webde-bs07>
References: <trinity-7bccada3-396b-45d3-a90d-2f89e8eaaacd-1450771058921@3capp-webde-bs07>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6F5484@mb02.ads.tamu.edu>

Look at scatter3d() in the car package. It has an option to add a confidence ellipsoid to an interactive 3d plot.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of taze4 at web.de
Sent: Tuesday, December 22, 2015 1:58 AM
To: R Help
Subject: [R] geomorph: adding confidence ellipses

Hello everyone,
I?m currently working with package geomorph in R to generate a PCA plot. Now I want to add confidence ellipses to my plot. I couldn?t find any command integrated in package geomorph for doing this.
So I tried package vegan for adding confidence ellipses but the command does not work with geomorph as my data is a 3D-array (for the PCA) and vegan needs a 2D-array for creating confidence ellipses. Changing the data from 3D to 2D doesn?t work either as in this case I can?t plot the PCA at all.

Does anyone know a command in geomorph for confidence ellipses or a package compatible with geomorph for adding confidence ellipses?

Technical details: I am currently working with RStudio Version 0.99.484

My code so far:
library(rgl)
library(ape)
library(geomorph)

morpho<-readland.tps(file,specID="ID") #adding my data#
classifiers<-read.table(file,header=T) #adding the classifiers#
attach(classifiers)

morpho.gpa<-gpagen(morpho, pointscale=1,ShowPlot=T) #doing the Procrustes Fit#
plotTangentSpace(morpho,axis1=2,axis2=3,label=T,group=classifiers$Stretch) #plotting the PCA with Stretch as group, there are 4 different levels in group#

Now I want to add confidence ellipses. As I know no command in geomorph, I used vegan:
library(lattice)
library(permute)
library(vegan)
ordiellipse(morpho,classifiers$Stretch,kind="sd",conf=0.95)

This gives me the following error message:
Error in ordiellipse(morpho, classifiers$Stretch, kind = "sd", conf = 0.95) : 
  ordiellipse needs two dimensions

My data for the PCA is a 3D-array:
dim(morpho) #asking if data is 3D or 2D, as my command returns 3 values, the data  is in the form of a 3D-array#
[1] 16  2 88

Changing my data into a 2D-array does not solve the problem, because then I am unable to plot the PCA at all:
morpho2D<-two.d.array(morpho.gpa$coords)
plotTangentSpace(morpho2D,axis1=2,axis2=3,label=T,group=classifiers$Stretch)
Error in plotTangentSpace(morpho2D, axis1 = 2, axis2 = 3, label = T, group = classifiers$Stretch) : 
  Data matrix not a 3D array (see 'arrayspecs').

I am grateful for any help on this topic!
Kind regards,
Tatze


This is what my data looks like:
morpho[,,1] #I have 16 landmarks per fish, in total 87 fish; each of the 16 landmarks is given as coordinates#
          [,1]     [,2]
 [1,] 3.632444 4.795980
 [2,] 4.767132 4.726264
 [3,] 5.428232 4.735880
 [4,] 8.721712 3.935348
 [5,] 9.810724 3.899288
 [6,] 9.844380 2.704500
 [7,] 8.361112 2.716520
 [8,] 5.750368 2.646804
 [9,] 3.240592 2.488140
[10,] 3.440124 3.074716
[11,] 3.663696 4.344028
[12,] 2.995384 4.358452
[13,] 1.382300 4.211808
[14,] 0.613020 3.673312
[15,] 1.447208 3.137220
[16,] 2.055420 2.853548

head(classifiers)
  ID   Nr TotalLength Weight Sex Stretch riverbank  centroid
1  0 L010        11.5   15.0   1     Kel     Kel_l 12.180512
2  1 L094         8.0    7.4   1     Reg     Reg_r  8.459462
3  2 L033         9.6   12.6   1    Vils    Vils_r 10.262179

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.

From michael.eisenring at agroscope.admin.ch  Tue Dec 22 17:18:21 2015
From: michael.eisenring at agroscope.admin.ch (michael.eisenring at agroscope.admin.ch)
Date: Tue, 22 Dec 2015 16:18:21 +0000
Subject: [R] How to conduct a PERMANOVA using a dissimilarity matrix
Message-ID: <6D5C009FB51EBD41BEE57F4C002350FD05D657@SB00112A.adb.intra.admin.ch>

Dear R-List members,

I have to compare how similar two types of forest (old growth=O) and (young forest=Y) in terms of moth communities are.
I sampled moths at 4 O and 4 Y sites.
I need to analyse the data using a PERMANOVA approach. But I am having a really hard time to do this in R.

I found out that I need to create a dissimilarity matrix and read this matrix then into R to conduct a one-way Permanova with forest type (O or Y) as factor.
The package vegan with the function "adonis" seems to be able to do a permanova.

I created the matrix (based on Soerenson (dis)similarities) and imported it into R.

Could anyone help me with the next step? How can I conduct a permanova on my dataset? In the end I would need an R value and significance level telling me if community compositions differ significantly between sites.

Below is my code (not too much) and the data for the matrix.

#dput for matrix:

structure(c("", "O", "Y", "Y", "Y", "O", "O", "Y", "O", "O",
"0", "0.544", "0.519", "0.533", "0.481", "0.548", "0.518", "0.479",
"Y", "0.544", "0", "0.383", "0.416", "0.383", "0.358", "0.434",
"0.399", "Y", "0.519", "0.383", "0", "0.398", "0.359", "0.392",
"0.401", "0.374", "Y", "0.533", "0.416", "0.398", "0", "0.398",
"0.399", "0.358", "0.348", "O", "0.481", "0.383", "0.359", "0.398",
"0", "0.37", "0.317", "0.354", "O", "0.548", "0.358", "0.392",
"0.399", "0.37", "0", "0.39", "0.365", "Y", "0.518", "0.434",
"0.401", "0.358", "0.317", "0.39", "0", "0.371", "O", "0.479",
"0.399", "0.374", "0.348", "0.354", "0.365", "0.371", "0"), .Dim = c(9L,
9L), .Dimnames = list(NULL, c("V1", "V2", "V3", "V4", "V5", "V6",
"V7", "V8", "V9")))


#Code
#load dissimilarity matrix (based on Soerenson similarity)
moth_dta<-read.csv("Geo_sorenson_8.csv",header=T,sep=";")#Creates matrix from imported data
moth_dta<-as.matrix(moth_dta)
moth_dta
library(vegan)


Thank you very much,
Michael

Eisenring Michael, Msc.
PhD Student

Federal Department of Economic Affairs, Education and Research
EAER
Institute of Sustainability Sciences ISS
Biosafety

Reckenholzstrasse 191, CH-8046 Z?rich
Tel. +41 44 37 77181
Fax +41 44 37 77201
michael.eisenring at agroscope.admin.ch<mailto:michael.eisenring at agroscope.admin.ch>
www.agroscope.ch<http://www.agroscope.ch/>


	[[alternative HTML version deleted]]


From ridgewang at 126.com  Tue Dec 22 17:32:03 2015
From: ridgewang at 126.com (=?ISO-8859-1?B?UmlkZ2U=?=)
Date: Wed, 23 Dec 2015 00:32:03 +0800
Subject: [R]  mice error: data<simpleError in cor(xobs[, keep, drop = FALSE],
	use = "all.obs"): 'x' is empty>
In-Reply-To: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6F5484@mb02.ads.tamu.edu>
References: <trinity-7bccada3-396b-45d3-a90d-2f89e8eaaacd-1450771058921@3capp-webde-bs07>
	<53BF8FB63FAF2E4A9455EF1EE94DA7262D6F5484@mb02.ads.tamu.edu>
Message-ID: <tencent_06FC03DE73C1BE786B148CF5@qq.com>

Hi All,
      I used mice to impute NA values in a data frame like this: 


envdata = complete(mice(envdata,m=1,maxit=1)). 


It seems that the mice function can only works well with 1st and 2nd envdata. When enter the 3rd data, it triggers an error: data<simpleError in cor(xobs[, keep, drop = FALSE], use = "all.obs"): 'x' is empty>.


iter imp variable
 1   1  data  data10  data25  data50


iter imp variable
 1   1  data  data10  data25  data50


iter imp variable
 1   1  data<simpleError in cor(xobs[, keep, drop = FALSE], use = "all.obs"): 'x' is empty>




So what should I do to fix it?


appreciate for any help.




Ridge
	[[alternative HTML version deleted]]


From mohsenhs82 at yahoo.com  Tue Dec 22 07:30:11 2015
From: mohsenhs82 at yahoo.com (mohsen hs)
Date: Tue, 22 Dec 2015 06:30:11 +0000 (UTC)
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <5678A5CD.3000302@auckland.ac.nz>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
Message-ID: <1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>

Hi Rolf

Thank you for your reply and time.Please let me provide more information about my problem.

My data is like this (I obtained itwith the kind support of WCC), it shows the number of counted cars for eachinterval like this:

timenum,count

1,1800

2,1400

4,1000

..

1,86868

It means I had 1800 cars with theinterarrival time of 1 second. 1400 cars with the interarrival time of 2seconds, 1000 cars with the interarrival time of 4 seconds, and finally 1 carwith the interarrival time of 86868. Interarrival in this context means thetime (in seconds) between two consecutive arrivals at a sampling device (position).The purpose is to find the best distribution model which describes the intervaltime. Then do some censoring and compare the result (GOF and QQ plots) and comeup with the best possible model. So far, lnorm is the best one. However, thewhole code is this:

require(gglot2) 

library(fitdistrplus)

df= read.csv ("E:/PathAll_TWOMONTH_BothDirection.csv")

z=rep(df$timenum,time=df$count)

serving<-z

fitln <- fitdist(serving,"lnorm",method="mle")

summary(fitln)

Fitting of the distribution ' lnorm' by maximum likelihood 

Parameters : 

??????? estimate?? Std. Error

meanlog 3.936191 0.0011227470

sdlog?? 1.622471 0.0007939006

Loglikelihood:? -12193668??AIC:? 24387340?? BIC:?24387365 

Correlation matrix:

???????????? meanlog??????? sdlog

meanlog 1.000000e+00 1.245201e-09

sdlog?? 1.245201e-09 1.000000e+00


?
> qqcomp(fitln)


?
Now, I would like to get the sameplot by using qqPlot command:

http://finzi.psych.upenn.edu/library/EnvStats/html/qqPlot.html

require(EnvStats)

qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)


?
The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp?and the left one is from qqPlot. Titles might be incorrect.


?
Cheers

Mohsen


?
 

    On Tuesday, December 22, 2015 4:52 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
 

 
On 18/12/15 22:57, mohsen hs via R-help wrote:

> Hello all I am using the following two commands and get two
> completely different qq plots while meanlog and sdlog are almost the
> same.

What are "meanlog" and "sdlog"?? My telepathic capacity is severely 
limited, and R's mind_read() function is still under development.

Any help is highly appreciated. dev.new() ; qqPlot(serving,
> dist = "lnorm", estimate.params = TRUE, add.line = TRUE)
> fitln <- fitdist(serving, "lnorm") dev.new();qqcomp(fitln)

There is no "qqPlot()" function in the automatically loaded packages in 
R.? There is a qqplot() function but it appears to have a different 
argument list.

So where (what package) does qqPlot() come from?

And while you're at it, how about a *reproducible* example?

Please refrain from posting in HTML.

When you are seeking help, have the decency to express yourself in a 
manner such that providing help is possible.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


  

From pushan.zoology at gmail.com  Tue Dec 22 09:56:03 2015
From: pushan.zoology at gmail.com (pushan chakraborty)
Date: Tue, 22 Dec 2015 14:26:03 +0530
Subject: [R] colour input in ribbon plots (geom ribbon)
Message-ID: <CAAATBeBYHhbkME8sxFmo1fVzmNfJeFb85V10cJS5cZdBQxyVPg@mail.gmail.com>

Hallow everybody
I want to know if there is any command for input colour in the ribbon plot
(geom ribbon) of ggplot2. Unless that, the mean line and the standard error
can not be distinguished. Please help me if anyone have any idea.

-- 
Pushan Chakraborty
CSIR - SRF

Center for Pollination Studies, University of Calcutta
35, Ballyguanje Circular Road, Kolkata - 700019
          &
Wildlife Institute of India
Chandrabani, Dehradun - 248001

webpage:

http://cpscu.in/?page_id=51

Skype: cpushan

	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Tue Dec 22 18:27:57 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 22 Dec 2015 12:27:57 -0500
Subject: [R] Trying to avoid the loop while merging two data frames
Message-ID: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>

Hello!
I have a solution for my task that is based on a loop. However, it's
too slow for my real-life problem that is much larger in scope.
However, I cannot use merge. Any advice on how to do it faster?
Thanks a lot for any hint on how to speed it up!

# I have 'mydata' data frame:
set.seed(123)
mydata <- data.frame(myid = 1001:1100,
                     version = sample(1:20, 100, replace = T))
head(mydata)
table(mydata$version)

# I have 'myinfo' data frame that contains information for each 'version':
set.seed(12)
myinfo <- data.frame(version = sort(rep(1:20, 30)), a = rnorm(60), b =
rnorm(60),
                                 c = rnorm(60), d = rnorm(60))
head(myinfo, 40)

### MY SOLUTION WITH A LOOP:
### Looping through each id of mydata and grabbing
### all columns from 'myinfo' for the corresponding 'version':

# 1. Creating placeholder list for the results:
result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
length(result)
(result)[1:3]


# 2. Looping through each element of 'result':
for(i in 1:length(result)){
      id <- result[[i]]$myid
      result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
      result[[i]]$myid <- id
      result[[i]] <- result[[i]][c(5, 1:4)]
}
result <- do.call(rbind, result)
head(result) # This is the desired result

-- 
Dimitri Liakhovitski


From utz.ryan at gmail.com  Tue Dec 22 18:59:34 2015
From: utz.ryan at gmail.com (Ryan Utz)
Date: Tue, 22 Dec 2015 12:59:34 -0500
Subject: [R] vjust unresponsive (ggplot2)
Message-ID: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>

Hi all,

I cannot for the life of me get my axis titles to adjust vertically in a
ggplot. I've seen several posts about this and have tried everything:
keeping vjust within 0 and 1, adjusting the margins, etc. hjust is behaving
just as it should but vjust just mocks me in silence. No error message is
produced.

Here's a sample code:

x=data.frame(sample(1:10))
x[,2]=sample(1:10)

ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(vjust=.1,hjust=0.6),
plot.margin=unit(c(1,1,2,2),'cm'))

No matter what I put into vjust, nothing happens. Am I missing something
obvious??

Thanks ahead of time for any help,
Ryan


-- 

Ryan Utz, Ph.D.
Assistant professor of water resources
*chatham**UNIVERSITY*
Home/Cell: (724) 272-7769

	[[alternative HTML version deleted]]


From istazahn at gmail.com  Tue Dec 22 19:28:13 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Tue, 22 Dec 2015 13:28:13 -0500
Subject: [R] vjust unresponsive (ggplot2)
In-Reply-To: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
References: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
Message-ID: <CA+vqiLHNKGpfg4ygkB2Zpv+1qqP5vTkPGFPFzLUh+ekJaxv8ng@mail.gmail.com>

That looks to me like it might be buggy. At least I would have
expected vjust to do _something_.

In terms of the practical issue, you can adjust the distance between
the title and the axis with margin, e.g.

ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(margin =
margin(0, 5, 0, 0)))

Best,
Ista

On Tue, Dec 22, 2015 at 12:59 PM, Ryan Utz <utz.ryan at gmail.com> wrote:
> Hi all,
>
> I cannot for the life of me get my axis titles to adjust vertically in a
> ggplot. I've seen several posts about this and have tried everything:
> keeping vjust within 0 and 1, adjusting the margins, etc. hjust is behaving
> just as it should but vjust just mocks me in silence. No error message is
> produced.
>
> Here's a sample code:
>
> x=data.frame(sample(1:10))
> x[,2]=sample(1:10)
>
> ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(vjust=.1,hjust=0.6),
> plot.margin=unit(c(1,1,2,2),'cm'))
>
> No matter what I put into vjust, nothing happens. Am I missing something
> obvious??
>
> Thanks ahead of time for any help,
> Ryan
>
>
> --
>
> Ryan Utz, Ph.D.
> Assistant professor of water resources
> *chatham**UNIVERSITY*
> Home/Cell: (724) 272-7769
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From NordlDJ at dshs.wa.gov  Tue Dec 22 19:37:40 2015
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 22 Dec 2015 18:37:40 +0000
Subject: [R] vjust unresponsive (ggplot2)
In-Reply-To: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
References: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662EDD6BC2@WAXMXOLYMB025.WAX.wa.lcl>

Are you sure it is not working for you?  Your example code did not work for me at all until I removed the plot .margin  parameter (unit wasn't recognized).  Once I did that hjust and vjust worked as expected.  However, values between .1 and .9 for vjust don't really move the axis title very much so it may not be real noticeable.  Try a value like 2 or 3, just to make sure you easily see the change in position before concluding that nothing is happening.

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ryan Utz
Sent: Tuesday, December 22, 2015 10:00 AM
To: r-help at r-project.org
Subject: [R] vjust unresponsive (ggplot2)

Hi all,

I cannot for the life of me get my axis titles to adjust vertically in a ggplot. I've seen several posts about this and have tried everything:
keeping vjust within 0 and 1, adjusting the margins, etc. hjust is behaving just as it should but vjust just mocks me in silence. No error message is produced.

Here's a sample code:

x=data.frame(sample(1:10))
x[,2]=sample(1:10)

ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(vjust=.1,hjust=0.6),
plot.margin=unit(c(1,1,2,2),'cm'))

No matter what I put into vjust, nothing happens. Am I missing something obvious??

Thanks ahead of time for any help,
Ryan


-- 

Ryan Utz, Ph.D.
Assistant professor of water resources
*chatham**UNIVERSITY*
Home/Cell: (724) 272-7769

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Tue Dec 22 19:38:39 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 22 Dec 2015 19:38:39 +0100
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
	<1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>


> On 22 Dec 2015, at 07:30 , mohsen hs via R-help <r-help at r-project.org> wrote:
> 
> The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp and the left one is from qqPlot. Titles might be incorrect.

They never arrived, but your data weren't actually needed. The crucial missing information was the packages used. This will do:

> library(EnvStats); library(fitdistrplus)
> serving <- exp(rnorm(100))
> qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)
> 
> fitln <- fitdist(serving,"lnorm",method="mle")
> qqcomp(fitln)

The difference is quite clearly that qqPlot is doing a QQ-plot of log(serving) vs. normal quantiles, whereas qqcomp plots serving itself against lognormal quantiles. So the former is pretty much equal to the latter on a log-log scale.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From istazahn at gmail.com  Tue Dec 22 19:47:53 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Tue, 22 Dec 2015 13:47:53 -0500
Subject: [R] vjust unresponsive (ggplot2)
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA27662EDD6BC2@WAXMXOLYMB025.WAX.wa.lcl>
References: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA27662EDD6BC2@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <CA+vqiLH2eTgOCt4g_tdNm4UR1noxDTGE=KFvE_=_zZs1vaQfQg@mail.gmail.com>

Hi Dan,

Chances are that you haven't yet upgraded to ggplot2 version 2.0. unit
(as well as arrow and alpha) are now re-exported from ggplot2.

Using the latest release I also see that vjust doesn't seem to do anything.

Best,
Ista

On Tue, Dec 22, 2015 at 1:37 PM, Nordlund, Dan (DSHS/RDA)
<NordlDJ at dshs.wa.gov> wrote:
> Are you sure it is not working for you?  Your example code did not work for me at all until I removed the plot .margin  parameter (unit wasn't recognized).  Once I did that hjust and vjust worked as expected.  However, values between .1 and .9 for vjust don't really move the axis title very much so it may not be real noticeable.  Try a value like 2 or 3, just to make sure you easily see the change in position before concluding that nothing is happening.
>
> Dan
>
> Daniel Nordlund, PhD
> Research and Data Analysis Division
> Services & Enterprise Support Administration
> Washington State Department of Social and Health Services
>
>
> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ryan Utz
> Sent: Tuesday, December 22, 2015 10:00 AM
> To: r-help at r-project.org
> Subject: [R] vjust unresponsive (ggplot2)
>
> Hi all,
>
> I cannot for the life of me get my axis titles to adjust vertically in a ggplot. I've seen several posts about this and have tried everything:
> keeping vjust within 0 and 1, adjusting the margins, etc. hjust is behaving just as it should but vjust just mocks me in silence. No error message is produced.
>
> Here's a sample code:
>
> x=data.frame(sample(1:10))
> x[,2]=sample(1:10)
>
> ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(vjust=.1,hjust=0.6),
> plot.margin=unit(c(1,1,2,2),'cm'))
>
> No matter what I put into vjust, nothing happens. Am I missing something obvious??
>
> Thanks ahead of time for any help,
> Ryan
>
>
> --
>
> Ryan Utz, Ph.D.
> Assistant professor of water resources
> *chatham**UNIVERSITY*
> Home/Cell: (724) 272-7769
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From NordlDJ at dshs.wa.gov  Tue Dec 22 20:15:37 2015
From: NordlDJ at dshs.wa.gov (Nordlund, Dan (DSHS/RDA))
Date: Tue, 22 Dec 2015 19:15:37 +0000
Subject: [R] vjust unresponsive (ggplot2)
In-Reply-To: <CA+vqiLH2eTgOCt4g_tdNm4UR1noxDTGE=KFvE_=_zZs1vaQfQg@mail.gmail.com>
References: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA27662EDD6BC2@WAXMXOLYMB025.WAX.wa.lcl>
	<CA+vqiLH2eTgOCt4g_tdNm4UR1noxDTGE=KFvE_=_zZs1vaQfQg@mail.gmail.com>
Message-ID: <F7E6D18CC2877149AB5296CE54EA27662EDD7BF3@WAXMXOLYMB025.WAX.wa.lcl>

Ista,

You are correct, I was not at the latest release of ggplot2.  I updated to the latest version and am now seeing the same result as you and the OP.  So it does look like an issue with the latest version of ggplot2.

Dan

Daniel Nordlund, PhD
Research and Data Analysis Division
Services & Enterprise Support Administration
Washington State Department of Social and Health Services


> -----Original Message-----
> From: Ista Zahn [mailto:istazahn at gmail.com]
> Sent: Tuesday, December 22, 2015 10:48 AM
> To: Nordlund, Dan (DSHS/RDA)
> Cc: r-help at r-project.org
> Subject: Re: [R] vjust unresponsive (ggplot2)
> 
> Hi Dan,
> 
> Chances are that you haven't yet upgraded to ggplot2 version 2.0. unit (as
> well as arrow and alpha) are now re-exported from ggplot2.
> 
> Using the latest release I also see that vjust doesn't seem to do anything.
> 
> Best,
> Ista
> 
> On Tue, Dec 22, 2015 at 1:37 PM, Nordlund, Dan (DSHS/RDA)
> <NordlDJ at dshs.wa.gov> wrote:
> > Are you sure it is not working for you?  Your example code did not work for
> me at all until I removed the plot .margin  parameter (unit wasn't
> recognized).  Once I did that hjust and vjust worked as expected.  However,
> values between .1 and .9 for vjust don't really move the axis title very much
> so it may not be real noticeable.  Try a value like 2 or 3, just to make sure you
> easily see the change in position before concluding that nothing is
> happening.
> >
> > Dan
> >
> > Daniel Nordlund, PhD
> > Research and Data Analysis Division
> > Services & Enterprise Support Administration Washington State
> > Department of Social and Health Services
> >
> >
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Ryan
> > Utz
> > Sent: Tuesday, December 22, 2015 10:00 AM
> > To: r-help at r-project.org
> > Subject: [R] vjust unresponsive (ggplot2)
> >
> > Hi all,
> >
> > I cannot for the life of me get my axis titles to adjust vertically in a ggplot.
> I've seen several posts about this and have tried everything:
> > keeping vjust within 0 and 1, adjusting the margins, etc. hjust is behaving
> just as it should but vjust just mocks me in silence. No error message is
> produced.
> >
> > Here's a sample code:
> >
> > x=data.frame(sample(1:10))
> > x[,2]=sample(1:10)
> >
> > ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(vjust=.1
> > ,hjust=0.6),
> > plot.margin=unit(c(1,1,2,2),'cm'))
> >
> > No matter what I put into vjust, nothing happens. Am I missing something
> obvious??
> >
> > Thanks ahead of time for any help,
> > Ryan
> >
> >
> > --
> >
> > Ryan Utz, Ph.D.
> > Assistant professor of water resources
> > *chatham**UNIVERSITY*
> > Home/Cell: (724) 272-7769
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> > http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.

From jholtman at gmail.com  Tue Dec 22 20:26:17 2015
From: jholtman at gmail.com (jim holtman)
Date: Tue, 22 Dec 2015 14:26:17 -0500
Subject: [R] Trying to avoid the loop while merging two data frames
In-Reply-To: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>
References: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>
Message-ID: <CAAxdm-6jYJL3ifk8cbxf4TNC8=a11biskYUQGbL3Gi0cXUErtA@mail.gmail.com>

You seem to be saving 'myid' and then overwriting it with the last
statement:

 result[[i]] <- result[[i]][c(5, 1:4)]

Why doesn't 'merge' work for you?  I tried it on your data, and seem to get
back the same number of rows; may not be in the same order, but the content
looks the same, and it does have 'myid' on it.


Jim Holtman
Data Munger Guru

What is the problem that you are trying to solve?
Tell me what you want to do, not how you want to do it.

On Tue, Dec 22, 2015 at 12:27 PM, Dimitri Liakhovitski <
dimitri.liakhovitski at gmail.com> wrote:

> Hello!
> I have a solution for my task that is based on a loop. However, it's
> too slow for my real-life problem that is much larger in scope.
> However, I cannot use merge. Any advice on how to do it faster?
> Thanks a lot for any hint on how to speed it up!
>
> # I have 'mydata' data frame:
> set.seed(123)
> mydata <- data.frame(myid = 1001:1100,
>                      version = sample(1:20, 100, replace = T))
> head(mydata)
> table(mydata$version)
>
> # I have 'myinfo' data frame that contains information for each 'version':
> set.seed(12)
> myinfo <- data.frame(version = sort(rep(1:20, 30)), a = rnorm(60), b =
> rnorm(60),
>                                  c = rnorm(60), d = rnorm(60))
> head(myinfo, 40)
>
> ### MY SOLUTION WITH A LOOP:
> ### Looping through each id of mydata and grabbing
> ### all columns from 'myinfo' for the corresponding 'version':
>
> # 1. Creating placeholder list for the results:
> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
> length(result)
> (result)[1:3]
>
>
> # 2. Looping through each element of 'result':
> for(i in 1:length(result)){
>       id <- result[[i]]$myid
>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>       result[[i]]$myid <- id
>       result[[i]] <- result[[i]][c(5, 1:4)]
> }
> result <- do.call(rbind, result)
> head(result) # This is the desired result
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Tue Dec 22 20:29:06 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Tue, 22 Dec 2015 19:29:06 +0000
Subject: [R] Trying to avoid the loop while merging two data frames
Message-ID: <248E6FA047A8C746BA491485764190F53715ED04@ESESSMB207.ericsson.se>

Library dplyr to use arrange() for ordering, in the case.

library(dplyr)

result.order <- arrange(result, d, version, a, b, c)
dim(result.order)
[1] 3000    5

head(result.order)
             d version            a              b           c
1 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
2 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
3 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
4 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
5 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
6 -2.986456069       3 0.2236414154 0.004258038663 1.089406822


my.merge <- merge(myinfo, mydata, by="version")
result2 <- my.merge[,c("d", "version", "a", "b", "c")]
result2.order <- arrange(result2, d, version, a, b, c)
dim(result2.order)
[1] 3000    5

head(result2.order)
             d version            a              b           c
1 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
2 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
3 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
4 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
5 -2.986456069       1 0.2236414154 0.004258038663 1.089406822
6 -2.986456069       3 0.2236414154 0.004258038663 1.089406822

all.equal(result.order, result2.order)
[1] TRUE


--

GG

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Tue Dec 22 20:50:31 2015
From: jrkrideau at inbox.com (John Kane)
Date: Tue, 22 Dec 2015 11:50:31 -0800
Subject: [R] colour input in ribbon plots (geom ribbon)
In-Reply-To: <CAAATBeBYHhbkME8sxFmo1fVzmNfJeFb85V10cJS5cZdBQxyVPg@mail.gmail.com>
Message-ID: <AC6186D424C.00000480jrkrideau@inbox.com>

It is not clear what you are doing. Please provide some code and data if possible. 

Otherwise adding something like colour = "red" in the aes() should do something. 

John Kane
Kingston ON Canada


> -----Original Message-----
> From: pushan.zoology at gmail.com
> Sent: Tue, 22 Dec 2015 14:26:03 +0530
> To: r-help at r-project.org
> Subject: [R] colour input in ribbon plots (geom ribbon)
> 
> Hallow everybody
> I want to know if there is any command for input colour in the ribbon
> plot
> (geom ribbon) of ggplot2. Unless that, the mean line and the standard
> error
> can not be distinguished. Please help me if anyone have any idea.
> 
> --
> Pushan Chakraborty
> CSIR - SRF
> 
> Center for Pollination Studies, University of Calcutta
> 35, Ballyguanje Circular Road, Kolkata - 700019
>           &
> Wildlife Institute of India
> Chandrabani, Dehradun - 248001
> 
> webpage:
> 
> http://cpscu.in/?page_id=51
> 
> Skype: cpushan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jdnewmil at dcn.davis.ca.us  Tue Dec 22 21:13:45 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 22 Dec 2015 12:13:45 -0800
Subject: [R] colour input in ribbon plots (geom ribbon)
In-Reply-To: <AC6186D424C.00000480jrkrideau@inbox.com>
References: <AC6186D424C.00000480jrkrideau@inbox.com>
Message-ID: <1445AB3A-9786-45C8-9243-F5E84AE7CD9D@dcn.davis.ca.us>

Um, NOT in the aesthetic... as a direct argument to geom_ribbon.

But yes,  examples are critical for communication about R questions, and posting on plain text format is critical for getting the example through the list server undamaged. 
-- 
Sent from my phone. Please excuse my brevity.

On December 22, 2015 11:50:31 AM PST, John Kane <jrkrideau at inbox.com> wrote:
>It is not clear what you are doing. Please provide some code and data
>if possible. 
>
>Otherwise adding something like colour = "red" in the aes() should do
>something. 
>
>John Kane
>Kingston ON Canada
>
>
>> -----Original Message-----
>> From: pushan.zoology at gmail.com
>> Sent: Tue, 22 Dec 2015 14:26:03 +0530
>> To: r-help at r-project.org
>> Subject: [R] colour input in ribbon plots (geom ribbon)
>> 
>> Hallow everybody
>> I want to know if there is any command for input colour in the ribbon
>> plot
>> (geom ribbon) of ggplot2. Unless that, the mean line and the standard
>> error
>> can not be distinguished. Please help me if anyone have any idea.
>> 
>> --
>> Pushan Chakraborty
>> CSIR - SRF
>> 
>> Center for Pollination Studies, University of Calcutta
>> 35, Ballyguanje Circular Road, Kolkata - 700019
>>           &
>> Wildlife Institute of India
>> Chandrabani, Dehradun - 248001
>> 
>> webpage:
>> 
>> http://cpscu.in/?page_id=51
>> 
>> Skype: cpushan
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>____________________________________________________________
>FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Tue Dec 22 21:34:23 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 22 Dec 2015 15:34:23 -0500
Subject: [R] Trying to avoid the loop while merging two data frames
In-Reply-To: <CAAxdm-6jYJL3ifk8cbxf4TNC8=a11biskYUQGbL3Gi0cXUErtA@mail.gmail.com>
References: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>
	<CAAxdm-6jYJL3ifk8cbxf4TNC8=a11biskYUQGbL3Gi0cXUErtA@mail.gmail.com>
Message-ID: <CAN2xGJaEGL2N-ZzzKt48m2_8G+EKVuvJXr8NAzFdfRE0+8wcZQ@mail.gmail.com>

I know I am overwriting.
merge doesn't solve it because each version in mydata is given to more
than one id. Hence, I thought I can't merge by version.
I am not sure how to answer the question about "the problem".
I described the current state and the desired state. If possible, I'd
like to get from the current state to the desired state faster than
when using a loop.

On Tue, Dec 22, 2015 at 2:26 PM, jim holtman <jholtman at gmail.com> wrote:
> You seem to be saving 'myid' and then overwriting it with the last
> statement:
>
>  result[[i]] <- result[[i]][c(5, 1:4)]
>
> Why doesn't 'merge' work for you?  I tried it on your data, and seem to get
> back the same number of rows; may not be in the same order, but the content
> looks the same, and it does have 'myid' on it.
>
>
> Jim Holtman
> Data Munger Guru
>
> What is the problem that you are trying to solve?
> Tell me what you want to do, not how you want to do it.
>
> On Tue, Dec 22, 2015 at 12:27 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>>
>> Hello!
>> I have a solution for my task that is based on a loop. However, it's
>> too slow for my real-life problem that is much larger in scope.
>> However, I cannot use merge. Any advice on how to do it faster?
>> Thanks a lot for any hint on how to speed it up!
>>
>> # I have 'mydata' data frame:
>> set.seed(123)
>> mydata <- data.frame(myid = 1001:1100,
>>                      version = sample(1:20, 100, replace = T))
>> head(mydata)
>> table(mydata$version)
>>
>> # I have 'myinfo' data frame that contains information for each 'version':
>> set.seed(12)
>> myinfo <- data.frame(version = sort(rep(1:20, 30)), a = rnorm(60), b =
>> rnorm(60),
>>                                  c = rnorm(60), d = rnorm(60))
>> head(myinfo, 40)
>>
>> ### MY SOLUTION WITH A LOOP:
>> ### Looping through each id of mydata and grabbing
>> ### all columns from 'myinfo' for the corresponding 'version':
>>
>> # 1. Creating placeholder list for the results:
>> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
>> length(result)
>> (result)[1:3]
>>
>>
>> # 2. Looping through each element of 'result':
>> for(i in 1:length(result)){
>>       id <- result[[i]]$myid
>>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>>       result[[i]]$myid <- id
>>       result[[i]] <- result[[i]][c(5, 1:4)]
>> }
>> result <- do.call(rbind, result)
>> head(result) # This is the desired result
>>
>> --
>> Dimitri Liakhovitski
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>



-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Tue Dec 22 21:50:39 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 22 Dec 2015 15:50:39 -0500
Subject: [R] Trying to avoid the loop while merging two data frames
In-Reply-To: <CAN2xGJaEGL2N-ZzzKt48m2_8G+EKVuvJXr8NAzFdfRE0+8wcZQ@mail.gmail.com>
References: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>
	<CAAxdm-6jYJL3ifk8cbxf4TNC8=a11biskYUQGbL3Gi0cXUErtA@mail.gmail.com>
	<CAN2xGJaEGL2N-ZzzKt48m2_8G+EKVuvJXr8NAzFdfRE0+8wcZQ@mail.gmail.com>
Message-ID: <CAN2xGJYjmb6GnrJgjiFqaC4J7wdGGKOJqtKN=+9wMxTm_Bnanw@mail.gmail.com>

You are right, guys, merge is working. Somehow I was under the
erroneous impression that because the second data frame (myinfo)
contains no column 'myid' merge will not work.
Below is the cleaner code and comparison:

#########################################
### Example with smaller data frames
#########################################

set.seed(123)
mydata <- data.frame(myid = 1001:1020,
                     version = sample(1:10, 20, replace = T))
head(mydata)
table(mydata$version)

set.seed(12)
myinfo <- data.frame(version = sort(rep(1:10, 5)), a = rnorm(50), b =
rnorm(50), c = rnorm(50), d = rnorm(50))
head(myinfo, 40)
table(myinfo$version)

###----------------------------------------
### METHOD 1 - Looping through each id of mydata and grabbing
### all columns of myinfo for the corresponding 'version':


# Create placeholder list for the results:
result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
length(result)
(result)[1:3]


# Looping through each element of 'result':
for(i in 1:length(result)){
      id <- result[[i]]$myid
      result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
      result[[i]]$myid <- id
      result[[i]] <- result[[i]][c(6, 1:5)]
}
result <- do.call(rbind, result)
result.order <- arrange(result,  myid, version, a, b, c, d)
head(result.order) # This is the desired result

###----------------------------------------
### METHOD 2 - merge

my.merge <- merge(myinfo, mydata, by="version")
names(my.merge)
result2 <- my.merge[,c("myid", "version", "a", "b", "c", "d")]
names(result2)
result2.order <- arrange(result2, myid, version, a, b, c, d)
dim(result2.order)
head(result2.order)

# Same result?
all.equal(result.order, result2.order)

On Tue, Dec 22, 2015 at 3:34 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> I know I am overwriting.
> merge doesn't solve it because each version in mydata is given to more
> than one id. Hence, I thought I can't merge by version.
> I am not sure how to answer the question about "the problem".
> I described the current state and the desired state. If possible, I'd
> like to get from the current state to the desired state faster than
> when using a loop.
>
> On Tue, Dec 22, 2015 at 2:26 PM, jim holtman <jholtman at gmail.com> wrote:
>> You seem to be saving 'myid' and then overwriting it with the last
>> statement:
>>
>>  result[[i]] <- result[[i]][c(5, 1:4)]
>>
>> Why doesn't 'merge' work for you?  I tried it on your data, and seem to get
>> back the same number of rows; may not be in the same order, but the content
>> looks the same, and it does have 'myid' on it.
>>
>>
>> Jim Holtman
>> Data Munger Guru
>>
>> What is the problem that you are trying to solve?
>> Tell me what you want to do, not how you want to do it.
>>
>> On Tue, Dec 22, 2015 at 12:27 PM, Dimitri Liakhovitski
>> <dimitri.liakhovitski at gmail.com> wrote:
>>>
>>> Hello!
>>> I have a solution for my task that is based on a loop. However, it's
>>> too slow for my real-life problem that is much larger in scope.
>>> However, I cannot use merge. Any advice on how to do it faster?
>>> Thanks a lot for any hint on how to speed it up!
>>>
>>> # I have 'mydata' data frame:
>>> set.seed(123)
>>> mydata <- data.frame(myid = 1001:1100,
>>>                      version = sample(1:20, 100, replace = T))
>>> head(mydata)
>>> table(mydata$version)
>>>
>>> # I have 'myinfo' data frame that contains information for each 'version':
>>> set.seed(12)
>>> myinfo <- data.frame(version = sort(rep(1:20, 30)), a = rnorm(60), b =
>>> rnorm(60),
>>>                                  c = rnorm(60), d = rnorm(60))
>>> head(myinfo, 40)
>>>
>>> ### MY SOLUTION WITH A LOOP:
>>> ### Looping through each id of mydata and grabbing
>>> ### all columns from 'myinfo' for the corresponding 'version':
>>>
>>> # 1. Creating placeholder list for the results:
>>> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
>>> length(result)
>>> (result)[1:3]
>>>
>>>
>>> # 2. Looping through each element of 'result':
>>> for(i in 1:length(result)){
>>>       id <- result[[i]]$myid
>>>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>>>       result[[i]]$myid <- id
>>>       result[[i]] <- result[[i]][c(5, 1:4)]
>>> }
>>> result <- do.call(rbind, result)
>>> head(result) # This is the desired result
>>>
>>> --
>>> Dimitri Liakhovitski
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>>
>
>
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Tue Dec 22 21:56:50 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 22 Dec 2015 15:56:50 -0500
Subject: [R] Trying to avoid the loop while merging two data frames
In-Reply-To: <CAN2xGJYjmb6GnrJgjiFqaC4J7wdGGKOJqtKN=+9wMxTm_Bnanw@mail.gmail.com>
References: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>
	<CAAxdm-6jYJL3ifk8cbxf4TNC8=a11biskYUQGbL3Gi0cXUErtA@mail.gmail.com>
	<CAN2xGJaEGL2N-ZzzKt48m2_8G+EKVuvJXr8NAzFdfRE0+8wcZQ@mail.gmail.com>
	<CAN2xGJYjmb6GnrJgjiFqaC4J7wdGGKOJqtKN=+9wMxTm_Bnanw@mail.gmail.com>
Message-ID: <CAN2xGJae33c2+My+-4hsXkM7Rv9UiyLwHBsZBD-JyN1hE58q9w@mail.gmail.com>

Actually, the correct merge line should be:
my.merge <- merge(myinfo, mydata, by="version", all.x = T, all.y = F)

On Tue, Dec 22, 2015 at 3:50 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> You are right, guys, merge is working. Somehow I was under the
> erroneous impression that because the second data frame (myinfo)
> contains no column 'myid' merge will not work.
> Below is the cleaner code and comparison:
>
> #########################################
> ### Example with smaller data frames
> #########################################
>
> set.seed(123)
> mydata <- data.frame(myid = 1001:1020,
>                      version = sample(1:10, 20, replace = T))
> head(mydata)
> table(mydata$version)
>
> set.seed(12)
> myinfo <- data.frame(version = sort(rep(1:10, 5)), a = rnorm(50), b =
> rnorm(50), c = rnorm(50), d = rnorm(50))
> head(myinfo, 40)
> table(myinfo$version)
>
> ###----------------------------------------
> ### METHOD 1 - Looping through each id of mydata and grabbing
> ### all columns of myinfo for the corresponding 'version':
>
>
> # Create placeholder list for the results:
> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
> length(result)
> (result)[1:3]
>
>
> # Looping through each element of 'result':
> for(i in 1:length(result)){
>       id <- result[[i]]$myid
>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>       result[[i]]$myid <- id
>       result[[i]] <- result[[i]][c(6, 1:5)]
> }
> result <- do.call(rbind, result)
> result.order <- arrange(result,  myid, version, a, b, c, d)
> head(result.order) # This is the desired result
>
> ###----------------------------------------
> ### METHOD 2 - merge
>
> my.merge <- merge(myinfo, mydata, by="version")
> names(my.merge)
> result2 <- my.merge[,c("myid", "version", "a", "b", "c", "d")]
> names(result2)
> result2.order <- arrange(result2, myid, version, a, b, c, d)
> dim(result2.order)
> head(result2.order)
>
> # Same result?
> all.equal(result.order, result2.order)
>
> On Tue, Dec 22, 2015 at 3:34 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> I know I am overwriting.
>> merge doesn't solve it because each version in mydata is given to more
>> than one id. Hence, I thought I can't merge by version.
>> I am not sure how to answer the question about "the problem".
>> I described the current state and the desired state. If possible, I'd
>> like to get from the current state to the desired state faster than
>> when using a loop.
>>
>> On Tue, Dec 22, 2015 at 2:26 PM, jim holtman <jholtman at gmail.com> wrote:
>>> You seem to be saving 'myid' and then overwriting it with the last
>>> statement:
>>>
>>>  result[[i]] <- result[[i]][c(5, 1:4)]
>>>
>>> Why doesn't 'merge' work for you?  I tried it on your data, and seem to get
>>> back the same number of rows; may not be in the same order, but the content
>>> looks the same, and it does have 'myid' on it.
>>>
>>>
>>> Jim Holtman
>>> Data Munger Guru
>>>
>>> What is the problem that you are trying to solve?
>>> Tell me what you want to do, not how you want to do it.
>>>
>>> On Tue, Dec 22, 2015 at 12:27 PM, Dimitri Liakhovitski
>>> <dimitri.liakhovitski at gmail.com> wrote:
>>>>
>>>> Hello!
>>>> I have a solution for my task that is based on a loop. However, it's
>>>> too slow for my real-life problem that is much larger in scope.
>>>> However, I cannot use merge. Any advice on how to do it faster?
>>>> Thanks a lot for any hint on how to speed it up!
>>>>
>>>> # I have 'mydata' data frame:
>>>> set.seed(123)
>>>> mydata <- data.frame(myid = 1001:1100,
>>>>                      version = sample(1:20, 100, replace = T))
>>>> head(mydata)
>>>> table(mydata$version)
>>>>
>>>> # I have 'myinfo' data frame that contains information for each 'version':
>>>> set.seed(12)
>>>> myinfo <- data.frame(version = sort(rep(1:20, 30)), a = rnorm(60), b =
>>>> rnorm(60),
>>>>                                  c = rnorm(60), d = rnorm(60))
>>>> head(myinfo, 40)
>>>>
>>>> ### MY SOLUTION WITH A LOOP:
>>>> ### Looping through each id of mydata and grabbing
>>>> ### all columns from 'myinfo' for the corresponding 'version':
>>>>
>>>> # 1. Creating placeholder list for the results:
>>>> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
>>>> length(result)
>>>> (result)[1:3]
>>>>
>>>>
>>>> # 2. Looping through each element of 'result':
>>>> for(i in 1:length(result)){
>>>>       id <- result[[i]]$myid
>>>>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>>>>       result[[i]]$myid <- id
>>>>       result[[i]] <- result[[i]][c(5, 1:4)]
>>>> }
>>>> result <- do.call(rbind, result)
>>>> head(result) # This is the desired result
>>>>
>>>> --
>>>> Dimitri Liakhovitski
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>
>>
>>
>> --
>> Dimitri Liakhovitski
>
>
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From dimitri.liakhovitski at gmail.com  Tue Dec 22 22:10:23 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 22 Dec 2015 16:10:23 -0500
Subject: [R] Trying to avoid the loop while merging two data frames
In-Reply-To: <CAN2xGJae33c2+My+-4hsXkM7Rv9UiyLwHBsZBD-JyN1hE58q9w@mail.gmail.com>
References: <CAN2xGJbhk1_MTA1BatEDzYxXeZhuZLrLyWjEkrubMMZHccgfJg@mail.gmail.com>
	<CAAxdm-6jYJL3ifk8cbxf4TNC8=a11biskYUQGbL3Gi0cXUErtA@mail.gmail.com>
	<CAN2xGJaEGL2N-ZzzKt48m2_8G+EKVuvJXr8NAzFdfRE0+8wcZQ@mail.gmail.com>
	<CAN2xGJYjmb6GnrJgjiFqaC4J7wdGGKOJqtKN=+9wMxTm_Bnanw@mail.gmail.com>
	<CAN2xGJae33c2+My+-4hsXkM7Rv9UiyLwHBsZBD-JyN1hE58q9w@mail.gmail.com>
Message-ID: <CAN2xGJa1JtFucm_FMMc=MV5nSSfgDwNAmgDnB55cuNuv31Kcqg@mail.gmail.com>

Taking it back - no need for all.x = T, all.y = F

On Tue, Dec 22, 2015 at 3:56 PM, Dimitri Liakhovitski
<dimitri.liakhovitski at gmail.com> wrote:
> Actually, the correct merge line should be:
> my.merge <- merge(myinfo, mydata, by="version", all.x = T, all.y = F)
>
> On Tue, Dec 22, 2015 at 3:50 PM, Dimitri Liakhovitski
> <dimitri.liakhovitski at gmail.com> wrote:
>> You are right, guys, merge is working. Somehow I was under the
>> erroneous impression that because the second data frame (myinfo)
>> contains no column 'myid' merge will not work.
>> Below is the cleaner code and comparison:
>>
>> #########################################
>> ### Example with smaller data frames
>> #########################################
>>
>> set.seed(123)
>> mydata <- data.frame(myid = 1001:1020,
>>                      version = sample(1:10, 20, replace = T))
>> head(mydata)
>> table(mydata$version)
>>
>> set.seed(12)
>> myinfo <- data.frame(version = sort(rep(1:10, 5)), a = rnorm(50), b =
>> rnorm(50), c = rnorm(50), d = rnorm(50))
>> head(myinfo, 40)
>> table(myinfo$version)
>>
>> ###----------------------------------------
>> ### METHOD 1 - Looping through each id of mydata and grabbing
>> ### all columns of myinfo for the corresponding 'version':
>>
>>
>> # Create placeholder list for the results:
>> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
>> length(result)
>> (result)[1:3]
>>
>>
>> # Looping through each element of 'result':
>> for(i in 1:length(result)){
>>       id <- result[[i]]$myid
>>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>>       result[[i]]$myid <- id
>>       result[[i]] <- result[[i]][c(6, 1:5)]
>> }
>> result <- do.call(rbind, result)
>> result.order <- arrange(result,  myid, version, a, b, c, d)
>> head(result.order) # This is the desired result
>>
>> ###----------------------------------------
>> ### METHOD 2 - merge
>>
>> my.merge <- merge(myinfo, mydata, by="version")
>> names(my.merge)
>> result2 <- my.merge[,c("myid", "version", "a", "b", "c", "d")]
>> names(result2)
>> result2.order <- arrange(result2, myid, version, a, b, c, d)
>> dim(result2.order)
>> head(result2.order)
>>
>> # Same result?
>> all.equal(result.order, result2.order)
>>
>> On Tue, Dec 22, 2015 at 3:34 PM, Dimitri Liakhovitski
>> <dimitri.liakhovitski at gmail.com> wrote:
>>> I know I am overwriting.
>>> merge doesn't solve it because each version in mydata is given to more
>>> than one id. Hence, I thought I can't merge by version.
>>> I am not sure how to answer the question about "the problem".
>>> I described the current state and the desired state. If possible, I'd
>>> like to get from the current state to the desired state faster than
>>> when using a loop.
>>>
>>> On Tue, Dec 22, 2015 at 2:26 PM, jim holtman <jholtman at gmail.com> wrote:
>>>> You seem to be saving 'myid' and then overwriting it with the last
>>>> statement:
>>>>
>>>>  result[[i]] <- result[[i]][c(5, 1:4)]
>>>>
>>>> Why doesn't 'merge' work for you?  I tried it on your data, and seem to get
>>>> back the same number of rows; may not be in the same order, but the content
>>>> looks the same, and it does have 'myid' on it.
>>>>
>>>>
>>>> Jim Holtman
>>>> Data Munger Guru
>>>>
>>>> What is the problem that you are trying to solve?
>>>> Tell me what you want to do, not how you want to do it.
>>>>
>>>> On Tue, Dec 22, 2015 at 12:27 PM, Dimitri Liakhovitski
>>>> <dimitri.liakhovitski at gmail.com> wrote:
>>>>>
>>>>> Hello!
>>>>> I have a solution for my task that is based on a loop. However, it's
>>>>> too slow for my real-life problem that is much larger in scope.
>>>>> However, I cannot use merge. Any advice on how to do it faster?
>>>>> Thanks a lot for any hint on how to speed it up!
>>>>>
>>>>> # I have 'mydata' data frame:
>>>>> set.seed(123)
>>>>> mydata <- data.frame(myid = 1001:1100,
>>>>>                      version = sample(1:20, 100, replace = T))
>>>>> head(mydata)
>>>>> table(mydata$version)
>>>>>
>>>>> # I have 'myinfo' data frame that contains information for each 'version':
>>>>> set.seed(12)
>>>>> myinfo <- data.frame(version = sort(rep(1:20, 30)), a = rnorm(60), b =
>>>>> rnorm(60),
>>>>>                                  c = rnorm(60), d = rnorm(60))
>>>>> head(myinfo, 40)
>>>>>
>>>>> ### MY SOLUTION WITH A LOOP:
>>>>> ### Looping through each id of mydata and grabbing
>>>>> ### all columns from 'myinfo' for the corresponding 'version':
>>>>>
>>>>> # 1. Creating placeholder list for the results:
>>>>> result <- split(mydata[c("myid", "version")], f = list(mydata$myid))
>>>>> length(result)
>>>>> (result)[1:3]
>>>>>
>>>>>
>>>>> # 2. Looping through each element of 'result':
>>>>> for(i in 1:length(result)){
>>>>>       id <- result[[i]]$myid
>>>>>       result[[i]] <- myinfo[myinfo$version == result[[i]]$version, ]
>>>>>       result[[i]]$myid <- id
>>>>>       result[[i]] <- result[[i]][c(5, 1:4)]
>>>>> }
>>>>> result <- do.call(rbind, result)
>>>>> head(result) # This is the desired result
>>>>>
>>>>> --
>>>>> Dimitri Liakhovitski
>>>>>
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>
>>>
>>>
>>> --
>>> Dimitri Liakhovitski
>>
>>
>>
>> --
>> Dimitri Liakhovitski
>
>
>
> --
> Dimitri Liakhovitski



-- 
Dimitri Liakhovitski


From ragia11 at hotmail.com  Tue Dec 22 22:10:58 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Tue, 22 Dec 2015 23:10:58 +0200
Subject: [R] R studio installation and running
Message-ID: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>

Dear group
I used to run Rstudio on windows.
currently ?I want to run Rstudio on Ubuntu server, start its GUI .
kindly, is there a guide on how to do this step by step, I already searched and installed R .. but confused ..should I install Rstudio for desktop or server ? how to start it?, I am the server admin.

thanks in advance
Ragia

 		 	   		  

From jdnewmil at dcn.davis.ca.us  Tue Dec 22 22:33:20 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 22 Dec 2015 13:33:20 -0800
Subject: [R] R studio installation and running
In-Reply-To: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
Message-ID: <6333056F-BFF1-43E6-AE5F-518BB7745604@dcn.davis.ca.us>

This is not the support area for RStudio.

FWIW if you will be sitting in front of this computer then you probably want the desktop version. 
-- 
Sent from my phone. Please excuse my brevity.

On December 22, 2015 1:10:58 PM PST, Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>Dear group
>I used to run Rstudio on windows.
>currently ?I want to run Rstudio on Ubuntu server, start its GUI .
>kindly, is there a guide on how to do this step by step, I already
>searched and installed R .. but confused ..should I install Rstudio for
>desktop or server ? how to start it?, I am the server admin.
>
>thanks in advance
>Ragia
>
> 		 	   		  
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From dimitri.liakhovitski at gmail.com  Tue Dec 22 23:42:51 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Tue, 22 Dec 2015 17:42:51 -0500
Subject: [R] Expanding matrix into dummies
Message-ID: <CAN2xGJbK+539fR4Qkm2MfE3pYJ7QPnDBmRKom=9fDc-=UTYSMw@mail.gmail.com>

# I have a matrix x:

k <- 20
N <- 5
set.seed(123)
x <- matrix(c(sample(1:k, N, replace = F),
              sample(1:k, N, replace = F),
              sample(1:k, N, replace = F),
              sample(1:k, N, replace = F),
              sample(1:k, N, replace = F),
              sample(1:k, N, replace = F)), byrow = T, ncol = 5)
colnames(x) <- paste0("column", 1:5)
(x)

# I want to reshape it into a matrix 'result' with k columns (not N).
# 'result' should contain the same number of rows as x, and it should have
# in each row 1s in those columns that correspond to the entries in x
in the same row.
# For example, the first row of 'result' should contain 1s in columns
6, 8, 15, 16, and 17, etc.
# The remaining entries should be zeros.

result <- matrix(rep(0, nrow(x) * k), nrow = nrow(x))
colnames(result) <- paste0("item", 1:k)

# I can see how to do it by looping through rows of result.
# But I need to do it fast (not using a loop).
# I feel like I should subset 'result' with 'x', but I am not sure how.

Thank you very much!


From marc_schwartz at me.com  Wed Dec 23 00:03:05 2015
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 22 Dec 2015 17:03:05 -0600
Subject: [R] Expanding matrix into dummies
In-Reply-To: <CAN2xGJbK+539fR4Qkm2MfE3pYJ7QPnDBmRKom=9fDc-=UTYSMw@mail.gmail.com>
References: <CAN2xGJbK+539fR4Qkm2MfE3pYJ7QPnDBmRKom=9fDc-=UTYSMw@mail.gmail.com>
Message-ID: <2F6A1DA9-B1EF-4841-A82D-C4CE89603B11@me.com>


> On Dec 22, 2015, at 4:42 PM, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
> 
> # I have a matrix x:
> 
> k <- 20
> N <- 5
> set.seed(123)
> x <- matrix(c(sample(1:k, N, replace = F),
>              sample(1:k, N, replace = F),
>              sample(1:k, N, replace = F),
>              sample(1:k, N, replace = F),
>              sample(1:k, N, replace = F),
>              sample(1:k, N, replace = F)), byrow = T, ncol = 5)
> colnames(x) <- paste0("column", 1:5)
> (x)
> 
> # I want to reshape it into a matrix 'result' with k columns (not N).
> # 'result' should contain the same number of rows as x, and it should have
> # in each row 1s in those columns that correspond to the entries in x
> in the same row.
> # For example, the first row of 'result' should contain 1s in columns
> 6, 8, 15, 16, and 17, etc.
> # The remaining entries should be zeros.
> 
> result <- matrix(rep(0, nrow(x) * k), nrow = nrow(x))
> colnames(result) <- paste0("item", 1:k)
> 
> # I can see how to do it by looping through rows of result.
> # But I need to do it fast (not using a loop).
> # I feel like I should subset 'result' with 'x', but I am not sure how.
> 
> Thank you very much!


Something like this might work for you:

result <- matrix(0, nrow(x), k)


> result
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]    0    0    0    0    0    0    0    0    0     0     0     0
[2,]    0    0    0    0    0    0    0    0    0     0     0     0
[3,]    0    0    0    0    0    0    0    0    0     0     0     0
[4,]    0    0    0    0    0    0    0    0    0     0     0     0
[5,]    0    0    0    0    0    0    0    0    0     0     0     0
[6,]    0    0    0    0    0    0    0    0    0     0     0     0
     [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
[1,]     0     0     0     0     0     0     0     0
[2,]     0     0     0     0     0     0     0     0
[3,]     0     0     0     0     0     0     0     0
[4,]     0     0     0     0     0     0     0     0
[5,]     0     0     0     0     0     0     0     0
[6,]     0     0     0     0     0     0     0     0


result[cbind(as.vector(row(x)), as.vector(x))] <- 1


> result
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]    0    0    0    0    0    1    0    1    0     0     0     0
[2,]    1    0    0    0    0    0    0    1    0     1     1     0
[3,]    0    1    0    0    0    0    0    0    1     1     0     0
[4,]    1    0    0    0    1    1    0    0    0     0     0     0
[5,]    0    0    0    0    0    0    0    0    0     0     1     1
[6,]    0    0    1    0    1    0    0    0    0     0     1     0
     [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
[1,]     0     0     1     1     1     0     0     0
[2,]     0     0     0     0     1     0     0     0
[3,]     1     0     0     0     0     0     0     1
[4,]     0     0     0     1     0     1     0     0
[5,]     0     1     0     0     1     1     0     0
[6,]     0     0     1     0     0     0     1     0


See ?row 

Regards,

Marc Schwartz


From bgunter.4567 at gmail.com  Wed Dec 23 01:14:49 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Tue, 22 Dec 2015 16:14:49 -0800
Subject: [R] Expanding matrix into dummies
In-Reply-To: <2F6A1DA9-B1EF-4841-A82D-C4CE89603B11@me.com>
References: <CAN2xGJbK+539fR4Qkm2MfE3pYJ7QPnDBmRKom=9fDc-=UTYSMw@mail.gmail.com>
	<2F6A1DA9-B1EF-4841-A82D-C4CE89603B11@me.com>
Message-ID: <CAGxFJbQXQJ8Rfi48VkyStST4UJFOh1T0F4R=E932od4RjsTJjA@mail.gmail.com>

... Perhaps worth noting is that the row indices can be created
directly without row():

result[cbind(rep.int(seq_len(6),5), as.vector(x))] <- 1

but the downside is that you have to know that a matrix is a vector
"stored" in column major order. I find this arcane detail quite handy,
though.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 22, 2015 at 3:03 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>
>> On Dec 22, 2015, at 4:42 PM, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
>>
>> # I have a matrix x:
>>
>> k <- 20
>> N <- 5
>> set.seed(123)
>> x <- matrix(c(sample(1:k, N, replace = F),
>>              sample(1:k, N, replace = F),
>>              sample(1:k, N, replace = F),
>>              sample(1:k, N, replace = F),
>>              sample(1:k, N, replace = F),
>>              sample(1:k, N, replace = F)), byrow = T, ncol = 5)
>> colnames(x) <- paste0("column", 1:5)
>> (x)
>>
>> # I want to reshape it into a matrix 'result' with k columns (not N).
>> # 'result' should contain the same number of rows as x, and it should have
>> # in each row 1s in those columns that correspond to the entries in x
>> in the same row.
>> # For example, the first row of 'result' should contain 1s in columns
>> 6, 8, 15, 16, and 17, etc.
>> # The remaining entries should be zeros.
>>
>> result <- matrix(rep(0, nrow(x) * k), nrow = nrow(x))
>> colnames(result) <- paste0("item", 1:k)
>>
>> # I can see how to do it by looping through rows of result.
>> # But I need to do it fast (not using a loop).
>> # I feel like I should subset 'result' with 'x', but I am not sure how.
>>
>> Thank you very much!
>
>
> Something like this might work for you:
>
> result <- matrix(0, nrow(x), k)
>
>
>> result
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,]    0    0    0    0    0    0    0    0    0     0     0     0
> [2,]    0    0    0    0    0    0    0    0    0     0     0     0
> [3,]    0    0    0    0    0    0    0    0    0     0     0     0
> [4,]    0    0    0    0    0    0    0    0    0     0     0     0
> [5,]    0    0    0    0    0    0    0    0    0     0     0     0
> [6,]    0    0    0    0    0    0    0    0    0     0     0     0
>      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
> [1,]     0     0     0     0     0     0     0     0
> [2,]     0     0     0     0     0     0     0     0
> [3,]     0     0     0     0     0     0     0     0
> [4,]     0     0     0     0     0     0     0     0
> [5,]     0     0     0     0     0     0     0     0
> [6,]     0     0     0     0     0     0     0     0
>
>
> result[cbind(as.vector(row(x)), as.vector(x))] <- 1
>
>
>> result
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
> [1,]    0    0    0    0    0    1    0    1    0     0     0     0
> [2,]    1    0    0    0    0    0    0    1    0     1     1     0
> [3,]    0    1    0    0    0    0    0    0    1     1     0     0
> [4,]    1    0    0    0    1    1    0    0    0     0     0     0
> [5,]    0    0    0    0    0    0    0    0    0     0     1     1
> [6,]    0    0    1    0    1    0    0    0    0     0     1     0
>      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
> [1,]     0     0     1     1     1     0     0     0
> [2,]     0     0     0     0     1     0     0     0
> [3,]     1     0     0     0     0     0     0     1
> [4,]     0     0     0     1     0     1     0     0
> [5,]     0     1     0     0     1     1     0     0
> [6,]     0     0     1     0     0     0     1     0
>
>
> See ?row
>
> Regards,
>
> Marc Schwartz
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jdnewmil at dcn.davis.ca.us  Wed Dec 23 05:24:12 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 22 Dec 2015 20:24:12 -0800
Subject: [R] colour input in ribbon plots (geom ribbon)
In-Reply-To: <567a160f.8b10620a.4dc64.ffffaafd@mx.google.com>
References: <AC6186D424C.00000480jrkrideau@inbox.com>
	<1445AB3A-9786-45C8-9243-F5E84AE7CD9D@dcn.davis.ca.us>
	<567a160f.8b10620a.4dc64.ffffaafd@mx.google.com>
Message-ID: <EFD75E9F-2FCD-4473-AB93-D7D16C73FAAE@dcn.davis.ca.us>

Please keep the discussion on the mailing list by using reply-to-all.

That function makes no line representing "mean". If you want a line in the middle then you need to add it yourself. The color attribute for geom_ribbon affects the outline of that ribbon area. 
-- 
Sent from my phone. Please excuse my brevity.

On December 22, 2015 7:33:33 PM PST, pushan chakraborty <pushan.zoology at gmail.com> wrote:
>Is it possible to change the colour of the mean line in geom ribbon?
>
>-----Original Message-----
>From: "Jeff Newmiller" <jdnewmil at dcn.davis.ca.us>
>Sent: ?23-?12-?2015 01:43
>To: "John Kane" <jrkrideau at inbox.com>; "pushan chakraborty"
><pushan.zoology at gmail.com>; "r-help at r-project.org"
><r-help at r-project.org>
>Subject: Re: [R] colour input in ribbon plots (geom ribbon)
>
>Um, NOT in the aesthetic... as a direct argument to geom_ribbon.
>
>But yes, examples are critical for communication about R questions, and
>posting on plain text format is critical for getting the example
>through the list server undamaged. 
>-- 
>Sent from my phone. Please excuse my brevity.
>
>
>On December 22, 2015 11:50:31 AM PST, John Kane <jrkrideau at inbox.com>
>wrote:
>It is not clear what you are doing. Please provide some code and data
>if possible. Otherwise adding something like colour = "red" in the
>aes() should do something. John KaneKingston ON Canada -----Original
>Message----- From: pushan.zoology at gmail.com Sent: Tue, 22 Dec 2015
>14:26:03 +0530 To: r-help at r-project.org Subject: [R] colour input in
>ribbon plots (geom ribbon)  Hallow everybody I want to know if there is
>any command for input colour in the ribbon plot (geom ribbon) of
>ggplot2. Unless that, the mean line and the standard error can not be
>distinguished. Please help me if anyone have any idea.  -- Pushan
>Chakraborty CSIR - SRF  Center for Pollination Studies, University of
>Calcutta 35,
>Ballyguanje Circular Road, Kolkata - 700019           & Wildlife
>Institute of India Chandrabani, Dehradun - 248001  webpage: 
>http://cpscu.in/?page_id=51  Skype: cpushan   [[alternative HTML
>version deleted]]  R-help at r-project.org mailing list -- To UNSUBSCRIBE
>and more, see https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do
>read the posting guide http://www.R-project.org/posting-guide.html and
>provide commented, minimal, self-contained, reproducible code.FREE 3D
>EARTH SCREENSAVER - Watch the Earth right on your
>desktop!R-help at r-project.org mailing list -- To UNSUBSCRIBE and more,
>seehttps://stat.ethz.ch/mailman/listinfo/r-helpPLEASE do read the
>posting guide http://www.R-project.org/posting-guide.htmland provide
>commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From pushan.zoology at gmail.com  Wed Dec 23 05:48:54 2015
From: pushan.zoology at gmail.com (pushan chakraborty)
Date: Wed, 23 Dec 2015 10:18:54 +0530
Subject: [R] Colour input in geom line
Message-ID: <567a27b8.824f620a.d9cc8.ffffa640@mx.google.com>

Thanks for the help in my previous quarry, I want to know is it possible to change the colour in geom line?
	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Wed Dec 23 06:36:55 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Tue, 22 Dec 2015 21:36:55 -0800
Subject: [R] Colour input in geom line
In-Reply-To: <567a27b8.824f620a.d9cc8.ffffa640@mx.google.com>
References: <567a27b8.824f620a.d9cc8.ffffa640@mx.google.com>
Message-ID: <D80CE395-2099-428F-BCF3-1CE9C5D9550A@dcn.davis.ca.us>

Yes.

This forum works best when you provide example code.  For questions like this you really should use a web search engine. Google comes up with many results,  including [1].

[1] http://www.ucl.ac.uk/~zctpep9/Archived%20webpages/Cookbook%20for%20R%20%C2%BB%20Colors%20(ggplot2).htm
-- 
Sent from my phone. Please excuse my brevity.

On December 22, 2015 8:48:54 PM PST, pushan chakraborty <pushan.zoology at gmail.com> wrote:
>Thanks for the help in my previous quarry, I want to know is it
>possible to change the colour in geom line?
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From manishm at dbs.com  Wed Dec 23 07:48:11 2015
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Wed, 23 Dec 2015 06:48:11 +0000
Subject: [R] Caret - Recursive Feature Elimination Error
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08D8A452@W01GMAILDAGA52.reg1.1bank.dbs.com>

Hi,

I am trying to use caret, for feature selection on glmnet. I get a strange error like below - "arguments imply differing number of rows: 2, 3".


x <- data.matrix(train[,features])

y <- train$quoteconversion_flag



> str(x)

 num [1:260753, 1:297] NA NA NA NA NA NA NA NA NA NA ...

 - attr(*, "dimnames")=List of 2

  ..$ : NULL

  ..$ : chr [1:297] "original_quote_date" "field6" "field7" "field8" ...

> str(y)

 Factor w/ 2 levels "X0","X1": 1 1 1 1 1 1 1 1 1 1 ...

> RFE <- rfe(x,y,sizes = seq(50,300,by=10),
+            metric = "ROC",maximize=TRUE,rfeControl = MyRFEcontrol,
+            method='glmnet',
+            tuneGrid = expand.grid(.alpha=0,.lambda=c(0.01,0.02)),
+            trControl = MyTrainControl)
+(rfe) fit Resample01 size: 297
+(rfe) fit Resample02 size: 297
+(rfe) fit Resample03 size: 297
+(rfe) fit Resample04 size: 297
+(rfe) fit Resample05 size: 297
+(rfe) fit Resample06 size: 297
+(rfe) fit Resample07 size: 297
+(rfe) fit Resample08 size: 297
+(rfe) fit Resample09 size: 297
+(rfe) fit Resample10 size: 297
+(rfe) fit Resample11 size: 297
+(rfe) fit Resample12 size: 297
+(rfe) fit Resample13 size: 297
+(rfe) fit Resample14 size: 297
+(rfe) fit Resample15 size: 297
+(rfe) fit Resample16 size: 297
+(rfe) fit Resample17 size: 297
+(rfe) fit Resample18 size: 297
+(rfe) fit Resample19 size: 297
+(rfe) fit Resample20 size: 297
+(rfe) fit Resample21 size: 297
+(rfe) fit Resample22 size: 297
+(rfe) fit Resample23 size: 297
+(rfe) fit Resample24 size: 297
+(rfe) fit Resample25 size: 297
Error in { :
  task 1 failed - "task 1 failed - "arguments imply differing number of rows: 2, 3""
In addition: There were 50 or more warnings (use warnings() to see the first 50)

Any idea what does this mean?

Thanks,
Manish

CONFIDENTIAL NOTE:
The information contained in this email is intended only...{{dropped:11}}


From pushan.zoology at gmail.com  Wed Dec 23 08:43:23 2015
From: pushan.zoology at gmail.com (pushan chakraborty)
Date: Wed, 23 Dec 2015 13:13:23 +0530
Subject: [R] colour input in geom line ggplote2
Message-ID: <CAAATBeDy=AAz=m970D0YN34QxA7eZrpT2ZwL-bjX7gSwFEthZg@mail.gmail.com>

Dear list

I want to put a line command in the following and also want to attribute
colour in the geom line command line. It would be a ribbon plot showing the
mean line. Is there any help?


p <- ggplot(mtcars, aes(x=carb, y=mpg)) + geom_point()

p + stat_summary(geom="ribbon", fun.ymin="min", fun.ymax="max")

p + stat_summary(geom="ribbon", fun.ymin="min", fun.ymax="max",
        alpha=0.2, fill="blue")





-- 
Pushan Chakraborty
CSIR - SRF

Center for Pollination Studies, University of Calcutta
35, Ballyguanje Circular Road, Kolkata - 700019
          &
Wildlife Institute of India
Chandrabani, Dehradun - 248001

webpage:

http://cpscu.in/?page_id=51

Skype: cpushan

	[[alternative HTML version deleted]]


From jwd at surewest.net  Wed Dec 23 08:56:21 2015
From: jwd at surewest.net (jwd)
Date: Tue, 22 Dec 2015 23:56:21 -0800
Subject: [R] Regular R freezes
In-Reply-To: <56796883.1050709@uni-bremen.de>
References: <56796883.1050709@uni-bremen.de>
Message-ID: <20151222235621.1832af3d@Draco.site>

On Tue, 22 Dec 2015 16:13:07 +0100
Tim Richter-Heitmann <trichter at uni-bremen.de> wrote:

> Dear List,
> 
> some days ago my R started to regularily freezes, especially during 
> these operations:
> 
> mso(,permutations=999), package vegan
> anova.cca(), package vegan
> forward.sel(), package packfor
> 
> I am running R on a 64-bit Windows 7 installation, 4 cores, 8 GB RAM.
> I just today updated R to 3.2.3, all packages, and R-Studio to their 
> most recent versions.
> Still the error persists.
> Two things are strange:
> 
> All these functions (and more) have been working without any problem
> for 2 years.
> The freezes occur at different points in the algorithms. For 
> forward.sel(), a stepwise variable selection process
> in species distribution modelling, the system fails at different
> steps (say after variable 8 or 18), but inevitably fails.

Look at how much memory is available.  Possibly increase the work space
available to R.  What other software was recently changed immediately
prior to the appearance of the problem.  Lastly, check that hardware it
self.  Most hard drives have diagnostic routines that can be accessed
to check drive integrity.  

The random nature to the fail suggests a memory availability problem -
some newly resident routine that uses more memory than it should.  Run
the Task Manager in monitor memory availability before, and at
failure.  But, I once chased a similar problem all the way to a failing
CPU before fixing it. I essentially rebuilt the computer.  


> R-Studio is still responsive meanwhile, but the "Stop"-Buttom does
> not work, and the programm needs to be restarted.
> 
> Is there any way of troubleshooting or finding out who the culprit is?
> 
> Thanks.
> 

JWDougherty


From matteo.richiardi at gmail.com  Wed Dec 23 09:44:35 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Wed, 23 Dec 2015 08:44:35 +0000
Subject: [R] assigning values to elements of matrixes
Message-ID: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>

I am following the example I find on ?assign:

a <- 1:4
assign("a[1]", 2)

This appears to create a new variable named "a[1]" rather than
changing the value of the vector.

Am I missing something here? How can I assign a value to a specified
element of a vector/matrix?

Of course, my problem is slightly more involved, but I guess the above
is its core. For those interested, I have the two matrixes
M_a <- matrix(0,10,10)
M_b <- matrix(0,10,10)

I want to have a function that assigns a value to a specific element
of one of the matrix, as in
foo <- function(s,i,j,value){
  assign(paste("M_",s)[i,j],value)
}

This however does not work:

foo('a',1,1,1)

Error in paste("M_", s)[1, j] : incorrect number of dimensions

Following the ?assign help, I tried

foo2 <- function(s,i,j,value){
  assign(paste("M_",s,"[i,j]"),value, envir = .GlobalEnv)
}

but this produces the problem I described above (namely, a new
variable is created, rather than replacing the specified element of
the matrix).

I know that in this example I could simply pass the matrix as an
argument to the function, but I am interested in understanding how
'assign' work.

Many thanks for your help.

Matteo


From michael.eisenring at agroscope.admin.ch  Wed Dec 23 10:07:47 2015
From: michael.eisenring at agroscope.admin.ch (michael.eisenring at agroscope.admin.ch)
Date: Wed, 23 Dec 2015 09:07:47 +0000
Subject: [R] How to conduct a PERMANOVA using a dissimilarity matrix
In-Reply-To: <AM3PR01MB1282E8CEC7DA5AED7067F7D2CEE50@AM3PR01MB1282.eurprd01.prod.exchangelabs.com>
References: <6D5C009FB51EBD41BEE57F4C002350FD05D657@SB00112A.adb.intra.admin.ch>
	<AM3PR01MB1282E8CEC7DA5AED7067F7D2CEE50@AM3PR01MB1282.eurprd01.prod.exchangelabs.com>
Message-ID: <6D5C009FB51EBD41BEE57F4C002350FD05D6A2@SB00112A.adb.intra.admin.ch>

Dear Erica and Hazel,
Thank you very much for your help.
Based on Ericas feedback I use now a the raw data giving me information on abundance of bat species (I changed the data set) that were caught at 6 different sites (file named bats)

Thanks to Hazels input I use now an additional file (named forest_type) assigning a forest type (O or Y) to each site.
Using Adonis I would like to convert my bats in a first step to a soerenson (dis)similarity matrix and then use the Adonis function to see how bat communities differ between forest types.

With the help of you I was now able to write some code
The code seems to work. However, I have 2 more (basic) questions.

1) Do I have to convert my data sets in a specific way (I structured it as in the R help example using the dune data)? I have the problem that R thinks the first row (the very left one) with the header "Site" is just another species of bat ( if you open the bat file in R and scroll to the top you see what I mean, "Site" is next to "Kerivoula...." and it is treated just like another bat species). How can I solve the problem?

2)Is the code correct? I am especially unsure if my conversion to a dissimilarity matrix worked, since the results are exactly the same , regardless if I add method="bray" or not to my code.

Below is the code and the 2 data sets (dput)



#load data sets
#file with abundance information of each caught bat at each site
bats<-read.csv("bats_abundance.csv",sep=";",header=T)
#file with forest type (O,Y) information
forest_type<-read.csv("forest_type_6.csv",sep=";")
attach(bats)
attach(forest_type)
bats
forest_type

#Aim: compare how bat communities differ between O and Y forests
#How can I convert bats first to soerenson dissimilarity matrix ()
adonis(bats~forest, data=forest_type,method="bray")


> dput(bats)
structure(c(1L, 2L, 3L, 4L, 5L, 6L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 
0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L), .Dim = c(6L, 14L), .Dimnames = list(
    NULL, c("Site", "Kerivoula_smithii", "Scotophilus_nigrita", 
    "Minipoterus_natalensis", "Miniopterus_fraterculus", "Neoromicia_nana", 
    "Otomops_martiensseni", "Chaerephon_pumilus", "Rhinolophus_clivosus", 
    "Hipposideros_caffer", "Lissonycteris_angolensis", "Pipistrellus_hesperidus", 
    "Pipistrellus_nanulus", "Rhinolophus_fumigatus")))


> dput(forest_type)
structure(list(Site = 1:6, forest = structure(c(1L, 2L, 2L, 2L, 
1L, 1L), .Label = c("O", "Y"), class = "factor")), .Names = c("Site", 
"forest"), class = "data.frame", row.names = c(NA, -6L))





Thanks a lot,
Mike















Eisenring Michael, Msc.
PhD Student

Federal Department of Economic Affairs, Education and Research
EAER
Institute of Sustainability Sciences ISS
Biosafety

Reckenholzstrasse 191, CH-8046 Z?rich
Tel. +41 44 37 77181
Fax +41 44 37 77201
michael.eisenring at agroscope.admin.ch
www.agroscope.ch

-----Urspr?ngliche Nachricht-----
Von: Hazel Knipe [mailto:hazel_knipe at hotmail.co.uk] 
Gesendet: Dienstag, 22. Dezember 2015 18:34
An: Eisenring Michael Agroscope <michael.eisenring at agroscope.admin.ch>
Betreff: Re: How to conduct a PERMANOVA using a dissimilarity matrix

Dear Michael, 

You need to create another file dividing the different categories/ variables (make sure that they are in the same order for the variable you are testing), then attach it and run the adonis function. For example:

attach(metadata) 

adonis(formula = vegdist(matrixfile, method = "bray") ~ Site, data = metadata) 

It's also important to think about if you have nestedness in your design, in which case you would need to add strata = factor: 

adonis(formula = vegdist(matrixfile, method = "bray") ~ Species, data = metadata,      strata = Site) 

I hope this helps for now! Let me know if you need more detail. 

Hazel (BSc student). 



________________________________________
From: R-help <r-help-bounces at r-project.org> on behalf of michael.eisenring at agroscope.admin.ch <michael.eisenring at agroscope.admin.ch>
Sent: 22 December 2015 16:18
To: r-help at r-project.org
Subject: [R] How to conduct a PERMANOVA using a dissimilarity matrix

Dear R-List members,

I have to compare how similar two types of forest (old growth=O) and (young forest=Y) in terms of moth communities are.
I sampled moths at 4 O and 4 Y sites.
I need to analyse the data using a PERMANOVA approach. But I am having a really hard time to do this in R.

I found out that I need to create a dissimilarity matrix and read this matrix then into R to conduct a one-way Permanova with forest type (O or Y) as factor.
The package vegan with the function "adonis" seems to be able to do a permanova.

I created the matrix (based on Soerenson (dis)similarities) and imported it into R.

Could anyone help me with the next step? How can I conduct a permanova on my dataset? In the end I would need an R value and significance level telling me if community compositions differ significantly between sites.

Below is my code (not too much) and the data for the matrix.

#dput for matrix:

structure(c("", "O", "Y", "Y", "Y", "O", "O", "Y", "O", "O", "0", "0.544", "0.519", "0.533", "0.481", "0.548", "0.518", "0.479", "Y", "0.544", "0", "0.383", "0.416", "0.383", "0.358", "0.434", "0.399", "Y", "0.519", "0.383", "0", "0.398", "0.359", "0.392", "0.401", "0.374", "Y", "0.533", "0.416", "0.398", "0", "0.398", "0.399", "0.358", "0.348", "O", "0.481", "0.383", "0.359", "0.398", "0", "0.37", "0.317", "0.354", "O", "0.548", "0.358", "0.392", "0.399", "0.37", "0", "0.39", "0.365", "Y", "0.518", "0.434", "0.401", "0.358", "0.317", "0.39", "0", "0.371", "O", "0.479", "0.399", "0.374", "0.348", "0.354", "0.365", "0.371", "0"), .Dim = c(9L, 9L), .Dimnames = list(NULL, c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9")))


#Code
#load dissimilarity matrix (based on Soerenson similarity) moth_dta<-read.csv("Geo_sorenson_8.csv",header=T,sep=";")#Creates matrix from imported data
moth_dta<-as.matrix(moth_dta)
moth_dta
library(vegan)


Thank you very much,
Michael

Eisenring Michael, Msc.
PhD Student

Federal Department of Economic Affairs, Education and Research EAER Institute of Sustainability Sciences ISS Biosafety

Reckenholzstrasse 191, CH-8046 Z rich
Tel. +41 44 37 77181
Fax +41 44 37 77201
michael.eisenring at agroscope.admin.ch<mailto:michael.eisenring at agroscope.admin.ch>
www.agroscope.ch<http://www.agroscope.ch/>


        [[alternative HTML version deleted]]


From soni.archit1989 at gmail.com  Wed Dec 23 11:46:29 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Wed, 23 Dec 2015 04:46:29 -0600
Subject: [R] Fwd: Trycatch in R
In-Reply-To: <CAJ7HxBzE7uf4_iwb6Ky-FBV8pi=ktJqjKitNG9fFD3bwKwN9aQ@mail.gmail.com>
References: <CAJ7HxBzE7uf4_iwb6Ky-FBV8pi=ktJqjKitNG9fFD3bwKwN9aQ@mail.gmail.com>
Message-ID: <CAJ7HxBzif8H2o4WdxhH8Fx2Wspf3MPzWjk7ybJZjoF2bNotJ7g@mail.gmail.com>

Hi All,

I didn't receive a mail from moderator that my post is pending for review,
hence sending once again.

Please suggest, I got answer from Stack overflow but somehow couldn't get
it implemented, if anyone of you can suggest how try catch can be
implemented in this case.

Thanks again!

Best Regards,
Archit
---------- Forwarded message ----------
From: Archit Soni <soni.archit1989 at gmail.com>
Date: Tue, Dec 22, 2015 at 6:34 AM
Subject: Trycatch in R
To: r-help at r-project.org


Hi All,

Please help me in the below code:

tryCatch({
 x <- capture.output(XML::xmlParse(y))
}, warning = function(w) {
  x <- capture.output(w)
}, error = function(e) {
  x <-capture.output(e)
}, finally = {

})

I am using Tibco Enterprise Runtime for R to get value of the parsed XML in
vector format, and spotfire is displaying each line as a row in new table,
but can you please suggest how to do error handling in this case when no
row is selected and we run this script, it is throwing error for length of
argument is 0, i need to display an error message as first row of the table.

Please suggest.

-- 
Regards
Archit



-- 
Regards
Archit

	[[alternative HTML version deleted]]


From soni.archit1989 at gmail.com  Wed Dec 23 11:48:51 2015
From: soni.archit1989 at gmail.com (Archit Soni)
Date: Wed, 23 Dec 2015 04:48:51 -0600
Subject: [R] TIBCO Enterprise Runtime for R
In-Reply-To: <CAF8bMcYwQXthO_FcLWHBJ0t_88y4du+nEoayRbJihN=OSKGciQ@mail.gmail.com>
References: <CAJ7HxBywpZJZLJ7T6ft+fzpDqM4D9WwnEtxfYQJ_2Vug_72HbQ@mail.gmail.com>
	<CAF8bMcYRssgx=K6cw-OT2J9QXq37UWn5dmLxGork10w9Omg=Kg@mail.gmail.com>
	<CAJ7HxBwLhz_kTU356hAE4cyJ0H=+FGU-jHVYxAw7Ag+pyQHPRQ@mail.gmail.com>
	<CAF8bMcaZnhGaaEPzsUJPJo7sBOdNh77aAL_6bCQEW=MuMmxRCw@mail.gmail.com>
	<CAJ7HxBy_nBiKaByKsB0rbKHUwNHOo6BLiKH3wiXqw3tQYbW65g@mail.gmail.com>
	<CAF8bMcYwQXthO_FcLWHBJ0t_88y4du+nEoayRbJihN=OSKGciQ@mail.gmail.com>
Message-ID: <CAJ7HxBytZuTMAbqHuW-Uz0sEtwOkWF2pDaWYo0WjN7nS2HCS+A@mail.gmail.com>

Thanks a lot William!, worked like a charm when I mapped output to table.
Better than nothing i suppose, however I am stuck at error handling now for
the case if no rows are selected the table should be empty and when
analysis is opened the table should not create any table till "create
XML(my custom button)" is clicked.

On Wed, Dec 16, 2015 at 11:33 AM, William Dunlap <wdunlap at tibco.com> wrote:

> You can use capture.output(xmlParseOutput) to make a character vector with
> one string per line of printed R output and send that back to Spotfire
> for display.
>
> You will need to consult with Spotfire experts (at TIBCO support or
> community.tibco.com) to figure out the best way to display this in
> Spotfire.  You may have to embed it
> in html to get the formatting right.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>
> On Wed, Dec 16, 2015 at 9:12 AM, Archit Soni <soni.archit1989 at gmail.com>
> wrote:
> > Thanks for the reply William, I wish to print the XML tree format in a
> text
> > area.
> > Can we do that by some way ? or is there a way to achieve that in
> > IronPython, i am open for both options :)
> >
> > Thanks again for reply.
> >
> > On Wed, Dec 16, 2015 at 9:26 PM, William Dunlap <wdunlap at tibco.com>
> wrote:
> >>
> >> Your code works in TERR under Spotfire, but since Spotfire deals with
> >> rectangular data sets the glue code between them puts your objects in
> >> a data.frame, which is not legal.
> >>
> >> What do you hope to do with the XMLInteralDocument object in Spotfire?
> >>  Such objects depend on R internal pointers and don't even survive a
> >> save/load cycle in R.  You probably want to a column of data, numeric
> >> or character, from it and send that back to Spotfire.
> >>
> >> Bill Dunlap
> >> TIBCO Software
> >> wdunlap tibco.com
> >>
> >>
> >> On Wed, Dec 16, 2015 at 12:35 AM, Archit Soni <
> soni.archit1989 at gmail.com>
> >> wrote:
> >> > Yes William i'll see if i can get any help from TIBCommunity, but my
> >> > code
> >> > worked in RStudio.
> >> >
> >> > On Tue, Dec 15, 2015 at 4:41 PM, William Dunlap <wdunlap at tibco.com>
> >> > wrote:
> >> >>
> >> >> It looks like you are calling TERR from Spotfire.  The Spotfire/TERR
> >> >> interface
> >> >> can only pass TERR data.frames (eq. to Spotfire tables) back to
> >> >> Spotfire
> >> >> and
> >> >> XMLInternalDocuments cannot be columns of data.frames (in neither
> TERR
> >> >> nor
> >> >> R).
> >> >>
> >> >> You should contact TIBCO support and/or participate in the forums at
> >> >> community.tibco.com to see how to solve your problem.
> >> >>
> >> >>
> >> >> Bill Dunlap
> >> >> TIBCO Software
> >> >> wdunlap tibco.com
> >> >>
> >> >>
> >> >> On Tue, Dec 15, 2015 at 5:09 AM, Archit Soni
> >> >> <soni.archit1989 at gmail.com>
> >> >> wrote:
> >> >> > Hi All,
> >> >> >
> >> >> > I have the code to print XML tree that is working successfully in R
> >> >> > Studio
> >> >> > but is failing when i try to work it out with TERR:
> >> >> >
> >> >> > x<- XML::xmlParse(y)
> >> >> >
> >> >> > y is input (Coming from a row only once)
> >> >> > x is output
> >> >> >
> >> >> > The above code is working in R studio but doesnt work in TERR,
> please
> >> >> > suggest.
> >> >> >
> >> >> > TIBCO Enterprise Runtime for R returned an error: 'Error in
> >> >> > as.data.frame.default(passed.args[[i]], stringsAsFactors = s :
> cannot
> >> >> > coerce class '"XMLInternalDocumentXMLAbstractDocument"' into a
> >> >> > data.frame'.
> >> >> > at
> >> >> >
> >> >> >
> >> >> >
> Spotfire.Dxp.Data.DataFunctions.Executors.LocalFunctionClient.OnExecuting(FunctionClient
> >> >> > funcClient) at
> >> >> >
> >> >> >
> >> >> >
> Spotfire.Dxp.Data.DataFunctions.Executors.AbstractFunctionClient.d__0.MoveNext()
> >> >> > at
> >> >> >
> >> >> >
> >> >> >
> Spotfire.Dxp.Data.DataFunctions.Executors.SPlusFunctionExecutor.d__0.MoveNext()
> >> >> > at
> >> >> >
> >> >> >
> >> >> >
> Spotfire.Dxp.Data.DataFunctions.DataFunctionExecutorService.d__6.MoveNext()
> >> >> >
> >> >> > --
> >> >> > Regards
> >> >> > Archit
> >> >> >
> >> >> >         [[alternative HTML version deleted]]
> >> >> >
> >> >> > ______________________________________________
> >> >> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> >> > https://stat.ethz.ch/mailman/listinfo/r-help
> >> >> > PLEASE do read the posting guide
> >> >> > http://www.R-project.org/posting-guide.html
> >> >> > and provide commented, minimal, self-contained, reproducible code.
> >> >
> >> >
> >> >
> >> >
> >> > --
> >> > Regards
> >> > Archit
> >
> >
> >
> >
> > --
> > Regards
> > Archit
>



-- 
Regards
Archit

	[[alternative HTML version deleted]]


From trichter at uni-bremen.de  Wed Dec 23 11:59:44 2015
From: trichter at uni-bremen.de (Tim Richter-Heitmann)
Date: Wed, 23 Dec 2015 11:59:44 +0100
Subject: [R] How to conduct a PERMANOVA using a dissimilarity matrix
In-Reply-To: <6D5C009FB51EBD41BEE57F4C002350FD05D6A2@SB00112A.adb.intra.admin.ch>
References: <6D5C009FB51EBD41BEE57F4C002350FD05D657@SB00112A.adb.intra.admin.ch>
	<AM3PR01MB1282E8CEC7DA5AED7067F7D2CEE50@AM3PR01MB1282.eurprd01.prod.exchangelabs.com>
	<6D5C009FB51EBD41BEE57F4C002350FD05D6A2@SB00112A.adb.intra.admin.ch>
Message-ID: <567A7EA0.40009@uni-bremen.de>

Hi!

R indeed cannot distinguish between your Site and Species columns. You 
either need to state "row.names=1" while loading your csv-files into 
workspace or you can rearrange your dataframe like this:

|bats2 <-bats[,-1]rownames(bats2)<-bats[,1] Alternatively, you can tell R 
which columns of your dataframes should be excluded in the adonis call. 
About your other questions: You may want to look into ?vegdist to see 
the different methods adonis() accepts. I think Soerensen is the 
presence/absence version of Bray-Curtis. Jaccard is another simple index 
for occurence data, and can be computed by vegdist. Since you are doing 
post hoc hypothesis testing, you might want to state a number of 
permutations within the adonis call. I think (you should update yourself 
on this) that the permutations also removes possible autocorrelation 
from the dataset, which would be possibly convenient in your study 
question. For starters, i suggest reading the many vegan tutorials 
available or - if you can get your hands on it - the Numerical Ecology 
textbook by Legendre & Legendre. I did so, and it served me quite well. 
Also, tutorials on basic data handling and navigation in R would come in 
quite handily, i guess. Cheers|,


On 23.12.2015 10:07, michael.eisenring at agroscope.admin.ch wrote:
> Dear Erica and Hazel,
> Thank you very much for your help.
> Based on Ericas feedback I use now a the raw data giving me information on abundance of bat species (I changed the data set) that were caught at 6 different sites (file named bats)
>
> Thanks to Hazels input I use now an additional file (named forest_type) assigning a forest type (O or Y) to each site.
> Using Adonis I would like to convert my bats in a first step to a soerenson (dis)similarity matrix and then use the Adonis function to see how bat communities differ between forest types.
>
> With the help of you I was now able to write some code
> The code seems to work. However, I have 2 more (basic) questions.
>
> 1) Do I have to convert my data sets in a specific way (I structured it as in the R help example using the dune data)? I have the problem that R thinks the first row (the very left one) with the header "Site" is just another species of bat ( if you open the bat file in R and scroll to the top you see what I mean, "Site" is next to "Kerivoula...." and it is treated just like another bat species). How can I solve the problem?
>
> 2)Is the code correct? I am especially unsure if my conversion to a dissimilarity matrix worked, since the results are exactly the same , regardless if I add method="bray" or not to my code.
>
> Below is the code and the 2 data sets (dput)
>
>
>
> #load data sets
> #file with abundance information of each caught bat at each site
> bats<-read.csv("bats_abundance.csv",sep=";",header=T)
> #file with forest type (O,Y) information
> forest_type<-read.csv("forest_type_6.csv",sep=";")
> attach(bats)
> attach(forest_type)
> bats
> forest_type
>
> #Aim: compare how bat communities differ between O and Y forests
> #How can I convert bats first to soerenson dissimilarity matrix ()
> adonis(bats~forest, data=forest_type,method="bray")
>
>
>> dput(bats)
> structure(c(1L, 2L, 3L, 4L, 5L, 6L, 1L, 1L, 1L, 1L, 1L, 1L, 0L,
> 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,
> 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L,
> 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L,
> 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L,
> 0L, 0L, 1L, 0L, 0L, 0L, 0L), .Dim = c(6L, 14L), .Dimnames = list(
>      NULL, c("Site", "Kerivoula_smithii", "Scotophilus_nigrita",
>      "Minipoterus_natalensis", "Miniopterus_fraterculus", "Neoromicia_nana",
>      "Otomops_martiensseni", "Chaerephon_pumilus", "Rhinolophus_clivosus",
>      "Hipposideros_caffer", "Lissonycteris_angolensis", "Pipistrellus_hesperidus",
>      "Pipistrellus_nanulus", "Rhinolophus_fumigatus")))
>
>
>> dput(forest_type)
> structure(list(Site = 1:6, forest = structure(c(1L, 2L, 2L, 2L,
> 1L, 1L), .Label = c("O", "Y"), class = "factor")), .Names = c("Site",
> "forest"), class = "data.frame", row.names = c(NA, -6L))
>
>
>
>
>
> Thanks a lot,
> Mike
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> Eisenring Michael, Msc.
> PhD Student
>
> Federal Department of Economic Affairs, Education and Research
> EAER
> Institute of Sustainability Sciences ISS
> Biosafety
>
> Reckenholzstrasse 191, CH-8046 Z?rich
> Tel. +41 44 37 77181
> Fax +41 44 37 77201
> michael.eisenring at agroscope.admin.ch
> www.agroscope.ch
>
> -----Urspr?ngliche Nachricht-----
> Von: Hazel Knipe [mailto:hazel_knipe at hotmail.co.uk]
> Gesendet: Dienstag, 22. Dezember 2015 18:34
> An: Eisenring Michael Agroscope <michael.eisenring at agroscope.admin.ch>
> Betreff: Re: How to conduct a PERMANOVA using a dissimilarity matrix
>
> Dear Michael,
>
> You need to create another file dividing the different categories/ variables (make sure that they are in the same order for the variable you are testing), then attach it and run the adonis function. For example:
>
> attach(metadata)
>
> adonis(formula = vegdist(matrixfile, method = "bray") ~ Site, data = metadata)
>
> It's also important to think about if you have nestedness in your design, in which case you would need to add strata = factor:
>
> adonis(formula = vegdist(matrixfile, method = "bray") ~ Species, data = metadata,      strata = Site)
>
> I hope this helps for now! Let me know if you need more detail.
>
> Hazel (BSc student).
>
>
>
> ________________________________________
> From: R-help <r-help-bounces at r-project.org> on behalf of michael.eisenring at agroscope.admin.ch <michael.eisenring at agroscope.admin.ch>
> Sent: 22 December 2015 16:18
> To: r-help at r-project.org
> Subject: [R] How to conduct a PERMANOVA using a dissimilarity matrix
>
> Dear R-List members,
>
> I have to compare how similar two types of forest (old growth=O) and (young forest=Y) in terms of moth communities are.
> I sampled moths at 4 O and 4 Y sites.
> I need to analyse the data using a PERMANOVA approach. But I am having a really hard time to do this in R.
>
> I found out that I need to create a dissimilarity matrix and read this matrix then into R to conduct a one-way Permanova with forest type (O or Y) as factor.
> The package vegan with the function "adonis" seems to be able to do a permanova.
>
> I created the matrix (based on Soerenson (dis)similarities) and imported it into R.
>
> Could anyone help me with the next step? How can I conduct a permanova on my dataset? In the end I would need an R value and significance level telling me if community compositions differ significantly between sites.
>
> Below is my code (not too much) and the data for the matrix.
>
> #dput for matrix:
>
> structure(c("", "O", "Y", "Y", "Y", "O", "O", "Y", "O", "O", "0", "0.544", "0.519", "0.533", "0.481", "0.548", "0.518", "0.479", "Y", "0.544", "0", "0.383", "0.416", "0.383", "0.358", "0.434", "0.399", "Y", "0.519", "0.383", "0", "0.398", "0.359", "0.392", "0.401", "0.374", "Y", "0.533", "0.416", "0.398", "0", "0.398", "0.399", "0.358", "0.348", "O", "0.481", "0.383", "0.359", "0.398", "0", "0.37", "0.317", "0.354", "O", "0.548", "0.358", "0.392", "0.399", "0.37", "0", "0.39", "0.365", "Y", "0.518", "0.434", "0.401", "0.358", "0.317", "0.39", "0", "0.371", "O", "0.479", "0.399", "0.374", "0.348", "0.354", "0.365", "0.371", "0"), .Dim = c(9L, 9L), .Dimnames = list(NULL, c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9")))
>
>
> #Code
> #load dissimilarity matrix (based on Soerenson similarity) moth_dta<-read.csv("Geo_sorenson_8.csv",header=T,sep=";")#Creates matrix from imported data
> moth_dta<-as.matrix(moth_dta)
> moth_dta
> library(vegan)
>
>
> Thank you very much,
> Michael
>
> Eisenring Michael, Msc.
> PhD Student
>
> Federal Department of Economic Affairs, Education and Research EAER Institute of Sustainability Sciences ISS Biosafety
>
> Reckenholzstrasse 191, CH-8046 Z rich
> Tel. +41 44 37 77181
> Fax +41 44 37 77201
> michael.eisenring at agroscope.admin.ch<mailto:michael.eisenring at agroscope.admin.ch>
> www.agroscope.ch<http://www.agroscope.ch/>
>
>
>          [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


-- 
Tim Richter-Heitmann (M.Sc.)
PhD Candidate



International Max-Planck Research School for Marine Microbiology
University of Bremen
Microbial Ecophysiology Group (AG Friedrich)
FB02 - Biologie/Chemie
Leobener Stra?e (NW2 A2130)
D-28359 Bremen
Tel.: 0049(0)421 218-63062
Fax: 0049(0)421 218-63069


From petr.pikal at precheza.cz  Wed Dec 23 12:50:13 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 23 Dec 2015 11:50:13 +0000
Subject: [R] assigning values to elements of matrixes
In-Reply-To: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
References: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007BB8@SRVEXCHMBX.precheza.cz>

Hi Matteo

From assign help page

x
a variable name, given as a character string. No coercion is done, and the first element of a character vector of length greater than one will be used, with a warning

Nothing is said about matrices and dimensions here so I wouldn't expect that assign can operate the way you are expecting.

However I am not sure how to do what you want. It has probably something to do with environments which is not my cup of tea. I would do it list way. Here is an example>

Named list with 2 matrices

> lll<-list(mat_a=mat_a, mat_b=mat_b)
> lll
$mat_a
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0

$mat_b
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0

Name of a matrix
> nam
[1] "mat_a"

Selection of appropriate matrix element and assigning it a value

> lll[[which(names(lll)==nam)]][2,2]<-10

Changed list
> lll
$mat_a
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0   10    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0

$mat_b
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    0    0    0    0
[2,]    0    0    0    0    0
[3,]    0    0    0    0    0
[4,]    0    0    0    0    0
[5,]    0    0    0    0    0

You can easily put everything in a function similar to yours without using assign.

Cheers
Petr


> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matteo
> Richiardi
> Sent: Wednesday, December 23, 2015 9:45 AM
> To: r-help at r-project.org
> Subject: [R] assigning values to elements of matrixes
>
> I am following the example I find on ?assign:
>
> a <- 1:4
> assign("a[1]", 2)
>
> This appears to create a new variable named "a[1]" rather than changing
> the value of the vector.
>
> Am I missing something here? How can I assign a value to a specified
> element of a vector/matrix?
>
> Of course, my problem is slightly more involved, but I guess the above
> is its core. For those interested, I have the two matrixes M_a <-
> matrix(0,10,10) M_b <- matrix(0,10,10)
>
> I want to have a function that assigns a value to a specific element of
> one of the matrix, as in foo <- function(s,i,j,value){
>   assign(paste("M_",s)[i,j],value)
> }
>
> This however does not work:
>
> foo('a',1,1,1)
>
> Error in paste("M_", s)[1, j] : incorrect number of dimensions
>
> Following the ?assign help, I tried
>
> foo2 <- function(s,i,j,value){
>   assign(paste("M_",s,"[i,j]"),value, envir = .GlobalEnv) }
>
> but this produces the problem I described above (namely, a new variable
> is created, rather than replacing the specified element of the matrix).
>
> I know that in this example I could simply pass the matrix as an
> argument to the function, but I am interested in understanding how
> 'assign' work.
>
> Many thanks for your help.
>
> Matteo
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From dimitri.liakhovitski at gmail.com  Wed Dec 23 15:07:18 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 23 Dec 2015 09:07:18 -0500
Subject: [R] Expanding matrix into dummies
In-Reply-To: <CAGxFJbQXQJ8Rfi48VkyStST4UJFOh1T0F4R=E932od4RjsTJjA@mail.gmail.com>
References: <CAN2xGJbK+539fR4Qkm2MfE3pYJ7QPnDBmRKom=9fDc-=UTYSMw@mail.gmail.com>
	<2F6A1DA9-B1EF-4841-A82D-C4CE89603B11@me.com>
	<CAGxFJbQXQJ8Rfi48VkyStST4UJFOh1T0F4R=E932od4RjsTJjA@mail.gmail.com>
Message-ID: <CAN2xGJbsqEpEQpSaEYkcU0yGT3rVyEQF-Q49ygpeVjkSRntKjw@mail.gmail.com>

Thank you very much, Marc and Bert - this is great!

On Tue, Dec 22, 2015 at 7:14 PM, Bert Gunter <bgunter.4567 at gmail.com> wrote:
> ... Perhaps worth noting is that the row indices can be created
> directly without row():
>
> result[cbind(rep.int(seq_len(6),5), as.vector(x))] <- 1
>
> but the downside is that you have to know that a matrix is a vector
> "stored" in column major order. I find this arcane detail quite handy,
> though.
>
> Cheers,
> Bert
>
>
> Bert Gunter
>
> "The trouble with having an open mind is that people keep coming along
> and sticking things into it."
> -- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )
>
>
> On Tue, Dec 22, 2015 at 3:03 PM, Marc Schwartz <marc_schwartz at me.com> wrote:
>>
>>> On Dec 22, 2015, at 4:42 PM, Dimitri Liakhovitski <dimitri.liakhovitski at gmail.com> wrote:
>>>
>>> # I have a matrix x:
>>>
>>> k <- 20
>>> N <- 5
>>> set.seed(123)
>>> x <- matrix(c(sample(1:k, N, replace = F),
>>>              sample(1:k, N, replace = F),
>>>              sample(1:k, N, replace = F),
>>>              sample(1:k, N, replace = F),
>>>              sample(1:k, N, replace = F),
>>>              sample(1:k, N, replace = F)), byrow = T, ncol = 5)
>>> colnames(x) <- paste0("column", 1:5)
>>> (x)
>>>
>>> # I want to reshape it into a matrix 'result' with k columns (not N).
>>> # 'result' should contain the same number of rows as x, and it should have
>>> # in each row 1s in those columns that correspond to the entries in x
>>> in the same row.
>>> # For example, the first row of 'result' should contain 1s in columns
>>> 6, 8, 15, 16, and 17, etc.
>>> # The remaining entries should be zeros.
>>>
>>> result <- matrix(rep(0, nrow(x) * k), nrow = nrow(x))
>>> colnames(result) <- paste0("item", 1:k)
>>>
>>> # I can see how to do it by looping through rows of result.
>>> # But I need to do it fast (not using a loop).
>>> # I feel like I should subset 'result' with 'x', but I am not sure how.
>>>
>>> Thank you very much!
>>
>>
>> Something like this might work for you:
>>
>> result <- matrix(0, nrow(x), k)
>>
>>
>>> result
>>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
>> [1,]    0    0    0    0    0    0    0    0    0     0     0     0
>> [2,]    0    0    0    0    0    0    0    0    0     0     0     0
>> [3,]    0    0    0    0    0    0    0    0    0     0     0     0
>> [4,]    0    0    0    0    0    0    0    0    0     0     0     0
>> [5,]    0    0    0    0    0    0    0    0    0     0     0     0
>> [6,]    0    0    0    0    0    0    0    0    0     0     0     0
>>      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
>> [1,]     0     0     0     0     0     0     0     0
>> [2,]     0     0     0     0     0     0     0     0
>> [3,]     0     0     0     0     0     0     0     0
>> [4,]     0     0     0     0     0     0     0     0
>> [5,]     0     0     0     0     0     0     0     0
>> [6,]     0     0     0     0     0     0     0     0
>>
>>
>> result[cbind(as.vector(row(x)), as.vector(x))] <- 1
>>
>>
>>> result
>>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
>> [1,]    0    0    0    0    0    1    0    1    0     0     0     0
>> [2,]    1    0    0    0    0    0    0    1    0     1     1     0
>> [3,]    0    1    0    0    0    0    0    0    1     1     0     0
>> [4,]    1    0    0    0    1    1    0    0    0     0     0     0
>> [5,]    0    0    0    0    0    0    0    0    0     0     1     1
>> [6,]    0    0    1    0    1    0    0    0    0     0     1     0
>>      [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20]
>> [1,]     0     0     1     1     1     0     0     0
>> [2,]     0     0     0     0     1     0     0     0
>> [3,]     1     0     0     0     0     0     0     1
>> [4,]     0     0     0     1     0     1     0     0
>> [5,]     0     1     0     0     1     1     0     0
>> [6,]     0     0     1     0     0     0     1     0
>>
>>
>> See ?row
>>
>> Regards,
>>
>> Marc Schwartz
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.



-- 
Dimitri Liakhovitski


From giorgio.garziano at ericsson.com  Wed Dec 23 15:13:34 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Wed, 23 Dec 2015 14:13:34 +0000
Subject: [R] assigning values to elements of matrixes
Message-ID: <248E6FA047A8C746BA491485764190F53715EE7B@ESESSMB207.ericsson.se>

I think this may help.

my_assign <- function(operand, value) {
  assignment <- paste(operand, value, sep = "<-")
  e <- parse(text = assignment)
  eval.parent(e)
}

a <- rep(0,5)
> a
[1] 0 0 0 0 0

my_assign("a[2]", 7)

> a
[1] 0 7 0 0 0

my_assign("a[4]", 12)

> a
[1] 0 7 0 12 0


--
GG

	[[alternative HTML version deleted]]


From mxkuhn at gmail.com  Wed Dec 23 16:09:49 2015
From: mxkuhn at gmail.com (Max Kuhn)
Date: Wed, 23 Dec 2015 10:09:49 -0500
Subject: [R] Caret - Recursive Feature Elimination Error
In-Reply-To: <8B5BC7735651764E884E3651BAA48D9A08D8A452@W01GMAILDAGA52.reg1.1bank.dbs.com>
References: <8B5BC7735651764E884E3651BAA48D9A08D8A452@W01GMAILDAGA52.reg1.1bank.dbs.com>
Message-ID: <CAJ9CoWkEtNDBppXd1sRprdWUoKitVUdAHDjnr6YFJCrrsHYAwg@mail.gmail.com>

Providing a reproducible example and the results of `sessionInfo` will help
get your question answered.

Also, what is the point of using glmnet with RFE? It already does feature
selection.

On Wed, Dec 23, 2015 at 1:48 AM, Manish MAHESHWARI <manishm at dbs.com> wrote:

> Hi,
>
> I am trying to use caret, for feature selection on glmnet. I get a strange
> error like below - "arguments imply differing number of rows: 2, 3".
>
>
> x <- data.matrix(train[,features])
>
> y <- train$quoteconversion_flag
>
>
>
> > str(x)
>
>  num [1:260753, 1:297] NA NA NA NA NA NA NA NA NA NA ...
>
>  - attr(*, "dimnames")=List of 2
>
>   ..$ : NULL
>
>   ..$ : chr [1:297] "original_quote_date" "field6" "field7" "field8" ...
>
> > str(y)
>
>  Factor w/ 2 levels "X0","X1": 1 1 1 1 1 1 1 1 1 1 ...
>
> > RFE <- rfe(x,y,sizes = seq(50,300,by=10),
> +            metric = "ROC",maximize=TRUE,rfeControl = MyRFEcontrol,
> +            method='glmnet',
> +            tuneGrid = expand.grid(.alpha=0,.lambda=c(0.01,0.02)),
> +            trControl = MyTrainControl)
> +(rfe) fit Resample01 size: 297
> +(rfe) fit Resample02 size: 297
> +(rfe) fit Resample03 size: 297
> +(rfe) fit Resample04 size: 297
> +(rfe) fit Resample05 size: 297
> +(rfe) fit Resample06 size: 297
> +(rfe) fit Resample07 size: 297
> +(rfe) fit Resample08 size: 297
> +(rfe) fit Resample09 size: 297
> +(rfe) fit Resample10 size: 297
> +(rfe) fit Resample11 size: 297
> +(rfe) fit Resample12 size: 297
> +(rfe) fit Resample13 size: 297
> +(rfe) fit Resample14 size: 297
> +(rfe) fit Resample15 size: 297
> +(rfe) fit Resample16 size: 297
> +(rfe) fit Resample17 size: 297
> +(rfe) fit Resample18 size: 297
> +(rfe) fit Resample19 size: 297
> +(rfe) fit Resample20 size: 297
> +(rfe) fit Resample21 size: 297
> +(rfe) fit Resample22 size: 297
> +(rfe) fit Resample23 size: 297
> +(rfe) fit Resample24 size: 297
> +(rfe) fit Resample25 size: 297
> Error in { :
>   task 1 failed - "task 1 failed - "arguments imply differing number of
> rows: 2, 3""
> In addition: There were 50 or more warnings (use warnings() to see the
> first 50)
>
> Any idea what does this mean?
>
> Thanks,
> Manish
>
> CONFIDENTIAL NOTE:
> The information contained in this email is intended on...{{dropped:13}}


From jrkrideau at inbox.com  Wed Dec 23 16:20:56 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 23 Dec 2015 07:20:56 -0800
Subject: [R] colour input in ribbon plots (geom ribbon)
In-Reply-To: <1445AB3A-9786-45C8-9243-F5E84AE7CD9D@dcn.davis.ca.us>
References: <ac6186d424c.00000480jrkrideau@inbox.com>
Message-ID: <B69997A1763.00000D05jrkrideau@inbox.com>

My bad. I was thinking that it could go in aes() within geom_ribbon.



John Kane
Kingston ON Canada

-----Original Message-----
From: jdnewmil at dcn.davis.ca.us
Sent: Tue, 22 Dec 2015 12:13:45 -0800
To: jrkrideau at inbox.com, pushan.zoology at gmail.com, r-help at r-project.org
Subject: Re: [R] colour input in ribbon plots (geom ribbon)

Um, NOT in the aesthetic... as a direct argument to geom_ribbon.

 But yes, examples are critical for communication about R questions, and posting on plain text format is critical for getting the example through the list server undamaged. 
 -- 
 Sent from my phone. Please excuse my brevity.

On December 22, 2015 11:50:31 AM PST, John Kane <jrkrideau at inbox.com> wrote:

	It is not clear what you are doing. Please provide some code and data if possible. 

Otherwise adding something like colour = "red" in the aes() should do something. 

John Kane
Kingston ON Canada

	 -----Original Message-----
 From: pushan.zoology at gmail.com
 Sent: Tue, 22 Dec 2015 14:26:03 +0530
 To: r-help at r-project.org
 Subject: [R] colour input in ribbon plots (geom ribbon)

 Hallow everybody
 I want to know if there is any command for input colour in the ribbon
 plot
 (geom ribbon) of ggplot2. Unless that, the mean line and the standard
 error
 can not be distinguished. Please help me if anyone have any idea.

 --
 Pushan Chakraborty
 CSIR - SRF

 Center for Pollination Studies, University of Calcutta
 35,
Ballyguanje Circular Road, Kolkata - 700019
           &
 Wildlife Institute of India
 Chandrabani, Dehradun - 248001

 webpage:

 http://cpscu.in/?page_id=51 [http://cpscu.in/?page_id=51]

 Skype: cpushan

  [[alternative HTML version deleted]]

 R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
 https://stat.ethz.ch/mailman/listinfo/r-help [https://stat.ethz.ch/mailman/listinfo/r-help]
 PLEASE do read the posting guide
 http://www.R-project.org/posting-guide.html [http://www.R-project.org/posting-guide.html]
 and provide commented, minimal, self-contained, reproducible code.

FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!

R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help [https://stat.ethz.ch/mailman/listinfo/r-help]
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html [http://www.R-project.org/posting-guide.html]
and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D MARINE AQUARIUM SCREENSAVER - Watch dolphins, sharks & orcas on your desktop!


From jrkrideau at inbox.com  Wed Dec 23 16:23:38 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 23 Dec 2015 07:23:38 -0800
Subject: [R] R studio installation and running
In-Reply-To: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
Message-ID: <B69F9F0DB07.00000D0Fjrkrideau@inbox.com>

Desktop but you would be better off asking this in an RStudio forum. It is a bit off-topic here.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: ragia11 at hotmail.com
> Sent: Tue, 22 Dec 2015 23:10:58 +0200
> To: r-help at r-project.org
> Subject: [R] R studio installation and running
> 
> Dear group
> I used to run Rstudio on windows.
> currently ?I want to run Rstudio on Ubuntu server, start its GUI .
> kindly, is there a guide on how to do this step by step, I already
> searched and installed R .. but confused ..should I install Rstudio for
> desktop or server ? how to start it?, I am the server admin.
> 
> thanks in advance
> Ragia
> 
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From jrkrideau at inbox.com  Wed Dec 23 16:30:40 2015
From: jrkrideau at inbox.com (John Kane)
Date: Wed, 23 Dec 2015 07:30:40 -0800
Subject: [R] Colour input in geom line
In-Reply-To: <567a27b8.824f620a.d9cc8.ffffa640@mx.google.com>
Message-ID: <B6AF5C89A34.00000D1Cjrkrideau@inbox.com>

It really would help to have sample code and data to see what you arfe doing.  An issue may be due to anything from a missing comma to a complete mis-specifcation of syntax and without code it is often impossible to guess.

Please have a look at http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

They suggest some strategies for asking questions on R-help.  

Otherwise, I think Jeff's link should help.

John Kane
Kingston ON Canada


> -----Original Message-----
> From: pushan.zoology at gmail.com
> Sent: Wed, 23 Dec 2015 10:18:54 +0530
> To: r-help at r-project.org
> Subject: [R] Colour input in geom line
> 
> Thanks for the help in my previous quarry, I want to know is it possible
> to change the colour in geom line?
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
Can't remember your password? Do you need a strong and secure password?
Use Password manager! It stores your passwords & protects your account.


From peter.anthoni at kit.edu  Wed Dec 23 17:05:35 2015
From: peter.anthoni at kit.edu (Anthoni, Peter (IMK))
Date: Wed, 23 Dec 2015 16:05:35 +0000
Subject: [R] assigning values to elements of matrixes
In-Reply-To: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
References: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
Message-ID: <591874FC-7B71-46D9-8C9A-1AB7919D0276@kit.edu>

Hi,

How about the following:
foo2 <- function(s,i,j,value)
{
  M = get(paste("M_",s,sep=""))
  M[i,j] = value
  assign(paste("M_",s,sep=""),M, envir = .GlobalEnv) 
}

foo2("a",1,2,15)

cheers
Peter

> On 23 Dec 2015, at 09:44, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
> 
> I am following the example I find on ?assign:
> 
> a <- 1:4
> assign("a[1]", 2)
> 
> This appears to create a new variable named "a[1]" rather than
> changing the value of the vector.
> 
> Am I missing something here? How can I assign a value to a specified
> element of a vector/matrix?
> 
> Of course, my problem is slightly more involved, but I guess the above
> is its core. For those interested, I have the two matrixes
> M_a <- matrix(0,10,10)
> M_b <- matrix(0,10,10)
> 
> I want to have a function that assigns a value to a specific element
> of one of the matrix, as in
> foo <- function(s,i,j,value){
>  assign(paste("M_",s)[i,j],value)
> }
> 
> This however does not work:
> 
> foo('a',1,1,1)
> 
> Error in paste("M_", s)[1, j] : incorrect number of dimensions
> 
> Following the ?assign help, I tried
> 
> foo2 <- function(s,i,j,value){
>  assign(paste("M_",s,"[i,j]"),value, envir = .GlobalEnv)
> }
> 
> but this produces the problem I described above (namely, a new
> variable is created, rather than replacing the specified element of
> the matrix).
> 
> I know that in this example I could simply pass the matrix as an
> argument to the function, but I am interested in understanding how
> 'assign' work.
> 
> Many thanks for your help.
> 
> Matteo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jfox at mcmaster.ca  Wed Dec 23 17:09:01 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Wed, 23 Dec 2015 16:09:01 +0000
Subject: [R] assigning values to elements of matrixes
In-Reply-To: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
References: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F46AC5@FHSDB2D11-2.csu.mcmaster.ca>

Dear Matteo,

Here's a function that does what you want and that you should be able to adapt to your problem. It does have the disadvantage of copying the object twice (beyond the overhead of assigning to the indexed value):

myassign <- function(x, index, value, envir=.GlobalEnv, ...){
  object <- get(x, envir=envir, ...)
  object[index] <- value
  assign(x, object, envir=envir, ...)
}

Your example:

> a <- 1:4
> myassign("a", 1, 2)
> a
[1] 2 2 3 4

I hope this helps,
 John

-----------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario
Canada L8S 4M4
Web: socserv.mcmaster.ca/jfox



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Matteo
> Richiardi
> Sent: December 23, 2015 3:45 AM
> To: r-help at r-project.org
> Subject: [R] assigning values to elements of matrixes
> 
> I am following the example I find on ?assign:
> 
> a <- 1:4
> assign("a[1]", 2)
> 
> This appears to create a new variable named "a[1]" rather than changing the
> value of the vector.
> 
> Am I missing something here? How can I assign a value to a specified element
> of a vector/matrix?
> 
> Of course, my problem is slightly more involved, but I guess the above is its
> core. For those interested, I have the two matrixes M_a <- matrix(0,10,10) M_b
> <- matrix(0,10,10)
> 
> I want to have a function that assigns a value to a specific element of one of the
> matrix, as in foo <- function(s,i,j,value){
>   assign(paste("M_",s)[i,j],value)
> }
> 
> This however does not work:
> 
> foo('a',1,1,1)
> 
> Error in paste("M_", s)[1, j] : incorrect number of dimensions
> 
> Following the ?assign help, I tried
> 
> foo2 <- function(s,i,j,value){
>   assign(paste("M_",s,"[i,j]"),value, envir = .GlobalEnv) }
> 
> but this produces the problem I described above (namely, a new variable is
> created, rather than replacing the specified element of the matrix).
> 
> I know that in this example I could simply pass the matrix as an argument to the
> function, but I am interested in understanding how 'assign' work.
> 
> Many thanks for your help.
> 
> Matteo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From jsorkin at grecc.umaryland.edu  Wed Dec 23 17:44:36 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Wed, 23 Dec 2015 11:44:36 -0500
Subject: [R] R studio installation and running
In-Reply-To: <B69F9F0DB07.00000D0Fjrkrideau@inbox.com>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
	<B69F9F0DB07.00000D0Fjrkrideau@inbox.com>
Message-ID: <567A892C020000CB00144887@smtp.medicine.umaryland.edu>

If you want to run Rstudio locally, install the desktop edition. If you want to be able to run Rstudio from a remote machine which will connect to your desktop via the web, install the server edition. If you are unsure which to use, or are having trouble, start with the desktop edition. You must install R before you install either edition of RStudio. Once RStudio is installed, enter rstudio , possibly RStudio at the command line to run the program


> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)


> On Dec 23, 2015, at 10:24 AM, John Kane <jrkrideau at inbox.com> wrote:
> 
> Desktop but you would be better off asking this in an RStudio forum. It is a bit off-topic here.
> 
> John Kane
> Kingston ON Canada
> 
> 
>> -----Original Message-----
>> From: ragia11 at hotmail.com
>> Sent: Tue, 22 Dec 2015 23:10:58 +0200
>> To: r-help at r-project.org
>> Subject: [R] R studio installation and running
>> 
>> Dear group
>> I used to run Rstudio on windows.
>> currently  I want to run Rstudio on Ubuntu server, start its GUI .
>> kindly, is there a guide on how to do this step by step, I already
>> searched and installed R .. but confused ..should I install Rstudio for
>> desktop or server ? how to start it?, I am the server admin.
>> 
>> thanks in advance
>> Ragia
>> 
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
> 
> ____________________________________________________________
> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From ragia11 at hotmail.com  Wed Dec 23 17:51:10 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Wed, 23 Dec 2015 18:51:10 +0200
Subject: [R] R studio installation and running
In-Reply-To: <567A892C020000CB00144887@smtp.medicine.umaryland.edu>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>,
	<B69F9F0DB07.00000D0Fjrkrideau@inbox.com>,
	<567A892C020000CB00144887@smtp.medicine.umaryland.edu>
Message-ID: <DUB125-W13B0FBB62EB6A57DED1786B3E60@phx.gbl>


many thanks for your help
its remote ubuntu server that I connect via Putty, and I install R and rstudio , now trying to make it run..but it doesnt..its only terminal..I will try the rstudio help mailing list as many of the list advice's me, many thanks prof for your help
Ragia
________________________________
> Date: Wed, 23 Dec 2015 11:44:36 -0500 
> From: jsorkin at grecc.umaryland.edu 
> To: jrkrideau at inbox.com 
> CC: ragia11 at hotmail.com; r-help at r-project.org 
> Subject: Re: [R] R studio installation and running 
> 
> If you want to run Rstudio locally, install the desktop edition. If you 
> want to be able to run Rstudio from a remote machine which will connect 
> to your desktop via the web, install the server edition. If you are 
> unsure which to use, or are having trouble, start with the desktop 
> edition. You must install R before you install either edition of 
> RStudio. Once RStudio is installed, enter rstudio , possibly RStudio at 
> the command line to run the program 
> 
> 
> John David Sorkin M.D., Ph.D. 
> Professor of Medicine 
> Chief, Biostatistics and Informatics 
> University of Maryland School of Medicine Division of Gerontology and 
> Geriatric Medicine 
> Baltimore VA Medical Center 
> 10 North Greene Street 
> GRECC (BT/18/GR) 
> Baltimore, MD 21201-1524 
> (Phone) 410-605-7119 
> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
> 
> On Dec 23, 2015, at 10:24 AM, John Kane 
> <jrkrideau at inbox.com<mailto:jrkrideau at inbox.com>> wrote: 
> 
> Desktop but you would be better off asking this in an RStudio forum. It 
> is a bit off-topic here. 
> 
> John Kane 
> Kingston ON Canada 
> 
> 
> -----Original Message----- 
> From: ragia11 at hotmail.com<mailto:ragia11 at hotmail.com> 
> Sent: Tue, 22 Dec 2015 23:10:58 +0200 
> To: r-help at r-project.org<mailto:r-help at r-project.org> 
> Subject: [R] R studio installation and running 
> 
> Dear group 
> I used to run Rstudio on windows. 
> currently I want to run Rstudio on Ubuntu server, start its GUI . 
> kindly, is there a guide on how to do this step by step, I already 
> searched and installed R .. but confused ..should I install Rstudio for 
> desktop or server ? how to start it?, I am the server admin. 
> 
> thanks in advance 
> Ragia 
> 
> 
> ______________________________________________ 
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
> UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html> 
> and provide commented, minimal, self-contained, reproducible code. 
> 
> ____________________________________________________________ 
> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop! 
> 
> ______________________________________________ 
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
> UNSUBSCRIBE and more, see 
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide 
> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html> 
> and provide commented, minimal, self-contained, reproducible code. 
> 
> 
> Confidentiality Statement: 
> 
> This email message, including any attachments, is for the sole use of 
> the intended recipient(s) and may contain confidential and privileged 
> information. Any unauthorized use, disclosure or distribution is 
> prohibited. If you are not the intended recipient, please contact the 
> sender by reply email and destroy all copies of the original message. 
 		 	   		  

From jdnewmil at dcn.davis.ca.us  Wed Dec 23 18:07:14 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Wed, 23 Dec 2015 09:07:14 -0800
Subject: [R] colour input in geom line ggplote2
In-Reply-To: <CAAATBeDy=AAz=m970D0YN34QxA7eZrpT2ZwL-bjX7gSwFEthZg@mail.gmail.com>
References: <CAAATBeDy=AAz=m970D0YN34QxA7eZrpT2ZwL-bjX7gSwFEthZg@mail.gmail.com>
Message-ID: <01F39E3B-BF3D-4714-ACA4-D2ED6B1DDCD9@dcn.davis.ca.us>

There is a problem built into your question: you are treating carb graphically as a continuous variable, yet by asking for a line plot of mean values you seem to be assuming it is discrete. Below are several possible interpretations of your intent. 

Continuous:

library(ggplot2)
p <- ggplot( mtcars, aes( x=carb, y=mpg ) ) + geom_point()
p + stat_summary( geom="ribbon", fun.ymin="min", fun.ymax="max" )
p2 <- p + stat_summary( geom="ribbon"
, fun.ymin="min"
, fun.ymax="max"
, alpha=0.2
, fill="blue"
)

# local regression
p + geom_smooth()
# literal interpretation 
p2 + geom_hline( yintercept = mean( mtcars$mpg ), colour="red" )
# linear regression 
p2 + geom_smooth( method="lm", se=FALSE, colour="red" )

Discrete:

mtcarsmean <- aggregate( mtcars$mpg, mtcars[ , "carb", drop=FALSE ], mean )

# discrete means
p2 + geom_line( data=mtcarsmean, mapping=aes( x=carb, y=x ), colour="red" )

The reason this has to be done explicitly outside of the geom_smooth mechanism is because of this conflict between graphically continuous and conceptually discrete data.

-- 
Sent from my phone. Please excuse my brevity.

On December 22, 2015 11:43:23 PM PST, pushan chakraborty <pushan.zoology at gmail.com> wrote:
>Dear list
>
>I want to put a line command in the following and also want to
>attribute
>colour in the geom line command line. It would be a ribbon plot showing
>the
>mean line. Is there any help?
>
>
>p <- ggplot(mtcars, aes(x=carb, y=mpg)) + geom_point()
>
>p + stat_summary(geom="ribbon", fun.ymin="min", fun.ymax="max")
>
>p + stat_summary(geom="ribbon", fun.ymin="min", fun.ymax="max",
>        alpha=0.2, fill="blue")
>
>
>
>
>
>-- 
>Pushan Chakraborty
>CSIR - SRF
>
>Center for Pollination Studies, University of Calcutta
>35, Ballyguanje Circular Road, Kolkata - 700019
>          &
>Wildlife Institute of India
>Chandrabani, Dehradun - 248001
>
>webpage:
>
>http://cpscu.in/?page_id=51
>
>Skype: cpushan
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From h.wickham at gmail.com  Wed Dec 23 18:35:02 2015
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 23 Dec 2015 11:35:02 -0600
Subject: [R] vjust unresponsive (ggplot2)
In-Reply-To: <F7E6D18CC2877149AB5296CE54EA27662EDD7BF3@WAXMXOLYMB025.WAX.wa.lcl>
References: <CAKJ8KViUu+YTuS05ONb5UeqjQ9yW2Fez42pnAAs4Qkk5FrUtrQ@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA27662EDD6BC2@WAXMXOLYMB025.WAX.wa.lcl>
	<CA+vqiLH2eTgOCt4g_tdNm4UR1noxDTGE=KFvE_=_zZs1vaQfQg@mail.gmail.com>
	<F7E6D18CC2877149AB5296CE54EA27662EDD7BF3@WAXMXOLYMB025.WAX.wa.lcl>
Message-ID: <CABdHhvFpqJvUpMg=p7QyA+ErrGtSSdZwTqGhobRC2ZAS1ASWKQ@mail.gmail.com>

vjust was always a hack that I never thought should work. The margins
parameter is the correct way to solve this problem as of ggplot2 2.0.0.

Hadley

On Tuesday, December 22, 2015, Nordlund, Dan (DSHS/RDA) <NordlDJ at dshs.wa.gov>
wrote:

> Ista,
>
> You are correct, I was not at the latest release of ggplot2.  I updated to
> the latest version and am now seeing the same result as you and the OP.  So
> it does look like an issue with the latest version of ggplot2.
>
> Dan
>
> Daniel Nordlund, PhD
> Research and Data Analysis Division
> Services & Enterprise Support Administration
> Washington State Department of Social and Health Services
>
>
> > -----Original Message-----
> > From: Ista Zahn [mailto:istazahn at gmail.com <javascript:;>]
> > Sent: Tuesday, December 22, 2015 10:48 AM
> > To: Nordlund, Dan (DSHS/RDA)
> > Cc: r-help at r-project.org <javascript:;>
> > Subject: Re: [R] vjust unresponsive (ggplot2)
> >
> > Hi Dan,
> >
> > Chances are that you haven't yet upgraded to ggplot2 version 2.0. unit
> (as
> > well as arrow and alpha) are now re-exported from ggplot2.
> >
> > Using the latest release I also see that vjust doesn't seem to do
> anything.
> >
> > Best,
> > Ista
> >
> > On Tue, Dec 22, 2015 at 1:37 PM, Nordlund, Dan (DSHS/RDA)
> > <NordlDJ at dshs.wa.gov <javascript:;>> wrote:
> > > Are you sure it is not working for you?  Your example code did not
> work for
> > me at all until I removed the plot .margin  parameter (unit wasn't
> > recognized).  Once I did that hjust and vjust worked as expected.
> However,
> > values between .1 and .9 for vjust don't really move the axis title very
> much
> > so it may not be real noticeable.  Try a value like 2 or 3, just to make
> sure you
> > easily see the change in position before concluding that nothing is
> > happening.
> > >
> > > Dan
> > >
> > > Daniel Nordlund, PhD
> > > Research and Data Analysis Division
> > > Services & Enterprise Support Administration Washington State
> > > Department of Social and Health Services
> > >
> > >
> > > -----Original Message-----
> > > From: R-help [mailto:r-help-bounces at r-project.org <javascript:;>] On
> Behalf Of Ryan
> > > Utz
> > > Sent: Tuesday, December 22, 2015 10:00 AM
> > > To: r-help at r-project.org <javascript:;>
> > > Subject: [R] vjust unresponsive (ggplot2)
> > >
> > > Hi all,
> > >
> > > I cannot for the life of me get my axis titles to adjust vertically in
> a ggplot.
> > I've seen several posts about this and have tried everything:
> > > keeping vjust within 0 and 1, adjusting the margins, etc. hjust is
> behaving
> > just as it should but vjust just mocks me in silence. No error message is
> > produced.
> > >
> > > Here's a sample code:
> > >
> > > x=data.frame(sample(1:10))
> > > x[,2]=sample(1:10)
> > >
> > > ggplot(data=x,aes(x=V2,y=V2))+theme(axis.title.y=element_text(vjust=.1
> > > ,hjust=0.6),
> > > plot.margin=unit(c(1,1,2,2),'cm'))
> > >
> > > No matter what I put into vjust, nothing happens. Am I missing
> something
> > obvious??
> > >
> > > Thanks ahead of time for any help,
> > > Ryan
> > >
> > >
> > > --
> > >
> > > Ryan Utz, Ph.D.
> > > Assistant professor of water resources
> > > *chatham**UNIVERSITY*
> > > Home/Cell: (724) 272-7769
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
> and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> > >
> > > ______________________________________________
> > > R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE
> and more, see
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide
> > > http://www.R-project.org/posting-guide.html
> > > and provide commented, minimal, self-contained, reproducible code.
> ______________________________________________
> R-help at r-project.org <javascript:;> mailing list -- To UNSUBSCRIBE and
> more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


-- 
http://had.co.nz/

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Wed Dec 23 19:04:17 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 23 Dec 2015 19:04:17 +0100
Subject: [R] R studio installation and running
In-Reply-To: <DUB125-W13B0FBB62EB6A57DED1786B3E60@phx.gbl>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
	<B69F9F0DB07.00000D0Fjrkrideau@inbox.com>
	<567A892C020000CB00144887@smtp.medicine.umaryland.edu>
	<DUB125-W13B0FBB62EB6A57DED1786B3E60@phx.gbl>
Message-ID: <B6615419-AF44-429F-B703-5773C3867316@gmail.com>

Correct me if I'm wrong, but I think the mode of operation is that you run a local Rstudio, which connects to Rstudio server via <mumble> protocol which lets the two Rstudios communicate via <mumble> Internet ports. I don't think you can just run Rstudio via a Putty connection. As others have pointed out, Rstudio support should give you the details.

-pd


> On 23 Dec 2015, at 17:51 , Ragia Ibrahim <ragia11 at hotmail.com> wrote:
> 
> 
> many thanks for your help
> its remote ubuntu server that I connect via Putty, and I install R and rstudio , now trying to make it run..but it doesnt..its only terminal..I will try the rstudio help mailing list as many of the list advice's me, many thanks prof for your help
> Ragia
> ________________________________
>> Date: Wed, 23 Dec 2015 11:44:36 -0500 
>> From: jsorkin at grecc.umaryland.edu 
>> To: jrkrideau at inbox.com 
>> CC: ragia11 at hotmail.com; r-help at r-project.org 
>> Subject: Re: [R] R studio installation and running 
>> 
>> If you want to run Rstudio locally, install the desktop edition. If you 
>> want to be able to run Rstudio from a remote machine which will connect 
>> to your desktop via the web, install the server edition. If you are 
>> unsure which to use, or are having trouble, start with the desktop 
>> edition. You must install R before you install either edition of 
>> RStudio. Once RStudio is installed, enter rstudio , possibly RStudio at 
>> the command line to run the program 
>> 
>> 
>> John David Sorkin M.D., Ph.D. 
>> Professor of Medicine 
>> Chief, Biostatistics and Informatics 
>> University of Maryland School of Medicine Division of Gerontology and 
>> Geriatric Medicine 
>> Baltimore VA Medical Center 
>> 10 North Greene Street 
>> GRECC (BT/18/GR) 
>> Baltimore, MD 21201-1524 
>> (Phone) 410-605-7119 
>> (Fax) 410-605-7913 (Please call phone number above prior to faxing) 
>> 
>> On Dec 23, 2015, at 10:24 AM, John Kane 
>> <jrkrideau at inbox.com<mailto:jrkrideau at inbox.com>> wrote: 
>> 
>> Desktop but you would be better off asking this in an RStudio forum. It 
>> is a bit off-topic here. 
>> 
>> John Kane 
>> Kingston ON Canada 
>> 
>> 
>> -----Original Message----- 
>> From: ragia11 at hotmail.com<mailto:ragia11 at hotmail.com> 
>> Sent: Tue, 22 Dec 2015 23:10:58 +0200 
>> To: r-help at r-project.org<mailto:r-help at r-project.org> 
>> Subject: [R] R studio installation and running 
>> 
>> Dear group 
>> I used to run Rstudio on windows. 
>> currently I want to run Rstudio on Ubuntu server, start its GUI . 
>> kindly, is there a guide on how to do this step by step, I already 
>> searched and installed R .. but confused ..should I install Rstudio for 
>> desktop or server ? how to start it?, I am the server admin. 
>> 
>> thanks in advance 
>> Ragia 
>> 
>> 
>> ______________________________________________ 
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
>> UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html> 
>> and provide commented, minimal, self-contained, reproducible code. 
>> 
>> ____________________________________________________________ 
>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop! 
>> 
>> ______________________________________________ 
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To 
>> UNSUBSCRIBE and more, see 
>> https://stat.ethz.ch/mailman/listinfo/r-help 
>> PLEASE do read the posting guide 
>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html> 
>> and provide commented, minimal, self-contained, reproducible code. 
>> 
>> 
>> Confidentiality Statement: 
>> 
>> This email message, including any attachments, is for the sole use of 
>> the intended recipient(s) and may contain confidential and privileged 
>> information. Any unauthorized use, disclosure or distribution is 
>> prohibited. If you are not the intended recipient, please contact the 
>> sender by reply email and destroy all copies of the original message. 
> 		 	   		  
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From istazahn at gmail.com  Wed Dec 23 19:17:48 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Wed, 23 Dec 2015 13:17:48 -0500
Subject: [R] R studio installation and running
In-Reply-To: <B6615419-AF44-429F-B703-5773C3867316@gmail.com>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
	<B69F9F0DB07.00000D0Fjrkrideau@inbox.com>
	<567A892C020000CB00144887@smtp.medicine.umaryland.edu>
	<DUB125-W13B0FBB62EB6A57DED1786B3E60@phx.gbl>
	<B6615419-AF44-429F-B703-5773C3867316@gmail.com>
Message-ID: <CA+vqiLFUKoVD0gqk1bKrQC_N08gn9snGMw+==6idJ51MxonAXQ@mail.gmail.com>

On Wed, Dec 23, 2015 at 1:04 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> Correct me if I'm wrong, but I think the mode of operation is that you run a local Rstudio, which connects to Rstudio server via <mumble> protocol which lets the two Rstudios communicate via <mumble> Internet ports.

I'm pretty sure Rstudio server communicates with a web browser, not
with a local Rstudio desktop application.

I don't think you can just run Rstudio via a Putty connection. As
others have pointed out, Rstudio support should give you the details.
>

If you install Rstudio desktop on the server you may be able to run it
via ssh with X forwarding. Not sure how/if this works on
Windows/Putty. If you install Rstudio server then ssh / putty doesn't
really come into play -- everything happens over http. And yes, this
conversation should definitely move to the rstudio support forum.

Best,
Ista

> -pd
>
>
>> On 23 Dec 2015, at 17:51 , Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>>
>>
>> many thanks for your help
>> its remote ubuntu server that I connect via Putty, and I install R and rstudio , now trying to make it run..but it doesnt..its only terminal..I will try the rstudio help mailing list as many of the list advice's me, many thanks prof for your help
>> Ragia
>> ________________________________
>>> Date: Wed, 23 Dec 2015 11:44:36 -0500
>>> From: jsorkin at grecc.umaryland.edu
>>> To: jrkrideau at inbox.com
>>> CC: ragia11 at hotmail.com; r-help at r-project.org
>>> Subject: Re: [R] R studio installation and running
>>>
>>> If you want to run Rstudio locally, install the desktop edition. If you
>>> want to be able to run Rstudio from a remote machine which will connect
>>> to your desktop via the web, install the server edition. If you are
>>> unsure which to use, or are having trouble, start with the desktop
>>> edition. You must install R before you install either edition of
>>> RStudio. Once RStudio is installed, enter rstudio , possibly RStudio at
>>> the command line to run the program
>>>
>>>
>>> John David Sorkin M.D., Ph.D.
>>> Professor of Medicine
>>> Chief, Biostatistics and Informatics
>>> University of Maryland School of Medicine Division of Gerontology and
>>> Geriatric Medicine
>>> Baltimore VA Medical Center
>>> 10 North Greene Street
>>> GRECC (BT/18/GR)
>>> Baltimore, MD 21201-1524
>>> (Phone) 410-605-7119
>>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>>
>>> On Dec 23, 2015, at 10:24 AM, John Kane
>>> <jrkrideau at inbox.com<mailto:jrkrideau at inbox.com>> wrote:
>>>
>>> Desktop but you would be better off asking this in an RStudio forum. It
>>> is a bit off-topic here.
>>>
>>> John Kane
>>> Kingston ON Canada
>>>
>>>
>>> -----Original Message-----
>>> From: ragia11 at hotmail.com<mailto:ragia11 at hotmail.com>
>>> Sent: Tue, 22 Dec 2015 23:10:58 +0200
>>> To: r-help at r-project.org<mailto:r-help at r-project.org>
>>> Subject: [R] R studio installation and running
>>>
>>> Dear group
>>> I used to run Rstudio on windows.
>>> currently I want to run Rstudio on Ubuntu server, start its GUI .
>>> kindly, is there a guide on how to do this step by step, I already
>>> searched and installed R .. but confused ..should I install Rstudio for
>>> desktop or server ? how to start it?, I am the server admin.
>>>
>>> thanks in advance
>>> Ragia
>>>
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>>> UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>> ____________________________________________________________
>>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>>>
>>> ______________________________________________
>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>>> UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>> and provide commented, minimal, self-contained, reproducible code.
>>>
>>>
>>> Confidentiality Statement:
>>>
>>> This email message, including any attachments, is for the sole use of
>>> the intended recipient(s) and may contain confidential and privileged
>>> information. Any unauthorized use, disclosure or distribution is
>>> prohibited. If you are not the intended recipient, please contact the
>>> sender by reply email and destroy all copies of the original message.
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From ragia11 at hotmail.com  Wed Dec 23 19:52:37 2015
From: ragia11 at hotmail.com (Ragia Ibrahim)
Date: Wed, 23 Dec 2015 20:52:37 +0200
Subject: [R] R studio installation and running
In-Reply-To: <CA+vqiLFUKoVD0gqk1bKrQC_N08gn9snGMw+==6idJ51MxonAXQ@mail.gmail.com>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
	<B69F9F0DB07.00000D0Fjrkrideau@inbox.com>,
	<567A892C020000CB00144887@smtp.medicine.umaryland.edu>
	<DUB125-W13B0FBB62EB6A57DED1786B3E60@phx.gbl>,
	<B6615419-AF44-429F-B703-5773C3867316@gmail.com>,
	<CA+vqiLFUKoVD0gqk1bKrQC_N08gn9snGMw+==6idJ51MxonAXQ@mail.gmail.com>
Message-ID: <DUB125-W20859E0535CDE473E553C2B3E60@phx.gbl>

many thanks for all replies?
Indeed I post the question on the rstudio support website, many thanks
Ragia

----------------------------------------
> From: istazahn at gmail.com
> Date: Wed, 23 Dec 2015 13:17:48 -0500
> Subject: Re: [R] R studio installation and running
> To: pdalgd at gmail.com
> CC: ragia11 at hotmail.com; r-help at r-project.org
>
> On Wed, Dec 23, 2015 at 1:04 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>> Correct me if I'm wrong, but I think the mode of operation is that you run a local Rstudio, which connects to Rstudio server via <mumble> protocol which lets the two Rstudios communicate via <mumble> Internet ports.
>
> I'm pretty sure Rstudio server communicates with a web browser, not
> with a local Rstudio desktop application.
>
> I don't think you can just run Rstudio via a Putty connection. As
> others have pointed out, Rstudio support should give you the details.
>>
>
> If you install Rstudio desktop on the server you may be able to run it
> via ssh with X forwarding. Not sure how/if this works on
> Windows/Putty. If you install Rstudio server then ssh / putty doesn't
> really come into play -- everything happens over http. And yes, this
> conversation should definitely move to the rstudio support forum.
>
> Best,
> Ista
>
>> -pd
>>
>>
>>> On 23 Dec 2015, at 17:51 , Ragia Ibrahim <ragia11 at hotmail.com> wrote:
>>>
>>>
>>> many thanks for your help
>>> its remote ubuntu server that I connect via Putty, and I install R and rstudio , now trying to make it run..but it doesnt..its only terminal..I will try the rstudio help mailing list as many of the list advice's me, many thanks prof for your help
>>> Ragia
>>> ________________________________
>>>> Date: Wed, 23 Dec 2015 11:44:36 -0500
>>>> From: jsorkin at grecc.umaryland.edu
>>>> To: jrkrideau at inbox.com
>>>> CC: ragia11 at hotmail.com; r-help at r-project.org
>>>> Subject: Re: [R] R studio installation and running
>>>>
>>>> If you want to run Rstudio locally, install the desktop edition. If you
>>>> want to be able to run Rstudio from a remote machine which will connect
>>>> to your desktop via the web, install the server edition. If you are
>>>> unsure which to use, or are having trouble, start with the desktop
>>>> edition. You must install R before you install either edition of
>>>> RStudio. Once RStudio is installed, enter rstudio , possibly RStudio at
>>>> the command line to run the program
>>>>
>>>>
>>>> John David Sorkin M.D., Ph.D.
>>>> Professor of Medicine
>>>> Chief, Biostatistics and Informatics
>>>> University of Maryland School of Medicine Division of Gerontology and
>>>> Geriatric Medicine
>>>> Baltimore VA Medical Center
>>>> 10 North Greene Street
>>>> GRECC (BT/18/GR)
>>>> Baltimore, MD 21201-1524
>>>> (Phone) 410-605-7119
>>>> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>>>>
>>>> On Dec 23, 2015, at 10:24 AM, John Kane
>>>> <jrkrideau at inbox.com<mailto:jrkrideau at inbox.com>> wrote:
>>>>
>>>> Desktop but you would be better off asking this in an RStudio forum. It
>>>> is a bit off-topic here.
>>>>
>>>> John Kane
>>>> Kingston ON Canada
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: ragia11 at hotmail.com<mailto:ragia11 at hotmail.com>
>>>> Sent: Tue, 22 Dec 2015 23:10:58 +0200
>>>> To: r-help at r-project.org<mailto:r-help at r-project.org>
>>>> Subject: [R] R studio installation and running
>>>>
>>>> Dear group
>>>> I used to run Rstudio on windows.
>>>> currently I want to run Rstudio on Ubuntu server, start its GUI .
>>>> kindly, is there a guide on how to do this step by step, I already
>>>> searched and installed R .. but confused ..should I install Rstudio for
>>>> desktop or server ? how to start it?, I am the server admin.
>>>>
>>>> thanks in advance
>>>> Ragia
>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>>>> UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>> ____________________________________________________________
>>>> FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To
>>>> UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide
>>>> http://www.R-project.org/posting-guide.html<http://www.r-project.org/posting-guide.html>
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>>
>>>> Confidentiality Statement:
>>>>
>>>> This email message, including any attachments, is for the sole use of
>>>> the intended recipient(s) and may contain confidential and privileged
>>>> information. Any unauthorized use, disclosure or distribution is
>>>> prohibited. If you are not the intended recipient, please contact the
>>>> sender by reply email and destroy all copies of the original message.
>>>
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
 		 	   		  

From dwinsemius at comcast.net  Wed Dec 23 20:16:19 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 23 Dec 2015 11:16:19 -0800
Subject: [R] assigning values to elements of matrixes
In-Reply-To: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
References: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
Message-ID: <49A6CDF6-4895-4D99-8950-B7A68BEEBDCB@comcast.net>


> On Dec 23, 2015, at 12:44 AM, Matteo Richiardi <matteo.richiardi at gmail.com> wrote:
> 
> I am following the example I find on ?assign:
> 
> a <- 1:4
> assign("a[1]", 2)


You appear to have completely misinterpreted the intent of the authors of that help page. The next two lines in that example (which you omitted) show that they intended you to see this as a demonstration of what NOT to do:

a <- 1:4
assign("a[1]", 2)
a[1] == 2          # FALSE
get("a[1]") == 2   # TRUE

Notice that a[1] was not assigned the value of 2 as demonstrated when the logical test returning FALSE, but that that an object with the pathological name `a[1]` was created.


> This appears to create a new variable named "a[1]" rather than
> changing the value of the vector.
> 
> Am I missing something here? How can I assign a value to a specified
> element of a vector/matrix?
> 
> Of course, my problem is slightly more involved, but I guess the above
> is its core. For those interested, I have the two matrixes
> M_a <- matrix(0,10,10)
> M_b <- matrix(0,10,10)
> 
> I want to have a function that assigns a value to a specific element
> of one of the matrix, as in
> foo <- function(s,i,j,value){
>  assign(paste("M_",s)[i,j],value)
> }

I see that Peter Antoni and John Fox have offered a demonstration of how to use get and assign. 


Those efforts at macro-ization are essentially recapitulating what happens when using:

M_a[5,5] <- value

The M_a object is looked up, copied to something named `*tmp*`, and then the assignment within that copy is performed and the value copied back into M_a. These semantics with there attendant inefficiencies have prompted the development of the data.table package where the assignments are done in place.


The alternate strategy of Giorgio Garzanio calls to mind this fortune:

> fortunes::fortune("parse()")

If the answer is parse() you should usually rethink the question.
   -- Thomas Lumley
      R-help (February 2005)


See pages 11-13 of https://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf to see how Lumley expands on this theme.


-- 
David.


> 
> This however does not work:
> 
> foo('a',1,1,1)
> 
> Error in paste("M_", s)[1, j] : incorrect number of dimensions
> 
> Following the ?assign help, I tried
> 
> foo2 <- function(s,i,j,value){
>  assign(paste("M_",s,"[i,j]"),value, envir = .GlobalEnv)
> }
> 
> but this produces the problem I described above (namely, a new
> variable is created, rather than replacing the specified element of
> the matrix).
> 
> I know that in this example I could simply pass the matrix as an
> argument to the function, but I am interested in understanding how
> 'assign' work.
> 
> Many thanks for your help.
> 
> Matteo
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From mohsenhs82 at yahoo.com  Wed Dec 23 16:22:21 2015
From: mohsenhs82 at yahoo.com (mohsen hs)
Date: Wed, 23 Dec 2015 15:22:21 +0000 (UTC)
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
	<1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
	<95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>
Message-ID: <1660893852.2256439.1450884141115.JavaMail.yahoo@mail.yahoo.com>

Hi Peter and Rolf
Thank you for your time and replying me. It makes sense now. I sincerely appreciate that.
CheersMohsen?
 

    On Tuesday, December 22, 2015 10:08 PM, peter dalgaard <pdalgd at gmail.com> wrote:
 

 
> On 22 Dec 2015, at 07:30 , mohsen hs via R-help <r-help at r-project.org> wrote:
> 
> The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp and the left one is from qqPlot. Titles might be incorrect.

They never arrived, but your data weren't actually needed. The crucial missing information was the packages used. This will do:

> library(EnvStats); library(fitdistrplus)
> serving <- exp(rnorm(100))
> qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)
> 
> fitln <- fitdist(serving,"lnorm",method="mle")
> qqcomp(fitln)

The difference is quite clearly that qqPlot is doing a QQ-plot of log(serving) vs. normal quantiles, whereas qqcomp plots serving itself against lognormal quantiles. So the former is pretty much equal to the latter on a log-log scale.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com










  
	[[alternative HTML version deleted]]


From belhajfraj.makram at gmail.com  Wed Dec 23 08:34:34 2015
From: belhajfraj.makram at gmail.com (Makram Belhaj Fraj)
Date: Wed, 23 Dec 2015 11:34:34 +0400
Subject: [R] need for help for solving operations in a vector
Message-ID: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>

 Dear colleagues
i need your generous help to solve the following problem

I have a  soil moisture time series qWC1 (61 values)
> qWC1
 75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059 74.70059
74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760 90.73760
90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135 80.69793
79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325 75.97412
75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087 75.46087
75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
75.07930 75.07930 74.95275 74.95275  74.95275

I want to measure consecutive increases corresponding to irrigation and
consecutive decreases  corresponding to recharge
I wrote the following code and it does not calculate for each increment in
i?
also note that I choose to not use diff command in time series because I
 want also that "plateaux" corresponding to a minimum of 2 equal
consecutive values are accounted as positive differences=irrigations so
when x[i+1]==x[i] the difference y might be equal to the previous value xi

following the code i wrote

x<-ts(qWC1,start=1, end=61, frequency=1)
x[1]
plot(x, type="h", col = "green")
y<-rep(0,61)
for (i in 1:61) {
if (x[i+1] > x[i]){
    y[i]==x[i+1]-x[i]
} else if (x[i+1]==x[i]){
    y[i]=x[i+2]-x[i]
} else {
    y[i]==x[i+1]-x[i]
}

}
plot(y, type="h", col = "blueviolet")

Many thank
Makram

	[[alternative HTML version deleted]]


From manishm at dbs.com  Wed Dec 23 10:49:18 2015
From: manishm at dbs.com (Manish MAHESHWARI)
Date: Wed, 23 Dec 2015 09:49:18 +0000
Subject: [R] Binary Classification / Logistic Regression Models - Metrics
Message-ID: <8B5BC7735651764E884E3651BAA48D9A08D8A4D1@W01GMAILDAGA52.reg1.1bank.dbs.com>

Hi All,

For Binary Classification / Logistic Regression Models, Is there a specific preference or standard of what metric to be used for comparison of 2 models, especially when the model types are different - e.g logistic regression vs svd vs gbm vs neural networks?

As I understand AUC is the one used most frequently. Is there some documentation that compares these from a practical standpoint?

Thanks,
Manish
CONFIDENTIAL NOTE:
The information contained in this email is intended only...{{dropped:8}}


From fridayzakari at gmail.com  Wed Dec 23 14:45:15 2015
From: fridayzakari at gmail.com (friday zakari)
Date: Wed, 23 Dec 2015 05:45:15 -0800
Subject: [R] Help on how to use cosinor analysis
Message-ID: <CAPT5noKUwBK9Hh9boxGjK-OJt3=OPCrN=F9zg9UVmro3L2Xhow@mail.gmail.com>

l am a beginner in the use of R for statistical analysis. I am finding it
difficult to use cosinor to analyze the circadian rhythm in rectal
temperature in broiler chicken.

Time=(1,2,3,4.........24)
Rectal temp=(33.8,37.6,37.1,35.5,......38.2)

l will be very please if you can guide me on how to do the analysis using
the sets of variables above as example and if you can also assist me with
the R functions required for the analysis. thank you very much

	[[alternative HTML version deleted]]


From dcarlson at tamu.edu  Wed Dec 23 20:57:15 2015
From: dcarlson at tamu.edu (David L Carlson)
Date: Wed, 23 Dec 2015 19:57:15 +0000
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
Message-ID: <53BF8FB63FAF2E4A9455EF1EE94DA7262D6F6F14@mb02.ads.tamu.edu>

I don't think you know what your code is doing. First, do not use html emails, only plain text. Secondly, provide the data in a portable way with the dput() function:

> dput(qWC1)
c(75.33336, 75.20617, 75.20617, 74.95275, 74.95275, 74.70059, 
74.70059, 74.70059, 74.57498, 74.44968, 74.32469, 74.07563, 85.57237, 
90.40123, 90.7376, 90.7376, 90.7376, 90.7376, 90.90648, 91.07582, 
91.24564, 90.90648, 86.82135, 80.69793, 79.30393, 78.62058, 78.21484, 
77.81226, 77.67876, 77.41279, 77.28032, 76.88495, 76.75383, 76.75383, 
76.4926, 76.36249, 76.2327, 76.2327, 76.10325, 75.97412, 75.84532, 
75.71685, 75.71685, 75.71685, 75.71685, 75.46087, 75.46087, 75.46087, 
75.33336, 75.20617, 75.20617, 75.20617, 75.20617, 75.20617, 75.20617, 
75.0793, 75.0793, 75.0793, 74.95275, 74.95275, 74.95275)

Third, your code is a mess. You do not seem to understand how R uses "<-", "==", and "=" in different ways so your y value has nothing to do with what you say you want. 

First, your loop will create missing values because you have 61 values, but you refer to i+1 (62) and i+2 (63) which do not exist. 

Second, if x[i+1] > x[i] is true, you execute y[i] == x[i+1] - x[i] which tests to see if the left side is equal to the right side and returns TRUE or FALSE, but does not change y[i] at all. The same with the last else statement. 

Once you figure out what you are doing, you can probably handle it all with ifelse() and not use a for loop at all. Or try combining the results of diff(x, 1) and diff(x, 2). You should not try to program in R until you have read more about the R language.

-------------------------------------
David L Carlson
Department of Anthropology
Texas A&M University
College Station, TX 77840-4352

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Makram Belhaj Fraj
Sent: Wednesday, December 23, 2015 1:35 AM
To: r-help at r-project.org; r-help-owner at r-project.org
Subject: [R] need for help for solving operations in a vector

 Dear colleagues
i need your generous help to solve the following problem

I have a  soil moisture time series qWC1 (61 values)
> qWC1
 75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059 74.70059
74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760 90.73760
90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135 80.69793
79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325 75.97412
75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087 75.46087
75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
75.07930 75.07930 74.95275 74.95275  74.95275

I want to measure consecutive increases corresponding to irrigation and
consecutive decreases  corresponding to recharge
I wrote the following code and it does not calculate for each increment in
i?
also note that I choose to not use diff command in time series because I
 want also that "plateaux" corresponding to a minimum of 2 equal
consecutive values are accounted as positive differences=irrigations so
when x[i+1]==x[i] the difference y might be equal to the previous value xi

following the code i wrote

x<-ts(qWC1,start=1, end=61, frequency=1)
x[1]
plot(x, type="h", col = "green")
y<-rep(0,61)
for (i in 1:61) {
if (x[i+1] > x[i]){
    y[i]==x[i+1]-x[i]
} else if (x[i+1]==x[i]){
    y[i]=x[i+2]-x[i]
} else {
    y[i]==x[i+1]-x[i]
}

}
plot(y, type="h", col = "blueviolet")

Many thank
Makram

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From wdunlap at tibco.com  Wed Dec 23 20:58:25 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 23 Dec 2015 11:58:25 -0800
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
Message-ID: <CAF8bMcZcuZQyRuivZ8-m2ZS5=635++U5ni-vfCezQvKmEdYLig@mail.gmail.com>

What answer do you want for the following data?
   x <- c(2,2,3,4,4,4,4,5,5,5,3,1,1,0,0,0,1,1,1)


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Dec 22, 2015 at 11:34 PM, Makram Belhaj Fraj <
belhajfraj.makram at gmail.com> wrote:

>  Dear colleagues
> i need your generous help to solve the following problem
>
> I have a  soil moisture time series qWC1 (61 values)
> > qWC1
>  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059 74.70059
> 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760 90.73760
> 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135 80.69793
> 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325 75.97412
> 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087 75.46087
> 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> 75.07930 75.07930 74.95275 74.95275  74.95275
>
> I want to measure consecutive increases corresponding to irrigation and
> consecutive decreases  corresponding to recharge
> I wrote the following code and it does not calculate for each increment in
> i?
> also note that I choose to not use diff command in time series because I
>  want also that "plateaux" corresponding to a minimum of 2 equal
> consecutive values are accounted as positive differences=irrigations so
> when x[i+1]==x[i] the difference y might be equal to the previous value xi
>
> following the code i wrote
>
> x<-ts(qWC1,start=1, end=61, frequency=1)
> x[1]
> plot(x, type="h", col = "green")
> y<-rep(0,61)
> for (i in 1:61) {
> if (x[i+1] > x[i]){
>     y[i]==x[i+1]-x[i]
> } else if (x[i+1]==x[i]){
>     y[i]=x[i+2]-x[i]
> } else {
>     y[i]==x[i+1]-x[i]
> }
>
> }
> plot(y, type="h", col = "blueviolet")
>
> Many thank
> Makram
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From wewolski at gmail.com  Wed Dec 23 22:29:28 2015
From: wewolski at gmail.com (Witold E Wolski)
Date: Wed, 23 Dec 2015 22:29:28 +0100
Subject: [R] packaging an R-application
Message-ID: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>

Dear List,


What I am seeking advice for is how to best package an R installation
with all the packages required?

Scenario:
I need to deliver an R script which will have quite a bit of package
dependencies, to packages which are not necessarily stable, are not on
cran and might dissapear in the near future. It should be possible to
execute the R-script without any changes during the next 3 years.


What I did look into is packrat. But if I am correct it pulls the
r-packages from cran so if a package was from somewhere else or isn't
available anymore it will fail.

what I have also found is R on an USB stick:
http://personal.bgsu.edu/~mrizzo/Rmisc/usbR.htm

Would this work?

Or is there anything on the lines of pyinstaller
(http://www.pyinstaller.org/) for R?


Thank you

-- 
Witold Eryk Wolski


From dimitri.liakhovitski at gmail.com  Wed Dec 23 22:35:17 2015
From: dimitri.liakhovitski at gmail.com (Dimitri Liakhovitski)
Date: Wed, 23 Dec 2015 16:35:17 -0500
Subject: [R] (no subject)
Message-ID: <CAN2xGJZ7y_8=ow+-kGfWg-gSyBafOy1HJUiw8mbhyoK5MX5D8w@mail.gmail.com>

Merry upcoming Christmas - for those of us who celebrate it!

# I have data frame x.
x <- data.frame(game = c(rep(1, 4), rep(2, 4)), set = rep(c(1,1,2,2),
2), player = rep(1:2, 4),
                char1 = c(1,0,0,0,0,0,0,1), char2 =
c(0,0,1,0,0,1,0,0), char3 = c(0,1,0,1,0,0,0,0),
                char4 = c(0,0,0,0,1,0,1,0))
x
# There are several games (2 here). Each game has several sets (2
here. In each set participate
# several players (2 here).
# Each player possesses 1 or 4 possible characteristics.
# For example, in game 1, set 1, player 1 has characteristic 1 and player 2 -
# characteristic 3

# I also have data frame myinfo:
(myinfo <- data.frame(game = 1:2, set1 = c(3, 4), set2 = c(2, 1)))
# It tells me:
# in game 1, set 1 the winner was the player with characteristic 3
# in game 1, set 2 the winner was the player with characteristic 2, etc.

# I need to merge the 2 to produce the result below.
# I just need an additional column that - for each game and each set -
# has a 1 in the row of the player who had the winning characteristic
# (as per myinfo) and has a 0 otherwise.

result <- x
result$winner <- c(0, 1, 1, 0, 1, 0, 0, 1)
result

# I have written a long loop that loops through each set of each game,
identifies
# which characteristic wins in myinfo, and puts a 1 against the winning row.
# But it's taking forever. Could it be done faster? Thanks a lot!

# Important: In my real game the number of players could be more than
2 and so can
# the number of games and the number of sets per game.
# However, we can assume that the number of sets per game is always the same,
# and the number of players per set is always the same.


-- 
Dimitri Liakhovitski


From macqueen1 at llnl.gov  Wed Dec 23 23:14:56 2015
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 23 Dec 2015 22:14:56 +0000
Subject: [R] packaging an R-application
In-Reply-To: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>
References: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>
Message-ID: <D2A05A6F.14A1B7%macqueen1@llnl.gov>

This sounds like more of an operating system task than an R task.

But within R, you can come close, I would think, by copying all of the
packages' directories to the same directory as the script. Then write
another script that will install all of the packages. In the future, run
that script before running your script. When you deliver the script,
deliver the whole directory that contains it, since that will then include
the package sources. If you're lucky, all of the packages are source
packages.

Of course, R itself will change over the next three years. Hopefully, none
of the improvements will break your script or the packages it depends on.

There was a long discussion on the topic of "enabling reproducible
research & R package management & install.package.version & BiocLite" in
March of 2013 on R-devel that you might want to look at. And something
similar not too long ago in either R-devel or R-package-devel, if I
remember correctly.

I've never used any of the three options you mention.

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 12/23/15, 1:29 PM, "R-help on behalf of Witold E Wolski"
<r-help-bounces at r-project.org on behalf of wewolski at gmail.com> wrote:

>Dear List,
>
>
>What I am seeking advice for is how to best package an R installation
>with all the packages required?
>
>Scenario:
>I need to deliver an R script which will have quite a bit of package
>dependencies, to packages which are not necessarily stable, are not on
>cran and might dissapear in the near future. It should be possible to
>execute the R-script without any changes during the next 3 years.
>
>
>What I did look into is packrat. But if I am correct it pulls the
>r-packages from cran so if a package was from somewhere else or isn't
>available anymore it will fail.
>
>what I have also found is R on an USB stick:
>http://personal.bgsu.edu/~mrizzo/Rmisc/usbR.htm
>
>Would this work?
>
>Or is there anything on the lines of pyinstaller
>(http://www.pyinstaller.org/) for R?
>
>
>Thank you
>
>-- 
>Witold Eryk Wolski
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.


From pdalgd at gmail.com  Wed Dec 23 23:18:42 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 23 Dec 2015 23:18:42 +0100
Subject: [R] R studio installation and running
In-Reply-To: <CA+vqiLFUKoVD0gqk1bKrQC_N08gn9snGMw+==6idJ51MxonAXQ@mail.gmail.com>
References: <DUB125-W319DD8F6644CC91D5EA0C0B3E50@phx.gbl>
	<B69F9F0DB07.00000D0Fjrkrideau@inbox.com>
	<567A892C020000CB00144887@smtp.medicine.umaryland.edu>
	<DUB125-W13B0FBB62EB6A57DED1786B3E60@phx.gbl>
	<B6615419-AF44-429F-B703-5773C3867316@gmail.com>
	<CA+vqiLFUKoVD0gqk1bKrQC_N08gn9snGMw+==6idJ51MxonAXQ@mail.gmail.com>
Message-ID: <CBA898FC-4CF1-4F54-8121-CC4BC7B2FCD9@gmail.com>


> On 23 Dec 2015, at 19:17 , Ista Zahn <istazahn at gmail.com> wrote:
> 
>> Correct me if I'm wrong, but I think the mode of operation is that you run a local Rstudio, which connects to Rstudio server via <mumble> protocol which lets the two Rstudios communicate via <mumble> Internet ports.
> 
> I'm pretty sure Rstudio server communicates with a web browser, not
> with a local Rstudio desktop application.

Corrected.... I must have had it confused with some other client-server setup.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From libya.tahani at gmail.com  Wed Dec 23 22:30:06 2015
From: libya.tahani at gmail.com (Tahani Libya)
Date: Wed, 23 Dec 2015 23:30:06 +0200
Subject: [R] Error in heatmap()
Message-ID: <CAEwfPz3wKWomLSNGSZWiAwp_vnXK=+58-S35m-s6Ya29QuN0EQ@mail.gmail.com>

*Hi,*

*while I try to analysis my data as the following ,I faced some problem
with (heatmap()):*


> dat<-ReadAffy()
> dat


AffyBatch object
size of arrays=1164x1164 features (20 kb)
cdf=HG-U133_Plus_2 (54675 affyids)
number of samples=10
number of genes=54675
annotation=hgu133plus2
notes=
> dat2<-rma(dat)
Background correcting
Normalizing
Calculating Expression
> dat.m<-exprs(dat2)

*The normalized data can be so large that clustering all the genes (or*
*arrays) becomes impossible. Clustering about 23000 genes takes about 1GB
of memory, and clustering 54675 genes would consume about more than 4 GBs
ofmemory, and would not be feasible on a standard Windows workstation.*

*So I tried to sample the data, and this sample*
*is then clustered. This should convey approximately the same information
asthe clustering of the whole dataset:*
> n<-1:nrow(dat.m)
> n.s<-sample(n, nrow(dat.m)*0.1)
> dat.sample<-dat.m[n.s,]
> library(amap)
> clust.genes<-hcluster(x=dat.sample, method="pearson",
+ link="average")
> clust.arrays<-hcluster(x=t(dat.sample), method="pearson",
+ link="average")

*The sample size is here 10% of the original dataset.*

Ok, then I tried to visualizing the clustering results as a heatmap:
> heatcol<-colorRampPalette(c("Green", "Red"))(32)
> heatmap(x=as.matrix(dat.m), Rowv=as.dendrogram(clust.genes),
+ Colv=as.dendrogram(clust.arrays), col=heatcol)

*Error in .heatmap(x=as.matrix(dat.m),
Rowv=as.dendrogram(clust.genes),  :**row dendrogram ordering gave
**index of wrong length*

*Was that sample of the data make an error with heatmap()??*

*Cheers,*

*Tahani.*

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 24 01:11:47 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 23 Dec 2015 16:11:47 -0800
Subject: [R] Error in heatmap()
In-Reply-To: <CAEwfPz3wKWomLSNGSZWiAwp_vnXK=+58-S35m-s6Ya29QuN0EQ@mail.gmail.com>
References: <CAEwfPz3wKWomLSNGSZWiAwp_vnXK=+58-S35m-s6Ya29QuN0EQ@mail.gmail.com>
Message-ID: <CAGxFJbTtcxov2ZKnE-rksaqdGHR8W-VygdQzpGvW9RBRCXjLDg@mail.gmail.com>

You might try posting on the Bioconductor list instead. They might
have more suitable tools for what you are trying to do. It shoudn't
hurt to ask, anyway ...

If you do this and find something there that better meets your needs,
please post back that information to this list so that others don't
waste time on your query here.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Wed, Dec 23, 2015 at 1:30 PM, Tahani Libya <libya.tahani at gmail.com> wrote:
> *Hi,*
>
> *while I try to analysis my data as the following ,I faced some problem
> with (heatmap()):*
>
>
>> dat<-ReadAffy()
>> dat
>
>
> AffyBatch object
> size of arrays=1164x1164 features (20 kb)
> cdf=HG-U133_Plus_2 (54675 affyids)
> number of samples=10
> number of genes=54675
> annotation=hgu133plus2
> notes=
>> dat2<-rma(dat)
> Background correcting
> Normalizing
> Calculating Expression
>> dat.m<-exprs(dat2)
>
> *The normalized data can be so large that clustering all the genes (or*
> *arrays) becomes impossible. Clustering about 23000 genes takes about 1GB
> of memory, and clustering 54675 genes would consume about more than 4 GBs
> ofmemory, and would not be feasible on a standard Windows workstation.*
>
> *So I tried to sample the data, and this sample*
> *is then clustered. This should convey approximately the same information
> asthe clustering of the whole dataset:*
>> n<-1:nrow(dat.m)
>> n.s<-sample(n, nrow(dat.m)*0.1)
>> dat.sample<-dat.m[n.s,]
>> library(amap)
>> clust.genes<-hcluster(x=dat.sample, method="pearson",
> + link="average")
>> clust.arrays<-hcluster(x=t(dat.sample), method="pearson",
> + link="average")
>
> *The sample size is here 10% of the original dataset.*
>
> Ok, then I tried to visualizing the clustering results as a heatmap:
>> heatcol<-colorRampPalette(c("Green", "Red"))(32)
>> heatmap(x=as.matrix(dat.m), Rowv=as.dendrogram(clust.genes),
> + Colv=as.dendrogram(clust.arrays), col=heatcol)
>
> *Error in .heatmap(x=as.matrix(dat.m),
> Rowv=as.dendrogram(clust.genes),  :**row dendrogram ordering gave
> **index of wrong length*
>
> *Was that sample of the data make an error with heatmap()??*
>
> *Cheers,*
>
> *Tahani.*
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Thu Dec 24 01:28:57 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 23 Dec 2015 16:28:57 -0800
Subject: [R] packaging an R-application
In-Reply-To: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>
References: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>
Message-ID: <567B3C49.4030203@gmail.com>

On 23/12/2015 1:29 PM, Witold E Wolski wrote:
> Dear List,
>
>
> What I am seeking advice for is how to best package an R installation
> with all the packages required?
>
> Scenario:
> I need to deliver an R script which will have quite a bit of package
> dependencies, to packages which are not necessarily stable, are not on
> cran and might dissapear in the near future. It should be possible to
> execute the R-script without any changes during the next 3 years.
>
>
> What I did look into is packrat. But if I am correct it pulls the
> r-packages from cran so if a package was from somewhere else or isn't
> available anymore it will fail.
>
> what I have also found is R on an USB stick:
> http://personal.bgsu.edu/~mrizzo/Rmisc/usbR.htm
>
> Would this work?
>
> Or is there anything on the lines of pyinstaller
> (http://www.pyinstaller.org/) for R?

What OS are you targetting?  If it's for Windows, there are instructions 
in the R Installation and Administration manual for building a custom 
installer.

Duncan Murdoch


From marna.wagley at gmail.com  Thu Dec 24 09:15:59 2015
From: marna.wagley at gmail.com (Marna Wagley)
Date: Thu, 24 Dec 2015 01:15:59 -0700
Subject: [R] to change the size of the line in the plot created in ggplot2
Message-ID: <CAMwU6B3GK2Gymtb2Vx40BX3jmaeiLozqf_KrfafrgNNW+NagVQ@mail.gmail.com>

Hi R user,
I was trying to make a figure for each of four sites. Each site has 7
classes, among the 7 classes, one is average. I am wondering how I can
change the color and size of the line that was average value. I want to
highlight average value.

Similarly, I have been using smooth function therefore I want to highlight
the smoothed line but still I want to show a original data with gray
colour. I tried it different ways but I could not change the line style (or
size) and colour. would you please give me a suggestion on how I can solve
the problem?

I used the following code.

> unique(dat$site)
[1] SiteA SiteC SiteB SiteD
Levels: SiteA SiteB SiteC SiteD

> unique(dat$variable)
[1] D       C       E       Average F       A       B
Levels: A Average B C D E F
A<-ggplot(data=dat, aes(x=Year, y=value, group = variable, colour =
variable)) +geom_line()+theme_bw()+stat_smooth() A+facet_wrap(~ site,
ncol=2)Thanks

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Thu Dec 24 13:10:03 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Thu, 24 Dec 2015 12:10:03 +0000
Subject: [R] to change the size of the line in the plot created in
 ggplot2
Message-ID: <248E6FA047A8C746BA491485764190F53715F14D@ESESSMB207.ericsson.se>

You can do it this way, for example:

geom_line(linetype="dashed", size=1, colour="blue")

Further info at:

http://docs.ggplot2.org/current/geom_line.html


--
GG



	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Thu Dec 24 13:39:49 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 24 Dec 2015 13:39:49 +0100
Subject: [R] packaging an R-application
In-Reply-To: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>
References: <CAAjnpdhQMXtRyGBteMb8-oNG4u5ymT4CqjeVLNBduXRt7-NM8w@mail.gmail.com>
Message-ID: <D90D2526-BAFD-4CE7-9DB3-786F80E8FE28@gmail.com>

It sounds like it could be the sort of issue that Docker/Rocker is trying to address. Someone called Dirk might chime in on this.

-pd

> On 23 Dec 2015, at 22:29 , Witold E Wolski <wewolski at gmail.com> wrote:
> 
> Dear List,
> 
> 
> What I am seeking advice for is how to best package an R installation
> with all the packages required?
> 
> Scenario:
> I need to deliver an R script which will have quite a bit of package
> dependencies, to packages which are not necessarily stable, are not on
> cran and might dissapear in the near future. It should be possible to
> execute the R-script without any changes during the next 3 years.
> 
> 
> What I did look into is packrat. But if I am correct it pulls the
> r-packages from cran so if a package was from somewhere else or isn't
> available anymore it will fail.
> 
> what I have also found is R on an USB stick:
> http://personal.bgsu.edu/~mrizzo/Rmisc/usbR.htm
> 
> Would this work?
> 
> Or is there anything on the lines of pyinstaller
> (http://www.pyinstaller.org/) for R?
> 
> 
> Thank you
> 
> -- 
> Witold Eryk Wolski
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Thu Dec 24 13:45:22 2015
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 24 Dec 2015 13:45:22 +0100
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <1790333972.2430395.1450947829967.JavaMail.yahoo@mail.yahoo.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
	<1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
	<95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>
	<1660893852.2256439.1450884141115.JavaMail.yahoo@mail.yahoo.com>
	<1790333972.2430395.1450947829967.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <B34BF210-DC08-48C5-B01D-C953CA432F82@gmail.com>

Two ideas:

a: Take log of your data and compare with normal distr.

b: Use log="xy" as a graphical parameter.

Otherwise, you're on your own.

-pd

> On 24 Dec 2015, at 10:03 , mohsen hs <mohsenhs82 at yahoo.com> wrote:
> 
> Hi Peter,
> 
> Thanks once again for your kind reply.
> 
> One quick question, could you please guide me and let me know how I can get the similar qq plot(log-log scale) that I get from qqcomp, from qqPlotCensored function(It is similar to qqPlot, and available in EnvStats   http://www.inside-r.org/node/218933 ).
> 
> Thanks a lot.
> 
> Cheers
> Mohsen
> 
> 
>  
> MHS
> 
> 
> On Wednesday, December 23, 2015 6:52 PM, mohsen hs <mohsenhs82 at yahoo.com> wrote:
> 
> 
> Hi Peter and Rolf
> 
> Thank you for your time and replying me. It makes sense now. I sincerely appreciate that.
> 
> Cheers
> Mohsen
>  
> 
> 
> 
> On Tuesday, December 22, 2015 10:08 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
> 
> > On 22 Dec 2015, at 07:30 , mohsen hs via R-help <r-help at r-project.org> wrote:
> > 
> > The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp and the left one is from qqPlot. Titles might be incorrect.
> 
> They never arrived, but your data weren't actually needed. The crucial missing information was the packages used. This will do:
> 
> > library(EnvStats); library(fitdistrplus)
> > serving <- exp(rnorm(100))
> > qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)
> 
> > 
> > fitln <- fitdist(serving,"lnorm",method="mle")
> > qqcomp(fitln)
> 
> 
> The difference is quite clearly that qqPlot is doing a QQ-plot of log(serving) vs. normal quantiles, whereas qqcomp plots serving itself against lognormal quantiles. So the former is pretty much equal to the latter on a log-log scale.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From janka.vanschoenwinkel at uhasselt.be  Thu Dec 24 14:03:12 2015
From: janka.vanschoenwinkel at uhasselt.be (Janka VANSCHOENWINKEL)
Date: Thu, 24 Dec 2015 14:03:12 +0100
Subject: [R] Weighted demean by group on only a selection of the dataset
Message-ID: <CAHymutKL3xu24mswefqRAS9Mur-f6AZrk8WH9_h-jiFNLEMb3g@mail.gmail.com>

Dear colleagues,

I am trying to find a simple code to demean
1) only certain values of a dataset,
2) by group
3) and in a weighted fasion.

Currently, I can only demean all the numeric variables in the dataset:

Data[,sapply(Data,  is.numeric)] <- apply(Data[sapply(Data,
is.numeric)], 2, function(x) scale(x, scale = FALSE))

Assume that my dataset looks like this:
Country<- c('BE','BE','DE','GR','IT','ES','DE','NL')
Landvalue<- c(21000, 23400, 26800, 15000,18000,23000,19000,23000)
Temperature_spring <- c('15','16','14','18','23','21','12','15')
Temperature_summer <- c('25','18','19','23','24','22','15','19')
Temperature_autumn <- c('14','12','12','10','20','20','11','13')
Temperature_winter <- c('9','4','12','14','15','13','17','12')
Weight<-c('5','20','3','2','15','21','13','8')
Data <- data.frame(Country, Landvalue,
Temperature_spring,Temperature_summer,
Temperature_autumn,Temperature_winter, Weight)


Now imagine I only want to demean the temperature-variables, grouped
by country and weighted by weight. With grouped by country I mean that
I want to subtract only the mean of Belgium from an observation in
Belgium.

Does somebody know how to add the three functions to the code line I
already have? Or if this does not work, what code should I use?

Thank you very much and have a nice Christmas!


From dwinsemius at comcast.net  Thu Dec 24 15:40:22 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 24 Dec 2015 06:40:22 -0800
Subject: [R] Weighted demean by group on only a selection of the dataset
In-Reply-To: <CAHymutKL3xu24mswefqRAS9Mur-f6AZrk8WH9_h-jiFNLEMb3g@mail.gmail.com>
References: <CAHymutKL3xu24mswefqRAS9Mur-f6AZrk8WH9_h-jiFNLEMb3g@mail.gmail.com>
Message-ID: <D40A32F5-B574-4476-8A86-6689C8F940B8@comcast.net>


> On Dec 24, 2015, at 5:03 AM, Janka VANSCHOENWINKEL <janka.vanschoenwinkel at uhasselt.be> wrote:
> 
> Dear colleagues,
> 
> I am trying to find a simple code to demean
> 1) only certain values of a dataset,
> 2) by group
> 3) and in a weighted fasion.
> 
> Currently, I can only demean all the numeric variables in the dataset:
> 
> Data[,sapply(Data,  is.numeric)] <- apply(Data[sapply(Data,
> is.numeric)], 2, function(x) scale(x, scale = FALSE))
> 
> Assume that my dataset looks like this:
> Country<- c('BE','BE','DE','GR','IT','ES','DE','NL')
> Landvalue<- c(21000, 23400, 26800, 15000,18000,23000,19000,23000)
> Temperature_spring <- c('15','16','14','18','23','21','12','15')
> Temperature_summer <- c('25','18','19','23','24','22','15','19')
> Temperature_autumn <- c('14','12','12','10','20','20','11','13')
> Temperature_winter <- c('9','4','12','14','15','13','17','12')
> Weight<-c('5','20','3','2','15','21','13','8')
> Data <- data.frame(Country, Landvalue,
> Temperature_spring,Temperature_summer,
> Temperature_autumn,Temperature_winter, Weight)

Do note that only the `Landvalue` column is numeric above. You would need to us as.numeric on the vectors that you are nominating for processing below.

> 
> 
> Now imagine I only want to demean the temperature-variables, grouped
> by country and weighted by weight. With grouped by country I mean that
> I want to subtract only the mean of Belgium from an observation in
> Belgium.
> 

Generally when one wants to use multiple columns in a calculation with grouping, the method needs to be along the lines of :

Data[ , grepl("Temp", Data) ] <- 
       lapply( split(Data, Data$Country), FUN= ...) 


> Does somebody know how to add the three functions to the code line I
> already have? Or if this does not work, what code should I use?
> 
> Thank you very much and have a nice Christmas!
> 
> 
-- 

David Winsemius
Alameda, CA, USA


From marongiu.luigi at gmail.com  Thu Dec 24 17:12:13 2015
From: marongiu.luigi at gmail.com (Luigi Marongiu)
Date: Thu, 24 Dec 2015 16:12:13 +0000
Subject: [R] interpolation using R for PCR quantification
Message-ID: <CAMk+s2SspgLwGrm7F1D_ytd87bkx_A22dYM4d8JQBzWmR-smOQ@mail.gmail.com>

Dear all,
I am a newbie in interpolation using R and I would like to learn
better the procedure.
I am applying interpolation to quantify nucleic acid targets using an
assay known as PCR. To do this, I have two sets of variables: standard
of known concentrations and query for which I need to identify the
concentration.
For each variable I have the output of the assay (cyc) and an
approximation of the concentration expressed in relation to the
concentration of the standard, so 5 means 10^5 etc.
Given that the actual concentration of the standards is given in the
'con' variable, the relation is that x=log10(con) and y = cyc, as
represented in the first plot of the following example. In black are
depicted the standard and in red the query samples.

Now, to obtain interpolation the only function that i know is
approx(). The first problem is that I need to switch the x-y variables
because the values specifying where interpolation is to take place go
in the 'xout' parameter and I have y outputs. If I maintain the
original x/y orientation the output from approx() is empty. How can I
keep the original layout? I must admit, anyhow, that the construct
x=log10(con) and y = cyc is an artifact of the PCR analysis, since the
independent variable is indeed the cyc value.

The second problem I am facing -- and the most important -- is that
the output seems weird. The values I get are simply the concentration
input as such and not calculated by interpolation. In the example, the
output I obtain is:
 [1]       NA 1480.600 1480.600  148.060  202.319  148.060   14.806
14.806   14.806
[10]       NA
the first and last value are OK because the cyc values are outside the
dynamic range under evaluation, but the only value that seems genuine
is 202.319, the others are just the values I placed in the 'con'
variable. For instance the second and third values have cyc = 26.992
and 26.961 and yet they are both assigned to 1480.600.
What I am getting wrong?
Thank you (and merry Christmas!)
L


>>>
dil <- c(5,    5,    5,    5,    4,    4,    4,    3,    3,    3,
3,    3,    2,    2,    2,    2,    2,    2,    1,    1,    1,    1,
 1,    1,    1)
sam <- c(0,    0,    0,    1,    0,    0,    0,    0,    0,    0,
1,    1,    0,    0,    0,    1,    1,    1,    0,    0,    0,    1,
 1,    1,    1)
cyc <- c(20.787,    20.494,    20.475,    20.189,    23.991,
24.084,    23.863,    26.298,    28.007,    27.413,    26.992,
26.961,    31.363,    30.979,    32.013,    31.004,    30.576,
31.195,    35.219,    34.096,    38.088,    34.934,    35.101,
35.206,    38.366)
con <- c(148060,    148060,    148060,    NA,    14806,    14806,
14806,    1480.6,    1480.6,    1480.6,    NA,    NA,    148.06,
148.06,    148.06,    NA,    NA,    NA,    14.806,    14.806,
14.806,    NA,    NA,    NA,    NA)
df <- data.frame(dil, sam, cyc, con)

std <- subset(df, sam == 0)
qry <- subset(df, sam == 1)

plot(std$cyc ~ std$dil)
points(qry$dil, qry$cyc, col ="red")

Q <- approx(x=std$cyc, y=log10(std$con), xout=qry$cyc,
method="linear", rule = 1)
(10^(Q$y))

plot(std$cyc, y=log10(std$con))
abline(lm(log10(df$con) ~ df$cyc))
abline(v=qry$cyc, col="blue")


From marna.wagley at gmail.com  Thu Dec 24 18:06:48 2015
From: marna.wagley at gmail.com (Marna Wagley)
Date: Thu, 24 Dec 2015 09:06:48 -0800
Subject: [R] to change the size of the line in the plot created in
	ggplot2
In-Reply-To: <248E6FA047A8C746BA491485764190F53715F14D@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F53715F14D@ESESSMB207.ericsson.se>
Message-ID: <CAMwU6B2o7m3xf7_F2iwBtj0X2WVxyB6bdhSY_kEtXaDqrMHrfQ@mail.gmail.com>

Hi Giorgio,
Thank you very much for the code and the link. I read it and also used but
this code changed the line into "dashed" for all variables. As I mentioned
earlier, I wanted to change for only one variable among 7 variables (for
only one variable).
Thanks

MW

On Thu, Dec 24, 2015 at 4:10 AM, Giorgio Garziano <
giorgio.garziano at ericsson.com> wrote:

> You can do it this way, for example:
>
> geom_line(linetype="dashed", size=1, colour="blue")
>
> Further info at:
>
> http://docs.ggplot2.org/current/geom_line.html
>
>
> --
> GG
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Thu Dec 24 18:48:20 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Thu, 24 Dec 2015 17:48:20 +0000
Subject: [R] to change the size of the line in the plot created in
 ggplot2
Message-ID: <248E6FA047A8C746BA491485764190F53715F257@ESESSMB207.ericsson.se>

Hi Marna,

I prepared this toy example that should help you.

x <- seq(1:100)
y <- x*x
avg <- mean(y)
avg.v <- rep(avg,100) # your average column data
df <- as.data.frame(cbind(x, y, avg.v))

library(ggplot2)
ggplot(data=df[,-3], aes(x=x, y=y)) + geom_line() +
  geom_line(data=df[,c(1,3)], color='blue', aes(x=x, y=avg))


Best,

--
GG

	[[alternative HTML version deleted]]


From matteo.richiardi at gmail.com  Thu Dec 24 20:18:31 2015
From: matteo.richiardi at gmail.com (Matteo Richiardi)
Date: Thu, 24 Dec 2015 19:18:31 +0000
Subject: [R] assigning values to elements of matrixes
In-Reply-To: <591874FC-7B71-46D9-8C9A-1AB7919D0276@kit.edu>
References: <CABSrU1LpjLiEZ1HKcbVcdAViiw+UbN0FtUpXyqsNYt3ukNJ8Ag@mail.gmail.com>
	<591874FC-7B71-46D9-8C9A-1AB7919D0276@kit.edu>
Message-ID: <CABSrU1JaNOA128OuTZrFw8MyxKYL-fMsw+V5cXvgUkih7z8Z3w@mail.gmail.com>

Dear all,
Thanks very much for your help. This completely clarifies my question.
Mayteo
Il giorno 23/dic/2015 17:05, "Anthoni, Peter (IMK)" <peter.anthoni at kit.edu>
ha scritto:

> Hi,
>
> How about the following:
> foo2 <- function(s,i,j,value)
> {
>   M = get(paste("M_",s,sep=""))
>   M[i,j] = value
>   assign(paste("M_",s,sep=""),M, envir = .GlobalEnv)
> }
>
> foo2("a",1,2,15)
>
> cheers
> Peter
>
> > On 23 Dec 2015, at 09:44, Matteo Richiardi <matteo.richiardi at gmail.com>
> wrote:
> >
> > I am following the example I find on ?assign:
> >
> > a <- 1:4
> > assign("a[1]", 2)
> >
> > This appears to create a new variable named "a[1]" rather than
> > changing the value of the vector.
> >
> > Am I missing something here? How can I assign a value to a specified
> > element of a vector/matrix?
> >
> > Of course, my problem is slightly more involved, but I guess the above
> > is its core. For those interested, I have the two matrixes
> > M_a <- matrix(0,10,10)
> > M_b <- matrix(0,10,10)
> >
> > I want to have a function that assigns a value to a specific element
> > of one of the matrix, as in
> > foo <- function(s,i,j,value){
> >  assign(paste("M_",s)[i,j],value)
> > }
> >
> > This however does not work:
> >
> > foo('a',1,1,1)
> >
> > Error in paste("M_", s)[1, j] : incorrect number of dimensions
> >
> > Following the ?assign help, I tried
> >
> > foo2 <- function(s,i,j,value){
> >  assign(paste("M_",s,"[i,j]"),value, envir = .GlobalEnv)
> > }
> >
> > but this produces the problem I described above (namely, a new
> > variable is created, rather than replacing the specified element of
> > the matrix).
> >
> > I know that in this example I could simply pass the matrix as an
> > argument to the function, but I am interested in understanding how
> > 'assign' work.
> >
> > Many thanks for your help.
> >
> > Matteo
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
>

	[[alternative HTML version deleted]]


From mohsenhs82 at yahoo.com  Thu Dec 24 10:03:49 2015
From: mohsenhs82 at yahoo.com (mohsen hs)
Date: Thu, 24 Dec 2015 09:03:49 +0000 (UTC)
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <1660893852.2256439.1450884141115.JavaMail.yahoo@mail.yahoo.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
	<1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
	<95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>
	<1660893852.2256439.1450884141115.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <1790333972.2430395.1450947829967.JavaMail.yahoo@mail.yahoo.com>

Hi Peter,
Thanks once again for your kind reply.
One quick question, could you please guide me and let me know how I can get the similar qq plot(log-log scale) that I get from qqcomp,?from?qqPlotCensored?function(It is similar to qqPlot, and available in EnvStats ??http://www.inside-r.org/node/218933?).
Thanks a lot.
CheersMohsen

?MHS 

    On Wednesday, December 23, 2015 6:52 PM, mohsen hs <mohsenhs82 at yahoo.com> wrote:
 

 Hi Peter and Rolf
Thank you for your time and replying me. It makes sense now. I sincerely appreciate that.
CheersMohsen?
 

    On Tuesday, December 22, 2015 10:08 PM, peter dalgaard <pdalgd at gmail.com> wrote:
 

 
> On 22 Dec 2015, at 07:30 , mohsen hs via R-help <r-help at r-project.org> wrote:
> 
> The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp and the left one is from qqPlot. Titles might be incorrect.

They never arrived, but your data weren't actually needed. The crucial missing information was the packages used. This will do:

> library(EnvStats); library(fitdistrplus)
> serving <- exp(rnorm(100))
> qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)
> 
> fitln <- fitdist(serving,"lnorm",method="mle")
> qqcomp(fitln)

The difference is quite clearly that qqPlot is doing a QQ-plot of log(serving) vs. normal quantiles, whereas qqcomp plots serving itself against lognormal quantiles. So the former is pretty much equal to the latter on a log-log scale.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com










   

  
	[[alternative HTML version deleted]]


From mohsenhs82 at yahoo.com  Thu Dec 24 18:40:31 2015
From: mohsenhs82 at yahoo.com (mohsen hs)
Date: Thu, 24 Dec 2015 17:40:31 +0000 (UTC)
Subject: [R] [FORGED]  qqPlot vs qqcomp
In-Reply-To: <B34BF210-DC08-48C5-B01D-C953CA432F82@gmail.com>
References: <717651811.463326.1450432658330.JavaMail.yahoo.ref@mail.yahoo.com>
	<717651811.463326.1450432658330.JavaMail.yahoo@mail.yahoo.com>
	<5678A5CD.3000302@auckland.ac.nz>
	<1819934632.1694446.1450765811908.JavaMail.yahoo@mail.yahoo.com>
	<95A9E6BE-3C6D-4225-BC0C-7C0950362A02@gmail.com>
	<1660893852.2256439.1450884141115.JavaMail.yahoo@mail.yahoo.com>
	<1790333972.2430395.1450947829967.JavaMail.yahoo@mail.yahoo.com>
	<B34BF210-DC08-48C5-B01D-C953CA432F82@gmail.com>
Message-ID: <204939070.2592192.1450978831558.JavaMail.yahoo@mail.yahoo.com>

Hi Peter
Thanks for your reply. I appreciate that.Murray Christmas.
CheersMohsen 

    On Thursday, December 24, 2015 4:15 PM, peter dalgaard <pdalgd at gmail.com> wrote:
 

 Two ideas:

a: Take log of your data and compare with normal distr.

b: Use log="xy" as a graphical parameter.

Otherwise, you're on your own.

-pd

> On 24 Dec 2015, at 10:03 , mohsen hs <mohsenhs82 at yahoo.com> wrote:
> 
> Hi Peter,
> 
> Thanks once again for your kind reply.
> 
> One quick question, could you please guide me and let me know how I can get the similar qq plot(log-log scale) that I get from qqcomp, from qqPlotCensored function(It is similar to qqPlot, and available in EnvStats? http://www.inside-r.org/node/218933 ).
> 
> Thanks a lot.
> 
> Cheers
> Mohsen
> 
> 
>? 
> MHS
> 
> 
> On Wednesday, December 23, 2015 6:52 PM, mohsen hs <mohsenhs82 at yahoo.com> wrote:
> 
> 
> Hi Peter and Rolf
> 
> Thank you for your time and replying me. It makes sense now. I sincerely appreciate that.
> 
> Cheers
> Mohsen
>? 
> 
> 
> 
> On Tuesday, December 22, 2015 10:08 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
> 
> > On 22 Dec 2015, at 07:30 , mohsen hs via R-help <r-help at r-project.org> wrote:
> > 
> > The above command gives me a differentplot. I am not sure what part I am doing wrong. I appreciate your time forconsidering my request and your feedback is highly appreciated. Please find the plots attached. The right one is from qqcomp and the left one is from qqPlot. Titles might be incorrect.
> 
> They never arrived, but your data weren't actually needed. The crucial missing information was the packages used. This will do:
> 
> > library(EnvStats); library(fitdistrplus)
> > serving <- exp(rnorm(100))
> > qqPlot ( serving, dist ="lnorm", estimate.params = TRUE, add.line = TRUE)
> 
> > 
> > fitln <- fitdist(serving,"lnorm",method="mle")
> > qqcomp(fitln)
> 
> 
> The difference is quite clearly that qqPlot is doing a QQ-plot of log(serving) vs. normal quantiles, whereas qqcomp plots serving itself against lognormal quantiles. So the former is pretty much equal to the latter on a log-log scale.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com










  
	[[alternative HTML version deleted]]


From t.rees.706713 at swansea.ac.uk  Thu Dec 24 14:41:21 2015
From: t.rees.706713 at swansea.ac.uk (REES T. (706713))
Date: Thu, 24 Dec 2015 13:41:21 +0000
Subject: [R] Right censored data, abundant in zeros for regression analysis.
Message-ID: <E1701231840BD64AB541311E63C1C7658A46DA83@ISS-MBX01.tawe.swan.ac.uk>

Hi there,

Firstly forgive me if this seem obvious, if there is existing literature on this i can't find it.

I am looking at conditioning to stimuli and there in the time taken to perform a certain task.

The IV for this data is Conditioning periods ranging from 1-34 periods and the DV is the time taken for the behavioral response to occur 0-300s.
I am aware that this could simply be looked at through a simple linear regression, however due to the nature of conditioning there is an abundance of zeros in the data.
On top of this the response time data is right censored (i believe), in that they were given a five minute period to respond after this five minute period (300 seconds) the conditioning period was terminated, so no more data was recorded.

Attached is the data (in .csv format) for time spent out, 0 indicated no time out and 300 indicated all time out during the 5 minutes.

I have considered looking at zero-inflated censored regressions and others similar analysis but I cannot find an analysis that suits the data I have and actually works.
So what is the best analysis method to deal with this data?
Admittedly i could be completely missing the target, if that's the case please feel free to say so. Any help with the route that I should go down here would be much appreciated, even if it is blindingly obvious.

Sincerely

Tom Rees

From jvadams at usgs.gov  Thu Dec 24 20:50:30 2015
From: jvadams at usgs.gov (Adams, Jean)
Date: Thu, 24 Dec 2015 13:50:30 -0600
Subject: [R] (no subject)
In-Reply-To: <CAN2xGJZ7y_8=ow+-kGfWg-gSyBafOy1HJUiw8mbhyoK5MX5D8w@mail.gmail.com>
References: <CAN2xGJZ7y_8=ow+-kGfWg-gSyBafOy1HJUiw8mbhyoK5MX5D8w@mail.gmail.com>
Message-ID: <CAN5YmCEm1OSz0CJsBfu+zVENJpqBe9YxP2T8xMQPTQqct2PK0w@mail.gmail.com>

Excellent job providing example data and a desired result.

This code works on the example you provided.  Hope it helps.

Jean

# reshape the info data frame
library(tidyr)
info2 <- gather(myinfo, set, val, -game)
info2$set <- as.numeric(gsub("[[:alpha:]]", "", info2$set))

# add a new column to the x data frame
y <- t(x[,grep("char", names(x))])
newx <- x
newx$char <- row(y)[y==1]

# merge and define winner
res <- merge(newx, info2)
res$winner <- with(res, ifelse(char==val, 1, 0))
res


On Wed, Dec 23, 2015 at 3:35 PM, Dimitri Liakhovitski <
dimitri.liakhovitski at gmail.com> wrote:

> Merry upcoming Christmas - for those of us who celebrate it!
>
> # I have data frame x.
> x <- data.frame(game = c(rep(1, 4), rep(2, 4)), set = rep(c(1,1,2,2),
> 2), player = rep(1:2, 4),
>                 char1 = c(1,0,0,0,0,0,0,1), char2 =
> c(0,0,1,0,0,1,0,0), char3 = c(0,1,0,1,0,0,0,0),
>                 char4 = c(0,0,0,0,1,0,1,0))
> x
> # There are several games (2 here). Each game has several sets (2
> here. In each set participate
> # several players (2 here).
> # Each player possesses 1 or 4 possible characteristics.
> # For example, in game 1, set 1, player 1 has characteristic 1 and player
> 2 -
> # characteristic 3
>
> # I also have data frame myinfo:
> (myinfo <- data.frame(game = 1:2, set1 = c(3, 4), set2 = c(2, 1)))
> # It tells me:
> # in game 1, set 1 the winner was the player with characteristic 3
> # in game 1, set 2 the winner was the player with characteristic 2, etc.
>
> # I need to merge the 2 to produce the result below.
> # I just need an additional column that - for each game and each set -
> # has a 1 in the row of the player who had the winning characteristic
> # (as per myinfo) and has a 0 otherwise.
>
> result <- x
> result$winner <- c(0, 1, 1, 0, 1, 0, 0, 1)
> result
>
> # I have written a long loop that loops through each set of each game,
> identifies
> # which characteristic wins in myinfo, and puts a 1 against the winning
> row.
> # But it's taking forever. Could it be done faster? Thanks a lot!
>
> # Important: In my real game the number of players could be more than
> 2 and so can
> # the number of games and the number of sets per game.
> # However, we can assume that the number of sets per game is always the
> same,
> # and the number of players per set is always the same.
>
>
> --
> Dimitri Liakhovitski
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Thu Dec 24 22:09:15 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 24 Dec 2015 13:09:15 -0800
Subject: [R] Right censored data,
	abundant in zeros for regression analysis.
In-Reply-To: <E1701231840BD64AB541311E63C1C7658A46DA83@ISS-MBX01.tawe.swan.ac.uk>
References: <E1701231840BD64AB541311E63C1C7658A46DA83@ISS-MBX01.tawe.swan.ac.uk>
Message-ID: <CAGxFJbRsmJbVfxTcHYhG1Qbfyd4GtXQ0Bg3K3Wk=TNF6Sy+B6A@mail.gmail.com>

Strictly speaking, this is a statistical analysis issue, not an R
question, although I grant you that the intersection of the two is
nonempty. Nevertheless, I would suggest that you post on a statistics
list like stats.stackexchange.com . In fact, because the issue of how
to effectively deal with such data appears to be far from trivial, you
might better seek local statistical advice. Once you have decided
**what** to do, you could then come back here to inquire about R
packages and procedures to do it -- if you are unable to first find
something through internet search of course.

Cheers,
Bert




Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 24, 2015 at 5:41 AM, REES T. (706713)
<t.rees.706713 at swansea.ac.uk> wrote:
> Hi there,
>
> Firstly forgive me if this seem obvious, if there is existing literature on this i can't find it.
>
> I am looking at conditioning to stimuli and there in the time taken to perform a certain task.
>
> The IV for this data is Conditioning periods ranging from 1-34 periods and the DV is the time taken for the behavioral response to occur 0-300s.
> I am aware that this could simply be looked at through a simple linear regression, however due to the nature of conditioning there is an abundance of zeros in the data.
> On top of this the response time data is right censored (i believe), in that they were given a five minute period to respond after this five minute period (300 seconds) the conditioning period was terminated, so no more data was recorded.
>
> Attached is the data (in .csv format) for time spent out, 0 indicated no time out and 300 indicated all time out during the 5 minutes.
>
> I have considered looking at zero-inflated censored regressions and others similar analysis but I cannot find an analysis that suits the data I have and actually works.
> So what is the best analysis method to deal with this data?
> Admittedly i could be completely missing the target, if that's the case please feel free to say so. Any help with the route that I should go down here would be much appreciated, even if it is blindingly obvious.
>
> Sincerely
>
> Tom Rees
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From giorgio.garziano at ericsson.com  Fri Dec 25 10:34:30 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Dec 2015 09:34:30 +0000
Subject: [R] to change the size of the line in the plot created in
 ggplot2
Message-ID: <248E6FA047A8C746BA491485764190F53715F30E@ESESSMB207.ericsson.se>

Hi Marna,

here is another example that should appear more similar to your scenario
than my previous one.

x <- seq(1:100)

y1 <- x*x
g1 <- rep("y1", 100)
df1 <- as.data.frame(cbind(x, y1), stringsAsFactors=FALSE)
df1 <- as.data.frame(cbind(df1, g1))
colnames(df1)<- c("x", "value", "variable")

y2 <- y1+1500
g2 <- rep("y2", 100)
df2 <- as.data.frame(cbind(x, y2), stringsAsFactors=FALSE)
df2 <- as.data.frame(cbind(df2, g2))
colnames(df2)<- c("x", "value", "variable")

y3 <- y1+6000
g3 <- rep("y3", 100)
df3 <- as.data.frame(cbind(x, y3), stringsAsFactors=FALSE)
df3 <- as.data.frame(cbind(df3, g3))
colnames(df3)<- c("x", "value", "variable")

avg <- (y1+y2+y3)/3
df4 <- as.data.frame(cbind(x, avg))
g4 <- rep("average", 100)
df4 <- as.data.frame(cbind(df4, g4))
colnames(df4) <- c("x", "value", "variable")

df <- data.frame(rbind(df1, df2, df3, df4))

# this is the data to start with ggplot()
df

# the df rows where the average value is stored
sel <- which(df[,"variable"]=="average")

library(ggplot2)

ggplot(data = df[-sel,], aes(x=x, y=value, group=variable)) + geom_line() +
  geom_line(data = df[sel,], aes(x=x, y=value, group=variable), size=0.5, linetype="dashed", color="blue")


Merry Christmas,

--
GG


	[[alternative HTML version deleted]]


From tr206 at kent.ac.uk  Fri Dec 25 20:27:17 2015
From: tr206 at kent.ac.uk (T.Riedle)
Date: Fri, 25 Dec 2015 19:27:17 +0000
Subject: [R] creating a xts object
Message-ID: <28a8f8a92852431297f8985df644a5a3@ex13-live-mbn1.ad.kent.ac.uk>

Dear all,
I am working with the rugarch package which requires xts objects. I have installed the xts package to convert my dataset from .csv to a xts object but it does not work. My code looks as follows:

VaRxts <- as.xts(VaR,order.by ="Date")
Error in xts(x, order.by = order.by, frequency = frequency, .CLASS = "double",  :
  order.by requires an appropriate time-based object

Could anyone help me with that? How can I create a xts object in R?

Thank you very much in advance.

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Fri Dec 25 20:38:22 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Dec 2015 19:38:22 +0000
Subject: [R] creating a xts object
Message-ID: <248E6FA047A8C746BA491485764190F53715F3ED@ESESSMB207.ericsson.se>

Some hints at the following link where the "order.by requires an appropriate time-based object" error is commented.

http://stackoverflow.com/questions/23224142/converting-data-frame-to-xts-order-by-requires-an-appropriate-time-based-object


--
GG

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Fri Dec 25 21:21:41 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Fri, 25 Dec 2015 20:21:41 +0000
Subject: [R] need for help for solving operations in a vector
Message-ID: <248E6FA047A8C746BA491485764190F53715F414@ESESSMB207.ericsson.se>

I think that the rle() function may help you to tackle the problem in a more general way.

https://stat.ethz.ch/R-manual/R-devel/library/base/html/rle.html

Using William's suggested series:

x <- c(2,2,3,4,4,4,4,5,5,5,3,1,1,0,0,0,1,1,1)

> x
[1] 2 2 3 4 4 4 4 5 5 5 3 1 1 0 0 0 1 1 1

rle.x <- rle(x)
rle.x
Run Length Encoding
  lengths: int [1:8] 2 1 4 3 1 2 3 3
  values : num [1:8] 2 3 4 5 3 1 0 1

And then you can apply diff() to rle.x$values while keeping in mind the run lengths (rle.x$lengths).

Good luck,

--
GG


	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Fri Dec 25 23:38:43 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Fri, 25 Dec 2015 14:38:43 -0800 (PST)
Subject: [R] to change the size of the line in the plot created in
 ggplot2
In-Reply-To: <248E6FA047A8C746BA491485764190F53715F30E@ESESSMB207.ericsson.se>
References: <248E6FA047A8C746BA491485764190F53715F30E@ESESSMB207.ericsson.se>
Message-ID: <alpine.BSF.2.00.1512251416510.16827@pedal.dcn.davis.ca.us>

Giorgio... beware of using cbind to form data frames from vectors.
It is inefficient in use of memory, doesn't set column names, and
will convert all columns to character if you do it with any vector 
columns that are of character type. Below are three revamps of this 
example.

Using cbind with data frames as input fixes most of these problems,
but you are still better off using the "data.frame" function in most 
cases.

#--- base R, simplified
x <- seq( 1:100 )

y1 <- x * x
df1 <- data.frame( x = x, value = y1, stringsAsFactors = FALSE )
# scalar automatically repeated to number of rows
df1$variable <- "y1"

y2 <- y1 + 1500
df2 <- data.frame(x=x, value=y2, stringsAsFactors=FALSE)
df2$variable <- "y2"

y3 <- y1 + 6000
df3 <- data.frame( x = x, value = y3, stringsAsFactors=FALSE)
df3$variable <- "y3"

avg <- ( y1 + y2 + y3 ) / 3
df4 <- data.frame( x = x, value = avg, variable = g4 )
df4$variable <- "average"

df <- rbind( df1, df2, df3, df4 )

df$variable <- factor( df$variable
                      , levels=c( "y1", "y2", "y3", "average" ) )

# this is the data to start with ggplot()
df

library(ggplot2)

# this example you made goes to all the effort to put the data into one
# data frame, and then fails to make use of the automatic legend creation 
# feature of ggplot
ggplot( data = df[ -sel, ]
       , aes( x=x, y=value, group=variable ) ) +
   geom_line() +
   geom_line( data = df[ sel, ]
            , mapping = aes( x=x, y=value, group=variable )
            , size=0.5, linetype="dashed", color="blue" )

# the output of this one is easier to interpret
ggplot( df
       , aes( x=x
            , y=value
            , colour=variable
            , linetype=variable
            , size=variable )
            ) +
   geom_line() +
   scale_colour_manual( name = "Curve"
                      , values = c( "red", "green", "blue", "black" ) ) +
   scale_linetype_manual( name = "Curve", values = c( 1, 1, 1, 2 ) ) +
   scale_size_manual( name="Curve", values = c( 1, 1, 1, 0.5 ) )

#----- base R data manipulation, a little more sophisticated

library( ggplot2 )

# There are better ways to make these kinds of y1, y2 etc dependent 
# variables

df0 <- data.frame( x = seq( 1:100 ) )
df0 <- within( df0
              , {
                 y1 <- x * x
                 y2 <- y1 + 1500
                 y3 <- y1 + 6000
                }
              )
# rowMeans can work with an arbitrary number of columns
df0$average <- rowMeans( df0[ , c( "y1", "y2", "y3" ) ] )
# take a look
df0
# in "wide" format...

# base R has the reshape function to convert to "long" format... the 
# arguments are a bit complicated to remember though (compare with next 
# example)
vars <- c( "y1", "y2", "y3", "average" )
df <- reshape( df0
              , idvar="x"
              , varying = vars
              , v.names="value"
              , times=vars
              , timevar = "variable"
              , direction = "long" )

# convert character labels to factor
# the levels of the factor define the order in which colors and linetypes 
# are specified
df$variable <- factor( df$variable, levels = vars )

# using the same input data for colour, linetype and size causes the three 
# legends to be combined
ggplot( df
       , aes( x=x
            , y=value
            , colour=variable
            , linetype=variable
            , size=variable
            )
       ) +
   geom_line() +
   scale_colour_manual( name = "Curve"
                      , values = c( "red", "green", "blue", "black" ) ) +
   scale_linetype_manual( name = "Curve", values = c( 1, 1, 1, 2 ) ) +
   scale_size_manual( name="Curve", values = c( 1, 1, 1, 0.5 ) )

#---- Nonstandard syntax from dplyr, easier to remember and use on the fly
#     but requires some more contributed packages

library( ggplot2 )
library( dplyr )
library( tidyr )

# dplyr "pipes" data from one function to the next
# read about it in the vignettes for the "dplyr" and "magrittr" packages
df0 <- (   data.frame( x = seq( 1:100 ) )
        %>% mutate( y1 = x * x
                  , y2 = y1 + 1500
                  , y3 = y1 + 6000
                  )
        )
# make a note of all names except the first column in this case
vars <- names( df0 )[ -1 ]
# if you need to refer to the whole dataset in the pipeline of functions,
# the "." refers to the data frame as it exists at that point.
df0 <- (   df0
        %>% mutate( average = rowMeans( .[ , vars ] ) )
        )
# all names with average too
allvars <- names( df0 )[ -1 ]

# "gather" all columns except x into a "value" column, with labels in 
# "variable" column and make variable column into a factor with specified 
# sequence of levels
df <- (   df0
       %>% gather( variable, value, -x )
       %>% mutate( variable = factor( variable, levels = allvars ) )
       )

# define some colours and linetype values in desired order corresponding 
# to levels of "variable"
colv <- c( rainbow( length( vars ) ), "black" )
lntypv <- c( rep( 1, length( vars ) ), 2 )
szv <- c( rep( 1, length( vars ) ), 0.5 )

ggplot( df
       , aes( x=x
            , y=value
            , colour=variable
            , linetype=variable
            , size=variable
            )
       ) +
   geom_line() +
   scale_colour_manual( name = "Curve", values = colv ) +
   scale_linetype_manual( name = "Curve", values = lntypv ) +
   scale_size_manual( name = "Curve", values = szv )

On Fri, 25 Dec 2015, Giorgio Garziano wrote:

> Hi Marna,
>
> here is another example that should appear more similar to your scenario
> than my previous one.
>
> x <- seq(1:100)
>
> y1 <- x*x
> g1 <- rep("y1", 100)
> df1 <- as.data.frame(cbind(x, y1), stringsAsFactors=FALSE)
> df1 <- as.data.frame(cbind(df1, g1))
> colnames(df1)<- c("x", "value", "variable")
>
> y2 <- y1+1500
> g2 <- rep("y2", 100)
> df2 <- as.data.frame(cbind(x, y2), stringsAsFactors=FALSE)
> df2 <- as.data.frame(cbind(df2, g2))
> colnames(df2)<- c("x", "value", "variable")
>
> y3 <- y1+6000
> g3 <- rep("y3", 100)
> df3 <- as.data.frame(cbind(x, y3), stringsAsFactors=FALSE)
> df3 <- as.data.frame(cbind(df3, g3))
> colnames(df3)<- c("x", "value", "variable")
>
> avg <- (y1+y2+y3)/3
> df4 <- as.data.frame(cbind(x, avg))
> g4 <- rep("average", 100)
> df4 <- as.data.frame(cbind(df4, g4))
> colnames(df4) <- c("x", "value", "variable")
>
> df <- data.frame(rbind(df1, df2, df3, df4))
>
> # this is the data to start with ggplot()
> df
>
> # the df rows where the average value is stored
> sel <- which(df[,"variable"]=="average")
>
> library(ggplot2)
>
> ggplot(data = df[-sel,], aes(x=x, y=value, group=variable)) + geom_line() +
>  geom_line(data = df[sel,], aes(x=x, y=value, group=variable), size=0.5, linetype="dashed", color="blue")
>
>
> Merry Christmas,
>
> --
> GG
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

---------------------------------------------------------------------------
Jeff Newmiller                        The     .....       .....  Go Live...
DCN:<jdnewmil at dcn.davis.ca.us>        Basics: ##.#.       ##.#.  Live Go...
                                       Live:   OO#.. Dead: OO#..  Playing
Research Engineer (Solar/Batteries            O.O#.       #.O#.  with
/Software/Embedded Controllers)               .OO#.       .OO#.  rocks...1k


From lists at revelle.net  Sat Dec 26 16:41:43 2015
From: lists at revelle.net (William Revelle)
Date: Sat, 26 Dec 2015 09:41:43 -0600
Subject: [R] Help on how to use cosinor analysis
In-Reply-To: <CAPT5noKUwBK9Hh9boxGjK-OJt3=OPCrN=F9zg9UVmro3L2Xhow@mail.gmail.com>
References: <CAPT5noKUwBK9Hh9boxGjK-OJt3=OPCrN=F9zg9UVmro3L2Xhow@mail.gmail.com>
Message-ID: <A5170657-46C7-451C-BD28-655FD050D379@revelle.net>

Dear Friday,

You need to specify what package you are using.

There is a package (cosinor) that is meant for doing this.

In addition, the psych package has a cosinor function, as do the CircStats and circular packages.

For your data, you need to use the c() function 

library(psych)

Time=c(1,2,3,4,24)     #add more times 
Rectal=c(33.8,37.6,37.1,35.5,38.2)  #add more temperatures
cosinor(Time,Rectal)

Then, to plot these, you will need to form a data frame

timeTemp <- data.frame(Time,Rectal)
cosinor.plot("Time","Rectal",data=timeTemp)



> On Dec 23, 2015, at 7:45 AM, friday zakari <fridayzakari at gmail.com> wrote:
> 
> l am a beginner in the use of R for statistical analysis. I am finding it
> difficult to use cosinor to analyze the circadian rhythm in rectal
> temperature in broiler chicken.
> 
> Time=(1,2,3,4.........24)
> Rectal temp=(33.8,37.6,37.1,35.5,......38.2)
> 
> l will be very please if you can guide me on how to do the analysis using
> the sets of variables above as example and if you can also assist me with
> the R functions required for the analysis. thank you very much
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
> 

William Revelle		           http://personality-project.org/revelle.html
Professor			           http://personality-project.org
Department of Psychology   http://www.wcas.northwestern.edu/psych/
Northwestern University	   http://www.northwestern.edu/
Use R for psychology             http://personality-project.org/r
It is 3 minutes to midnight	   http://www.thebulletin.org


From shivi.bhatia at safexpress.com  Sat Dec 26 07:33:55 2015
From: shivi.bhatia at safexpress.com (SHIVI BHATIA)
Date: Sat, 26 Dec 2015 12:03:55 +0530
Subject: [R] Subscribe to Post
Message-ID: <003d01d13fa7$58339980$089acc80$@safexpress.com>

Dear Team, 

 

Kindly look into the same at the earliest. 

 

Regards, Shivi

 

From: SHIVI BHATIA [mailto:shivi.bhatia at safexpress.com] 
Sent: Friday, December 25, 2015 6:10 PM
To: 'r-help at r-project.org' <r-help at r-project.org>
Subject: Subscribe to Post

 

Dear Team, 

 

I have recently re-subscribed to R help moving ahead from nabble. Got an
email where it asked to send email to the above group which will allow me to
post questions hence request to post questions. 

Thanks, Shivi

Mb: 9891002021

 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151226/80ac39c3/attachment.pl>

From jdnewmil at dcn.davis.ca.us  Sat Dec 26 18:21:16 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 26 Dec 2015 09:21:16 -0800
Subject: [R] Subscribe to Post
In-Reply-To: <003d01d13fa7$58339980$089acc80$@safexpress.com>
References: <003d01d13fa7$58339980$089acc80$@safexpress.com>
Message-ID: <F2470633-D616-419A-924B-327A7C32C5A8@dcn.davis.ca.us>

You have managed to send an email to the list. That means users like you saw your message, but we are not going to "look into" anything for you. 

You might find [1] helpful.  You definitely should read the Posting Guide (mentioned in the footer below) and learn to look through the archives [2] (mentioned in the listinfo link in the email footer) to see what you and other people have successfully posted to the list before you post in the future. 

[1] http://www.livinginternet.com/l/lu.htm
[2] https://stat.ethz.ch/pipermail/r-help/
-- 
Sent from my phone. Please excuse my brevity.

On December 25, 2015 10:33:55 PM PST, SHIVI BHATIA <shivi.bhatia at safexpress.com> wrote:
>Dear Team, 
>
> 
>
>Kindly look into the same at the earliest. 
>
> 
>
>Regards, Shivi
>
> 
>
>From: SHIVI BHATIA [mailto:shivi.bhatia at safexpress.com] 
>Sent: Friday, December 25, 2015 6:10 PM
>To: 'r-help at r-project.org' <r-help at r-project.org>
>Subject: Subscribe to Post
>
> 
>
>Dear Team, 
>
> 
>
>I have recently re-subscribed to R help moving ahead from nabble. Got
>an
>email where it asked to send email to the above group which will allow
>me to
>post questions hence request to post questions. 
>
>Thanks, Shivi
>
>Mb: 9891002021
>
> 
>
>
>
>------------------------------------------------------------------------
>
>This e-mail is confidential. It may also be legally privileged. If you
>are not the addressee you may not copy, forward, disclose or use any
>part of it. If you have received this message in error, please delete
>it and all copies from your system and notify the sender immediately by
>return e-mail. Internet communications cannot be guaranteed to be
>timely, secure, error or virus-free. The sender does not accept
>liability for any errors or omissions.
>
>
>------------------------------------------------------------------------
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From tr206 at kent.ac.uk  Sat Dec 26 19:29:28 2015
From: tr206 at kent.ac.uk (T.Riedle)
Date: Sat, 26 Dec 2015 18:29:28 +0000
Subject: [R] rugarch package: VaR exceedances plot
Message-ID: <e210ac91017a46a0b48f5fb23c8cf16f@ex13-live-mbn1.ad.kent.ac.uk>

Dear all,

I am trying to backtest my VaR model in R using the rugarch package. Hence, I am trying to plot the VaR exceedances using following code from the rugarch package:

VaRplot(alpha=0.025,actual = returns,VaR = VaR,ylab = "daily log returns",xlab = "date")

Unfortunately, I get this error message and do not know the reason for this

Error in plot.window(...) : invalid 'ylim' value
In addition: Warning message:
In as.double.xts(actual) : NAs introduced by coercion

If I take the ylim error message into account I get the error:

Error in VaRplot(alpha = 0.025, actual = returns, VaR = VaR, ylab = "daily log returns",  : 
  unused argument (ylim = rangereturns)

Does anyone have an idea? Many thanks in advance.

-----Original Message-----
From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Giorgio Garziano
Sent: 25 December 2015 19:38
To: r-help at r-project.org
Subject: Re: [R] creating a xts object

Some hints at the following link where the "order.by requires an appropriate time-based object" error is commented.

http://stackoverflow.com/questions/23224142/converting-data-frame-to-xts-order-by-requires-an-appropriate-time-based-object


--
GG

	[[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


From marna.wagley at gmail.com  Sat Dec 26 19:55:39 2015
From: marna.wagley at gmail.com (Marna Wagley)
Date: Sat, 26 Dec 2015 10:55:39 -0800
Subject: [R] How do we do correlation for big matrices?
Message-ID: <CAMwU6B2MxgKkgo6s=CWoC+tBJJ-oqeVC4DSRS1qOThYjgbE80g@mail.gmail.com>

Hi R users,
I have a very big two matrices of 12 columns and over 0.5 million columns
(50,4710) and trying to get correlation value between two tables but I
could not compute it because of big files.
Would you give me any suggestion on how I can do the correlations for the
big files?

I used the following codes and the example data.

df1<-structure(list(X = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 2L, 3L, 4L), .Label = c("env1", "env10", "env11", "env12",
"env2", "env3", "env4", "env5", "env6", "env7", "env8", "env9"
), class = "factor"), site1 = c(0.38, 0.83, 0.53, 0.48, 0.66,
0.09, 0.21, 0.02, 0.76, 0.62, 0.2, 0.47), site2 = c(0.19, 0.14,
0.66, 0.35, 0.18, 0.24, 0.18, 0.2, 0.86, 0.06, 0.51, 0.29), site3 = c(0.95,
0.51, 0.91, 0.48, 0.74, 0.67, 0.34, 0.72, 0.43, 0.49, 0.1, 0.48
), site4 = c(0.89, 0.54, 0.93, 0.18, 0.99, 0.21, 0.69, 0.29,
0.89, 0.84, 0.45, 0.2), site5 = c(0.38, 0.37, 0.01, 0.26, 0.97,
0.49, 0.39, 0.31, 0.14, 0.83, 0.99, 0.2), site6 = c(0.68, 0.67,
0.6, 0.92, 0.01, 0.04, 0.49, 0.38, 0.5, 0.37, 0.51, 0.17), site7 = c(0.08,
0.54, 0.31, 0.3, 0.77, 0.39, 0.03, 0.51, 0.28, 0.32, 0.86, 0.95
), site8 = c(0.54, 0.26, 0.87, 0.91, 0.12, 0.51, 0.31, 0.67,
0.69, 0.79, 0.76, 0.08), site9 = c(0.1, 0.68, 0.17, 0.44, 0.78,
0.9, 0.16, 0.31, 0.13, 0.34, 0.9, 0.16), site10 = c(0.53, 0.31,
0.88, 0.61, 0.92, 0.44, 0.92, 0.94, 0.55, 0.8, 0.27, 0.07)), .Names =
c("X",
"site1", "site2", "site3", "site4", "site5", "site6", "site7",
"site8", "site9", "site10"), class = "data.frame", row.names = c(NA,
-12L))
df1<-df1[-1]

df2<-structure(list(X = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
12L, 2L, 3L, 4L), .Label = c("env1", "env10", "env11", "env12",
"env2", "env3", "env4", "env5", "env6", "env7", "env8", "env9"
), class = "factor"), site1 = c(0.36, 0.29, 0.09, 0.07, 0.82,
0.88, 0.59, 0.57, 0.2, 0.29, 0.76, 0.2), site2 = c(0.91, 0.87,
0.91, 0.54, 0.53, 0.2, 0.23, 0.16, 0.42, 0.44, 0.01, 0.29), site3 = c(0.96,
0.56, 0.34, 0.34, 0.6, 0.63, 0.28, 0.25, 0.73, 0.45, 0.88, 0.39
), site4 = c(0.73, 0.79, 0.39, 0.59, 0.63, 0.24, 0.69, 0.94,
0.07, 0.23, 0.01, 0.99), site5 = c(0.88, 0.18, 0.37, 0.24, 0.61,
0.61, 0.54, 0.71, 0.12, 0.82, 0.26, 0.5), site6 = c(0.43, 0.52,
0.01, 0.76, 0.41, 0.57, 0.08, 0.75, 0.82, 0.98, 0.61, 0.74),
    site7 = c(0.84, 0.14, 0.96, 0.04, 0.41, 0.84, 0.26, 0.59,
    0.29, 0.3, 0.76, 0.05), site8 = c(0.12, 0.18, 0.75, 0.23,
    0.96, 0.64, 0.33, 0.61, 0.25, 0.13, 0.99, 0.6), site9 = c(0.26,
    0.58, 0.32, 0.67, 0.11, 0.8, 0.87, 0.05, 0.03, 0.47, 0.95,
    0.81), site10 = c(0.94, 0.63, 0.64, 0.5, 0.94, 0.75, 0.44,
    0.57, 0.19, 0.23, 0.08, 0.18)), .Names = c("X", "site1",
"site2", "site3", "site4", "site5", "site6", "site7", "site8",
"site9", "site10"), class = "data.frame", row.names = c(NA, -12L
))
df2<-df2[-1]
df2
# here I put only 12 columns, but as I mentioned above I have more than 1/2
million columns
cor_site<-data.matrix(diag(cor(df1,df2)))
It works fine for a small data but this big files did not work.

Thanks for your suggestions.
MW

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Sat Dec 26 20:14:02 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sat, 26 Dec 2015 11:14:02 -0800
Subject: [R] How do we do correlation for big matrices?
In-Reply-To: <CAMwU6B2MxgKkgo6s=CWoC+tBJJ-oqeVC4DSRS1qOThYjgbE80g@mail.gmail.com>
References: <CAMwU6B2MxgKkgo6s=CWoC+tBJJ-oqeVC4DSRS1qOThYjgbE80g@mail.gmail.com>
Message-ID: <CAF8bMcaTtn-6HjKBh0tWV7yZXQG=4_T4hOoKoLYRAa_F3NKr9w@mail.gmail.com>

Since you only want the diagonal of the correlation matrix, the following
will probably
do the job using less memory.  The mapply versions works on the data.frames
you supplied, but will not work on matrices - be careful not to conflate
the two classes of data objects.

  > vapply(colnames(df1), function(i)cor(df1[,i],df2[,i]), 0)
         site1        site2        site3        site4        site5
 site6        site7
  -0.540644946  0.006898188 -0.035279748 -0.261648270  0.274059055
-0.076396648   -0.147696334
         site8        site9       site10
  -0.138916728  0.330632540  0.366095090
  > mapply(FUN=cor, df1, df2)
         site1        site2        site3        site4        site5
 site6        site7
  -0.540644946  0.006898188 -0.035279748 -0.261648270  0.274059055
-0.076396648   -0.147696334
         site8        site9       site10
  -0.138916728  0.330632540  0.366095090
Compare to your:
  > diag(cor(df1,df2))
         site1        site2        site3        site4        site5
 site6        site7
  -0.540644946  0.006898188 -0.035279748 -0.261648270  0.274059055
-0.076396648   -0.147696334
         site8        site9       site10
  -0.138916728  0.330632540  0.366095090


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Dec 26, 2015 at 10:55 AM, Marna Wagley <marna.wagley at gmail.com>
wrote:

> Hi R users,
> I have a very big two matrices of 12 columns and over 0.5 million columns
> (50,4710) and trying to get correlation value between two tables but I
> could not compute it because of big files.
> Would you give me any suggestion on how I can do the correlations for the
> big files?
>
> I used the following codes and the example data.
>
> df1<-structure(list(X = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
> 12L, 2L, 3L, 4L), .Label = c("env1", "env10", "env11", "env12",
> "env2", "env3", "env4", "env5", "env6", "env7", "env8", "env9"
> ), class = "factor"), site1 = c(0.38, 0.83, 0.53, 0.48, 0.66,
> 0.09, 0.21, 0.02, 0.76, 0.62, 0.2, 0.47), site2 = c(0.19, 0.14,
> 0.66, 0.35, 0.18, 0.24, 0.18, 0.2, 0.86, 0.06, 0.51, 0.29), site3 = c(0.95,
> 0.51, 0.91, 0.48, 0.74, 0.67, 0.34, 0.72, 0.43, 0.49, 0.1, 0.48
> ), site4 = c(0.89, 0.54, 0.93, 0.18, 0.99, 0.21, 0.69, 0.29,
> 0.89, 0.84, 0.45, 0.2), site5 = c(0.38, 0.37, 0.01, 0.26, 0.97,
> 0.49, 0.39, 0.31, 0.14, 0.83, 0.99, 0.2), site6 = c(0.68, 0.67,
> 0.6, 0.92, 0.01, 0.04, 0.49, 0.38, 0.5, 0.37, 0.51, 0.17), site7 = c(0.08,
> 0.54, 0.31, 0.3, 0.77, 0.39, 0.03, 0.51, 0.28, 0.32, 0.86, 0.95
> ), site8 = c(0.54, 0.26, 0.87, 0.91, 0.12, 0.51, 0.31, 0.67,
> 0.69, 0.79, 0.76, 0.08), site9 = c(0.1, 0.68, 0.17, 0.44, 0.78,
> 0.9, 0.16, 0.31, 0.13, 0.34, 0.9, 0.16), site10 = c(0.53, 0.31,
> 0.88, 0.61, 0.92, 0.44, 0.92, 0.94, 0.55, 0.8, 0.27, 0.07)), .Names =
> c("X",
> "site1", "site2", "site3", "site4", "site5", "site6", "site7",
> "site8", "site9", "site10"), class = "data.frame", row.names = c(NA,
> -12L))
> df1<-df1[-1]
>
> df2<-structure(list(X = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
> 12L, 2L, 3L, 4L), .Label = c("env1", "env10", "env11", "env12",
> "env2", "env3", "env4", "env5", "env6", "env7", "env8", "env9"
> ), class = "factor"), site1 = c(0.36, 0.29, 0.09, 0.07, 0.82,
> 0.88, 0.59, 0.57, 0.2, 0.29, 0.76, 0.2), site2 = c(0.91, 0.87,
> 0.91, 0.54, 0.53, 0.2, 0.23, 0.16, 0.42, 0.44, 0.01, 0.29), site3 = c(0.96,
> 0.56, 0.34, 0.34, 0.6, 0.63, 0.28, 0.25, 0.73, 0.45, 0.88, 0.39
> ), site4 = c(0.73, 0.79, 0.39, 0.59, 0.63, 0.24, 0.69, 0.94,
> 0.07, 0.23, 0.01, 0.99), site5 = c(0.88, 0.18, 0.37, 0.24, 0.61,
> 0.61, 0.54, 0.71, 0.12, 0.82, 0.26, 0.5), site6 = c(0.43, 0.52,
> 0.01, 0.76, 0.41, 0.57, 0.08, 0.75, 0.82, 0.98, 0.61, 0.74),
>     site7 = c(0.84, 0.14, 0.96, 0.04, 0.41, 0.84, 0.26, 0.59,
>     0.29, 0.3, 0.76, 0.05), site8 = c(0.12, 0.18, 0.75, 0.23,
>     0.96, 0.64, 0.33, 0.61, 0.25, 0.13, 0.99, 0.6), site9 = c(0.26,
>     0.58, 0.32, 0.67, 0.11, 0.8, 0.87, 0.05, 0.03, 0.47, 0.95,
>     0.81), site10 = c(0.94, 0.63, 0.64, 0.5, 0.94, 0.75, 0.44,
>     0.57, 0.19, 0.23, 0.08, 0.18)), .Names = c("X", "site1",
> "site2", "site3", "site4", "site5", "site6", "site7", "site8",
> "site9", "site10"), class = "data.frame", row.names = c(NA, -12L
> ))
> df2<-df2[-1]
> df2
> # here I put only 12 columns, but as I mentioned above I have more than 1/2
> million columns
> cor_site<-data.matrix(diag(cor(df1,df2)))
> It works fine for a small data but this big files did not work.
>
> Thanks for your suggestions.
> MW
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Sat Dec 26 21:17:26 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Sat, 26 Dec 2015 12:17:26 -0800
Subject: [R] rugarch package: VaR exceedances plot
In-Reply-To: <e210ac91017a46a0b48f5fb23c8cf16f@ex13-live-mbn1.ad.kent.ac.uk>
References: <e210ac91017a46a0b48f5fb23c8cf16f@ex13-live-mbn1.ad.kent.ac.uk>
Message-ID: <9D1518E6-324D-4F17-8589-17401387F780@dcn.davis.ca.us>

Some ideas:

You made a mistake.  Couldn't help you with that even if I was at my computer for lack of a reproducible example. 

The package author made a mistake,  or some package they depended on has changed how it works.  In either case,  you would need to correspond with the package maintainer using the email address that is returned by the maintainer() function. 
-- 
Sent from my phone. Please excuse my brevity.

On December 26, 2015 10:29:28 AM PST, "T.Riedle" <tr206 at kent.ac.uk> wrote:
>Dear all,
>
>I am trying to backtest my VaR model in R using the rugarch package.
>Hence, I am trying to plot the VaR exceedances using following code
>from the rugarch package:
>
>VaRplot(alpha=0.025,actual = returns,VaR = VaR,ylab = "daily log
>returns",xlab = "date")
>
>Unfortunately, I get this error message and do not know the reason for
>this
>
>Error in plot.window(...) : invalid 'ylim' value
>In addition: Warning message:
>In as.double.xts(actual) : NAs introduced by coercion
>
>If I take the ylim error message into account I get the error:
>
>Error in VaRplot(alpha = 0.025, actual = returns, VaR = VaR, ylab =
>"daily log returns",  : 
>  unused argument (ylim = rangereturns)
>
>Does anyone have an idea? Many thanks in advance.
>
>-----Original Message-----
>From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Giorgio
>Garziano
>Sent: 25 December 2015 19:38
>To: r-help at r-project.org
>Subject: Re: [R] creating a xts object
>
>Some hints at the following link where the "order.by requires an
>appropriate time-based object" error is commented.
>
>http://stackoverflow.com/questions/23224142/converting-data-frame-to-xts-order-by-requires-an-appropriate-time-based-object
>
>
>--
>GG
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From peter.langfelder at gmail.com  Sun Dec 27 00:44:46 2015
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Sat, 26 Dec 2015 15:44:46 -0800
Subject: [R] How do we do correlation for big matrices?
In-Reply-To: <CAF8bMcaTtn-6HjKBh0tWV7yZXQG=4_T4hOoKoLYRAa_F3NKr9w@mail.gmail.com>
References: <CAMwU6B2MxgKkgo6s=CWoC+tBJJ-oqeVC4DSRS1qOThYjgbE80g@mail.gmail.com>
	<CAF8bMcaTtn-6HjKBh0tWV7yZXQG=4_T4hOoKoLYRAa_F3NKr9w@mail.gmail.com>
Message-ID: <CA+hbrhVYEeL_9N5oQCYen04opD2M_HjnvJcOCMcgZ_zt5Lvy0A@mail.gmail.com>

My guess is that a mapply would take forever to run. I would split it
up into smaller blocks - not too large so the calculation can fit into
the RAM, and not too small to make the calculation tun too long. Say
500 columns per block, that way each correlation matrix takes up
500*500*8 bytes = 1.9 MB, so a even the full 1000 blocks would fit
into a reasonably sized RAM (hopefully R will do a garbage collection
from time to time anyway). At the risk of tooting my own horn,

library(WGCNA) ## For allocateJobs
n = ncol(df1)
blocks = allocateJobs(n, 1000) # With 1000 blocks, roughly 500 columns
per block...
results.lst = lapply(blocks, function(index) diag(cor(df1[, index],
df2[, index])));
result = unlist(results.lst)

I haven't tested this code, but it shouldn't be too far from correct.

On Sat, Dec 26, 2015 at 11:14 AM, William Dunlap via R-help
<r-help at r-project.org> wrote:
> Since you only want the diagonal of the correlation matrix, the following
> will probably
> do the job using less memory.  The mapply versions works on the data.frames
> you supplied, but will not work on matrices - be careful not to conflate
> the two classes of data objects.
>
>   > vapply(colnames(df1), function(i)cor(df1[,i],df2[,i]), 0)
>          site1        site2        site3        site4        site5
>  site6        site7
>   -0.540644946  0.006898188 -0.035279748 -0.261648270  0.274059055
> -0.076396648   -0.147696334
>          site8        site9       site10
>   -0.138916728  0.330632540  0.366095090
>   > mapply(FUN=cor, df1, df2)
>          site1        site2        site3        site4        site5
>  site6        site7
>   -0.540644946  0.006898188 -0.035279748 -0.261648270  0.274059055
> -0.076396648   -0.147696334
>          site8        site9       site10
>   -0.138916728  0.330632540  0.366095090
> Compare to your:
>   > diag(cor(df1,df2))
>          site1        site2        site3        site4        site5
>  site6        site7
>   -0.540644946  0.006898188 -0.035279748 -0.261648270  0.274059055
> -0.076396648   -0.147696334
>          site8        site9       site10
>   -0.138916728  0.330632540  0.366095090
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sat, Dec 26, 2015 at 10:55 AM, Marna Wagley <marna.wagley at gmail.com>
> wrote:
>
>> Hi R users,
>> I have a very big two matrices of 12 columns and over 0.5 million columns
>> (50,4710) and trying to get correlation value between two tables but I
>> could not compute it because of big files.
>> Would you give me any suggestion on how I can do the correlations for the
>> big files?
>>
>> I used the following codes and the example data.
>>
>> df1<-structure(list(X = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
>> 12L, 2L, 3L, 4L), .Label = c("env1", "env10", "env11", "env12",
>> "env2", "env3", "env4", "env5", "env6", "env7", "env8", "env9"
>> ), class = "factor"), site1 = c(0.38, 0.83, 0.53, 0.48, 0.66,
>> 0.09, 0.21, 0.02, 0.76, 0.62, 0.2, 0.47), site2 = c(0.19, 0.14,
>> 0.66, 0.35, 0.18, 0.24, 0.18, 0.2, 0.86, 0.06, 0.51, 0.29), site3 = c(0.95,
>> 0.51, 0.91, 0.48, 0.74, 0.67, 0.34, 0.72, 0.43, 0.49, 0.1, 0.48
>> ), site4 = c(0.89, 0.54, 0.93, 0.18, 0.99, 0.21, 0.69, 0.29,
>> 0.89, 0.84, 0.45, 0.2), site5 = c(0.38, 0.37, 0.01, 0.26, 0.97,
>> 0.49, 0.39, 0.31, 0.14, 0.83, 0.99, 0.2), site6 = c(0.68, 0.67,
>> 0.6, 0.92, 0.01, 0.04, 0.49, 0.38, 0.5, 0.37, 0.51, 0.17), site7 = c(0.08,
>> 0.54, 0.31, 0.3, 0.77, 0.39, 0.03, 0.51, 0.28, 0.32, 0.86, 0.95
>> ), site8 = c(0.54, 0.26, 0.87, 0.91, 0.12, 0.51, 0.31, 0.67,
>> 0.69, 0.79, 0.76, 0.08), site9 = c(0.1, 0.68, 0.17, 0.44, 0.78,
>> 0.9, 0.16, 0.31, 0.13, 0.34, 0.9, 0.16), site10 = c(0.53, 0.31,
>> 0.88, 0.61, 0.92, 0.44, 0.92, 0.94, 0.55, 0.8, 0.27, 0.07)), .Names =
>> c("X",
>> "site1", "site2", "site3", "site4", "site5", "site6", "site7",
>> "site8", "site9", "site10"), class = "data.frame", row.names = c(NA,
>> -12L))
>> df1<-df1[-1]
>>
>> df2<-structure(list(X = structure(c(1L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,
>> 12L, 2L, 3L, 4L), .Label = c("env1", "env10", "env11", "env12",
>> "env2", "env3", "env4", "env5", "env6", "env7", "env8", "env9"
>> ), class = "factor"), site1 = c(0.36, 0.29, 0.09, 0.07, 0.82,
>> 0.88, 0.59, 0.57, 0.2, 0.29, 0.76, 0.2), site2 = c(0.91, 0.87,
>> 0.91, 0.54, 0.53, 0.2, 0.23, 0.16, 0.42, 0.44, 0.01, 0.29), site3 = c(0.96,
>> 0.56, 0.34, 0.34, 0.6, 0.63, 0.28, 0.25, 0.73, 0.45, 0.88, 0.39
>> ), site4 = c(0.73, 0.79, 0.39, 0.59, 0.63, 0.24, 0.69, 0.94,
>> 0.07, 0.23, 0.01, 0.99), site5 = c(0.88, 0.18, 0.37, 0.24, 0.61,
>> 0.61, 0.54, 0.71, 0.12, 0.82, 0.26, 0.5), site6 = c(0.43, 0.52,
>> 0.01, 0.76, 0.41, 0.57, 0.08, 0.75, 0.82, 0.98, 0.61, 0.74),
>>     site7 = c(0.84, 0.14, 0.96, 0.04, 0.41, 0.84, 0.26, 0.59,
>>     0.29, 0.3, 0.76, 0.05), site8 = c(0.12, 0.18, 0.75, 0.23,
>>     0.96, 0.64, 0.33, 0.61, 0.25, 0.13, 0.99, 0.6), site9 = c(0.26,
>>     0.58, 0.32, 0.67, 0.11, 0.8, 0.87, 0.05, 0.03, 0.47, 0.95,
>>     0.81), site10 = c(0.94, 0.63, 0.64, 0.5, 0.94, 0.75, 0.44,
>>     0.57, 0.19, 0.23, 0.08, 0.18)), .Names = c("X", "site1",
>> "site2", "site3", "site4", "site5", "site6", "site7", "site8",
>> "site9", "site10"), class = "data.frame", row.names = c(NA, -12L
>> ))
>> df2<-df2[-1]
>> df2
>> # here I put only 12 columns, but as I mentioned above I have more than 1/2
>> million columns
>> cor_site<-data.matrix(diag(cor(df1,df2)))
>> It works fine for a small data but this big files did not work.
>>
>> Thanks for your suggestions.
>> MW
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From kipi at kt.dtu.dk  Sat Dec 26 12:17:01 2015
From: kipi at kt.dtu.dk (Kim Pilegaard)
Date: Sat, 26 Dec 2015 11:17:01 +0000
Subject: [R] Installation of S-Plus 6.2 on Ubuntu 14.04
Message-ID: <D2A435B8.2D866%kipi@env.dtu.dk>

I have many old datasets, scripts and functions written in S-Plus. I want to transfer these to R. Basically, I want to transfer my old chapters (.Data) entirely to R and have tried with dump(?., oldStyle =T). However, this is not very successful; R cannot digest the dump files entirely.

Therefore, I want to install S-Plus 6.2 again under Linux Ubuntu 14.04. I have the license information, and the installation works OK. But S-Plus needs some old libraries like libstdt++, libg2c, and JRE 1.4.0_01, related to linux kernel 2.4 . Re-installing them causes conflicts with my present Linux installation.

Does anyone have a solution to this, or do I need to run S-Plus under an older Linux version such as Redhat 9, which was approved to run S-Plus 6.2?

Alternatively, is there a better way than dump() to transfer chapters from S-Plus to R?

Kim Pilegaard
Professor
Atmospheric Environment
DTU Environment


Technical University of Denmark


Department of Environmental Engineering
Miljoevej
Building 113
2800 Kgs. Lyngby
Denmark
Mobile +45 4025 6839
kipi at env.dtu.dk<mailto:kipi at kt.dtu.dk>
www.env.dtu.dk<http://www.kt.dtu.dk/>
http://orcid.org/0000-0002-5169-5717

	[[alternative HTML version deleted]]


From calum.polwart at nhs.net  Sun Dec 27 12:08:29 2015
From: calum.polwart at nhs.net (Polwart Calum (COUNTY DURHAM AND DARLINGTON NHS FOUNDATION TRUST))
Date: Sun, 27 Dec 2015 11:08:29 +0000
Subject: [R] ?currency challenge ?knapsack challenge ?probabilities
Message-ID: <20151227110907.A06DB449194@nhs-pd1e-esg107.ad1.nhs.net>

I am currently working on a project that is producing Gigabyte sized vectors that kill R.  It can take 24 hours to run. So frustrating to get that far and have to scale back my inputs and rerun...


The problem i'm trying to solve as i see it is very similar to optimal currency denominations problems combined with a knapsack problem. Let me TRY to explain.


We have a product that we want to manufacture in as few sizes as possible.  like currency if you want 30cents but a 30cent coin doesnt exist you can join multiple products together. (3x10 cent, 25cent +5 cent, etc)


Unlike currency we dont need every value to be possible, we have a list of known values which are effectively related to each other by the next size up being 25% bigger. So for instance 64, 80 100.


We have some rules that say you can't use more than X products combined to make the final size.  A bit like saying never give more than 10 coins as change, so you cant issue 20x5cents for a dollar of change.


All of that fits a standard currency denomination challenge.


We dont need the combinations to be calculated using greedy method. [We will calculate and store as a table]


BUT - we do have a manufacturing limitation that means can manufacture to any whole number size, we cant do smaller than size5. (We dont go as low as that anyway... size 11 is as low as needed).  So different from any currency problem I've seen where the lowest coin size is always a 1 allowing any size to be produced.


So i have three questions I'm trying to answer:


- what is the smallest product range we can make that achieves our rules for max combinations of sizes?


- Is there a more optimal range. Say the smallest range was 4 sizes, for example 5,6,23,40.  Its possible adding a 22 and a 46 to that may actually be cheaper than supplying 2x5 and 2x6 or 2x23...

Currently I'm identifying every possible combination into a matrix.  We have a manufacturing constraint of max size 49 as well.  So i take every end user size possible (from 11 thru to 125).  For each size i then take every combination of possible sizes from 5 to 49 (45 sizes) that we COULD make and work out how i can achieve all the possible end user sizes, discarding any combinations that break our rules for max combinations.

Thats a giant set of for loops.  Once i establish the options we  can apply the manufacturing costs and usage data to find the answer.

For now 45 sizes,combined in any of up to 5 different combinations to do 10 end user sizes is creating vectors too big for R to handle...

Long explanation of the problem, to basically say... has anyone come across a function in R that might simplify this?



Sent from TypeMail<http://www.typeapp.com/r>


On 27 Dec 2015, at 08:00, "Polwart Calum (COUNTY DURHAM AND DARLINGTON NHS FOUNDATION TRUST)" <calum.polwart at nhs.net<mailto:calum.polwart at nhs.net>> wrote:

********************************************************************************************************************

This message may contain confidential information. If you are not the intended recipient please inform the
sender that you have received the message in error before deleting it.
Please do not disclose, copy or distribute information in this e-mail or take any action in reliance on its contents:
to do so is strictly prohibited and may be unlawful.

Thank you for your co-operation.

NHSmail is the secure email and directory service available for all NHS staff in England and Scotland
NHSmail is approved for exchanging patient data and other sensitive information with NHSmail and GSi recipients
NHSmail provides an email address for your career in the NHS and can be accessed anywhere

********************************************************************************************************************

	[[alternative HTML version deleted]]


From shivi.bhatia at safexpress.com  Sun Dec 27 11:06:30 2015
From: shivi.bhatia at safexpress.com (SHIVI BHATIA)
Date: Sun, 27 Dec 2015 15:36:30 +0530
Subject: [R] Subscribe to Post
In-Reply-To: <CAAJSdjikAMWX5vaZb3XZi_WsjtWstUNjY3NOS6A9bTu0f1goXQ@mail.gmail.com>
References: <003d01d13fa7$58339980$089acc80$@safexpress.com>
	<CAAJSdjikAMWX5vaZb3XZi_WsjtWstUNjY3NOS6A9bTu0f1goXQ@mail.gmail.com>
Message-ID: <004401d1408e$34929390$9db7bab0$@safexpress.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151227/c728f8a1/attachment.pl>

From emsolo at hotmail.fr  Sun Dec 27 16:49:56 2015
From: emsolo at hotmail.fr (Emma MONTARSOLO)
Date: Sun, 27 Dec 2015 15:49:56 +0000
Subject: [R] Errors with maxNR:  NA in gradient
Message-ID: <DB3PR02MB1065B16E639F654C9892FB2ADFA0@DB3PR02MB106.eurprd02.prod.outlook.com>

Hi !

I use maxNR function (from maxLik package) to estimate parameters via maximum likelihood and Newton?Raphson method.

Context :
I have a variable Y which follows a Gumbel distribution of parameters Beta1X1 + Beta2X2 (location parameter) and sigma (scale parameter).
I have to estimate sigma, beta1 and beta2

The following errors appear when I run maxNR :

> algo=maxNR(loglikelihood, start=c(sigma:8.686603,beta1=-4.976215,beta2=7.313875))
Iteration 1
Parameter:
                  beta1       beta2
   4.251275 -249.985063    5.376905
     beta1 beta2
NaN   NaN   NaN
Error in maxNRCompute(fn = function (theta, fnOrig, gradOrig = NULL, hessOrig = NULL,  :
  NA in gradient
In addition: There were 15 warnings (use warnings() to see them)

> warnings()
Messages d'avis :
1: In log(sigma) : NaNs produced
2: In log(sigma) : NaNs produced
3: In log(sigma) : NaNs produced
4: In log(sigma) : NaNs produced
5: In log(sigma) : NaNs produced
6: In log(sigma) : NaNs produced
7: In log(sigma) : NaNs produced
8: In log(sigma) : NaNs produced
9: In log(sigma) : NaNs produced
10: In log(sigma) : NaNs produced
11: In log(sigma) : NaNs produced
12: In log(sigma) : NaNs produced
13: In log(sigma) : NaNs produced
14: In log(sigma) : NaNs produced
15: In log(sigma) : NaNs produced

My code is the following :

n=length(data_reg$y)

loglikelihood<-function(parameters)
{
  sigma<-parameters[1]
  beta1<-parameters[2]
  beta2<-parameters[3]

  Z=(data_reg$y-(beta1*data_reg$x1)-(beta2*data_reg$x2))/sigma
  ll=-(n*log(sigma))+sum(Z+exp(-Z))
  ll
}

algo=maxNR(loglikelihood, start=c(sigma:8.686603,beta1=-4.976215,beta2=7.313875))



I use results from a previous estimation method as starting values.

Thanks for your help. ??

EM

	[[alternative HTML version deleted]]


From ruipbarradas at sapo.pt  Sun Dec 27 18:58:16 2015
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Sun, 27 Dec 2015 17:58:16 +0000
Subject: [R] Errors with maxNR: NA in gradient
In-Reply-To: <DB3PR02MB1065B16E639F654C9892FB2ADFA0@DB3PR02MB106.eurprd02.prod.outlook.com>
Message-ID: <20151227175816.Horde.KMC-bRtTR08A-e1AoL0PwEQ@mail.sapo.pt>

Hello,

Try using an equal sign after 'sigma':

algo=maxNR(loglikelihood,  
start=c(sigma=8.686603,beta1=-4.976215,beta2=7.313875))

Hope this helps,

Rui Barradas
?

Citando Emma MONTARSOLO <emsolo at hotmail.fr>:

> Hi !
>
> I use maxNR function (from maxLik package) to estimate parameters  
> via maximum likelihood and Newton?Raphson method.
>
> Context :
> I have a variable Y which follows a Gumbel distribution of  
> parameters Beta1X1 + Beta2X2 (location parameter) and sigma (scale  
> parameter).
> I have to estimate sigma, beta1 and beta2
>
> The following errors appear when I run maxNR :
>> algo=maxNR(loglikelihood,  
>> start=c(sigma:8.686603,beta1=-4.976215,beta2=7.313875))
>
> Iteration 1
> Parameter:
> ? ? ? ? ? ? ? ? ?beta1? ? ? ?beta2
> ? 4.251275 -249.985063? ? 5.376905
> ? ? beta1 beta2
> NaN? ?NaN? ?NaN
> Error in maxNRCompute(fn = function (theta, fnOrig, gradOrig = NULL,  
> hessOrig = NULL,? :
> NA in gradient
> In addition: There were 15 warnings (use warnings() to see them)
>> warnings()
>
> Messages d'avis :
> 1: In log(sigma) : NaNs produced
> 2: In log(sigma) : NaNs produced
> 3: In log(sigma) : NaNs produced
> 4: In log(sigma) : NaNs produced
> 5: In log(sigma) : NaNs produced
> 6: In log(sigma) : NaNs produced
> 7: In log(sigma) : NaNs produced
> 8: In log(sigma) : NaNs produced
> 9: In log(sigma) : NaNs produced
> 10: In log(sigma) : NaNs produced
> 11: In log(sigma) : NaNs produced
> 12: In log(sigma) : NaNs produced
> 13: In log(sigma) : NaNs produced
> 14: In log(sigma) : NaNs produced
> 15: In log(sigma) : NaNs produced
>
> My code is the following :
>
> n=length(data_reg$y)
>
> loglikelihood<-function(parameters)
> {
> sigma<-parameters[1]
> beta1<-parameters[2]
> beta2<-parameters[3]
>
> Z=(data_reg$y-(beta1*data_reg$x1)-(beta2*data_reg$x2))/sigma
> ll=-(n*log(sigma))+sum(Z+exp(-Z))
> ll
> }
>
> algo=maxNR(loglikelihood,  
> start=c(sigma:8.686603,beta1=-4.976215,beta2=7.313875))
>
> I use results from a previous estimation method as starting values.
>
> Thanks for your help. ??
>
> EM
>
> ? ? ? ? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide  
> http://www.R-project.org/posting-guide.htmland provide commented,  
> minimal, self-contained, reproducible code.

?

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Sun Dec 27 20:55:14 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 27 Dec 2015 11:55:14 -0800
Subject: [R] Installation of S-Plus 6.2 on Ubuntu 14.04
In-Reply-To: <D2A435B8.2D866%kipi@env.dtu.dk>
References: <D2A435B8.2D866%kipi@env.dtu.dk>
Message-ID: <CAF8bMcbcqSbEHjoF18f8En7rSy+aT+q1F_Vn0UFckQ55um6dwA@mail.gmail.com>

Do you know what sort of S+ objects do not survive the
dump-from-S+-source-into-R procedure unscathed?  E.g., do you have S4
classes that don't make it or certain classes
of data objects?

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Dec 26, 2015 at 3:17 AM, Kim Pilegaard <kipi at kt.dtu.dk> wrote:

> I have many old datasets, scripts and functions written in S-Plus. I want
> to transfer these to R. Basically, I want to transfer my old chapters
> (.Data) entirely to R and have tried with dump(?., oldStyle =T). However,
> this is not very successful; R cannot digest the dump files entirely.
>
> Therefore, I want to install S-Plus 6.2 again under Linux Ubuntu 14.04. I
> have the license information, and the installation works OK. But S-Plus
> needs some old libraries like libstdt++, libg2c, and JRE 1.4.0_01, related
> to linux kernel 2.4 . Re-installing them causes conflicts with my present
> Linux installation.
>
> Does anyone have a solution to this, or do I need to run S-Plus under an
> older Linux version such as Redhat 9, which was approved to run S-Plus 6.2?
>
> Alternatively, is there a better way than dump() to transfer chapters from
> S-Plus to R?
>
> Kim Pilegaard
> Professor
> Atmospheric Environment
> DTU Environment
>
>
> Technical University of Denmark
>
>
> Department of Environmental Engineering
> Miljoevej
> Building 113
> 2800 Kgs. Lyngby
> Denmark
> Mobile +45 4025 6839
> kipi at env.dtu.dk<mailto:kipi at kt.dtu.dk>
> www.env.dtu.dk<http://www.kt.dtu.dk/>
> http://orcid.org/0000-0002-5169-5717
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From john.archie.mckown at gmail.com  Sun Dec 27 22:20:43 2015
From: john.archie.mckown at gmail.com (John McKown)
Date: Sun, 27 Dec 2015 15:20:43 -0600
Subject: [R] Subscribe to Post
In-Reply-To: <004401d1408e$34929390$9db7bab0$@safexpress.com>
References: <003d01d13fa7$58339980$089acc80$@safexpress.com>
	<CAAJSdjikAMWX5vaZb3XZi_WsjtWstUNjY3NOS6A9bTu0f1goXQ@mail.gmail.com>
	<004401d1408e$34929390$9db7bab0$@safexpress.com>
Message-ID: <CAAJSdjj_gnfXGPxMLV0mV3ios8xL5KmrzBgy662CnMJm6wau_w@mail.gmail.com>

Yes. This is an _email_ group. It is not a web based discussion group.
There is an "archive" of emails somewhere around, but the proper (at least
IMO) way to ask a new question is to send a new email. A bit "old
fashioned", but in many way it is simpler for people on many different
types of computers (some of us are not Windows people!) to communicate.

And, very important, be sure to post in plain text. Avoid HTML encoded
email the the abomination that it is (IMO). They tend to be either ignored,
or replied to in a "corrective" way (i.e. you get a nasty note back for
some people).
? What this means in most cases is that you cannot insert an "screen print"
as a "picture" (as you did in the email to which I am replying). That will
get nasty-grams. And eventually many of the really good R people will just
ignore you.?

?You can not longer post via Nabble (if you did in the past). It has
"abused" by way to many (the sending of HTML encoded email). ?


On Sun, Dec 27, 2015 at 4:06 AM, SHIVI BHATIA <shivi.bhatia at safexpress.com>
wrote:
>
> Thanks John. Though I am in a fix now. Do I need to send in an email
every time for any help because I can?t find any URL to ask questions?
>
>
>
> I have tried the below URL?s: https://stat.ethz.ch/mailman/listinfo/r-help
- it only asks me to change or reset user preference.
>
>
-- 
Computer Science is the only discipline in which we view adding a new wing
to a building as being maintenance -- Jim Horning

Schrodinger's backup: The condition of any backup is unknown until a
restore is attempted.

Yoda of Borg, we are. Futile, resistance is, yes. Assimilated, you will be.

He's about as useful as a wax frying pan.

10 to the 12th power microphones = 1 Megaphone

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From drjimlemon at gmail.com  Sun Dec 27 23:04:49 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Mon, 28 Dec 2015 09:04:49 +1100
Subject: [R] ?currency challenge ?knapsack challenge ?probabilities
In-Reply-To: <20151227110907.A06DB449194@nhs-pd1e-esg107.ad1.nhs.net>
References: <20151227110907.A06DB449194@nhs-pd1e-esg107.ad1.nhs.net>
Message-ID: <CA+8X3fVEoRGwXDVqBLxHhKo3UoTwmO+ZzMXjkgw6+uRFFBwXeA@mail.gmail.com>

Hi Calum,
Does this include an error tolerance for the match between the ordered and
delivered quantities? That is, is it okay to have a maximum of one unit
difference or do deliveries have to exactly match orders?

Jim


On Sun, Dec 27, 2015 at 10:08 PM, Polwart Calum (COUNTY DURHAM AND
DARLINGTON NHS FOUNDATION TRUST) <calum.polwart at nhs.net> wrote:

> I am currently working on a project that is producing Gigabyte sized
> vectors that kill R.  It can take 24 hours to run. So frustrating to get
> that far and have to scale back my inputs and rerun...
>
>
> The problem i'm trying to solve as i see it is very similar to optimal
> currency denominations problems combined with a knapsack problem. Let me
> TRY to explain.
>
>
> We have a product that we want to manufacture in as few sizes as
> possible.  like currency if you want 30cents but a 30cent coin doesnt exist
> you can join multiple products together. (3x10 cent, 25cent +5 cent, etc)
>
>
> Unlike currency we dont need every value to be possible, we have a list of
> known values which are effectively related to each other by the next size
> up being 25% bigger. So for instance 64, 80 100.
>
>
> We have some rules that say you can't use more than X products combined to
> make the final size.  A bit like saying never give more than 10 coins as
> change, so you cant issue 20x5cents for a dollar of change.
>
>
> All of that fits a standard currency denomination challenge.
>
>
> We dont need the combinations to be calculated using greedy method. [We
> will calculate and store as a table]
>
>
> BUT - we do have a manufacturing limitation that means can manufacture to
> any whole number size, we cant do smaller than size5. (We dont go as low as
> that anyway... size 11 is as low as needed).  So different from any
> currency problem I've seen where the lowest coin size is always a 1
> allowing any size to be produced.
>
>
> So i have three questions I'm trying to answer:
>
>
> - what is the smallest product range we can make that achieves our rules
> for max combinations of sizes?
>
>
> - Is there a more optimal range. Say the smallest range was 4 sizes, for
> example 5,6,23,40.  Its possible adding a 22 and a 46 to that may actually
> be cheaper than supplying 2x5 and 2x6 or 2x23...
>
> Currently I'm identifying every possible combination into a matrix.  We
> have a manufacturing constraint of max size 49 as well.  So i take every
> end user size possible (from 11 thru to 125).  For each size i then take
> every combination of possible sizes from 5 to 49 (45 sizes) that we COULD
> make and work out how i can achieve all the possible end user sizes,
> discarding any combinations that break our rules for max combinations.
>
> Thats a giant set of for loops.  Once i establish the options we  can
> apply the manufacturing costs and usage data to find the answer.
>
> For now 45 sizes,combined in any of up to 5 different combinations to do
> 10 end user sizes is creating vectors too big for R to handle...
>
> Long explanation of the problem, to basically say... has anyone come
> across a function in R that might simplify this?
>
>
>
> Sent from TypeMail<http://www.typeapp.com/r>
>
>
> On 27 Dec 2015, at 08:00, "Polwart Calum (COUNTY DURHAM AND DARLINGTON NHS
> FOUNDATION TRUST)" <calum.polwart at nhs.net<mailto:calum.polwart at nhs.net>>
> wrote:
>
>
> ********************************************************************************************************************
>
> This message may contain confidential information. If you are not the
> intended recipient please inform the
> sender that you have received the message in error before deleting it.
> Please do not disclose, copy or distribute information in this e-mail or
> take any action in reliance on its contents:
> to do so is strictly prohibited and may be unlawful.
>
> Thank you for your co-operation.
>
> NHSmail is the secure email and directory service available for all NHS
> staff in England and Scotland
> NHSmail is approved for exchanging patient data and other sensitive
> information with NHSmail and GSi recipients
> NHSmail provides an email address for your career in the NHS and can be
> accessed anywhere
>
>
> ********************************************************************************************************************
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jsorkin at grecc.umaryland.edu  Mon Dec 28 03:55:44 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 27 Dec 2015 21:55:44 -0500
Subject: [R] by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
Message-ID: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>

When I run by, I get an error message and no results. Any help in understanding what is wrong would be appreciated.
 
Error message:
Warning messages:
1: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA
2: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA

 
Results:
Arm: MUFA
[1] NA
----------------------------------------------------------------------------------------------------------------------- 
Arm: PUFA
[1] NA

Code:
by(hold,Arm,mean,na.rm=TRUE)

I don't understand why I am getting the error message, and why I am not getting any results. I don't believe my data are non-numeric. 

BY str works fine and confirms that the data are numeric
> by(hold,Arm,str)
'data.frame':	23 obs. of  3 variables:
 $ Wtscr: num  97.2 103.9 58.2 130.9 135 ...
 $ Wt0  : num  96.2 106.1 56.7 127.4 133.1 ...
 $ Wt6  : num  93.8 101.7 55.5 127.6 130.9 ...
'data.frame':	16 obs. of  3 variables:
 $ Wtscr: num  120.2 104.6 100.1 74.8 112.6 ...
 $ Wt0  : num  117.2 105.3 99.5 75.7 110.7 ...
 $ Wt6  : num  114.6 104.8 84.5 77.7 107.4 ...
 Here is a listing of my data:
> hold
   Wtscr   Wt0    Wt6
1  120.2 117.2 114.60
2  104.6 105.3 104.80
3   97.2  96.2  93.80
4  103.9 106.1 101.70
5   58.2  56.7  55.50
6  130.9 127.4 127.60
7  135.0 133.1 130.90
8  100.1  99.5  84.50
9  130.3 115.3 115.80
10 150.5 148.7 133.40
11  74.8  75.7  77.70
12 112.6 110.7 107.40
13  90.0  91.0  83.40
14 139.1 138.5 126.70
15  99.1  96.4  95.70
16 108.3 107.5 109.30
17  75.1  72.9  72.20
18  97.5 102.1  98.50
19 202.2  90.1  90.60
20  91.7  89.4  93.40
21 102.1 102.2 100.80
22 116.9 118.9 118.00
23  94.6  95.3  90.30
24 122.2 117.0 117.00
25 105.6 103.3 103.60
26  96.9  96.8  98.80
27 102.9 100.3  89.00
28 115.8 118.5 117.30
29  95.7  96.2  95.40
30  88.2  86.9  88.30
31 108.7 108.8 108.80
32  89.2  88.6  81.20
33  86.8  86.5  82.70
34 135.5 130.1 125.40
35 112.5 113.9 111.45
36 111.0 105.3 109.50
37 103.4 100.5  95.50
38 117.6 117.4 101.40
39 116.7 118.5 101.80

The INDEX is clearly a factor: 
> Arm
 [1] PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA MUFA PUFA MUFA PUFA
[32] MUFA MUFA MUFA MUFA MUFA PUFA PUFA PUFA
Levels: MUFA PUFA

The data and the index have the same length:
> cbind(hold,Arm)
   Wtscr   Wt0    Wt6  Arm
1  120.2 117.2 114.60 PUFA
2  104.6 105.3 104.80 PUFA
3   97.2  96.2  93.80 MUFA
4  103.9 106.1 101.70 MUFA
5   58.2  56.7  55.50 MUFA
6  130.9 127.4 127.60 MUFA
7  135.0 133.1 130.90 MUFA
8  100.1  99.5  84.50 PUFA
9  130.3 115.3 115.80 MUFA
10 150.5 148.7 133.40 MUFA
11  74.8  75.7  77.70 PUFA
12 112.6 110.7 107.40 PUFA
13  90.0  91.0  83.40 PUFA
14 139.1 138.5 126.70 MUFA
15  99.1  96.4  95.70 MUFA
16 108.3 107.5 109.30 PUFA
17  75.1  72.9  72.20 PUFA
18  97.5 102.1  98.50 PUFA
19 202.2  90.1  90.60 MUFA
20  91.7  89.4  93.40 MUFA
21 102.1 102.2 100.80 MUFA
22 116.9 118.9 118.00 MUFA
23  94.6  95.3  90.30 MUFA
24 122.2 117.0 117.00 PUFA
25 105.6 103.3 103.60 MUFA
26  96.9  96.8  98.80 MUFA
27 102.9 100.3  89.00 PUFA
28 115.8 118.5 117.30 MUFA
29  95.7  96.2  95.40 PUFA
30  88.2  86.9  88.30 MUFA
31 108.7 108.8 108.80 PUFA
32  89.2  88.6  81.20 MUFA
33  86.8  86.5  82.70 MUFA
34 135.5 130.1 125.40 MUFA
35 112.5 113.9 111.45 MUFA
36 111.0 105.3 109.50 MUFA
37 103.4 100.5  95.50 PUFA
38 117.6 117.4 101.40 PUFA
39 116.7 118.5 101.80 PUFA

But the by function does not work!
> by(hold,Arm,mean,na.rm=TRUE)
Arm: MUFA
[1] NA
----------------------------------------------------------------------------------------------------------------------- 
Arm: PUFA
[1] NA
Warning messages:
1: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA
2: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA


Perhaps this is a hint, print does not give two separate group:
> by(hold,Arm,print)
   Wtscr   Wt0    Wt6
3   97.2  96.2  93.80
4  103.9 106.1 101.70
5   58.2  56.7  55.50
6  130.9 127.4 127.60
7  135.0 133.1 130.90
9  130.3 115.3 115.80
10 150.5 148.7 133.40
14 139.1 138.5 126.70
15  99.1  96.4  95.70
19 202.2  90.1  90.60
20  91.7  89.4  93.40
21 102.1 102.2 100.80
22 116.9 118.9 118.00
23  94.6  95.3  90.30
25 105.6 103.3 103.60
26  96.9  96.8  98.80
28 115.8 118.5 117.30
30  88.2  86.9  88.30
32  89.2  88.6  81.20
33  86.8  86.5  82.70
34 135.5 130.1 125.40
35 112.5 113.9 111.45
36 111.0 105.3 109.50
   Wtscr   Wt0   Wt6
1  120.2 117.2 114.6
2  104.6 105.3 104.8
8  100.1  99.5  84.5
11  74.8  75.7  77.7
12 112.6 110.7 107.4
13  90.0  91.0  83.4
16 108.3 107.5 109.3
17  75.1  72.9  72.2
18  97.5 102.1  98.5
24 122.2 117.0 117.0
27 102.9 100.3  89.0
29  95.7  96.2  95.4
31 108.7 108.8 108.8
37 103.4 100.5  95.5
38 117.6 117.4 101.4
39 116.7 118.5 101.8
Arm: MUFA
   Wtscr   Wt0    Wt6
3   97.2  96.2  93.80
4  103.9 106.1 101.70
5   58.2  56.7  55.50
6  130.9 127.4 127.60
7  135.0 133.1 130.90
9  130.3 115.3 115.80
10 150.5 148.7 133.40
14 139.1 138.5 126.70
15  99.1  96.4  95.70
19 202.2  90.1  90.60
20  91.7  89.4  93.40
21 102.1 102.2 100.80
22 116.9 118.9 118.00
23  94.6  95.3  90.30
25 105.6 103.3 103.60
26  96.9  96.8  98.80
28 115.8 118.5 117.30
30  88.2  86.9  88.30
32  89.2  88.6  81.20
33  86.8  86.5  82.70
34 135.5 130.1 125.40
35 112.5 113.9 111.45
36 111.0 105.3 109.50
----------------------------------------------------------------------------------------------------------------------- 
Arm: PUFA
   Wtscr   Wt0   Wt6
1  120.2 117.2 114.6
2  104.6 105.3 104.8
8  100.1  99.5  84.5
11  74.8  75.7  77.7
12 112.6 110.7 107.4
13  90.0  91.0  83.4
16 108.3 107.5 109.3
17  75.1  72.9  72.2
18  97.5 102.1  98.5
24 122.2 117.0 117.0
27 102.9 100.3  89.0
29  95.7  96.2  95.4
31 108.7 108.8 108.8
37 103.4 100.5  95.5
38 117.6 117.4 101.4
39 116.7 118.5 101.8

But summary works as expected, giving two groups of results!
> by(hold,Arm,summary)
Arm: MUFA
     Wtscr             Wt0             Wt6       
 Min.   : 58.20   Min.   : 56.7   Min.   : 55.5  
 1st Qu.: 95.75   1st Qu.: 92.7   1st Qu.: 92.0  
 Median :105.60   Median :103.3   Median :101.7  
 Mean   :112.75   Mean   :106.3   Mean   :104.0  
 3rd Qu.:130.60   3rd Qu.:118.7   3rd Qu.:117.7  
 Max.   :202.20   Max.   :148.7   Max.   :133.4  
----------------------------------------------------------------------------------------------------------------------- 
Arm: PUFA
     Wtscr             Wt0              Wt6        
 Min.   : 74.80   Min.   : 72.90   Min.   : 72.20  
 1st Qu.: 97.05   1st Qu.: 98.67   1st Qu.: 87.88  
 Median :104.00   Median :103.70   Median : 99.95  
 Mean   :103.15   Mean   :102.54   Mean   : 97.58  
 3rd Qu.:113.62   3rd Qu.:112.28   3rd Qu.:107.75  
 Max.   :122.20   Max.   :118.50   Max.   :117.00  

BY also shows that there are no NAs in the data, and the BY works properly.
> by(hold,Arm,is.na)
Arm: MUFA
   Wtscr

John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From r.turner at auckland.ac.nz  Mon Dec 28 04:39:55 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Mon, 28 Dec 2015 16:39:55 +1300
Subject: [R] by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
In-Reply-To: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>
References: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>
Message-ID: <5680AF0B.40401@auckland.ac.nz>


You are trying to take the mean of data frames.  There is no 
"data.frame" method for mean().

Try:

by(hold,Arm,function(x){sapply(x,mean)})


BTW what's the point of "na.rm=TRUE" in your call?  There are no missing 
values in the data that you present.

In future, please use dput() to present your data; it makes life a lot 
easier for respondents.

cheers,

Rolf

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276

On 28/12/15 15:55, John Sorkin wrote:
> When I run by, I get an error message and no results. Any help in understanding what is wrong would be appreciated.
>
> Error message:
> Warning messages:
> 1: In mean.default(data[x, , drop = FALSE], ...) :
>    argument is not numeric or logical: returning NA
> 2: In mean.default(data[x, , drop = FALSE], ...) :
>    argument is not numeric or logical: returning NA
>
>
> Results:
> Arm: MUFA
> [1] NA
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
> [1] NA
>
> Code:
> by(hold,Arm,mean,na.rm=TRUE)
>
> I don't understand why I am getting the error message, and why I am not getting any results. I don't believe my data are non-numeric.
>
> BY str works fine and confirms that the data are numeric
>> by(hold,Arm,str)
> 'data.frame':	23 obs. of  3 variables:
>   $ Wtscr: num  97.2 103.9 58.2 130.9 135 ...
>   $ Wt0  : num  96.2 106.1 56.7 127.4 133.1 ...
>   $ Wt6  : num  93.8 101.7 55.5 127.6 130.9 ...
> 'data.frame':	16 obs. of  3 variables:
>   $ Wtscr: num  120.2 104.6 100.1 74.8 112.6 ...
>   $ Wt0  : num  117.2 105.3 99.5 75.7 110.7 ...
>   $ Wt6  : num  114.6 104.8 84.5 77.7 107.4 ...
>   Here is a listing of my data:
>> hold
>     Wtscr   Wt0    Wt6
> 1  120.2 117.2 114.60
> 2  104.6 105.3 104.80
> 3   97.2  96.2  93.80
> 4  103.9 106.1 101.70
> 5   58.2  56.7  55.50
> 6  130.9 127.4 127.60
> 7  135.0 133.1 130.90
> 8  100.1  99.5  84.50
> 9  130.3 115.3 115.80
> 10 150.5 148.7 133.40
> 11  74.8  75.7  77.70
> 12 112.6 110.7 107.40
> 13  90.0  91.0  83.40
> 14 139.1 138.5 126.70
> 15  99.1  96.4  95.70
> 16 108.3 107.5 109.30
> 17  75.1  72.9  72.20
> 18  97.5 102.1  98.50
> 19 202.2  90.1  90.60
> 20  91.7  89.4  93.40
> 21 102.1 102.2 100.80
> 22 116.9 118.9 118.00
> 23  94.6  95.3  90.30
> 24 122.2 117.0 117.00
> 25 105.6 103.3 103.60
> 26  96.9  96.8  98.80
> 27 102.9 100.3  89.00
> 28 115.8 118.5 117.30
> 29  95.7  96.2  95.40
> 30  88.2  86.9  88.30
> 31 108.7 108.8 108.80
> 32  89.2  88.6  81.20
> 33  86.8  86.5  82.70
> 34 135.5 130.1 125.40
> 35 112.5 113.9 111.45
> 36 111.0 105.3 109.50
> 37 103.4 100.5  95.50
> 38 117.6 117.4 101.40
> 39 116.7 118.5 101.80
>
> The INDEX is clearly a factor:
>> Arm
>   [1] PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA MUFA PUFA MUFA PUFA
> [32] MUFA MUFA MUFA MUFA MUFA PUFA PUFA PUFA
> Levels: MUFA PUFA
>
> The data and the index have the same length:
>> cbind(hold,Arm)
>     Wtscr   Wt0    Wt6  Arm
> 1  120.2 117.2 114.60 PUFA
> 2  104.6 105.3 104.80 PUFA
> 3   97.2  96.2  93.80 MUFA
> 4  103.9 106.1 101.70 MUFA
> 5   58.2  56.7  55.50 MUFA
> 6  130.9 127.4 127.60 MUFA
> 7  135.0 133.1 130.90 MUFA
> 8  100.1  99.5  84.50 PUFA
> 9  130.3 115.3 115.80 MUFA
> 10 150.5 148.7 133.40 MUFA
> 11  74.8  75.7  77.70 PUFA
> 12 112.6 110.7 107.40 PUFA
> 13  90.0  91.0  83.40 PUFA
> 14 139.1 138.5 126.70 MUFA
> 15  99.1  96.4  95.70 MUFA
> 16 108.3 107.5 109.30 PUFA
> 17  75.1  72.9  72.20 PUFA
> 18  97.5 102.1  98.50 PUFA
> 19 202.2  90.1  90.60 MUFA
> 20  91.7  89.4  93.40 MUFA
> 21 102.1 102.2 100.80 MUFA
> 22 116.9 118.9 118.00 MUFA
> 23  94.6  95.3  90.30 MUFA
> 24 122.2 117.0 117.00 PUFA
> 25 105.6 103.3 103.60 MUFA
> 26  96.9  96.8  98.80 MUFA
> 27 102.9 100.3  89.00 PUFA
> 28 115.8 118.5 117.30 MUFA
> 29  95.7  96.2  95.40 PUFA
> 30  88.2  86.9  88.30 MUFA
> 31 108.7 108.8 108.80 PUFA
> 32  89.2  88.6  81.20 MUFA
> 33  86.8  86.5  82.70 MUFA
> 34 135.5 130.1 125.40 MUFA
> 35 112.5 113.9 111.45 MUFA
> 36 111.0 105.3 109.50 MUFA
> 37 103.4 100.5  95.50 PUFA
> 38 117.6 117.4 101.40 PUFA
> 39 116.7 118.5 101.80 PUFA
>
> But the by function does not work!
>> by(hold,Arm,mean,na.rm=TRUE)
> Arm: MUFA
> [1] NA
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
> [1] NA
> Warning messages:
> 1: In mean.default(data[x, , drop = FALSE], ...) :
>    argument is not numeric or logical: returning NA
> 2: In mean.default(data[x, , drop = FALSE], ...) :
>    argument is not numeric or logical: returning NA
>
>
> Perhaps this is a hint, print does not give two separate group:
>> by(hold,Arm,print)
>     Wtscr   Wt0    Wt6
> 3   97.2  96.2  93.80
> 4  103.9 106.1 101.70
> 5   58.2  56.7  55.50
> 6  130.9 127.4 127.60
> 7  135.0 133.1 130.90
> 9  130.3 115.3 115.80
> 10 150.5 148.7 133.40
> 14 139.1 138.5 126.70
> 15  99.1  96.4  95.70
> 19 202.2  90.1  90.60
> 20  91.7  89.4  93.40
> 21 102.1 102.2 100.80
> 22 116.9 118.9 118.00
> 23  94.6  95.3  90.30
> 25 105.6 103.3 103.60
> 26  96.9  96.8  98.80
> 28 115.8 118.5 117.30
> 30  88.2  86.9  88.30
> 32  89.2  88.6  81.20
> 33  86.8  86.5  82.70
> 34 135.5 130.1 125.40
> 35 112.5 113.9 111.45
> 36 111.0 105.3 109.50
>     Wtscr   Wt0   Wt6
> 1  120.2 117.2 114.6
> 2  104.6 105.3 104.8
> 8  100.1  99.5  84.5
> 11  74.8  75.7  77.7
> 12 112.6 110.7 107.4
> 13  90.0  91.0  83.4
> 16 108.3 107.5 109.3
> 17  75.1  72.9  72.2
> 18  97.5 102.1  98.5
> 24 122.2 117.0 117.0
> 27 102.9 100.3  89.0
> 29  95.7  96.2  95.4
> 31 108.7 108.8 108.8
> 37 103.4 100.5  95.5
> 38 117.6 117.4 101.4
> 39 116.7 118.5 101.8
> Arm: MUFA
>     Wtscr   Wt0    Wt6
> 3   97.2  96.2  93.80
> 4  103.9 106.1 101.70
> 5   58.2  56.7  55.50
> 6  130.9 127.4 127.60
> 7  135.0 133.1 130.90
> 9  130.3 115.3 115.80
> 10 150.5 148.7 133.40
> 14 139.1 138.5 126.70
> 15  99.1  96.4  95.70
> 19 202.2  90.1  90.60
> 20  91.7  89.4  93.40
> 21 102.1 102.2 100.80
> 22 116.9 118.9 118.00
> 23  94.6  95.3  90.30
> 25 105.6 103.3 103.60
> 26  96.9  96.8  98.80
> 28 115.8 118.5 117.30
> 30  88.2  86.9  88.30
> 32  89.2  88.6  81.20
> 33  86.8  86.5  82.70
> 34 135.5 130.1 125.40
> 35 112.5 113.9 111.45
> 36 111.0 105.3 109.50
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
>     Wtscr   Wt0   Wt6
> 1  120.2 117.2 114.6
> 2  104.6 105.3 104.8
> 8  100.1  99.5  84.5
> 11  74.8  75.7  77.7
> 12 112.6 110.7 107.4
> 13  90.0  91.0  83.4
> 16 108.3 107.5 109.3
> 17  75.1  72.9  72.2
> 18  97.5 102.1  98.5
> 24 122.2 117.0 117.0
> 27 102.9 100.3  89.0
> 29  95.7  96.2  95.4
> 31 108.7 108.8 108.8
> 37 103.4 100.5  95.5
> 38 117.6 117.4 101.4
> 39 116.7 118.5 101.8
>
> But summary works as expected, giving two groups of results!
>> by(hold,Arm,summary)
> Arm: MUFA
>       Wtscr             Wt0             Wt6
>   Min.   : 58.20   Min.   : 56.7   Min.   : 55.5
>   1st Qu.: 95.75   1st Qu.: 92.7   1st Qu.: 92.0
>   Median :105.60   Median :103.3   Median :101.7
>   Mean   :112.75   Mean   :106.3   Mean   :104.0
>   3rd Qu.:130.60   3rd Qu.:118.7   3rd Qu.:117.7
>   Max.   :202.20   Max.   :148.7   Max.   :133.4
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
>       Wtscr             Wt0              Wt6
>   Min.   : 74.80   Min.   : 72.90   Min.   : 72.20
>   1st Qu.: 97.05   1st Qu.: 98.67   1st Qu.: 87.88
>   Median :104.00   Median :103.70   Median : 99.95
>   Mean   :103.15   Mean   :102.54   Mean   : 97.58
>   3rd Qu.:113.62   3rd Qu.:112.28   3rd Qu.:107.75
>   Max.   :122.20   Max.   :118.50   Max.   :117.00
>
> BY also shows that there are no NAs in the data, and the BY works properly.
>> by(hold,Arm,is.na)
> Arm: MUFA
>     Wtscr


From jsorkin at grecc.umaryland.edu  Mon Dec 28 05:47:57 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Sun, 27 Dec 2015 23:47:57 -0500
Subject: [R] by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
In-Reply-To: <5680AF0B.40401@auckland.ac.nz>
References: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>
	<5680AF0B.40401@auckland.ac.nz>
Message-ID: <568078BA020000CB00144C90@smtp.medicine.umaryland.edu>

Rolf,
Thank you!
John

> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)


> On Dec 27, 2015, at 10:40 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
> 
> You are trying to take the mean of data frames.  There is no 
> "data.frame" method for mean().
> 
> Try:
> 
> by(hold,Arm,function(x){sapply(x,mean)})
> 
> 
> BTW what's the point of "na.rm=TRUE" in your call?  There are no missing 
> values in the data that you present.
> 
> In future, please use dput() to present your data; it makes life a lot 
> easier for respondents.
> 
> cheers,
> 
> Rolf
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
> 
>> On 28/12/15 15:55, John Sorkin wrote:
>> When I run by, I get an error message and no results. Any help in understanding what is wrong would be appreciated.
>> 
>> Error message:
>> Warning messages:
>> 1: In mean.default(data[x, , drop = FALSE], ...) :
>>   argument is not numeric or logical: returning NA
>> 2: In mean.default(data[x, , drop = FALSE], ...) :
>>   argument is not numeric or logical: returning NA
>> 
>> 
>> Results:
>> Arm: MUFA
>> [1] NA
>> -----------------------------------------------------------------------------------------------------------------------
>> Arm: PUFA
>> [1] NA
>> 
>> Code:
>> by(hold,Arm,mean,na.rm=TRUE)
>> 
>> I don't understand why I am getting the error message, and why I am not getting any results. I don't believe my data are non-numeric.
>> 
>> BY str works fine and confirms that the data are numeric
>>> by(hold,Arm,str)
>> 'data.frame':    23 obs. of  3 variables:
>>  $ Wtscr: num  97.2 103.9 58.2 130.9 135 ...
>>  $ Wt0  : num  96.2 106.1 56.7 127.4 133.1 ...
>>  $ Wt6  : num  93.8 101.7 55.5 127.6 130.9 ...
>> 'data.frame':    16 obs. of  3 variables:
>>  $ Wtscr: num  120.2 104.6 100.1 74.8 112.6 ...
>>  $ Wt0  : num  117.2 105.3 99.5 75.7 110.7 ...
>>  $ Wt6  : num  114.6 104.8 84.5 77.7 107.4 ...
>>  Here is a listing of my data:
>>> hold
>>    Wtscr   Wt0    Wt6
>> 1  120.2 117.2 114.60
>> 2  104.6 105.3 104.80
>> 3   97.2  96.2  93.80
>> 4  103.9 106.1 101.70
>> 5   58.2  56.7  55.50
>> 6  130.9 127.4 127.60
>> 7  135.0 133.1 130.90
>> 8  100.1  99.5  84.50
>> 9  130.3 115.3 115.80
>> 10 150.5 148.7 133.40
>> 11  74.8  75.7  77.70
>> 12 112.6 110.7 107.40
>> 13  90.0  91.0  83.40
>> 14 139.1 138.5 126.70
>> 15  99.1  96.4  95.70
>> 16 108.3 107.5 109.30
>> 17  75.1  72.9  72.20
>> 18  97.5 102.1  98.50
>> 19 202.2  90.1  90.60
>> 20  91.7  89.4  93.40
>> 21 102.1 102.2 100.80
>> 22 116.9 118.9 118.00
>> 23  94.6  95.3  90.30
>> 24 122.2 117.0 117.00
>> 25 105.6 103.3 103.60
>> 26  96.9  96.8  98.80
>> 27 102.9 100.3  89.00
>> 28 115.8 118.5 117.30
>> 29  95.7  96.2  95.40
>> 30  88.2  86.9  88.30
>> 31 108.7 108.8 108.80
>> 32  89.2  88.6  81.20
>> 33  86.8  86.5  82.70
>> 34 135.5 130.1 125.40
>> 35 112.5 113.9 111.45
>> 36 111.0 105.3 109.50
>> 37 103.4 100.5  95.50
>> 38 117.6 117.4 101.40
>> 39 116.7 118.5 101.80
>> 
>> The INDEX is clearly a factor:
>>> Arm
>>  [1] PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA MUFA PUFA MUFA PUFA
>> [32] MUFA MUFA MUFA MUFA MUFA PUFA PUFA PUFA
>> Levels: MUFA PUFA
>> 
>> The data and the index have the same length:
>>> cbind(hold,Arm)
>>    Wtscr   Wt0    Wt6  Arm
>> 1  120.2 117.2 114.60 PUFA
>> 2  104.6 105.3 104.80 PUFA
>> 3   97.2  96.2  93.80 MUFA
>> 4  103.9 106.1 101.70 MUFA
>> 5   58.2  56.7  55.50 MUFA
>> 6  130.9 127.4 127.60 MUFA
>> 7  135.0 133.1 130.90 MUFA
>> 8  100.1  99.5  84.50 PUFA
>> 9  130.3 115.3 115.80 MUFA
>> 10 150.5 148.7 133.40 MUFA
>> 11  74.8  75.7  77.70 PUFA
>> 12 112.6 110.7 107.40 PUFA
>> 13  90.0  91.0  83.40 PUFA
>> 14 139.1 138.5 126.70 MUFA
>> 15  99.1  96.4  95.70 MUFA
>> 16 108.3 107.5 109.30 PUFA
>> 17  75.1  72.9  72.20 PUFA
>> 18  97.5 102.1  98.50 PUFA
>> 19 202.2  90.1  90.60 MUFA
>> 20  91.7  89.4  93.40 MUFA
>> 21 102.1 102.2 100.80 MUFA
>> 22 116.9 118.9 118.00 MUFA
>> 23  94.6  95.3  90.30 MUFA
>> 24 122.2 117.0 117.00 PUFA
>> 25 105.6 103.3 103.60 MUFA
>> 26  96.9  96.8  98.80 MUFA
>> 27 102.9 100.3  89.00 PUFA
>> 28 115.8 118.5 117.30 MUFA
>> 29  95.7  96.2  95.40 PUFA
>> 30  88.2  86.9  88.30 MUFA
>> 31 108.7 108.8 108.80 PUFA
>> 32  89.2  88.6  81.20 MUFA
>> 33  86.8  86.5  82.70 MUFA
>> 34 135.5 130.1 125.40 MUFA
>> 35 112.5 113.9 111.45 MUFA
>> 36 111.0 105.3 109.50 MUFA
>> 37 103.4 100.5  95.50 PUFA
>> 38 117.6 117.4 101.40 PUFA
>> 39 116.7 118.5 101.80 PUFA
>> 
>> But the by function does not work!
>>> by(hold,Arm,mean,na.rm=TRUE)
>> Arm: MUFA
>> [1] NA
>> -----------------------------------------------------------------------------------------------------------------------
>> Arm: PUFA
>> [1] NA
>> Warning messages:
>> 1: In mean.default(data[x, , drop = FALSE], ...) :
>>   argument is not numeric or logical: returning NA
>> 2: In mean.default(data[x, , drop = FALSE], ...) :
>>   argument is not numeric or logical: returning NA
>> 
>> 
>> Perhaps this is a hint, print does not give two separate group:
>>> by(hold,Arm,print)
>>    Wtscr   Wt0    Wt6
>> 3   97.2  96.2  93.80
>> 4  103.9 106.1 101.70
>> 5   58.2  56.7  55.50
>> 6  130.9 127.4 127.60
>> 7  135.0 133.1 130.90
>> 9  130.3 115.3 115.80
>> 10 150.5 148.7 133.40
>> 14 139.1 138.5 126.70
>> 15  99.1  96.4  95.70
>> 19 202.2  90.1  90.60
>> 20  91.7  89.4  93.40
>> 21 102.1 102.2 100.80
>> 22 116.9 118.9 118.00
>> 23  94.6  95.3  90.30
>> 25 105.6 103.3 103.60
>> 26  96.9  96.8  98.80
>> 28 115.8 118.5 117.30
>> 30  88.2  86.9  88.30
>> 32  89.2  88.6  81.20
>> 33  86.8  86.5  82.70
>> 34 135.5 130.1 125.40
>> 35 112.5 113.9 111.45
>> 36 111.0 105.3 109.50
>>    Wtscr   Wt0   Wt6
>> 1  120.2 117.2 114.6
>> 2  104.6 105.3 104.8
>> 8  100.1  99.5  84.5
>> 11  74.8  75.7  77.7
>> 12 112.6 110.7 107.4
>> 13  90.0  91.0  83.4
>> 16 108.3 107.5 109.3
>> 17  75.1  72.9  72.2
>> 18  97.5 102.1  98.5
>> 24 122.2 117.0 117.0
>> 27 102.9 100.3  89.0
>> 29  95.7  96.2  95.4
>> 31 108.7 108.8 108.8
>> 37 103.4 100.5  95.5
>> 38 117.6 117.4 101.4
>> 39 116.7 118.5 101.8
>> Arm: MUFA
>>    Wtscr   Wt0    Wt6
>> 3   97.2  96.2  93.80
>> 4  103.9 106.1 101.70
>> 5   58.2  56.7  55.50
>> 6  130.9 127.4 127.60
>> 7  135.0 133.1 130.90
>> 9  130.3 115.3 115.80
>> 10 150.5 148.7 133.40
>> 14 139.1 138.5 126.70
>> 15  99.1  96.4  95.70
>> 19 202.2  90.1  90.60
>> 20  91.7  89.4  93.40
>> 21 102.1 102.2 100.80
>> 22 116.9 118.9 118.00
>> 23  94.6  95.3  90.30
>> 25 105.6 103.3 103.60
>> 26  96.9  96.8  98.80
>> 28 115.8 118.5 117.30
>> 30  88.2  86.9  88.30
>> 32  89.2  88.6  81.20
>> 33  86.8  86.5  82.70
>> 34 135.5 130.1 125.40
>> 35 112.5 113.9 111.45
>> 36 111.0 105.3 109.50
>> -----------------------------------------------------------------------------------------------------------------------
>> Arm: PUFA
>>    Wtscr   Wt0   Wt6
>> 1  120.2 117.2 114.6
>> 2  104.6 105.3 104.8
>> 8  100.1  99.5  84.5
>> 11  74.8  75.7  77.7
>> 12 112.6 110.7 107.4
>> 13  90.0  91.0  83.4
>> 16 108.3 107.5 109.3
>> 17  75.1  72.9  72.2
>> 18  97.5 102.1  98.5
>> 24 122.2 117.0 117.0
>> 27 102.9 100.3  89.0
>> 29  95.7  96.2  95.4
>> 31 108.7 108.8 108.8
>> 37 103.4 100.5  95.5
>> 38 117.6 117.4 101.4
>> 39 116.7 118.5 101.8
>> 
>> But summary works as expected, giving two groups of results!
>>> by(hold,Arm,summary)
>> Arm: MUFA
>>      Wtscr             Wt0             Wt6
>>  Min.   : 58.20   Min.   : 56.7   Min.   : 55.5
>>  1st Qu.: 95.75   1st Qu.: 92.7   1st Qu.: 92.0
>>  Median :105.60   Median :103.3   Median :101.7
>>  Mean   :112.75   Mean   :106.3   Mean   :104.0
>>  3rd Qu.:130.60   3rd Qu.:118.7   3rd Qu.:117.7
>>  Max.   :202.20   Max.   :148.7   Max.   :133.4
>> -----------------------------------------------------------------------------------------------------------------------
>> Arm: PUFA
>>      Wtscr             Wt0              Wt6
>>  Min.   : 74.80   Min.   : 72.90   Min.   : 72.20
>>  1st Qu.: 97.05   1st Qu.: 98.67   1st Qu.: 87.88
>>  Median :104.00   Median :103.70   Median : 99.95
>>  Mean   :103.15   Mean   :102.54   Mean   : 97.58
>>  3rd Qu.:113.62   3rd Qu.:112.28   3rd Qu.:107.75
>>  Max.   :122.20   Max.   :118.50   Max.   :117.00
>> 
>> BY also shows that there are no NAs in the data, and the BY works properly.
>>> by(hold,Arm,is.na)
>> Arm: MUFA
>>    Wtscr

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From wdunlap at tibco.com  Mon Dec 28 06:54:50 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Sun, 27 Dec 2015 21:54:50 -0800
Subject: [R] by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
In-Reply-To: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>
References: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>
Message-ID: <CAF8bMcZq-Ay=ZAbQyj=8aS8o8D+gFYoawP9=xgWoZ7x15-sfXA@mail.gmail.com>

by(dataFrame, groupId, FUN) applies FUN a bunch of data.frames (row subsets
of the dataFrame input).  mean() returns NA for data.frames.  You could use
FUN=colMeans if you wanted column means or FUN=function(x)mean(colMeans(x))
or FUN=function(x)mean(unlist(x)) if you wanted some version of a grand mean
over all the columns.

If you want column means, you may find aggregate() more suited to the job,
as it
applies FUN to each column in each row subset of the data and returns a
data.frame
instead of a list of outputs of FUN.
  > aggregate(mtcars[,3:5], mtcars[,2,drop=FALSE], mean)
    cyl     disp        hp     drat
  1   4 105.1364  82.63636 4.070909
  2   6 183.3143 122.28571 3.585714
  3   8 353.1000 209.21429 3.229286



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Dec 27, 2015 at 6:55 PM, John Sorkin <jsorkin at grecc.umaryland.edu>
wrote:

> When I run by, I get an error message and no results. Any help in
> understanding what is wrong would be appreciated.
>
> Error message:
> Warning messages:
> 1: In mean.default(data[x, , drop = FALSE], ...) :
>   argument is not numeric or logical: returning NA
> 2: In mean.default(data[x, , drop = FALSE], ...) :
>   argument is not numeric or logical: returning NA
>
>
> Results:
> Arm: MUFA
> [1] NA
>
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
> [1] NA
>
> Code:
> by(hold,Arm,mean,na.rm=TRUE)
>
> I don't understand why I am getting the error message, and why I am not
> getting any results. I don't believe my data are non-numeric.
>
> BY str works fine and confirms that the data are numeric
> > by(hold,Arm,str)
> 'data.frame':   23 obs. of  3 variables:
>  $ Wtscr: num  97.2 103.9 58.2 130.9 135 ...
>  $ Wt0  : num  96.2 106.1 56.7 127.4 133.1 ...
>  $ Wt6  : num  93.8 101.7 55.5 127.6 130.9 ...
> 'data.frame':   16 obs. of  3 variables:
>  $ Wtscr: num  120.2 104.6 100.1 74.8 112.6 ...
>  $ Wt0  : num  117.2 105.3 99.5 75.7 110.7 ...
>  $ Wt6  : num  114.6 104.8 84.5 77.7 107.4 ...
>  Here is a listing of my data:
> > hold
>    Wtscr   Wt0    Wt6
> 1  120.2 117.2 114.60
> 2  104.6 105.3 104.80
> 3   97.2  96.2  93.80
> 4  103.9 106.1 101.70
> 5   58.2  56.7  55.50
> 6  130.9 127.4 127.60
> 7  135.0 133.1 130.90
> 8  100.1  99.5  84.50
> 9  130.3 115.3 115.80
> 10 150.5 148.7 133.40
> 11  74.8  75.7  77.70
> 12 112.6 110.7 107.40
> 13  90.0  91.0  83.40
> 14 139.1 138.5 126.70
> 15  99.1  96.4  95.70
> 16 108.3 107.5 109.30
> 17  75.1  72.9  72.20
> 18  97.5 102.1  98.50
> 19 202.2  90.1  90.60
> 20  91.7  89.4  93.40
> 21 102.1 102.2 100.80
> 22 116.9 118.9 118.00
> 23  94.6  95.3  90.30
> 24 122.2 117.0 117.00
> 25 105.6 103.3 103.60
> 26  96.9  96.8  98.80
> 27 102.9 100.3  89.00
> 28 115.8 118.5 117.30
> 29  95.7  96.2  95.40
> 30  88.2  86.9  88.30
> 31 108.7 108.8 108.80
> 32  89.2  88.6  81.20
> 33  86.8  86.5  82.70
> 34 135.5 130.1 125.40
> 35 112.5 113.9 111.45
> 36 111.0 105.3 109.50
> 37 103.4 100.5  95.50
> 38 117.6 117.4 101.40
> 39 116.7 118.5 101.80
>
> The INDEX is clearly a factor:
> > Arm
>  [1] PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA
> MUFA PUFA PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA MUFA PUFA
> MUFA PUFA
> [32] MUFA MUFA MUFA MUFA MUFA PUFA PUFA PUFA
> Levels: MUFA PUFA
>
> The data and the index have the same length:
> > cbind(hold,Arm)
>    Wtscr   Wt0    Wt6  Arm
> 1  120.2 117.2 114.60 PUFA
> 2  104.6 105.3 104.80 PUFA
> 3   97.2  96.2  93.80 MUFA
> 4  103.9 106.1 101.70 MUFA
> 5   58.2  56.7  55.50 MUFA
> 6  130.9 127.4 127.60 MUFA
> 7  135.0 133.1 130.90 MUFA
> 8  100.1  99.5  84.50 PUFA
> 9  130.3 115.3 115.80 MUFA
> 10 150.5 148.7 133.40 MUFA
> 11  74.8  75.7  77.70 PUFA
> 12 112.6 110.7 107.40 PUFA
> 13  90.0  91.0  83.40 PUFA
> 14 139.1 138.5 126.70 MUFA
> 15  99.1  96.4  95.70 MUFA
> 16 108.3 107.5 109.30 PUFA
> 17  75.1  72.9  72.20 PUFA
> 18  97.5 102.1  98.50 PUFA
> 19 202.2  90.1  90.60 MUFA
> 20  91.7  89.4  93.40 MUFA
> 21 102.1 102.2 100.80 MUFA
> 22 116.9 118.9 118.00 MUFA
> 23  94.6  95.3  90.30 MUFA
> 24 122.2 117.0 117.00 PUFA
> 25 105.6 103.3 103.60 MUFA
> 26  96.9  96.8  98.80 MUFA
> 27 102.9 100.3  89.00 PUFA
> 28 115.8 118.5 117.30 MUFA
> 29  95.7  96.2  95.40 PUFA
> 30  88.2  86.9  88.30 MUFA
> 31 108.7 108.8 108.80 PUFA
> 32  89.2  88.6  81.20 MUFA
> 33  86.8  86.5  82.70 MUFA
> 34 135.5 130.1 125.40 MUFA
> 35 112.5 113.9 111.45 MUFA
> 36 111.0 105.3 109.50 MUFA
> 37 103.4 100.5  95.50 PUFA
> 38 117.6 117.4 101.40 PUFA
> 39 116.7 118.5 101.80 PUFA
>
> But the by function does not work!
> > by(hold,Arm,mean,na.rm=TRUE)
> Arm: MUFA
> [1] NA
>
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
> [1] NA
> Warning messages:
> 1: In mean.default(data[x, , drop = FALSE], ...) :
>   argument is not numeric or logical: returning NA
> 2: In mean.default(data[x, , drop = FALSE], ...) :
>   argument is not numeric or logical: returning NA
>
>
> Perhaps this is a hint, print does not give two separate group:
> > by(hold,Arm,print)
>    Wtscr   Wt0    Wt6
> 3   97.2  96.2  93.80
> 4  103.9 106.1 101.70
> 5   58.2  56.7  55.50
> 6  130.9 127.4 127.60
> 7  135.0 133.1 130.90
> 9  130.3 115.3 115.80
> 10 150.5 148.7 133.40
> 14 139.1 138.5 126.70
> 15  99.1  96.4  95.70
> 19 202.2  90.1  90.60
> 20  91.7  89.4  93.40
> 21 102.1 102.2 100.80
> 22 116.9 118.9 118.00
> 23  94.6  95.3  90.30
> 25 105.6 103.3 103.60
> 26  96.9  96.8  98.80
> 28 115.8 118.5 117.30
> 30  88.2  86.9  88.30
> 32  89.2  88.6  81.20
> 33  86.8  86.5  82.70
> 34 135.5 130.1 125.40
> 35 112.5 113.9 111.45
> 36 111.0 105.3 109.50
>    Wtscr   Wt0   Wt6
> 1  120.2 117.2 114.6
> 2  104.6 105.3 104.8
> 8  100.1  99.5  84.5
> 11  74.8  75.7  77.7
> 12 112.6 110.7 107.4
> 13  90.0  91.0  83.4
> 16 108.3 107.5 109.3
> 17  75.1  72.9  72.2
> 18  97.5 102.1  98.5
> 24 122.2 117.0 117.0
> 27 102.9 100.3  89.0
> 29  95.7  96.2  95.4
> 31 108.7 108.8 108.8
> 37 103.4 100.5  95.5
> 38 117.6 117.4 101.4
> 39 116.7 118.5 101.8
> Arm: MUFA
>    Wtscr   Wt0    Wt6
> 3   97.2  96.2  93.80
> 4  103.9 106.1 101.70
> 5   58.2  56.7  55.50
> 6  130.9 127.4 127.60
> 7  135.0 133.1 130.90
> 9  130.3 115.3 115.80
> 10 150.5 148.7 133.40
> 14 139.1 138.5 126.70
> 15  99.1  96.4  95.70
> 19 202.2  90.1  90.60
> 20  91.7  89.4  93.40
> 21 102.1 102.2 100.80
> 22 116.9 118.9 118.00
> 23  94.6  95.3  90.30
> 25 105.6 103.3 103.60
> 26  96.9  96.8  98.80
> 28 115.8 118.5 117.30
> 30  88.2  86.9  88.30
> 32  89.2  88.6  81.20
> 33  86.8  86.5  82.70
> 34 135.5 130.1 125.40
> 35 112.5 113.9 111.45
> 36 111.0 105.3 109.50
>
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
>    Wtscr   Wt0   Wt6
> 1  120.2 117.2 114.6
> 2  104.6 105.3 104.8
> 8  100.1  99.5  84.5
> 11  74.8  75.7  77.7
> 12 112.6 110.7 107.4
> 13  90.0  91.0  83.4
> 16 108.3 107.5 109.3
> 17  75.1  72.9  72.2
> 18  97.5 102.1  98.5
> 24 122.2 117.0 117.0
> 27 102.9 100.3  89.0
> 29  95.7  96.2  95.4
> 31 108.7 108.8 108.8
> 37 103.4 100.5  95.5
> 38 117.6 117.4 101.4
> 39 116.7 118.5 101.8
>
> But summary works as expected, giving two groups of results!
> > by(hold,Arm,summary)
> Arm: MUFA
>      Wtscr             Wt0             Wt6
>  Min.   : 58.20   Min.   : 56.7   Min.   : 55.5
>  1st Qu.: 95.75   1st Qu.: 92.7   1st Qu.: 92.0
>  Median :105.60   Median :103.3   Median :101.7
>  Mean   :112.75   Mean   :106.3   Mean   :104.0
>  3rd Qu.:130.60   3rd Qu.:118.7   3rd Qu.:117.7
>  Max.   :202.20   Max.   :148.7   Max.   :133.4
>
> -----------------------------------------------------------------------------------------------------------------------
> Arm: PUFA
>      Wtscr             Wt0              Wt6
>  Min.   : 74.80   Min.   : 72.90   Min.   : 72.20
>  1st Qu.: 97.05   1st Qu.: 98.67   1st Qu.: 87.88
>  Median :104.00   Median :103.70   Median : 99.95
>  Mean   :103.15   Mean   :102.54   Mean   : 97.58
>  3rd Qu.:113.62   3rd Qu.:112.28   3rd Qu.:107.75
>  Max.   :122.20   Max.   :118.50   Max.   :117.00
>
> BY also shows that there are no NAs in the data, and the BY works properly.
> > by(hold,Arm,is.na)
> Arm: MUFA
>    Wtscr
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From petr.pikal at precheza.cz  Mon Dec 28 09:15:10 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Mon, 28 Dec 2015 08:15:10 +0000
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>

Hi

On top of answers you have got here is some plotting you need to answer yourself

plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)

Which from those red points you want to be included in irrigation period? All of them? Only part? Which part?

Based on your figures you probably will not get 100% correct answer.

Cheers
Petr



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Makram
> Belhaj Fraj
> Sent: Wednesday, December 23, 2015 8:35 AM
> To: r-help at r-project.org; r-help-owner at r-project.org
> Subject: [R] need for help for solving operations in a vector
>
>  Dear colleagues
> i need your generous help to solve the following problem
>
> I have a  soil moisture time series qWC1 (61 values)
> > qWC1
>  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> 74.70059
> 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> 80.69793
> 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> 75.97412
> 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> 75.46087
> 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> 75.07930 75.07930 74.95275 74.95275  74.95275
>
> I want to measure consecutive increases corresponding to irrigation and
> consecutive decreases  corresponding to recharge I wrote the following
> code and it does not calculate for each increment in i?
> also note that I choose to not use diff command in time series because
> I  want also that "plateaux" corresponding to a minimum of 2 equal
> consecutive values are accounted as positive differences=irrigations so
> when x[i+1]==x[i] the difference y might be equal to the previous value
> xi
>
> following the code i wrote
>
> x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> "green")
> y<-rep(0,61)
> for (i in 1:61) {
> if (x[i+1] > x[i]){
>     y[i]==x[i+1]-x[i]
> } else if (x[i+1]==x[i]){
>     y[i]=x[i+2]-x[i]
> } else {
>     y[i]==x[i+1]-x[i]
> }
>
> }
> plot(y, type="h", col = "blueviolet")
>
> Many thank
> Makram
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.

________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

From calum.polwart at nhs.net  Mon Dec 28 11:19:33 2015
From: calum.polwart at nhs.net (Polwart Calum (COUNTY DURHAM AND DARLINGTON NHS FOUNDATION TRUST))
Date: Mon, 28 Dec 2015 10:19:33 +0000
Subject: [R] ?currency challenge ?knapsack challenge ?probabilities
In-Reply-To: <20151227220451.DADC931815E@nhs-pd1e-esg007.ad1.nhs.net>
References: <20151227110907.A06DB449194@nhs-pd1e-esg107.ad1.nhs.net>
	<20151227220451.DADC931815E@nhs-pd1e-esg007.ad1.nhs.net>
Message-ID: <20151228101941.B61DE44995B@nhs-pd1e-esg110.ad1.nhs.net>

Hi Jim

Working on basis of exact match.  but the 25% inncrements are rounded to imtegers, so like buying from a shop priced in whole numbers but changeis what you expect not 'roughly right'

Thanks

Calum

On 27 Dec 2015, at 22:04, Jim Lemon <drjimlemon at gmail.com<mailto:drjimlemon at gmail.com>> wrote:
Hi Calum,
Does this include an error tolerance for the match between the ordered and delivered quantities? That is, is it okay to have a maximum of one unit difference or do deliveries have to exactly match orders?

Jim


On Sun, Dec 27, 2015 at 10:08 PM, Polwart Calum (COUNTY DURHAM AND DARLINGTON NHS FOUNDATION TRUST) <calum.polwart at nhs.net<mailto:calum.polwart at nhs.net>> wrote:
I am currently working on a project that is producing Gigabyte sized vectors that kill R.  It can take 24 hours to run. So frustrating to get that far and have to scale back my inputs and rerun...


The problem i'm trying to solve as i see it is very similar to optimal currency denominations problems combined with a knapsack problem. Let me TRY to explain.


We have a product that we want to manufacture in as few sizes as possible.  like currency if you want 30cents but a 30cent coin doesnt exist you can join multiple products together. (3x10 cent, 25cent +5 cent, etc)


Unlike currency we dont need every value to be possible, we have a list of known values which are effectively related to each other by the next size up being 25% bigger. So for instance 64, 80 100.


We have some rules that say you can't use more than X products combined to make the final size.  A bit like saying never give more than 10 coins as change, so you cant issue 20x5cents for a dollar of change.


All of that fits a standard currency denomination challenge.


We dont need the combinations to be calculated using greedy method. [We will calculate and store as a table]


BUT - we do have a manufacturing limitation that means can manufacture to any whole number size, we cant do smaller than size5. (We dont go as low as that anyway... size 11 is as low as needed).  So different from any currency problem I've seen where the lowest coin size is always a 1 allowing any size to be produced.


So i have three questions I'm trying to answer:


- what is the smallest product range we can make that achieves our rules for max combinations of sizes?


- Is there a more optimal range. Say the smallest range was 4 sizes, for example 5,6,23,40.  Its possible adding a 22 and a 46 to that may actually be cheaper than supplying 2x5 and 2x6 or 2x23...

Currently I'm identifying every possible combination into a matrix.  We have a manufacturing constraint of max size 49 as well.  So i take every end user size possible (from 11 thru to 125).  For each size i then take every combination of possible sizes from 5 to 49 (45 sizes) that we COULD make and work out how i can achieve all the possible end user sizes, discarding any combinations that break our rules for max combinations.

Thats a giant set of for loops.  Once i establish the options we  can apply the manufacturing costs and usage data to find the answer.

For now 45 sizes,combined in any of up to 5 different combinations to do 10 end user sizes is creating vectors too big for R to handle...

Long explanation of the problem, to basically say... has anyone come across a function in R that might simplify this?



Sent from TypeMail<http://www.typeapp.com/r>


On 27 Dec 2015, at 08:00, "Polwart Calum (COUNTY DURHAM AND DARLINGTON NHS FOUNDATION TRUST)" <calum.polwart at nhs.net<mailto:calum.polwart at nhs.net><mailto:calum.polwart at nhs.net<mailto:calum.polwart at nhs.net>>> wrote:

********************************************************************************************************************

This message may contain confidential information. If you are not the intended recipient please inform the
sender that you have received the message in error before deleting it.
Please do not disclose, copy or distribute information in this e-mail or take any action in reliance on its contents:
to do so is strictly prohibited and may be unlawful.

Thank you for your co-operation.

NHSmail is the secure email and directory service available for all NHS staff in England and Scotland
NHSmail is approved for exchanging patient data and other sensitive information with NHSmail and GSi recipients
NHSmail provides an email address for your career in the NHS and can be accessed anywhere

********************************************************************************************************************

        [[alternative HTML version deleted]]

______________________________________________
R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.


********************************************************************************************************************

This message may contain confidential information. If you are not the intended recipient please inform the
sender that you have received the message in error before deleting it.
Please do not disclose, copy or distribute information in this e-mail or take any action in reliance on its contents:
to do so is strictly prohibited and may be unlawful.

Thank you for your co-operation.

NHSmail is the secure email and directory service available for all NHS staff in England and Scotland
NHSmail is approved for exchanging patient data and other sensitive information with NHSmail and GSi recipients
NHSmail provides an email address for your career in the NHS and can be accessed anywhere

********************************************************************************************************************

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Mon Dec 28 13:00:17 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Mon, 28 Dec 2015 12:00:17 +0000
Subject: [R] rugarch package: VaR exceedances plot
Message-ID: <248E6FA047A8C746BA491485764190F53715F6CF@ESESSMB207.ericsson.se>

My suggestion is to inspect the VaRplot source code and, also with the help of debug() if necessary,
you may verify how ylim results with your data.

> VaRplot
function (alpha, actual, VaR, title = paste("Daily Returns and Value-at-Risk \nExceedances\n",
    "(alpha=", alpha, ")", sep = ""), ylab = "Daily Log Returns",
    xlab = "Time")
{
    period = diff(index(actual))
    if (attr(period, "units") == "mins") {
        A = as.numeric(actual)
        V = as.numeric(VaR)
        ep <- axTicksByTime(actual)
        plot(A, type = "n", main = title, ylab = ylab, xlab = xlab,
            ylim = c(min(A, V), max(A, V)), ann = FALSE, xaxt = "n",
            cex.main = 0.8, cex.lab = 0.9, cex.axis = 0.8)
    ...
    }
    else {
        plot(index(actual), as.numeric(actual), type = "n", main = title,
            ylab = ylab, xlab = xlab, ylim = c(min(actual, VaR),
                max(actual, VaR)), ann = FALSE, cex.main = 0.8,
            cex.lab = 0.9, cex.axis = 0.8)
    ....
}
    return(invisible())
}
<environment: namespace:rugarch>

File: rugarch-plots.R

Good luck,

--
GG

	[[alternative HTML version deleted]]


From JSorkin at grecc.umaryland.edu  Mon Dec 28 14:03:22 2015
From: JSorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 28 Dec 2015 08:03:22 -0500
Subject: [R] by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
In-Reply-To: <CAF8bMcZq-Ay=ZAbQyj=8aS8o8D+gFYoawP9=xgWoZ7x15-sfXA@mail.gmail.com>
References: <56805E60020000CB00144C3A@smtp.medicine.umaryland.edu>
	<CAF8bMcZq-Ay=ZAbQyj=8aS8o8D+gFYoawP9=xgWoZ7x15-sfXA@mail.gmail.com>
Message-ID: <5680ECCA020000CB00144CCC@smtp.medicine.umaryland.edu>

Thank you,
John



John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 
>>> William Dunlap <wdunlap at tibco.com> 12/28/15 12:55 AM >>>

by(dataFrame, groupId, FUN) applies FUN a bunch of data.frames (row subsetsof the dataFrame input).  mean() returns NA for data.frames.  You could use
FUN=colMeans if you wanted column means or FUN=function(x)mean(colMeans(x))
or FUN=function(x)mean(unlist(x)) if you wanted some version of a grand mean
over all the columns.

If you want column means, you may find aggregate() more suited to the job, as it
applies FUN to each column in each row subset of the data and returns a data.frame
instead of a list of outputs of FUN.
  > aggregate(mtcars[,3:5], mtcars[,2,drop=FALSE], mean)
    cyl     disp        hp     drat
  1   4 105.1364  82.63636 4.070909
  2   6 183.3143 122.28571 3.585714
  3   8 353.1000 209.21429 3.229286







Bill Dunlap
TIBCO Software
wdunlap tibco.com



On Sun, Dec 27, 2015 at 6:55 PM, John Sorkin <jsorkin at grecc.umaryland.edu> wrote:

When I run by, I get an error message and no results. Any help in understanding what is wrong would be appreciated.

Error message:
Warning messages:
1: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA
2: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA


Results:
Arm: MUFA
[1] NA
-----------------------------------------------------------------------------------------------------------------------
Arm: PUFA
[1] NA

Code:
by(hold,Arm,mean,na.rm=TRUE)

I don't understand why I am getting the error message, and why I am not getting any results. I don't believe my data are non-numeric.

BY str works fine and confirms that the data are numeric
> by(hold,Arm,str)
'data.frame':   23 obs. of  3 variables:
 $ Wtscr: num  97.2 103.9 58.2 130.9 135 ...
 $ Wt0  : num  96.2 106.1 56.7 127.4 133.1 ...
 $ Wt6  : num  93.8 101.7 55.5 127.6 130.9 ...
'data.frame':   16 obs. of  3 variables:
 $ Wtscr: num  120.2 104.6 100.1 74.8 112.6 ...
 $ Wt0  : num  117.2 105.3 99.5 75.7 110.7 ...
 $ Wt6  : num  114.6 104.8 84.5 77.7 107.4 ...
 Here is a listing of my data:
> hold
   Wtscr   Wt0    Wt6
1  120.2 117.2 114.60
2  104.6 105.3 104.80
3   97.2  96.2  93.80
4  103.9 106.1 101.70
5   58.2  56.7  55.50
6  130.9 127.4 127.60
7  135.0 133.1 130.90
8  100.1  99.5  84.50
9  130.3 115.3 115.80
10 150.5 148.7 133.40
11  74.8  75.7  77.70
12 112.6 110.7 107.40
13  90.0  91.0  83.40
14 139.1 138.5 126.70
15  99.1  96.4  95.70
16 108.3 107.5 109.30
17  75.1  72.9  72.20
18  97.5 102.1  98.50
19 202.2  90.1  90.60
20  91.7  89.4  93.40
21 102.1 102.2 100.80
22 116.9 118.9 118.00
23  94.6  95.3  90.30
24 122.2 117.0 117.00
25 105.6 103.3 103.60
26  96.9  96.8  98.80
27 102.9 100.3  89.00
28 115.8 118.5 117.30
29  95.7  96.2  95.40
30  88.2  86.9  88.30
31 108.7 108.8 108.80
32  89.2  88.6  81.20
33  86.8  86.5  82.70
34 135.5 130.1 125.40
35 112.5 113.9 111.45
36 111.0 105.3 109.50
37 103.4 100.5  95.50
38 117.6 117.4 101.40
39 116.7 118.5 101.80

The INDEX is clearly a factor:
> Arm
 [1] PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA PUFA PUFA PUFA MUFA MUFA MUFA MUFA MUFA PUFA MUFA MUFA PUFA MUFA PUFA MUFA PUFA
[32] MUFA MUFA MUFA MUFA MUFA PUFA PUFA PUFA
Levels: MUFA PUFA

The data and the index have the same length:
> cbind(hold,Arm)
   Wtscr   Wt0    Wt6  Arm
1  120.2 117.2 114.60 PUFA
2  104.6 105.3 104.80 PUFA
3   97.2  96.2  93.80 MUFA
4  103.9 106.1 101.70 MUFA
5   58.2  56.7  55.50 MUFA
6  130.9 127.4 127.60 MUFA
7  135.0 133.1 130.90 MUFA
8  100.1  99.5  84.50 PUFA
9  130.3 115.3 115.80 MUFA
10 150.5 148.7 133.40 MUFA
11  74.8  75.7  77.70 PUFA
12 112.6 110.7 107.40 PUFA
13  90.0  91.0  83.40 PUFA
14 139.1 138.5 126.70 MUFA
15  99.1  96.4  95.70 MUFA
16 108.3 107.5 109.30 PUFA
17  75.1  72.9  72.20 PUFA
18  97.5 102.1  98.50 PUFA
19 202.2  90.1  90.60 MUFA
20  91.7  89.4  93.40 MUFA
21 102.1 102.2 100.80 MUFA
22 116.9 118.9 118.00 MUFA
23  94.6  95.3  90.30 MUFA
24 122.2 117.0 117.00 PUFA
25 105.6 103.3 103.60 MUFA
26  96.9  96.8  98.80 MUFA
27 102.9 100.3  89.00 PUFA
28 115.8 118.5 117.30 MUFA
29  95.7  96.2  95.40 PUFA
30  88.2  86.9  88.30 MUFA
31 108.7 108.8 108.80 PUFA
32  89.2  88.6  81.20 MUFA
33  86.8  86.5  82.70 MUFA
34 135.5 130.1 125.40 MUFA
35 112.5 113.9 111.45 MUFA
36 111.0 105.3 109.50 MUFA
37 103.4 100.5  95.50 PUFA
38 117.6 117.4 101.40 PUFA
39 116.7 118.5 101.80 PUFA

But the by function does not work!
> by(hold,Arm,mean,na.rm=TRUE)
Arm: MUFA
[1] NA
-----------------------------------------------------------------------------------------------------------------------
Arm: PUFA
[1] NA
Warning messages:
1: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA
2: In mean.default(data[x, , drop = FALSE], ...) :
  argument is not numeric or logical: returning NA


Perhaps this is a hint, print does not give two separate group:
> by(hold,Arm,print)
   Wtscr   Wt0    Wt6
3   97.2  96.2  93.80
4  103.9 106.1 101.70
5   58.2  56.7  55.50
6  130.9 127.4 127.60
7  135.0 133.1 130.90
9  130.3 115.3 115.80
10 150.5 148.7 133.40
14 139.1 138.5 126.70
15  99.1  96.4  95.70
19 202.2  90.1  90.60
20  91.7  89.4  93.40
21 102.1 102.2 100.80
22 116.9 118.9 118.00
23  94.6  95.3  90.30
25 105.6 103.3 103.60
26  96.9  96.8  98.80
28 115.8 118.5 117.30
30  88.2  86.9  88.30
32  89.2  88.6  81.20
33  86.8  86.5  82.70
34 135.5 130.1 125.40
35 112.5 113.9 111.45
36 111.0 105.3 109.50
   Wtscr   Wt0   Wt6
1  120.2 117.2 114.6
2  104.6 105.3 104.8
8  100.1  99.5  84.5
11  74.8  75.7  77.7
12 112.6 110.7 107.4
13  90.0  91.0  83.4
16 108.3 107.5 109.3
17  75.1  72.9  72.2
18  97.5 102.1  98.5
24 122.2 117.0 117.0
27 102.9 100.3  89.0
29  95.7  96.2  95.4
31 108.7 108.8 108.8
37 103.4 100.5  95.5
38 117.6 117.4 101.4
39 116.7 118.5 101.8
Arm: MUFA
   Wtscr   Wt0    Wt6
3   97.2  96.2  93.80
4  103.9 106.1 101.70
5   58.2  56.7  55.50
6  130.9 127.4 127.60
7  135.0 133.1 130.90
9  130.3 115.3 115.80
10 150.5 148.7 133.40
14 139.1 138.5 126.70
15  99.1  96.4  95.70
19 202.2  90.1  90.60
20  91.7  89.4  93.40
21 102.1 102.2 100.80
22 116.9 118.9 118.00
23  94.6  95.3  90.30
25 105.6 103.3 103.60
26  96.9  96.8  98.80
28 115.8 118.5 117.30
30  88.2  86.9  88.30
32  89.2  88.6  81.20
33  86.8  86.5  82.70
34 135.5 130.1 125.40
35 112.5 113.9 111.45
36 111.0 105.3 109.50
-----------------------------------------------------------------------------------------------------------------------
Arm: PUFA
   Wtscr   Wt0   Wt6
1  120.2 117.2 114.6
2  104.6 105.3 104.8
8  100.1  99.5  84.5
11  74.8  75.7  77.7
12 112.6 110.7 107.4
13  90.0  91.0  83.4
16 108.3 107.5 109.3
17  75.1  72.9  72.2
18  97.5 102.1  98.5
24 122.2 117.0 117.0
27 102.9 100.3  89.0
29  95.7  96.2  95.4
31 108.7 108.8 108.8
37 103.4 100.5  95.5
38 117.6 117.4 101.4
39 116.7 118.5 101.8

But summary works as expected, giving two groups of results!
> by(hold,Arm,summary)
Arm: MUFA
     Wtscr             Wt0             Wt6
 Min.   : 58.20   Min.   : 56.7   Min.   : 55.5
 1st Qu.: 95.75   1st Qu.: 92.7   1st Qu.: 92.0
 Median :105.60   Median :103.3   Median :101.7
 Mean   :112.75   Mean   :106.3   Mean   :104.0
 3rd Qu.:130.60   3rd Qu.:118.7   3rd Qu.:117.7
 Max.   :202.20   Max.   :148.7   Max.   :133.4
-----------------------------------------------------------------------------------------------------------------------
Arm: PUFA
     Wtscr             Wt0              Wt6
 Min.   : 74.80   Min.   : 72.90   Min.   : 72.20
 1st Qu.: 97.05   1st Qu.: 98.67   1st Qu.: 87.88
 Median :104.00   Median :103.70   Median : 99.95
 Mean   :103.15   Mean   :102.54   Mean   : 97.58
 3rd Qu.:113.62   3rd Qu.:112.28   3rd Qu.:107.75
 Max.   :122.20   Max.   :118.50   Max.   :117.00

BY also shows that there are no NAs in the data, and the BY works properly.
> by(hold,Arm,is.na)
Arm: MUFA
   Wtscr

John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing)

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message.
______________________________________________
R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
and provide commented, minimal, self-contained, reproducible code.
Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From veniarakelian at yahoo.com  Mon Dec 28 08:49:42 2015
From: veniarakelian at yahoo.com (veni arakelian)
Date: Mon, 28 Dec 2015 07:49:42 +0000 (UTC)
Subject: [R] Multidimensional array
In-Reply-To: <1543321241.3828013.1451208312024.JavaMail.yahoo@mail.yahoo.com>
References: <1543321241.3828013.1451208312024.JavaMail.yahoo.ref@mail.yahoo.com>
	<1543321241.3828013.1451208312024.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <991054079.4186506.1451288982722.JavaMail.yahoo@mail.yahoo.com>



Hi All.

I'm very new in R. I'd like to import data of the form MxMxT, i.e., MxM array in T distinct points.
Any ideas how to do that?
Regards,
Veni?

  
	[[alternative HTML version deleted]]


From shivi.bhatia at safexpress.com  Mon Dec 28 10:12:03 2015
From: shivi.bhatia at safexpress.com (SHIVI BHATIA)
Date: Mon, 28 Dec 2015 14:42:03 +0530
Subject: [R] Incorrect Summarization after As.numeric
Message-ID: <000001d1414f$c350cf90$49f26eb0$@safexpress.com>

Dear Team,

 

I am working on aggregate function in R & have used the below code:

 

aggregate(a1$Final,list(Year=a1$Year),sum)

 

initially a1$Final was factor hence I used a1$Final= as.numeric(a1$Final) to
convert this into numeric. After conversion it shows the output but the
summarization from the aggregate is incorrect. This I have reconciled with
the excel pivot. 

 

Could you please suggest where could be the issue?

 

Thanks, Shivi

Mb: 9891002021

 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151228/41f24cf2/attachment.pl>

From stefano.sofia at regione.marche.it  Mon Dec 28 17:11:57 2015
From: stefano.sofia at regione.marche.it (Stefano Sofia)
Date: Mon, 28 Dec 2015 16:11:57 +0000
Subject: [R] matrix which results singular but at the same time positive
 definite
In-Reply-To: <567023A9.7040306@gmail.com>
References: <mailman.5.1449831602.13581.r-help@r-project.org>,
	<567023A9.7040306@gmail.com>
Message-ID: <8B435C9568170B469AE31E8891E8CC4F3DB73A1E@ESINO.regionemarche.intra>

Dear Dr.Gilbert,

it took me a bit of time to understand your thoughtful comment.
You are right on everything. I was not able to see it, and likely I still have something to understand better some consequences on what I am trying to do.

Thank you
Stefano

________________________________________
Da: Paul Gilbert [pgilbert902 at gmail.com]
Inviato: marted? 15 dicembre 2015 15.28
A: Stefano Sofia
Cc: r-help at r-project.org; Fox, John; peter dalgaard
Oggetto: Re: [R] matrix which results singular but at the same time positive definite

Stefano

I think in other response to in this thread you got the answer to the
question you asked, but you may be asking the wrong question. I'm not
familiar with the specific papers you mention and you have not provided
enough detail about what you are doing, so I am guessing a bit. The term
"dynamic linear model" can refer to both linear ARMA/ARIMA models and to
linear state-space models, however some authors use it to refer
exclusively to state-space models and from your phrasing I am guessing
you are doing that. There would be many state-space models equivalent to
a given ARMA/ARIMA model, but without specifying structural aspects of
the system you will likely be using one of the innovations form
state-space models that are equivalent. In an innovations form
state-space model the state is defined as an expectation. From a
practical point of view, this is one of the most important differences
between an innovation form and a non-innovations form state-space model.
Since the expectation is known exactly, the state-tracking error is
zero. That means the covariance matrix from the filter or smoother
should be a zero matrix, which you should not be trying to invert. In a
non-innovations form the state has a physical interpretation rather than
being an expectation, so the state tracking error should not be
degenerate in that case.

I mention all this because your covariance matrix looks very close to zero.

Paul Gilbert

On 12/11/2015 06:00 AM, r-help-request at r-project.org wrote:
> Dear John, thank you for your considerations. This matrix (which is a
> variance matrix) is part of an algorithm for forward-filtering and
> backward-sampling of Dynamic Linear Models (West and Harrison, 1997),
> applied to DLM representation of ARIMA processes (Petris, Petrone,
> Campagnoli).  It is therefore very difficult to explain why this
> variance matrix becomes so ill conditioned. This already happens at
> the first iteration of the algorithm. I will try to work on initial
> conditions and some fixed parameters.
>
> Thank you again Stefano
>

________________________________

AVVISO IMPORTANTE: Questo messaggio di posta elettronica pu? contenere informazioni confidenziali, pertanto ? destinato solo a persone autorizzate alla ricezione. I messaggi di posta elettronica per i client di Regione Marche possono contenere informazioni confidenziali e con privilegi legali. Se non si ? il destinatario specificato, non leggere, copiare, inoltrare o archiviare questo messaggio. Se si ? ricevuto questo messaggio per errore, inoltrarlo al mittente ed eliminarlo completamente dal sistema del proprio computer. Ai sensi dell?art. 6 della DGR n. 1394/2008 si segnala che, in caso di necessit? ed urgenza, la risposta al presente messaggio di posta elettronica pu? essere visionata da persone estranee al destinatario.
IMPORTANT NOTICE: This e-mail message is intended to be received only by persons entitled to receive the confidential information it may contain. E-mail messages to clients of Regione Marche may contain information that is confidential and legally privileged. Please do not read, copy, forward, or store this message unless you are an intended recipient of it. If you have received this message in error, please forward it to the sender and delete it completely from your computer system.

From istazahn at gmail.com  Mon Dec 28 17:13:27 2015
From: istazahn at gmail.com (Ista Zahn)
Date: Mon, 28 Dec 2015 11:13:27 -0500
Subject: [R] Incorrect Summarization after As.numeric
In-Reply-To: <000001d1414f$c350cf90$49f26eb0$@safexpress.com>
References: <000001d1414f$c350cf90$49f26eb0$@safexpress.com>
Message-ID: <CA+vqiLFwH6G9jdL=JOqXV8bejdj4QfV1tEkmVPPQbwzFFjRTMg@mail.gmail.com>

FAQ 7.10 perhaps.
On Dec 28, 2015 10:51 AM, "SHIVI BHATIA" <shivi.bhatia at safexpress.com>
wrote:

> Dear Team,
>
>
>
> I am working on aggregate function in R & have used the below code:
>
>
>
> aggregate(a1$Final,list(Year=a1$Year),sum)
>
>
>
> initially a1$Final was factor hence I used a1$Final= as.numeric(a1$Final)
> to
> convert this into numeric. After conversion it shows the output but the
> summarization from the aggregate is incorrect. This I have reconciled with
> the excel pivot.
>
>
>
> Could you please suggest where could be the issue?
>
>
>
> Thanks, Shivi
>
> Mb: 9891002021
>
>
>
>
> This e-mail is confidential. It may also be legally privileged. If you are
> not the addressee you may not copy, forward, disclose or use any part of
> it. If you have received this message in error, please delete it and all
> copies from your system and notify the sender immediately by return e-mail.
> Internet communications cannot be guaranteed to be timely, secure, error or
> virus-free. The sender does not accept liability for any errors or
> omissions.
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From cadeb at usgs.gov  Mon Dec 28 17:30:56 2015
From: cadeb at usgs.gov (Cade, Brian)
Date: Mon, 28 Dec 2015 09:30:56 -0700
Subject: [R] Right censored data,
	abundant in zeros for regression analysis.
In-Reply-To: <E1701231840BD64AB541311E63C1C7658A46DA83@ISS-MBX01.tawe.swan.ac.uk>
References: <E1701231840BD64AB541311E63C1C7658A46DA83@ISS-MBX01.tawe.swan.ac.uk>
Message-ID: <CAM5M9BT25qbLv9_0uzqXVOg6XDD4vgD_gHiCELjMYP50rObAOA@mail.gmail.com>

Tom:  One possibility might be to use the censored quantile regression
implementation (crq) in the quantreg package (accommodates left or right
censoring) across a range of quantiles (e.g., 0.05 to 0.95) but where
interest is likely to be focused on estimates for quantiles greater than
the quantiles associated with the mass of zeros.

Brian

Brian S. Cade, PhD

U. S. Geological Survey
Fort Collins Science Center
2150 Centre Ave., Bldg. C
Fort Collins, CO  80526-8818

email:  cadeb at usgs.gov <brian_cade at usgs.gov>
tel:  970 226-9326


On Thu, Dec 24, 2015 at 6:41 AM, REES T. (706713) <
t.rees.706713 at swansea.ac.uk> wrote:

> Hi there,
>
> Firstly forgive me if this seem obvious, if there is existing literature
> on this i can't find it.
>
> I am looking at conditioning to stimuli and there in the time taken to
> perform a certain task.
>
> The IV for this data is Conditioning periods ranging from 1-34 periods and
> the DV is the time taken for the behavioral response to occur 0-300s.
> I am aware that this could simply be looked at through a simple linear
> regression, however due to the nature of conditioning there is an abundance
> of zeros in the data.
> On top of this the response time data is right censored (i believe), in
> that they were given a five minute period to respond after this five minute
> period (300 seconds) the conditioning period was terminated, so no more
> data was recorded.
>
> Attached is the data (in .csv format) for time spent out, 0 indicated no
> time out and 300 indicated all time out during the 5 minutes.
>
> I have considered looking at zero-inflated censored regressions and others
> similar analysis but I cannot find an analysis that suits the data I have
> and actually works.
> So what is the best analysis method to deal with this data?
> Admittedly i could be completely missing the target, if that's the case
> please feel free to say so. Any help with the route that I should go down
> here would be much appreciated, even if it is blindingly obvious.
>
> Sincerely
>
> Tom Rees
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jrkrideau at inbox.com  Mon Dec 28 19:32:46 2015
From: jrkrideau at inbox.com (John Kane)
Date: Mon, 28 Dec 2015 10:32:46 -0800
Subject: [R] Incorrect Summarization after As.numeric
In-Reply-To: <000001d1414f$c350cf90$49f26eb0$@safexpress.com>
Message-ID: <F723A27288F.000004A3jrkrideau@inbox.com>

It would likely help to have some sample data. Please have a look at
http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html

John Kane
Kingston ON Canada


> -----Original Message-----
> From: shivi.bhatia at safexpress.com
> Sent: Mon, 28 Dec 2015 14:42:03 +0530
> To: r-help at r-project.org
> Subject: [R] Incorrect Summarization after As.numeric
> 
> Dear Team,
> 
> 
> 
> I am working on aggregate function in R & have used the below code:
> 
> 
> 
> aggregate(a1$Final,list(Year=a1$Year),sum)
> 
> 
> 
> initially a1$Final was factor hence I used a1$Final= as.numeric(a1$Final)
> to
> convert this into numeric. After conversion it shows the output but the
> summarization from the aggregate is incorrect. This I have reconciled
> with
> the excel pivot.
> 
> 
> 
> Could you please suggest where could be the issue?
> 
> 
> 
> Thanks, Shivi
> 
> Mb: 9891002021
> 
> 
> 
> This e-mail is confidential. It may also be legally privileged. If you
> are not the addressee you may not copy, forward, disclose or use any part
> of it. If you have received this message in error, please delete it and
> all copies from your system and notify the sender immediately by return
> e-mail. Internet communications cannot be guaranteed to be timely,
> secure, error or virus-free. The sender does not accept liability for any
> errors or omissions.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

____________________________________________________________
FREE 3D EARTH SCREENSAVER - Watch the Earth right on your desktop!


From sarah.goslee at gmail.com  Mon Dec 28 19:41:37 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Mon, 28 Dec 2015 13:41:37 -0500
Subject: [R] Incorrect Summarization after As.numeric
In-Reply-To: <F723A27288F.000004A3jrkrideau@inbox.com>
References: <000001d1414f$c350cf90$49f26eb0$@safexpress.com>
	<F723A27288F.000004A3jrkrideau@inbox.com>
Message-ID: <CAM_vju=3UDpt22ThJPKuKSK=couZWB-tV9M1g1LzW6JX7qwkFg@mail.gmail.com>

You probably want

as.numeric(as.character(myfactor))

since the underlying numeric representation of a factor that's
obtained with as.numeric() does not match what you see if your factor
"appears" numeric.

> myfactor <- as.factor(c("5", "6", "43", "22", "5", "22", "22"))
> myfactor
[1] 5  6  43 22 5  22 22
Levels: 22 43 5 6
> as.numeric(myfactor)
[1] 3 4 2 1 3 1 1
> as.numeric(as.character(myfactor))
[1]  5  6 43 22  5 22 22

One of the examples for ?as.numeric demonstrates this, but it's not
explicitly explained there.

Sarah


On Mon, Dec 28, 2015 at 1:32 PM, John Kane <jrkrideau at inbox.com> wrote:
> It would likely help to have some sample data. Please have a look at
> http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example and/or http://adv-r.had.co.nz/Reproducibility.html
>
> John Kane
> Kingston ON Canada
>
>
>> -----Original Message-----
>> From: shivi.bhatia at safexpress.com
>> Sent: Mon, 28 Dec 2015 14:42:03 +0530
>> To: r-help at r-project.org
>> Subject: [R] Incorrect Summarization after As.numeric
>>
>> Dear Team,
>>
>>
>>
>> I am working on aggregate function in R & have used the below code:
>>
>>
>>
>> aggregate(a1$Final,list(Year=a1$Year),sum)
>>
>>
>>
>> initially a1$Final was factor hence I used a1$Final= as.numeric(a1$Final)
>> to
>> convert this into numeric. After conversion it shows the output but the
>> summarization from the aggregate is incorrect. This I have reconciled
>> with
>> the excel pivot.
>>
>>
>>
>> Could you please suggest where could be the issue?
>>
>>
>>
>> Thanks, Shivi
>>
>> Mb: 9891002021
>>
>>


From dwinsemius at comcast.net  Mon Dec 28 20:24:51 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 28 Dec 2015 11:24:51 -0800
Subject: [R] Multidimensional array
In-Reply-To: <991054079.4186506.1451288982722.JavaMail.yahoo@mail.yahoo.com>
References: <1543321241.3828013.1451208312024.JavaMail.yahoo.ref@mail.yahoo.com>
	<1543321241.3828013.1451208312024.JavaMail.yahoo@mail.yahoo.com>
	<991054079.4186506.1451288982722.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <F0D1C062-2049-4F48-8CA9-E7EFB147FBC8@comcast.net>


> On Dec 27, 2015, at 11:49 PM, veni arakelian via R-help <r-help at r-project.org> wrote:
> 
> 
> 
> Hi All.
> 
> I'm very new in R. I'd like to import data of the form MxMxT, i.e., MxM array in T distinct points.
> Any ideas how to do that?

It will depend on the form in which this data is represented as a file object. Can you show the appearance of the first two or three MxM items as they would be viewed in a text editor. And PLEASE post in plain text.


> Regards,
> Veni 
> 
> 
> 	[[alternative HTML version deleted]]

Postin in HTML is generally a sign that you have not read the posting-guide page.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From jdnewmil at dcn.davis.ca.us  Mon Dec 28 20:54:35 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 28 Dec 2015 11:54:35 -0800
Subject: [R] Multidimensional array
In-Reply-To: <991054079.4186506.1451288982722.JavaMail.yahoo@mail.yahoo.com>
References: <1543321241.3828013.1451208312024.JavaMail.yahoo.ref@mail.yahoo.com>
	<1543321241.3828013.1451208312024.JavaMail.yahoo@mail.yahoo.com>
	<991054079.4186506.1451288982722.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <491F591F-FDEB-4FCF-81E3-A4475E861DED@dcn.davis.ca.us>

Welcome to r-help. There are multiple ways to import various kinds of data, but they tend to be organized around how the data are represented in disk files, but you have given no hints about what you have on disk to work with. Also, such data are sometimes represented in memory in so-called "sparse" or "data frame" format, depending on the types of calculations you plan to perform on that data. 

The best way to clarify how much you need to know is to provide a reproducible example that illustrates how much you already know how to do, with sample data representing both your input and your desired output. You should read about what makes an example clear by reading e.g. [1].

You should also read the Posting Guide mentioned at the bottom of every message on this list.  One bit of advice mentioned there that is crucial is sending questions in plain text rather than HTML (as you did this time), since the mailing list usually tries to remove the formatting which tends to damage R code. 

[1] http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
-- 
Sent from my phone. Please excuse my brevity.

On December 27, 2015 11:49:42 PM PST, veni arakelian via R-help <r-help at r-project.org> wrote:
>
>
>Hi All.
>
>I'm very new in R. I'd like to import data of the form MxMxT, i.e., MxM
>array in T distinct points.
>Any ideas how to do that?
>Regards,
>Veni?
>
>  
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From jerrelharlanmast at gmail.com  Mon Dec 28 19:01:02 2015
From: jerrelharlanmast at gmail.com (Claire's Daddy)
Date: Mon, 28 Dec 2015 13:01:02 -0500
Subject: [R] randomForestSRC VIMP interpretation
Message-ID: <CAEgan+8tsNcVx4QGpcM_piXeh+Ubr3QqrCPSGtetwiYVU8QguQ@mail.gmail.com>

Hi,

For a binary classification model, why variable importance produces
separate importance for each outcome? What does column N and R stand for? I
thought only the first column 'all' is needed. (i.e. randomly permute
variable x, calculated change in out-of-bag prediction error.

Code
*model <- rfsrc(status ~ ., data = breast)*
*x <- predict(model)*
*x$importance *

produces the following

*                              all             N             R
mean_radius          2.880195e-03  2.175676e-03 -2.543478e-03
mean_texture         1.129752e-03  7.837838e-04 -8.043478e-04
mean_perimeter       1.976838e-03  1.466216e-03 -1.652174e-03
mean_area            3.143459e-03  2.054054e-03 -1.782609e-03*

......



Thanks a lot!


Turing

	[[alternative HTML version deleted]]


From mesudebayrakci at gmail.com  Mon Dec 28 20:40:23 2015
From: mesudebayrakci at gmail.com (mesude bayrakci)
Date: Mon, 28 Dec 2015 14:40:23 -0500
Subject: [R] Mixed Beta Disrubutions
Message-ID: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>

Hello,

I have data; one column and 310 rows. When I plot the histogram, it has two
peaks; please see the attachment. I would like to find appropriate
distribution that fits the data. I tried to mixtools in R, but it did not
fit well.

I want to mix two beta distribution. I found that there is betareg package
in R but the shape1,shape2 were known or there were two different data in
the all examples.

I do not know where to start. How can I use betamix in R to fit the data?
Any hint?

I really appreciate.


Thank you
-------------- next part --------------
A non-text attachment was scrubbed...
Name: histogram.png
Type: image/png
Size: 16608 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151228/be516049/attachment.png>

From jsorkin at grecc.umaryland.edu  Mon Dec 28 22:54:57 2015
From: jsorkin at grecc.umaryland.edu (John Sorkin)
Date: Mon, 28 Dec 2015 16:54:57 -0500
Subject: [R] error using by(goop[, c("a", "b", "c")], goop[, "Arm"],
 mySD) was Re:  by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
Message-ID: <56816961020000CB00144DC1@smtp.medicine.umaryland.edu>


I am  trying to use the by function to get the SD of each column of a data frame, stratified by ARM. Using a suggestion provided by both William Dunlap and Rolf Turner, I have written the code below which fails with the error:
Error in match.fun(FUN) :   'sqrt(var(x))' is not a function, character or symbol

 
code:
mySD <- function(x) {sapply(x,sqrt(var(x)))}
class(mySD)
by(goop[,c("a","b","c")],goop[,"Arm"],mySD)
 
Below I include code which will create goop (thank you Bill for suggesting I use dput), which will run will recreate mmy error. Any thoughts on why the by function fails and how to fix it would be appreciated.
 
Thank you,
John
 
goop <- structure(c(180L, 153L, 149L, 218L, 217L, 147L, 141L, 141L, 174L, 
                    162L, 163L, 206L, 168L, 227L, 142L, 179L, 270L, 206L, 243L, 134L, 
                    131L, 222L, 142L, 119L, 149L, 190L, 196L, 145L, 185L, 235L, 165L, 
                    246L, 152L, 186L, 222L, 248L, 125L, 149L, 233L, 145L, 154L, 231L, 
                    146L, 161L, 150L, 133L, 115L, 145L, 180L, 202L, 158L, 210L, 153L, 
                    136L, 265L, 255L, 199L, 131L, 110L, 169L, 147L, 133L, 108L, 203L, 
                    185L, 153L, 154L, 207L, 167L, 233L, 135L, 127L, 197L, 245L, 124L, 
                    125L, 146L, 139L, 158L, 211L, 183L, 169L, 137L, 114L, 176L, 143L, 
                    156L, 209L, 163L, 194L, 151L, 238L, 253L, 293L, 208L, 147L, 186L, 
                    179L, 150L, 203L, 112L, 185L, 156L, 127L, 132L, 243L, 177L, 242L, 
                    144L, 140L, 194L, 249L, 109L, 91L, 10163L, 10987L, 19882L, 19899L, 
                    20104L, 20105L, 20167L, 20318L, 20338L, 20392L, 20453L, 20454L, 
                    20467L, 20500L, 20682L, 20687L, 20692L, 20744L, 20750L, 20751L, 
                    20806L, 20848L, 20856L, 20959L, 21023L, 21048L, 21114L, 21134L, 
                    21135L, 21320L, 21329L, 21349L, 21353L, 21521L, 21663L, 21784L, 
                    22031L, 22081L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 2L, 
                    2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 
                    2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L), .Dim = c(38L, 5L), .Dimnames = list(
                      NULL, c("a", "b", "c", "ID", "Arm")), na.action = structure(37, class = "omit"))
goop
mySD <- function(x) {sapply(x,sqrt(var(x)))}
class(mySD)
by(goop[,c("a","b","c")],goop[,"Arm"],mySD)

 
 
 


 
John David Sorkin M.D., Ph.D.
Professor of Medicine
Chief, Biostatistics and Informatics
University of Maryland School of Medicine Division of Gerontology and Geriatric Medicine
Baltimore VA Medical Center
10 North Greene Street
GRECC (BT/18/GR)
Baltimore, MD 21201-1524
(Phone) 410-605-7119
(Fax) 410-605-7913 (Please call phone number above prior to faxing) 

Confidentiality Statement:
This email message, including any attachments, is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized use, disclosure or distribution is prohibited. If you are not the intended recipient, please contact the sender by reply email and destroy all copies of the original message. 

From ruipbarradas at sapo.pt  Mon Dec 28 23:10:45 2015
From: ruipbarradas at sapo.pt (ruipbarradas at sapo.pt)
Date: Mon, 28 Dec 2015 22:10:45 +0000
Subject: [R] error using by(goop[, c("a", "b", "c")], goop[, "Arm"],
 mySD) was Re: by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
In-Reply-To: <56816961020000CB00144DC1@smtp.medicine.umaryland.edu>
Message-ID: <20151228221045.Horde.ILZ34zQoN_ymMy-eE6y8B9P@mail.sapo.pt>

Hello,

Try the following.

mySD <- function(x) {sapply(x, function(y) sqrt(var(y)))}

Hope this helps,

Rui Barradas
?

Citando John Sorkin <jsorkin at grecc.umaryland.edu>:

> I am? trying to use the by function to get the SD of each column of  
> a data frame, stratified by ARM. Using a suggestion provided by both  
> William Dunlap and Rolf Turner, I have written the code below which  
> fails with the error:
> Error in match.fun(FUN) :? ?'sqrt(var(x))' is not a function,  
> character or symbol
>
> code:
> mySD <- function(x) {sapply(x,sqrt(var(x)))}
> class(mySD)
> by(goop[,c("a","b","c")],goop[,"Arm"],mySD)
>
> Below I include code which will create goop (thank you Bill for  
> suggesting I use dput), which will run will recreate mmy error. Any  
> thoughts on why the by function fails and how to fix it would be  
> appreciated.
>
> Thank you,
> John
>
> goop <- structure(c(180L, 153L, 149L, 218L, 217L, 147L, 141L, 141L, 174L,
> ? ? ? ? ? ? ? ? ? ?162L, 163L, 206L, 168L, 227L, 142L, 179L, 270L,  
> 206L, 243L, 134L,
> ? ? ? ? ? ? ? ? ? ?131L, 222L, 142L, 119L, 149L, 190L, 196L, 145L,  
> 185L, 235L, 165L,
> ? ? ? ? ? ? ? ? ? ?246L, 152L, 186L, 222L, 248L, 125L, 149L, 233L,  
> 145L, 154L, 231L,
> ? ? ? ? ? ? ? ? ? ?146L, 161L, 150L, 133L, 115L, 145L, 180L, 202L,  
> 158L, 210L, 153L,
> ? ? ? ? ? ? ? ? ? ?136L, 265L, 255L, 199L, 131L, 110L, 169L, 147L,  
> 133L, 108L, 203L,
> ? ? ? ? ? ? ? ? ? ?185L, 153L, 154L, 207L, 167L, 233L, 135L, 127L,  
> 197L, 245L, 124L,
> ? ? ? ? ? ? ? ? ? ?125L, 146L, 139L, 158L, 211L, 183L, 169L, 137L,  
> 114L, 176L, 143L,
> ? ? ? ? ? ? ? ? ? ?156L, 209L, 163L, 194L, 151L, 238L, 253L, 293L,  
> 208L, 147L, 186L,
> ? ? ? ? ? ? ? ? ? ?179L, 150L, 203L, 112L, 185L, 156L, 127L, 132L,  
> 243L, 177L, 242L,
> ? ? ? ? ? ? ? ? ? ?144L, 140L, 194L, 249L, 109L, 91L, 10163L,  
> 10987L, 19882L, 19899L,
> ? ? ? ? ? ? ? ? ? ?20104L, 20105L, 20167L, 20318L, 20338L, 20392L,  
> 20453L, 20454L,
> ? ? ? ? ? ? ? ? ? ?20467L, 20500L, 20682L, 20687L, 20692L, 20744L,  
> 20750L, 20751L,
> ? ? ? ? ? ? ? ? ? ?20806L, 20848L, 20856L, 20959L, 21023L, 21048L,  
> 21114L, 21134L,
> ? ? ? ? ? ? ? ? ? ?21135L, 21320L, 21329L, 21349L, 21353L, 21521L,  
> 21663L, 21784L,
> ? ? ? ? ? ? ? ? ? ?22031L, 22081L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L,  
> 1L, 1L, 2L, 2L,
> ? ? ? ? ? ? ? ? ? ?2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L,  
> 1L, 1L, 2L, 1L,
> ? ? ? ? ? ? ? ? ? ?2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L), .Dim =  
> c(38L, 5L), .Dimnames = list(
> ? ? ? ? ? ? ? ? ? ? ?NULL, c("a", "b", "c", "ID", "Arm")), na.action  
> = structure(37, class = "omit"))
> goop
> mySD <- function(x) {sapply(x,sqrt(var(x)))}
> class(mySD)
> by(goop[,c("a","b","c")],goop[,"Arm"],mySD)
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology  
> and Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:18}}


From veniarakelian at yahoo.com  Mon Dec 28 22:10:09 2015
From: veniarakelian at yahoo.com (veni arakelian)
Date: Mon, 28 Dec 2015 21:10:09 +0000 (UTC)
Subject: [R] Multidimensional array
In-Reply-To: <F0D1C062-2049-4F48-8CA9-E7EFB147FBC8@comcast.net>
References: <F0D1C062-2049-4F48-8CA9-E7EFB147FBC8@comcast.net>
Message-ID: <983637899.4389936.1451337009642.JavaMail.yahoo@mail.yahoo.com>

Dear David

The first three arrays are of the formval(:,:,1) =
? ? ?1 ? ? 1 ? ? 1? ? ?1 ? ? 1 ? ? 1? ? ?1 ? ? 1 ? ? 1

val(:,:,2) =
? ? ?1 ? ? 0 ? ? 0? ? ?0 ? ? 1 ? ? 1? ? ?0 ? ? 1 ? ? 1

val(:,:,3) =
? ? ?1 ? ? 0 ? ? 0? ? ?0 ? ? 1 ? ? 1? ? ?0 ? ? 1 ? ? 1
So I plan to import them in R as a single time series (i.e., 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1) and 'reshape' it in the form val[ , , t].?
If there is another way more efficient, please let me know.
Kind regards,Veni
      From: David Winsemius <dwinsemius at comcast.net>
 To: veni arakelian <veniarakelian at yahoo.com> 
Cc: "r-help at r-project.org" <r-help at r-project.org>
 Sent: Monday, December 28, 2015 9:24 PM
 Subject: Re: [R] Multidimensional array
   

> On Dec 27, 2015, at 11:49 PM, veni arakelian via R-help <r-help at r-project.org> wrote:
> 
> 
> 
> Hi All.
> 
> I'm very new in R. I'd like to import data of the form MxMxT, i.e., MxM array in T distinct points.
> Any ideas how to do that?

It will depend on the form in which this data is represented as a file object. Can you show the appearance of the first two or three MxM items as they would be viewed in a text editor. And PLEASE post in plain text.


> Regards,
> Veni 
> 
> 
> ??? [[alternative HTML version deleted]]

Postin in HTML is generally a sign that you have not read the posting-guide page.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


  
	[[alternative HTML version deleted]]


From finkel.mark at gmail.com  Mon Dec 28 22:39:00 2015
From: finkel.mark at gmail.com (Mark Finkelstein)
Date: Mon, 28 Dec 2015 16:39:00 -0500
Subject: [R] Format a dataset for use with R with chunking
Message-ID: <CAMn=oqfydioxba4PNoSFAO51gdVDKtN29ausBXkWwoShiE+QkQ@mail.gmail.com>

The problem is common, I have 100GB of data, but only 8GB of RAM. I was
thinking of transforming the 100GB of data, which right now is in a nonCSV,
fixed row format, to something that R could load quickly and easily in
chunks - sort of like pages perhaps.

I might be able to do this with some SQL server, but I'm unsure how well
this works out with the constant conversion, and I feel there might be a
better approach, since I am particularly interested in speed, as I will
have to go through several iterations with this data, and speed counts.

I was hoping someone much more experienced than I might have a good answer
since there's a lot out there.

Any advice would be very much appreciated.

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Mon Dec 28 23:56:46 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 28 Dec 2015 14:56:46 -0800
Subject: [R] Format a dataset for use with R with chunking
In-Reply-To: <CAMn=oqfydioxba4PNoSFAO51gdVDKtN29ausBXkWwoShiE+QkQ@mail.gmail.com>
References: <CAMn=oqfydioxba4PNoSFAO51gdVDKtN29ausBXkWwoShiE+QkQ@mail.gmail.com>
Message-ID: <CAF8bMcYYx=VrUNF4+=zNcA2FW0Qt29vTUg6=nq-Ttrcd421JMg@mail.gmail.com>

What do you hope to do with this data while it is in R?
E.g., do you want to plot it, fit a model to it, to select a few
rows or columns from it, sort it, summarize lots of small subsets
of it, or something else?

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Dec 28, 2015 at 1:39 PM, Mark Finkelstein <finkel.mark at gmail.com>
wrote:

> The problem is common, I have 100GB of data, but only 8GB of RAM. I was
> thinking of transforming the 100GB of data, which right now is in a nonCSV,
> fixed row format, to something that R could load quickly and easily in
> chunks - sort of like pages perhaps.
>
> I might be able to do this with some SQL server, but I'm unsure how well
> this works out with the constant conversion, and I feel there might be a
> better approach, since I am particularly interested in speed, as I will
> have to go through several iterations with this data, and speed counts.
>
> I was hoping someone much more experienced than I might have a good answer
> since there's a lot out there.
>
> Any advice would be very much appreciated.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From jdnewmil at dcn.davis.ca.us  Tue Dec 29 00:02:07 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 28 Dec 2015 15:02:07 -0800
Subject: [R] interpolation using R for PCR quantification
In-Reply-To: <CAMk+s2SspgLwGrm7F1D_ytd87bkx_A22dYM4d8JQBzWmR-smOQ@mail.gmail.com>
References: <CAMk+s2SspgLwGrm7F1D_ytd87bkx_A22dYM4d8JQBzWmR-smOQ@mail.gmail.com>
Message-ID: <8E9490D6-A00E-4DB5-B8E8-9B472CE30893@dcn.davis.ca.us>

There is some terminology confusion here... interpolation as implemented by approx or spline usually means estimating values between known points.  You seem to have approximate (not known) points, and are looking to apply a linear regression model to estimate missing data. Beware that mixing estimates (a.k.a. yhat) with measured data can be misleading because yhat is missing the error term(s) that are present in the original data.

If you really want to do this after reconsidering, then perhaps the following does what you want?

logconlm  <- lm( log10( con ) ~ cyc, data = df )
df$yhat <- predict( logconlm, newdata=df )
df$logconfixed  <- ifelse( is.na( df$con ), df$yhat, log10( df$con ) )

plot( df$cyc, df$logconfixed, pch=df$sam )
-- 
Sent from my phone. Please excuse my brevity.

On December 24, 2015 8:12:13 AM PST, Luigi Marongiu <marongiu.luigi at gmail.com> wrote:
>Dear all,
>I am a newbie in interpolation using R and I would like to learn
>better the procedure.
>I am applying interpolation to quantify nucleic acid targets using an
>assay known as PCR. To do this, I have two sets of variables: standard
>of known concentrations and query for which I need to identify the
>concentration.
>For each variable I have the output of the assay (cyc) and an
>approximation of the concentration expressed in relation to the
>concentration of the standard, so 5 means 10^5 etc.
>Given that the actual concentration of the standards is given in the
>'con' variable, the relation is that x=log10(con) and y = cyc, as
>represented in the first plot of the following example. In black are
>depicted the standard and in red the query samples.
>
>Now, to obtain interpolation the only function that i know is
>approx(). The first problem is that I need to switch the x-y variables
>because the values specifying where interpolation is to take place go
>in the 'xout' parameter and I have y outputs. If I maintain the
>original x/y orientation the output from approx() is empty. How can I
>keep the original layout? I must admit, anyhow, that the construct
>x=log10(con) and y = cyc is an artifact of the PCR analysis, since the
>independent variable is indeed the cyc value.
>
>The second problem I am facing -- and the most important -- is that
>the output seems weird. The values I get are simply the concentration
>input as such and not calculated by interpolation. In the example, the
>output I obtain is:
> [1]       NA 1480.600 1480.600  148.060  202.319  148.060   14.806
>14.806   14.806
>[10]       NA
>the first and last value are OK because the cyc values are outside the
>dynamic range under evaluation, but the only value that seems genuine
>is 202.319, the others are just the values I placed in the 'con'
>variable. For instance the second and third values have cyc = 26.992
>and 26.961 and yet they are both assigned to 1480.600.
>What I am getting wrong?
>Thank you (and merry Christmas!)
>L
>
>
>>>>
>dil <- c(5,    5,    5,    5,    4,    4,    4,    3,    3,    3,
>3,    3,    2,    2,    2,    2,    2,    2,    1,    1,    1,    1,
> 1,    1,    1)
>sam <- c(0,    0,    0,    1,    0,    0,    0,    0,    0,    0,
>1,    1,    0,    0,    0,    1,    1,    1,    0,    0,    0,    1,
> 1,    1,    1)
>cyc <- c(20.787,    20.494,    20.475,    20.189,    23.991,
>24.084,    23.863,    26.298,    28.007,    27.413,    26.992,
>26.961,    31.363,    30.979,    32.013,    31.004,    30.576,
>31.195,    35.219,    34.096,    38.088,    34.934,    35.101,
>35.206,    38.366)
>con <- c(148060,    148060,    148060,    NA,    14806,    14806,
>14806,    1480.6,    1480.6,    1480.6,    NA,    NA,    148.06,
>148.06,    148.06,    NA,    NA,    NA,    14.806,    14.806,
>14.806,    NA,    NA,    NA,    NA)
>df <- data.frame(dil, sam, cyc, con)
>
>std <- subset(df, sam == 0)
>qry <- subset(df, sam == 1)
>
>plot(std$cyc ~ std$dil)
>points(qry$dil, qry$cyc, col ="red")
>
>Q <- approx(x=std$cyc, y=log10(std$con), xout=qry$cyc,
>method="linear", rule = 1)
>(10^(Q$y))
>
>plot(std$cyc, y=log10(std$con))
>abline(lm(log10(df$con) ~ df$cyc))
>abline(v=qry$cyc, col="blue")
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Dec 29 00:21:51 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 29 Dec 2015 12:21:51 +1300
Subject: [R] [FORGED]  Mixed Beta Disrubutions
In-Reply-To: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
Message-ID: <5681C40F.1060504@auckland.ac.nz>

On 29/12/15 08:40, mesude bayrakci wrote:
> Hello,
>
> I have data; one column and 310 rows. When I plot the histogram, it has two
> peaks; please see the attachment. I would like to find appropriate
> distribution that fits the data. I tried to mixtools in R, but it did not
> fit well.
>
> I want to mix two beta distribution. I found that there is betareg package
> in R but the shape1,shape2 were known or there were two different data in
> the all examples.
>
> I do not know where to start. How can I use betamix in R to fit the data?
> Any hint?
>
> I really appreciate.
>
>
> Thank you


GIYF

Searching on "beta mixture model in r" leads to a number of hits, the 
most relevant one (it seems to me) being:

r - Mixture of beta distributions: full example - Cross Validated

which leads to

http://stats.stackexchange.com/questions/114959/mixture-of-beta-distributions-full-example

A fully worked example is provided.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From wdunlap at tibco.com  Tue Dec 29 00:27:47 2015
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 28 Dec 2015 15:27:47 -0800
Subject: [R] error using by(goop[, c("a", "b", "c")], goop[, "Arm"],
 mySD) was Re: by gives no results, gives warning that data are non-numeric,
 but the data appears to be numeric.
In-Reply-To: <56816961020000CB00144DC1@smtp.medicine.umaryland.edu>
References: <56816961020000CB00144DC1@smtp.medicine.umaryland.edu>
Message-ID: <CAF8bMcYyiTTaq6jwyf7L8Or=_UG7fpghtx49KQpaGsoapi0bkA@mail.gmail.com>

sapply's FUN argument must be a function (or a character string naming
a function) and sqrt(var(x)) evaluates to a number, not a function.
  mySD <- function(x) {sapply(x,function(x)sqrt(var(x)))}
would work, but I like to make it
  mySD <- function(x) {sapply(x,function(xi)sqrt(var(xi)))}
to make it clear to the user that the 'x's in the first function
have 2 different meanings.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Dec 28, 2015 at 1:54 PM, John Sorkin <jsorkin at grecc.umaryland.edu>
wrote:

>
> I am  trying to use the by function to get the SD of each column of a data
> frame, stratified by ARM. Using a suggestion provided by both William
> Dunlap and Rolf Turner, I have written the code below which fails with the
> error:
> Error in match.fun(FUN) :   'sqrt(var(x))' is not a function, character or
> symbol
>
>
> code:
> mySD <- function(x) {sapply(x,sqrt(var(x)))}
> class(mySD)
> by(goop[,c("a","b","c")],goop[,"Arm"],mySD)
>
> Below I include code which will create goop (thank you Bill for suggesting
> I use dput), which will run will recreate mmy error. Any thoughts on why
> the by function fails and how to fix it would be appreciated.
>
> Thank you,
> John
>
> goop <- structure(c(180L, 153L, 149L, 218L, 217L, 147L, 141L, 141L, 174L,
>                     162L, 163L, 206L, 168L, 227L, 142L, 179L, 270L, 206L,
> 243L, 134L,
>                     131L, 222L, 142L, 119L, 149L, 190L, 196L, 145L, 185L,
> 235L, 165L,
>                     246L, 152L, 186L, 222L, 248L, 125L, 149L, 233L, 145L,
> 154L, 231L,
>                     146L, 161L, 150L, 133L, 115L, 145L, 180L, 202L, 158L,
> 210L, 153L,
>                     136L, 265L, 255L, 199L, 131L, 110L, 169L, 147L, 133L,
> 108L, 203L,
>                     185L, 153L, 154L, 207L, 167L, 233L, 135L, 127L, 197L,
> 245L, 124L,
>                     125L, 146L, 139L, 158L, 211L, 183L, 169L, 137L, 114L,
> 176L, 143L,
>                     156L, 209L, 163L, 194L, 151L, 238L, 253L, 293L, 208L,
> 147L, 186L,
>                     179L, 150L, 203L, 112L, 185L, 156L, 127L, 132L, 243L,
> 177L, 242L,
>                     144L, 140L, 194L, 249L, 109L, 91L, 10163L, 10987L,
> 19882L, 19899L,
>                     20104L, 20105L, 20167L, 20318L, 20338L, 20392L,
> 20453L, 20454L,
>                     20467L, 20500L, 20682L, 20687L, 20692L, 20744L,
> 20750L, 20751L,
>                     20806L, 20848L, 20856L, 20959L, 21023L, 21048L,
> 21114L, 21134L,
>                     21135L, 21320L, 21329L, 21349L, 21353L, 21521L,
> 21663L, 21784L,
>                     22031L, 22081L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L,
> 1L, 2L, 2L,
>                     2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L,
> 1L, 2L, 1L,
>                     2L, 1L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L), .Dim = c(38L,
> 5L), .Dimnames = list(
>                       NULL, c("a", "b", "c", "ID", "Arm")), na.action =
> structure(37, class = "omit"))
> goop
> mySD <- function(x) {sapply(x,sqrt(var(x)))}
> class(mySD)
> by(goop[,c("a","b","c")],goop[,"Arm"],mySD)
>
>
>
>
>
>
>
> John David Sorkin M.D., Ph.D.
> Professor of Medicine
> Chief, Biostatistics and Informatics
> University of Maryland School of Medicine Division of Gerontology and
> Geriatric Medicine
> Baltimore VA Medical Center
> 10 North Greene Street
> GRECC (BT/18/GR)
> Baltimore, MD 21201-1524
> (Phone) 410-605-7119
> (Fax) 410-605-7913 (Please call phone number above prior to faxing)
>
> Confidentiality Statement:
> This email message, including any attachments, is for ...{{dropped:16}}


From jdnewmil at dcn.davis.ca.us  Tue Dec 29 00:28:55 2015
From: jdnewmil at dcn.davis.ca.us (Jeff Newmiller)
Date: Mon, 28 Dec 2015 15:28:55 -0800
Subject: [R] Format a dataset for use with R with chunking
In-Reply-To: <CAMn=oqfydioxba4PNoSFAO51gdVDKtN29ausBXkWwoShiE+QkQ@mail.gmail.com>
References: <CAMn=oqfydioxba4PNoSFAO51gdVDKtN29ausBXkWwoShiE+QkQ@mail.gmail.com>
Message-ID: <0A867F37-E8E5-4189-9F64-853362735BDB@dcn.davis.ca.us>

Have you looked at the High Performance Computing Task View on CRAN?

Whatever you do,  keep in mind that the algorithms you intend to apply will have a strong impact on which data management approach is going to work best. Start small before diving in with all your data, and try successively larger amounts of data to help extrapolate weekday will happen when you process the whole data set. 

In addition,  if you do use SQL, keep in mind that your table schema and index selection can make or break your project (but this is not a SQL support forum).
-- 
Sent from my phone. Please excuse my brevity.

On December 28, 2015 1:39:00 PM PST, Mark Finkelstein <finkel.mark at gmail.com> wrote:
>The problem is common, I have 100GB of data, but only 8GB of RAM. I was
>thinking of transforming the 100GB of data, which right now is in a
>nonCSV,
>fixed row format, to something that R could load quickly and easily in
>chunks - sort of like pages perhaps.
>
>I might be able to do this with some SQL server, but I'm unsure how
>well
>this works out with the constant conversion, and I feel there might be
>a
>better approach, since I am particularly interested in speed, as I will
>have to go through several iterations with this data, and speed counts.
>
>I was hoping someone much more experienced than I might have a good
>answer
>since there's a lot out there.
>
>Any advice would be very much appreciated.
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide
>http://www.R-project.org/posting-guide.html
>and provide commented, minimal, self-contained, reproducible code.

	[[alternative HTML version deleted]]


From mesudebayrakci at gmail.com  Tue Dec 29 00:30:32 2015
From: mesudebayrakci at gmail.com (mesude bayrakci)
Date: Mon, 28 Dec 2015 18:30:32 -0500
Subject: [R] [FORGED]  Mixed Beta Disrubutions
In-Reply-To: <5681C40F.1060504@auckland.ac.nz>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<5681C40F.1060504@auckland.ac.nz>
Message-ID: <EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>

Thank you for your response. I saw already that example and some others too. However, they defined alpha and beta in the examples or use two different dataset. 
I did not know alpha and beta values and have only one data set.  I could calculate alpha and beta by using variance and means for the data has one peaks. 

How can I calculate alpha and beta for two peak distributions? 


Thanks


> On Dec 28, 2015, at 6:21 PM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
> 
>> On 29/12/15 08:40, mesude bayrakci wrote:
>> Hello,
>> 
>> I have data; one column and 310 rows. When I plot the histogram, it has two
>> peaks; please see the attachment. I would like to find appropriate
>> distribution that fits the data. I tried to mixtools in R, but it did not
>> fit well.
>> 
>> I want to mix two beta distribution. I found that there is betareg package
>> in R but the shape1,shape2 were known or there were two different data in
>> the all examples.
>> 
>> I do not know where to start. How can I use betamix in R to fit the data?
>> Any hint?
>> 
>> I really appreciate.
>> 
>> 
>> Thank you
> 
> 
> GIYF
> 
> Searching on "beta mixture model in r" leads to a number of hits, the most relevant one (it seems to me) being:
> 
> r - Mixture of beta distributions: full example - Cross Validated
> 
> which leads to
> 
> http://stats.stackexchange.com/questions/114959/mixture-of-beta-distributions-full-example
> 
> A fully worked example is provided.
> 
> cheers,
> 
> Rolf Turner
> 
> -- 
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Tue Dec 29 00:38:19 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 28 Dec 2015 15:38:19 -0800
Subject: [R] Multidimensional array
In-Reply-To: <983637899.4389936.1451337009642.JavaMail.yahoo@mail.yahoo.com>
References: <F0D1C062-2049-4F48-8CA9-E7EFB147FBC8@comcast.net>
	<983637899.4389936.1451337009642.JavaMail.yahoo@mail.yahoo.com>
Message-ID: <8F3C4435-85EA-4EA5-9771-77D351C28C9C@comcast.net>


> On Dec 28, 2015, at 1:10 PM, veni arakelian <veniarakelian at yahoo.com> wrote:
> 
> Dear David
> 
> The first three arrays are of the form
> val(:,:,1) =
> 
>      1     1     1
>      1     1     1
>      1     1     1
> 
> 
> val(:,:,2) =
> 
>      1     0     0
>      0     1     1
>      0     1     1
> 
> 
> val(:,:,3) =
> 
>      1     0     0
>      0     1     1
>      0     1     1
> 
> So I plan to import them in R as a single time series (i.e., 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1) and 'reshape' it in the form val[ , , t]. 

I do not think you have demonstrated great powers of reading comprehension. I asked that you post the file as it would appear when viewed in a text editor. I also asked that you post in HTML, and it does not appear that you did that either. Please read the Posting Guide and do whatever self-study is needed so that you can configure your email client to post in plain text.

I'm going to make the assumption that the file looks like:

1     1     1
1     1     1
1     1     1
1     0     0
0     1     1
0     1     1
1     0     0
0     1     1
0     1     1

We can make a text/character representation of such a file for demonstration purposes:

txt <- "1     1     1
1     1     1
1     1     1
1     0     0
0     1     1
0     1     1
1     0     0
0     1     1
0     1     1"


The `scan` function can read from either character vectors or from files. The matrix function has a byrow argument whose default os FALSE but I think we need to set it to TRUE.

a <- matrix( scan(text=txt), ncol=3, byrow=TRUE) 

So now we have a 9 x 3 matrix and need to stratify into 3 3x3 matrices within an array-classed object. Simply using `array( a, c(3,3,3) )` will not succeed. However, one can reshape that value into the desired object with `aperm`

 aperm( array(a, c(3,3,3) ) , c(1,3,2) )

#   -- result --------
, , 1

     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    1    1    1

, , 2

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    1
[3,]    0    1    1

, , 3

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    1
[3,]    0    1    1



-- David.




> 
> If there is another way more efficient, please let me know.
> 
> Kind regards,
> Veni
> 
> From: David Winsemius <dwinsemius at comcast.net>
> To: veni arakelian <veniarakelian at yahoo.com> 
> Cc: "r-help at r-project.org" <r-help at r-project.org>
> Sent: Monday, December 28, 2015 9:24 PM
> Subject: Re: [R] Multidimensional array
> 
> 
> > On Dec 27, 2015, at 11:49 PM, veni arakelian via R-help <r-help at r-project.org> wrote:
> > 
> > 
> > 
> > Hi All.
> > 
> > I'm very new in R. I'd like to import data of the form MxMxT, i.e., MxM array in T distinct points.
> > Any ideas how to do that?
> 
> It will depend on the form in which this data is represented as a file object. Can you show the appearance of the first two or three MxM items as they would be viewed in a text editor. And PLEASE post in plain text.
> 
> 
> > Regards,
> > Veni 
> > 
> > 
> >     [[alternative HTML version deleted]]
> 
> Postin in HTML is generally a sign that you have not read the posting-guide page.
> 
> > 
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> > and provide commented, minimal, self-contained, reproducible code.
> 
> 
> David Winsemius
> Alameda, CA, USA
> 
> 
> 

David Winsemius
Alameda, CA, USA


From r.turner at auckland.ac.nz  Tue Dec 29 00:39:40 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 29 Dec 2015 12:39:40 +1300
Subject: [R] [FORGED]  Mixed Beta Disrubutions
In-Reply-To: <EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<5681C40F.1060504@auckland.ac.nz>
	<EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>
Message-ID: <5681C83C.9090503@auckland.ac.nz>

On 29/12/15 12:30, mesude bayrakci wrote:
> Thank you for your response. I saw already that example and some
> others too. However, they defined alpha and beta in the examples or
> use two different dataset. I did not know alpha and beta values and
> have only one data set.  I could calculate alpha and beta by using
> variance and means for the data has one peaks.
>
> How can I calculate alpha and beta for two peak distributions?


Given your level of obtuseness I think that the advancement of science 
would be best served if you were not encouraged to pursue this line of 
endeavour any further.

Be that as it may:  *NO*, "they" did not define alpha and beta in the
example (singular).  They *simulated* a data set using known values of 
alpha and beta, and then fitted a beta mixture model to the simulated 
data, obtaining fitted values of the alphas and betas that were 
satisfyingly close to the "true" values from which the data were simulated.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From mesudebayrakci at gmail.com  Tue Dec 29 01:35:42 2015
From: mesudebayrakci at gmail.com (mesude bayrakci)
Date: Mon, 28 Dec 2015 19:35:42 -0500
Subject: [R] [FORGED]  Mixed Beta Disrubutions
In-Reply-To: <5681C83C.9090503@auckland.ac.nz>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<5681C40F.1060504@auckland.ac.nz>
	<EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>
	<5681C83C.9090503@auckland.ac.nz>
Message-ID: <CAH6SrewsZzjA0FQa72gKJGAz2sSPe0XfTfaZXBOnsLJdrV8cjg@mail.gmail.com>

Hello,

"Treat everyone with politeness, even those who are rude to you - not
because they are nice, but because you are"

The forum's name is  "R-help",  not "R-help for people who are experts in
statistic and R". Please if you would like to help and just help. If you do
not like the posts (or questions), you simply do not answer.








On Mon, Dec 28, 2015 at 6:39 PM, Rolf Turner <r.turner at auckland.ac.nz>
wrote:

> On 29/12/15 12:30, mesude bayrakci wrote:
>
>> Thank you for your response. I saw already that example and some
>> others too. However, they defined alpha and beta in the examples or
>> use two different dataset. I did not know alpha and beta values and
>> have only one data set.  I could calculate alpha and beta by using
>> variance and means for the data has one peaks.
>>
>> How can I calculate alpha and beta for two peak distributions?
>>
>
>
> Given your level of obtuseness I think that the advancement of science
> would be best served if you were not encouraged to pursue this line of
> endeavour any further.
>
> Be that as it may:  *NO*, "they" did not define alpha and beta in the
> example (singular).  They *simulated* a data set using known values of
> alpha and beta, and then fitted a beta mixture model to the simulated data,
> obtaining fitted values of the alphas and betas that were satisfyingly
> close to the "true" values from which the data were simulated.
>
>
> cheers,
>
> Rolf Turner
>
> --
> Technical Editor ANZJS
> Department of Statistics
> University of Auckland
> Phone: +64-9-373-7599 ext. 88276
>

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Tue Dec 29 03:06:35 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Mon, 28 Dec 2015 18:06:35 -0800
Subject: [R] [FORGED]  Mixed Beta Disrubutions
In-Reply-To: <CAH6SrewsZzjA0FQa72gKJGAz2sSPe0XfTfaZXBOnsLJdrV8cjg@mail.gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<5681C40F.1060504@auckland.ac.nz>
	<EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>
	<5681C83C.9090503@auckland.ac.nz>
	<CAH6SrewsZzjA0FQa72gKJGAz2sSPe0XfTfaZXBOnsLJdrV8cjg@mail.gmail.com>
Message-ID: <7687A477-AF20-4C75-88FC-DAC7DB495BE1@comcast.net>


> On Dec 28, 2015, at 4:35 PM, mesude bayrakci <mesudebayrakci at gmail.com> wrote:
> 
> Hello,
> 
> "Treat everyone with politeness, even those who are rude to you - not
> because they are nice, but because you are"

It's generally considered rude to accuse someone of rudeness. However customs in the regard are highly variable as the Posting Guide reminds us: " Remember that customs differ. Some people are very direct. Others surround everything they say with hedges and apologies. Be tolerant. Rudeness is never warranted, but sometimes `read the manual? is the appropriate response. Don?t waste time discussing such matters on the list. Ad hominem comments are absolutely out of place."

> 
> The forum's name is  "R-help",  not "R-help for people who are experts in
> statistic and R". Please if you would like to help and just help. If you do
> not like the posts (or questions), you simply do not answer.

Rolf _did_ answer. Initially, even with "cheers". 

So, hgave you now awakened to the fact that Rolf's reference to Achim Zeileis' response to a basically identical question was on point and addressed all of your questions? And that, therefore, you were wrong (in your reply) to his cheery initial answer about him having not answered your question? I think the word "obtuse" was technically correct in this instance, at least in the sense of "exhibiting an unwillingness to accept or acknowledge helpful advice". 

Another word that has occurred to me in this context:

in?grate
?in?r?t/
noun
	? 1.
an ungrateful person.
adjective  literary
	? 1.
ungrateful.

-- 
David.


> 
> On Mon, Dec 28, 2015 at 6:39 PM, Rolf Turner <r.turner at auckland.ac.nz>
> wrote:
> 
>> On 29/12/15 12:30, mesude bayrakci wrote:
>> 
>>> Thank you for your response. I saw already that example and some
>>> others too. However, they defined alpha and beta in the examples or
>>> use two different dataset. I did not know alpha and beta values and
>>> have only one data set.  I could calculate alpha and beta by using
>>> variance and means for the data has one peaks.
>>> 
>>> How can I calculate alpha and beta for two peak distributions?
>>> 
>> 
>> 
>> Given your level of obtuseness I think that the advancement of science
>> would be best served if you were not encouraged to pursue this line of
>> endeavour any further.
>> 
>> Be that as it may:  *NO*, "they" did not define alpha and beta in the
>> example (singular).  They *simulated* a data set using known values of
>> alpha and beta, and then fitted a beta mixture model to the simulated data,
>> obtaining fitted values of the alphas and betas that were satisfyingly
>> close to the "true" values from which the data were simulated.
>> 
>> 
>> cheers,
>> 
>> Rolf Turner
>> 
>> --
>> Technical Editor ANZJS
>> Department of Statistics
>> University of Auckland
>> Phone: +64-9-373-7599 ext. 88276
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Tue Dec 29 03:58:09 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Tue, 29 Dec 2015 13:58:09 +1100
Subject: [R] Mixed Beta Disrubutions
In-Reply-To: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
Message-ID: <CA+8X3fUv6QyKp7uKVpUHSqbL+J96zrjX0X8qYZbi8bm_b9PBWg@mail.gmail.com>

Hi mesude,
Achim's example seems particularly clear. Install the "betareg" and
"flexmix" packages. I obtain a reasonable looking result for alpha and beta
for a simulated dataset very similar to yours.

> a
    Comp.1     Comp.2
10.0674445  0.6452801
> b
  Comp.1   Comp.2
2.830934 0.769768

Jim


On Tue, Dec 29, 2015 at 6:40 AM, mesude bayrakci <mesudebayrakci at gmail.com>
wrote:

> Hello,
>
> I have data; one column and 310 rows. When I plot the histogram, it has two
> peaks; please see the attachment. I would like to find appropriate
> distribution that fits the data. I tried to mixtools in R, but it did not
> fit well.
>
> I want to mix two beta distribution. I found that there is betareg package
> in R but the shape1,shape2 were known or there were two different data in
> the all examples.
>
> I do not know where to start. How can I use betamix in R to fit the data?
> Any hint?
>
> I really appreciate.
>
>
> Thank you
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


From okeyes at wikimedia.org  Tue Dec 29 03:21:08 2015
From: okeyes at wikimedia.org (Oliver Keyes)
Date: Mon, 28 Dec 2015 21:21:08 -0500
Subject: [R] [FORGED] Mixed Beta Disrubutions
In-Reply-To: <7687A477-AF20-4C75-88FC-DAC7DB495BE1@comcast.net>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<5681C40F.1060504@auckland.ac.nz>
	<EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>
	<5681C83C.9090503@auckland.ac.nz>
	<CAH6SrewsZzjA0FQa72gKJGAz2sSPe0XfTfaZXBOnsLJdrV8cjg@mail.gmail.com>
	<7687A477-AF20-4C75-88FC-DAC7DB495BE1@comcast.net>
Message-ID: <CAAUQgdC1QixRNqFx9KAUJW3QeWNrUMmiVgfXf6Ukuk-EbZko0Q@mail.gmail.com>

I'm sorry, but there's no universe in which replying to a good faith
question with "Given your level of obtuseness I think that the
advancement of science would be best served if you were not encouraged
to pursue this line of endeavour any further." is polite. That's rude.
It would be totally unacceptable behaviour at any university or
company worth a damn, and I see no reason why we should treat people
who come here worse than we would people who are literally paid to be
around us.

The idea that pointing /out/ it is rude is rude does nothing except to
shield those who perpetuate the reputation this list has for being an
unfriendly place. Every time I see this list discussed elsewhere,
every time I see a list of locations for newcomers to go about R, the
constant refrain is 'don't read the mailing list'. Comments like that,
and the defence of comments like that, are precisely why.

Mesude: I would recommend looking at either Cross-Validated (think
StackOverflow for statistics) or StackOverflow itself. Twitter, with
the #rstats hashtag, is also surprisingly useful, as is the IRC
channel (#R on Freenode). I have consistently got a more communal and
professional response from these locations. I hope they serve you
well. And if they do not work, feel free to reach out to me offlist
and I will do my best to put you in touch with those
statistics-focused R programmers I know.

On 28 December 2015 at 21:06, David Winsemius <dwinsemius at comcast.net> wrote:
>
>> On Dec 28, 2015, at 4:35 PM, mesude bayrakci <mesudebayrakci at gmail.com> wrote:
>>
>> Hello,
>>
>> "Treat everyone with politeness, even those who are rude to you - not
>> because they are nice, but because you are"
>
> It's generally considered rude to accuse someone of rudeness. However customs in the regard are highly variable as the Posting Guide reminds us: " Remember that customs differ. Some people are very direct. Others surround everything they say with hedges and apologies. Be tolerant. Rudeness is never warranted, but sometimes `read the manual? is the appropriate response. Don?t waste time discussing such matters on the list. Ad hominem comments are absolutely out of place."
>
>>
>> The forum's name is  "R-help",  not "R-help for people who are experts in
>> statistic and R". Please if you would like to help and just help. If you do
>> not like the posts (or questions), you simply do not answer.
>
> Rolf _did_ answer. Initially, even with "cheers".
>
> So, hgave you now awakened to the fact that Rolf's reference to Achim Zeileis' response to a basically identical question was on point and addressed all of your questions? And that, therefore, you were wrong (in your reply) to his cheery initial answer about him having not answered your question? I think the word "obtuse" was technically correct in this instance, at least in the sense of "exhibiting an unwillingness to accept or acknowledge helpful advice".
>
> Another word that has occurred to me in this context:
>
> in?grate
> ?in?r?t/
> noun
>         ? 1.
> an ungrateful person.
> adjective  literary
>         ? 1.
> ungrateful.
>
> --
> David.
>
>
>>
>> On Mon, Dec 28, 2015 at 6:39 PM, Rolf Turner <r.turner at auckland.ac.nz>
>> wrote:
>>
>>> On 29/12/15 12:30, mesude bayrakci wrote:
>>>
>>>> Thank you for your response. I saw already that example and some
>>>> others too. However, they defined alpha and beta in the examples or
>>>> use two different dataset. I did not know alpha and beta values and
>>>> have only one data set.  I could calculate alpha and beta by using
>>>> variance and means for the data has one peaks.
>>>>
>>>> How can I calculate alpha and beta for two peak distributions?
>>>>
>>>
>>>
>>> Given your level of obtuseness I think that the advancement of science
>>> would be best served if you were not encouraged to pursue this line of
>>> endeavour any further.
>>>
>>> Be that as it may:  *NO*, "they" did not define alpha and beta in the
>>> example (singular).  They *simulated* a data set using known values of
>>> alpha and beta, and then fitted a beta mixture model to the simulated data,
>>> obtaining fitted values of the alphas and betas that were satisfyingly
>>> close to the "true" values from which the data were simulated.
>>>
>>>
>>> cheers,
>>>
>>> Rolf Turner
>>>
>>> --
>>> Technical Editor ANZJS
>>> Department of Statistics
>>> University of Auckland
>>> Phone: +64-9-373-7599 ext. 88276
>>>
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



-- 
Oliver Keyes
Count Logula
Wikimedia Foundation


From mesudebayrakci at gmail.com  Tue Dec 29 04:32:32 2015
From: mesudebayrakci at gmail.com (mesude bayrakci)
Date: Mon, 28 Dec 2015 22:32:32 -0500
Subject: [R] Mixed Beta Disrubutions
In-Reply-To: <CA+8X3fUv6QyKp7uKVpUHSqbL+J96zrjX0X8qYZbi8bm_b9PBWg@mail.gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<CA+8X3fUv6QyKp7uKVpUHSqbL+J96zrjX0X8qYZbi8bm_b9PBWg@mail.gmail.com>
Message-ID: <CAH6Sreyw7zcyrwSdeO+gZpGmWW0wnVb4virCLvFGaHRCOYNGxw@mail.gmail.com>

Hello all,

This would be my last comments on "politeness" discussion started after my
email, and do not want to keep the forum busy with this. Please do not
continue writing on this matter to the forum.

First of all, I just started using R for a short period of time, and using
it for a small part of my research, given that I am coming from different
technical background.
1) It is my understanding that the some of experts on this forum are
expecting high quality questions from people who have just started learning
things 2) It seems that Rolf's email has custom "cheer" signature. 3) In my
first email, I thank him for his response and tried to explain him that I
am aware of that example and did not think it would help me and thus asked
second question and did not claim that he would not answer my question, my
second response to Rolf's email was just basically reaction to his first
paragraph. 4) I think Oliver's comment is the most important one among the
points I stated here.

Thank you for your support and suggestions, Oliver. It is greatly
appreciated.

Thank you for your response, Jim.

Best,

Mesude


On Mon, Dec 28, 2015 at 9:58 PM, Jim Lemon <drjimlemon at gmail.com> wrote:

> Hi mesude,
> Achim's example seems particularly clear. Install the "betareg" and
> "flexmix" packages. I obtain a reasonable looking result for alpha and beta
> for a simulated dataset very similar to yours.
>
> > a
>     Comp.1     Comp.2
> 10.0674445  0.6452801
> > b
>   Comp.1   Comp.2
> 2.830934 0.769768
>
> Jim
>
>
> On Tue, Dec 29, 2015 at 6:40 AM, mesude bayrakci <mesudebayrakci at gmail.com
> > wrote:
>
>> Hello,
>>
>> I have data; one column and 310 rows. When I plot the histogram, it has
>> two
>> peaks; please see the attachment. I would like to find appropriate
>> distribution that fits the data. I tried to mixtools in R, but it did not
>> fit well.
>>
>> I want to mix two beta distribution. I found that there is betareg package
>> in R but the shape1,shape2 were known or there were two different data in
>> the all examples.
>>
>> I do not know where to start. How can I use betamix in R to fit the data?
>> Any hint?
>>
>> I really appreciate.
>>
>>
>> Thank you
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide
>> http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>>
>
>

	[[alternative HTML version deleted]]


From rlderickson at gmail.com  Tue Dec 29 04:59:12 2015
From: rlderickson at gmail.com (Ryan Derickson)
Date: Mon, 28 Dec 2015 22:59:12 -0500
Subject: [R] Mixed Beta Disrubutions
In-Reply-To: <CAH6Sreyw7zcyrwSdeO+gZpGmWW0wnVb4virCLvFGaHRCOYNGxw@mail.gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<CA+8X3fUv6QyKp7uKVpUHSqbL+J96zrjX0X8qYZbi8bm_b9PBWg@mail.gmail.com>
	<CAH6Sreyw7zcyrwSdeO+gZpGmWW0wnVb4virCLvFGaHRCOYNGxw@mail.gmail.com>
Message-ID: <E84357ED-88A7-410C-8EBA-B39B5F41415A@gmail.com>

Smug, self-satisfied responses are becoming more common as exemplified here. These contribute to nothing except the author's ego and distract from the generous and patient help others provide. Regardless of how naive the questioner or how well-credentialed the respondent, the community would benefit if the sources of such comments found other outlets for condescension. 

Ryan Derickson


> On Dec 28, 2015, at 10:32 PM, mesude bayrakci <mesudebayrakci at gmail.com> wrote:
> 
> Hello all,
> 
> This would be my last comments on "politeness" discussion started after my
> email, and do not want to keep the forum busy with this. Please do not
> continue writing on this matter to the forum.
> 
> First of all, I just started using R for a short period of time, and using
> it for a small part of my research, given that I am coming from different
> technical background.
> 1) It is my understanding that the some of experts on this forum are
> expecting high quality questions from people who have just started learning
> things 2) It seems that Rolf's email has custom "cheer" signature. 3) In my
> first email, I thank him for his response and tried to explain him that I
> am aware of that example and did not think it would help me and thus asked
> second question and did not claim that he would not answer my question, my
> second response to Rolf's email was just basically reaction to his first
> paragraph. 4) I think Oliver's comment is the most important one among the
> points I stated here.
> 
> Thank you for your support and suggestions, Oliver. It is greatly
> appreciated.
> 
> Thank you for your response, Jim.
> 
> Best,
> 
> Mesude
> 
> 
>> On Mon, Dec 28, 2015 at 9:58 PM, Jim Lemon <drjimlemon at gmail.com> wrote:
>> 
>> Hi mesude,
>> Achim's example seems particularly clear. Install the "betareg" and
>> "flexmix" packages. I obtain a reasonable looking result for alpha and beta
>> for a simulated dataset very similar to yours.
>> 
>>> a
>>    Comp.1     Comp.2
>> 10.0674445  0.6452801
>>> b
>>  Comp.1   Comp.2
>> 2.830934 0.769768
>> 
>> Jim
>> 
>> 
>> On Tue, Dec 29, 2015 at 6:40 AM, mesude bayrakci <mesudebayrakci at gmail.com
>>> wrote:
>> 
>>> Hello,
>>> 
>>> I have data; one column and 310 rows. When I plot the histogram, it has
>>> two
>>> peaks; please see the attachment. I would like to find appropriate
>>> distribution that fits the data. I tried to mixtools in R, but it did not
>>> fit well.
>>> 
>>> I want to mix two beta distribution. I found that there is betareg package
>>> in R but the shape1,shape2 were known or there were two different data in
>>> the all examples.
>>> 
>>> I do not know where to start. How can I use betamix in R to fit the data?
>>> Any hint?
>>> 
>>> I really appreciate.
>>> 
>>> 
>>> Thank you
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From petr.pikal at precheza.cz  Tue Dec 29 08:32:17 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 29 Dec 2015 07:32:17 +0000
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOfH-ZB8EpjV57kaRxtTyoG_Ek5tCzKx+T=G+2zKwokTiA@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfH-ZB8EpjV57kaRxtTyoG_Ek5tCzKx+T=G+2zKwokTiA@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50080F1@SRVEXCHMBX.precheza.cz>

Hi

You shall send your posts to the list, others can answer your questions and not only you can benefit from their answers too.

As you did not post any data it is hard to say what are your issues. I believe that there are several values which are the same not only near the peak but also at the bottom part. If your data look like I remember and you want to keep all values near the peak value regardless they are slightly growing or falling, one approach can be to identify peak value, and select all values near the peak (the threshold is up to you).

something like that

sss<-smooth.spline(temp$theta, temp$int, nknots=length(temp$int)/2)
peak <- which.max(predict(sss)$y)
baseline <- min(predict(sss)$y)
vyska <- predict(sss)$y[peak]
# 50% threshold
HM <- (vyska+baseline)/2
plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
#10% threshold
HM<-vyska-(vyska*.1)
plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)

> dput(temp)
structure(list(theta = c(28.995, 29.005, 29.015, 29.025, 29.035,
29.045, 29.055, 29.065, 29.075, 29.085, 29.095, 29.105, 29.115,
29.125, 29.135, 29.145, 29.155, 29.165, 29.175, 29.185, 29.195,
29.205, 29.215, 29.225, 29.235, 29.245, 29.255, 29.265, 29.275,
29.285, 29.295, 29.305, 29.315, 29.325, 29.335, 29.345, 29.355,
29.365, 29.375, 29.385, 29.395, 29.405, 29.415, 29.425, 29.435,
29.445, 29.455, 29.465, 29.475, 29.485, 29.495, 29.505, 29.515,
29.525, 29.535, 29.545, 29.555, 29.565, 29.575, 29.585, 29.595,
29.605, 29.615, 29.625, 29.635, 29.645, 29.655, 29.665, 29.675,
29.685, 29.695, 29.705, 29.715, 29.725, 29.735, 29.745, 29.755,
29.765, 29.775, 29.785, 29.795, 29.805, 29.815, 29.825, 29.835,
29.845, 29.855, 29.865, 29.875, 29.885, 29.895, 29.905, 29.915,
29.925, 29.935, 29.945, 29.955, 29.965, 29.975, 29.985, 29.995
), int = c(329L, 330L, 318L, 287L, 315L, 344L, 333L, 324L, 334L,
366L, 339L, 374L, 375L, 335L, 415L, 371L, 413L, 382L, 408L, 406L,
407L, 440L, 475L, 465L, 516L, 510L, 490L, 550L, 663L, 647L, 628L,
721L, 789L, 814L, 890L, 923L, 1085L, 1102L, 1222L, 1356L, 1521L,
1729L, 1868L, 2120L, 2491L, 2656L, 3196L, 3599L, 4128L, 4536L,
5043L, 5310L, 5638L, 5792L, 5699L, 5374L, 4886L, 4473L, 4293L,
3757L, 3319L, 2934L, 2422L, 1998L, 1753L, 1397L, 1163L, 972L,
854L, 775L, 648L, 695L, 616L, 553L, 554L, 509L, 530L, 483L, 482L,
406L, 451L, 422L, 403L, 393L, 396L, 348L, 367L, 428L, 345L, 384L,
330L, 342L, 312L, 313L, 323L, 328L, 340L, 322L, 330L, 305L, 311L
)), .Names = c("theta", "int"), row.names = 100:200, class = "data.frame")
>

Cheers
Petr


From: Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
Sent: Tuesday, December 29, 2015 5:54 AM
To: PIKAL Petr
Subject: Re: [R] need for help for solving operations in a vector

Hi Petr,
I would like to thank you for your time
i choose the following modification on the plotting so to keep only successive equals in red
plot(qWC1, col=(c(0, diff(qWC1))==0 )+1)
then, the red points I want to include in the irrigation period are the 3 successive red points in the plateau (equal values close to the max)
These points are very important as they are corresponding to saturation, we continue to irrigate with the same flow and values remains constant for a certain time, so I must add them to irrigation

up to now I didn't succeed in adding this plateau values to  theirrigations using the diff(qWC1, lag=1)
however, I wrote also some loops trying to catch all the irrigation and recharge separately and I still have some issues,
following are the loops I used, with comments corresponding to the issues
x<-qWC1
length(x)
irrig<-rep(1,61)
for (i in 2:61) {
if (x[i-1]<x[i]){
    irrig[i]<-x[i]-x[i-1]
}
}
rech<-rep(1,61)
for (i in 2:61) {
if (x[i-1]>x[i]){
    rech[i]<-x[i-1]-x[i]
}
}
plot(x, type = "l", col = "black", ylim = c(min(0), max(92)))
lines(irrig, type = "l", col = "cornflowerblue", ylim = c(min(0), max(15))) lines(rech, type = "l", col = "brown", ylim = c(min(0), max(15)))
temp<-irrig<1.000001 #logical command to identify low values into a temporary vector temp
temp2<-as.numeric(irrig>1.000001) #logical command to identify high values with 1
temp2
temp3<-as.numeric(rech>1.000001)
temp3
irrig2<-irrig*temp2  #remove values inf 1 mm inirrig
rech2<-rech*temp3    #same for rech
plot(irrig2, type = "l", col = "cornflowerblue", ylim = c(min(0), max(15))) lines(rech2, type = "l", col = "brown", ylim = c(min(0), max(15)))
Many thanks for your time and intellectual generosity
Cheers
Makram

On Mon, Dec 28, 2015 at 12:15 PM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

On top of answers you have got here is some plotting you need to answer yourself

plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)

Which from those red points you want to be included in irrigation period? All of them? Only part? Which part?

Based on your figures you probably will not get 100% correct answer.

Cheers
Petr



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Makram
> Belhaj Fraj
> Sent: Wednesday, December 23, 2015 8:35 AM
> To: r-help at r-project.org<mailto:r-help at r-project.org>; r-help-owner at r-project.org<mailto:r-help-owner at r-project.org>
> Subject: [R] need for help for solving operations in a vector
>
>  Dear colleagues
> i need your generous help to solve the following problem
>
> I have a  soil moisture time series qWC1 (61 values)
> > qWC1
>  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> 74.70059
> 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> 80.69793
> 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> 75.97412
> 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> 75.46087
> 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> 75.07930 75.07930 74.95275 74.95275  74.95275
>
> I want to measure consecutive increases corresponding to irrigation and
> consecutive decreases  corresponding to recharge I wrote the following
> code and it does not calculate for each increment in i?
> also note that I choose to not use diff command in time series because
> I  want also that "plateaux" corresponding to a minimum of 2 equal
> consecutive values are accounted as positive differences=irrigations so
> when x[i+1]==x[i] the difference y might be equal to the previous value
> xi
>
> following the code i wrote
>
> x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> "green")
> y<-rep(0,61)
> for (i in 1:61) {
> if (x[i+1] > x[i]){
>     y[i]==x[i+1]-x[i]
> } else if (x[i+1]==x[i]){
>     y[i]=x[i+2]-x[i]
> } else {
>     y[i]==x[i+1]-x[i]
> }
>
> }
> plot(y, type="h", col = "blueviolet")
>
> Many thank
> Makram
>


________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From belhajfraj.makram at gmail.com  Tue Dec 29 12:26:40 2015
From: belhajfraj.makram at gmail.com (Makram Belhaj Fraj)
Date: Tue, 29 Dec 2015 15:26:40 +0400
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C50080F1@SRVEXCHMBX.precheza.cz>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfH-ZB8EpjV57kaRxtTyoG_Ek5tCzKx+T=G+2zKwokTiA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C50080F1@SRVEXCHMBX.precheza.cz>
Message-ID: <CAP9hYOdkLULWT3VMk1buPvMqhOkWcjVqujG+NLRZZPFG5W_6Jw@mail.gmail.com>

Hi Petr,
I apologize It was my first time using r-help so I didn't know how to
replay to email to all or not,
I am replaying to all for this email,

Many thanks for the code, I am trying to use it,
please find attached the data file in csv,

following are the first lines of code to read the data and calculate qWC1

sdata<-read.csv("almondc_10augv.csv",head=TRUE,sep=",")

tint=sdata$scan #time intervall

mtime=sdata$mtime #measurement time

v1=sdata$vwc1 #value of moisture in percent

qWC1=200*v1 #conversion in mm to get the variable I am working on



On Tue, Dec 29, 2015 at 11:32 AM, PIKAL Petr <petr.pikal at precheza.cz> wrote:

> Hi
>
>
>
> You shall send your posts to the list, others can answer your questions
> and not only you can benefit from their answers too.
>
>
>
> As you did not post any data it is hard to say what are your issues. I
> believe that there are several values which are the same not only near the
> peak but also at the bottom part. If your data look like I remember and you
> want to keep all values near the peak value regardless they are slightly
> growing or falling, one approach can be to identify peak value, and select
> all values near the peak (the threshold is up to you).
>
>
>
> something like that
>
>
>
> sss<-smooth.spline(temp$theta, temp$int, nknots=length(temp$int)/2)
>
> peak <- which.max(predict(sss)$y)
>
> baseline <- min(predict(sss)$y)
>
> vyska <- predict(sss)$y[peak]
>
> # 50% threshold
>
> HM <- (vyska+baseline)/2
>
> plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
>
> #10% threshold
>
> HM<-vyska-(vyska*.1)
>
> plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
>
>
>
> > dput(temp)
>
> structure(list(theta = c(28.995, 29.005, 29.015, 29.025, 29.035,
>
> 29.045, 29.055, 29.065, 29.075, 29.085, 29.095, 29.105, 29.115,
>
> 29.125, 29.135, 29.145, 29.155, 29.165, 29.175, 29.185, 29.195,
>
> 29.205, 29.215, 29.225, 29.235, 29.245, 29.255, 29.265, 29.275,
>
> 29.285, 29.295, 29.305, 29.315, 29.325, 29.335, 29.345, 29.355,
>
> 29.365, 29.375, 29.385, 29.395, 29.405, 29.415, 29.425, 29.435,
>
> 29.445, 29.455, 29.465, 29.475, 29.485, 29.495, 29.505, 29.515,
>
> 29.525, 29.535, 29.545, 29.555, 29.565, 29.575, 29.585, 29.595,
>
> 29.605, 29.615, 29.625, 29.635, 29.645, 29.655, 29.665, 29.675,
>
> 29.685, 29.695, 29.705, 29.715, 29.725, 29.735, 29.745, 29.755,
>
> 29.765, 29.775, 29.785, 29.795, 29.805, 29.815, 29.825, 29.835,
>
> 29.845, 29.855, 29.865, 29.875, 29.885, 29.895, 29.905, 29.915,
>
> 29.925, 29.935, 29.945, 29.955, 29.965, 29.975, 29.985, 29.995
>
> ), int = c(329L, 330L, 318L, 287L, 315L, 344L, 333L, 324L, 334L,
>
> 366L, 339L, 374L, 375L, 335L, 415L, 371L, 413L, 382L, 408L, 406L,
>
> 407L, 440L, 475L, 465L, 516L, 510L, 490L, 550L, 663L, 647L, 628L,
>
> 721L, 789L, 814L, 890L, 923L, 1085L, 1102L, 1222L, 1356L, 1521L,
>
> 1729L, 1868L, 2120L, 2491L, 2656L, 3196L, 3599L, 4128L, 4536L,
>
> 5043L, 5310L, 5638L, 5792L, 5699L, 5374L, 4886L, 4473L, 4293L,
>
> 3757L, 3319L, 2934L, 2422L, 1998L, 1753L, 1397L, 1163L, 972L,
>
> 854L, 775L, 648L, 695L, 616L, 553L, 554L, 509L, 530L, 483L, 482L,
>
> 406L, 451L, 422L, 403L, 393L, 396L, 348L, 367L, 428L, 345L, 384L,
>
> 330L, 342L, 312L, 313L, 323L, 328L, 340L, 322L, 330L, 305L, 311L
>
> )), .Names = c("theta", "int"), row.names = 100:200, class = "data.frame")
>
> >
>
>
>
> Cheers
>
> Petr
>
>
>
>
>
> *From:* Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
> *Sent:* Tuesday, December 29, 2015 5:54 AM
> *To:* PIKAL Petr
> *Subject:* Re: [R] need for help for solving operations in a vector
>
>
>
> Hi Petr,
>
> I would like to thank you for your time
>
> i choose the following modification on the plotting so to keep only
> successive equals in red
>
> plot(qWC1, col=(c(0, diff(qWC1))==0 )+1)
>
> then, the red points I want to include in the irrigation period are the 3
> successive red points in the plateau (equal values close to the max)
>
> These points are very important as they are corresponding to saturation,
> we continue to irrigate with the same flow and values remains constant for
> a certain time, so I must add them to irrigation
>
>
>
> up to now I didn't succeed in adding this plateau values to
>  theirrigations using the diff(qWC1, lag=1)
>
> however, I wrote also some loops trying to catch all the irrigation and
> recharge separately and I still have some issues,
>
> following are the loops I used, with comments corresponding to the issues
>
> x<-qWC1
>
> length(x)
>
> irrig<-rep(1,61)
>
> for (i in 2:61) {
>
> if (x[i-1]<x[i]){
>
>     irrig[i]<-x[i]-x[i-1]
>
> }
>
> }
>
> rech<-rep(1,61)
>
> for (i in 2:61) {
>
> if (x[i-1]>x[i]){
>
>     rech[i]<-x[i-1]-x[i]
>
> }
>
> }
>
> plot(x, type = "l", col = "black", ylim = c(min(0), max(92)))
>
> lines(irrig, type = "l", col = "cornflowerblue", ylim = c(min(0),
> max(15))) lines(rech, type = "l", col = "brown", ylim = c(min(0), max(15)))
>
> temp<-irrig<1.000001 #logical command to identify low values into a
> temporary vector temp
>
> temp2<-as.numeric(irrig>1.000001) #logical command to identify high values
> with 1
>
> temp2
>
> temp3<-as.numeric(rech>1.000001)
>
> temp3
>
> irrig2<-irrig*temp2  #remove values inf 1 mm inirrig
>
> rech2<-rech*temp3    #same for rech
>
> plot(irrig2, type = "l", col = "cornflowerblue", ylim = c(min(0),
> max(15))) lines(rech2, type = "l", col = "brown", ylim = c(min(0), max(15)))
>
> Many thanks for your time and intellectual generosity
>
> Cheers
>
> Makram
>
>
>
> On Mon, Dec 28, 2015 at 12:15 PM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
> Hi
>
> On top of answers you have got here is some plotting you need to answer
> yourself
>
> plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)
>
> Which from those red points you want to be included in irrigation period?
> All of them? Only part? Which part?
>
> Based on your figures you probably will not get 100% correct answer.
>
> Cheers
> Petr
>
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Makram
> > Belhaj Fraj
> > Sent: Wednesday, December 23, 2015 8:35 AM
> > To: r-help at r-project.org; r-help-owner at r-project.org
> > Subject: [R] need for help for solving operations in a vector
> >
>
> >  Dear colleagues
> > i need your generous help to solve the following problem
> >
> > I have a  soil moisture time series qWC1 (61 values)
> > > qWC1
> >  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> > 74.70059
> > 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> > 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> > 80.69793
> > 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> > 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> > 75.97412
> > 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> > 75.46087
> > 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> > 75.07930 75.07930 74.95275 74.95275  74.95275
> >
> > I want to measure consecutive increases corresponding to irrigation and
> > consecutive decreases  corresponding to recharge I wrote the following
> > code and it does not calculate for each increment in i?
> > also note that I choose to not use diff command in time series because
> > I  want also that "plateaux" corresponding to a minimum of 2 equal
> > consecutive values are accounted as positive differences=irrigations so
> > when x[i+1]==x[i] the difference y might be equal to the previous value
> > xi
> >
> > following the code i wrote
> >
> > x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> > "green")
> > y<-rep(0,61)
> > for (i in 1:61) {
> > if (x[i+1] > x[i]){
> >     y[i]==x[i+1]-x[i]
> > } else if (x[i+1]==x[i]){
> >     y[i]=x[i+2]-x[i]
> > } else {
> >     y[i]==x[i+1]-x[i]
> > }
> >
> > }
> > plot(y, type="h", col = "blueviolet")
> >
> > Many thank
> > Makram
> >
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

From belhajfraj.makram at gmail.com  Tue Dec 29 12:33:25 2015
From: belhajfraj.makram at gmail.com (Makram Belhaj Fraj)
Date: Tue, 29 Dec 2015 15:33:25 +0400
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOdkLULWT3VMk1buPvMqhOkWcjVqujG+NLRZZPFG5W_6Jw@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfH-ZB8EpjV57kaRxtTyoG_Ek5tCzKx+T=G+2zKwokTiA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C50080F1@SRVEXCHMBX.precheza.cz>
	<CAP9hYOdkLULWT3VMk1buPvMqhOkWcjVqujG+NLRZZPFG5W_6Jw@mail.gmail.com>
Message-ID: <CAP9hYOfRaRjLRDR=vCn12fZ=nC1sngnUFiuncNawhQAs3=XqZw@mail.gmail.com>

Hi Petr

I runned the code you gave me as following, and I am adjusting for the
threshold as suggested,

sss<-smooth.spline(qWC1, tint, nknots=length(tint)/2)

peak <- which.max(predict(sss)$y)   #qWC1=temp$theta mtime=temp$int

baseline <- min(predict(sss)$y)

vyska <- predict(sss)$y[peak]

# 50% threshold

HM <- (vyska+baseline)/2

plot(tint, qWC1,col=(tint>HM)+1, pch=19) #10% threshold

HM<-vyska-(vyska*.1)

plot(tint,qWC1, col=(tint>HM)+1, pch=19)

cheers

Makram

On Tue, Dec 29, 2015 at 3:26 PM, Makram Belhaj Fraj <
belhajfraj.makram at gmail.com> wrote:

> Hi Petr,
> I apologize It was my first time using r-help so I didn't know how to
> replay to email to all or not,
> I am replaying to all for this email,
>
> Many thanks for the code, I am trying to use it,
> please find attached the data file in csv,
>
> following are the first lines of code to read the data and calculate qWC1
>
> sdata<-read.csv("almondc_10augv.csv",head=TRUE,sep=",")
>
> tint=sdata$scan #time intervall
>
> mtime=sdata$mtime #measurement time
>
> v1=sdata$vwc1 #value of moisture in percent
>
> qWC1=200*v1 #conversion in mm to get the variable I am working on
>
>
>
> On Tue, Dec 29, 2015 at 11:32 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
>> Hi
>>
>>
>>
>> You shall send your posts to the list, others can answer your questions
>> and not only you can benefit from their answers too.
>>
>>
>>
>> As you did not post any data it is hard to say what are your issues. I
>> believe that there are several values which are the same not only near the
>> peak but also at the bottom part. If your data look like I remember and you
>> want to keep all values near the peak value regardless they are slightly
>> growing or falling, one approach can be to identify peak value, and select
>> all values near the peak (the threshold is up to you).
>>
>>
>>
>> something like that
>>
>>
>>
>> sss<-smooth.spline(temp$theta, temp$int, nknots=length(temp$int)/2)
>>
>> peak <- which.max(predict(sss)$y)
>>
>> baseline <- min(predict(sss)$y)
>>
>> vyska <- predict(sss)$y[peak]
>>
>> # 50% threshold
>>
>> HM <- (vyska+baseline)/2
>>
>> plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
>>
>> #10% threshold
>>
>> HM<-vyska-(vyska*.1)
>>
>> plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
>>
>>
>>
>> > dput(temp)
>>
>> structure(list(theta = c(28.995, 29.005, 29.015, 29.025, 29.035,
>>
>> 29.045, 29.055, 29.065, 29.075, 29.085, 29.095, 29.105, 29.115,
>>
>> 29.125, 29.135, 29.145, 29.155, 29.165, 29.175, 29.185, 29.195,
>>
>> 29.205, 29.215, 29.225, 29.235, 29.245, 29.255, 29.265, 29.275,
>>
>> 29.285, 29.295, 29.305, 29.315, 29.325, 29.335, 29.345, 29.355,
>>
>> 29.365, 29.375, 29.385, 29.395, 29.405, 29.415, 29.425, 29.435,
>>
>> 29.445, 29.455, 29.465, 29.475, 29.485, 29.495, 29.505, 29.515,
>>
>> 29.525, 29.535, 29.545, 29.555, 29.565, 29.575, 29.585, 29.595,
>>
>> 29.605, 29.615, 29.625, 29.635, 29.645, 29.655, 29.665, 29.675,
>>
>> 29.685, 29.695, 29.705, 29.715, 29.725, 29.735, 29.745, 29.755,
>>
>> 29.765, 29.775, 29.785, 29.795, 29.805, 29.815, 29.825, 29.835,
>>
>> 29.845, 29.855, 29.865, 29.875, 29.885, 29.895, 29.905, 29.915,
>>
>> 29.925, 29.935, 29.945, 29.955, 29.965, 29.975, 29.985, 29.995
>>
>> ), int = c(329L, 330L, 318L, 287L, 315L, 344L, 333L, 324L, 334L,
>>
>> 366L, 339L, 374L, 375L, 335L, 415L, 371L, 413L, 382L, 408L, 406L,
>>
>> 407L, 440L, 475L, 465L, 516L, 510L, 490L, 550L, 663L, 647L, 628L,
>>
>> 721L, 789L, 814L, 890L, 923L, 1085L, 1102L, 1222L, 1356L, 1521L,
>>
>> 1729L, 1868L, 2120L, 2491L, 2656L, 3196L, 3599L, 4128L, 4536L,
>>
>> 5043L, 5310L, 5638L, 5792L, 5699L, 5374L, 4886L, 4473L, 4293L,
>>
>> 3757L, 3319L, 2934L, 2422L, 1998L, 1753L, 1397L, 1163L, 972L,
>>
>> 854L, 775L, 648L, 695L, 616L, 553L, 554L, 509L, 530L, 483L, 482L,
>>
>> 406L, 451L, 422L, 403L, 393L, 396L, 348L, 367L, 428L, 345L, 384L,
>>
>> 330L, 342L, 312L, 313L, 323L, 328L, 340L, 322L, 330L, 305L, 311L
>>
>> )), .Names = c("theta", "int"), row.names = 100:200, class = "data.frame")
>>
>> >
>>
>>
>>
>> Cheers
>>
>> Petr
>>
>>
>>
>>
>>
>> *From:* Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
>> *Sent:* Tuesday, December 29, 2015 5:54 AM
>> *To:* PIKAL Petr
>> *Subject:* Re: [R] need for help for solving operations in a vector
>>
>>
>>
>> Hi Petr,
>>
>> I would like to thank you for your time
>>
>> i choose the following modification on the plotting so to keep only
>> successive equals in red
>>
>> plot(qWC1, col=(c(0, diff(qWC1))==0 )+1)
>>
>> then, the red points I want to include in the irrigation period are the 3
>> successive red points in the plateau (equal values close to the max)
>>
>> These points are very important as they are corresponding to saturation,
>> we continue to irrigate with the same flow and values remains constant for
>> a certain time, so I must add them to irrigation
>>
>>
>>
>> up to now I didn't succeed in adding this plateau values to
>>  theirrigations using the diff(qWC1, lag=1)
>>
>> however, I wrote also some loops trying to catch all the irrigation and
>> recharge separately and I still have some issues,
>>
>> following are the loops I used, with comments corresponding to the issues
>>
>> x<-qWC1
>>
>> length(x)
>>
>> irrig<-rep(1,61)
>>
>> for (i in 2:61) {
>>
>> if (x[i-1]<x[i]){
>>
>>     irrig[i]<-x[i]-x[i-1]
>>
>> }
>>
>> }
>>
>> rech<-rep(1,61)
>>
>> for (i in 2:61) {
>>
>> if (x[i-1]>x[i]){
>>
>>     rech[i]<-x[i-1]-x[i]
>>
>> }
>>
>> }
>>
>> plot(x, type = "l", col = "black", ylim = c(min(0), max(92)))
>>
>> lines(irrig, type = "l", col = "cornflowerblue", ylim = c(min(0),
>> max(15))) lines(rech, type = "l", col = "brown", ylim = c(min(0), max(15)))
>>
>> temp<-irrig<1.000001 #logical command to identify low values into a
>> temporary vector temp
>>
>> temp2<-as.numeric(irrig>1.000001) #logical command to identify high
>> values with 1
>>
>> temp2
>>
>> temp3<-as.numeric(rech>1.000001)
>>
>> temp3
>>
>> irrig2<-irrig*temp2  #remove values inf 1 mm inirrig
>>
>> rech2<-rech*temp3    #same for rech
>>
>> plot(irrig2, type = "l", col = "cornflowerblue", ylim = c(min(0),
>> max(15))) lines(rech2, type = "l", col = "brown", ylim = c(min(0), max(15)))
>>
>> Many thanks for your time and intellectual generosity
>>
>> Cheers
>>
>> Makram
>>
>>
>>
>> On Mon, Dec 28, 2015 at 12:15 PM, PIKAL Petr <petr.pikal at precheza.cz>
>> wrote:
>>
>> Hi
>>
>> On top of answers you have got here is some plotting you need to answer
>> yourself
>>
>> plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)
>>
>> Which from those red points you want to be included in irrigation period?
>> All of them? Only part? Which part?
>>
>> Based on your figures you probably will not get 100% correct answer.
>>
>> Cheers
>> Petr
>>
>>
>>
>> > -----Original Message-----
>> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Makram
>> > Belhaj Fraj
>> > Sent: Wednesday, December 23, 2015 8:35 AM
>> > To: r-help at r-project.org; r-help-owner at r-project.org
>> > Subject: [R] need for help for solving operations in a vector
>> >
>>
>> >  Dear colleagues
>> > i need your generous help to solve the following problem
>> >
>> > I have a  soil moisture time series qWC1 (61 values)
>> > > qWC1
>> >  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
>> > 74.70059
>> > 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
>> > 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
>> > 80.69793
>> > 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
>> > 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
>> > 75.97412
>> > 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
>> > 75.46087
>> > 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
>> > 75.07930 75.07930 74.95275 74.95275  74.95275
>> >
>> > I want to measure consecutive increases corresponding to irrigation and
>> > consecutive decreases  corresponding to recharge I wrote the following
>> > code and it does not calculate for each increment in i?
>> > also note that I choose to not use diff command in time series because
>> > I  want also that "plateaux" corresponding to a minimum of 2 equal
>> > consecutive values are accounted as positive differences=irrigations so
>> > when x[i+1]==x[i] the difference y might be equal to the previous value
>> > xi
>> >
>> > following the code i wrote
>> >
>> > x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
>> > "green")
>> > y<-rep(0,61)
>> > for (i in 1:61) {
>> > if (x[i+1] > x[i]){
>> >     y[i]==x[i+1]-x[i]
>> > } else if (x[i+1]==x[i]){
>> >     y[i]=x[i+2]-x[i]
>> > } else {
>> >     y[i]==x[i+1]-x[i]
>> > }
>> >
>> > }
>> > plot(y, type="h", col = "blueviolet")
>> >
>> > Many thank
>> > Makram
>> >
>>
>>
>>
>> ------------------------------
>> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
>> ur?eny pouze jeho adres?t?m.
>> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
>> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
>> vyma?te ze sv?ho syst?mu.
>> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento
>> email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
>> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
>> ?i zpo?d?n?m p?enosu e-mailu.
>>
>> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
>> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
>> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
>> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn?
>> p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze
>> strany p??jemce s dodatkem ?i odchylkou.
>> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
>> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
>> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
>> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
>> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
>> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
>> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>>
>> This e-mail and any documents attached to it may be confidential and are
>> intended only for its intended recipients.
>> If you received this e-mail by mistake, please immediately inform its
>> sender. Delete the contents of this e-mail with all attachments and its
>> copies from your system.
>> If you are not the intended recipient of this e-mail, you are not
>> authorized to use, disseminate, copy or disclose this e-mail in any manner.
>> The sender of this e-mail shall not be liable for any possible damage
>> caused by modifications of the e-mail or by delay with transfer of the
>> email.
>>
>> In case that this e-mail forms part of business dealings:
>> - the sender reserves the right to end negotiations about entering into a
>> contract in any time, for any reason, and without stating any reasoning.
>> - if the e-mail contains an offer, the recipient is entitled to
>> immediately accept such offer; The sender of this e-mail (offer) excludes
>> any acceptance of the offer on the part of the recipient containing any
>> amendment or variation.
>> - the sender insists on that the respective contract is concluded only
>> upon an express mutual agreement on all its aspects.
>> - the sender of this e-mail informs that he/she is not authorized to
>> enter into any contracts on behalf of the company except for cases in which
>> he/she is expressly authorized to do so in writing, and such authorization
>> or power of attorney is submitted to the recipient or the person
>> represented by the recipient, or the existence of such authorization is
>> known to the recipient of the person represented by the recipient.
>>
>
>

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Tue Dec 29 13:11:11 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Tue, 29 Dec 2015 12:11:11 +0000
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOfRaRjLRDR=vCn12fZ=nC1sngnUFiuncNawhQAs3=XqZw@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfH-ZB8EpjV57kaRxtTyoG_Ek5tCzKx+T=G+2zKwokTiA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C50080F1@SRVEXCHMBX.precheza.cz>
	<CAP9hYOdkLULWT3VMk1buPvMqhOkWcjVqujG+NLRZZPFG5W_6Jw@mail.gmail.com>
	<CAP9hYOfRaRjLRDR=vCn12fZ=nC1sngnUFiuncNawhQAs3=XqZw@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5008194@SRVEXCHMBX.precheza.cz>

Hi

First you shall adjust your mail client to send post in plain text not HTML, as suggested by Posting guide. It shall be available in gmail too.

Second you do not run my code:-(
You run **your** code which is (slightly) **similar** to the code I sent you however it is completely wrong. If you do not understand what each function is doing you shall consult its help page. ?smooth.spline, ?predict, ?which.max, ...

With your data

plot(tint, qWC1)
sss<-smooth.spline(tint,qWC1)
lines(predict(sss))
peak <- which.max(predict(sss)$y)
abline(v=tint[peak], col=2)
baseline <- min(predict(sss)$y)
vyska <- predict(sss)$y[peak]
HM <- (vyska+baseline)/2
abline(h=HM)

plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)

HM<-vyska-(vyska*.1)
plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)

HM<-vyska-(vyska*.05)
plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)

HM<-vyska-(vyska*.02)
plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)

You can see which values are considered as belonging to the peak when you are changing the threshold.

However this simple approach works only if you have only one peak in your data.

Cheers
Petr



From: Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
Sent: Tuesday, December 29, 2015 12:33 PM
To: PIKAL Petr
Cc: r-help at r-project.org
Subject: Re: [R] need for help for solving operations in a vector

Hi Petr
I runned the code you gave me as following, and I am adjusting for the threshold as suggested,
sss<-smooth.spline(qWC1, tint, nknots=length(tint)/2)
peak <- which.max(predict(sss)$y)   #qWC1=temp$theta mtime=temp$int
baseline <- min(predict(sss)$y)
vyska <- predict(sss)$y[peak]
# 50% threshold
HM <- (vyska+baseline)/2
plot(tint, qWC1,col=(tint>HM)+1, pch=19) #10% threshold
HM<-vyska-(vyska*.1)
plot(tint,qWC1, col=(tint>HM)+1, pch=19)
cheers
Makram

On Tue, Dec 29, 2015 at 3:26 PM, Makram Belhaj Fraj <belhajfraj.makram at gmail.com<mailto:belhajfraj.makram at gmail.com>> wrote:
Hi Petr,
I apologize It was my first time using r-help so I didn't know how to replay to email to all or not,
I am replaying to all for this email,

Many thanks for the code, I am trying to use it,
please find attached the data file in csv,

following are the first lines of code to read the data and calculate qWC1

sdata<-read.csv("almondc_10augv.csv",head=TRUE,sep=",")

tint=sdata$scan #time intervall

mtime=sdata$mtime #measurement time

v1=sdata$vwc1 #value of moisture in percent

qWC1=200*v1 #conversion in mm to get the variable I am working on



On Tue, Dec 29, 2015 at 11:32 AM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

You shall send your posts to the list, others can answer your questions and not only you can benefit from their answers too.

As you did not post any data it is hard to say what are your issues. I believe that there are several values which are the same not only near the peak but also at the bottom part. If your data look like I remember and you want to keep all values near the peak value regardless they are slightly growing or falling, one approach can be to identify peak value, and select all values near the peak (the threshold is up to you).

something like that

sss<-smooth.spline(temp$theta, temp$int, nknots=length(temp$int)/2)
peak <- which.max(predict(sss)$y)
baseline <- min(predict(sss)$y)
vyska <- predict(sss)$y[peak]
# 50% threshold
HM <- (vyska+baseline)/2
plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
#10% threshold
HM<-vyska-(vyska*.1)
plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)

> dput(temp)
structure(list(theta = c(28.995, 29.005, 29.015, 29.025, 29.035,
29.045, 29.055, 29.065, 29.075, 29.085, 29.095, 29.105, 29.115,
29.125, 29.135, 29.145, 29.155, 29.165, 29.175, 29.185, 29.195,
29.205, 29.215, 29.225, 29.235, 29.245, 29.255, 29.265, 29.275,
29.285, 29.295, 29.305, 29.315, 29.325, 29.335, 29.345, 29.355,
29.365, 29.375, 29.385, 29.395, 29.405, 29.415, 29.425, 29.435,
29.445, 29.455, 29.465, 29.475, 29.485, 29.495, 29.505, 29.515,
29.525, 29.535, 29.545, 29.555, 29.565, 29.575, 29.585, 29.595,
29.605, 29.615, 29.625, 29.635, 29.645, 29.655, 29.665, 29.675,
29.685, 29.695, 29.705, 29.715, 29.725, 29.735, 29.745, 29.755,
29.765, 29.775, 29.785, 29.795, 29.805, 29.815, 29.825, 29.835,
29.845, 29.855, 29.865, 29.875, 29.885, 29.895, 29.905, 29.915,
29.925, 29.935, 29.945, 29.955, 29.965, 29.975, 29.985, 29.995
), int = c(329L, 330L, 318L, 287L, 315L, 344L, 333L, 324L, 334L,
366L, 339L, 374L, 375L, 335L, 415L, 371L, 413L, 382L, 408L, 406L,
407L, 440L, 475L, 465L, 516L, 510L, 490L, 550L, 663L, 647L, 628L,
721L, 789L, 814L, 890L, 923L, 1085L, 1102L, 1222L, 1356L, 1521L,
1729L, 1868L, 2120L, 2491L, 2656L, 3196L, 3599L, 4128L, 4536L,
5043L, 5310L, 5638L, 5792L, 5699L, 5374L, 4886L, 4473L, 4293L,
3757L, 3319L, 2934L, 2422L, 1998L, 1753L, 1397L, 1163L, 972L,
854L, 775L, 648L, 695L, 616L, 553L, 554L, 509L, 530L, 483L, 482L,
406L, 451L, 422L, 403L, 393L, 396L, 348L, 367L, 428L, 345L, 384L,
330L, 342L, 312L, 313L, 323L, 328L, 340L, 322L, 330L, 305L, 311L
)), .Names = c("theta", "int"), row.names = 100:200, class = "data.frame")
>

Cheers
Petr


From: Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com<mailto:belhajfraj.makram at gmail.com>]
Sent: Tuesday, December 29, 2015 5:54 AM
To: PIKAL Petr
Subject: Re: [R] need for help for solving operations in a vector

Hi Petr,
I would like to thank you for your time
i choose the following modification on the plotting so to keep only successive equals in red
plot(qWC1, col=(c(0, diff(qWC1))==0 )+1)
then, the red points I want to include in the irrigation period are the 3 successive red points in the plateau (equal values close to the max)
These points are very important as they are corresponding to saturation, we continue to irrigate with the same flow and values remains constant for a certain time, so I must add them to irrigation

up to now I didn't succeed in adding this plateau values to  theirrigations using the diff(qWC1, lag=1)
however, I wrote also some loops trying to catch all the irrigation and recharge separately and I still have some issues,
following are the loops I used, with comments corresponding to the issues
x<-qWC1
length(x)
irrig<-rep(1,61)
for (i in 2:61) {
if (x[i-1]<x[i]){
    irrig[i]<-x[i]-x[i-1]
}
}
rech<-rep(1,61)
for (i in 2:61) {
if (x[i-1]>x[i]){
    rech[i]<-x[i-1]-x[i]
}
}
plot(x, type = "l", col = "black", ylim = c(min(0), max(92)))
lines(irrig, type = "l", col = "cornflowerblue", ylim = c(min(0), max(15))) lines(rech, type = "l", col = "brown", ylim = c(min(0), max(15)))
temp<-irrig<1.000001 #logical command to identify low values into a temporary vector temp
temp2<-as.numeric(irrig>1.000001) #logical command to identify high values with 1
temp2
temp3<-as.numeric(rech>1.000001)
temp3
irrig2<-irrig*temp2  #remove values inf 1 mm inirrig
rech2<-rech*temp3    #same for rech
plot(irrig2, type = "l", col = "cornflowerblue", ylim = c(min(0), max(15))) lines(rech2, type = "l", col = "brown", ylim = c(min(0), max(15)))
Many thanks for your time and intellectual generosity
Cheers
Makram

On Mon, Dec 28, 2015 at 12:15 PM, PIKAL Petr <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> wrote:
Hi

On top of answers you have got here is some plotting you need to answer yourself

plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)

Which from those red points you want to be included in irrigation period? All of them? Only part? Which part?

Based on your figures you probably will not get 100% correct answer.

Cheers
Petr



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Makram
> Belhaj Fraj
> Sent: Wednesday, December 23, 2015 8:35 AM
> To: r-help at r-project.org<mailto:r-help at r-project.org>; r-help-owner at r-project.org<mailto:r-help-owner at r-project.org>
> Subject: [R] need for help for solving operations in a vector
>
>  Dear colleagues
> i need your generous help to solve the following problem
>
> I have a  soil moisture time series qWC1 (61 values)
> > qWC1
>  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> 74.70059
> 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> 80.69793
> 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> 75.97412
> 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> 75.46087
> 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> 75.07930 75.07930 74.95275 74.95275  74.95275
>
> I want to measure consecutive increases corresponding to irrigation and
> consecutive decreases  corresponding to recharge I wrote the following
> code and it does not calculate for each increment in i?
> also note that I choose to not use diff command in time series because
> I  want also that "plateaux" corresponding to a minimum of 2 equal
> consecutive values are accounted as positive differences=irrigations so
> when x[i+1]==x[i] the difference y might be equal to the previous value
> xi
>
> following the code i wrote
>
> x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> "green")
> y<-rep(0,61)
> for (i in 1:61) {
> if (x[i+1] > x[i]){
>     y[i]==x[i+1]-x[i]
> } else if (x[i+1]==x[i]){
>     y[i]=x[i+2]-x[i]
> } else {
>     y[i]==x[i+1]-x[i]
> }
>
> }
> plot(y, type="h", col = "blueviolet")
>
> Many thank
> Makram
>



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From axel.urbiz at gmail.com  Tue Dec 29 15:09:39 2015
From: axel.urbiz at gmail.com (Axel Urbiz)
Date: Tue, 29 Dec 2015 09:09:39 -0500
Subject: [R] Extract Standard Errors of Model Coefficients
Message-ID: <A36C7746-6CFC-47FB-B3D0-814351727513@gmail.com>

Hello, 

Is it possible to extract or compute the standard errors of model coefficients from a glm.fit object? This can be easily done from a fitted glm object, but I need glm.fit.


set.seed(1)
n <- 100
x <- rnorm(n)
y1 <- rnorm(n)
y2 <- rbinom(n, 1, .25)

M1 <- glm (y1 ~ x)
M2 <- glm.fit(x = x, y = y1)
seCoef <- sqrt(diag(vcov(M1)))
seCoef 

(Intercept)           x 
 0.09698729  0.10772703 

Thank you,
Axel.

From jfox at mcmaster.ca  Tue Dec 29 15:35:48 2015
From: jfox at mcmaster.ca (Fox, John)
Date: Tue, 29 Dec 2015 14:35:48 +0000
Subject: [R] Extract Standard Errors of Model Coefficients
In-Reply-To: <A36C7746-6CFC-47FB-B3D0-814351727513@gmail.com>
References: <A36C7746-6CFC-47FB-B3D0-814351727513@gmail.com>
Message-ID: <ACD1644AA6C67E4FBD0C350625508EC810F47EDE@FHSDB2D11-2.csu.mcmaster.ca>

Dear Axel,

If you look at the content of the list returned by glm.fit, you'll see that it contains almost everything in a "glm" object, and what's needed to compute the coefficient covariance matrix. Here's one way to do what you want (but note that your example was faulty in that you didn't include the regression constant in the call to glm.fit):

> set.seed(1)
> n <- 100
> x <- rnorm(n)
> y1 <- rnorm(n)
> y2 <- rbinom(n, 1, .25) # you never use this in your example
> 
> M1 <- glm (y1 ~ x)
> M2 <- glm.fit(x = cbind(1, x), y = y1) # corrected
> class(M2) <- "glm"
> vcov(M1)
             (Intercept)           x
(Intercept)  0.009406535 -0.00126365
x           -0.001263650  0.01160511
> vcov(M2)
                         x
   0.009406535 -0.00126365
x -0.001263650  0.01160511

You may have a reason to use glm.fit in preference to glm, but I'm not sure why you'd want to do that.

I hope this helps,
 John

-----------------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
http://socserv.socsci.mcmaster.ca/jfox/



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Axel
> Urbiz
> Sent: Tuesday, December 29, 2015 9:10 AM
> To: R-help at r-project.org
> Subject: [R] Extract Standard Errors of Model Coefficients
> 
> Hello,
> 
> Is it possible to extract or compute the standard errors of model
> coefficients from a glm.fit object? This can be easily done from a
> fitted glm object, but I need glm.fit.
> 
> 
> set.seed(1)
> n <- 100
> x <- rnorm(n)
> y1 <- rnorm(n)
> y2 <- rbinom(n, 1, .25)
> 
> M1 <- glm (y1 ~ x)
> M2 <- glm.fit(x = x, y = y1)
> seCoef <- sqrt(diag(vcov(M1)))
> seCoef
> 
> (Intercept)           x
>  0.09698729  0.10772703
> 
> Thank you,
> Axel.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.


From frankieboytje at hotmail.com  Tue Dec 29 14:33:33 2015
From: frankieboytje at hotmail.com (Frank van Berkum)
Date: Tue, 29 Dec 2015 13:33:33 +0000
Subject: [R] GLM: include observations with missing explanatory variables
Message-ID: <DUB109-W861A9EB7E461DC81D5093EBCFC0@phx.gbl>

Hi all,
My problem is the following. 
Suppose I have a dataset with observations Y and explanatory variables X1, ..., Xn, and suppose one of these explanatory variables is geographical area (of which there are ten, j=1,...,10).  For some observations I know the area, but for others it is unknown and therefore record as NA.
I want to estimate a model of the form Y[i] ~ Poisson( lambda[i] ) with log(lambda[i]) = constant + \sum_j I[!is.na(area[i])] * I[area[i]==j] * beta[j]
In words: we estimate a constant for all observations and a factor for each area. If it is unknown what the area is, we only include the constant. 
When estimating this model using glm(), the records with is.na(area[i]) are 'deleted' from the dataset, and this I do not want. I had hoped that the model as described above could be estimated using the function I() (interpret as), but so far my attempts have not succeeded. 
Any help on how to approach this is kindly appreciated.
Kind regards,
Frank van Berkum 		 	   		  
	[[alternative HTML version deleted]]


From mkashif at uaf.edu.pk  Tue Dec 29 10:32:05 2015
From: mkashif at uaf.edu.pk (Muhammad  Kashif)
Date: Tue, 29 Dec 2015 09:32:05 +0000
Subject: [R] The Birnbaum-Saunders Distribution simmulation Problem
Message-ID: <AMSPR07MB2124843FC7FAC352A6174D994FC0@AMSPR07MB212.eurprd07.prod.outlook.com>

Dear i optimized the gama and beta value using simmulation. if i run this code it generate very small value of beta. Could any one help me in this regard. i use gbs package to generate data.


gama=1.0
beta=1.3
n=25
iterCount=1000
for(i in 1:iterCount){
  x<-rgbs(n,gama,beta)
  P<-function(theta,x){
    n<-length(x)
    gama<-theta[1]
    beta<-theta[2]
    -n*log(5.013)+ n*(theta[1])^-2-n*log(theta[1])-n/2*log(theta[2])+ sum(log(theta[2]+x)-0.5/theta[1]^2*(x/theta[2]+theta[2]/x))}}
  P.out<-optim(theta<-c(gama,beta),ll.wd,x=x,method = "Nelder-Mead",hessian=FALSE)
  gamhat<-P.out$par[1]
  betahat<-P.out$par[2]


	[[alternative HTML version deleted]]


From liiz_1226 at hotmail.com  Tue Dec 29 18:29:04 2015
From: liiz_1226 at hotmail.com (Liliana Zazueta)
Date: Tue, 29 Dec 2015 17:29:04 -0000
Subject: [R]  Rgraphviz overlap of nodes and edges
Message-ID: <BLU403-EAS2346DA0111F5EA9F6CDEC8181FC0@phx.gbl>

Excuse me I have this dude, how to make that Rgraphviz graphic without
intersection between nodes and edges?

 

If answer me I will be eternally gratefull! 


	[[alternative HTML version deleted]]


From axel.urbiz at gmail.com  Tue Dec 29 20:48:53 2015
From: axel.urbiz at gmail.com (Axel Urbiz)
Date: Tue, 29 Dec 2015 14:48:53 -0500
Subject: [R] Extract Standard Errors of Model Coefficients
In-Reply-To: <ACD1644AA6C67E4FBD0C350625508EC810F47EDE@FHSDB2D11-2.csu.mcmaster.ca>
References: <A36C7746-6CFC-47FB-B3D0-814351727513@gmail.com>
	<ACD1644AA6C67E4FBD0C350625508EC810F47EDE@FHSDB2D11-2.csu.mcmaster.ca>
Message-ID: <161E18BA-CFD0-48CA-AE2C-889D7F84268C@gmail.com>

Thanks a lot John. Forgot I could arbitrarily change the class of objects, which against all critics, can be very helpful at times. 

Best,
Axel.




> On Dec 29, 2015, at 9:35 AM, Fox, John <jfox at mcmaster.ca> wrote:
> 
> Dear Axel,
> 
> If you look at the content of the list returned by glm.fit, you'll see that it contains almost everything in a "glm" object, and what's needed to compute the coefficient covariance matrix. Here's one way to do what you want (but note that your example was faulty in that you didn't include the regression constant in the call to glm.fit):
> 
>> set.seed(1)
>> n <- 100
>> x <- rnorm(n)
>> y1 <- rnorm(n)
>> y2 <- rbinom(n, 1, .25) # you never use this in your example
>> 
>> M1 <- glm (y1 ~ x)
>> M2 <- glm.fit(x = cbind(1, x), y = y1) # corrected
>> class(M2) <- "glm"
>> vcov(M1)
>             (Intercept)           x
> (Intercept)  0.009406535 -0.00126365
> x           -0.001263650  0.01160511
>> vcov(M2)
>                         x
>   0.009406535 -0.00126365
> x -0.001263650  0.01160511
> 
> You may have a reason to use glm.fit in preference to glm, but I'm not sure why you'd want to do that.
> 
> I hope this helps,
> John
> 
> -----------------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> http://socserv.socsci.mcmaster.ca/jfox/
> 
> 
> 
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Axel
>> Urbiz
>> Sent: Tuesday, December 29, 2015 9:10 AM
>> To: R-help at r-project.org
>> Subject: [R] Extract Standard Errors of Model Coefficients
>> 
>> Hello,
>> 
>> Is it possible to extract or compute the standard errors of model
>> coefficients from a glm.fit object? This can be easily done from a
>> fitted glm object, but I need glm.fit.
>> 
>> 
>> set.seed(1)
>> n <- 100
>> x <- rnorm(n)
>> y1 <- rnorm(n)
>> y2 <- rbinom(n, 1, .25)
>> 
>> M1 <- glm (y1 ~ x)
>> M2 <- glm.fit(x = x, y = y1)
>> seCoef <- sqrt(diag(vcov(M1)))
>> seCoef
>> 
>> (Intercept)           x
>> 0.09698729  0.10772703
>> 
>> Thank you,
>> Axel.
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.


From belhajfraj.makram at gmail.com  Tue Dec 29 20:56:47 2015
From: belhajfraj.makram at gmail.com (Makram Belhaj Fraj)
Date: Tue, 29 Dec 2015 23:56:47 +0400
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5008194@SRVEXCHMBX.precheza.cz>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfH-ZB8EpjV57kaRxtTyoG_Ek5tCzKx+T=G+2zKwokTiA@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C50080F1@SRVEXCHMBX.precheza.cz>
	<CAP9hYOdkLULWT3VMk1buPvMqhOkWcjVqujG+NLRZZPFG5W_6Jw@mail.gmail.com>
	<CAP9hYOfRaRjLRDR=vCn12fZ=nC1sngnUFiuncNawhQAs3=XqZw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5008194@SRVEXCHMBX.precheza.cz>
Message-ID: <CAP9hYOeYg-r-G-3rYg_UVSrZVVBH-20SosJR3z0X8fVEZfn7CA@mail.gmail.com>

Hi Petr
I am adjusting my email and sorry for th? inconvenience
I read about these commands. The method is good for my case with threshold
of .02.

I need just to have a vector with the values of the r?d points in r?d
equal to 1-do you have an idea how to do it?-
so I could calculate total irrigation as follows
Indeed the difference to next lower value to the plateaux is a constant
=hydraulic conductivity at saturation for th? upper soil layer. So all what
I have to do is to multiply the 1 values by 6.123 mm.
Many thanks for all
Cheers
Makram
Le 29 d?c. 2015 16:11, "PIKAL Petr" <petr.pikal at precheza.cz> a ?crit :

> Hi
>
>
>
> First you shall adjust your mail client to send post in plain text not
> HTML, as suggested by Posting guide. It shall be available in gmail too.
>
>
>
> Second you do not run my code:-(
>
> You run **your** code which is (slightly) **similar** to the code I sent
> you however it is completely wrong. If you do not understand what each
> function is doing you shall consult its help page. ?smooth.spline,
> ?predict, ?which.max, ...
>
>
>
> With your data
>
>
>
> plot(tint, qWC1)
>
> sss<-smooth.spline(tint,qWC1)
>
> lines(predict(sss))
>
> peak <- which.max(predict(sss)$y)
>
> abline(v=tint[peak], col=2)
>
> baseline <- min(predict(sss)$y)
>
> vyska <- predict(sss)$y[peak]
>
> HM <- (vyska+baseline)/2
>
> abline(h=HM)
>
>
>
> plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)
>
>
>
> HM<-vyska-(vyska*.1)
>
> plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)
>
>
>
> HM<-vyska-(vyska*.05)
>
> plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)
>
>
>
> HM<-vyska-(vyska*.02)
>
> plot(tint, qWC1,col=(qWC1>HM)+1, pch=19)
>
>
>
> You can see which values are considered as belonging to the peak when you
> are changing the threshold.
>
>
>
> However this simple approach works only if you have only one peak in your
> data.
>
>
>
> Cheers
>
> Petr
>
>
>
>
>
>
>
> *From:* Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
> *Sent:* Tuesday, December 29, 2015 12:33 PM
> *To:* PIKAL Petr
> *Cc:* r-help at r-project.org
> *Subject:* Re: [R] need for help for solving operations in a vector
>
>
>
> Hi Petr
>
> I runned the code you gave me as following, and I am adjusting for the
> threshold as suggested,
>
> sss<-smooth.spline(qWC1, tint, nknots=length(tint)/2)
>
> peak <- which.max(predict(sss)$y)   #qWC1=temp$theta mtime=temp$int
>
> baseline <- min(predict(sss)$y)
>
> vyska <- predict(sss)$y[peak]
>
> # 50% threshold
>
> HM <- (vyska+baseline)/2
>
> plot(tint, qWC1,col=(tint>HM)+1, pch=19) #10% threshold
>
> HM<-vyska-(vyska*.1)
>
> plot(tint,qWC1, col=(tint>HM)+1, pch=19)
>
> cheers
>
> Makram
>
>
>
> On Tue, Dec 29, 2015 at 3:26 PM, Makram Belhaj Fraj <
> belhajfraj.makram at gmail.com> wrote:
>
> Hi Petr,
>
> I apologize It was my first time using r-help so I didn't know how to
> replay to email to all or not,
>
> I am replaying to all for this email,
>
>
>
> Many thanks for the code, I am trying to use it,
>
> please find attached the data file in csv,
>
>
>
> following are the first lines of code to read the data and calculate qWC1
>
> sdata<-read.csv("almondc_10augv.csv",head=TRUE,sep=",")
>
> tint=sdata$scan #time intervall
>
> mtime=sdata$mtime #measurement time
>
> v1=sdata$vwc1 #value of moisture in percent
>
> qWC1=200*v1 #conversion in mm to get the variable I am working on
>
>
>
>
>
> On Tue, Dec 29, 2015 at 11:32 AM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
> Hi
>
>
>
> You shall send your posts to the list, others can answer your questions
> and not only you can benefit from their answers too.
>
>
>
> As you did not post any data it is hard to say what are your issues. I
> believe that there are several values which are the same not only near the
> peak but also at the bottom part. If your data look like I remember and you
> want to keep all values near the peak value regardless they are slightly
> growing or falling, one approach can be to identify peak value, and select
> all values near the peak (the threshold is up to you).
>
>
>
> something like that
>
>
>
> sss<-smooth.spline(temp$theta, temp$int, nknots=length(temp$int)/2)
>
> peak <- which.max(predict(sss)$y)
>
> baseline <- min(predict(sss)$y)
>
> vyska <- predict(sss)$y[peak]
>
> # 50% threshold
>
> HM <- (vyska+baseline)/2
>
> plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
>
> #10% threshold
>
> HM<-vyska-(vyska*.1)
>
> plot(temp$theta, temp$int, col=(temp$int>HM)+1, pch=19)
>
>
>
> > dput(temp)
>
> structure(list(theta = c(28.995, 29.005, 29.015, 29.025, 29.035,
>
> 29.045, 29.055, 29.065, 29.075, 29.085, 29.095, 29.105, 29.115,
>
> 29.125, 29.135, 29.145, 29.155, 29.165, 29.175, 29.185, 29.195,
>
> 29.205, 29.215, 29.225, 29.235, 29.245, 29.255, 29.265, 29.275,
>
> 29.285, 29.295, 29.305, 29.315, 29.325, 29.335, 29.345, 29.355,
>
> 29.365, 29.375, 29.385, 29.395, 29.405, 29.415, 29.425, 29.435,
>
> 29.445, 29.455, 29.465, 29.475, 29.485, 29.495, 29.505, 29.515,
>
> 29.525, 29.535, 29.545, 29.555, 29.565, 29.575, 29.585, 29.595,
>
> 29.605, 29.615, 29.625, 29.635, 29.645, 29.655, 29.665, 29.675,
>
> 29.685, 29.695, 29.705, 29.715, 29.725, 29.735, 29.745, 29.755,
>
> 29.765, 29.775, 29.785, 29.795, 29.805, 29.815, 29.825, 29.835,
>
> 29.845, 29.855, 29.865, 29.875, 29.885, 29.895, 29.905, 29.915,
>
> 29.925, 29.935, 29.945, 29.955, 29.965, 29.975, 29.985, 29.995
>
> ), int = c(329L, 330L, 318L, 287L, 315L, 344L, 333L, 324L, 334L,
>
> 366L, 339L, 374L, 375L, 335L, 415L, 371L, 413L, 382L, 408L, 406L,
>
> 407L, 440L, 475L, 465L, 516L, 510L, 490L, 550L, 663L, 647L, 628L,
>
> 721L, 789L, 814L, 890L, 923L, 1085L, 1102L, 1222L, 1356L, 1521L,
>
> 1729L, 1868L, 2120L, 2491L, 2656L, 3196L, 3599L, 4128L, 4536L,
>
> 5043L, 5310L, 5638L, 5792L, 5699L, 5374L, 4886L, 4473L, 4293L,
>
> 3757L, 3319L, 2934L, 2422L, 1998L, 1753L, 1397L, 1163L, 972L,
>
> 854L, 775L, 648L, 695L, 616L, 553L, 554L, 509L, 530L, 483L, 482L,
>
> 406L, 451L, 422L, 403L, 393L, 396L, 348L, 367L, 428L, 345L, 384L,
>
> 330L, 342L, 312L, 313L, 323L, 328L, 340L, 322L, 330L, 305L, 311L
>
> )), .Names = c("theta", "int"), row.names = 100:200, class = "data.frame")
>
> >
>
>
>
> Cheers
>
> Petr
>
>
>
>
>
> *From:* Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
> *Sent:* Tuesday, December 29, 2015 5:54 AM
> *To:* PIKAL Petr
> *Subject:* Re: [R] need for help for solving operations in a vector
>
>
>
> Hi Petr,
>
> I would like to thank you for your time
>
> i choose the following modification on the plotting so to keep only
> successive equals in red
>
> plot(qWC1, col=(c(0, diff(qWC1))==0 )+1)
>
> then, the red points I want to include in the irrigation period are the 3
> successive red points in the plateau (equal values close to the max)
>
> These points are very important as they are corresponding to saturation,
> we continue to irrigate with the same flow and values remains constant for
> a certain time, so I must add them to irrigation
>
>
>
> up to now I didn't succeed in adding this plateau values to
>  theirrigations using the diff(qWC1, lag=1)
>
> however, I wrote also some loops trying to catch all the irrigation and
> recharge separately and I still have some issues,
>
> following are the loops I used, with comments corresponding to the issues
>
> x<-qWC1
>
> length(x)
>
> irrig<-rep(1,61)
>
> for (i in 2:61) {
>
> if (x[i-1]<x[i]){
>
>     irrig[i]<-x[i]-x[i-1]
>
> }
>
> }
>
> rech<-rep(1,61)
>
> for (i in 2:61) {
>
> if (x[i-1]>x[i]){
>
>     rech[i]<-x[i-1]-x[i]
>
> }
>
> }
>
> plot(x, type = "l", col = "black", ylim = c(min(0), max(92)))
>
> lines(irrig, type = "l", col = "cornflowerblue", ylim = c(min(0),
> max(15))) lines(rech, type = "l", col = "brown", ylim = c(min(0), max(15)))
>
> temp<-irrig<1.000001 #logical command to identify low values into a
> temporary vector temp
>
> temp2<-as.numeric(irrig>1.000001) #logical command to identify high values
> with 1
>
> temp2
>
> temp3<-as.numeric(rech>1.000001)
>
> temp3
>
> irrig2<-irrig*temp2  #remove values inf 1 mm inirrig
>
> rech2<-rech*temp3    #same for rech
>
> plot(irrig2, type = "l", col = "cornflowerblue", ylim = c(min(0),
> max(15))) lines(rech2, type = "l", col = "brown", ylim = c(min(0), max(15)))
>
> Many thanks for your time and intellectual generosity
>
> Cheers
>
> Makram
>
>
>
> On Mon, Dec 28, 2015 at 12:15 PM, PIKAL Petr <petr.pikal at precheza.cz>
> wrote:
>
> Hi
>
> On top of answers you have got here is some plotting you need to answer
> yourself
>
> plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)
>
> Which from those red points you want to be included in irrigation period?
> All of them? Only part? Which part?
>
> Based on your figures you probably will not get 100% correct answer.
>
> Cheers
> Petr
>
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Makram
> > Belhaj Fraj
> > Sent: Wednesday, December 23, 2015 8:35 AM
> > To: r-help at r-project.org; r-help-owner at r-project.org
> > Subject: [R] need for help for solving operations in a vector
> >
>
> >  Dear colleagues
> > i need your generous help to solve the following problem
> >
> > I have a  soil moisture time series qWC1 (61 values)
> > > qWC1
> >  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> > 74.70059
> > 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> > 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> > 80.69793
> > 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> > 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> > 75.97412
> > 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> > 75.46087
> > 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> > 75.07930 75.07930 74.95275 74.95275  74.95275
> >
> > I want to measure consecutive increases corresponding to irrigation and
> > consecutive decreases  corresponding to recharge I wrote the following
> > code and it does not calculate for each increment in i?
> > also note that I choose to not use diff command in time series because
> > I  want also that "plateaux" corresponding to a minimum of 2 equal
> > consecutive values are accounted as positive differences=irrigations so
> > when x[i+1]==x[i] the difference y might be equal to the previous value
> > xi
> >
> > following the code i wrote
> >
> > x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> > "green")
> > y<-rep(0,61)
> > for (i in 1:61) {
> > if (x[i+1] > x[i]){
> >     y[i]==x[i+1]-x[i]
> > } else if (x[i+1]==x[i]){
> >     y[i]=x[i+2]-x[i]
> > } else {
> >     y[i]==x[i+1]-x[i]
> > }
> >
> > }
> > plot(y, type="h", col = "blueviolet")
> >
> > Many thank
> > Makram
> >
>
>
>
>
>
> ------------------------------
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From belhajfraj.makram at gmail.com  Tue Dec 29 21:11:53 2015
From: belhajfraj.makram at gmail.com (Makram Belhaj Fraj)
Date: Wed, 30 Dec 2015 00:11:53 +0400
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
Message-ID: <CAP9hYOfLP_-ho8vPLs9oLWmyK4u=ExwJ7FGQ7Nx3SQsDmv35EQ@mail.gmail.com>

Hi Petr
I do th? calculation left as follows

Irrig=qWC1
Irrig [Irrig>HM]=-1
Irrig[Irrig>0]=0
Irrig.tot=-Irrig*6.123 #6.123 is the hydraulic conductivity at saturation
And I got the vector needed
Cheers
Makram
Le 28 d?c. 2015 12:15, "PIKAL Petr" <petr.pikal at precheza.cz> a ?crit :

> Hi
>
> On top of answers you have got here is some plotting you need to answer
> yourself
>
> plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)
>
> Which from those red points you want to be included in irrigation period?
> All of them? Only part? Which part?
>
> Based on your figures you probably will not get 100% correct answer.
>
> Cheers
> Petr
>
>
>
> > -----Original Message-----
> > From: R-help [mailto:r-help-bounces at r-project.org] On Behalf Of Makram
> > Belhaj Fraj
> > Sent: Wednesday, December 23, 2015 8:35 AM
> > To: r-help at r-project.org; r-help-owner at r-project.org
> > Subject: [R] need for help for solving operations in a vector
> >
> >  Dear colleagues
> > i need your generous help to solve the following problem
> >
> > I have a  soil moisture time series qWC1 (61 values)
> > > qWC1
> >  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> > 74.70059
> > 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> > 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> > 80.69793
> > 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> > 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> > 75.97412
> > 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> > 75.46087
> > 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> > 75.07930 75.07930 74.95275 74.95275  74.95275
> >
> > I want to measure consecutive increases corresponding to irrigation and
> > consecutive decreases  corresponding to recharge I wrote the following
> > code and it does not calculate for each increment in i?
> > also note that I choose to not use diff command in time series because
> > I  want also that "plateaux" corresponding to a minimum of 2 equal
> > consecutive values are accounted as positive differences=irrigations so
> > when x[i+1]==x[i] the difference y might be equal to the previous value
> > xi
> >
> > following the code i wrote
> >
> > x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> > "green")
> > y<-rep(0,61)
> > for (i in 1:61) {
> > if (x[i+1] > x[i]){
> >     y[i]==x[i+1]-x[i]
> > } else if (x[i+1]==x[i]){
> >     y[i]=x[i+2]-x[i]
> > } else {
> >     y[i]==x[i+1]-x[i]
> > }
> >
> > }
> > plot(y, type="h", col = "blueviolet")
> >
> > Many thank
> > Makram
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide http://www.R-project.org/posting-
> > guide.html
> > and provide commented, minimal, self-contained, reproducible code.
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou
> ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav?
> neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie
> vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email
> jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi
> ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en?
> smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout;
> Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany
> p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve
> v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za
> spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n
> nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto
> emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich
> existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are
> intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its
> sender. Delete the contents of this e-mail with all attachments and its
> copies from your system.
> If you are not the intended recipient of this e-mail, you are not
> authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage
> caused by modifications of the e-mail or by delay with transfer of the
> email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a
> contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to
> immediately accept such offer; The sender of this e-mail (offer) excludes
> any acceptance of the offer on the part of the recipient containing any
> amendment or variation.
> - the sender insists on that the respective contract is concluded only
> upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter
> into any contracts on behalf of the company except for cases in which
> he/she is expressly authorized to do so in writing, and such authorization
> or power of attorney is submitted to the recipient or the person
> represented by the recipient, or the existence of such authorization is
> known to the recipient of the person represented by the recipient.
>

	[[alternative HTML version deleted]]


From r.turner at auckland.ac.nz  Tue Dec 29 21:54:30 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 30 Dec 2015 09:54:30 +1300
Subject: [R] GLM: include observations with missing explanatory variables
In-Reply-To: <DUB109-W861A9EB7E461DC81D5093EBCFC0@phx.gbl>
References: <DUB109-W861A9EB7E461DC81D5093EBCFC0@phx.gbl>
Message-ID: <5682F306.6080501@auckland.ac.nz>


On 30/12/15 02:33, Frank van Berkum wrote:

> Hi all, My problem is the following. Suppose I have a dataset with
> observations Y and explanatory variables X1, ..., Xn, and suppose one
> of these explanatory variables is geographical area (of which there
> are ten, j=1,...,10).  For some observations I know the area, but for
> others it is unknown and therefore record as NA. I want to estimate a
> model of the form Y[i] ~ Poisson( lambda[i] ) with log(lambda[i]) =
> constant + \sum_j I[!is.na(area[i])] * I[area[i]==j] * beta[j] In
> words: we estimate a constant for all observations and a factor for
> each area. If it is unknown what the area is, we only include the
> constant. When estimating this model using glm(), the records with
> is.na(area[i]) are 'deleted' from the dataset, and this I do not
> want. I had hoped that the model as described above could be
> estimated using the function I() (interpret as), but so far my
> attempts have not succeeded. Any help on how to approach this is
> kindly appreciated.

As Deep Thought was heard to remark: "Hmmmmm.  Tricky."

After pondering for a while it seems to me that you want to make NA the 
reference level of the factor "geographical area".

I.e. convert the NA values to a level of that factor and make it the 
*first* level.

E.g.:

set.seed(42)
GA <- factor(sample(LETTERS[1:10],200,TRUE))
GA[sample(1:200,10)] <- NA
tmp <- as.character(GA)
tmp[is.na(tmp)] <- "unknown"
GA <- factor(tmp,levels=c("unknown",LETTERS[1:10]))

y <- rpois(200,20) # Artificial response.
fit <- glm(y ~ GA,family=poisson)

Note that if you set

     z <- predict(fit)

(this gives predictions on the scale of the linear predictor)
then the entries of z corresponding to "unknown" are equal to the 
intercept coefficient of fit.

I can't quite get my head around whether this is *really* accomplishing 
what you want --- but it's a thought.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Tue Dec 29 21:55:58 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 30 Dec 2015 09:55:58 +1300
Subject: [R] [FORGED] Re:  [FORGED] Mixed Beta Disrubutions
In-Reply-To: <CAAUQgdC1QixRNqFx9KAUJW3QeWNrUMmiVgfXf6Ukuk-EbZko0Q@mail.gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<5681C40F.1060504@auckland.ac.nz>
	<EA3894E3-B648-4B5C-9D5A-73718DF061A8@gmail.com>
	<5681C83C.9090503@auckland.ac.nz>
	<CAH6SrewsZzjA0FQa72gKJGAz2sSPe0XfTfaZXBOnsLJdrV8cjg@mail.gmail.com>
	<7687A477-AF20-4C75-88FC-DAC7DB495BE1@comcast.net>
	<CAAUQgdC1QixRNqFx9KAUJW3QeWNrUMmiVgfXf6Ukuk-EbZko0Q@mail.gmail.com>
Message-ID: <5682F35E.1090804@auckland.ac.nz>

On 29/12/15 15:21, Oliver Keyes wrote:

<SNIP>

I do not feed trolls.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From r.turner at auckland.ac.nz  Tue Dec 29 21:57:08 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Wed, 30 Dec 2015 09:57:08 +1300
Subject: [R] [FORGED] Re:  Mixed Beta Disrubutions
In-Reply-To: <E84357ED-88A7-410C-8EBA-B39B5F41415A@gmail.com>
References: <CAH6Srezb1wewSfXe7tbPh=a_aMxjdpq1obK26-4q_FZKBfdEjQ@mail.gmail.com>
	<CA+8X3fUv6QyKp7uKVpUHSqbL+J96zrjX0X8qYZbi8bm_b9PBWg@mail.gmail.com>
	<CAH6Sreyw7zcyrwSdeO+gZpGmWW0wnVb4virCLvFGaHRCOYNGxw@mail.gmail.com>
	<E84357ED-88A7-410C-8EBA-B39B5F41415A@gmail.com>
Message-ID: <5682F3A4.9040902@auckland.ac.nz>

On 29/12/15 16:59, Ryan Derickson wrote:

<SNIP>

I do not feed trolls.

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From dwinsemius at comcast.net  Tue Dec 29 22:03:19 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Dec 2015 13:03:19 -0800
Subject: [R] GLM: include observations with missing explanatory variables
In-Reply-To: <DUB109-W861A9EB7E461DC81D5093EBCFC0@phx.gbl>
References: <DUB109-W861A9EB7E461DC81D5093EBCFC0@phx.gbl>
Message-ID: <49EA15E7-2F40-45DA-926A-431007453DEA@comcast.net>


> On Dec 29, 2015, at 5:33 AM, Frank van Berkum <frankieboytje at hotmail.com> wrote:
> 
> Hi all,
> My problem is the following. 
> Suppose I have a dataset with observations Y and explanatory variables X1, ..., Xn, and suppose one of these explanatory variables is geographical area (of which there are ten, j=1,...,10).  For some observations I know the area, but for others it is unknown and therefore record as NA.
> I want to estimate a model of the form Y[i] ~ Poisson( lambda[i] ) with log(lambda[i]) = constant + \sum_j I[!is.na(area[i])] * I[area[i]==j] * beta[j]
> In words: we estimate a constant for all observations and a factor for each area. If it is unknown what the area is, we only include the constant. 
> When estimating this model using glm(), the records with is.na(area[i]) are 'deleted' from the dataset, and this I do not want. I had hoped that the model as described above could be estimated using the function I() (interpret as), but so far my attempts have not succeeded. 
> Any help on how to approach this is kindly appreciated.
> Kind regards,
> Frank van Berkum 		 	   		  
> 	[[alternative HTML version deleted]]

I don't understand why you don't just recode the NA's to "unknown" and redo the model. This code is untested, but I think demonstrates the two steps needed: add a level and then recode the NA values assuming this factor is named `X6`:

dat$X6 <- factor(dat$X6, levels=c( levels(dat$X6), "unknown") )
dat$X6[ is.na(dat$X6) ] <- "unknown"


(I think this might be is a bit quicker than Rolf's approach.)


In R the default contrasts are "treatment" (which is different than the contrasts you describe) and so each area will be referenced to the first area (and the first level of all other factors) in the lexical ordering of area names. This ordering and the contrast type can be changed. The are many postings on rhelp over the years demonstrating how to do this.

Study these and the basics of R before reposting and if you do so, then post in plain text and include a small example constructed in R:

?factor
?C
?constrasts
?contr.sum

-- 

David Winsemius
Alameda, CA, USA


From drjimlemon at gmail.com  Tue Dec 29 22:41:50 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Wed, 30 Dec 2015 08:41:50 +1100
Subject: [R] Rgraphviz overlap of nodes and edges
In-Reply-To: <BLU403-EAS2346DA0111F5EA9F6CDEC8181FC0@phx.gbl>
References: <BLU403-EAS2346DA0111F5EA9F6CDEC8181FC0@phx.gbl>
Message-ID: <CA+8X3fUT2-rQvw=73xBBdanyadCzB+Nf-cbfve4mQWwF7gMo1A@mail.gmail.com>

Hi Liliana,
see inline comments

On Sat, Jul 18, 2015 at 5:57 AM, Liliana Zazueta <liiz_1226 at hotmail.com>
wrote:

> Excuse me I have this dude,


Suppressing my desire to make a bad joke, I think you mean "doubt"


> how to make that Rgraphviz graphic without
> intersection between nodes and edges?
>
> I can't test this as I don't have Rgraphviz, but perhaps if you plot the
graph, then overplot with:

plot(my_graph_object,...)
par(new=TRUE)
# now plot with invisible edges
plot(my_graph_object,attrs=list(...,edge=list(color=NA),...))

you will cover the "intersections". Be aware that this is a guess.

>
> If answer me I will be eternally gratefull!
>
> Very kind of you, but I doubt that I will last that long.

Jim

	[[alternative HTML version deleted]]


From anindya55 at gmail.com  Wed Dec 30 07:35:58 2015
From: anindya55 at gmail.com (Anindya Sankar Dey)
Date: Wed, 30 Dec 2015 12:05:58 +0530
Subject: [R] Probable Error in fmsb package
Message-ID: <CAKC+_z_7r2prUr=6uh9_tvQ5UU_qGp+wR73VnC=BgJTpBuRvqg@mail.gmail.com>

Hi All,

The fmsb package has a function called Variance Inflation Factor and it
states the definition of the function as follows:-

"To evaluate multicolinearity of multiple regression model, calculating the
variance inflation factor (VIF) from the result of lm(). If VIF is more
than 10, multicolinearity is strongly suggested.
"

?The function computes VIF of a model as 1/(1-R^2) where R^2 is the
coefficient of determination.

Now nowhere in literature I have come across this definition of VIF, as VIF
is always computed at individual variable level. Though the structure is
almost the same, R^2 in theoretical VIF is the partial correlation
coefficient.

?I only came aware when lots of freshers from non statistics background I
interviewed for analytics position answered that the only definition of VIF
they know is 1/(1 - Coeff. of Determination), and there is a R package
which calculates VIF like that.

After researched I found that such a function indeed exist in fmsb package.

Please help me understand has an alternate definition of Variance Inflation
Factor has ever emerged in theory? Does it really make sense to have VIF at
a model level, as it does not help in solving the problem of
multicollinearity during model building.

And if I am right, what steps I should do about it.


-- 
Anindya Sankar Dey

	[[alternative HTML version deleted]]


From petr.pikal at precheza.cz  Wed Dec 30 08:28:15 2015
From: petr.pikal at precheza.cz (PIKAL Petr)
Date: Wed, 30 Dec 2015 07:28:15 +0000
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <CAP9hYOfLP_-ho8vPLs9oLWmyK4u=ExwJ7FGQ7Nx3SQsDmv35EQ@mail.gmail.com>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfLP_-ho8vPLs9oLWmyK4u=ExwJ7FGQ7Nx3SQsDmv35EQ@mail.gmail.com>
Message-ID: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5008221@SRVEXCHMBX.precheza.cz>

Hi

You probably can do also only

Irig.tot <- (Irrig>HM)*6.123

to get desired vector without any further typing around.

Logical vector can be treated as c(0,1) vector. FALSE equals zero and TRUE equals 1 on calculations.

Maybe you shall spend some time reading R intro manual which can serve you as an excellent starting point to learn R. Although it has one hundred pages it is quite readable and you may find it as a best present you found under Christmas tree.

Cheers
Petr

From: Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
Sent: Tuesday, December 29, 2015 9:12 PM
To: PIKAL Petr
Cc: r-help at r-project.org
Subject: RE: [R] need for help for solving operations in a vector


Hi Petr
I do th? calculation left as follows

Irrig=qWC1
Irrig [Irrig>HM]=-1
Irrig[Irrig>0]=0
Irrig.tot=-Irrig*6.123 #6.123 is the hydraulic conductivity at saturation
And I got the vector needed
Cheers
Makram
Le 28 d?c. 2015 12:15, "PIKAL Petr" <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> a ?crit :
Hi

On top of answers you have got here is some plotting you need to answer yourself

plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)

Which from those red points you want to be included in irrigation period? All of them? Only part? Which part?

Based on your figures you probably will not get 100% correct answer.

Cheers
Petr



> -----Original Message-----
> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Makram
> Belhaj Fraj
> Sent: Wednesday, December 23, 2015 8:35 AM
> To: r-help at r-project.org<mailto:r-help at r-project.org>; r-help-owner at r-project.org<mailto:r-help-owner at r-project.org>
> Subject: [R] need for help for solving operations in a vector
>
>  Dear colleagues
> i need your generous help to solve the following problem
>
> I have a  soil moisture time series qWC1 (61 values)
> > qWC1
>  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
> 74.70059
> 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
> 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
> 80.69793
> 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
> 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
> 75.97412
> 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
> 75.46087
> 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
> 75.07930 75.07930 74.95275 74.95275  74.95275
>
> I want to measure consecutive increases corresponding to irrigation and
> consecutive decreases  corresponding to recharge I wrote the following
> code and it does not calculate for each increment in i?
> also note that I choose to not use diff command in time series because
> I  want also that "plateaux" corresponding to a minimum of 2 equal
> consecutive values are accounted as positive differences=irrigations so
> when x[i+1]==x[i] the difference y might be equal to the previous value
> xi
>
> following the code i wrote
>
> x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
> "green")
> y<-rep(0,61)
> for (i in 1:61) {
> if (x[i+1] > x[i]){
>     y[i]==x[i+1]-x[i]
> } else if (x[i+1]==x[i]){
>     y[i]=x[i+2]-x[i]
> } else {
>     y[i]==x[i+1]-x[i]
> }
>
> }
> plot(y, type="h", col = "blueviolet")
>
> Many thank
> Makram
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-
> guide.html
> and provide commented, minimal, self-contained, reproducible code.



________________________________
Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.

V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
- vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
- a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
- trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
- odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.

This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.

In case that this e-mail forms part of business dealings:
- the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
- if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
- the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
- the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.

	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Wed Dec 30 08:45:41 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 29 Dec 2015 23:45:41 -0800
Subject: [R] Probable Error in fmsb package
In-Reply-To: <CAKC+_z_7r2prUr=6uh9_tvQ5UU_qGp+wR73VnC=BgJTpBuRvqg@mail.gmail.com>
References: <CAKC+_z_7r2prUr=6uh9_tvQ5UU_qGp+wR73VnC=BgJTpBuRvqg@mail.gmail.com>
Message-ID: <2B6448FE-2543-49D7-B39F-63A3CF2F8A0F@comcast.net>


> On Dec 29, 2015, at 10:35 PM, Anindya Sankar Dey <anindya55 at gmail.com> wrote:
> 
> Hi All,
> 
> The fmsb package has a function called Variance Inflation Factor and it
> states the definition of the function as follows:-
> 
> "To evaluate multicolinearity of multiple regression model, calculating the
> variance inflation factor (VIF) from the result of lm(). If VIF is more
> than 10, multicolinearity is strongly suggested.
> "
> 
> ?The function computes VIF of a model as 1/(1-R^2) where R^2 is the
> coefficient of determination.
> 
> Now nowhere in literature I have come across this definition of VIF, as VIF
> is always computed at individual variable level. Though the structure is
> almost the same, R^2 in theoretical VIF is the partial correlation
> coefficient.
> 
> ?I only came aware when lots of freshers from non statistics background I
> interviewed for analytics position answered that the only definition of VIF
> they know is 1/(1 - Coeff. of Determination), and there is a R package
> which calculates VIF like that.
> 
> After researched I found that such a function indeed exist in fmsb package.
> 
> Please help me understand has an alternate definition of Variance Inflation
> Factor has ever emerged in theory? Does it really make sense to have VIF at
> a model level, as it does not help in solving the problem of
> multicollinearity during model building.
> 
> And if I am right, what steps I should do about it.

This is not the correct location to post questions about non-base packages. There is a `maintainer` funciton that should deliver the correct email address for submission of complaints, advice, revisions, and feature requests.

-- 
David.
> 
> 
> -- 
> Anindya Sankar Dey
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From michael.eisenring at agroscope.admin.ch  Wed Dec 30 14:06:59 2015
From: michael.eisenring at agroscope.admin.ch (michael.eisenring at agroscope.admin.ch)
Date: Wed, 30 Dec 2015 13:06:59 +0000
Subject: [R] "Group" argument of kruskal function (agricolae package) does
 not coincide with p-values
Message-ID: <6D5C009FB51EBD41BEE57F4C002350FD067B77@SB00112A.adb.intra.admin.ch>

Hello,
I work with the kruskla function of the agricolae package to conduct Kurskal-Wallis tests.
The kurskal function has the argument "group"=T/F.
If group=T, the output of the kruskal test assigns a "significance letter" to each mean of each tested treatment (means with the same letter are not significantly different).
However, if I compare the "significance letters" with the p-values (obtained if group=F) sometimes the sign. letters and p-values do not coincide.
E.g. in my example the treatments "2" and "2C" differ significantly from each other (p.value=0.0332, level of significance=0.05) , however both of them are assigned the letter "a" if group=T is used.

Does anyone know why that's the case? First I thought is because I use a holm correction, but this should already be seen in the actual p-values.

The problem is that I would like to use the "sign. letters" in one of my figures (so the reader can easily distinguish which treatments are different from each other) but also use the p-values in the text (and of course sometimes the letters and p-values do not coincide)

Thank you Mike

EXAMPLE:
#Below is the code and the dput of my data frame (called "example")

kruskal(example$H.polone,example$Position, p.adj = 'holm', console = T, group = F)
#group=F shows p-values, group=T shows sign. letters



> dput(example)

structure(list(leaf = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L,

2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,

2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L,

2L, 2L, 2L, 2L), .Label = c("Cot", "L1", "L2", "L4", "L7"), class = "factor"),

    H.polone = c(980.310618, 2474.133482, 4406.901817, 2233.141989,

    1284.855658, 1202.881507, 2393.532397, 2639.775244, 2853.759572,

    3009.791049, 2734.731629, 1539.309599, 9634.265878, 15627.14214,

    13212.8464, 4.679175916, 5004.820592, 17045.7964, 17461.49723,

    14065.03002, 24452.43564, 17267.91494, 9483.597645, 535.05702,

    582.0554615, 163.9588478, 527.1015784, 621.3924963, 519.8953943,

    1429.334963, 584.2620172, 600.5646899, 568.2045799, 583.0237882,

    1054.557537, 576.5345625, 366.6277524, 644.2932199, 877.2948815,

    343.9021574, 960.3431623, 899.6237855, 834.5025197), Position = structure(c(6L,

    6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 5L, 5L, 5L, 5L,

    5L, 5L, 5L, 5L, 5L, 5L, 5L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L,

    8L, 8L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L), .Label = c("1",

    "1C", "1L4", "1L4C", "2", "2C", "2L4", "2L4C", "3", "3C",

    "3L4", "3L4C", "4L4", "4L4C", "5L4", "5L4C"), class = "factor")), class = "data.frame", row.names = c(NA,

-43L), .Names = c("leaf", "H.polone", "Position"))







Eisenring Michael, Msc.
PhD Student

Federal Department of Economic Affairs, Education and Research
EAER
Institute of Sustainability Sciences ISS
Biosafety

Reckenholzstrasse 191, CH-8046 Z?rich
Tel. +41 44 37 77181
Fax +41 44 37 77201
michael.eisenring at agroscope.admin.ch<mailto:michael.eisenring at agroscope.admin.ch>
www.agroscope.ch<http://www.agroscope.ch/>


	[[alternative HTML version deleted]]


From ashisdeb83 at gmail.com  Wed Dec 30 14:26:04 2015
From: ashisdeb83 at gmail.com (Ashis Deb)
Date: Wed, 30 Dec 2015 18:56:04 +0530
Subject: [R] R and Mt4 trading platform connection
Message-ID: <CAFcJUToZ_yyXZY8ihtvyvdWEpWsBMz89F4oD4=KkweK2Bgs00g@mail.gmail.com>

Hi guys ,

                Can anyone suggest  how to make a MT4 EA strategy using R
code .
I had seen many strategy builders online but have no idea about how to
build ne using R.

THanks and Regards ,
ASHIS

	[[alternative HTML version deleted]]


From bgunter.4567 at gmail.com  Wed Dec 30 15:26:01 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 30 Dec 2015 06:26:01 -0800
Subject: [R] Probable Error in fmsb package
In-Reply-To: <2B6448FE-2543-49D7-B39F-63A3CF2F8A0F@comcast.net>
References: <CAKC+_z_7r2prUr=6uh9_tvQ5UU_qGp+wR73VnC=BgJTpBuRvqg@mail.gmail.com>
	<2B6448FE-2543-49D7-B39F-63A3CF2F8A0F@comcast.net>
Message-ID: <CAGxFJbTEMcO+-g_UY412dDXaYKsQmk9LnyvKqZ7e5bfxxSfFpw@mail.gmail.com>

... Nor is this forum usually appropriate for questions about
statistical methodology (your model building remark at the end). I
suggest you try a statistical forum like stats.stackexchange.com for
that instead.

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 29, 2015 at 11:45 PM, David Winsemius
<dwinsemius at comcast.net> wrote:
>
>> On Dec 29, 2015, at 10:35 PM, Anindya Sankar Dey <anindya55 at gmail.com> wrote:
>>
>> Hi All,
>>
>> The fmsb package has a function called Variance Inflation Factor and it
>> states the definition of the function as follows:-
>>
>> "To evaluate multicolinearity of multiple regression model, calculating the
>> variance inflation factor (VIF) from the result of lm(). If VIF is more
>> than 10, multicolinearity is strongly suggested.
>> "
>>
>> The function computes VIF of a model as 1/(1-R^2) where R^2 is the
>> coefficient of determination.
>>
>> Now nowhere in literature I have come across this definition of VIF, as VIF
>> is always computed at individual variable level. Though the structure is
>> almost the same, R^2 in theoretical VIF is the partial correlation
>> coefficient.
>>
>> I only came aware when lots of freshers from non statistics background I
>> interviewed for analytics position answered that the only definition of VIF
>> they know is 1/(1 - Coeff. of Determination), and there is a R package
>> which calculates VIF like that.
>>
>> After researched I found that such a function indeed exist in fmsb package.
>>
>> Please help me understand has an alternate definition of Variance Inflation
>> Factor has ever emerged in theory? Does it really make sense to have VIF at
>> a model level, as it does not help in solving the problem of
>> multicollinearity during model building.
>>
>> And if I am right, what steps I should do about it.
>
> This is not the correct location to post questions about non-base packages. There is a `maintainer` funciton that should deliver the correct email address for submission of complaints, advice, revisions, and feature requests.
>
> --
> David.
>>
>>
>> --
>> Anindya Sankar Dey
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
> David Winsemius
> Alameda, CA, USA
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From bgunter.4567 at gmail.com  Wed Dec 30 15:30:46 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Wed, 30 Dec 2015 06:30:46 -0800
Subject: [R] need for help for solving operations in a vector
In-Reply-To: <6E8D8DFDE5FA5D4ABCB8508389D1BF88C5008221@SRVEXCHMBX.precheza.cz>
References: <CAP9hYOf84JUDipjFxjjxiN3yPMWPBqOO9eYg+a3eu3whN20ppw@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5007F23@SRVEXCHMBX.precheza.cz>
	<CAP9hYOfLP_-ho8vPLs9oLWmyK4u=ExwJ7FGQ7Nx3SQsDmv35EQ@mail.gmail.com>
	<6E8D8DFDE5FA5D4ABCB8508389D1BF88C5008221@SRVEXCHMBX.precheza.cz>
Message-ID: <CAGxFJbQ4==UCZ7p3=8WSwDH-8v-gkB3MZLLTxX4o+GX6zmwJgA@mail.gmail.com>

"Maybe you shall spend some time reading R intro manual which can
serve you as an excellent starting point to learn R. Although it has
one hundred pages it is quite readable and you may find it as a best
present you found under Christmas tree."

Fortune nomination!

Cheers,
Bert


Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Tue, Dec 29, 2015 at 11:28 PM, PIKAL Petr <petr.pikal at precheza.cz> wrote:
> Hi
>
> You probably can do also only
>
> Irig.tot <- (Irrig>HM)*6.123
>
> to get desired vector without any further typing around.
>
> Logical vector can be treated as c(0,1) vector. FALSE equals zero and TRUE equals 1 on calculations.
>
> Maybe you shall spend some time reading R intro manual which can serve you as an excellent starting point to learn R. Although it has one hundred pages it is quite readable and you may find it as a best present you found under Christmas tree.
>
> Cheers
> Petr
>
> From: Makram Belhaj Fraj [mailto:belhajfraj.makram at gmail.com]
> Sent: Tuesday, December 29, 2015 9:12 PM
> To: PIKAL Petr
> Cc: r-help at r-project.org
> Subject: RE: [R] need for help for solving operations in a vector
>
>
> Hi Petr
> I do th? calculation left as follows
>
> Irrig=qWC1
> Irrig [Irrig>HM]=-1
> Irrig[Irrig>0]=0
> Irrig.tot=-Irrig*6.123 #6.123 is the hydraulic conductivity at saturation
> And I got the vector needed
> Cheers
> Makram
> Le 28 d?c. 2015 12:15, "PIKAL Petr" <petr.pikal at precheza.cz<mailto:petr.pikal at precheza.cz>> a ?crit :
> Hi
>
> On top of answers you have got here is some plotting you need to answer yourself
>
> plot(qWC1, col=(c(0, diff(qWC1))>=0 )+1)
>
> Which from those red points you want to be included in irrigation period? All of them? Only part? Which part?
>
> Based on your figures you probably will not get 100% correct answer.
>
> Cheers
> Petr
>
>
>
>> -----Original Message-----
>> From: R-help [mailto:r-help-bounces at r-project.org<mailto:r-help-bounces at r-project.org>] On Behalf Of Makram
>> Belhaj Fraj
>> Sent: Wednesday, December 23, 2015 8:35 AM
>> To: r-help at r-project.org<mailto:r-help at r-project.org>; r-help-owner at r-project.org<mailto:r-help-owner at r-project.org>
>> Subject: [R] need for help for solving operations in a vector
>>
>>  Dear colleagues
>> i need your generous help to solve the following problem
>>
>> I have a  soil moisture time series qWC1 (61 values)
>> > qWC1
>>  75.33336 75.20617 75.20617 74.95275 74.95275 74.70059 74.70059
>> 74.70059
>> 74.57498 74.44968 74.32469 74.07563  85.57237 90.40123 90.73760
>> 90.73760 90.73760 90.73760 90.90648 91.07582 91.24564 90.90648 86.82135
>> 80.69793
>> 79.30393 78.62058 78.21484 77.81226 77.67876 77.41279 77.28032 76.88495
>> 76.75383 76.75383 76.49260 76.36249  76.23270 76.23270 76.10325
>> 75.97412
>> 75.84532 75.71685  75.71685 75.71685 75.71685 75.46087 75.46087
>> 75.46087
>> 75.33336 75.20617 75.20617 75.20617 75.20617 75.20617 75.20617 75.07930
>> 75.07930 75.07930 74.95275 74.95275  74.95275
>>
>> I want to measure consecutive increases corresponding to irrigation and
>> consecutive decreases  corresponding to recharge I wrote the following
>> code and it does not calculate for each increment in i?
>> also note that I choose to not use diff command in time series because
>> I  want also that "plateaux" corresponding to a minimum of 2 equal
>> consecutive values are accounted as positive differences=irrigations so
>> when x[i+1]==x[i] the difference y might be equal to the previous value
>> xi
>>
>> following the code i wrote
>>
>> x<-ts(qWC1,start=1, end=61, frequency=1) x[1] plot(x, type="h", col =
>> "green")
>> y<-rep(0,61)
>> for (i in 1:61) {
>> if (x[i+1] > x[i]){
>>     y[i]==x[i+1]-x[i]
>> } else if (x[i+1]==x[i]){
>>     y[i]=x[i+2]-x[i]
>> } else {
>>     y[i]==x[i+1]-x[i]
>> }
>>
>> }
>> plot(y, type="h", col = "blueviolet")
>>
>> Many thank
>> Makram
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at r-project.org<mailto:R-help at r-project.org> mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-
>> guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>
>
>
> ________________________________
> Tento e-mail a jak?koliv k n?mu p?ipojen? dokumenty jsou d?v?rn? a jsou ur?eny pouze jeho adres?t?m.
> Jestli?e jste obdr?el(a) tento e-mail omylem, informujte laskav? neprodlen? jeho odes?latele. Obsah tohoto emailu i s p??lohami a jeho kopie vyma?te ze sv?ho syst?mu.
> Nejste-li zam??len?m adres?tem tohoto emailu, nejste opr?vn?ni tento email jakkoliv u??vat, roz?i?ovat, kop?rovat ?i zve?ej?ovat.
> Odes?latel e-mailu neodpov?d? za eventu?ln? ?kodu zp?sobenou modifikacemi ?i zpo?d?n?m p?enosu e-mailu.
>
> V p??pad?, ?e je tento e-mail sou??st? obchodn?ho jedn?n?:
> - vyhrazuje si odes?latel pr?vo ukon?it kdykoliv jedn?n? o uzav?en? smlouvy, a to z jak?hokoliv d?vodu i bez uveden? d?vodu.
> - a obsahuje-li nab?dku, je adres?t opr?vn?n nab?dku bezodkladn? p?ijmout; Odes?latel tohoto e-mailu (nab?dky) vylu?uje p?ijet? nab?dky ze strany p??jemce s dodatkem ?i odchylkou.
> - trv? odes?latel na tom, ?e p??slu?n? smlouva je uzav?ena teprve v?slovn?m dosa?en?m shody na v?ech jej?ch n?le?itostech.
> - odes?latel tohoto emailu informuje, ?e nen? opr?vn?n uzav?rat za spole?nost ??dn? smlouvy s v?jimkou p??pad?, kdy k tomu byl p?semn? zmocn?n nebo p?semn? pov??en a takov? pov??en? nebo pln? moc byly adres?tovi tohoto emailu p??padn? osob?, kterou adres?t zastupuje, p?edlo?eny nebo jejich existence je adres?tovi ?i osob? j?m zastoupen? zn?m?.
>
> This e-mail and any documents attached to it may be confidential and are intended only for its intended recipients.
> If you received this e-mail by mistake, please immediately inform its sender. Delete the contents of this e-mail with all attachments and its copies from your system.
> If you are not the intended recipient of this e-mail, you are not authorized to use, disseminate, copy or disclose this e-mail in any manner.
> The sender of this e-mail shall not be liable for any possible damage caused by modifications of the e-mail or by delay with transfer of the email.
>
> In case that this e-mail forms part of business dealings:
> - the sender reserves the right to end negotiations about entering into a contract in any time, for any reason, and without stating any reasoning.
> - if the e-mail contains an offer, the recipient is entitled to immediately accept such offer; The sender of this e-mail (offer) excludes any acceptance of the offer on the part of the recipient containing any amendment or variation.
> - the sender insists on that the respective contract is concluded only upon an express mutual agreement on all its aspects.
> - the sender of this e-mail informs that he/she is not authorized to enter into any contracts on behalf of the company except for cases in which he/she is expressly authorized to do so in writing, and such authorization or power of attorney is submitted to the recipient or the person represented by the recipient, or the existence of such authorization is known to the recipient of the person represented by the recipient.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From judsonblake at msn.com  Wed Dec 30 06:41:44 2015
From: judsonblake at msn.com (Judson)
Date: Wed, 30 Dec 2015 05:41:44 +0000
Subject: [R] How to install pkg when "not found"  ?
Message-ID: <CY1PR18MB06137CFD456B96DA64852BF7ADFD0@CY1PR18MB0613.namprd18.prod.outlook.com>


Using "Install Packages" from CRAN, in RStudio on Windows 7,
I downloaded (and supposedly installed)  ggplot2 package to here:

C:\Program Files\R\R-3.1.0\library\ggplot2_2.0.0\ggplot2


.... when I try this:
> require(ggplot2)
.... I get the following:
Loading required package: ggplot2
Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :

  there is no package called 'ggplot2'


.... I also tried this:
> install.packages(ggplot2)
Error in install.packages : object 'ggplot2' not found

......


The above library has the usual things like MASS, stats, base, utils, graphics and so on....
..... and if I do this:
> require(MASS)
.... I get this:
Loading required package: MASS

.... so that works....


..... Any idea what I'm doing wrong with ggplot2?
................... judson blake

	[[alternative HTML version deleted]]


From shivi.bhatia at safexpress.com  Wed Dec 30 11:56:26 2015
From: shivi.bhatia at safexpress.com (SHIVI BHATIA)
Date: Wed, 30 Dec 2015 16:26:26 +0530
Subject: [R] Dput Help in R
Message-ID: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>

Dear Team, 

 

I am facing an error while performing a manipulation using a dplyr package.
In the code below, I am using mutate to build a new calculated column:

 

kp<-read.csv("collection_last.csv",header=TRUE)

mutate(kp,dif=DOC_AMOUNT-RECEIPT_AMT+TDS_AMT+REBATE)

 

However it gives an error:-

Warning messages:

1: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :

  '-' not meaningful for factors

2: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :

  '+' not meaningful for factors

3: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :

  '+' not meaningful for factors

 

This is an error when some of my variables are factors hence I have tried to
change these to numeric so used the expression as:

kp$DOC_TYPE=as.numeric(kp$DOC_TYPE). 

 

this now shows as variable type of as "double". So expedite help on this one
i was trying to create a reproducible example and i am highly struggling to 

create one. the data i have is approx. around 1 million rows with 21 columns
hence when i use a dput option it does not capture the entire detailing and
row level info required to share and even dput(head(kp$DOC_TYPE) does not
help either. 

I have seen many stack overflow & r help column before composing this email.
Hence i need help to create this reproducible example to share with the
experts in the community. Apologies if this is a repeat.

 

PLEASE HELP AS I AM HIGHLY STRUGGLING TO BUILD ANY OUTCOME. 

Regards, Shivi

 

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-help/attachments/20151230/77760eee/attachment.pl>

From Matthias.Kohl at stamats.de  Wed Dec 30 16:34:03 2015
From: Matthias.Kohl at stamats.de (Prof. Dr. Matthias Kohl)
Date: Wed, 30 Dec 2015 16:34:03 +0100
Subject: [R] How to install pkg when "not found" ?
In-Reply-To: <CY1PR18MB06137CFD456B96DA64852BF7ADFD0@CY1PR18MB0613.namprd18.prod.outlook.com>
References: <CY1PR18MB06137CFD456B96DA64852BF7ADFD0@CY1PR18MB0613.namprd18.prod.outlook.com>
Message-ID: <5683F96B.8060405@stamats.de>

use quotes!
install.packages("ggplot2")

Am 30.12.2015 um 06:41 schrieb Judson:
>
> Using "Install Packages" from CRAN, in RStudio on Windows 7,
> I downloaded (and supposedly installed)  ggplot2 package to here:
>
> C:\Program Files\R\R-3.1.0\library\ggplot2_2.0.0\ggplot2
>
>
> .... when I try this:
>> require(ggplot2)
> .... I get the following:
> Loading required package: ggplot2
> Warning message:
> In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
>
>    there is no package called 'ggplot2'
>
>
> .... I also tried this:
>> install.packages(ggplot2)
> Error in install.packages : object 'ggplot2' not found
>
> ......
>
>
> The above library has the usual things like MASS, stats, base, utils, graphics and so on....
> ..... and if I do this:
>> require(MASS)
> .... I get this:
> Loading required package: MASS
>
> .... so that works....
>
>
> ..... Any idea what I'm doing wrong with ggplot2?
> ................... judson blake
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Prof. Dr. Matthias Kohl
www.stamats.de


From murdoch.duncan at gmail.com  Wed Dec 30 17:53:25 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 30 Dec 2015 11:53:25 -0500
Subject: [R] Dput Help in R
In-Reply-To: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
References: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
Message-ID: <56840C05.3070401@gmail.com>

On 30/12/2015 5:56 AM, SHIVI BHATIA wrote:
> Dear Team,
>
>
>
> I am facing an error while performing a manipulation using a dplyr package.
> In the code below, I am using mutate to build a new calculated column:
>
>
>
> kp<-read.csv("collection_last.csv",header=TRUE)
>
> mutate(kp,dif=DOC_AMOUNT-RECEIPT_AMT+TDS_AMT+REBATE)
>
>
>
> However it gives an error:-
>
> Warning messages:
>
> 1: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>
>    '-' not meaningful for factors
>
> 2: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>
>    '+' not meaningful for factors
>
> 3: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>
>    '+' not meaningful for factors
>
>
>
> This is an error when some of my variables are factors hence I have tried to
> change these to numeric so used the expression as:
>
> kp$DOC_TYPE=as.numeric(kp$DOC_TYPE).
>
>
>
> this now shows as variable type of as "double". So expedite help on this one
> i was trying to create a reproducible example and i am highly struggling to
>
> create one. the data i have is approx. around 1 million rows with 21 columns
> hence when i use a dput option it does not capture the entire detailing and
> row level info required to share and even dput(head(kp$DOC_TYPE) does not
> help either.
>
> I have seen many stack overflow & r help column before composing this email.
> Hence i need help to create this reproducible example to share with the
> experts in the community. Apologies if this is a repeat.
>
>
>
> PLEASE HELP AS I AM HIGHLY STRUGGLING TO BUILD ANY OUTCOME.

If you are working with a dataframe or matrix named x, just use

y <- x[1:10,]

to extract the first 10 rows.  The error will probably occur with this 
subset as well, and dput() will give you a reasonably sized amount of 
output.  If the error doesn't happen, just take a bigger subset, and 
possibly leave off the beginning, e.g.

y <- x[101:110,]

for 10 lines starting at line 101.

Duncan Murdoch


From andrewcd at gmail.com  Wed Dec 30 18:36:44 2015
From: andrewcd at gmail.com (Andrew Crane-Droesch)
Date: Wed, 30 Dec 2015 12:36:44 -0500
Subject: [R] Thread parallelism and memory management on shared-memory
 supercomputers
Message-ID: <5684162C.6030307@gmail.com>

I've got allocations on a couple of shared memory supercomputers, which 
I use to run computationally-intensive scripts on multiple cores of the 
same node.  I've got 24 cores on the one, and 48 on the other.

In both cases, there is a hard memory limit, which is shared among the 
cores in the node.  In the latter, the limit is 255G. If my job requests 
more than that, the job gets aborted.

Now, I don't fully understand resource allocation in these sorts of 
systems.  But I do get that the sort of "thread parallelism" done by 
e.g. the `parallel` package in R isn't identical to the sort of 
parallelism commonly done in lower-level languages.  For example, when I 
request a node, I only ask for one of its cores.  My R script then 
detects the number of cores on the node, and farms out tasks to the 
cores via the `foreach` package.  My understanding is that lower-level 
languages need the number of cores to be specified in the shell script, 
and a particular job script is given directly to each worker.

My problem is that my parallel-calling R script is crashing the cluster, 
which terminates my script because the sum of the memory being requested 
by each thread is greater than what I'm allocated. I don't get this 
problem when running on my laptop's 4 cores, presumably because my 
laptop has a higher ratio of memory/core.

My question:  how can I ensure that the total memory being requested by 
N workers remains below a certain threshold?  Is this even possible?  If 
not, is it possible to benchmark a process locally, collecting the 
maximum per-worker memory requested, and use this to back out the number 
of workers that I can request for a given node's memory limit?

Thanks in advance!


From dwinsemius at comcast.net  Wed Dec 30 18:59:31 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 30 Dec 2015 09:59:31 -0800
Subject: [R] Dput Help in R
In-Reply-To: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
References: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
Message-ID: <D2B3DBE9-CE0E-4912-AF4F-E5283E1E70EF@comcast.net>


> On Dec 30, 2015, at 2:56 AM, SHIVI BHATIA <shivi.bhatia at safexpress.com> wrote:
> 
> Dear Team, 
> 
> 
> 
> I am facing an error while performing a manipulation using a dplyr package.
> In the code below, I am using mutate to build a new calculated column:
> 
> 
> 
> kp<-read.csv("collection_last.csv",header=TRUE)

Given the material below, I suspect that columns which you suspected of being 'numeric' were actually found to have some values that could not be converted to that class and so were entered as 'factor's. The approach of converting such a set of factor-values back to their intended numeric-values is not as simple as coercing to numeric. I would instead suggest that you learn how to use the colClasses argument for the `read.*`-functions. If all of the values are numeric then it could be as simple as:

kp<-read.csv("collection_last.csv",  # header=TRUE is default for read csv
               colClasses="numeric" )

If it is not that simple, then this might succeed:

kp[ , c('DOC_AMOUNT', 'RECEIPT_AMT', 'TDS_AMT', 'REBATE')] <- 
         lapply( kp[ , c('DOC_AMOUNT', 'RECEIPT_AMT', 'TDS_AMT', 'REBATE')], 
                 function(x) as.numeric(as.character(x))
                )

The care and fixing of factor arguments is just one of the items covered in the R-FAQ which, like the "Introduction to R" should be read by all R-noobs.

-- 
David.

> 
> mutate(kp,dif=DOC_AMOUNT-RECEIPT_AMT+TDS_AMT+REBATE)
> 
> 
> 
> However it gives an error:-
> 
> Warning messages:
> 
> 1: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
> 
>  '-' not meaningful for factors
> 
> 2: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
> 
>  '+' not meaningful for factors
> 
> 3: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
> 
>  '+' not meaningful for factors
> 
> 
> 
> This is an error when some of my variables are factors hence I have tried to
> change these to numeric so used the expression as:
> 
> kp$DOC_TYPE=as.numeric(kp$DOC_TYPE). 
> 
> 
> 
> this now shows as variable type of as "double". So expedite help on this one
> i was trying to create a reproducible example and i am highly struggling to 
> 
> create one. the data i have is approx. around 1 million rows with 21 columns
> hence when i use a dput option it does not capture the entire detailing and
> row level info required to share and even dput(head(kp$DOC_TYPE) does not
> help either. 
> 
> I have seen many stack overflow & r help column before composing this email.
> Hence i need help to create this reproducible example to share with the
> experts in the community. Apologies if this is a repeat.
> 
> 
> 
> PLEASE HELP AS I AM HIGHLY STRUGGLING TO BUILD ANY OUTCOME. 
> 
> Regards, Shivi
> 
> 
> 
> This e-mail is confidential. It may also be legally privileged. If you are not the addressee you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return e-mail. Internet communications cannot be guaranteed to be timely, secure, error or virus-free. The sender does not accept liability for any errors or omissions.
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From peter.langfelder at gmail.com  Wed Dec 30 19:44:17 2015
From: peter.langfelder at gmail.com (Peter Langfelder)
Date: Wed, 30 Dec 2015 10:44:17 -0800
Subject: [R] Thread parallelism and memory management on shared-memory
	supercomputers
In-Reply-To: <5684162C.6030307@gmail.com>
References: <5684162C.6030307@gmail.com>
Message-ID: <CA+hbrhXtq2XfJzxE0SNTXyGVWXJdAhvUneXgTkHhtmMmtQ9_zw@mail.gmail.com>

I'm not really an expert, but here are my 2 cents:

To the best of my limited knowlede, there is no direct way of ensuring
that the total memory being requested by N workers remains below a
certain threshold. You can control the number of child processes
forked by foreach/doPar in the registerDoParallel call using argument
'cores'. The parallel computation implemented in parallel and
foreach/doPar uses process forking (at least last time I checked it
did). When a process is forked, the entire memory of its parent is
"forked" as well (not sure what the right terms is). This does not
mean a real copy (modern systems use copy-on-write), but for the OS
memory management purposes each child occupies as much memory as the
parent.

If you want to benchmark your memory usage, run a single (non-forked)
process and at the end, look at the output of gc() which gives you,
among other things, maximum memory usage. For a more detailed
information on memory usage, you can run Rprof, tracemem, or Rprofmem,
see their help for details.

To decrease memory usage, you will have to optimize your code and
perhaps sprinkle in garbage collection (gc()) calls after large object
manipulations. Just be aware that garbage collection is rather slow,
so you don't want to do it too often.

The difference between the cluster and your laptop may be that on the
laptop the system doesn't care so much about how much memory each
child uses, so you can fork a process with a large memory footprint as
long as you don't cause copying by modifying large chunks of memory.

HTH,

Peter

On Wed, Dec 30, 2015 at 9:36 AM, Andrew Crane-Droesch
<andrewcd at gmail.com> wrote:
> I've got allocations on a couple of shared memory supercomputers, which I
> use to run computationally-intensive scripts on multiple cores of the same
> node.  I've got 24 cores on the one, and 48 on the other.
>
> In both cases, there is a hard memory limit, which is shared among the cores
> in the node.  In the latter, the limit is 255G. If my job requests more than
> that, the job gets aborted.
>
> Now, I don't fully understand resource allocation in these sorts of systems.
> But I do get that the sort of "thread parallelism" done by e.g. the
> `parallel` package in R isn't identical to the sort of parallelism commonly
> done in lower-level languages.  For example, when I request a node, I only
> ask for one of its cores.  My R script then detects the number of cores on
> the node, and farms out tasks to the cores via the `foreach` package.  My
> understanding is that lower-level languages need the number of cores to be
> specified in the shell script, and a particular job script is given directly
> to each worker.
>
> My problem is that my parallel-calling R script is crashing the cluster,
> which terminates my script because the sum of the memory being requested by
> each thread is greater than what I'm allocated. I don't get this problem
> when running on my laptop's 4 cores, presumably because my laptop has a
> higher ratio of memory/core.
>
> My question:  how can I ensure that the total memory being requested by N
> workers remains below a certain threshold?  Is this even possible?  If not,
> is it possible to benchmark a process locally, collecting the maximum
> per-worker memory requested, and use this to back out the number of workers
> that I can request for a given node's memory limit?
>
> Thanks in advance!
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From 9rogr4mm3r at gmail.com  Wed Dec 30 20:32:12 2015
From: 9rogr4mm3r at gmail.com (David Brand)
Date: Wed, 30 Dec 2015 19:32:12 +0000
Subject: [R] Error: could not find function "VectorSource" in package tm
Message-ID: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>

Error: could not find function "VectorSource" in package tm

Windows 64bit using Rstudio

please advise

thanks
David

	[[alternative HTML version deleted]]


From sarah.goslee at gmail.com  Wed Dec 30 21:54:08 2015
From: sarah.goslee at gmail.com (Sarah Goslee)
Date: Wed, 30 Dec 2015 15:54:08 -0500
Subject: [R] Error: could not find function "VectorSource" in package tm
In-Reply-To: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>
References: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>
Message-ID: <CAM_vjukY62mmsn9C8p12MhSsZtux0HqVcpPg3mLxpMss6GhLkg@mail.gmail.com>

What did you do to get that error?

Is tm installed?
Is tm loaded?

What does
library(tm)
return?
And
sessionInfo()
?


On Wed, Dec 30, 2015 at 2:32 PM, David Brand <9rogr4mm3r at gmail.com> wrote:
> Error: could not find function "VectorSource" in package tm
>
> Windows 64bit using Rstudio
>
> please advise
>
> thanks
> David
>
>         [[alternative HTML version deleted]]
>
Please use plain text.


-- 
Sarah Goslee
http://www.numberwright.com


From bob at rudis.net  Wed Dec 30 21:54:09 2015
From: bob at rudis.net (boB Rudis)
Date: Wed, 30 Dec 2015 15:54:09 -0500
Subject: [R] Error: could not find function "VectorSource" in package tm
In-Reply-To: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>
References: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>
Message-ID: <CAJ4QxaNtGGtidVjnVq4dimfJbT0sq0aAahO-Ki=9nV3pcuc7Vw@mail.gmail.com>

Do you have any code? Any more logs from the error? It's hard to help
when you've provided little more than an error message. What does the
output of:

   library(tm)
   docs <- c("This is a text.", "This another one.")
   (vs <- VectorSource(docs))

generate?

On Wed, Dec 30, 2015 at 2:32 PM, David Brand <9rogr4mm3r at gmail.com> wrote:
> Error: could not find function "VectorSource" in package tm
>
> Windows 64bit using Rstudio
>
> please advise
>
> thanks
> David
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From murdoch.duncan at gmail.com  Wed Dec 30 21:56:38 2015
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 30 Dec 2015 15:56:38 -0500
Subject: [R] Error: could not find function "VectorSource" in package tm
In-Reply-To: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>
References: <CADoNpst2f9mHfGkVp0H7C+S1-R+uF7Ex=oeUBZo_49Ze5TLp6g@mail.gmail.com>
Message-ID: <56844506.4040805@gmail.com>

On 30/12/2015 2:32 PM, David Brand wrote:
> Error: could not find function "VectorSource" in package tm
>
> Windows 64bit using Rstudio
>
> please advise

If you want advice about the error message, you need to tell us what you 
did to produce it.

Duncan Murdoch


From dwinsemius at comcast.net  Thu Dec 31 03:28:53 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 30 Dec 2015 18:28:53 -0800
Subject: [R] (old) rgl package crashes MacGUI using R 3.2.3 in El Cap,
	new compiled one does not.
Message-ID: <35CB09D1-01FE-48AF-A148-45671AA963A1@comcast.net>

Earlier today I had been getting an warning message when loading pkg:rgl (and then failing to get any plotting to an X11 window),  so I decided to re-install the binary 0.95.1201 from a CRAN mirror:

XQuartz 2.7.8 has been installed and reinstalled multiple times including just a few minutes before this. Also have current XCode and CLT.

> library(rgl)
> sessionInfo()
R version 3.2.3 (2015-12-10)
Platform: x86_64-apple-darwin13.4.0 (64-bit)
Running under: OS X 10.11.2 (El Capitan)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] grDevices utils     datasets  stats     graphics  grid      methods  
[8] base     

other attached packages:
 [1] rgl_0.95.1201   rms_4.4-0       SparseM_1.7     Hmisc_3.17-0   
 [5] ggplot2_2.0.0   Formula_1.2-1   survival_2.38-3 sos_1.3-8      
 [9] brew_1.0-6      lattice_0.20-33

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.2         cluster_2.0.3       splines_3.2.3      
 [4] munsell_0.4.2       colorspace_1.2-6    multcomp_1.4-1     
 [7] plyr_1.8.3          tools_3.2.3         nnet_7.3-11        
[10] gtable_0.1.2        nlme_3.1-122        quantreg_5.19      
[13] TH.data_1.0-6       latticeExtra_0.6-26 MatrixModels_0.4-1 
[16] polspline_1.1.12    Matrix_1.2-3        gridExtra_2.0.0    
[19] RColorBrewer_1.1-2  codetools_0.2-14    acepack_1.3-3.3    
[22] rpart_4.1-10        sandwich_2.3-4      scales_0.3.0.9000  
[25] mvtnorm_1.0-3       foreign_0.8-66      zoo_1.7-12         
[28] proto_0.3-10       


This is( ... er was) the crash log when done from the macGUI. When running from a Terminal window I get no error and the XQuartz/X11 window displays everything without problem. 

I also tried renaming the .Rprofile file that loads all that other stuff to hide it from the GUI-launch process, and just loading with rgl and the base packages and it still crashes (i.e. the MacGUI window disappears and a crash report appears.) 

Last minute update: I then installed the source version: rgl_0.95.1435.tar.gz and it loaded without complaint and executes the code in either session type:

In a Terminal window session with nothing in the .Rprofile:

install.packages("/Users/davidwinsemius/Downloads/rgl_0.95.1435.tar.gz", repo=NULL, dependencies=TRUE, type="source") 


Then in MacGUI:

library(rgl)
spheres3d(0,0,0,front="lines",back="lines")
set.seed(101)
n <- 50
theta <- runif(n,0,2*pi)
u <- runif(n,-1,1)
x <- sqrt(1-u^2)*cos(theta)
y <- sqrt(1-u^2)*sin(theta)
z <- u
points3d(x,y,z,col="red")

I see that the compile process fails with a message:

----------------------
	? checking files in ?vignettes? ... OK
	? checking examples ... ERROR
Running examples in ?rgl-Ex.R? failed
The error most likely occurred in:

> ### Name: rgl.pixels
> ### Title: Extract pixel information from window
> ### Aliases: rgl.pixels
> ### Keywords: dynamic
> 
> ### ** Examples
> 
> example(surface3d)

srfc3d> #
srfc3d> # volcano example taken from "persp"
srfc3d> #
srfc3d> 
srfc3d> data(volcano)

srfc3d> z <- 2 * volcano # Exaggerate the relief
-------------------

I tried running the contents of test/demo.R and got 21 different rgl windows, but no error. I was eventually able to find the offending code of : 'rgl-Ex.R' in the rgl.pixels.Rd file and copied and that code in a  GUI session with no errors reported.

So I guess the question is ... why is the build process for the Mavericks/Yosemite/El Capitan machines still failing to produce a current (working) version of rgl?

--- 

snipped crash logs.

 
-- 
David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Thu Dec 31 03:40:11 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 30 Dec 2015 18:40:11 -0800
Subject: [R] (old) rgl package crashes MacGUI using R 3.2.3 in El Cap,
	new compiled one does not.
In-Reply-To: <35CB09D1-01FE-48AF-A148-45671AA963A1@comcast.net>
References: <35CB09D1-01FE-48AF-A148-45671AA963A1@comcast.net>
Message-ID: <35763D9C-5463-4B34-8190-6206415885F7@comcast.net>

Apologies.... I meant to send this to R-SIG-MAC. Please ignore or respond over there.


> On Dec 30, 2015, at 6:28 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> Earlier today I had been getting an warning message when loading pkg:rgl (and then failing to get any plotting to an X11 window),  so I decided to re-install the binary 0.95.1201 from a CRAN mirror:
> 
> XQuartz 2.7.8 has been installed and reinstalled multiple times including just a few minutes before this. Also have current XCode and CLT.
snipped

David Winsemius
Alameda, CA, USA


From tuechler at gmx.at  Thu Dec 31 09:32:05 2015
From: tuechler at gmx.at (Heinz Tuechler)
Date: Thu, 31 Dec 2015 09:32:05 +0100
Subject: [R] Probable Error in fmsb package
In-Reply-To: <CAKC+_z_7r2prUr=6uh9_tvQ5UU_qGp+wR73VnC=BgJTpBuRvqg@mail.gmail.com>
References: <CAKC+_z_7r2prUr=6uh9_tvQ5UU_qGp+wR73VnC=BgJTpBuRvqg@mail.gmail.com>
Message-ID: <5684E805.8040703@gmx.at>



Anindya Sankar Dey wrote/hat geschrieben on/am 30.12.2015 07:35:
> Hi All,
>
> The fmsb package has a function called Variance Inflation Factor and it
> states the definition of the function as follows:-
>
> "To evaluate multicolinearity of multiple regression model, calculating the
> variance inflation factor (VIF) from the result of lm(). If VIF is more
> than 10, multicolinearity is strongly suggested.
> "
>
> ?The function computes VIF of a model as 1/(1-R^2) where R^2 is the
> coefficient of determination.
>
> Now nowhere in literature I have come across this definition of VIF, as VIF
> is always computed at individual variable level. Though the structure is
> almost the same, R^2 in theoretical VIF is the partial correlation
> coefficient.
>
> ?I only came aware when lots of freshers from non statistics background I
> interviewed for analytics position answered that the only definition of VIF
> they know is 1/(1 - Coeff. of Determination), and there is a R package
> which calculates VIF like that.
>
> After researched I found that such a function indeed exist in fmsb package.
>
> Please help me understand has an alternate definition of Variance Inflation
> Factor has ever emerged in theory? Does it really make sense to have VIF at
> a model level, as it does not help in solving the problem of
> multicollinearity during model building.
>
> And if I am right, what steps I should do about it.
>
>
Dear Anindya,

to me it seems clear from the example on the help page that VIF() is not 
intended to be applied to the model of interest, but to separate models 
for each covariable.

The model of interest in the example is
# the target multiple regression model
res <- lm(Ozone ~ Wind+Temp+Solar.R, data=airquality)

The VIF is calculated on submodels for each covariate.
# checking multicolinearity for independent variables.
VIF(lm(Wind ~ Temp+Solar.R, data=airquality))
VIF(lm(Temp ~ Wind+Solar.R, data=airquality))
VIF(lm(Solar.R ~ Wind+Temp, data=airquality))

Does that agree with your usual definition of a variance inflation factor?

best regards,

Heinz


From rmscriven at gmail.com  Thu Dec 31 05:24:38 2015
From: rmscriven at gmail.com (Richard M. Scriven)
Date: Wed, 30 Dec 2015 20:24:38 -0800
Subject: [R] unname() not working as stated in documentation
Message-ID: <CADgD3AtjGrfLw4cxj1Su+LT=Eu=ZoUGReqqztU2ushcsAXvYWA@mail.gmail.com>

Hi all,

In help(unname), for the "force" argument, it states that

"if true, the dimnames (names and row names) are removed even from
data.frames."

However, when I use unname() to remove all the names from mtcars, it fails.

> unname(mtcars, force = TRUE)
Error in `dimnames<-.data.frame`(`*tmp*`, value = NULL) :
  invalid 'dimnames' given for data frame

This traces back to `dimnames<-.data.frame` not accepting NULL as a
replacement value.  So my question is, do you think this is a bug worth
reporting as such, or simply a mistake in the documentation?

Thank you for your time.

-- 
*Richard M Scriven*
*Economics, **Applied Statistics*
*Undergraduate **Class of 2014*

* University of California, DavisTel: (925) 487-5034*

	[[alternative HTML version deleted]]


From shivi.bhatia at safexpress.com  Thu Dec 31 08:26:38 2015
From: shivi.bhatia at safexpress.com (SHIVI BHATIA)
Date: Thu, 31 Dec 2015 12:56:38 +0530
Subject: [R] Dput Help in R
In-Reply-To: <56840C05.3070401@gmail.com>
References: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
	<56840C05.3070401@gmail.com>
Message-ID: <005101d1439c$87b26670$97173350$@safexpress.com>

Hi Duncan,
Please find the dput from the data.

ab<-read.csv("collection_last.csv",header=TRUE)
y<-ab[1:10,]


ab<- "2,458", "2,461", "2,462", "2,463", "2,464", "2,465", "2,468",
"2,469", "2,470", "2,473", "2,474", "2,475", "2,476", "2,477",
"2,478", "2,479", "2,480", "2,483", "2,484,267", "2,485",
"2,486", "2,487", "2,490", "2,491", "2,491,176", "2,492",
"2,494", "2,495,976", "2,496", "2,497", "2,498", "2,499",
"2,500", "2,500,001", "2,501", "2,502", "2,503", "2,504",
"2,506", "2,507", "2,508", "2,509", "2,510", "2,511", "2,512",
"2,513", "2,514", "2,515", "2,516", "2,517", "2,519", "2,520",
"2,521", "2,523", "2,523,257", "2,524", "2,525", "2,526",
"2,527", "2,528", "2,529", "2,530", "2,531", "2,535", "2,536",
"2,538", "2,539", "2,540", "2,542", "2,543", "2,544", "2,545",
"2,546", "2,547", "2,549", "2,550", "2,551", "2,553", "2,554",
"2,556", "2,557", "2,558", "2,559", "2,560", "2,561", "2,563",
"2,564", "2,565", "2,566", "2,567", "2,570", "2,572", "2,574",
"2,576", "2,577", "2,578", "2,579", "2,580", "2,582", "2,583",
"2,584", "2,585", "2,588", "2,589", "2,590", "2,591", "2,592",
"2,593", "2,594", "2,596", "2,599", "2,600", "2,601", "2,602",
"2,604", "2,605", "2,606", "2,607", "2,609", "2,611", "2,612",
"2,613", "2,614", "2,615", "2,616", "2,617", "2,618", "2,619",
"2,620", "2,621", "2,622", "2,624", "2,626", "2,627", "2,628",
"2,628,385", "2,629", "2,630", "2,633", "2,634", "2,636",
"2,637", "2,638", "2,640", "2,642", "2,644", "2,647", "2,648",
"2,649", "2,650", "2,651", "2,654", "2,655", "2,656", "2,658",
"2,660", "2,661", "2,663", "2,665", "2,666", "2,667", "2,668",
"2,670", "2,671", "2,672", "2,673", "2,674", "2,676", "2,677",
"2,678", "2,679", "2,679,505", "2,680", "2,682", "2,684",
"2,687", "2,688", "2,689", "2,690", "2,692", "2,694", "2,695",
"2,696", "2,697", "2,699", "2,700", "2,702", "2,703", "2,705",
"2,706", "2,707", "2,708", "2,709", "2,710", "2,711", "2,712",
"2,713", "2,714", "2,715", "2,716", "2,717", "2,718", "2,720",
"2,721", "2,722", "2,723", "2,727", "2,728", "2,730", "2,732",
"2,733", "2,736", "2,737", "2,738", "2,739", "2,742", "2,744",
"2,747", "2,748", "2,749", "2,750", "2,752", "2,753", "2,754",
"2,758", "2,759", "2,760", "2,761", "2,765,189", "2,766",
"2,767", "2,770", "2,772", "2,773", "2,774", "2,776", "2,778",
"2,780", "2,783", "2,784", "2,785", "2,786", "2,787", "2,789",
"2,790", "2,792", "2,793", "2,794", "2,795", "2,797", "2,798",
"2,799", "2,800", "2,803", "2,804", "2,806", "2,808", "2,808,003",
"2,809", "2,810", "2,812", "2,813", "2,814", "2,816", "2,818",
"2,820", "2,824", "2,825", "2,826", "2,832", "2,835", "2,839,850",
"2,840", "2,841", "2,842", "2,844", "2,845", "2,846", "2,847",
"2,848", "2,850", "2,851", "2,852", "2,853", "2,855", "2,856",
"2,857", "2,858", "2,859", "2,861", "2,864", "2,865", "2,866",
"2,867", "2,869", "2,870", "2,873", "2,874", "2,875", "2,876",
"2,877", "2,878", "2,879", "2,880", "2,883", "2,884", "2,885",
"2,886", "2,890", "2,891", "2,892", "2,893", "2,894", "2,895",
"2,896", "2,897", "2,898", "2,899", "2,902", "2,903", "2,904",
"2,905", "2,907", "2,908", "2,908,956", "2,910", "2,911",
"2,912", "2,913", "2,916", "2,917", "2,922", "2,923", "2,924",
"2,925", "2,926", "2,929", "2,930", "2,931", "2,932", "2,933",
"2,934", "2,936", "2,937", "2,939", "2,941", "2,943", "2,944",
"2,946", "2,947", "2,948", "2,950", "2,951", "2,953", "2,954",
"2,955", "2,957", "2,959", "2,960", "2,961", "2,965", "2,967",
"2,968", "2,970", "2,971", "2,973", "2,975", "2,976", "2,979",
"2,981", "2,982", "2,983", "2,987", "2,988", "2,989", "2,990",
"2,991", "2,992", "2,993", "2,994", "2,996", "2,997", "2,998",
"2,999", "20", "20,000", "20,001", "20,003", "20,004", "20,028",
"20,035", "20,054", "20,066", "20,071", "20,077", "20,079",
"20,088", "20,101", "20,104", "20,116", "20,120", "20,126",
"20,151", "20,153", "20,157", "20,174", "20,176", "20,190",
"20,191", "20,196", "20,199", "20,213", "20,214", "20,217",
"20,225", "20,257", "20,262", "20,263", "20,288", "20,294",
"20,307", "20,320", "20,325", "20,349", "20,356", "20,375",
"20,385", "20,387", "20,401", "20,405", "20,412", "20,425",
"20,443", "20,462", "20,517", "20,525", "20,526", "20,532",
"20,542", "20,547", "20,557", "20,576,265", "20,601", "20,612",
"20,623", "20,625", "20,641", "20,657", "20,690", "20,691",
"20,693", "20,700", "20,712", "20,725", "20,728", "20,752,792",
"20,754", "20,773", "20,779", "20,780", "20,784", "20,792",
"20,821", "20,830", "20,873", "20,882", "20,890", "20,900",
"20,906", "20,947", "20,956", "20,964", "20,979", "20,980",
"200", "200,000", "200,014", "200,023", "200,058", "200,105",
"200,372", "200,476", "200,736", "201", "201,583", "201,759",
"201,844", "202", "202,093", "202,278", "202,414", "202,753",
"203", "203,380", "203,388", "204", "204,184", "204,403",
"204,760", "204,846", "204,922", "205", "205,000", "205,307",
"205,559", "206", "206,705", "206,916", "207", "207,367",
"208", "208,096", "208,267", "208,284", "209", "209,075",
"209,355", "209,653", "21", "21,010", "21,042", "21,044",
"21,061", "21,083", "21,084", "21,087", "21,090", "21,095",
"21,106", "21,110", "21,147", "21,149", "21,163", "21,186",
"21,215", "21,238", "21,246", "21,257", "21,265", "21,282",
"21,292", "21,304", "21,317", "21,320", "21,324", "21,349",
"21,371", "21,377", "21,382", "21,407", "21,428", "21,445",
"21,450", "21,456", "21,480", "21,487", "21,498", "21,519",
"21,555", "21,556", "21,559", "21,613", "21,614", "21,663",
"21,666", "21,673", "21,681", "21,695", "21,698", "21,715",
"21,732", "21,733", "21,739", "21,743", "21,752", "21,755",
"21,785", "21,820", "21,822", "21,827", "21,841", "21,845",
"21,857", "21,858", "21,860", "21,864", "21,883", "21,912",
"21,914", "21,921", "21,937", "21,940", "21,941", "21,945",
"21,948", "21,966", "21,970", "21,981", "21,991", "21,992",
"210", "210,181", "210,386", "210,556", "210,760", "210,926",
"211", "211,014", "211,361", "211,644", "212", "212,086",
"212,117", "212,348", "212,651", "212,775", "212,874", "212,890",
"212,925", "213", "213,095", "213,153", "213,723", "214",
"214,186", "214,196", "214,216", "214,280", "214,355", "214,373",
"214,511", "215", "215,146", "215,266", "215,270", "215,543",
"216", "216,655", "216,787", "217", "217,075", "217,709",
"217,851", "218", "218,073", "218,266", "219", "219,244",
"219,285", "219,352", "219,458", "219,566", "22", "22,000",
"22,004", "22,020", "22,046", "22,058", "22,070", "22,083",
"22,175", "22,223", "22,245", "22,275", "22,285", "22,301",
"22,304", "22,308", "22,320", "22,351", "22,352", "22,358",
"22,374", "22,388", "22,435", "22,448", "22,449", "22,457",
"22,462", "22,471", "22,473", "22,482", "22,493", "22,499",
"22,500", "22,523", "22,531", "22,577", "22,594", "22,601",
"22,613", "22,615", "22,652", "22,660", "22,666", "22,669",
"22,679", "22,734", "22,743", "22,758", "22,776", "22,785",
"22,791", "22,829", "22,911", "22,917", "22,928", "22,953",
"22,981", "22,994", "220", "220,240", "220,297", "220,845",
"221", "221,108", "221,113", "221,311", "221,536", "222",
"222,378", "222,834", "222,877", "223", "223,016", "223,457",
"223,729", "224", "224,258", "224,273", "224,551", "224,673",
"224,735", "224,933", "224,948", "225", "225,000", "225,076",
"225,118", "225,388", "225,398", "225,574", "225,708", "225,759",
"225,779", "225,808", "225,933", "226", "226,210", "226,864",
"226,871", "226,876", "227", "227,053", "227,361", "227,574",
"228", "228,188", "228,540", "228,547", "228,697", "228,968",
"229", "229,182", "229,445", "229,732", "229,893", "229,967",
"23", "23,012", "23,019", "23,038", "23,041", "23,062", "23,068",
"23,081", "23,098", "23,110", "23,117", "23,132", "23,164",
"23,198", "23,200", "23,207", "23,225", "23,234", "23,239",
"23,244", "23,257", "23,299", "23,304", "23,316", "23,324",
"23,334", "23,342", "23,351", "23,363", "23,367", "23,373",
"23,375", "23,387", "23,390", "23,404", "23,412", "23,420",
"23,449", "23,454", "23,460", "23,468", "23,473", "23,484",
"23,490", "23,549", "23,581", "23,633", "23,640", "23,641",
"23,671", "23,674", "23,691", "23,694", "23,698", "23,707",
"23,712", "23,730", "23,795", "23,805", "23,833", "23,838",
"23,847", "23,851", "23,871", "23,881", "23,902", "23,924",
"23,926", "23,935", "23,940", "23,984", "230", "230,005",
"230,568", "230,675", "230,921", "231", "231,225", "232",
"232,397", "232,420", "232,744", "233", "233,216", "233,481",
"234", "234,226", "234,462", "234,699", "234,919", "235",
"235,095", "235,577", "235,779", "236", "236,072", "236,809",
"237", "237,094", "237,564", "238", "238,949", "239", "239,135",
"239,748", "239,831", "24", "24,015", "24,017", "24,026",
"24,045", "24,057", "24,070", "24,098", "24,112", "24,130",
"24,135", "24,144", "24,166", "24,178", "24,185", "24,205",
"24,208", "24,219", "24,230", "24,276", "24,290", "24,296",
"24,345", "24,352", "24,384", "24,413", "24,432", "24,441",
"24,468", "24,472", "24,480", "24,490", "24,519", "24,523",
"24,554", "24,574", "24,616", "24,673", "24,678", "24,679",
"24,705", "24,714", "24,746", "24,781", "24,825", "24,840",
"24,876", "24,887", "24,899", "24,907", "24,944", "24,960",
"24,979", "240", "240,000", "240,479", "240,548", "241",
"241,657", "241,980", "242", "242,202", "242,450", "242,936",
"243", "243,126", "243,142", "243,151", "243,223", "243,379",
"244", "244,118", "244,136", "244,289", "244,690", "244,950",
"245", "245,578", "246", "246,192", "246,262", "246,291",
"246,292", "246,984", "247", "247,063", "247,183", "247,528",
"247,778", "247,784", "248", "248,102", "248,405", "249",
"249,169", "249,173", "249,530", "25", "25,000", "25,003",
"25,013", "25,018", "25,039", "25,057", "25,070", "25,080",
"25,089", "25,106", "25,116", "25,162", "25,166", "25,168",
"25,205", "25,209", "25,231", "25,258", "25,273", "25,318",
"25,345", "25,354", "25,373", "25,384", "25,388", "25,402",
"25,476", "25,488", "25,529", "25,535", "25,556", "25,572",
"25,573", "25,581", "25,659", "25,662", "25,685", "25,696",
"25,706", "25,722", "25,727", "25,749", "25,751", "25,760",
"25,782", "25,792", "25,820", "25,841", "25,858", "25,864",
"25,876", "25,879", "25,881", "25,886", "25,905", "25,929",
"25,954", "25,965", "250", "250,000", "250,275", "250,517",
"250,954", "251", "251,329", "252", "252,000", "252,890",
"253", "253,073", "253,983", "254", "254,151", "254,822",
"255", "255,388", "255,737", "255,951", "256", "257", "257,111",
"257,284", "257,475", "257,564", "257,585", "257,682", "258",
"258,754", "258,814", "258,843", "259", "26", "26,000", "26,010",
"26,017", "26,026", "26,063", "26,111", "26,123", "26,127",
"26,130", "26,169", "26,183", "26,205", "26,216", "26,220",
"26,237", "26,239", "26,274", "26,275", "26,296", "26,298",
"26,311", "26,314", "26,332", "26,367", "26,395", "26,429",
"26,452", "26,464", "26,487", "26,495", "26,500", "26,505",
"26,506", "26,511", "26,608", "26,636", "26,645", "26,745",
"26,758", "26,770", "26,781", "26,834", "26,873", "26,880",
"26,889", "26,953", "26,981", "26,982", "260", "260,000",
"260,002", "260,964", "261", "262", "262,378", "262,477",
"262,724", "262,838", "263", "263,615", "263,638", "263,969",
"264", "264,000", "264,967", "265", "265,366", "265,432",
"265,536", "265,656", "265,691", "266", "266,164", "266,172",
"266,327", "266,738", "267", "267,019", "267,232", "267,349",
"267,856", "268", "269", "269,876", "27", "27,000", "27,069",
"27,077", "27,112", "27,126", "27,131", "27,204", "27,214",
"27,230", "27,249", "27,261", "27,280", "27,283", "27,318",
"27,332", "27,339", "27,342", "27,350", "27,351", "27,361",
"27,376", "27,398", "27,407", "27,420", "27,465", "27,472",
"27,523", "27,534", "27,584", "27,610", "27,616", "27,638",
"27,656", "27,767", "27,846", "27,861", "27,863", "27,895",
"27,934", "27,935", "27,939", "27,943", "27,965", "27,973",
"27,990", "270", "270,109", "270,130", "270,340", "270,623",
"270,662", "271", "271,153", "271,515", "271,616", "271,709",
"272", "272,672", "272,834", "273", "273,386", "273,866",
"274", "274,170", "275", "275,227", "276", "276,000", "277",
"277,464", "278", "279", "279,002", "279,119", "279,448",
"279,871", "279,947", "28", "28,000", "28,008", "28,010",
"28,021", "28,026", "28,051", "28,070", "28,090", "28,124",
"28,144", "28,162", "28,193", "28,250", "28,251", "28,264",
"28,276", "28,278", "28,312", "28,342", "28,343", "28,412",
"28,445", "28,456", "28,487", "28,507", "28,595", "28,670",
"28,672", "28,701", "28,703", "28,706", "28,757", "28,759",
"28,764", "28,984", "28,990", "280", "281", "281,412", "282",
"282,375", "282,512", "283", "283,951", "284", "285", "285,082",
"285,170", "286", "286,123", "286,209", "286,632", "287",
"287,456", "287,612", "288", "288,404", "288,827", "289",
"289,903", "29", "29,002", "29,022", "29,030", "29,053",
"29,090", "29,109", "29,164", "29,198", "29,226", "29,269",
"29,300", "29,325", "29,331", "29,354", "29,356", "29,369",
"29,418", "29,440", "29,445", "29,482", "29,557", "29,597",
"29,600", "29,612", "29,622", "29,626", "29,632", "29,697",
"29,753", "29,785", "29,794", "29,858", "29,876", "29,911",
"29,928", "29,945", "29,949", "29,956", "29,959", "29,976",
"29,985", "290", "290,000", "290,154", "291", "291,202",
"291,970", "292", "293", "293,227", "293,316", "293,368",
"294", "294,191", "294,674", "294,762", "294,816", "295",
"296", "297", "297,884", "298", "298,408", "298,807", "298,848",
"299", "3", "3,000", "3,001", "3,002", "3,004", "3,006",
"3,007", "3,008,846", "3,010", "3,012", "3,013", "3,014",
"3,015", "3,016", "3,019", "3,023", "3,024", "3,025", "3,026",
"3,027", "3,028", "3,029", "3,030", "3,033", "3,034", "3,035",
"3,036", "3,037", "3,038", "3,039", "3,041", "3,042", "3,047",
"3,048", "3,051", "3,052", "3,054", "3,056", "3,058", "3,059",
"3,060", "3,063", "3,064", "3,065", "3,066", "3,069", "3,071",
"3,076", "3,077", "3,078", "3,079", "3,080", "3,082", "3,084",
"3,085", "3,086", "3,088", "3,090", "3,091", "3,094", "3,096",
"3,097", "3,098", "3,099", "3,100", "3,101", "3,103", "3,104",
"3,107", "3,108", "3,110", "3,112", "3,113", "3,115", "3,116",
"3,118", "3,119", "3,120", "3,121", "3,125", "3,126", "3,127",
"3,128", "3,131", "3,132", "3,133", "3,134", "3,135,396",
"3,136", "3,139,265", "3,143", "3,144", "3,146", "3,147",
"3,148", "3,149", "3,150", "3,157", "3,159", "3,160", "3,161",
"3,162", "3,164", "3,168", "3,169", "3,170", "3,171", "3,172",
"3,173", "3,175", "3,177", "3,178", "3,179", "3,180", "3,181",
"3,182", "3,183", "3,184,325", "3,185", "3,186", "3,187",
"3,188", "3,189", "3,190", "3,191", "3,192", "3,194", "3,195",
"3,196", "3,199", "3,200", "3,201", "3,205", "3,206", "3,208",
"3,209", "3,210", "3,212", "3,214", "3,218", "3,220", "3,221",
"3,224", "3,225", "3,226", "3,228", "3,233", "3,236", "3,239",
"3,240", "3,243", "3,246", "3,247", "3,249", "3,250", "3,251",
"3,254", "3,256", "3,257", "3,260", "3,261", "3,262", "3,263",
"3,266", "3,267", "3,270", "3,272", "3,274", "3,276", "3,277",
"3,278", "3,279", "3,280", "3,281", "3,282", "3,285", "3,287",
"3,288", "3,289", "3,290", "3,292", "3,293", "3,294", "3,295",
"3,296", "3,300", "3,300,000", "3,301", "3,303", "3,305",
"3,306", "3,310", "3,313", "3,314", "3,317", "3,318", "3,321",
"3,323", "3,324", "3,325", "3,327", "3,328", "3,330", "3,332",
"3,332,653", "3,334", "3,335", "3,336", "3,338", "3,340",
"3,341", "3,343", "3,347", "3,348", "3,350", "3,353", "3,355",
"3,356", "3,359", "3,360", "3,361", "3,363", "3,365", "3,367",
"3,368", "3,372", "3,375", "3,376", "3,377", "3,380", "3,381",
"3,383", "3,384", "3,384,046", "3,385", "3,386", "3,388",
"3,389", "3,389,352", "3,392", "3,393", "3,396", "3,398",
"3,400", "3,401", "3,405", "3,406", "3,409", "3,410", "3,412",
"3,413", "3,414", "3,419", "3,420", "3,422", "3,424", "3,425",
"3,426", "3,428", "3,429", "3,430", "3,433", "3,435", "3,436",
"3,437", "3,438", "3,439", "3,442", "3,444", "3,446", "3,447",
"3,448", "3,450", "3,460", "3,461", "3,463", "3,464", "3,465",
"3,466", "3,470", "3,473", "3,475", "3,476", "3,477", "3,478",
"3,481", "3,482", "3,483", "3,486", "3,487", "3,488", "3,489",
"3,491", "3,494", "3,495", "3,496", "3,500", "3,504", "3,506",
"3,509", "3,510", "3,511", "3,514", "3,517", "3,518", "3,519",
"3,525", "3,526", "3,527", "3,528", "3,529", "3,530", "3,537",
"3,538", "3,539", "3,540", "3,541", "3,543", "3,544", "3,546",
"3,548", "3,549", "3,550", "3,552", "3,554", "3,556", "3,557",
"3,560", "3,563", "3,565", "3,566", "3,567", "3,569", "3,570",
"3,571", "3,574", "3,575", "3,579", "3,580", "3,582", "3,583",
"3,584", "3,585", "3,588", "3,589", "3,590", "3,591", "3,593",
"3,595", "3,596", "3,597", "3,600", "3,601", "3,603", "3,605",
"3,606", "3,607", "3,610", "3,612", "3,613", "3,615", "3,617",
"3,618", "3,620", "3,621,805", "3,623", "3,624", "3,625",
"3,627", "3,628,570", "3,631,857", "3,634", "3,635", "3,636",
"3,638", "3,639,265", "3,641", "3,643", "3,645", "3,647",
"3,648", "3,649", "3,650", "3,654", "3,656", "3,658", "3,662",
"3,663", "3,666", "3,668", "3,669", "3,670", "3,671", "3,674",
"3,675", "3,677", "3,678", "3,680", "3,681", "3,683", "3,684",
"3,685", "3,690", "3,692", "3,693", "3,694", "3,695", "3,699",
"3,700", "3,700,390", "3,701", "3,703", "3,706", "3,707",
"3,709", "3,710", "3,712", "3,713", "3,714", "3,715", "3,716",
"3,718", "3,719", "3,720", "3,722", "3,724", "3,725", "3,726",
"3,728", "3,730", "3,732", "3,733", "3,736", "3,737", "3,738",
"3,739,265", "3,740", "3,741", "3,742", "3,744", "3,746",
"3,750", "3,753", "3,754", "3,755", "3,756", "3,758", "3,759",
"3,760", "3,762", "3,763", "3,764", "3,769", "3,771", "3,772",
"3,773", "3,775", "3,777", "3,780", "3,781", "3,782", "3,783",
"3,786", "3,789", "3,791", "3,795", "3,800", "3,801", "3,803",
"3,806", "3,807", "3,808", "3,810", "3,811", "3,812", "3,813",
"3,815", "3,817", "3,818", "3,821", "3,823", "3,824", "3,825",
"3,826", "3,827", "3,829", "3,830", "3,831", "3,836", "3,837",
"3,838", "3,839", "3,839,265", "3,844", "3,845", "3,846,942",
"3,849", "3,850", "3,851", "3,853", "3,854", "3,855", "3,857",
"3,859", "3,860", "3,863", "3,865", "3,867", "3,868", "3,869",
"3,870", "3,871", "3,873", "3,878", "3,879", "3,881", "3,882",
"3,885", "3,893", "3,894", "3,895", "3,897", "3,899", "3,900",
"3,903", "3,906", "3,907", "3,912", "3,914", "3,920", "3,921",
"3,922", "3,926", "3,928,443", "3,933", "3,936", "3,938",
"3,939", "3,940", "3,942", "3,943", "3,944", "3,947", "3,950",
"3,956", "3,961", "3,962", "3,966", "3,968", "3,969", "3,971",
"3,976", "3,978", "3,980", "3,980,495", "3,980,758", "3,982",
"3,984", "3,986", "3,989", "3,989,249", "3,990", "3,990,681",
"3,991", "3,991,146", "3,992", "3,994", "3,995", "3,999",
"30", "30,000", "30,032", "30,058", "30,059", "30,061", "30,064",
"30,067", "30,112", "30,113", "30,172", "30,186", "30,259",
"30,263", "30,282", "30,290", "30,334", "30,337", "30,347",
"30,349", "30,388", "30,404", "30,455", "30,460", "30,462",
"30,463", "30,485", "30,534", "30,545", "30,571", "30,629",
"30,645", "30,649", "30,651", "30,673", "30,710", "30,747",
"30,771", "30,772", "30,811", "30,812", "30,847", "30,850",
"30,864", "30,881", "30,946", "30,963", "30,980", "300",
"300,000", "301", "301,727", "302", "303", "304", "304,314",
"304,925", "304,946", "305", "305,197", "305,407", "305,491",
"305,751", "306", "306,353", "306,357", "306,526", "307",
"308", "308,402", "308,754", "308,910", "309", "309,353",
"309,601", "309,785", "31", "31,001", "31,030", "31,066",
"31,071", "31,100", "31,118", "31,129", "31,137", "31,150",
"31,182", "31,201", "31,214", "31,257", "31,289", "31,291",
"31,335", "31,364", "31,435", "31,451", "31,455", "31,460",
"31,461", "31,475", "31,485", "31,495", "31,496", "31,502",
"31,503", "31,534", "31,567", "31,583", "31,606", "31,632",
"31,638", "31,662", "31,714", "31,763", "31,774", "31,801",
"31,817", "31,846", "31,904", "31,932", "31,938", "31,941",
"31,961", "31,972", "310", "310,230", "310,596", "310,617",
"311", "312", "312,109", "312,701", "313", "314", "314,132",
"315", "315,137", "315,753", "315,953", "316", "316,164",
"317", "317,040", "317,317", "318", "318,278", "318,589",
"319", "319,280", "319,411", "319,693", "32", "32,001", "32,087",
"32,089", "32,101", "32,107", "32,109", "32,126", "32,134",
"32,178", "32,197", "32,225", "32,242", "32,254", "32,275",
"32,285", "32,337", "32,343", "32,370", "32,458", "32,461",
"32,464", "32,490", "32,553", "32,584", "32,603", "32,628",
"32,760", "32,844", "32,845", "32,860", "32,873", "32,889",
"32,901", "32,927", "32,931", "32,980", "32,986", "320",
"320,000", "320,920", "321", "321,490", "322", "322,250",
"323", "323,445", "323,932", "324", "324,342", "324,523",
"325", "325,769", "326", "326,079", "326,305", "327", "327,333",
"327,936", "328", "328,093", "328,397", "328,644", "329",
"329,101", "329,175", "329,352", "329,923", "33", "33,000",
"33,024", "33,030", "33,035", "33,062", "33,064", "33,080",
"33,082", "33,088", "33,098", "33,109", "33,199", "33,227",
"33,232", "33,240", "33,248", "33,272", "33,280", "33,320",
"33,325", "33,344", "33,408", "33,428", "33,434", "33,488",
"33,519", "33,523", "33,540", "33,559", "33,562", "33,580",
"33,624", "33,651", "33,690", "33,698", "33,724", "33,730",
"33,831", "33,902", "33,917", "33,955", "33,962", "330",
"330,168", "330,505", "330,641", "330,937", "331", "331,274",
"332", "332,349", "333", "333,741", "334", "335", "335,770",
"335,870", "336", "336,419", "336,658", "337", "337,245",
"337,350", "337,855", "337,875", "338", "338,338", "338,507",
"338,700", "339", "339,550", "339,675", "339,753", "34",
"34,002", "34,003", "34,006", "34,018", "34,040", "34,047",
"34,099", "34,129", "34,202", "34,207", "34,214", "34,216",
"34,228", "34,232", "34,243", "34,250", "34,276", "34,292",
"34,295", "34,329", "34,348", "34,453", "34,517", "34,560",
"34,624", "34,625", "34,641", "34,667", "34,704", "34,722",
"34,743", "34,756", "34,760", "34,762", "34,792", "34,827",
"34,909", "34,934", "34,939", "34,946", "34,954", "34,964",
"34,975", "34,980", "34,988", "340", "340,777", "341", "341,098",
"341,213", "341,321", "342", "342,212", "343", "343,435",
"344", "344,184", "344,374", "345", "345,000", "345,548",
"346", "347", "347,413", "347,423", "347,699", "348", "348,936",
"348,986", "349", "349,377", "349,380", "35", "35,000", "35,008",
"35,009", "35,012", "35,054", "35,082", "35,104", "35,174",
"35,180", "35,266", "35,340", "35,373", "35,392", "35,402",
"35,437", "35,439", "35,462", "35,481", "35,528", "35,560",
"35,564", "35,578", "35,630", "35,682", "35,731", "35,733",
"35,737", "35,756", "35,823", "35,948", "35,951", "35,956",
"35,959", "350", "350,000", "350,445", "350,614", "350,924",
"351", "351,282", "351,401", "351,769", "352", "352,505",
"353", "353,794", "353,938", "354", "355", "355,666", "355,924",
"355,951", "356", "356,516", "356,517", "356,638", "357",
"357,164", "358", "359", "359,697", "359,829", "359,857",
"36", "36,077", "36,101", "36,156", "36,193", "36,210", "36,230",
"36,238", "36,275", "36,300", "36,466", "36,481", "36,512",
"36,545", "36,549", "36,577", "36,624", "36,631", "36,653",
"36,679", "36,697", "36,772", "36,790", "36,808", "36,900",
"36,941", "36,950", "36,972", "360", "360,924", "361", "362",
"362,195", "363", "363,076", "364", "364,884", "365", "365,114",
"365,253", "365,979", "366", "366,234", "366,930", "367",
"367,164", "367,319", "368", "368,178", "368,852", "369",
"369,660", "369,870", "37", "37,000", "37,080", "37,101",
"37,111", "37,122", "37,170", "37,199", "37,205", "37,226",
"37,229", "37,259", "37,261", "37,298", "37,305", "37,328",
"37,365", "37,389", "37,401", "37,480", "37,489", "37,500",
"37,506", "37,527", "37,667", "37,721", "37,760", "37,809",
"37,860", "37,885", "37,924", "37,930", "37,943", "37,981",
"370", "371", "372", "372,706", "372,761", "373", "374",
"374,107", "375", "375,530", "375,531", "375,618", "375,738",
"375,988", "376", "376,118", "376,406", "376,584", "377",
"377,082", "377,309", "377,951", "377,952", "378", "378,415",
"379", "379,012", "379,248", "379,547", "38", "38,013", "38,026",
"38,121", "38,124", "38,185", "38,234", "38,265", "38,289",
"38,307", "38,349", "38,363", "38,412", "38,416", "38,495",
"38,606", "38,643", "38,648", "38,714", "38,727", "38,736",
"38,776", "38,780", "38,794", "38,809", "38,852", "38,862",
"38,909", "38,954", "38,992", "380", "380,000", "380,586",
"381", "382", "383", "383,145", "383,728", "384", "385",
"386", "387", "387,870", "388", "388,322", "388,589", "388,687",
"388,762", "389", "39", "39,035", "39,095", "39,178", "39,180",
"39,200", "39,214", "39,289", "39,291", "39,365", "39,372",
"39,396", "39,413", "39,449", "39,460", "39,500", "39,534",
"39,540", "39,560", "39,600", "39,665", "39,700", "39,800",
"39,858", "39,860", "39,864", "39,875", "39,876", "39,922",
"39,923", "390", "390,381", "390,909", "391", "391,199",
"392", "393", "393,262", "393,802", "394", "394,969", "395",
"395,462", "395,841", "396", "396,358", "396,743", "396,815",
"397", "397,626", "398", "398,336", "398,763", "399", "399,326",
"4", "4,000", "4,002", "4,003", "4,006", "4,007", "4,009",
"4,011", "4,012", "4,017", "4,021", "4,022", "4,026", "4,028",
"4,029", "4,031", "4,033", "4,033,984", "4,036", "4,042",
"4,043", "4,045", "4,046", "4,047", "4,050", "4,052", "4,058",
"4,060", "4,061", "4,062", "4,065", "4,066", "4,069", "4,070",
"4,071", "4,073", "4,076", "4,077", "4,078", "4,079", "4,082",
"4,083", "4,084", "4,085", "4,088", "4,090", "4,092", "4,094",
"4,095", "4,096", "4,096,451", "4,097", "4,098", "4,100",
"4,104", "4,105", "4,106", "4,107", "4,113", "4,114", "4,118",
"4,119", "4,120,981", "4,121", "4,122", "4,125", "4,127",
"4,129,366", "4,131", "4,136", "4,138", "4,139", "4,140",
"4,142", "4,144", "4,146", "4,147", "4,149", "4,150", "4,152",
"4,153", "4,156", "4,157", "4,159,363", "4,161", "4,163",
"4,166", "4,167", "4,168", "4,169", "4,172,565", "4,173",
"4,174", "4,175", "4,177", "4,178", "4,181", "4,182", "4,183",
"4,184", "4,187", "4,189", "4,191", "4,193", "4,194", "4,195",
"4,196", "4,200", "4,201", "4,204", "4,209", "4,210", "4,211",
"4,214", "4,215", "4,216", "4,217", "4,218", "4,219", "4,221",
"4,222", "4,223", "4,226", "4,227", "4,228,443", "4,230",
"4,233", "4,234", "4,235", "4,237", "4,239", "4,241", "4,242",
"4,244", "4,247", "4,250", "4,253", "4,254", "4,255", "4,257",
"4,261", "4,262", "4,263", "4,266", "4,268", "4,272", "4,274",
"4,275", "4,277", "4,280", "4,281", "4,284", "4,287", "4,289",
"4,290", "4,291", "4,292", "4,295", "4,296", "4,298", "4,300",
"4,306", "4,309", "4,315", "4,316", "4,318", "4,319", "4,321",
"4,323", "4,326", "4,327", "4,328", "4,331", "4,333", "4,334",
"4,340", "4,341", "4,344", "4,345", "4,346", "4,347", "4,349",
"4,353", "4,354", "4,358", "4,359", "4,361", "4,362", "4,363",
"4,365", "4,368", "4,369", "4,372", "4,374", "4,375", "4,376",
"4,377", "4,379", "4,382", "4,386", "4,387", "4,389", "4,390",
"4,394", "4,396", "4,399", "4,400", "4,404,618", "4,407",
"4,410", "4,411", "4,412", "4,415", "4,416", "4,418", "4,421",
"4,425", "4,426", "4,427", "4,428", "4,428,443", "4,432",
"4,433", "4,435", "4,437", "4,439", "4,440", "4,444", "4,445",
"4,446", "4,447", "4,448", "4,452", "4,453", "4,455", "4,456",
"4,458", "4,460", "4,463", "4,466", "4,468", "4,470", "4,471",
"4,472", "4,474", "4,476", "4,484", "4,485", "4,486,590",
"4,489", "4,490", "4,491", "4,492", "4,493", "4,494", "4,496",
"4,497", "4,500", "4,501", "4,502", "4,504", "4,505", "4,506",
"4,507", "4,510", "4,511", "4,512", "4,515", "4,518", "4,520",
"4,521", "4,522", "4,523", "4,526", "4,529", "4,530", "4,532",
"4,535", "4,539", "4,540", "4,541", "4,542", "4,544", "4,545",
"4,546", "4,548", "4,549", "4,550", "4,552", "4,556", "4,557",
"4,560", "4,563", "4,564", "4,565", "4,567", "4,569", "4,570",
"4,576", "4,577", "4,578", "4,582", "4,587", "4,589", "4,590",
"4,591", "4,592", "4,596", "4,603", "4,608", "4,609", "4,610",
"4,616", "4,619", "4,620", "4,621", "4,623", "4,626", "4,630",
"4,631", "4,632", "4,639", "4,640", "4,641", "4,645", "4,648",
"4,650", "4,652", "4,662", "4,664", "4,665", "4,666", "4,672",
"4,673", "4,675", "4,680", "4,681", "4,682", "4,687", "4,689",
"4,690", "4,691", "4,692", "4,693", "4,697", "4,698", "4,700",
"4,706", "4,711", "4,713", "4,718", "4,719", "4,720", "4,723",
"4,726", "4,729", "4,730", "4,731", "4,733", "4,736", "4,738",
"4,739", "4,740", "4,742", "4,743", "4,744", "4,749", "4,754",
"4,760", "4,763", "4,765", "4,767", "4,768", "4,773", "4,774",
"4,775", "4,776", "4,777", "4,778", "4,780", "4,786", "4,789",
"4,792", "4,793", "4,794", "4,795", "4,797", "4,799", "4,800",
"4,812", "4,820", "4,823", "4,827", "4,828", "4,830", "4,832",
"4,834", "4,838", "4,839", "4,840", "4,841", "4,843", "4,844",
"4,851", "4,856", "4,864", "4,866", "4,870", "4,871,309",
"4,873", "4,876", "4,881", "4,882", "4,888", "4,889", "4,890",
"4,893", "4,894", "4,895", "4,896", "4,900", "4,902", "4,903",
"4,905", "4,906", "4,908", "4,909", "4,910", "4,912", "4,914",
"4,915", "4,920", "4,928", "4,929", "4,930", "4,931", "4,934",
"4,938", "4,939", "4,943", "4,945", "4,948", "4,950", "4,954",
"4,955", "4,957", "4,960", "4,961", "4,962", "4,968", "4,974",
"4,975", "4,976", "4,978", "4,979", "4,982", "4,984", "4,988",
"4,990", "4,993", "4,995", "4,996", "4,997", "4,999", "40",
"40,000", "40,044", "40,066", "40,208", "40,221", "40,223",
"40,290", "40,294", "40,361", "40,402", "40,429", "40,481",
"40,506", "40,548", "40,603", "40,682", "40,703", "40,858",
"40,865", "40,868", "40,888", "40,894", "40,927", "40,929",
"40,986", "400", "400,000", "400,005", "400,077", "400,226",
"401", "401,081", "402", "402,207", "402,982", "403", "403,434",
"403,536", "404", "404,289", "404,665", "405", "406", "406,160",
"407", "407,358", "407,450", "407,746", "407,795", "408",
"408,305", "408,758", "409", "409,574", "409,745", "41",
"41,066", "41,080", "41,099", "41,102", "41,108", "41,110",
"41,111", "41,120", "41,154", "41,250", "41,254", "41,282",
"41,299", "41,312", "41,404", "41,421", "41,466", "41,478",
"41,510", "41,614", "41,636", "41,642", "41,653", "41,658",
"41,665", "41,919", "41,952", "41,956", "41,964", "41,986",
"410", "410,003", "411", "412", "412,558", "412,710", "412,837",
"412,890", "413", "413,098", "414", "414,149", "414,150",
"414,176", "415", "415,330", "415,390", "415,476", "415,709",
"415,710", "416", "417", "417,502", "418", "418,375", "419",
"419,711", "42", "42,009", "42,120", "42,154", "42,159",
"42,228", "42,254", "42,268", "42,300", "42,301", "42,360",
"42,465", "42,472", "42,517", "42,553", "42,561", "42,574",
"42,618", "42,659", "42,718", "42,735", "42,737", "42,781",
"42,836", "42,844", "42,915", "42,936", "42,949", "42,994",
"420", "420,630", "421", "421,228", "421,537", "422", "422,006",
"422,285", "422,286", "422,668", "422,895", "423", "424",
"424,378", "425", "425,988", "426", "427", "427,448", "427,737",
"428", "428,111", "428,114", "428,234", "428,887", "429",
"429,066", "429,343", "43", "43,022", "43,058", "43,078",
"43,132", "43,153", "43,206", "43,303", "43,408", "43,416",
"43,486", "43,496", "43,499", "43,501", "43,515", "43,544",
"43,646", "43,688", "43,747", "43,773", "43,830", "43,849",
"43,910", "43,945", "43,963", "43,990", "430", "431", "431,296",
"431,737", "432", "432,368", "433", "433,053", "433,631",
"434", "435", "435,081", "435,450", "435,891", "436", "436,480",
"436,778", "437", "437,001", "437,766", "437,799", "438",
"439", "44", "44,080", "44,189", "44,236", "44,261", "44,275",
"44,444", "44,469", "44,492", "44,517", "44,539", "44,561",
"44,590", "44,591", "44,660", "44,710", "44,714", "44,727",
"44,732", "44,832", "44,885", "44,926", "44,957", "440",
"441", "441,354", "442", "442,942", "443", "443,581", "444",
"444,143", "444,395", "444,620", "445", "445,267", "445,469",
"445,517", "445,562", "445,903", "446", "446,020", "447",
"447,640", "448", "448,290", "448,631", "448,671", "449",
"449,913", "45", "45,000", "45,008", "45,041", "45,077",
"45,144", "45,160", "45,255", "45,277", "45,282", "45,341",
"45,366", "45,371", "45,544", "45,580", "45,612", "45,624",
"45,625", "45,757", "45,829", "45,894", "45,964", "45,994",
"450", "451", "451,183", "451,967", "452", "452,922", "453",
"453,867", "454", "454,739", "455", "455,816", "456", "456,805",
"457", "457,490", "457,638", "458", "458,920", "459", "459,447",
"46", "46,030", "46,031", "46,084", "46,150", "46,198", "46,215",
"46,232", "46,269", "46,350", "46,376", "46,383", "46,426",
"46,447", "46,453", "46,454", "46,501", "46,511", "46,556",
"46,565", "46,588", "46,598", "46,608", "46,612", "46,708",
"46,743", "46,754", "46,759", "46,765", "46,780", "46,803",
"46,906", "46,920", "46,936", "46,939", "46,979", "460",
"460,110", "461", "461,473", "461,474", "462", "462,923",
"462,951", "463,219", "464", "464,055", "464,818", "465",
"465,741", "466", "466,563", "466,617", "466,965", "467",
"468", "468,342", "469", "469,019", "469,049", "47", "47,004",
"47,044", "47,096", "47,100", "47,217", "47,269", "47,391",
"47,420", "47,462", "47,530", "47,584", "47,613", "47,698",
"47,704", "47,714", "47,722", "47,764", "47,911", "47,967",
"47,984", "470", "471", "472", "473", "474", "474,300", "475",
"475,076", "476", "478", "479", "48", "48,018", "48,125",
"48,215", "48,333", "48,397", "48,404", "48,407", "48,421",
"48,439", "48,495", "48,604", "48,605", "48,609", "48,663",
"48,702", "48,728", "48,774", "48,880", "48,911", "48,968",
"48,977", "480", "481", "481,043", "481,101", "481,596",
"481,599", "481,798", "482", "482,037", "482,158", "482,692",
"483", "484", "484,505", "485", "485,793", "486", "486,911",
"487", "487,310", "488", "488,953", "489", "49", "49,012",
"49,092", "49,106", "49,141", "49,266", "49,269", "49,291",
"49,297", "49,298", "49,453", "49,464", "49,512", "49,537",
"49,538", "49,625", "49,682", "49,726", "49,741", "49,797",
"49,811", "49,841", "49,949", "49,963", "49,976", "49,988",
"490", "490,025", "490,026", "490,513", "491", "492", "492,651",
"492,652", "492,991", "493", "493,502", "493,530", "494",
"494,654", "495", "496", "496,291", "497", "497,528", "497,641",
"498", "499", "5", "5,000", "5,003", "5,004", "5,005", "5,009",
"5,011", "5,012", "5,019", "5,020", "5,022", "5,024", "5,028",
"5,031", "5,033", "5,038", "5,039", "5,040", "5,041", "5,055",
"5,056", "5,059", "5,064", "5,067", "5,071", "5,072", "5,074",
"5,075", "5,076", "5,077", "5,078", "5,079", "5,081", "5,083",
"5,084", "5,085", "5,088", "5,090", "5,092", "5,099", "5,103",
"5,104", "5,105", "5,107", "5,117", "5,119", "5,120", "5,121",
"5,121,023", "5,123", "5,127", "5,129", "5,130", "5,131",
"5,132", "5,135", "5,136", "5,140", "5,149", "5,150", "5,151",
"5,152", "5,153", "5,155", "5,156", "5,159", "5,160", "5,164",
"5,169", "5,170", "5,171", "5,172", "5,175", "5,176", "5,178",
"5,179", "5,182", "5,183", "5,189", "5,192", "5,194", "5,199",
"5,202", "5,206", "5,212", "5,222", "5,225", "5,226", "5,229",
"5,231", "5,232", "5,233", "5,234", "5,239", "5,241", "5,243",
"5,244", "5,247", "5,253", "5,256", "5,258", "5,260", "5,262",
"5,263", "5,267", "5,270", "5,271", "5,272", "5,276", "5,278",
"5,280", "5,282", "5,285", "5,287", "5,288", "5,289", "5,290",
"5,292", "5,295", "5,297", "5,298", "5,300", "5,302", "5,306",
"5,307", "5,308", "5,310", "5,312", "5,313", "5,317", "5,319",
"5,322", "5,325", "5,326", "5,331", "5,333", "5,334", "5,337",
"5,338", "5,340", "5,341", "5,343", "5,345", "5,346", "5,347",
"5,350", "5,356", "5,357,862", "5,359", "5,360", "5,363",
"5,366", "5,367", "5,370", "5,377", "5,380", "5,387", "5,390",
"5,392", "5,393", "5,395", "5,396", "5,397", "5,399", "5,400",
"5,401", "5,407", "5,414", "5,417", "5,424", "5,430", "5,432",
"5,435", "5,437", "5,438", "5,439", "5,441", "5,442", "5,443",
"5,446", "5,448", "5,450", "5,451", "5,454", "5,455", "5,457",
"5,458", "5,460", "5,461", "5,462", "5,469", "5,470", "5,471",
"5,473", "5,482", "5,487", "5,489", "5,490", "5,493", "5,495",
"5,496", "5,498", "5,499", "5,500", "5,501", "5,502", "5,503",
"5,505", "5,506", "5,507", "5,510", "5,511", "5,512", "5,518",
"5,520", "5,529", "5,531", "5,535", "5,539", "5,540", "5,542",
"5,543", "5,544", "5,545", "5,546", "5,548", "5,554", "5,558",
"5,561,368", "5,572", "5,573", "5,574", "5,576", "5,578",
"5,579", "5,580", "5,582", "5,587", "5,594", "5,595", "5,600",
"5,602", "5,610", "5,612", "5,614", "5,618", "5,619", "5,622",
"5,623", "5,624", "5,625", "5,626", "5,629", "5,633", "5,635",
"5,636", "5,637", "5,642", "5,643", "5,647", "5,656", "5,658",
"5,667", "5,668", "5,672", "5,678", "5,683", "5,685", "5,686",
"5,687", "5,690", "5,691", "5,695", "5,697", "5,698", "5,700",
"5,702", "5,703", "5,704", "5,705", "5,707", "5,708", "5,709",
"5,712", "5,713", "5,714", "5,720", "5,724", "5,725", "5,727",
"5,728", "5,729", "5,731,780", "5,733", "5,734", "5,736",
"5,742", "5,743", "5,744", "5,746", "5,748", "5,752", "5,755",
"5,762", "5,768", "5,771", "5,783", "5,784", "5,785", "5,787",
"5,790", "5,792", "5,794", "5,799,213", "5,802", "5,803",
"5,805", "5,813", "5,814", "5,816", "5,817", "5,819", "5,822",
"5,823", "5,827", "5,834", "5,835", "5,836", "5,838", "5,839",
"5,843", "5,844", "5,845", "5,846", "5,847", "5,850", "5,854",
"5,860", "5,861", "5,864", "5,865", "5,870", "5,871", "5,873",
"5,874", "5,878", "5,880", "5,881", "5,884", "5,885", "5,886",
"5,891", "5,893", "5,895", "5,896", "5,898", "5,900", "5,902",
"5,905", "5,914", "5,917", "5,918", "5,919", "5,924", "5,926",
"5,927", "5,931", "5,932", "5,933", "5,936", "5,940", "5,942",
"5,942,756", "5,952", "5,956", "5,958", "5,964", "5,965",
"5,966", "5,970", "5,974", "5,978", "5,979", "5,982", "5,984",
"5,993,253", "5,999", "50", "50,000", "50,120", "50,152",
"50,154", "50,189", "50,205", "50,220", "50,223", "50,242",
"50,344", "50,488", "50,543", "50,564", "50,591", "50,600",
"50,640", "50,701", "50,746", "50,762", "50,874", "50,890",
"500", "500,000", "500,160", "500,739", "500,768", "501",
"501,115", "502", "502,341", "503", "503,474", "504", "504,516",
"504,808", "504,816", "504,873", "505,000", "505,516", "506",
"507", "507,516", "508", "509", "51", "51,000", "51,036",
"51,044", "51,092", "51,228", "51,229", "51,292", "51,327",
"51,339", "51,343", "51,345", "51,369", "51,394", "51,425",
"51,504", "51,589", "51,649", "51,694", "51,757", "51,795",
"51,841", "51,863", "51,951", "51,959", "51,972", "51,981",
"510", "510,245", "511", "511,338", "511,362", "511,730",
"511,780", "511,808", "512", "513", "514", "514,620", "515",
"515,585", "516", "516,125", "517", "517,825", "518", "518,782",
"518,871", "519", "519,230", "519,830", "52", "52,098", "52,104",
"52,379", "52,401", "52,543", "52,557", "52,663", "52,744",
"52,866", "52,868", "52,966", "520", "521", "522", "522,210",
"523", "524", "524,510", "525", "526", "526,800", "527",
"527,449", "528", "528,103", "529", "53", "53,023", "53,025",
"53,143", "53,209", "53,286", "53,297", "53,385", "53,407",
"53,420", "53,563", "53,632", "53,638", "53,687", "53,722",
"53,755", "53,777", "53,815", "53,833", "53,933", "53,978",
"530", "530,066", "530,292", "530,418", "530,577", "530,948",
"531", "532", "533", "533,487", "533,659", "534", "535",
"535,790", "536", "536,891", "537", "538", "539", "539,466",
"539,689", "54", "54,000", "54,127", "54,158", "54,197",
"54,222", "54,267", "54,486", "54,496", "54,566", "54,602",
"54,614", "54,621", "54,630", "54,663", "54,670", "54,728",
"54,751", "54,865", "54,888", "54,967", "54,981", "54,997",
"540", "541", "541,432", "542", "542,114", "542,597", "543",
"543,063", "543,660", "544", "544,094", "545", "545,593",
"546", "547", "547,017", "547,966", "548", "549,076", "549,245",
"55", "55,000", "55,010", "55,039", "55,089", "55,127", "55,388",
"55,433", "55,442", "55,480", "55,527", "55,563", "55,626",
"55,631", "55,653", "55,682", "55,690", "55,716", "55,810",
"55,819", "55,919", "55,987", "550", "550,000", "550,027",
"550,174", "550,946", "551", "551,778", "552", "552,640",
"553", "553,835", "554", "555", "556", "556,457", "557",
"557,051", "558", "558,658", "559", "56", "56,081", "56,194",
"56,260", "56,261", "56,410", "56,414", "56,477", "56,568",
"56,575", "56,607", "56,617", "56,715", "56,718", "56,810",
"56,898", "56,913", "56,919", "56,943", "56,964", "560",
"560,305", "560,878", "561", "562", "563", "563,878", "564",
"565", "565,621", "565,815", "566", "566,440", "567", "567,159",
"567,755", "568", "568,658", "569", "569,839", "57", "57,000",
"57,001", "57,276", "57,288", "57,363", "57,443", "57,496",
"57,546", "57,602", "57,642", "57,754", "57,780", "570",
"570,424", "570,790", "571", "572", "573", "574", "574,204",
"574,393", "574,419", "575,669", "576", "577,227", "577,764",
"577,941", "578", "578,276", "579", "579,095", "579,399",
"579,505", "58", "58,172", "58,200", "58,214", "58,257",
"58,358", "58,424", "58,425", "58,539", "58,638", "58,675",
"58,717", "58,718", "58,725", "58,748", "58,780", "58,844",
"58,901", "58,947", "580", "581", "581,403", "582", "582,022",
"582,093", "582,747", "582,974", "583", "583,657", "583,850",
"584", "585", "586", "586,564", "587", "588", "588,249",
"589", "589,698", "59", "59,152", "59,218", "59,261", "59,298",
"59,375", "59,457", "59,654", "59,684", "59,826", "59,845",
"59,925", "59,939", "590", "591", "591,363", "591,885", "591,998",
"592", "593", "593,154", "594", "594,771", "595", "596",
"596,317", "597", "598", "599", "6", "6,000", "6,002", "6,004",
"6,006", "6,007", "6,008", "6,010", "6,011", "6,012", "6,015",
"6,017", "6,018", "6,020", "6,023", "6,028", "6,031", "6,033",
"6,036", "6,039", "6,040", "6,049", "6,054", "6,055", "6,056",
"6,059", "6,061,368", "6,067", "6,069", "6,070", "6,072",
"6,075", "6,082", "6,092", "6,093", "6,095,382", "6,096",
"6,098", "6,099", "6,101", "6,106", "6,107", "6,109", "6,110",
"6,112", "6,116", "6,117", "6,122", "6,125", "6,126", "6,136",
"6,139", "6,141", "6,143", "6,144", "6,145", "6,148", "6,156",
"6,158", "6,159", "6,172", "6,177", "6,179", "6,183", "6,186",
"6,189", "6,191", "6,195", "6,197", "6,200", "6,202", "6,205",
"6,206", "6,207", "6,210", "6,214", "6,215", "6,218", "6,219",
"6,220", "6,224", "6,227", "6,228", "6,230", "6,231", "6,232",
"6,241", "6,242", "6,243", "6,247", "6,252", "6,253", "6,256",
"6,257", "6,265", "6,266", "6,270", "6,272", "6,273", "6,274",
"6,275", "6,280", "6,282", "6,283", "6,284", "6,285", "6,287",
"6,292", "6,295", "6,299", "6,303", "6,304", "6,306", "6,308",
"6,310", "6,314", "6,320", "6,323", "6,324", "6,326", "6,328",
"6,330", "6,333", "6,335", "6,342", "6,351", "6,360", "6,362",
"6,371", "6,372", "6,374", "6,377", "6,382", "6,383", "6,385",
"6,392", "6,394", "6,395", "6,396", "6,399", "6,403", "6,404",
"6,411", "6,420", "6,422", "6,423", "6,424", "6,425", "6,431",
"6,435", "6,443", "6,444", "6,445", "6,450", "6,456", "6,461",
"6,465", "6,465,986", "6,467", "6,469", "6,470", "6,471",
"6,480", "6,486", "6,493", "6,502", "6,505", "6,513", "6,514",
"6,517", "6,520", "6,521", "6,530", "6,546", "6,546,586",
"6,550", "6,556", "6,560", "6,561", "6,562", "6,565", "6,566",
"6,569", "6,570", "6,571", "6,577", "6,578", "6,580", "6,581",
"6,582", "6,586", "6,591", "6,594", "6,595", "6,597", "6,599",
"6,600", "6,601", "6,604", "6,611", "6,615", "6,616", "6,619",
"6,622", "6,625", "6,628", "6,630", "6,636", "6,637", "6,638",
"6,643", "6,646", "6,650", "6,652", "6,653", "6,654", "6,659",
"6,660", "6,661", "6,664", "6,666", "6,667", "6,669", "6,670",
"6,675", "6,678", "6,680", "6,681", "6,682", "6,685", "6,686",
"6,689", "6,691", "6,692", "6,694", "6,700", "6,701", "6,706",
"6,707", "6,709", "6,712", "6,713", "6,716", "6,717", "6,719",
"6,723", "6,725", "6,731", "6,732", "6,733", "6,734", "6,737",
"6,741", "6,745", "6,750", "6,752", "6,753", "6,758", "6,765",
"6,766", "6,769", "6,778", "6,779", "6,782", "6,783", "6,784",
"6,787", "6,790", "6,803", "6,806", "6,811", "6,812", "6,815",
"6,817", "6,818,080", "6,828", "6,829", "6,830", "6,832",
"6,836", "6,839", "6,841", "6,842", "6,845", "6,846", "6,847",
"6,850", "6,852", "6,855", "6,860", "6,861", "6,862", "6,863",
"6,867", "6,869", "6,872", "6,877", "6,880", "6,890", "6,893",
"6,898", "6,901", "6,902", "6,903", "6,908", "6,912", "6,916",
"6,918", "6,919", "6,928", "6,930", "6,931", "6,932", "6,935",
"6,939", "6,941", "6,943", "6,945", "6,948", "6,950", "6,952",
"6,955", "6,960", "6,961", "6,965", "6,970", "6,971", "6,972",
"6,978", "6,997", "6,998", "60", "60,000", "60,001", "60,007",
"60,155", "60,162", "60,199", "60,270", "60,320", "60,443",
"60,590", "60,602", "60,635", "60,638", "60,766", "60,802",
"60,842", "60,968", "60,977", "60,992", "600", "600,000",
"600,270", "600,687", "600,774", "601", "601,174", "602",
"602,016", "603", "603,500", "604", "604,235", "604,858",
"605", "605,302", "605,644", "606", "606,124", "606,261",
"606,275", "607", "608", "609", "61", "61,023", "61,097",
"61,113", "61,351", "61,389", "61,437", "61,709", "61,750",
"61,798", "61,899", "61,955", "61,981", "610", "611", "611,459",
"612", "612,000", "613", "613,478", "614", "614,167", "615",
"616", "616,084", "617", "617,344", "618", "618,788", "618,998",
"619", "619,871", "62", "62,130", "62,134", "62,165", "62,237",
"62,348", "62,393", "62,400", "62,476", "62,501", "62,513",
"62,570", "62,626", "62,773", "62,832", "62,854", "62,866",
"62,882", "62,911", "62,997", "620", "620,236", "620,998",
"621", "622", "623", "623,686", "624", "626", "627", "627,152",
"627,459", "628", "629", "629,304", "629,673", "629,797",
"63", "63,010", "63,066", "63,211", "63,219", "63,305", "63,332",
"63,425", "63,439", "63,482", "63,536", "63,619", "63,783",
"63,875", "63,935", "630", "630,687", "631", "631,194", "632",
"633", "634", "634,167", "635", "637", "638", "639", "64",
"64,079", "64,089", "64,216", "64,268", "64,326", "64,339",
"64,417", "64,474", "64,481", "64,547", "64,607", "64,848",
"64,967", "640", "640,433", "640,589", "640,667", "641",
"642", "643", "644", "645", "645,652", "645,957", "646",
"647", "647,275", "648", "648,564", "649", "65", "65,000",
"65,122", "65,127", "65,344", "65,450", "65,612", "65,800",
"65,847", "65,856", "65,999", "650", "650,174", "650,456",
"651", "652", "652,971", "653", "654", "654,156", "654,176",
"655", "656,803", "657", "658", "658,112", "658,471", "659",
"66", "66,034", "66,133", "66,158", "66,224", "66,238", "66,327",
"66,338", "66,499", "66,500", "66,510", "66,545", "66,628",
"66,688", "66,739", "66,862", "66,919", "66,987", "660",
"661", "661,042", "661,359", "661,579", "661,951", "662",
"662,205", "662,680", "663", "663,477", "663,807", "664",
"664,079", "665", "665,110", "666", "666,432", "666,925",
"667", "668", "668,906", "669", "67", "67,056", "67,111",
"67,165", "67,167", "67,370", "67,380", "67,416", "67,887",
"67,918", "670", "671", "671,199", "671,737", "672", "673",
"673,437", "674", "674,675", "675", "676", "676,294", "676,296",
"677", "678", "678,500", "679", "679,260", "68", "68,012",
"68,017", "68,032", "68,064", "68,091", "68,103", "68,470",
"68,521", "68,630", "68,795", "68,971", "680", "680,845",
"681,062", "682", "683", "683,859", "684", "685", "685,929",
"686", "687", "687,886", "688", "688,785", "689", "689,918",
"69", "69,047", "69,063", "69,075", "69,365", "69,377", "69,379",
"69,492", "69,552", "69,599", "69,665", "69,682", "69,774",
"690", "690,344", "691", "691,116", "692,442", "693", "694",
"694,856", "695", "695,466", "696", "697", "697,272", "697,418",
"698", "698,618", "699,629", "699,716", "7", "7,000", "7,001",
"7,013", "7,015", "7,017", "7,030", "7,039", "7,042", "7,052",
"7,054", "7,057", "7,060", "7,064", "7,065", "7,066", "7,070",
"7,071", "7,075", "7,076", "7,080", "7,082", "7,085", "7,090",
"7,098", "7,101", "7,105", "7,106", "7,110", "7,115", "7,116",
"7,121", "7,122", "7,125", "7,126", "7,129", "7,130", "7,131",
"7,135", "7,138", "7,141", "7,143", "7,148", "7,150", "7,154",
"7,159", "7,160", "7,171", "7,182", "7,184", "7,186", "7,189",
"7,190", "7,196", "7,198", "7,202", "7,207", "7,208", "7,209",
"7,211", "7,228", "7,232", "7,233", "7,235", "7,236", "7,241",
"7,250", "7,255", "7,256", "7,259", "7,260", "7,264", "7,265",
"7,266", "7,270", "7,271", "7,272", "7,274", "7,275", "7,277",
"7,280", "7,289", "7,291", "7,292", "7,293", "7,300", "7,309",
"7,310", "7,312", "7,317", "7,325", "7,326", "7,328", "7,329",
"7,330", "7,337", "7,344", "7,346", "7,353", "7,359", "7,363",
"7,368", "7,377", "7,378", "7,382", "7,383", "7,387", "7,388",
"7,397", "7,398", "7,400", "7,401", "7,404", "7,409", "7,410",
"7,412", "7,416", "7,418", "7,419", "7,420", "7,422", "7,426",
"7,432", "7,434", "7,439", "7,445", "7,450", "7,451", "7,454",
"7,459", "7,466", "7,467", "7,468", "7,473", "7,479", "7,486",
"7,487", "7,488", "7,501", "7,508", "7,509", "7,515", "7,519",
"7,522", "7,523", "7,533", "7,534", "7,538", "7,543", "7,544",
"7,548", "7,555", "7,557", "7,559", "7,560", "7,561", "7,566",
"7,573", "7,580", "7,585", "7,592", "7,595", "7,599", "7,600",
"7,603", "7,608", "7,615", "7,616", "7,624", "7,632", "7,645",
"7,647", "7,649", "7,653", "7,661", "7,664", "7,668", "7,670",
"7,674", "7,676", "7,679", "7,684", "7,699", "7,706", "7,715",
"7,718", "7,722", "7,725", "7,728", "7,732", "7,735", "7,743",
"7,744", "7,752", "7,758", "7,767", "7,770", "7,779", "7,780",
"7,781", "7,786", "7,793", "7,794", "7,800", "7,801", "7,802",
"7,805", "7,806", "7,812", "7,814", "7,823", "7,824", "7,826",
"7,827", "7,836", "7,838", "7,840", "7,848", "7,851", "7,852",
"7,853,689", "7,856", "7,865", "7,866", "7,871", "7,873",
"7,876", "7,878", "7,880", "7,884", "7,890", "7,898", "7,901",
"7,905", "7,912", "7,915", "7,926", "7,927", "7,928", "7,932",
"7,935", "7,937", "7,946", "7,954", "7,955", "7,965", "7,980",
"7,985", "7,991", "7,994", "7,995", "70", "70,000", "70,020",
"70,137", "70,219", "70,221", "70,322", "70,439", "70,446",
"70,457", "70,536", "70,554", "70,580", "70,601", "70,658",
"70,664", "70,690", "70,758", "70,768", "70,821", "700",
"700,000", "701", "702", "702,522", "703", "703,916", "704",
"705", "705,024", "706", "707", "708", "708,772", "708,812",
"709", "709,083", "71", "71,009", "71,028", "71,113", "71,270",
"71,291", "71,399", "71,433", "71,551", "71,630", "71,664",
"71,678", "71,857", "71,944", "710", "711", "711,417", "712",
"712,648", "714", "715", "716", "717", "717,721", "719,199",
"72", "72,089", "72,096", "72,110", "72,126", "72,159", "72,172",
"72,393", "72,422", "72,459", "72,545", "72,667", "72,872",
"72,877", "72,974", "720", "721", "721,860", "722", "723",
"723,576", "724", "724,152", "725", "725,842", "726", "726,078",
"726,581", "726,916", "727", "727,789", "728", "729", "73",
"73,022", "73,070", "73,114", "73,255", "73,518", "73,679",
"73,828", "73,894", "73,960", "730", "730,834", "731", "731,062",
"732", "733", "734", "734,873", "735", "736,572", "737",
"738", "739", "739,923", "74", "74,044", "74,326", "74,464",
"74,576", "74,630", "74,656", "74,850", "74,942", "740",
"740,029", "740,350", "740,752", "740,866", "741", "742",
"743", "743,582", "743,926", "744", "745", "745,866", "746",
"746,130", "747", "748", "748,866", "749,000", "749,866",
"75", "75,000", "75,022", "75,207", "75,302", "75,360", "75,367",
"75,672", "75,819", "75,915", "75,927", "75,943", "75,966",
"750", "750,581", "750,866", "751", "751,866", "752", "753",
"753,866", "754", "755", "755,981", "756", "757", "758,634",
"759", "76", "76,031", "76,035", "76,218", "76,316", "76,717",
"76,721", "76,758", "76,853", "76,863", "76,934", "76,936",
"760", "761", "762", "762,243", "762,516", "763", "763,466",
"764", "764,205", "764,663", "765,903", "766", "767,970",
"768", "768,572", "769", "77", "77,097", "77,281", "77,388",
"77,538", "77,706", "77,741", "77,756", "77,873", "770",
"770,239", "771", "772", "772,831", "773", "774", "775",
"776,668", "778", "779", "78", "78,002", "78,004", "78,067",
"78,113", "78,162", "78,198", "78,247", "78,310", "78,390",
"78,401", "78,556", "78,840", "78,873", "78,905", "780",
"780,535", "781", "782", "783", "784", "785", "785,302",
"786", "787", "787,103", "788", "788,205", "789", "79", "79,072",
"79,103", "79,123", "79,173", "79,193", "79,240", "79,300",
"79,515", "79,750", "790", "791", "791,473", "792", "793",
"794", "794,296", "794,636", "795", "795,961", "796", "796,008",
"797", "798", "798,443", "798,660", "799", "799,209", "799,360",
"8", "8,000", "8,010", "8,012", "8,013", "8,025", "8,028",
"8,030", "8,031", "8,034", "8,037", "8,040", "8,043", "8,044",
"8,046", "8,051", "8,053", "8,054", "8,056", "8,062", "8,063",
"8,066", "8,068", "8,076", "8,082", "8,086", "8,089", "8,093",
"8,094", "8,095", "8,096", "8,099", "8,100", "8,102", "8,106",
"8,115", "8,116", "8,123", "8,125", "8,129", "8,139", "8,142",
"8,143", "8,147", "8,150", "8,153", "8,154", "8,156", "8,159",
"8,161", "8,166", "8,167", "8,168", "8,169", "8,178", "8,179",
"8,188", "8,193", "8,194", "8,200", "8,203", "8,204", "8,207",
"8,215", "8,220", "8,221", "8,222", "8,224", "8,228", "8,231",
"8,244", "8,245", "8,257", "8,258", "8,260", "8,264", "8,269",
"8,271", "8,272", "8,274", "8,280", "8,293", "8,295", "8,297",
"8,299", "8,300", "8,303", "8,304", "8,305", "8,313", "8,319",
"8,321", "8,325", "8,327", "8,331", "8,337", "8,345", "8,349",
"8,351", "8,354", "8,356", "8,359", "8,363", "8,364", "8,367",
"8,371", "8,372", "8,377", "8,378", "8,379", "8,381", "8,383",
"8,384", "8,389", "8,391", "8,392", "8,400", "8,404,990",
"8,409", "8,410", "8,412", "8,418", "8,421", "8,426", "8,427",
"8,428", "8,433", "8,437", "8,441", "8,442", "8,449", "8,455",
"8,456", "8,457", "8,460", "8,462", "8,463", "8,465", "8,468",
"8,477", "8,478", "8,480", "8,483", "8,485", "8,485,478",
"8,492", "8,496", "8,505", "8,506", "8,512", "8,519,374",
"8,524", "8,531", "8,533", "8,557", "8,562", "8,565", "8,566",
"8,574", "8,575", "8,576", "8,578", "8,583", "8,585", "8,595",
"8,596", "8,599", "8,601", "8,604", "8,612", "8,619", "8,620",
"8,623", "8,633", "8,638", "8,640", "8,643", "8,651", "8,655",
"8,664", "8,666", "8,671", "8,673", "8,674", "8,675", "8,684",
"8,690", "8,692", "8,696", "8,699", "8,715", "8,719", "8,720",
"8,724", "8,725", "8,734", "8,735", "8,740", "8,741", "8,744",
"8,746", "8,747", "8,750", "8,751", "8,755", "8,764", "8,772",
"8,775", "8,777", "8,779", "8,796", "8,800", "8,803,372",
"8,812", "8,813", "8,822", "8,824", "8,827", "8,830", "8,835",
"8,842", "8,844", "8,848", "8,849", "8,853", "8,855", "8,857",
"8,866", "8,868", "8,869", "8,870", "8,871", "8,886", "8,888",
"8,892", "8,909", "8,911", "8,914,854", "8,915", "8,927",
"8,932", "8,946", "8,947", "8,951", "8,963", "8,964", "8,967",
"8,973", "8,981", "8,983", "8,991", "8,994", "8,996", "80",
"80,089", "80,234", "80,280", "80,900", "80,937", "80,960",
"800", "800,000", "800,782", "801", "802", "802,588", "803",
"804", "804,534", "805", "805,188", "806", "806,795", "807",
"808", "809", "81", "81,254", "81,287", "81,402", "81,516",
"81,742", "81,811", "81,938", "81,948", "81,953", "810",
"810,525", "811", "812", "812,686", "814,874", "815", "816",
"817", "817,928", "818", "819", "819,445", "82", "82,000",
"82,071", "82,177", "82,194", "82,398", "82,452", "82,510",
"82,524", "82,608", "82,680", "82,692", "82,741", "82,764",
"82,805", "82,985", "820", "821", "821,147", "822", "823",
"824", "825", "826", "827", "827,003", "827,183", "828",
"828,610", "83", "83,052", "83,118", "83,199", "83,222",
"83,235", "83,386", "83,387", "83,466", "83,597", "83,604",
"83,817", "830", "830,543", "831", "831,330", "832", "833",
"833,804", "834", "834,183", "834,464", "835", "835,017",
"836", "836,704", "837", "837,146", "838", "839", "84", "84,011",
"84,012", "84,168", "84,270", "84,345", "84,473", "84,550",
"84,580", "84,719", "84,741", "84,826", "840", "840,833",
"841", "841,855", "841,887", "841,925", "842", "842,555",
"843", "844,317", "844,621", "844,700", "845", "846", "847",
"848", "848,288", "848,573", "849", "849,036", "85", "85,000",
"85,128", "85,149", "85,157", "85,397", "85,452", "85,639",
"85,751", "85,818", "85,973", "850", "851,620", "852", "852,301",
"853", "854", "855", "856", "856,334", "857", "859", "859,964",
"86", "86,267", "86,571", "86,794", "86,835", "86,849", "86,899",
"86,946", "860", "860,842", "861", "861,477", "862", "863",
"863,015", "863,423", "864", "864,771", "865", "865,001",
"865,789", "866", "867", "867,814", "868", "869", "87", "87,093",
"87,113", "87,297", "87,391", "87,614", "87,646", "87,669",
"87,727", "87,866", "87,907", "870", "871", "872", "873",
"873,069", "873,191", "874", "874,766", "874,881", "875",
"876", "877", "878", "878,934", "879", "879,219", "879,296",
"88", "88,158", "88,348", "88,382", "88,414", "88,448", "88,479",
"88,565", "88,678", "88,697", "88,721", "88,757", "880",
"880,500", "881", "882", "883", "884", "884,430", "885",
"885,983", "886", "887", "887,472", "888", "889", "89", "89,199",
"89,324", "89,418", "89,539", "89,562", "89,741", "89,911",
"890", "890,941", "891", "891,379", "891,473", "892", "893",
"893,032", "893,554", "894", "894,007", "894,290", "894,366",
"895", "895,902", "896", "897", "897,079", "898", "899",
"9", "9,000", "9,010", "9,011", "9,014", "9,018", "9,023",
"9,024", "9,026", "9,029", "9,032", "9,033", "9,034", "9,035",
"9,042", "9,051", "9,053", "9,055", "9,058", "9,063", "9,065",
"9,067", "9,072", "9,084", "9,085", "9,086", "9,089", "9,090",
"9,092", "9,096", "9,099", "9,100", "9,102", "9,109", "9,110",
"9,112", "9,134", "9,141", "9,142", "9,145", "9,152", "9,154",
"9,155", "9,158", "9,168", "9,180", "9,183", "9,186", "9,189",
"9,200", "9,203", "9,205", "9,214", "9,219", "9,221", "9,226",
"9,227", "9,229", "9,232", "9,234", "9,235", "9,239", "9,240",
"9,242", "9,243", "9,250", "9,251", "9,254", "9,261", "9,268",
"9,270", "9,279", "9,283", "9,290", "9,296", "9,297", "9,298",
"9,300", "9,316", "9,318", "9,329", "9,332", "9,333", "9,335",
"9,337", "9,347", "9,348", "9,354", "9,373", "9,374", "9,375",
"9,380", "9,382", "9,387", "9,389", "9,390", "9,396", "9,400",
"9,404", "9,406", "9,409", "9,410", "9,413", "9,415", "9,417",
"9,423", "9,427", "9,428", "9,430", "9,441", "9,445", "9,447",
"9,456", "9,465", "9,470", "9,475", "9,477", "9,479", "9,484",
"9,486", "9,490", "9,496", "9,497", "9,501", "9,510", "9,519",
"9,523", "9,524", "9,525", "9,528", "9,535", "9,544", "9,546",
"9,558", "9,562", "9,568", "9,578", "9,586", "9,588", "9,589",
"9,590", "9,591", "9,592", "9,596", "9,605", "9,612", "9,618",
"9,620", "9,623", "9,640", "9,658", "9,662", "9,664", "9,675",
"9,676", "9,679", "9,684", "9,690", "9,696", "9,697", "9,700",
"9,702", "9,716", "9,717", "9,718", "9,719", "9,725", "9,727",
"9,728", "9,735", "9,754,859", "9,756", "9,757", "9,769",
"9,775", "9,776", "9,785", "9,791", "9,797", "9,799", "9,805",
"9,810", "9,814", "9,820", "9,823", "9,824", "9,833", "9,845",
"9,848", "9,851", "9,856", "9,858", "9,864", "9,867", "9,877",
"9,879", "9,891", "9,901", "9,909", "9,917", "9,918", "9,924",
"9,925", "9,936", "9,942", "9,946", "9,948", "9,952", "9,957",
"9,964", "9,983", "9,984", "9,985", "9,989", "9,990", "9,995",
"9,998", "9,999", "90", "90,000", "90,239", "90,405", "90,425",
"90,536", "90,775", "90,799", "90,918", "90,960", "90,962",
"900", "900,000", "901", "901,147", "901,416", "901,465",
"902", "903", "903,850", "904", "904,516", "905", "905,050",
"906", "906,183", "906,268", "907", "908", "908,067", "909",
"909,048", "91", "91,039", "91,093", "91,144", "91,192",
"91,233", "91,495", "91,517", "91,527", "91,672", "91,745",
"91,826", "910", "911", "912", "913", "913,943", "914", "915",
"915,367", "916", "917", "918", "919", "919,950", "92", "92,238",
"92,318", "92,435", "92,603", "92,629", "920", "920,236",
"921", "922", "923", "924", "925", "926", "926,953", "927",
"927,003", "928", "928,576", "929", "93", "93,201", "93,206",
"93,302", "93,971", "930", "930,682", "931", "932", "933",
"934", "935", "936", "937", "938", "939", "94", "94,073",
"94,121", "94,522", "94,574", "94,633", "94,855", "94,977",
"940", "941", "943", "944", "944,222", "945", "946", "947",
"949", "949,435", "949,789", "95", "95,091", "95,096", "95,137",
"95,157", "95,275", "95,361", "95,488", "95,502", "95,640",
"95,849", "950", "950,316", "950,559", "950,788", "952",
"953", "954", "955", "955,200", "956", "956,484", "957",
"958", "959", "96", "96,059", "96,111", "96,138", "96,205",
"96,236", "96,346", "96,771", "96,848", "960", "961", "962",
"962,790", "964", "964,170", "964,336", "965", "965,154",
"965,166", "965,390", "966", "967", "967,753", "968,629",
"969", "97", "97,473", "97,746", "97,759", "97,850", "97,978",
"970", "970,225", "971", "971,405", "972", "973", "973,902",
"974", "974,109", "974,557", "975", "976", "977", "977,452",
"978", "98", "98,134", "98,385", "98,428", "98,495", "98,562",
"98,955", "980", "980,556", "981", "981,998", "982", "982,526",
"983", "984,148", "984,861", "985", "985,369", "986", "986,580",
"987", "988", "989", "99", "99,115", "99,214", "99,409",
"99,581", "99,834", "990", "992", "992,489", "993", "994",
"994,195", "995", "996", "998", "999"), class = "factor"),
Month = structure(c(11L, 11L, 7L, 2L, 2L, 12L, 11L, 11L,
                    11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
11L,
                    11L, 11L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L),
.Label = c("Apr",

"Aug", "Dec", "Feb", "Jan", "Jul", "Jun", "Mar", "May", "Nov",

"Oct", "Sep"), class = "factor"), Year = c(2010L, 2010L,

2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,

2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,

2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,

2011L)), .Names = c("DOC_TYPE", "DOC_NO", "DOC_DT", "SFX_CODE",

"CUSTOMER", "DOC_AMOUNT", "OS_ASON_RPT_DT", "OS_DAYS", "BILLING_BRANCH",

"COLL_BR", "RECEIPT_NO", "RECEIPT_DT", "Applied.Date", "RECEIPT_AMT",

"TDS_AMT", "REBATE", "Final", "Month", "Year"), row.names = c(NA,

30L), class = "data.frame")


Not sure if this would help.
>
-----Original Message-----
From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
Sent: Wednesday, December 30, 2015 10:23 PM
To: SHIVI BHATIA <shivi.bhatia at safexpress.com>; r-help at r-project.org
Subject: Re: [R] Dput Help in R

On 30/12/2015 5:56 AM, SHIVI BHATIA wrote:
> Dear Team,
>
>
>
> I am facing an error while performing a manipulation using a dplyr
package.
> In the code below, I am using mutate to build a new calculated column:
>
>
>
> kp<-read.csv("collection_last.csv",header=TRUE)
>
> mutate(kp,dif=DOC_AMOUNT-RECEIPT_AMT+TDS_AMT+REBATE)
>
>
>
> However it gives an error:-
>
> Warning messages:
>
> 1: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>
>    '-' not meaningful for factors
>
> 2: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>
>    '+' not meaningful for factors
>
> 3: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>
>    '+' not meaningful for factors
>
>
>
> This is an error when some of my variables are factors hence I have
> tried to change these to numeric so used the expression as:
>
> kp$DOC_TYPE=as.numeric(kp$DOC_TYPE).
>
>
>
> this now shows as variable type of as "double". So expedite help on
> this one i was trying to create a reproducible example and i am highly
> struggling to
>
> create one. the data i have is approx. around 1 million rows with 21
> columns hence when i use a dput option it does not capture the entire
> detailing and row level info required to share and even
> dput(head(kp$DOC_TYPE) does not help either.
>
> I have seen many stack overflow & r help column before composing this
email.
> Hence i need help to create this reproducible example to share with
> the experts in the community. Apologies if this is a repeat.
>
>
>
> PLEASE HELP AS I AM HIGHLY STRUGGLING TO BUILD ANY OUTCOME.

If you are working with a dataframe or matrix named x, just use

y <- x[1:10,]

to extract the first 10 rows.  The error will probably occur with this
subset as well, and dput() will give you a reasonably sized amount of
output.  If the error doesn't happen, just take a bigger subset, and
possibly leave off the beginning, e.g.

y <- x[101:110,]

for 10 lines starting at line 101.

Duncan Murdoch

This e-mail is confidential. It may also be legally privileged. If you are not the addressee you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return e-mail. Internet communications cannot be guaranteed to be timely, secure, error or virus-free. The sender does not accept liability for any errors or omissions.


From sstoline at gmail.com  Thu Dec 31 11:20:05 2015
From: sstoline at gmail.com (Steven Stoline)
Date: Thu, 31 Dec 2015 05:20:05 -0500
Subject: [R] Histogram for Left Censored Data
Message-ID: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>

Dear All:

I need helps with creating histograms for data that include left censored
observations.

Here is an example of left censored data



*Sulfate.Concentration*
<-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)


*Column 2* is an indicator for censoring "*1*" for left censored
observations and "*0*" for non-censored (fully measured) observations.

with many thanks
steve

-- 
Steven M. Stoline
1123 Forest Avenue
Portland, ME 04112
sstoline at gmail.com

	[[alternative HTML version deleted]]


From benedetto.rocchi at unifi.it  Thu Dec 31 16:28:44 2015
From: benedetto.rocchi at unifi.it (Benedetto Rocchi)
Date: Thu, 31 Dec 2015 16:28:44 +0100
Subject: [R] bootstrapping statistics from leaken package
Message-ID: <568549AC.10408@unifi.it>

I am trying to generate a confidence interval for the statistic arpt (at 
risk of poverty threshold generated with the package leaken) using the 
package boot.
This is my temptative script (where XH090 and DB030 and w are variables 
of an attached dataframe):
> arpt.boot <- function(data,i){
>   d = data[i]
>   arpt <- arpr(d, sort=DB030, p=c(0.4))
>   arpt <- as.vector(arpt)
>  }
> arpt40 <- boot(HX090,arpt.boot,R=1000,weights=w)
> boot.ci(arpt40, conf = 0.95, type = "norm")

R answer with the following error message:
> Error in boot(dati_mauro$HX090, arpt.boot, R = 1000, weights = 
> dati_mauro$w) : incorrect number of subscripts on matrix
Thank you in advance for any help in solving this problem.
Benedetto Rocchi

-- 
Dipartimento di Scienze per l'Economia e l'Impresa
Universit? degli Studi di Firenze
Via delle Pandette, 9 50127 Firenze
telefono 0552759706 cellulare 3204309360


	[[alternative HTML version deleted]]


From msharp at txbiomed.org  Thu Dec 31 18:18:07 2015
From: msharp at txbiomed.org (Mark Sharp)
Date: Thu, 31 Dec 2015 17:18:07 +0000
Subject: [R] Dput Help in R
In-Reply-To: <005101d1439c$87b26670$97173350$@safexpress.com>
References: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
	<56840C05.3070401@gmail.com>
	<005101d1439c$87b26670$97173350$@safexpress.com>
Message-ID: <E4D78A37-9B02-457E-BB25-1EFB2C9E9D22@txbiomed.org>

Shivi,

It looks like you have copied and pasted with errors. When you use dput() on a dataframe, it will output a list (see example that follows). I think you have cut off the beginning of the output and have manually added the assignment ?ab<-?. Also it is clear that the read.csv is interpreting your numbers as character strings. Note the quotation marks.

> sample_df <- data.frame(nums = 1:10, rand_num = runif(10))
> dput(sample_df)
structure(list(nums = 1:10, rand_num = c(0.346553232055157, 0.996620914200321, 
0.412795166717842, 0.250930240144953, 0.809035068377852, 0.782051374204457, 
0.0857722735963762, 0.938713687239215, 0.819887164747342, 0.870529293781146
)), .Names = c("nums", "rand_num"), row.names = c(NA, -10L), class = "data.frame?)


> On Dec 31, 2015, at 1:26 AM, SHIVI BHATIA <shivi.bhatia at safexpress.com> wrote:
> 
> Hi Duncan,
> Please find the dput from the data.
> 
> ab<-read.csv("collection_last.csv",header=TRUE)
> y<-ab[1:10,]
> 
> 
> ab<- "2,458", "2,461", "2,462", "2,463", "2,464", "2,465", "2,468",
> "2,469", "2,470", "2,473", "2,474", "2,475", "2,476", "2,477",
> "2,478", "2,479", "2,480", "2,483", "2,484,267", "2,485",
> "2,486", "2,487", "2,490", "2,491", "2,491,176", "2,492",
> "2,494", "2,495,976", "2,496", "2,497", "2,498", "2,499",
> "2,500", "2,500,001", "2,501", "2,502", "2,503", "2,504",
> "2,506", "2,507", "2,508", "2,509", "2,510", "2,511", "2,512",
> "2,513", "2,514", "2,515", "2,516", "2,517", "2,519", "2,520",
> "2,521", "2,523", "2,523,257", "2,524", "2,525", "2,526",
> "2,527", "2,528", "2,529", "2,530", "2,531", "2,535", "2,536",
> "2,538", "2,539", "2,540", "2,542", "2,543", "2,544", "2,545",
> "2,546", "2,547", "2,549", "2,550", "2,551", "2,553", "2,554",
> "2,556", "2,557", "2,558", "2,559", "2,560", "2,561", "2,563",
> "2,564", "2,565", "2,566", "2,567", "2,570", "2,572", "2,574",
> "2,576", "2,577", "2,578", "2,579", "2,580", "2,582", "2,583",
> "2,584", "2,585", "2,588", "2,589", "2,590", "2,591", "2,592",
> "2,593", "2,594", "2,596", "2,599", "2,600", "2,601", "2,602",
> "2,604", "2,605", "2,606", "2,607", "2,609", "2,611", "2,612",
> "2,613", "2,614", "2,615", "2,616", "2,617", "2,618", "2,619",
> "2,620", "2,621", "2,622", "2,624", "2,626", "2,627", "2,628",
> "2,628,385", "2,629", "2,630", "2,633", "2,634", "2,636",
> "2,637", "2,638", "2,640", "2,642", "2,644", "2,647", "2,648",
> "2,649", "2,650", "2,651", "2,654", "2,655", "2,656", "2,658",
> "2,660", "2,661", "2,663", "2,665", "2,666", "2,667", "2,668",
> "2,670", "2,671", "2,672", "2,673", "2,674", "2,676", "2,677",
> "2,678", "2,679", "2,679,505", "2,680", "2,682", "2,684",
> "2,687", "2,688", "2,689", "2,690", "2,692", "2,694", "2,695",
> "2,696", "2,697", "2,699", "2,700", "2,702", "2,703", "2,705",
> "2,706", "2,707", "2,708", "2,709", "2,710", "2,711", "2,712",
> "2,713", "2,714", "2,715", "2,716", "2,717", "2,718", "2,720",
> "2,721", "2,722", "2,723", "2,727", "2,728", "2,730", "2,732",
> "2,733", "2,736", "2,737", "2,738", "2,739", "2,742", "2,744",
> "2,747", "2,748", "2,749", "2,750", "2,752", "2,753", "2,754",
> "2,758", "2,759", "2,760", "2,761", "2,765,189", "2,766",
> "2,767", "2,770", "2,772", "2,773", "2,774", "2,776", "2,778",
> "2,780", "2,783", "2,784", "2,785", "2,786", "2,787", "2,789",
> "2,790", "2,792", "2,793", "2,794", "2,795", "2,797", "2,798",
> "2,799", "2,800", "2,803", "2,804", "2,806", "2,808", "2,808,003",
> "2,809", "2,810", "2,812", "2,813", "2,814", "2,816", "2,818",
> "2,820", "2,824", "2,825", "2,826", "2,832", "2,835", "2,839,850",
> "2,840", "2,841", "2,842", "2,844", "2,845", "2,846", "2,847",
> "2,848", "2,850", "2,851", "2,852", "2,853", "2,855", "2,856",
> "2,857", "2,858", "2,859", "2,861", "2,864", "2,865", "2,866",
> "2,867", "2,869", "2,870", "2,873", "2,874", "2,875", "2,876",
> "2,877", "2,878", "2,879", "2,880", "2,883", "2,884", "2,885",
> "2,886", "2,890", "2,891", "2,892", "2,893", "2,894", "2,895",
> "2,896", "2,897", "2,898", "2,899", "2,902", "2,903", "2,904",
> "2,905", "2,907", "2,908", "2,908,956", "2,910", "2,911",
> "2,912", "2,913", "2,916", "2,917", "2,922", "2,923", "2,924",
> "2,925", "2,926", "2,929", "2,930", "2,931", "2,932", "2,933",
> "2,934", "2,936", "2,937", "2,939", "2,941", "2,943", "2,944",
> "2,946", "2,947", "2,948", "2,950", "2,951", "2,953", "2,954",
> "2,955", "2,957", "2,959", "2,960", "2,961", "2,965", "2,967",
> "2,968", "2,970", "2,971", "2,973", "2,975", "2,976", "2,979",
> "2,981", "2,982", "2,983", "2,987", "2,988", "2,989", "2,990",
> "2,991", "2,992", "2,993", "2,994", "2,996", "2,997", "2,998",
> "2,999", "20", "20,000", "20,001", "20,003", "20,004", "20,028",
> "20,035", "20,054", "20,066", "20,071", "20,077", "20,079",
> "20,088", "20,101", "20,104", "20,116", "20,120", "20,126",
> "20,151", "20,153", "20,157", "20,174", "20,176", "20,190",
> "20,191", "20,196", "20,199", "20,213", "20,214", "20,217",
> "20,225", "20,257", "20,262", "20,263", "20,288", "20,294",
> "20,307", "20,320", "20,325", "20,349", "20,356", "20,375",
> "20,385", "20,387", "20,401", "20,405", "20,412", "20,425",
> "20,443", "20,462", "20,517", "20,525", "20,526", "20,532",
> "20,542", "20,547", "20,557", "20,576,265", "20,601", "20,612",
> "20,623", "20,625", "20,641", "20,657", "20,690", "20,691",
> "20,693", "20,700", "20,712", "20,725", "20,728", "20,752,792",
> "20,754", "20,773", "20,779", "20,780", "20,784", "20,792",
> "20,821", "20,830", "20,873", "20,882", "20,890", "20,900",
> "20,906", "20,947", "20,956", "20,964", "20,979", "20,980",
> "200", "200,000", "200,014", "200,023", "200,058", "200,105",
> "200,372", "200,476", "200,736", "201", "201,583", "201,759",
> "201,844", "202", "202,093", "202,278", "202,414", "202,753",
> "203", "203,380", "203,388", "204", "204,184", "204,403",
> "204,760", "204,846", "204,922", "205", "205,000", "205,307",
> "205,559", "206", "206,705", "206,916", "207", "207,367",
> "208", "208,096", "208,267", "208,284", "209", "209,075",
> "209,355", "209,653", "21", "21,010", "21,042", "21,044",
> "21,061", "21,083", "21,084", "21,087", "21,090", "21,095",
> "21,106", "21,110", "21,147", "21,149", "21,163", "21,186",
> "21,215", "21,238", "21,246", "21,257", "21,265", "21,282",
> "21,292", "21,304", "21,317", "21,320", "21,324", "21,349",
> "21,371", "21,377", "21,382", "21,407", "21,428", "21,445",
> "21,450", "21,456", "21,480", "21,487", "21,498", "21,519",
> "21,555", "21,556", "21,559", "21,613", "21,614", "21,663",
> "21,666", "21,673", "21,681", "21,695", "21,698", "21,715",
> "21,732", "21,733", "21,739", "21,743", "21,752", "21,755",
> "21,785", "21,820", "21,822", "21,827", "21,841", "21,845",
> "21,857", "21,858", "21,860", "21,864", "21,883", "21,912",
> "21,914", "21,921", "21,937", "21,940", "21,941", "21,945",
> "21,948", "21,966", "21,970", "21,981", "21,991", "21,992",
> "210", "210,181", "210,386", "210,556", "210,760", "210,926",
> "211", "211,014", "211,361", "211,644", "212", "212,086",
> "212,117", "212,348", "212,651", "212,775", "212,874", "212,890",
> "212,925", "213", "213,095", "213,153", "213,723", "214",
> "214,186", "214,196", "214,216", "214,280", "214,355", "214,373",
> "214,511", "215", "215,146", "215,266", "215,270", "215,543",
> "216", "216,655", "216,787", "217", "217,075", "217,709",
> "217,851", "218", "218,073", "218,266", "219", "219,244",
> "219,285", "219,352", "219,458", "219,566", "22", "22,000",
> "22,004", "22,020", "22,046", "22,058", "22,070", "22,083",
> "22,175", "22,223", "22,245", "22,275", "22,285", "22,301",
> "22,304", "22,308", "22,320", "22,351", "22,352", "22,358",
> "22,374", "22,388", "22,435", "22,448", "22,449", "22,457",
> "22,462", "22,471", "22,473", "22,482", "22,493", "22,499",
> "22,500", "22,523", "22,531", "22,577", "22,594", "22,601",
> "22,613", "22,615", "22,652", "22,660", "22,666", "22,669",
> "22,679", "22,734", "22,743", "22,758", "22,776", "22,785",
> "22,791", "22,829", "22,911", "22,917", "22,928", "22,953",
> "22,981", "22,994", "220", "220,240", "220,297", "220,845",
> "221", "221,108", "221,113", "221,311", "221,536", "222",
> "222,378", "222,834", "222,877", "223", "223,016", "223,457",
> "223,729", "224", "224,258", "224,273", "224,551", "224,673",
> "224,735", "224,933", "224,948", "225", "225,000", "225,076",
> "225,118", "225,388", "225,398", "225,574", "225,708", "225,759",
> "225,779", "225,808", "225,933", "226", "226,210", "226,864",
> "226,871", "226,876", "227", "227,053", "227,361", "227,574",
> "228", "228,188", "228,540", "228,547", "228,697", "228,968",
> "229", "229,182", "229,445", "229,732", "229,893", "229,967",
> "23", "23,012", "23,019", "23,038", "23,041", "23,062", "23,068",
> "23,081", "23,098", "23,110", "23,117", "23,132", "23,164",
> "23,198", "23,200", "23,207", "23,225", "23,234", "23,239",
> "23,244", "23,257", "23,299", "23,304", "23,316", "23,324",
> "23,334", "23,342", "23,351", "23,363", "23,367", "23,373",
> "23,375", "23,387", "23,390", "23,404", "23,412", "23,420",
> "23,449", "23,454", "23,460", "23,468", "23,473", "23,484",
> "23,490", "23,549", "23,581", "23,633", "23,640", "23,641",
> "23,671", "23,674", "23,691", "23,694", "23,698", "23,707",
> "23,712", "23,730", "23,795", "23,805", "23,833", "23,838",
> "23,847", "23,851", "23,871", "23,881", "23,902", "23,924",
> "23,926", "23,935", "23,940", "23,984", "230", "230,005",
> "230,568", "230,675", "230,921", "231", "231,225", "232",
> "232,397", "232,420", "232,744", "233", "233,216", "233,481",
> "234", "234,226", "234,462", "234,699", "234,919", "235",
> "235,095", "235,577", "235,779", "236", "236,072", "236,809",
> "237", "237,094", "237,564", "238", "238,949", "239", "239,135",
> "239,748", "239,831", "24", "24,015", "24,017", "24,026",
> "24,045", "24,057", "24,070", "24,098", "24,112", "24,130",
> "24,135", "24,144", "24,166", "24,178", "24,185", "24,205",
> "24,208", "24,219", "24,230", "24,276", "24,290", "24,296",
> "24,345", "24,352", "24,384", "24,413", "24,432", "24,441",
> "24,468", "24,472", "24,480", "24,490", "24,519", "24,523",
> "24,554", "24,574", "24,616", "24,673", "24,678", "24,679",
> "24,705", "24,714", "24,746", "24,781", "24,825", "24,840",
> "24,876", "24,887", "24,899", "24,907", "24,944", "24,960",
> "24,979", "240", "240,000", "240,479", "240,548", "241",
> "241,657", "241,980", "242", "242,202", "242,450", "242,936",
> "243", "243,126", "243,142", "243,151", "243,223", "243,379",
> "244", "244,118", "244,136", "244,289", "244,690", "244,950",
> "245", "245,578", "246", "246,192", "246,262", "246,291",
> "246,292", "246,984", "247", "247,063", "247,183", "247,528",
> "247,778", "247,784", "248", "248,102", "248,405", "249",
> "249,169", "249,173", "249,530", "25", "25,000", "25,003",
> "25,013", "25,018", "25,039", "25,057", "25,070", "25,080",
> "25,089", "25,106", "25,116", "25,162", "25,166", "25,168",
> "25,205", "25,209", "25,231", "25,258", "25,273", "25,318",
> "25,345", "25,354", "25,373", "25,384", "25,388", "25,402",
> "25,476", "25,488", "25,529", "25,535", "25,556", "25,572",
> "25,573", "25,581", "25,659", "25,662", "25,685", "25,696",
> "25,706", "25,722", "25,727", "25,749", "25,751", "25,760",
> "25,782", "25,792", "25,820", "25,841", "25,858", "25,864",
> "25,876", "25,879", "25,881", "25,886", "25,905", "25,929",
> "25,954", "25,965", "250", "250,000", "250,275", "250,517",
> "250,954", "251", "251,329", "252", "252,000", "252,890",
> "253", "253,073", "253,983", "254", "254,151", "254,822",
> "255", "255,388", "255,737", "255,951", "256", "257", "257,111",
> "257,284", "257,475", "257,564", "257,585", "257,682", "258",
> "258,754", "258,814", "258,843", "259", "26", "26,000", "26,010",
> "26,017", "26,026", "26,063", "26,111", "26,123", "26,127",
> "26,130", "26,169", "26,183", "26,205", "26,216", "26,220",
> "26,237", "26,239", "26,274", "26,275", "26,296", "26,298",
> "26,311", "26,314", "26,332", "26,367", "26,395", "26,429",
> "26,452", "26,464", "26,487", "26,495", "26,500", "26,505",
> "26,506", "26,511", "26,608", "26,636", "26,645", "26,745",
> "26,758", "26,770", "26,781", "26,834", "26,873", "26,880",
> "26,889", "26,953", "26,981", "26,982", "260", "260,000",
> "260,002", "260,964", "261", "262", "262,378", "262,477",
> "262,724", "262,838", "263", "263,615", "263,638", "263,969",
> "264", "264,000", "264,967", "265", "265,366", "265,432",
> "265,536", "265,656", "265,691", "266", "266,164", "266,172",
> "266,327", "266,738", "267", "267,019", "267,232", "267,349",
> "267,856", "268", "269", "269,876", "27", "27,000", "27,069",
> "27,077", "27,112", "27,126", "27,131", "27,204", "27,214",
> "27,230", "27,249", "27,261", "27,280", "27,283", "27,318",
> "27,332", "27,339", "27,342", "27,350", "27,351", "27,361",
> "27,376", "27,398", "27,407", "27,420", "27,465", "27,472",
> "27,523", "27,534", "27,584", "27,610", "27,616", "27,638",
> "27,656", "27,767", "27,846", "27,861", "27,863", "27,895",
> "27,934", "27,935", "27,939", "27,943", "27,965", "27,973",
> "27,990", "270", "270,109", "270,130", "270,340", "270,623",
> "270,662", "271", "271,153", "271,515", "271,616", "271,709",
> "272", "272,672", "272,834", "273", "273,386", "273,866",
> "274", "274,170", "275", "275,227", "276", "276,000", "277",
> "277,464", "278", "279", "279,002", "279,119", "279,448",
> "279,871", "279,947", "28", "28,000", "28,008", "28,010",
> "28,021", "28,026", "28,051", "28,070", "28,090", "28,124",
> "28,144", "28,162", "28,193", "28,250", "28,251", "28,264",
> "28,276", "28,278", "28,312", "28,342", "28,343", "28,412",
> "28,445", "28,456", "28,487", "28,507", "28,595", "28,670",
> "28,672", "28,701", "28,703", "28,706", "28,757", "28,759",
> "28,764", "28,984", "28,990", "280", "281", "281,412", "282",
> "282,375", "282,512", "283", "283,951", "284", "285", "285,082",
> "285,170", "286", "286,123", "286,209", "286,632", "287",
> "287,456", "287,612", "288", "288,404", "288,827", "289",
> "289,903", "29", "29,002", "29,022", "29,030", "29,053",
> "29,090", "29,109", "29,164", "29,198", "29,226", "29,269",
> "29,300", "29,325", "29,331", "29,354", "29,356", "29,369",
> "29,418", "29,440", "29,445", "29,482", "29,557", "29,597",
> "29,600", "29,612", "29,622", "29,626", "29,632", "29,697",
> "29,753", "29,785", "29,794", "29,858", "29,876", "29,911",
> "29,928", "29,945", "29,949", "29,956", "29,959", "29,976",
> "29,985", "290", "290,000", "290,154", "291", "291,202",
> "291,970", "292", "293", "293,227", "293,316", "293,368",
> "294", "294,191", "294,674", "294,762", "294,816", "295",
> "296", "297", "297,884", "298", "298,408", "298,807", "298,848",
> "299", "3", "3,000", "3,001", "3,002", "3,004", "3,006",
> "3,007", "3,008,846", "3,010", "3,012", "3,013", "3,014",
> "3,015", "3,016", "3,019", "3,023", "3,024", "3,025", "3,026",
> "3,027", "3,028", "3,029", "3,030", "3,033", "3,034", "3,035",
> "3,036", "3,037", "3,038", "3,039", "3,041", "3,042", "3,047",
> "3,048", "3,051", "3,052", "3,054", "3,056", "3,058", "3,059",
> "3,060", "3,063", "3,064", "3,065", "3,066", "3,069", "3,071",
> "3,076", "3,077", "3,078", "3,079", "3,080", "3,082", "3,084",
> "3,085", "3,086", "3,088", "3,090", "3,091", "3,094", "3,096",
> "3,097", "3,098", "3,099", "3,100", "3,101", "3,103", "3,104",
> "3,107", "3,108", "3,110", "3,112", "3,113", "3,115", "3,116",
> "3,118", "3,119", "3,120", "3,121", "3,125", "3,126", "3,127",
> "3,128", "3,131", "3,132", "3,133", "3,134", "3,135,396",
> "3,136", "3,139,265", "3,143", "3,144", "3,146", "3,147",
> "3,148", "3,149", "3,150", "3,157", "3,159", "3,160", "3,161",
> "3,162", "3,164", "3,168", "3,169", "3,170", "3,171", "3,172",
> "3,173", "3,175", "3,177", "3,178", "3,179", "3,180", "3,181",
> "3,182", "3,183", "3,184,325", "3,185", "3,186", "3,187",
> "3,188", "3,189", "3,190", "3,191", "3,192", "3,194", "3,195",
> "3,196", "3,199", "3,200", "3,201", "3,205", "3,206", "3,208",
> "3,209", "3,210", "3,212", "3,214", "3,218", "3,220", "3,221",
> "3,224", "3,225", "3,226", "3,228", "3,233", "3,236", "3,239",
> "3,240", "3,243", "3,246", "3,247", "3,249", "3,250", "3,251",
> "3,254", "3,256", "3,257", "3,260", "3,261", "3,262", "3,263",
> "3,266", "3,267", "3,270", "3,272", "3,274", "3,276", "3,277",
> "3,278", "3,279", "3,280", "3,281", "3,282", "3,285", "3,287",
> "3,288", "3,289", "3,290", "3,292", "3,293", "3,294", "3,295",
> "3,296", "3,300", "3,300,000", "3,301", "3,303", "3,305",
> "3,306", "3,310", "3,313", "3,314", "3,317", "3,318", "3,321",
> "3,323", "3,324", "3,325", "3,327", "3,328", "3,330", "3,332",
> "3,332,653", "3,334", "3,335", "3,336", "3,338", "3,340",
> "3,341", "3,343", "3,347", "3,348", "3,350", "3,353", "3,355",
> "3,356", "3,359", "3,360", "3,361", "3,363", "3,365", "3,367",
> "3,368", "3,372", "3,375", "3,376", "3,377", "3,380", "3,381",
> "3,383", "3,384", "3,384,046", "3,385", "3,386", "3,388",
> "3,389", "3,389,352", "3,392", "3,393", "3,396", "3,398",
> "3,400", "3,401", "3,405", "3,406", "3,409", "3,410", "3,412",
> "3,413", "3,414", "3,419", "3,420", "3,422", "3,424", "3,425",
> "3,426", "3,428", "3,429", "3,430", "3,433", "3,435", "3,436",
> "3,437", "3,438", "3,439", "3,442", "3,444", "3,446", "3,447",
> "3,448", "3,450", "3,460", "3,461", "3,463", "3,464", "3,465",
> "3,466", "3,470", "3,473", "3,475", "3,476", "3,477", "3,478",
> "3,481", "3,482", "3,483", "3,486", "3,487", "3,488", "3,489",
> "3,491", "3,494", "3,495", "3,496", "3,500", "3,504", "3,506",
> "3,509", "3,510", "3,511", "3,514", "3,517", "3,518", "3,519",
> "3,525", "3,526", "3,527", "3,528", "3,529", "3,530", "3,537",
> "3,538", "3,539", "3,540", "3,541", "3,543", "3,544", "3,546",
> "3,548", "3,549", "3,550", "3,552", "3,554", "3,556", "3,557",
> "3,560", "3,563", "3,565", "3,566", "3,567", "3,569", "3,570",
> "3,571", "3,574", "3,575", "3,579", "3,580", "3,582", "3,583",
> "3,584", "3,585", "3,588", "3,589", "3,590", "3,591", "3,593",
> "3,595", "3,596", "3,597", "3,600", "3,601", "3,603", "3,605",
> "3,606", "3,607", "3,610", "3,612", "3,613", "3,615", "3,617",
> "3,618", "3,620", "3,621,805", "3,623", "3,624", "3,625",
> "3,627", "3,628,570", "3,631,857", "3,634", "3,635", "3,636",
> "3,638", "3,639,265", "3,641", "3,643", "3,645", "3,647",
> "3,648", "3,649", "3,650", "3,654", "3,656", "3,658", "3,662",
> "3,663", "3,666", "3,668", "3,669", "3,670", "3,671", "3,674",
> "3,675", "3,677", "3,678", "3,680", "3,681", "3,683", "3,684",
> "3,685", "3,690", "3,692", "3,693", "3,694", "3,695", "3,699",
> "3,700", "3,700,390", "3,701", "3,703", "3,706", "3,707",
> "3,709", "3,710", "3,712", "3,713", "3,714", "3,715", "3,716",
> "3,718", "3,719", "3,720", "3,722", "3,724", "3,725", "3,726",
> "3,728", "3,730", "3,732", "3,733", "3,736", "3,737", "3,738",
> "3,739,265", "3,740", "3,741", "3,742", "3,744", "3,746",
> "3,750", "3,753", "3,754", "3,755", "3,756", "3,758", "3,759",
> "3,760", "3,762", "3,763", "3,764", "3,769", "3,771", "3,772",
> "3,773", "3,775", "3,777", "3,780", "3,781", "3,782", "3,783",
> "3,786", "3,789", "3,791", "3,795", "3,800", "3,801", "3,803",
> "3,806", "3,807", "3,808", "3,810", "3,811", "3,812", "3,813",
> "3,815", "3,817", "3,818", "3,821", "3,823", "3,824", "3,825",
> "3,826", "3,827", "3,829", "3,830", "3,831", "3,836", "3,837",
> "3,838", "3,839", "3,839,265", "3,844", "3,845", "3,846,942",
> "3,849", "3,850", "3,851", "3,853", "3,854", "3,855", "3,857",
> "3,859", "3,860", "3,863", "3,865", "3,867", "3,868", "3,869",
> "3,870", "3,871", "3,873", "3,878", "3,879", "3,881", "3,882",
> "3,885", "3,893", "3,894", "3,895", "3,897", "3,899", "3,900",
> "3,903", "3,906", "3,907", "3,912", "3,914", "3,920", "3,921",
> "3,922", "3,926", "3,928,443", "3,933", "3,936", "3,938",
> "3,939", "3,940", "3,942", "3,943", "3,944", "3,947", "3,950",
> "3,956", "3,961", "3,962", "3,966", "3,968", "3,969", "3,971",
> "3,976", "3,978", "3,980", "3,980,495", "3,980,758", "3,982",
> "3,984", "3,986", "3,989", "3,989,249", "3,990", "3,990,681",
> "3,991", "3,991,146", "3,992", "3,994", "3,995", "3,999",
> "30", "30,000", "30,032", "30,058", "30,059", "30,061", "30,064",
> "30,067", "30,112", "30,113", "30,172", "30,186", "30,259",
> "30,263", "30,282", "30,290", "30,334", "30,337", "30,347",
> "30,349", "30,388", "30,404", "30,455", "30,460", "30,462",
> "30,463", "30,485", "30,534", "30,545", "30,571", "30,629",
> "30,645", "30,649", "30,651", "30,673", "30,710", "30,747",
> "30,771", "30,772", "30,811", "30,812", "30,847", "30,850",
> "30,864", "30,881", "30,946", "30,963", "30,980", "300",
> "300,000", "301", "301,727", "302", "303", "304", "304,314",
> "304,925", "304,946", "305", "305,197", "305,407", "305,491",
> "305,751", "306", "306,353", "306,357", "306,526", "307",
> "308", "308,402", "308,754", "308,910", "309", "309,353",
> "309,601", "309,785", "31", "31,001", "31,030", "31,066",
> "31,071", "31,100", "31,118", "31,129", "31,137", "31,150",
> "31,182", "31,201", "31,214", "31,257", "31,289", "31,291",
> "31,335", "31,364", "31,435", "31,451", "31,455", "31,460",
> "31,461", "31,475", "31,485", "31,495", "31,496", "31,502",
> "31,503", "31,534", "31,567", "31,583", "31,606", "31,632",
> "31,638", "31,662", "31,714", "31,763", "31,774", "31,801",
> "31,817", "31,846", "31,904", "31,932", "31,938", "31,941",
> "31,961", "31,972", "310", "310,230", "310,596", "310,617",
> "311", "312", "312,109", "312,701", "313", "314", "314,132",
> "315", "315,137", "315,753", "315,953", "316", "316,164",
> "317", "317,040", "317,317", "318", "318,278", "318,589",
> "319", "319,280", "319,411", "319,693", "32", "32,001", "32,087",
> "32,089", "32,101", "32,107", "32,109", "32,126", "32,134",
> "32,178", "32,197", "32,225", "32,242", "32,254", "32,275",
> "32,285", "32,337", "32,343", "32,370", "32,458", "32,461",
> "32,464", "32,490", "32,553", "32,584", "32,603", "32,628",
> "32,760", "32,844", "32,845", "32,860", "32,873", "32,889",
> "32,901", "32,927", "32,931", "32,980", "32,986", "320",
> "320,000", "320,920", "321", "321,490", "322", "322,250",
> "323", "323,445", "323,932", "324", "324,342", "324,523",
> "325", "325,769", "326", "326,079", "326,305", "327", "327,333",
> "327,936", "328", "328,093", "328,397", "328,644", "329",
> "329,101", "329,175", "329,352", "329,923", "33", "33,000",
> "33,024", "33,030", "33,035", "33,062", "33,064", "33,080",
> "33,082", "33,088", "33,098", "33,109", "33,199", "33,227",
> "33,232", "33,240", "33,248", "33,272", "33,280", "33,320",
> "33,325", "33,344", "33,408", "33,428", "33,434", "33,488",
> "33,519", "33,523", "33,540", "33,559", "33,562", "33,580",
> "33,624", "33,651", "33,690", "33,698", "33,724", "33,730",
> "33,831", "33,902", "33,917", "33,955", "33,962", "330",
> "330,168", "330,505", "330,641", "330,937", "331", "331,274",
> "332", "332,349", "333", "333,741", "334", "335", "335,770",
> "335,870", "336", "336,419", "336,658", "337", "337,245",
> "337,350", "337,855", "337,875", "338", "338,338", "338,507",
> "338,700", "339", "339,550", "339,675", "339,753", "34",
> "34,002", "34,003", "34,006", "34,018", "34,040", "34,047",
> "34,099", "34,129", "34,202", "34,207", "34,214", "34,216",
> "34,228", "34,232", "34,243", "34,250", "34,276", "34,292",
> "34,295", "34,329", "34,348", "34,453", "34,517", "34,560",
> "34,624", "34,625", "34,641", "34,667", "34,704", "34,722",
> "34,743", "34,756", "34,760", "34,762", "34,792", "34,827",
> "34,909", "34,934", "34,939", "34,946", "34,954", "34,964",
> "34,975", "34,980", "34,988", "340", "340,777", "341", "341,098",
> "341,213", "341,321", "342", "342,212", "343", "343,435",
> "344", "344,184", "344,374", "345", "345,000", "345,548",
> "346", "347", "347,413", "347,423", "347,699", "348", "348,936",
> "348,986", "349", "349,377", "349,380", "35", "35,000", "35,008",
> "35,009", "35,012", "35,054", "35,082", "35,104", "35,174",
> "35,180", "35,266", "35,340", "35,373", "35,392", "35,402",
> "35,437", "35,439", "35,462", "35,481", "35,528", "35,560",
> "35,564", "35,578", "35,630", "35,682", "35,731", "35,733",
> "35,737", "35,756", "35,823", "35,948", "35,951", "35,956",
> "35,959", "350", "350,000", "350,445", "350,614", "350,924",
> "351", "351,282", "351,401", "351,769", "352", "352,505",
> "353", "353,794", "353,938", "354", "355", "355,666", "355,924",
> "355,951", "356", "356,516", "356,517", "356,638", "357",
> "357,164", "358", "359", "359,697", "359,829", "359,857",
> "36", "36,077", "36,101", "36,156", "36,193", "36,210", "36,230",
> "36,238", "36,275", "36,300", "36,466", "36,481", "36,512",
> "36,545", "36,549", "36,577", "36,624", "36,631", "36,653",
> "36,679", "36,697", "36,772", "36,790", "36,808", "36,900",
> "36,941", "36,950", "36,972", "360", "360,924", "361", "362",
> "362,195", "363", "363,076", "364", "364,884", "365", "365,114",
> "365,253", "365,979", "366", "366,234", "366,930", "367",
> "367,164", "367,319", "368", "368,178", "368,852", "369",
> "369,660", "369,870", "37", "37,000", "37,080", "37,101",
> "37,111", "37,122", "37,170", "37,199", "37,205", "37,226",
> "37,229", "37,259", "37,261", "37,298", "37,305", "37,328",
> "37,365", "37,389", "37,401", "37,480", "37,489", "37,500",
> "37,506", "37,527", "37,667", "37,721", "37,760", "37,809",
> "37,860", "37,885", "37,924", "37,930", "37,943", "37,981",
> "370", "371", "372", "372,706", "372,761", "373", "374",
> "374,107", "375", "375,530", "375,531", "375,618", "375,738",
> "375,988", "376", "376,118", "376,406", "376,584", "377",
> "377,082", "377,309", "377,951", "377,952", "378", "378,415",
> "379", "379,012", "379,248", "379,547", "38", "38,013", "38,026",
> "38,121", "38,124", "38,185", "38,234", "38,265", "38,289",
> "38,307", "38,349", "38,363", "38,412", "38,416", "38,495",
> "38,606", "38,643", "38,648", "38,714", "38,727", "38,736",
> "38,776", "38,780", "38,794", "38,809", "38,852", "38,862",
> "38,909", "38,954", "38,992", "380", "380,000", "380,586",
> "381", "382", "383", "383,145", "383,728", "384", "385",
> "386", "387", "387,870", "388", "388,322", "388,589", "388,687",
> "388,762", "389", "39", "39,035", "39,095", "39,178", "39,180",
> "39,200", "39,214", "39,289", "39,291", "39,365", "39,372",
> "39,396", "39,413", "39,449", "39,460", "39,500", "39,534",
> "39,540", "39,560", "39,600", "39,665", "39,700", "39,800",
> "39,858", "39,860", "39,864", "39,875", "39,876", "39,922",
> "39,923", "390", "390,381", "390,909", "391", "391,199",
> "392", "393", "393,262", "393,802", "394", "394,969", "395",
> "395,462", "395,841", "396", "396,358", "396,743", "396,815",
> "397", "397,626", "398", "398,336", "398,763", "399", "399,326",
> "4", "4,000", "4,002", "4,003", "4,006", "4,007", "4,009",
> "4,011", "4,012", "4,017", "4,021", "4,022", "4,026", "4,028",
> "4,029", "4,031", "4,033", "4,033,984", "4,036", "4,042",
> "4,043", "4,045", "4,046", "4,047", "4,050", "4,052", "4,058",
> "4,060", "4,061", "4,062", "4,065", "4,066", "4,069", "4,070",
> "4,071", "4,073", "4,076", "4,077", "4,078", "4,079", "4,082",
> "4,083", "4,084", "4,085", "4,088", "4,090", "4,092", "4,094",
> "4,095", "4,096", "4,096,451", "4,097", "4,098", "4,100",
> "4,104", "4,105", "4,106", "4,107", "4,113", "4,114", "4,118",
> "4,119", "4,120,981", "4,121", "4,122", "4,125", "4,127",
> "4,129,366", "4,131", "4,136", "4,138", "4,139", "4,140",
> "4,142", "4,144", "4,146", "4,147", "4,149", "4,150", "4,152",
> "4,153", "4,156", "4,157", "4,159,363", "4,161", "4,163",
> "4,166", "4,167", "4,168", "4,169", "4,172,565", "4,173",
> "4,174", "4,175", "4,177", "4,178", "4,181", "4,182", "4,183",
> "4,184", "4,187", "4,189", "4,191", "4,193", "4,194", "4,195",
> "4,196", "4,200", "4,201", "4,204", "4,209", "4,210", "4,211",
> "4,214", "4,215", "4,216", "4,217", "4,218", "4,219", "4,221",
> "4,222", "4,223", "4,226", "4,227", "4,228,443", "4,230",
> "4,233", "4,234", "4,235", "4,237", "4,239", "4,241", "4,242",
> "4,244", "4,247", "4,250", "4,253", "4,254", "4,255", "4,257",
> "4,261", "4,262", "4,263", "4,266", "4,268", "4,272", "4,274",
> "4,275", "4,277", "4,280", "4,281", "4,284", "4,287", "4,289",
> "4,290", "4,291", "4,292", "4,295", "4,296", "4,298", "4,300",
> "4,306", "4,309", "4,315", "4,316", "4,318", "4,319", "4,321",
> "4,323", "4,326", "4,327", "4,328", "4,331", "4,333", "4,334",
> "4,340", "4,341", "4,344", "4,345", "4,346", "4,347", "4,349",
> "4,353", "4,354", "4,358", "4,359", "4,361", "4,362", "4,363",
> "4,365", "4,368", "4,369", "4,372", "4,374", "4,375", "4,376",
> "4,377", "4,379", "4,382", "4,386", "4,387", "4,389", "4,390",
> "4,394", "4,396", "4,399", "4,400", "4,404,618", "4,407",
> "4,410", "4,411", "4,412", "4,415", "4,416", "4,418", "4,421",
> "4,425", "4,426", "4,427", "4,428", "4,428,443", "4,432",
> "4,433", "4,435", "4,437", "4,439", "4,440", "4,444", "4,445",
> "4,446", "4,447", "4,448", "4,452", "4,453", "4,455", "4,456",
> "4,458", "4,460", "4,463", "4,466", "4,468", "4,470", "4,471",
> "4,472", "4,474", "4,476", "4,484", "4,485", "4,486,590",
> "4,489", "4,490", "4,491", "4,492", "4,493", "4,494", "4,496",
> "4,497", "4,500", "4,501", "4,502", "4,504", "4,505", "4,506",
> "4,507", "4,510", "4,511", "4,512", "4,515", "4,518", "4,520",
> "4,521", "4,522", "4,523", "4,526", "4,529", "4,530", "4,532",
> "4,535", "4,539", "4,540", "4,541", "4,542", "4,544", "4,545",
> "4,546", "4,548", "4,549", "4,550", "4,552", "4,556", "4,557",
> "4,560", "4,563", "4,564", "4,565", "4,567", "4,569", "4,570",
> "4,576", "4,577", "4,578", "4,582", "4,587", "4,589", "4,590",
> "4,591", "4,592", "4,596", "4,603", "4,608", "4,609", "4,610",
> "4,616", "4,619", "4,620", "4,621", "4,623", "4,626", "4,630",
> "4,631", "4,632", "4,639", "4,640", "4,641", "4,645", "4,648",
> "4,650", "4,652", "4,662", "4,664", "4,665", "4,666", "4,672",
> "4,673", "4,675", "4,680", "4,681", "4,682", "4,687", "4,689",
> "4,690", "4,691", "4,692", "4,693", "4,697", "4,698", "4,700",
> "4,706", "4,711", "4,713", "4,718", "4,719", "4,720", "4,723",
> "4,726", "4,729", "4,730", "4,731", "4,733", "4,736", "4,738",
> "4,739", "4,740", "4,742", "4,743", "4,744", "4,749", "4,754",
> "4,760", "4,763", "4,765", "4,767", "4,768", "4,773", "4,774",
> "4,775", "4,776", "4,777", "4,778", "4,780", "4,786", "4,789",
> "4,792", "4,793", "4,794", "4,795", "4,797", "4,799", "4,800",
> "4,812", "4,820", "4,823", "4,827", "4,828", "4,830", "4,832",
> "4,834", "4,838", "4,839", "4,840", "4,841", "4,843", "4,844",
> "4,851", "4,856", "4,864", "4,866", "4,870", "4,871,309",
> "4,873", "4,876", "4,881", "4,882", "4,888", "4,889", "4,890",
> "4,893", "4,894", "4,895", "4,896", "4,900", "4,902", "4,903",
> "4,905", "4,906", "4,908", "4,909", "4,910", "4,912", "4,914",
> "4,915", "4,920", "4,928", "4,929", "4,930", "4,931", "4,934",
> "4,938", "4,939", "4,943", "4,945", "4,948", "4,950", "4,954",
> "4,955", "4,957", "4,960", "4,961", "4,962", "4,968", "4,974",
> "4,975", "4,976", "4,978", "4,979", "4,982", "4,984", "4,988",
> "4,990", "4,993", "4,995", "4,996", "4,997", "4,999", "40",
> "40,000", "40,044", "40,066", "40,208", "40,221", "40,223",
> "40,290", "40,294", "40,361", "40,402", "40,429", "40,481",
> "40,506", "40,548", "40,603", "40,682", "40,703", "40,858",
> "40,865", "40,868", "40,888", "40,894", "40,927", "40,929",
> "40,986", "400", "400,000", "400,005", "400,077", "400,226",
> "401", "401,081", "402", "402,207", "402,982", "403", "403,434",
> "403,536", "404", "404,289", "404,665", "405", "406", "406,160",
> "407", "407,358", "407,450", "407,746", "407,795", "408",
> "408,305", "408,758", "409", "409,574", "409,745", "41",
> "41,066", "41,080", "41,099", "41,102", "41,108", "41,110",
> "41,111", "41,120", "41,154", "41,250", "41,254", "41,282",
> "41,299", "41,312", "41,404", "41,421", "41,466", "41,478",
> "41,510", "41,614", "41,636", "41,642", "41,653", "41,658",
> "41,665", "41,919", "41,952", "41,956", "41,964", "41,986",
> "410", "410,003", "411", "412", "412,558", "412,710", "412,837",
> "412,890", "413", "413,098", "414", "414,149", "414,150",
> "414,176", "415", "415,330", "415,390", "415,476", "415,709",
> "415,710", "416", "417", "417,502", "418", "418,375", "419",
> "419,711", "42", "42,009", "42,120", "42,154", "42,159",
> "42,228", "42,254", "42,268", "42,300", "42,301", "42,360",
> "42,465", "42,472", "42,517", "42,553", "42,561", "42,574",
> "42,618", "42,659", "42,718", "42,735", "42,737", "42,781",
> "42,836", "42,844", "42,915", "42,936", "42,949", "42,994",
> "420", "420,630", "421", "421,228", "421,537", "422", "422,006",
> "422,285", "422,286", "422,668", "422,895", "423", "424",
> "424,378", "425", "425,988", "426", "427", "427,448", "427,737",
> "428", "428,111", "428,114", "428,234", "428,887", "429",
> "429,066", "429,343", "43", "43,022", "43,058", "43,078",
> "43,132", "43,153", "43,206", "43,303", "43,408", "43,416",
> "43,486", "43,496", "43,499", "43,501", "43,515", "43,544",
> "43,646", "43,688", "43,747", "43,773", "43,830", "43,849",
> "43,910", "43,945", "43,963", "43,990", "430", "431", "431,296",
> "431,737", "432", "432,368", "433", "433,053", "433,631",
> "434", "435", "435,081", "435,450", "435,891", "436", "436,480",
> "436,778", "437", "437,001", "437,766", "437,799", "438",
> "439", "44", "44,080", "44,189", "44,236", "44,261", "44,275",
> "44,444", "44,469", "44,492", "44,517", "44,539", "44,561",
> "44,590", "44,591", "44,660", "44,710", "44,714", "44,727",
> "44,732", "44,832", "44,885", "44,926", "44,957", "440",
> "441", "441,354", "442", "442,942", "443", "443,581", "444",
> "444,143", "444,395", "444,620", "445", "445,267", "445,469",
> "445,517", "445,562", "445,903", "446", "446,020", "447",
> "447,640", "448", "448,290", "448,631", "448,671", "449",
> "449,913", "45", "45,000", "45,008", "45,041", "45,077",
> "45,144", "45,160", "45,255", "45,277", "45,282", "45,341",
> "45,366", "45,371", "45,544", "45,580", "45,612", "45,624",
> "45,625", "45,757", "45,829", "45,894", "45,964", "45,994",
> "450", "451", "451,183", "451,967", "452", "452,922", "453",
> "453,867", "454", "454,739", "455", "455,816", "456", "456,805",
> "457", "457,490", "457,638", "458", "458,920", "459", "459,447",
> "46", "46,030", "46,031", "46,084", "46,150", "46,198", "46,215",
> "46,232", "46,269", "46,350", "46,376", "46,383", "46,426",
> "46,447", "46,453", "46,454", "46,501", "46,511", "46,556",
> "46,565", "46,588", "46,598", "46,608", "46,612", "46,708",
> "46,743", "46,754", "46,759", "46,765", "46,780", "46,803",
> "46,906", "46,920", "46,936", "46,939", "46,979", "460",
> "460,110", "461", "461,473", "461,474", "462", "462,923",
> "462,951", "463,219", "464", "464,055", "464,818", "465",
> "465,741", "466", "466,563", "466,617", "466,965", "467",
> "468", "468,342", "469", "469,019", "469,049", "47", "47,004",
> "47,044", "47,096", "47,100", "47,217", "47,269", "47,391",
> "47,420", "47,462", "47,530", "47,584", "47,613", "47,698",
> "47,704", "47,714", "47,722", "47,764", "47,911", "47,967",
> "47,984", "470", "471", "472", "473", "474", "474,300", "475",
> "475,076", "476", "478", "479", "48", "48,018", "48,125",
> "48,215", "48,333", "48,397", "48,404", "48,407", "48,421",
> "48,439", "48,495", "48,604", "48,605", "48,609", "48,663",
> "48,702", "48,728", "48,774", "48,880", "48,911", "48,968",
> "48,977", "480", "481", "481,043", "481,101", "481,596",
> "481,599", "481,798", "482", "482,037", "482,158", "482,692",
> "483", "484", "484,505", "485", "485,793", "486", "486,911",
> "487", "487,310", "488", "488,953", "489", "49", "49,012",
> "49,092", "49,106", "49,141", "49,266", "49,269", "49,291",
> "49,297", "49,298", "49,453", "49,464", "49,512", "49,537",
> "49,538", "49,625", "49,682", "49,726", "49,741", "49,797",
> "49,811", "49,841", "49,949", "49,963", "49,976", "49,988",
> "490", "490,025", "490,026", "490,513", "491", "492", "492,651",
> "492,652", "492,991", "493", "493,502", "493,530", "494",
> "494,654", "495", "496", "496,291", "497", "497,528", "497,641",
> "498", "499", "5", "5,000", "5,003", "5,004", "5,005", "5,009",
> "5,011", "5,012", "5,019", "5,020", "5,022", "5,024", "5,028",
> "5,031", "5,033", "5,038", "5,039", "5,040", "5,041", "5,055",
> "5,056", "5,059", "5,064", "5,067", "5,071", "5,072", "5,074",
> "5,075", "5,076", "5,077", "5,078", "5,079", "5,081", "5,083",
> "5,084", "5,085", "5,088", "5,090", "5,092", "5,099", "5,103",
> "5,104", "5,105", "5,107", "5,117", "5,119", "5,120", "5,121",
> "5,121,023", "5,123", "5,127", "5,129", "5,130", "5,131",
> "5,132", "5,135", "5,136", "5,140", "5,149", "5,150", "5,151",
> "5,152", "5,153", "5,155", "5,156", "5,159", "5,160", "5,164",
> "5,169", "5,170", "5,171", "5,172", "5,175", "5,176", "5,178",
> "5,179", "5,182", "5,183", "5,189", "5,192", "5,194", "5,199",
> "5,202", "5,206", "5,212", "5,222", "5,225", "5,226", "5,229",
> "5,231", "5,232", "5,233", "5,234", "5,239", "5,241", "5,243",
> "5,244", "5,247", "5,253", "5,256", "5,258", "5,260", "5,262",
> "5,263", "5,267", "5,270", "5,271", "5,272", "5,276", "5,278",
> "5,280", "5,282", "5,285", "5,287", "5,288", "5,289", "5,290",
> "5,292", "5,295", "5,297", "5,298", "5,300", "5,302", "5,306",
> "5,307", "5,308", "5,310", "5,312", "5,313", "5,317", "5,319",
> "5,322", "5,325", "5,326", "5,331", "5,333", "5,334", "5,337",
> "5,338", "5,340", "5,341", "5,343", "5,345", "5,346", "5,347",
> "5,350", "5,356", "5,357,862", "5,359", "5,360", "5,363",
> "5,366", "5,367", "5,370", "5,377", "5,380", "5,387", "5,390",
> "5,392", "5,393", "5,395", "5,396", "5,397", "5,399", "5,400",
> "5,401", "5,407", "5,414", "5,417", "5,424", "5,430", "5,432",
> "5,435", "5,437", "5,438", "5,439", "5,441", "5,442", "5,443",
> "5,446", "5,448", "5,450", "5,451", "5,454", "5,455", "5,457",
> "5,458", "5,460", "5,461", "5,462", "5,469", "5,470", "5,471",
> "5,473", "5,482", "5,487", "5,489", "5,490", "5,493", "5,495",
> "5,496", "5,498", "5,499", "5,500", "5,501", "5,502", "5,503",
> "5,505", "5,506", "5,507", "5,510", "5,511", "5,512", "5,518",
> "5,520", "5,529", "5,531", "5,535", "5,539", "5,540", "5,542",
> "5,543", "5,544", "5,545", "5,546", "5,548", "5,554", "5,558",
> "5,561,368", "5,572", "5,573", "5,574", "5,576", "5,578",
> "5,579", "5,580", "5,582", "5,587", "5,594", "5,595", "5,600",
> "5,602", "5,610", "5,612", "5,614", "5,618", "5,619", "5,622",
> "5,623", "5,624", "5,625", "5,626", "5,629", "5,633", "5,635",
> "5,636", "5,637", "5,642", "5,643", "5,647", "5,656", "5,658",
> "5,667", "5,668", "5,672", "5,678", "5,683", "5,685", "5,686",
> "5,687", "5,690", "5,691", "5,695", "5,697", "5,698", "5,700",
> "5,702", "5,703", "5,704", "5,705", "5,707", "5,708", "5,709",
> "5,712", "5,713", "5,714", "5,720", "5,724", "5,725", "5,727",
> "5,728", "5,729", "5,731,780", "5,733", "5,734", "5,736",
> "5,742", "5,743", "5,744", "5,746", "5,748", "5,752", "5,755",
> "5,762", "5,768", "5,771", "5,783", "5,784", "5,785", "5,787",
> "5,790", "5,792", "5,794", "5,799,213", "5,802", "5,803",
> "5,805", "5,813", "5,814", "5,816", "5,817", "5,819", "5,822",
> "5,823", "5,827", "5,834", "5,835", "5,836", "5,838", "5,839",
> "5,843", "5,844", "5,845", "5,846", "5,847", "5,850", "5,854",
> "5,860", "5,861", "5,864", "5,865", "5,870", "5,871", "5,873",
> "5,874", "5,878", "5,880", "5,881", "5,884", "5,885", "5,886",
> "5,891", "5,893", "5,895", "5,896", "5,898", "5,900", "5,902",
> "5,905", "5,914", "5,917", "5,918", "5,919", "5,924", "5,926",
> "5,927", "5,931", "5,932", "5,933", "5,936", "5,940", "5,942",
> "5,942,756", "5,952", "5,956", "5,958", "5,964", "5,965",
> "5,966", "5,970", "5,974", "5,978", "5,979", "5,982", "5,984",
> "5,993,253", "5,999", "50", "50,000", "50,120", "50,152",
> "50,154", "50,189", "50,205", "50,220", "50,223", "50,242",
> "50,344", "50,488", "50,543", "50,564", "50,591", "50,600",
> "50,640", "50,701", "50,746", "50,762", "50,874", "50,890",
> "500", "500,000", "500,160", "500,739", "500,768", "501",
> "501,115", "502", "502,341", "503", "503,474", "504", "504,516",
> "504,808", "504,816", "504,873", "505,000", "505,516", "506",
> "507", "507,516", "508", "509", "51", "51,000", "51,036",
> "51,044", "51,092", "51,228", "51,229", "51,292", "51,327",
> "51,339", "51,343", "51,345", "51,369", "51,394", "51,425",
> "51,504", "51,589", "51,649", "51,694", "51,757", "51,795",
> "51,841", "51,863", "51,951", "51,959", "51,972", "51,981",
> "510", "510,245", "511", "511,338", "511,362", "511,730",
> "511,780", "511,808", "512", "513", "514", "514,620", "515",
> "515,585", "516", "516,125", "517", "517,825", "518", "518,782",
> "518,871", "519", "519,230", "519,830", "52", "52,098", "52,104",
> "52,379", "52,401", "52,543", "52,557", "52,663", "52,744",
> "52,866", "52,868", "52,966", "520", "521", "522", "522,210",
> "523", "524", "524,510", "525", "526", "526,800", "527",
> "527,449", "528", "528,103", "529", "53", "53,023", "53,025",
> "53,143", "53,209", "53,286", "53,297", "53,385", "53,407",
> "53,420", "53,563", "53,632", "53,638", "53,687", "53,722",
> "53,755", "53,777", "53,815", "53,833", "53,933", "53,978",
> "530", "530,066", "530,292", "530,418", "530,577", "530,948",
> "531", "532", "533", "533,487", "533,659", "534", "535",
> "535,790", "536", "536,891", "537", "538", "539", "539,466",
> "539,689", "54", "54,000", "54,127", "54,158", "54,197",
> "54,222", "54,267", "54,486", "54,496", "54,566", "54,602",
> "54,614", "54,621", "54,630", "54,663", "54,670", "54,728",
> "54,751", "54,865", "54,888", "54,967", "54,981", "54,997",
> "540", "541", "541,432", "542", "542,114", "542,597", "543",
> "543,063", "543,660", "544", "544,094", "545", "545,593",
> "546", "547", "547,017", "547,966", "548", "549,076", "549,245",
> "55", "55,000", "55,010", "55,039", "55,089", "55,127", "55,388",
> "55,433", "55,442", "55,480", "55,527", "55,563", "55,626",
> "55,631", "55,653", "55,682", "55,690", "55,716", "55,810",
> "55,819", "55,919", "55,987", "550", "550,000", "550,027",
> "550,174", "550,946", "551", "551,778", "552", "552,640",
> "553", "553,835", "554", "555", "556", "556,457", "557",
> "557,051", "558", "558,658", "559", "56", "56,081", "56,194",
> "56,260", "56,261", "56,410", "56,414", "56,477", "56,568",
> "56,575", "56,607", "56,617", "56,715", "56,718", "56,810",
> "56,898", "56,913", "56,919", "56,943", "56,964", "560",
> "560,305", "560,878", "561", "562", "563", "563,878", "564",
> "565", "565,621", "565,815", "566", "566,440", "567", "567,159",
> "567,755", "568", "568,658", "569", "569,839", "57", "57,000",
> "57,001", "57,276", "57,288", "57,363", "57,443", "57,496",
> "57,546", "57,602", "57,642", "57,754", "57,780", "570",
> "570,424", "570,790", "571", "572", "573", "574", "574,204",
> "574,393", "574,419", "575,669", "576", "577,227", "577,764",
> "577,941", "578", "578,276", "579", "579,095", "579,399",
> "579,505", "58", "58,172", "58,200", "58,214", "58,257",
> "58,358", "58,424", "58,425", "58,539", "58,638", "58,675",
> "58,717", "58,718", "58,725", "58,748", "58,780", "58,844",
> "58,901", "58,947", "580", "581", "581,403", "582", "582,022",
> "582,093", "582,747", "582,974", "583", "583,657", "583,850",
> "584", "585", "586", "586,564", "587", "588", "588,249",
> "589", "589,698", "59", "59,152", "59,218", "59,261", "59,298",
> "59,375", "59,457", "59,654", "59,684", "59,826", "59,845",
> "59,925", "59,939", "590", "591", "591,363", "591,885", "591,998",
> "592", "593", "593,154", "594", "594,771", "595", "596",
> "596,317", "597", "598", "599", "6", "6,000", "6,002", "6,004",
> "6,006", "6,007", "6,008", "6,010", "6,011", "6,012", "6,015",
> "6,017", "6,018", "6,020", "6,023", "6,028", "6,031", "6,033",
> "6,036", "6,039", "6,040", "6,049", "6,054", "6,055", "6,056",
> "6,059", "6,061,368", "6,067", "6,069", "6,070", "6,072",
> "6,075", "6,082", "6,092", "6,093", "6,095,382", "6,096",
> "6,098", "6,099", "6,101", "6,106", "6,107", "6,109", "6,110",
> "6,112", "6,116", "6,117", "6,122", "6,125", "6,126", "6,136",
> "6,139", "6,141", "6,143", "6,144", "6,145", "6,148", "6,156",
> "6,158", "6,159", "6,172", "6,177", "6,179", "6,183", "6,186",
> "6,189", "6,191", "6,195", "6,197", "6,200", "6,202", "6,205",
> "6,206", "6,207", "6,210", "6,214", "6,215", "6,218", "6,219",
> "6,220", "6,224", "6,227", "6,228", "6,230", "6,231", "6,232",
> "6,241", "6,242", "6,243", "6,247", "6,252", "6,253", "6,256",
> "6,257", "6,265", "6,266", "6,270", "6,272", "6,273", "6,274",
> "6,275", "6,280", "6,282", "6,283", "6,284", "6,285", "6,287",
> "6,292", "6,295", "6,299", "6,303", "6,304", "6,306", "6,308",
> "6,310", "6,314", "6,320", "6,323", "6,324", "6,326", "6,328",
> "6,330", "6,333", "6,335", "6,342", "6,351", "6,360", "6,362",
> "6,371", "6,372", "6,374", "6,377", "6,382", "6,383", "6,385",
> "6,392", "6,394", "6,395", "6,396", "6,399", "6,403", "6,404",
> "6,411", "6,420", "6,422", "6,423", "6,424", "6,425", "6,431",
> "6,435", "6,443", "6,444", "6,445", "6,450", "6,456", "6,461",
> "6,465", "6,465,986", "6,467", "6,469", "6,470", "6,471",
> "6,480", "6,486", "6,493", "6,502", "6,505", "6,513", "6,514",
> "6,517", "6,520", "6,521", "6,530", "6,546", "6,546,586",
> "6,550", "6,556", "6,560", "6,561", "6,562", "6,565", "6,566",
> "6,569", "6,570", "6,571", "6,577", "6,578", "6,580", "6,581",
> "6,582", "6,586", "6,591", "6,594", "6,595", "6,597", "6,599",
> "6,600", "6,601", "6,604", "6,611", "6,615", "6,616", "6,619",
> "6,622", "6,625", "6,628", "6,630", "6,636", "6,637", "6,638",
> "6,643", "6,646", "6,650", "6,652", "6,653", "6,654", "6,659",
> "6,660", "6,661", "6,664", "6,666", "6,667", "6,669", "6,670",
> "6,675", "6,678", "6,680", "6,681", "6,682", "6,685", "6,686",
> "6,689", "6,691", "6,692", "6,694", "6,700", "6,701", "6,706",
> "6,707", "6,709", "6,712", "6,713", "6,716", "6,717", "6,719",
> "6,723", "6,725", "6,731", "6,732", "6,733", "6,734", "6,737",
> "6,741", "6,745", "6,750", "6,752", "6,753", "6,758", "6,765",
> "6,766", "6,769", "6,778", "6,779", "6,782", "6,783", "6,784",
> "6,787", "6,790", "6,803", "6,806", "6,811", "6,812", "6,815",
> "6,817", "6,818,080", "6,828", "6,829", "6,830", "6,832",
> "6,836", "6,839", "6,841", "6,842", "6,845", "6,846", "6,847",
> "6,850", "6,852", "6,855", "6,860", "6,861", "6,862", "6,863",
> "6,867", "6,869", "6,872", "6,877", "6,880", "6,890", "6,893",
> "6,898", "6,901", "6,902", "6,903", "6,908", "6,912", "6,916",
> "6,918", "6,919", "6,928", "6,930", "6,931", "6,932", "6,935",
> "6,939", "6,941", "6,943", "6,945", "6,948", "6,950", "6,952",
> "6,955", "6,960", "6,961", "6,965", "6,970", "6,971", "6,972",
> "6,978", "6,997", "6,998", "60", "60,000", "60,001", "60,007",
> "60,155", "60,162", "60,199", "60,270", "60,320", "60,443",
> "60,590", "60,602", "60,635", "60,638", "60,766", "60,802",
> "60,842", "60,968", "60,977", "60,992", "600", "600,000",
> "600,270", "600,687", "600,774", "601", "601,174", "602",
> "602,016", "603", "603,500", "604", "604,235", "604,858",
> "605", "605,302", "605,644", "606", "606,124", "606,261",
> "606,275", "607", "608", "609", "61", "61,023", "61,097",
> "61,113", "61,351", "61,389", "61,437", "61,709", "61,750",
> "61,798", "61,899", "61,955", "61,981", "610", "611", "611,459",
> "612", "612,000", "613", "613,478", "614", "614,167", "615",
> "616", "616,084", "617", "617,344", "618", "618,788", "618,998",
> "619", "619,871", "62", "62,130", "62,134", "62,165", "62,237",
> "62,348", "62,393", "62,400", "62,476", "62,501", "62,513",
> "62,570", "62,626", "62,773", "62,832", "62,854", "62,866",
> "62,882", "62,911", "62,997", "620", "620,236", "620,998",
> "621", "622", "623", "623,686", "624", "626", "627", "627,152",
> "627,459", "628", "629", "629,304", "629,673", "629,797",
> "63", "63,010", "63,066", "63,211", "63,219", "63,305", "63,332",
> "63,425", "63,439", "63,482", "63,536", "63,619", "63,783",
> "63,875", "63,935", "630", "630,687", "631", "631,194", "632",
> "633", "634", "634,167", "635", "637", "638", "639", "64",
> "64,079", "64,089", "64,216", "64,268", "64,326", "64,339",
> "64,417", "64,474", "64,481", "64,547", "64,607", "64,848",
> "64,967", "640", "640,433", "640,589", "640,667", "641",
> "642", "643", "644", "645", "645,652", "645,957", "646",
> "647", "647,275", "648", "648,564", "649", "65", "65,000",
> "65,122", "65,127", "65,344", "65,450", "65,612", "65,800",
> "65,847", "65,856", "65,999", "650", "650,174", "650,456",
> "651", "652", "652,971", "653", "654", "654,156", "654,176",
> "655", "656,803", "657", "658", "658,112", "658,471", "659",
> "66", "66,034", "66,133", "66,158", "66,224", "66,238", "66,327",
> "66,338", "66,499", "66,500", "66,510", "66,545", "66,628",
> "66,688", "66,739", "66,862", "66,919", "66,987", "660",
> "661", "661,042", "661,359", "661,579", "661,951", "662",
> "662,205", "662,680", "663", "663,477", "663,807", "664",
> "664,079", "665", "665,110", "666", "666,432", "666,925",
> "667", "668", "668,906", "669", "67", "67,056", "67,111",
> "67,165", "67,167", "67,370", "67,380", "67,416", "67,887",
> "67,918", "670", "671", "671,199", "671,737", "672", "673",
> "673,437", "674", "674,675", "675", "676", "676,294", "676,296",
> "677", "678", "678,500", "679", "679,260", "68", "68,012",
> "68,017", "68,032", "68,064", "68,091", "68,103", "68,470",
> "68,521", "68,630", "68,795", "68,971", "680", "680,845",
> "681,062", "682", "683", "683,859", "684", "685", "685,929",
> "686", "687", "687,886", "688", "688,785", "689", "689,918",
> "69", "69,047", "69,063", "69,075", "69,365", "69,377", "69,379",
> "69,492", "69,552", "69,599", "69,665", "69,682", "69,774",
> "690", "690,344", "691", "691,116", "692,442", "693", "694",
> "694,856", "695", "695,466", "696", "697", "697,272", "697,418",
> "698", "698,618", "699,629", "699,716", "7", "7,000", "7,001",
> "7,013", "7,015", "7,017", "7,030", "7,039", "7,042", "7,052",
> "7,054", "7,057", "7,060", "7,064", "7,065", "7,066", "7,070",
> "7,071", "7,075", "7,076", "7,080", "7,082", "7,085", "7,090",
> "7,098", "7,101", "7,105", "7,106", "7,110", "7,115", "7,116",
> "7,121", "7,122", "7,125", "7,126", "7,129", "7,130", "7,131",
> "7,135", "7,138", "7,141", "7,143", "7,148", "7,150", "7,154",
> "7,159", "7,160", "7,171", "7,182", "7,184", "7,186", "7,189",
> "7,190", "7,196", "7,198", "7,202", "7,207", "7,208", "7,209",
> "7,211", "7,228", "7,232", "7,233", "7,235", "7,236", "7,241",
> "7,250", "7,255", "7,256", "7,259", "7,260", "7,264", "7,265",
> "7,266", "7,270", "7,271", "7,272", "7,274", "7,275", "7,277",
> "7,280", "7,289", "7,291", "7,292", "7,293", "7,300", "7,309",
> "7,310", "7,312", "7,317", "7,325", "7,326", "7,328", "7,329",
> "7,330", "7,337", "7,344", "7,346", "7,353", "7,359", "7,363",
> "7,368", "7,377", "7,378", "7,382", "7,383", "7,387", "7,388",
> "7,397", "7,398", "7,400", "7,401", "7,404", "7,409", "7,410",
> "7,412", "7,416", "7,418", "7,419", "7,420", "7,422", "7,426",
> "7,432", "7,434", "7,439", "7,445", "7,450", "7,451", "7,454",
> "7,459", "7,466", "7,467", "7,468", "7,473", "7,479", "7,486",
> "7,487", "7,488", "7,501", "7,508", "7,509", "7,515", "7,519",
> "7,522", "7,523", "7,533", "7,534", "7,538", "7,543", "7,544",
> "7,548", "7,555", "7,557", "7,559", "7,560", "7,561", "7,566",
> "7,573", "7,580", "7,585", "7,592", "7,595", "7,599", "7,600",
> "7,603", "7,608", "7,615", "7,616", "7,624", "7,632", "7,645",
> "7,647", "7,649", "7,653", "7,661", "7,664", "7,668", "7,670",
> "7,674", "7,676", "7,679", "7,684", "7,699", "7,706", "7,715",
> "7,718", "7,722", "7,725", "7,728", "7,732", "7,735", "7,743",
> "7,744", "7,752", "7,758", "7,767", "7,770", "7,779", "7,780",
> "7,781", "7,786", "7,793", "7,794", "7,800", "7,801", "7,802",
> "7,805", "7,806", "7,812", "7,814", "7,823", "7,824", "7,826",
> "7,827", "7,836", "7,838", "7,840", "7,848", "7,851", "7,852",
> "7,853,689", "7,856", "7,865", "7,866", "7,871", "7,873",
> "7,876", "7,878", "7,880", "7,884", "7,890", "7,898", "7,901",
> "7,905", "7,912", "7,915", "7,926", "7,927", "7,928", "7,932",
> "7,935", "7,937", "7,946", "7,954", "7,955", "7,965", "7,980",
> "7,985", "7,991", "7,994", "7,995", "70", "70,000", "70,020",
> "70,137", "70,219", "70,221", "70,322", "70,439", "70,446",
> "70,457", "70,536", "70,554", "70,580", "70,601", "70,658",
> "70,664", "70,690", "70,758", "70,768", "70,821", "700",
> "700,000", "701", "702", "702,522", "703", "703,916", "704",
> "705", "705,024", "706", "707", "708", "708,772", "708,812",
> "709", "709,083", "71", "71,009", "71,028", "71,113", "71,270",
> "71,291", "71,399", "71,433", "71,551", "71,630", "71,664",
> "71,678", "71,857", "71,944", "710", "711", "711,417", "712",
> "712,648", "714", "715", "716", "717", "717,721", "719,199",
> "72", "72,089", "72,096", "72,110", "72,126", "72,159", "72,172",
> "72,393", "72,422", "72,459", "72,545", "72,667", "72,872",
> "72,877", "72,974", "720", "721", "721,860", "722", "723",
> "723,576", "724", "724,152", "725", "725,842", "726", "726,078",
> "726,581", "726,916", "727", "727,789", "728", "729", "73",
> "73,022", "73,070", "73,114", "73,255", "73,518", "73,679",
> "73,828", "73,894", "73,960", "730", "730,834", "731", "731,062",
> "732", "733", "734", "734,873", "735", "736,572", "737",
> "738", "739", "739,923", "74", "74,044", "74,326", "74,464",
> "74,576", "74,630", "74,656", "74,850", "74,942", "740",
> "740,029", "740,350", "740,752", "740,866", "741", "742",
> "743", "743,582", "743,926", "744", "745", "745,866", "746",
> "746,130", "747", "748", "748,866", "749,000", "749,866",
> "75", "75,000", "75,022", "75,207", "75,302", "75,360", "75,367",
> "75,672", "75,819", "75,915", "75,927", "75,943", "75,966",
> "750", "750,581", "750,866", "751", "751,866", "752", "753",
> "753,866", "754", "755", "755,981", "756", "757", "758,634",
> "759", "76", "76,031", "76,035", "76,218", "76,316", "76,717",
> "76,721", "76,758", "76,853", "76,863", "76,934", "76,936",
> "760", "761", "762", "762,243", "762,516", "763", "763,466",
> "764", "764,205", "764,663", "765,903", "766", "767,970",
> "768", "768,572", "769", "77", "77,097", "77,281", "77,388",
> "77,538", "77,706", "77,741", "77,756", "77,873", "770",
> "770,239", "771", "772", "772,831", "773", "774", "775",
> "776,668", "778", "779", "78", "78,002", "78,004", "78,067",
> "78,113", "78,162", "78,198", "78,247", "78,310", "78,390",
> "78,401", "78,556", "78,840", "78,873", "78,905", "780",
> "780,535", "781", "782", "783", "784", "785", "785,302",
> "786", "787", "787,103", "788", "788,205", "789", "79", "79,072",
> "79,103", "79,123", "79,173", "79,193", "79,240", "79,300",
> "79,515", "79,750", "790", "791", "791,473", "792", "793",
> "794", "794,296", "794,636", "795", "795,961", "796", "796,008",
> "797", "798", "798,443", "798,660", "799", "799,209", "799,360",
> "8", "8,000", "8,010", "8,012", "8,013", "8,025", "8,028",
> "8,030", "8,031", "8,034", "8,037", "8,040", "8,043", "8,044",
> "8,046", "8,051", "8,053", "8,054", "8,056", "8,062", "8,063",
> "8,066", "8,068", "8,076", "8,082", "8,086", "8,089", "8,093",
> "8,094", "8,095", "8,096", "8,099", "8,100", "8,102", "8,106",
> "8,115", "8,116", "8,123", "8,125", "8,129", "8,139", "8,142",
> "8,143", "8,147", "8,150", "8,153", "8,154", "8,156", "8,159",
> "8,161", "8,166", "8,167", "8,168", "8,169", "8,178", "8,179",
> "8,188", "8,193", "8,194", "8,200", "8,203", "8,204", "8,207",
> "8,215", "8,220", "8,221", "8,222", "8,224", "8,228", "8,231",
> "8,244", "8,245", "8,257", "8,258", "8,260", "8,264", "8,269",
> "8,271", "8,272", "8,274", "8,280", "8,293", "8,295", "8,297",
> "8,299", "8,300", "8,303", "8,304", "8,305", "8,313", "8,319",
> "8,321", "8,325", "8,327", "8,331", "8,337", "8,345", "8,349",
> "8,351", "8,354", "8,356", "8,359", "8,363", "8,364", "8,367",
> "8,371", "8,372", "8,377", "8,378", "8,379", "8,381", "8,383",
> "8,384", "8,389", "8,391", "8,392", "8,400", "8,404,990",
> "8,409", "8,410", "8,412", "8,418", "8,421", "8,426", "8,427",
> "8,428", "8,433", "8,437", "8,441", "8,442", "8,449", "8,455",
> "8,456", "8,457", "8,460", "8,462", "8,463", "8,465", "8,468",
> "8,477", "8,478", "8,480", "8,483", "8,485", "8,485,478",
> "8,492", "8,496", "8,505", "8,506", "8,512", "8,519,374",
> "8,524", "8,531", "8,533", "8,557", "8,562", "8,565", "8,566",
> "8,574", "8,575", "8,576", "8,578", "8,583", "8,585", "8,595",
> "8,596", "8,599", "8,601", "8,604", "8,612", "8,619", "8,620",
> "8,623", "8,633", "8,638", "8,640", "8,643", "8,651", "8,655",
> "8,664", "8,666", "8,671", "8,673", "8,674", "8,675", "8,684",
> "8,690", "8,692", "8,696", "8,699", "8,715", "8,719", "8,720",
> "8,724", "8,725", "8,734", "8,735", "8,740", "8,741", "8,744",
> "8,746", "8,747", "8,750", "8,751", "8,755", "8,764", "8,772",
> "8,775", "8,777", "8,779", "8,796", "8,800", "8,803,372",
> "8,812", "8,813", "8,822", "8,824", "8,827", "8,830", "8,835",
> "8,842", "8,844", "8,848", "8,849", "8,853", "8,855", "8,857",
> "8,866", "8,868", "8,869", "8,870", "8,871", "8,886", "8,888",
> "8,892", "8,909", "8,911", "8,914,854", "8,915", "8,927",
> "8,932", "8,946", "8,947", "8,951", "8,963", "8,964", "8,967",
> "8,973", "8,981", "8,983", "8,991", "8,994", "8,996", "80",
> "80,089", "80,234", "80,280", "80,900", "80,937", "80,960",
> "800", "800,000", "800,782", "801", "802", "802,588", "803",
> "804", "804,534", "805", "805,188", "806", "806,795", "807",
> "808", "809", "81", "81,254", "81,287", "81,402", "81,516",
> "81,742", "81,811", "81,938", "81,948", "81,953", "810",
> "810,525", "811", "812", "812,686", "814,874", "815", "816",
> "817", "817,928", "818", "819", "819,445", "82", "82,000",
> "82,071", "82,177", "82,194", "82,398", "82,452", "82,510",
> "82,524", "82,608", "82,680", "82,692", "82,741", "82,764",
> "82,805", "82,985", "820", "821", "821,147", "822", "823",
> "824", "825", "826", "827", "827,003", "827,183", "828",
> "828,610", "83", "83,052", "83,118", "83,199", "83,222",
> "83,235", "83,386", "83,387", "83,466", "83,597", "83,604",
> "83,817", "830", "830,543", "831", "831,330", "832", "833",
> "833,804", "834", "834,183", "834,464", "835", "835,017",
> "836", "836,704", "837", "837,146", "838", "839", "84", "84,011",
> "84,012", "84,168", "84,270", "84,345", "84,473", "84,550",
> "84,580", "84,719", "84,741", "84,826", "840", "840,833",
> "841", "841,855", "841,887", "841,925", "842", "842,555",
> "843", "844,317", "844,621", "844,700", "845", "846", "847",
> "848", "848,288", "848,573", "849", "849,036", "85", "85,000",
> "85,128", "85,149", "85,157", "85,397", "85,452", "85,639",
> "85,751", "85,818", "85,973", "850", "851,620", "852", "852,301",
> "853", "854", "855", "856", "856,334", "857", "859", "859,964",
> "86", "86,267", "86,571", "86,794", "86,835", "86,849", "86,899",
> "86,946", "860", "860,842", "861", "861,477", "862", "863",
> "863,015", "863,423", "864", "864,771", "865", "865,001",
> "865,789", "866", "867", "867,814", "868", "869", "87", "87,093",
> "87,113", "87,297", "87,391", "87,614", "87,646", "87,669",
> "87,727", "87,866", "87,907", "870", "871", "872", "873",
> "873,069", "873,191", "874", "874,766", "874,881", "875",
> "876", "877", "878", "878,934", "879", "879,219", "879,296",
> "88", "88,158", "88,348", "88,382", "88,414", "88,448", "88,479",
> "88,565", "88,678", "88,697", "88,721", "88,757", "880",
> "880,500", "881", "882", "883", "884", "884,430", "885",
> "885,983", "886", "887", "887,472", "888", "889", "89", "89,199",
> "89,324", "89,418", "89,539", "89,562", "89,741", "89,911",
> "890", "890,941", "891", "891,379", "891,473", "892", "893",
> "893,032", "893,554", "894", "894,007", "894,290", "894,366",
> "895", "895,902", "896", "897", "897,079", "898", "899",
> "9", "9,000", "9,010", "9,011", "9,014", "9,018", "9,023",
> "9,024", "9,026", "9,029", "9,032", "9,033", "9,034", "9,035",
> "9,042", "9,051", "9,053", "9,055", "9,058", "9,063", "9,065",
> "9,067", "9,072", "9,084", "9,085", "9,086", "9,089", "9,090",
> "9,092", "9,096", "9,099", "9,100", "9,102", "9,109", "9,110",
> "9,112", "9,134", "9,141", "9,142", "9,145", "9,152", "9,154",
> "9,155", "9,158", "9,168", "9,180", "9,183", "9,186", "9,189",
> "9,200", "9,203", "9,205", "9,214", "9,219", "9,221", "9,226",
> "9,227", "9,229", "9,232", "9,234", "9,235", "9,239", "9,240",
> "9,242", "9,243", "9,250", "9,251", "9,254", "9,261", "9,268",
> "9,270", "9,279", "9,283", "9,290", "9,296", "9,297", "9,298",
> "9,300", "9,316", "9,318", "9,329", "9,332", "9,333", "9,335",
> "9,337", "9,347", "9,348", "9,354", "9,373", "9,374", "9,375",
> "9,380", "9,382", "9,387", "9,389", "9,390", "9,396", "9,400",
> "9,404", "9,406", "9,409", "9,410", "9,413", "9,415", "9,417",
> "9,423", "9,427", "9,428", "9,430", "9,441", "9,445", "9,447",
> "9,456", "9,465", "9,470", "9,475", "9,477", "9,479", "9,484",
> "9,486", "9,490", "9,496", "9,497", "9,501", "9,510", "9,519",
> "9,523", "9,524", "9,525", "9,528", "9,535", "9,544", "9,546",
> "9,558", "9,562", "9,568", "9,578", "9,586", "9,588", "9,589",
> "9,590", "9,591", "9,592", "9,596", "9,605", "9,612", "9,618",
> "9,620", "9,623", "9,640", "9,658", "9,662", "9,664", "9,675",
> "9,676", "9,679", "9,684", "9,690", "9,696", "9,697", "9,700",
> "9,702", "9,716", "9,717", "9,718", "9,719", "9,725", "9,727",
> "9,728", "9,735", "9,754,859", "9,756", "9,757", "9,769",
> "9,775", "9,776", "9,785", "9,791", "9,797", "9,799", "9,805",
> "9,810", "9,814", "9,820", "9,823", "9,824", "9,833", "9,845",
> "9,848", "9,851", "9,856", "9,858", "9,864", "9,867", "9,877",
> "9,879", "9,891", "9,901", "9,909", "9,917", "9,918", "9,924",
> "9,925", "9,936", "9,942", "9,946", "9,948", "9,952", "9,957",
> "9,964", "9,983", "9,984", "9,985", "9,989", "9,990", "9,995",
> "9,998", "9,999", "90", "90,000", "90,239", "90,405", "90,425",
> "90,536", "90,775", "90,799", "90,918", "90,960", "90,962",
> "900", "900,000", "901", "901,147", "901,416", "901,465",
> "902", "903", "903,850", "904", "904,516", "905", "905,050",
> "906", "906,183", "906,268", "907", "908", "908,067", "909",
> "909,048", "91", "91,039", "91,093", "91,144", "91,192",
> "91,233", "91,495", "91,517", "91,527", "91,672", "91,745",
> "91,826", "910", "911", "912", "913", "913,943", "914", "915",
> "915,367", "916", "917", "918", "919", "919,950", "92", "92,238",
> "92,318", "92,435", "92,603", "92,629", "920", "920,236",
> "921", "922", "923", "924", "925", "926", "926,953", "927",
> "927,003", "928", "928,576", "929", "93", "93,201", "93,206",
> "93,302", "93,971", "930", "930,682", "931", "932", "933",
> "934", "935", "936", "937", "938", "939", "94", "94,073",
> "94,121", "94,522", "94,574", "94,633", "94,855", "94,977",
> "940", "941", "943", "944", "944,222", "945", "946", "947",
> "949", "949,435", "949,789", "95", "95,091", "95,096", "95,137",
> "95,157", "95,275", "95,361", "95,488", "95,502", "95,640",
> "95,849", "950", "950,316", "950,559", "950,788", "952",
> "953", "954", "955", "955,200", "956", "956,484", "957",
> "958", "959", "96", "96,059", "96,111", "96,138", "96,205",
> "96,236", "96,346", "96,771", "96,848", "960", "961", "962",
> "962,790", "964", "964,170", "964,336", "965", "965,154",
> "965,166", "965,390", "966", "967", "967,753", "968,629",
> "969", "97", "97,473", "97,746", "97,759", "97,850", "97,978",
> "970", "970,225", "971", "971,405", "972", "973", "973,902",
> "974", "974,109", "974,557", "975", "976", "977", "977,452",
> "978", "98", "98,134", "98,385", "98,428", "98,495", "98,562",
> "98,955", "980", "980,556", "981", "981,998", "982", "982,526",
> "983", "984,148", "984,861", "985", "985,369", "986", "986,580",
> "987", "988", "989", "99", "99,115", "99,214", "99,409",
> "99,581", "99,834", "990", "992", "992,489", "993", "994",
> "994,195", "995", "996", "998", "999"), class = "factor"),
> Month = structure(c(11L, 11L, 7L, 2L, 2L, 12L, 11L, 11L,
>                    11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
> 11L,
>                    11L, 11L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L),
> .Label = c("Apr",
> 
> "Aug", "Dec", "Feb", "Jan", "Jul", "Jun", "Mar", "May", "Nov",
> 
> "Oct", "Sep"), class = "factor"), Year = c(2010L, 2010L,
> 
> 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,
> 
> 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,
> 
> 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,
> 
> 2011L)), .Names = c("DOC_TYPE", "DOC_NO", "DOC_DT", "SFX_CODE",
> 
> "CUSTOMER", "DOC_AMOUNT", "OS_ASON_RPT_DT", "OS_DAYS", "BILLING_BRANCH",
> 
> "COLL_BR", "RECEIPT_NO", "RECEIPT_DT", "Applied.Date", "RECEIPT_AMT",
> 
> "TDS_AMT", "REBATE", "Final", "Month", "Year"), row.names = c(NA,
> 
> 30L), class = "data.frame")
> 
> 
> Not sure if this would help.
>> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> Sent: Wednesday, December 30, 2015 10:23 PM
> To: SHIVI BHATIA <shivi.bhatia at safexpress.com>; r-help at r-project.org
> Subject: Re: [R] Dput Help in R
> 
> On 30/12/2015 5:56 AM, SHIVI BHATIA wrote:
>> Dear Team,
>> 
>> 
>> 
>> I am facing an error while performing a manipulation using a dplyr
> package.
>> In the code below, I am using mutate to build a new calculated column:
>> 
>> 
>> 
>> kp<-read.csv("collection_last.csv",header=TRUE)
>> 
>> mutate(kp,dif=DOC_AMOUNT-RECEIPT_AMT+TDS_AMT+REBATE)
>> 
>> 
>> 
>> However it gives an error:-
>> 
>> Warning messages:
>> 
>> 1: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>> 
>>   '-' not meaningful for factors
>> 
>> 2: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>> 
>>   '+' not meaningful for factors
>> 
>> 3: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>> 
>>   '+' not meaningful for factors
>> 
>> 
>> 
>> This is an error when some of my variables are factors hence I have
>> tried to change these to numeric so used the expression as:
>> 
>> kp$DOC_TYPE=as.numeric(kp$DOC_TYPE).
>> 
>> 
>> 
>> this now shows as variable type of as "double". So expedite help on
>> this one i was trying to create a reproducible example and i am highly
>> struggling to
>> 
>> create one. the data i have is approx. around 1 million rows with 21
>> columns hence when i use a dput option it does not capture the entire
>> detailing and row level info required to share and even
>> dput(head(kp$DOC_TYPE) does not help either.
>> 
>> I have seen many stack overflow & r help column before composing this
> email.
>> Hence i need help to create this reproducible example to share with
>> the experts in the community. Apologies if this is a repeat.
>> 
>> 
>> 
>> PLEASE HELP AS I AM HIGHLY STRUGGLING TO BUILD ANY OUTCOME.
> 
> If you are working with a dataframe or matrix named x, just use
> 
> y <- x[1:10,]
> 
> to extract the first 10 rows.  The error will probably occur with this
> subset as well, and dput() will give you a reasonably sized amount of
> output.  If the error doesn't happen, just take a bigger subset, and
> possibly leave off the beginning, e.g.
> 
> y <- x[101:110,]
> 
> for 10 lines starting at line 101.
> 
> Duncan Murdoch
> 
> This e-mail is confidential. It may also be legally privileged. If you are not the addressee you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return e-mail. Internet communications cannot be guaranteed to be timely, secure, error or virus-free. The sender does not accept liability for any errors or omissions.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.



R. Mark Sharp, Ph.D.
Director of Primate Records Database
Southwest National Primate Research Center
Texas Biomedical Research Institute
P.O. Box 760549
San Antonio, TX 78245-0549
Telephone: (210)258-9476
e-mail: msharp at TxBiomed.org








From bgunter.4567 at gmail.com  Thu Dec 31 18:27:13 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 31 Dec 2015 09:27:13 -0800
Subject: [R] bootstrapping statistics from leaken package
In-Reply-To: <568549AC.10408@unifi.it>
References: <568549AC.10408@unifi.it>
Message-ID: <CAGxFJbTum7-M7AWPnGNKBUot7dK4FfhKLK2JDcLEGhpg=gfTTw@mail.gmail.com>

Try

d= data[ ,i]

instead of d= data[i]

in your function. If that doesn't help, I think we would have to know
more about the structure of your data.

-- Bert



Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 31, 2015 at 7:28 AM, Benedetto Rocchi
<benedetto.rocchi at unifi.it> wrote:
> I am trying to generate a confidence interval for the statistic arpt (at
> risk of poverty threshold generated with the package leaken) using the
> package boot.
> This is my temptative script (where XH090 and DB030 and w are variables
> of an attached dataframe):
>> arpt.boot <- function(data,i){
>>   d = data[i]
>>   arpt <- arpr(d, sort=DB030, p=c(0.4))
>>   arpt <- as.vector(arpt)
>>  }
>> arpt40 <- boot(HX090,arpt.boot,R=1000,weights=w)
>> boot.ci(arpt40, conf = 0.95, type = "norm")
>
> R answer with the following error message:
>> Error in boot(dati_mauro$HX090, arpt.boot, R = 1000, weights =
>> dati_mauro$w) : incorrect number of subscripts on matrix
> Thank you in advance for any help in solving this problem.
> Benedetto Rocchi
>
> --
> Dipartimento di Scienze per l'Economia e l'Impresa
> Universit? degli Studi di Firenze
> Via delle Pandette, 9 50127 Firenze
> telefono 0552759706 cellulare 3204309360
>
>
>         [[alternative HTML version deleted]]
>
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec 31 18:30:03 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 31 Dec 2015 09:30:03 -0800
Subject: [R] Dput Help in R
In-Reply-To: <005101d1439c$87b26670$97173350$@safexpress.com>
References: <004201d142f0$adc2e3f0$0948abd0$@safexpress.com>
	<56840C05.3070401@gmail.com>
	<005101d1439c$87b26670$97173350$@safexpress.com>
Message-ID: <D6D40AEC-6158-45BC-A52F-B0D32EDF4379@comcast.net>


> On Dec 30, 2015, at 11:26 PM, SHIVI BHATIA <shivi.bhatia at safexpress.com> wrote:
> 
> Hi Duncan,
> Please find the dput from the data.
> 
> ab<-read.csv("collection_last.csv",header=TRUE)
> y<-ab[1:10,]
> 

This is (possibly) partial output from a dput call. Unable to repair at any rate.
> 
> ab<- "2,458", "2,461", "2,462", "2,463", "2,464", "2,465", "2,468",
> "2,469", "2,470", "2,473", "2,474", "2,475", "2,476", "2,477",
> "2,478", "2,479", "2,480", "2,483", "2,484,267", "2,485",
> 
snipped
> "99,581", "99,834", "990", "992", "992,489", "993", "994",
> "994,195", "995", "996", "998", "999"), class = "factor"),


It is useful in showing that these items (presumably the column named "Final" are factors. Notice the commas in the values you might think were numeric. You will need to remove the commas (probably with `gsub`) before using `as.numeric`.

I haven't quite figured out how a dataframe could have a factor column that was so much longer than the adjacent columns named "Month" and "Year". I would suggest redoing the read.csv with stringsAsFactor=FALSE so that you can then work on "pure" text before the coercion to numeric.

-- David.



> Month = structure(c(11L, 11L, 7L, 2L, 2L, 12L, 11L, 11L,
>                    11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L,
> 11L,
>                    11L, 11L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L),
> .Label = c("Apr",
> 
> "Aug", "Dec", "Feb", "Jan", "Jul", "Jun", "Mar", "May", "Nov",
> 
> "Oct", "Sep"), class = "factor"), Year = c(2010L, 2010L,
> 
> 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,
> 
> 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,
> 
> 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L, 2011L,
> 
> 2011L)), .Names = c("DOC_TYPE", "DOC_NO", "DOC_DT", "SFX_CODE",
> 
> "CUSTOMER", "DOC_AMOUNT", "OS_ASON_RPT_DT", "OS_DAYS", "BILLING_BRANCH",
> 
> "COLL_BR", "RECEIPT_NO", "RECEIPT_DT", "Applied.Date", "RECEIPT_AMT",
> 
> "TDS_AMT", "REBATE", "Final", "Month", "Year"), row.names = c(NA,
> 
> 30L), class = "data.frame")
> 
> 
> Not sure if this would help.
>> 
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> Sent: Wednesday, December 30, 2015 10:23 PM
> To: SHIVI BHATIA <shivi.bhatia at safexpress.com>; r-help at r-project.org
> Subject: Re: [R] Dput Help in R
> 
> On 30/12/2015 5:56 AM, SHIVI BHATIA wrote:
>> Dear Team,
>> 
>> 
>> 
>> I am facing an error while performing a manipulation using a dplyr
> package.
>> In the code below, I am using mutate to build a new calculated column:
>> 
>> 
>> 
>> kp<-read.csv("collection_last.csv",header=TRUE)
>> 
>> mutate(kp,dif=DOC_AMOUNT-RECEIPT_AMT+TDS_AMT+REBATE)
>> 
>> 
>> 
>> However it gives an error:-
>> 
>> Warning messages:
>> 
>> 1: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>> 
>>   '-' not meaningful for factors
>> 
>> 2: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>> 
>>   '+' not meaningful for factors
>> 
>> 3: In Ops.factor(c(28831L, 28831L, 17504L, 4184L, 36187L, 25819L, 699L,  :
>> 
>>   '+' not meaningful for factors
>> 
>> 
>> 
>> This is an error when some of my variables are factors hence I have
>> tried to change these to numeric so used the expression as:
>> 
>> kp$DOC_TYPE=as.numeric(kp$DOC_TYPE).
>> 
>> 
>> 
>> this now shows as variable type of as "double". So expedite help on
>> this one i was trying to create a reproducible example and i am highly
>> struggling to
>> 
>> create one. the data i have is approx. around 1 million rows with 21
>> columns hence when i use a dput option it does not capture the entire
>> detailing and row level info required to share and even
>> dput(head(kp$DOC_TYPE) does not help either.
>> 
>> I have seen many stack overflow & r help column before composing this
> email.
>> Hence i need help to create this reproducible example to share with
>> the experts in the community. Apologies if this is a repeat.
>> 
>> 
>> 
>> PLEASE HELP AS I AM HIGHLY STRUGGLING TO BUILD ANY OUTCOME.
> 
> If you are working with a dataframe or matrix named x, just use
> 
> y <- x[1:10,]
> 
> to extract the first 10 rows.  The error will probably occur with this
> subset as well, and dput() will give you a reasonably sized amount of
> output.  If the error doesn't happen, just take a bigger subset, and
> possibly leave off the beginning, e.g.
> 
> y <- x[101:110,]
> 
> for 10 lines starting at line 101.
> 
> Duncan Murdoch
> 
> This e-mail is confidential. It may also be legally privileged. If you are not the addressee you may not copy, forward, disclose or use any part of it. If you have received this message in error, please delete it and all copies from your system and notify the sender immediately by return e-mail. Internet communications cannot be guaranteed to be timely, secure, error or virus-free. The sender does not accept liability for any errors or omissions.
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From kathryn.lord2000 at gmail.com  Thu Dec 31 21:28:23 2015
From: kathryn.lord2000 at gmail.com (Kathryn Lord)
Date: Fri, 1 Jan 2016 05:28:23 +0900
Subject: [R] create one bigger matrix with one smaller matrix
Message-ID: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>

Dear R users,

Suppose that I have a matrix A

A <- matrix(c(1,2,3,4),2,2)
> A
     [,1] [,2]
[1,]    1    3
[2,]    2    4

With this matrix A, I'd like to create bigger one, for example,

      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
[,14]
 [1,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
 [2,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4
 [3,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
 [4,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4
 [5,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
 [6,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4
 [7,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
 [8,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4
 [9,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
[10,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4
[11,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
[12,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4
[13,]    1    3    1    3    1    3    1    3    1     3     1     3
1     3
[14,]    2    4    2    4    2    4    2    4    2     4     2     4
2     4


In fact, I want much bigger one. I wonder if there is an elegant way to do
this?

Any suggestions? Thank you!

Best wishes and Happy new year

Kathie

	[[alternative HTML version deleted]]


From giorgio.garziano at ericsson.com  Thu Dec 31 21:57:16 2015
From: giorgio.garziano at ericsson.com (Giorgio Garziano)
Date: Thu, 31 Dec 2015 20:57:16 +0000
Subject: [R] create one bigger matrix with one smaller matrix
Message-ID: <248E6FA047A8C746BA491485764190F53715FCF2@ESESSMB207.ericsson.se>

A <- matrix(c(1,2,3,4),2,2)

B <- matrix(A, nrow=14, ncol=14)

> B
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]
[1,]    1    3    1    3    1    3    1    3    1     3     1     3     1     3
[2,]    2    4    2    4    2    4    2    4    2     4     2     4     2     4
[3,]    3    1    3    1    3    1    3    1    3     1     3     1     3     1
[4,]    4    2    4    2    4    2    4    2    4     2     4     2     4     2
[5,]    1    3    1    3    1    3    1    3    1     3     1     3     1     3
[6,]    2    4    2    4    2    4    2    4    2     4     2     4     2     4
[7,]    3    1    3    1    3    1    3    1    3     1     3     1     3     1
[8,]    4    2    4    2    4    2    4    2    4     2     4     2     4     2
[9,]    1    3    1    3    1    3    1    3    1     3     1     3     1     3
[10,]    2    4    2    4    2    4    2    4    2     4     2     4     2     4
[11,]    3    1    3    1    3    1    3    1    3     1     3     1     3     1
[12,]    4    2    4    2    4    2    4    2    4     2     4     2     4     2
[13,]    1    3    1    3    1    3    1    3    1     3     1     3     1     3
[14,]    2    4    2    4    2    4    2    4    2     4     2     4     2     4
>


Happy New Year!

--
GG


	[[alternative HTML version deleted]]


From dwinsemius at comcast.net  Thu Dec 31 22:05:40 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 31 Dec 2015 13:05:40 -0800
Subject: [R] create one bigger matrix with one smaller matrix
In-Reply-To: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>
References: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>
Message-ID: <1CCA1E73-3561-4B51-9BC2-C8B3742213E3@comcast.net>


> On Dec 31, 2015, at 12:28 PM, Kathryn Lord <kathryn.lord2000 at gmail.com> wrote:
> 
> Dear R users,
> 
> Suppose that I have a matrix A
> 
> A <- matrix(c(1,2,3,4),2,2)
>> A
>     [,1] [,2]
> [1,]    1    3
> [2,]    2    4
> 
> With this matrix A, I'd like to create bigger one, for example,
> 
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
> [,14]
> [1,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [2,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [3,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [4,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [5,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [6,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [7,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [8,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [9,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [10,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [11,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [12,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [13,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [14,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> 
> 

str( do.call('rbind', rep(list( do.call('cbind' , rep(list(A), 20) ) ), 20) )  )

#result
 num [1:40, 1:40] 1 2 1 2 1 2 1 2 1 2 ...

The "upper left" corner:

do.call('rbind', rep(list( do.call('cbind' , rep(list(A), 20) ) ), 20) )[1:10, 1:10]
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
 [1,]    1    3    1    3    1    3    1    3    1     3
 [2,]    2    4    2    4    2    4    2    4    2     4
 [3,]    1    3    1    3    1    3    1    3    1     3
 [4,]    2    4    2    4    2    4    2    4    2     4
 [5,]    1    3    1    3    1    3    1    3    1     3
 [6,]    2    4    2    4    2    4    2    4    2     4
 [7,]    1    3    1    3    1    3    1    3    1     3
 [8,]    2    4    2    4    2    4    2    4    2     4
 [9,]    1    3    1    3    1    3    1    3    1     3
[10,]    2    4    2    4    2    4    2    4    2     4
 

> In fact, I want much bigger one. I wonder if there is an elegant way to do
> this?
> 
> Any suggestions? Thank you!
> 
> Best wishes and Happy new year
> 
> Kathie
> 
> 	[[alternative HTML version deleted]]

Your use of HTML for email is not appropriate for this list. It messed up your example although the intent was not that difficult to discrn.

-- 
David.

> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From a.rwooten at aol.com  Thu Dec 31 20:16:57 2015
From: a.rwooten at aol.com (Abram Wooten)
Date: Thu, 31 Dec 2015 13:16:57 -0600
Subject: [R] R:Exponent
Message-ID: <201512311917.tBVJH1xE000963@hypatia.math.ethz.ch>

Hello, 

I am new to R and I am having trouble adding an exponent to my problem. For instance,
 > mat5 <-matrix(c(0.5,0.5,0.25,0.75),2,byrow=T)
> mat6 <-matrix(c(0.5,0.5,0.25,0.75),2,byrow=T)
> mat5%^%2
Error: could not find function "%^%"

I unsure on weather I should square mat5 or use the two mats and square them both. I really need some help. 

Thanks,
Abram 
Sent from Mail for Windows 10


	[[alternative HTML version deleted]]


From jmhannon.ucdavis at gmail.com  Thu Dec 31 22:07:47 2015
From: jmhannon.ucdavis at gmail.com (Michael Hannon)
Date: Thu, 31 Dec 2015 13:07:47 -0800
Subject: [R] create one bigger matrix with one smaller matrix
In-Reply-To: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>
References: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>
Message-ID: <CACdH2ZasmaGCuN=GD7bdguXxKEkE-zsN2539r_HUcdTQDZFmwg@mail.gmail.com>

Something like:

A <- matrix(c(1,2,3,4),2,2)
A

B <- matrix(rep(A, 4), nrow=2)
B

C <- do.call(rbind, lapply(1:8, function(x) B))
C


On Thu, Dec 31, 2015 at 12:28 PM, Kathryn Lord
<kathryn.lord2000 at gmail.com> wrote:
> Dear R users,
>
> Suppose that I have a matrix A
>
> A <- matrix(c(1,2,3,4),2,2)
>> A
>      [,1] [,2]
> [1,]    1    3
> [2,]    2    4
>
> With this matrix A, I'd like to create bigger one, for example,
>
>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
> [,14]
>  [1,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [2,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [3,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [4,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [5,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [6,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [7,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [8,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [9,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [10,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [11,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [12,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [13,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [14,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>
>
> In fact, I want much bigger one. I wonder if there is an elegant way to do
> this?
>
> Any suggestions? Thank you!
>
> Best wishes and Happy new year
>
> Kathie
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From dwinsemius at comcast.net  Thu Dec 31 22:12:00 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 31 Dec 2015 13:12:00 -0800
Subject: [R] R:Exponent
In-Reply-To: <201512311917.tBVJH1xE000963@hypatia.math.ethz.ch>
References: <201512311917.tBVJH1xE000963@hypatia.math.ethz.ch>
Message-ID: <29DB6292-E6EE-4001-9569-0150A626934A@comcast.net>


> On Dec 31, 2015, at 11:16 AM, Abram Wooten via R-help <r-help at r-project.org> wrote:
> 
> Hello, 
> 
> I am new to R and I am having trouble adding an exponent to my problem. For instance,
>> mat5 <-matrix(c(0.5,0.5,0.25,0.75),2,byrow=T)
>> mat6 <-matrix(c(0.5,0.5,0.25,0.75),2,byrow=T)
>> mat5%^%2
> Error: could not find function "%^%"

The package expm provides matrix exponentials. I think that package Matrix has the same capability, although its help page says that expm::expm is likely to be faster.

-- 
David
> 
> I unsure on weather I should square mat5 or use the two mats and square them both. I really need some help. 
> 
> Thanks,
> Abram 
> Sent from Mail for Windows 10
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

David Winsemius
Alameda, CA, USA


From dwinsemius at comcast.net  Thu Dec 31 22:16:12 2015
From: dwinsemius at comcast.net (David Winsemius)
Date: Thu, 31 Dec 2015 13:16:12 -0800
Subject: [R] R:Exponent
In-Reply-To: <29DB6292-E6EE-4001-9569-0150A626934A@comcast.net>
References: <201512311917.tBVJH1xE000963@hypatia.math.ethz.ch>
	<29DB6292-E6EE-4001-9569-0150A626934A@comcast.net>
Message-ID: <2A2FB38F-9C9B-4B06-AB7F-6DD585921905@comcast.net>


> On Dec 31, 2015, at 1:12 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
>> 
>> On Dec 31, 2015, at 11:16 AM, Abram Wooten via R-help <r-help at r-project.org> wrote:
>> 
>> Hello, 
>> 
>> I am new to R and I am having trouble adding an exponent to my problem. For instance,
>>> mat5 <-matrix(c(0.5,0.5,0.25,0.75),2,byrow=T)
>>> mat6 <-matrix(c(0.5,0.5,0.25,0.75),2,byrow=T)
>>> mat5%^%2
>> Error: could not find function "%^%"
> 
> The package expm provides matrix exponentials. I think that package Matrix has the same capability, although its help page says that expm::expm is likely to be faster.

And I think I may have conflated matrix power function with matrix exponential. Nonetheless, one can also get a `matpow` function from the same package:


?expm::`%^%`

# returns page for 
matpow {expm}


> 
> -- 
> David
>> 
>> I unsure on weather I should square mat5 or use the two mats and square them both. I really need some help. 
>> 
>> Thanks,
>> Abram 
>> Sent from Mail for Windows 10
>> 

David Winsemius
Alameda, CA, USA


From r.turner at auckland.ac.nz  Thu Dec 31 22:16:38 2015
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Fri, 1 Jan 2016 10:16:38 +1300
Subject: [R] [FORGED]  Histogram for Left Censored Data
In-Reply-To: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
Message-ID: <56859B36.4010202@auckland.ac.nz>

On 31/12/15 23:20, Steven Stoline wrote:
> Dear All:
>
> I need helps with creating histograms for data that include left
> censored observations.
>
> Here is an example of left censored data
>
>
>
> *Sulfate.Concentration*
><-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,
> 1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
>
>
> *Column 2* is an indicator for censoring "*1*" for left censored
> observations and "*0*" for non-censored (fully measured)
> observations.

And what, pray tell, do you want the resulting histogram to look like?
See e.g. fortune("mind_read").

cheers,

Rolf Turner

-- 
Technical Editor ANZJS
Department of Statistics
University of Auckland
Phone: +64-9-373-7599 ext. 88276


From bgunter.4567 at gmail.com  Thu Dec 31 22:31:20 2015
From: bgunter.4567 at gmail.com (Bert Gunter)
Date: Thu, 31 Dec 2015 13:31:20 -0800
Subject: [R] create one bigger matrix with one smaller matrix
In-Reply-To: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>
References: <CAMFx86xmfWi6kDDUZtXFewmnOMMzqMn0eUfDbAeUE9akMe5mCA@mail.gmail.com>
Message-ID: <CAGxFJbRCpCs5BM73jfaLYTLaSJJm7TkUFWhFYReeybYSZbx_hw@mail.gmail.com>

Well, all of the solutions proposed are a bit tricky in that the
matrix must be "constructed" by hand. They are also reinventing
wheels. What I think you really want is the kronecker product, which
is the matrix operation that does exactly what you want. e.g.

A <- matrix(1:4, nr=2)

to create a new matrix with e.g. 2 x 6 "positions" in each of which is
A, simply do

kronecker (matrix(1, nr = 2, nc=6), A)

or if you want to use the operator form:

matrix(1,nr=2, nc=6) %x% A

See ?kronecker
Bert Gunter

"The trouble with having an open mind is that people keep coming along
and sticking things into it."
-- Opus (aka Berkeley Breathed in his "Bloom County" comic strip )


On Thu, Dec 31, 2015 at 12:28 PM, Kathryn Lord
<kathryn.lord2000 at gmail.com> wrote:
> Dear R users,
>
> Suppose that I have a matrix A
>
> A <- matrix(c(1,2,3,4),2,2)
>> A
>      [,1] [,2]
> [1,]    1    3
> [2,]    2    4
>
> With this matrix A, I'd like to create bigger one, for example,
>
>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
> [,14]
>  [1,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [2,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [3,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [4,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [5,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [6,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [7,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
>  [8,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>  [9,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [10,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [11,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [12,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
> [13,]    1    3    1    3    1    3    1    3    1     3     1     3
> 1     3
> [14,]    2    4    2    4    2    4    2    4    2     4     2     4
> 2     4
>
>
> In fact, I want much bigger one. I wonder if there is an elegant way to do
> this?
>
> Any suggestions? Thank you!
>
> Best wishes and Happy new year
>
> Kathie
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.


From drjimlemon at gmail.com  Thu Dec 31 23:01:54 2015
From: drjimlemon at gmail.com (Jim Lemon)
Date: Fri, 1 Jan 2016 09:01:54 +1100
Subject: [R] Histogram for Left Censored Data
In-Reply-To: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
References: <CAHDp66DXOV=0+0iC5wORhOQZC9TQeGd0UbFqzDau0WqUrvz6Hw@mail.gmail.com>
Message-ID: <CA+8X3fV4+-Ta8YY=AAnu+VH+rTzH3yx69NyyeNGZhwC+3mX-XA@mail.gmail.com>

Hi Steve,
Maybe something like this:

Sconc<-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,
 1770,1790,1780,1850,1760,1450,1710,1575,1475,1780,1790,
 1780,1450,1790,1800,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),
 24,2)
hist(Sconc[,1],breaks=c(1450,1550,1650,1750,1900))
abline(v=1450,col="red")
text(1550,0.003,"limit of measurement")
arrows(1480,0.003,1450,0.003)

Jim


On Thu, Dec 31, 2015 at 9:20 PM, Steven Stoline <sstoline at gmail.com> wrote:

> Dear All:
>
> I need helps with creating histograms for data that include left censored
> observations.
>
> Here is an example of left censored data
>
>
>
> *Sulfate.Concentration*
>
> <-matrix(c(1450,1800,1840,1820,1860,1780,1760,1800,1900,1770,1790,1780,1850,1760,1450,1710,1575,1475,1780,1790,1780,1450,1790,1800,
> 1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0),24,2)
>
>
> *Column 2* is an indicator for censoring "*1*" for left censored
> observations and "*0*" for non-censored (fully measured) observations.
>
> with many thanks
> steve
>
> --
> Steven M. Stoline
> 1123 Forest Avenue
> Portland, ME 04112
> sstoline at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide
> http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

	[[alternative HTML version deleted]]


